[
  {
    "id" : "3dee71f3-8ab5-4e37-9760-d5312bf5c4d9",
    "prId" : 35932,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35932#pullrequestreview-6741909",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aeccaeee-17ef-4287-9ccd-c27034bcde35",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "This is only called at the end right?  \n",
        "createdAt" : "2016-11-01T19:37:38Z",
        "updatedAt" : "2016-11-03T17:53:52Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "3357572e-49bc-4a0b-b492-a438f736dcfb",
        "parentId" : "aeccaeee-17ef-4287-9ccd-c27034bcde35",
        "authorId" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "body" : "yup.  `ScheduleOne` is the call that will grab the evnts and push em to etcd.\n",
        "createdAt" : "2016-11-02T02:43:41Z",
        "updatedAt" : "2016-11-03T17:53:52Z",
        "lastEditedBy" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d5bc6759e4751f941a348a1f990ebb7e828673d",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +47,51 @@\n// Error returns detailed information of why the pod failed to fit on each node\nfunc (f *FitError) Error() string {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(fmt.Sprintf(\"pod (%s) failed to fit in any node\\n\", f.Pod.Name))"
  },
  {
    "id" : "a2b1e3db-1eb4-4b98-afef-53f86a854a3d",
    "prId" : 35932,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35932#pullrequestreview-6883709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c096d659-41db-4e6f-a609-e4cc36e32440",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "We're still storing internally (N), it's just the outside summary will now be a bin'd histogram.  \n",
        "createdAt" : "2016-11-02T15:32:06Z",
        "updatedAt" : "2016-11-03T17:53:52Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "999c0c88-8606-4ee5-b539-5e47f4fd855d",
        "parentId" : "c096d659-41db-4e6f-a609-e4cc36e32440",
        "authorId" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "body" : "event summation will be rollup, store O(unique failure types)\n",
        "createdAt" : "2016-11-02T15:57:39Z",
        "updatedAt" : "2016-11-03T17:53:52Z",
        "lastEditedBy" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "tags" : [
        ]
      },
      {
        "id" : "94a4bde2-ead1-40d4-8c9c-97d726bb89a0",
        "parentId" : "c096d659-41db-4e6f-a609-e4cc36e32440",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Yeah I get that, just denoting that internally we are storing f.FailedPredicates which will still be (N).  \n",
        "createdAt" : "2016-11-02T18:15:49Z",
        "updatedAt" : "2016-11-03T17:53:52Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "ec1fe7f7-4043-4339-97dd-b50f470cb761",
        "parentId" : "c096d659-41db-4e6f-a609-e4cc36e32440",
        "authorId" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "body" : "yup, the parellelization factor i think will determine the total memory consumed by failed nodes \n",
        "createdAt" : "2016-11-02T18:43:36Z",
        "updatedAt" : "2016-11-03T17:53:52Z",
        "lastEditedBy" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d5bc6759e4751f941a348a1f990ebb7e828673d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +51,55 @@\tbuf.WriteString(fmt.Sprintf(\"pod (%s) failed to fit in any node\\n\", f.Pod.Name))\n\treasons := make(map[string]int)\n\tfor _, predicates := range f.FailedPredicates {\n\t\tfor _, pred := range predicates {\n\t\t\treasons[pred.GetReason()] += 1"
  },
  {
    "id" : "25636ec8-854e-4f97-8a7d-66d2ccbe086a",
    "prId" : 31606,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b2f5286-c550-45ea-963c-9f038240fe75",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Shouldn't it run in yet another go-routine?\n",
        "createdAt" : "2016-09-06T10:40:49Z",
        "updatedAt" : "2016-09-06T10:40:49Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "4901a8e8-e06f-4bcc-b042-be593cdbfbd6",
        "parentId" : "9b2f5286-c550-45ea-963c-9f038240fe75",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "At least not initially. Once everything is using this pattern, we will be in better position for optimizations and parallelization at the framework basis.\n",
        "createdAt" : "2016-09-06T10:47:04Z",
        "updatedAt" : "2016-09-06T10:47:04Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "33c710adf06ec844ed65127aaab14fc2bc2bf79c",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +272,276 @@\t\t\t\tprioritizedList := make(schedulerapi.HostPriorityList, 0, len(nodes))\n\t\t\t\tfor i := range nodes {\n\t\t\t\t\thostResult, err := config.Map(pod, meta, nodeNameToInfo[nodes[i].Name])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err"
  },
  {
    "id" : "f4ce4517-87e4-4816-a761-c52b66d5f395",
    "prId" : 28104,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b51246a5-5e39-459d-95e9-1f78e70c3814",
        "parentId" : null,
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "What if there are multiple extenders overriding the same node name?\n",
        "createdAt" : "2016-06-29T17:29:18Z",
        "updatedAt" : "2016-08-04T06:05:18Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      },
      {
        "id" : "c74cd3fa-b359-4482-98e7-0f99464531bf",
        "parentId" : "b51246a5-5e39-459d-95e9-1f78e70c3814",
        "authorId" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "body" : "It's no problem because  the \"nodes\" parameter of Filter is  the filtered  nodes  of  the  last extender, the  next extender  will  filter  from  it.\n",
        "createdAt" : "2016-06-29T21:57:44Z",
        "updatedAt" : "2016-08-04T06:05:18Z",
        "lastEditedBy" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "tags" : [
        ]
      },
      {
        "id" : "0f78ee6b-523a-4f47-944f-13b121f23282",
        "parentId" : "b51246a5-5e39-459d-95e9-1f78e70c3814",
        "authorId" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "body" : "Ideally failedPredicateMap should be failedNodeMap as a node can fail because of a predicate or an extender.\n",
        "createdAt" : "2016-07-12T18:53:53Z",
        "updatedAt" : "2016-08-04T06:05:18Z",
        "lastEditedBy" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "tags" : [
        ]
      }
    ],
    "commit" : "4106eb70b0b8b729184b95c20a721fae5d333aef",
    "line" : null,
    "diffHunk" : "@@ -1,1 +190,194 @@\n\t\t\tfor failedNodeName, failedMsg := range failedMap {\n\t\t\t\tfailedPredicateMap[failedNodeName] = failedMsg\n\t\t\t}\n\t\t\tfiltered = filteredList"
  },
  {
    "id" : "c956846b-e213-4fbf-9974-d5f354af7977",
    "prId" : 19907,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7bae002d-8a3e-4e3b-b59a-740875520d94",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "You can do it in one pass if you do idx = idx[:0] here...\n",
        "createdAt" : "2016-01-26T01:11:31Z",
        "updatedAt" : "2016-02-08T17:43:46Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "a24afcac-08d3-42bf-a98f-3dd45945e8c5",
        "parentId" : "7bae002d-8a3e-4e3b-b59a-740875520d94",
        "authorId" : "ad0a3561-db60-47a9-b82f-ac613199f968",
        "body" : "Agreed, this should be done in one pass.\n",
        "createdAt" : "2016-01-28T19:18:17Z",
        "updatedAt" : "2016-02-08T17:43:46Z",
        "lastEditedBy" : "ad0a3561-db60-47a9-b82f-ac613199f968",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a36dfb306c248070accfbbe7b6a24482ba7e450",
    "line" : null,
    "diffHunk" : "@@ -1,1 +114,118 @@\tfor i, entry := range priorityList {\n\t\tif entry.Score > maxScore {\n\t\t\tmaxScore = entry.Score\n\t\t\tidx = []int{i}\n\t\t} else if entry.Score == maxScore {"
  },
  {
    "id" : "f448577e-495b-42df-9f1d-f7225a09c4f7",
    "prId" : 19527,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f23d649-b592-4c6a-9c9c-51620b09812c",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "for this case can you do some kind of assertion that !fit ?\n",
        "createdAt" : "2016-01-12T07:56:16Z",
        "updatedAt" : "2016-01-12T16:47:02Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "252a956150e7862255961007c74a31cd89a362bf",
    "line" : null,
    "diffHunk" : "@@ -1,1 +131,135 @@\t\t\tif err != nil {\n\t\t\t\tswitch e := err.(type) {\n\t\t\t\tcase *predicates.InsufficientResourceError:\n\t\t\t\t\tif fit {\n\t\t\t\t\t\terr := fmt.Errorf(\"got InsufficientResourceError: %v, but also fit='true' which is unexpected\", e)"
  },
  {
    "id" : "16b4e4e3-67b4-4666-abe6-02e7ad655599",
    "prId" : 19527,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6a8b99a-7b8e-4847-ac10-5d2135066e8b",
        "parentId" : null,
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "@davidopp \nAddressed.\n",
        "createdAt" : "2016-01-12T16:47:58Z",
        "updatedAt" : "2016-01-12T16:47:58Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      }
    ],
    "commit" : "252a956150e7862255961007c74a31cd89a362bf",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +132,136 @@\t\t\t\tswitch e := err.(type) {\n\t\t\t\tcase *predicates.InsufficientResourceError:\n\t\t\t\t\tif fit {\n\t\t\t\t\t\terr := fmt.Errorf(\"got InsufficientResourceError: %v, but also fit='true' which is unexpected\", e)\n\t\t\t\t\t\treturn api.NodeList{}, FailedPredicateMap{}, err"
  },
  {
    "id" : "961749b1-1cb1-4af3-86e5-3320957ffdd5",
    "prId" : 18413,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46ca743c-0238-4dea-847b-4329f1a15846",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I think locking & unlocking in every single look is very inefficient.\nAlso, making a copy of every element in the list doesn't make much sense to.\n\nI suggest changing to:\n\nmu.Lock()\nfor i := range prioritizedList {\n  combinedScores[prioritizedList[i].Host] += hostEntry.Score \\* weight\n}\nmu.Unlock()\n",
        "createdAt" : "2015-12-18T14:50:45Z",
        "updatedAt" : "2015-12-19T03:14:03Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "d5c4a08d-7258-4f0b-b499-891748bbf6b0",
        "parentId" : "46ca743c-0238-4dea-847b-4329f1a15846",
        "authorId" : "7d5c8d70-3142-473f-b353-8238213d5024",
        "body" : "IMHO\n\n```\ncombinedScores[hostEntry.Host] += hostEntry.Score * weight\n```\n\nmore readable than\n\n```\ncombinedScores[prioritizedList[i].Host] += hostEntry.Score * weight\n```\n",
        "createdAt" : "2015-12-18T16:22:56Z",
        "updatedAt" : "2015-12-19T03:14:03Z",
        "lastEditedBy" : "7d5c8d70-3142-473f-b353-8238213d5024",
        "tags" : [
        ]
      },
      {
        "id" : "a5740297-f5ea-4f6c-ae59-56c3a1114030",
        "parentId" : "46ca743c-0238-4dea-847b-4329f1a15846",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "It is more readable but it also is MUCH SLOWER.\n",
        "createdAt" : "2015-12-18T16:41:13Z",
        "updatedAt" : "2015-12-19T03:14:03Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "686071c4-a9b9-49db-9447-0bdf951ea7f0",
        "parentId" : "46ca743c-0238-4dea-847b-4329f1a15846",
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "I am not sure about the performance impact. My guess is that it is minimum. But avoiding a copy will not hurt me a lot. Will do. \n",
        "createdAt" : "2015-12-18T18:13:16Z",
        "updatedAt" : "2015-12-19T03:14:03Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f4f754106caa4ec24f96ec6fd57c68b19608316",
    "line" : null,
    "diffHunk" : "@@ -1,1 +198,202 @@\t\t\t\tmu.Lock()\n\t\t\t\terrs = append(errs, err)\n\t\t\t\tmu.Unlock()\n\t\t\t\treturn\n\t\t\t}"
  },
  {
    "id" : "7ab812ba-a232-4741-9334-1b4d6f56b10e",
    "prId" : 18413,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3e4c90f-f864-4716-bf0f-fd0ec97ba008",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Also, if we change to use WaitGroup in both loops, we can probably wait once at the end and parallelize even more.\n",
        "createdAt" : "2015-12-18T15:00:07Z",
        "updatedAt" : "2015-12-19T03:14:03Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "ae9ea30e-d3ee-4e2f-be6d-37da710692ab",
        "parentId" : "e3e4c90f-f864-4716-bf0f-fd0ec97ba008",
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "I would rather keep the previous behavior. We can revisit this once there is a real issue. (I am not sure about this since we do not have any test with extender, and we probably do not want to do optimization for extender.)\n",
        "createdAt" : "2015-12-18T18:14:25Z",
        "updatedAt" : "2015-12-19T03:14:03Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f4f754106caa4ec24f96ec6fd57c68b19608316",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +241,245 @@\t// wait for all go routines to finish\n\twg.Wait()\n\n\tfor host, score := range combinedScores {\n\t\tglog.V(10).Infof(\"Host %s Score %d\", host, score)"
  },
  {
    "id" : "4eeadca9-4595-47ce-875a-e6d9c49e48a4",
    "prId" : 13580,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2eff5420-138b-452b-9304-6196ebd20806",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "after this line, maybe\nif len(filtered) == 0 break\n",
        "createdAt" : "2015-10-18T07:23:58Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "4cb91504-e057-46ce-aace-e0dcce80539c",
        "parentId" : "2eff5420-138b-452b-9304-6196ebd20806",
        "authorId" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "body" : "Done\n",
        "createdAt" : "2015-11-13T02:51:16Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "tags" : [
        ]
      }
    ],
    "commit" : "cadc24e9fd7f2bccc972df4d67985aa33a4cd823",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +147,151 @@\t\t\t\treturn api.NodeList{}, FailedPredicateMap{}, err\n\t\t\t}\n\t\t\tfiltered = filteredList.Items\n\t\t\tif len(filtered) == 0 {\n\t\t\t\tbreak"
  },
  {
    "id" : "43b286be-6338-4021-98f8-f3b3c0b3118d",
    "prId" : 13580,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3632302a-f150-4c93-9e9e-3ac1ba701ebf",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "should you also pass the full machinetoPods map, since predicate() on L121 gets it (for the node on the current iteration)?\n",
        "createdAt" : "2015-10-18T07:25:51Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "da3cbbbd-ecb2-4bbc-b5e6-cd3e6f220758",
        "parentId" : "3632302a-f150-4c93-9e9e-3ac1ba701ebf",
        "authorId" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "body" : "Same concern as above with sending a large pod list\n",
        "createdAt" : "2015-10-20T06:51:36Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "tags" : [
        ]
      },
      {
        "id" : "b05989d8-4940-4c84-a623-8c2bdeb1c011",
        "parentId" : "3632302a-f150-4c93-9e9e-3ac1ba701ebf",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Yeah, this is a really painful decision. Without passing the list of pods running on each machine, the extender writer is really limited in what they can do. For example, they can't do anything that requires knowing the free and used resources on the machine (since that requires knowing the requests of the pods on the machine), or implement a policy that avoids conflicts between the pod-to-schedule and already-running pods. OTOH I think the whole mechanism will be impossibly slow if we pass the list of pods on each machine. This is why this mechanism is really a bad way to extend the scheduler. I wish you would consider some other approach, like figuring out a way we can dynamically link scheduling policies (predicates and priorities) into the existing scheduler. (Dynamically link meaning requires restarting the scheduler process but not recompiling.) Since we already agreed to let you do this, I'm going to say what you have here is OK, but I have a feeling you're the only person who is ever going to use this extension mechanism.\n",
        "createdAt" : "2015-11-13T10:38:01Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "d53bac49-bd8d-4fab-89bb-7eb833a843ab",
        "parentId" : "3632302a-f150-4c93-9e9e-3ac1ba701ebf",
        "authorId" : "766f642e-1622-4803-803b-05ce306fc30e",
        "body" : "I feel the opposite, when you give someone such a simple interface they are going to use it, rather than 'doing it right'...\n",
        "createdAt" : "2015-11-13T14:00:32Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "766f642e-1622-4803-803b-05ce306fc30e",
        "tags" : [
        ]
      },
      {
        "id" : "cd193828-2149-46aa-ba8e-a8a7624289d3",
        "parentId" : "3632302a-f150-4c93-9e9e-3ac1ba701ebf",
        "authorId" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "body" : "I would like to have a clean split of responsibilities between k8s scheduler and the extender. k8s will do the cpu/mem/label/... based scheduling. extender is limited to solving use cases beyond what k8s provides, like storage and network aware scheduling. I dont want the extender to be very rich, then it gravitates towards being another scheduler on its own.\n",
        "createdAt" : "2015-11-13T15:58:14Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "tags" : [
        ]
      },
      {
        "id" : "28526714-0369-4c4a-8b02-619749b60d3e",
        "parentId" : "3632302a-f150-4c93-9e9e-3ac1ba701ebf",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "> like storage and network aware scheduling. I dont want the extender to be very rich, then it gravitates towards being another scheduler on its own.\n\nthat statement is antithetical.  \n",
        "createdAt" : "2015-11-13T16:09:09Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "b63de58d-4f39-4b7d-91d0-45b8417e11c0",
        "parentId" : "3632302a-f150-4c93-9e9e-3ac1ba701ebf",
        "authorId" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "body" : "As I was explaining in other comments, the intention is to have a split of responsibilities, not overlapping.\n",
        "createdAt" : "2015-11-13T16:49:26Z",
        "updatedAt" : "2015-11-25T16:19:55Z",
        "lastEditedBy" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "tags" : [
        ]
      }
    ],
    "commit" : "cadc24e9fd7f2bccc972df4d67985aa33a4cd823",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +143,147 @@\tif len(filtered) > 0 && len(extenders) != 0 {\n\t\tfor _, extender := range extenders {\n\t\t\tfilteredList, err := extender.Filter(pod, &api.NodeList{Items: filtered})\n\t\t\tif err != nil {\n\t\t\t\treturn api.NodeList{}, FailedPredicateMap{}, err"
  },
  {
    "id" : "28a4e8a3-7e8f-4643-b98b-65e82eb3c150",
    "prId" : 11788,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b2c2424-1cf2-4ed6-a872-7a463ea6a5b7",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I'm sure I'm missing something obvious, but how does FailedResourceType get passed form CheckPodsExceedingCapacity() into here?\n",
        "createdAt" : "2015-07-27T05:04:57Z",
        "updatedAt" : "2015-08-07T08:57:26Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "5db375e8-4632-40d9-8755-53716d39d2a5",
        "parentId" : "8b2c2424-1cf2-4ed6-a872-7a463ea6a5b7",
        "authorId" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "body" : "FailedResourceType is given value at https://github.com/HaiyangDING/kubernetes/blob/ImproveClarityResource/plugin/pkg/scheduler/algorithm/predicates/predicates.go#L168 (and line 163/173 for other 2 cases respectively). Since `FailedResourceType`  can be called from outside package predicates, so I just use it here.  Every time before using it, `FailedResourceType`  is set to `\"\"` at https://github.com/HaiyangDING/kubernetes/blob/ImproveClarityResource/plugin/pkg/scheduler/generic_scheduler.go#L119.\n",
        "createdAt" : "2015-07-27T05:57:48Z",
        "updatedAt" : "2015-08-07T08:57:26Z",
        "lastEditedBy" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "tags" : [
        ]
      },
      {
        "id" : "217861e1-f0eb-42fa-92f6-98e3c7b44da1",
        "parentId" : "8b2c2424-1cf2-4ed6-a872-7a463ea6a5b7",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I see. Using a global (package-level) variable is a little janky but we need to clean up the whole scheduler and can take care of it then.\n",
        "createdAt" : "2015-08-07T08:17:03Z",
        "updatedAt" : "2015-08-07T08:57:26Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "dab7280ae4656d9a05207571a71c56b020aa35c8",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +128,132 @@\t\t\t\t}\n\t\t\t\tif predicates.FailedResourceType != \"\" {\n\t\t\t\t\tfailedPredicateMap[node.Name].Insert(predicates.FailedResourceType)\n\t\t\t\t\tbreak\n\t\t\t\t}"
  },
  {
    "id" : "55434e76-00d4-430f-9023-7501e1940582",
    "prId" : 10661,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7ec63a0-0117-452d-b68c-d586c3540134",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "From what I've seen we use V(4) for 'full logging', but I may be mistaken.\n",
        "createdAt" : "2015-07-03T09:57:05Z",
        "updatedAt" : "2015-07-06T04:32:38Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "70f2f566-7a12-4e14-88ac-0e9d5a287548",
        "parentId" : "c7ec63a0-0117-452d-b68c-d586c3540134",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "I think we started V(10)'ing because we found that clients with large clusters given us huge logs filled with too much noise, since this happens for each pod for each node. \n",
        "createdAt" : "2015-07-05T21:53:39Z",
        "updatedAt" : "2015-07-06T04:32:38Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "4e8bcfd5-981c-4069-bc2d-68e4ea980e1c",
        "parentId" : "c7ec63a0-0117-452d-b68c-d586c3540134",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "It's OK, I just remember working in Kubelet or some controller, where we put logs of pretty much every step in V(4). I just think we should be consistent across components.\n",
        "createdAt" : "2015-07-06T07:46:46Z",
        "updatedAt" : "2015-07-06T07:49:29Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e3f2ea20bcca76cb37c95363aa597d6757aa543",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +166,170 @@\t}\n\tfor host, score := range combinedScores {\n\t\tglog.V(10).Infof(\"Host %s Score %d\", host, score)\n\t\tresult = append(result, algorithm.HostPriority{Host: host, Score: score})\n\t}"
  },
  {
    "id" : "ef585b29-7a9e-4598-9bd3-7493b34ca0de",
    "prId" : 10661,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4626aff2-bb8c-4db2-a9d6-bcdc86bc6103",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Is this better placed in the priorities package? \n",
        "createdAt" : "2015-07-05T21:53:38Z",
        "updatedAt" : "2015-07-06T04:32:38Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "cfc33c57-c923-40e2-9cc5-c6943b11c4b3",
        "parentId" : "4626aff2-bb8c-4db2-a9d6-bcdc86bc6103",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Probably. But either way it needs to be a public function because it is now called from both priorities package (due to my new test) and scheduler package (existing usage in generic scheduler). So I'm not sure this helps much?\n",
        "createdAt" : "2015-07-05T22:42:07Z",
        "updatedAt" : "2015-07-06T04:32:38Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e3f2ea20bcca76cb37c95363aa597d6757aa543",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +140,144 @@// The minion scores returned by the priority function are multiplied by the weights to get weighted scores\n// All scores are finally combined (added) to get the total weighted scores of all minions\nfunc PrioritizeNodes(pod *api.Pod, podLister algorithm.PodLister, priorityConfigs []algorithm.PriorityConfig, minionLister algorithm.MinionLister) (algorithm.HostPriorityList, error) {\n\tresult := algorithm.HostPriorityList{}\n"
  },
  {
    "id" : "16fc630c-c134-4c00-b953-4a45676e51e0",
    "prId" : 9341,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e62aa89-b8be-4365-8dcb-442d5db607c5",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "this is the sort of thing that should go on a scheduler status page...\n",
        "createdAt" : "2015-06-05T22:41:50Z",
        "updatedAt" : "2015-06-06T01:53:09Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "6df24d62-61d9-497e-beff-1d19b2749b28",
        "parentId" : "0e62aa89-b8be-4365-8dcb-442d5db607c5",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "seems fine here until we have a scheduler status page (and I think what you mean is more of a customized per-pod \"why pending\" page rather than a generic scheduler status page)\n",
        "createdAt" : "2015-06-05T22:59:31Z",
        "updatedAt" : "2015-06-06T01:53:09Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "6ae5af7a-361b-41b6-af86-4e9ed6c7c839",
        "parentId" : "0e62aa89-b8be-4365-8dcb-442d5db607c5",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yes, that's what I meant and I didn't mean to ask for a status page in this PR :)\n",
        "createdAt" : "2015-06-05T23:09:56Z",
        "updatedAt" : "2015-06-06T01:53:09Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bb3efaaad6013b8f5bea2da956bd96a08efaf85",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +45,49 @@\tfor node, predicateList := range f.FailedPredicates {\n\t\tpredicates = predicates.Union(predicateList)\n\t\tglog.Infof(\"failed to find fit for pod %v on node %s: %s\", f.Pod.Name, node, strings.Join(predicateList.List(), \",\"))\n\t}\n\treturn fmt.Sprintf(\"For each of these fitness predicates, pod %v failed on at least one node: %v.\", f.Pod.Name, strings.Join(predicates.List(), \",\"))"
  },
  {
    "id" : "88b80af0-9104-4ce1-b0ec-faa3a3e343c4",
    "prId" : 8532,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a879a0a-a838-43e9-97c7-09b663ee3f55",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Print pod's namespace & name, and add line separators?\n",
        "createdAt" : "2015-05-20T23:58:57Z",
        "updatedAt" : "2015-05-20T23:58:57Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "8ab4721f-c972-47f5-8c63-45c95529f3c2",
        "parentId" : "7a879a0a-a838-43e9-97c7-09b663ee3f55",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "The event is in the namespace of the pod so that seems redundant. Line separator is fine. Also the pod is the event.involvedObject so name is know from there. \n\nSent from my iPhone\n\n> On May 20, 2015, at 7:59 PM, Daniel Smith notifications@github.com wrote:\n> \n> In plugin/pkg/scheduler/generic_scheduler.go:\n> \n> > @@ -40,7 +40,7 @@ var ErrNoNodesAvailable = fmt.Errorf(\"no nodes available to schedule pods\")\n> > \n> >  // implementation of the error interface\n> >  func (f *FitError) Error() string {\n> > -   output := fmt.Sprintf(\"failed to find fit for pod: %v\", f.Pod)\n> > -   output := fmt.Sprintf(\"failed to find fit for pod, \")\n> >   Print pod's namespace & name, and add line separators?\n> \n> â€”\n> Reply to this email directly or view it on GitHub.\n",
        "createdAt" : "2015-05-21T00:35:14Z",
        "updatedAt" : "2015-05-21T00:35:14Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ee06a99834ca3676f325e1bf0d19a126ddb085b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +41,45 @@// implementation of the error interface\nfunc (f *FitError) Error() string {\n\toutput := fmt.Sprintf(\"failed to find fit for pod, \")\n\tfor node, predicateList := range f.FailedPredicates {\n\t\toutput = output + fmt.Sprintf(\"Node %s: %s\", node, strings.Join(predicateList.List(), \",\"))"
  }
]