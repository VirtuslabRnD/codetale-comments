[
  {
    "id" : "c854e6f3-e45b-4efa-a3be-96753a3852d0",
    "prId" : 55906,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55906#pullrequestreview-78110116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92f09457-daff-4283-bfc3-c1f45dd3095d",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Shouldn't we give a better score to node which fits both CPU and memory?",
        "createdAt" : "2017-11-20T16:31:12Z",
        "updatedAt" : "2017-11-27T17:55:21Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "e3514951-ba43-425f-8c20-6e345eada72b",
        "parentId" : "92f09457-daff-4283-bfc3-c1f45dd3095d",
        "authorId" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "body" : "wanted to keep score as low as possible to avoid significant impact on scores assigned by least/most requested priority functions.",
        "createdAt" : "2017-11-21T13:31:11Z",
        "updatedAt" : "2017-11-27T17:55:21Z",
        "lastEditedBy" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5710019994985a830c4efdb96ac9fde75dcddae",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +34,38 @@// of the pod are satisfied, the node is assigned a score of 1.\n// Rationale of choosing the lowest score of 1 is that this is mainly selected to break ties between nodes that have\n// same scores assigned by one of least and most requested priority functions.\nfunc ResourceLimitsPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) {\n\tnode := nodeInfo.Node()"
  },
  {
    "id" : "102dd270-13cd-449a-ba12-b1346624a339",
    "prId" : 55906,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55906#pullrequestreview-79196425",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "900a755d-7741-4dd2-86bc-c3dc46062f45",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "We do this exact loop in `GetResourceRequest`, but for `Requests` instead of `Limits`. I think we should create a function that receives a `ResourceList` and returns a `Resource`. That way when other resource dimensions are added, we will need to update one place.",
        "createdAt" : "2017-11-22T18:57:33Z",
        "updatedAt" : "2017-11-27T17:55:21Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "caff46e9-b87d-4120-b56c-0ae1f154d1c7",
        "parentId" : "900a755d-7741-4dd2-86bc-c3dc46062f45",
        "authorId" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "body" : "I will look into a follow up PR, dont want to disturb more code at this point. thanks.",
        "createdAt" : "2017-11-27T15:07:51Z",
        "updatedAt" : "2017-11-27T17:55:21Z",
        "lastEditedBy" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5710019994985a830c4efdb96ac9fde75dcddae",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +94,98 @@\t// take max_resource(sum_pod, any_init_container)\n\tfor _, container := range pod.Spec.InitContainers {\n\t\tfor rName, rQuantity := range container.Resources.Limits {\n\t\t\tswitch rName {\n\t\t\tcase v1.ResourceMemory:"
  }
]