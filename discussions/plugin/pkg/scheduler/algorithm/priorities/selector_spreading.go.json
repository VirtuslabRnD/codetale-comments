[
  {
    "id" : "d259498b-5c08-4269-af8d-9b693388288e",
    "prId" : 56317,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56317#pullrequestreview-84995007",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1fbf9f5-3178-4cf3-8ff2-ddf144a219db",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Is this \"selector ==nil\" added only for the purpose of tests?\r\nIf so, I'm against adding it and we should change tests to always pass selector.\r\n",
        "createdAt" : "2017-12-21T06:32:10Z",
        "updatedAt" : "2018-01-04T08:05:08Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "c1bcd33e-10e8-4312-9dcf-c8f61ffdb663",
        "parentId" : "f1fbf9f5-3178-4cf3-8ff2-ddf144a219db",
        "authorId" : "c09bfaa0-c459-4cac-bc30-9a4dd3d651d3",
        "body" : "In TestZoneSpreadPriority  there are special test cases - e.g. no service/nothing scheduled/different services  test cases. In these special test cases \"selector == nil\" is reasonable. WDYT?",
        "createdAt" : "2017-12-21T08:37:38Z",
        "updatedAt" : "2018-01-04T08:05:08Z",
        "lastEditedBy" : "c09bfaa0-c459-4cac-bc30-9a4dd3d651d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "94d75929b68a6c65ab4ab7b28df3e356e06a2a79",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +204,208 @@// filteredPod get pods based on namespace and selector\nfunc filteredPod(namespace string, selector labels.Selector, nodeInfo *schedulercache.NodeInfo) (pods []*v1.Pod) {\n\tif nodeInfo.Pods() == nil || len(nodeInfo.Pods()) == 0 || selector == nil {\n\t\treturn []*v1.Pod{}\n\t}"
  },
  {
    "id" : "f29d741a-4339-4ad7-a28c-557345734e04",
    "prId" : 42578,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42578#pullrequestreview-30145345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30a2cd0b-96fe-48cf-ac73-ea46737cd486",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "fwiw as a follow on... I think we need to start creating cache indexes, b/c we're doing a bunch of operations sub-optimally.  ",
        "createdAt" : "2017-03-30T16:05:39Z",
        "updatedAt" : "2017-03-30T16:05:39Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "14814449-71e9-478b-8642-b23e1f381b48",
        "parentId" : "30a2cd0b-96fe-48cf-ac73-ea46737cd486",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "+1, but node maybe changed during the scheduling cycle, maybe we can share `schedulercache` with predicates & priorities.",
        "createdAt" : "2017-03-31T00:25:04Z",
        "updatedAt" : "2017-03-31T00:25:04Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "55d3c82782122dec7e8671fee31be65b57976c46",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +211,215 @@\tlabeledNodes := map[string]string{}\n\tnonLabeledNodes := []string{}\n\tfor _, node := range nodes {\n\t\tif labels.Set(node.Labels).Has(s.label) {\n\t\t\tlabel := labels.Set(node.Labels).Get(s.label)"
  },
  {
    "id" : "e1800666-5fd4-4179-90c8-17925e506027",
    "prId" : 42578,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42578#pullrequestreview-30145656",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57bcc5ee-e317-4408-ab5a-3dbbbb4bf2c6",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "s/Classifies/getNodeClassificationByLabels classifies/",
        "createdAt" : "2017-03-31T00:03:19Z",
        "updatedAt" : "2017-03-31T00:03:20Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "7ee8190a-d4bc-4149-bfcc-eb75f204cf69",
        "parentId" : "57bcc5ee-e317-4408-ab5a-3dbbbb4bf2c6",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "similar comments to other funcs to make golint happy.",
        "createdAt" : "2017-03-31T00:28:14Z",
        "updatedAt" : "2017-03-31T00:28:14Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "55d3c82782122dec7e8671fee31be65b57976c46",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +207,211 @@}\n\n// Classifies nodes into ones with labels and without labels.\nfunc (s *ServiceAntiAffinity) getNodeClassificationByLabels(nodes []*v1.Node) (map[string]string, []string) {\n\tlabeledNodes := map[string]string{}"
  },
  {
    "id" : "d03147be-73f6-4720-be48-5e2ff6ae6170",
    "prId" : 41708,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41708#pullrequestreview-24802576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd186d0a-eaa4-4faa-9f1a-3bfd82231dc1",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "@bsalamat Any reason, why this is having 3, was it b/c it was previously having 3 selectors ?",
        "createdAt" : "2017-03-02T03:08:02Z",
        "updatedAt" : "2017-03-02T03:08:02Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "8c3589ad-962e-47bb-96e7-8e99febf964a",
        "parentId" : "bd186d0a-eaa4-4faa-9f1a-3bfd82231dc1",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I don't think that it was related to the 3 selectors. New elements are appended to the \"selectors\" array in loops, so there could be potentially more than 3 elements. Appending to an array resizes it if there is not enough room in the array. So, this should be find with the new changes.",
        "createdAt" : "2017-03-02T06:55:57Z",
        "updatedAt" : "2017-03-02T06:55:57Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "8cbf9bf6-3965-4539-a2e8-13504fd868a1",
        "parentId" : "bd186d0a-eaa4-4faa-9f1a-3bfd82231dc1",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "@bsalamat Thanks for responding back. My concern was not a programmatic one, rather from code readability perspective. A new developer coming in like me, may get confused with - something like why 3? I may be missing something here. ",
        "createdAt" : "2017-03-02T11:58:04Z",
        "updatedAt" : "2017-03-02T11:58:05Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "4e1deab0-65db-40a5-a3ca-6460310f5929",
        "parentId" : "bd186d0a-eaa4-4faa-9f1a-3bfd82231dc1",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "This code predates my changes. I don't know why 3 was chosen. I don't see any particular significance.",
        "createdAt" : "2017-03-02T18:12:58Z",
        "updatedAt" : "2017-03-02T18:12:58Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef686716ba9dd15b6deb5aef1bfbb4616cf5fe5e",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +62,66 @@// Returns selectors of services, RCs and RSs matching the given pod.\nfunc getSelectors(pod *v1.Pod, sl algorithm.ServiceLister, cl algorithm.ControllerLister, rsl algorithm.ReplicaSetLister, ssl algorithm.StatefulSetLister) []labels.Selector {\n\tselectors := make([]labels.Selector, 0, 3)\n\tif services, err := sl.GetPodServices(pod); err == nil {\n\t\tfor _, service := range services {"
  },
  {
    "id" : "fa18339e-9549-42aa-907e-b4284eb869b1",
    "prId" : 33834,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33834#pullrequestreview-2682727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b507d20e-c6c9-4539-a1df-b990af8cb9e5",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Please add a comment what kind of selectors we're getting here.\n",
        "createdAt" : "2016-10-04T07:28:17Z",
        "updatedAt" : "2016-10-04T10:19:16Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "a056ace8-ee72-48ae-8ab9-24447568879e",
        "parentId" : "b507d20e-c6c9-4539-a1df-b990af8cb9e5",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "done\n",
        "createdAt" : "2016-10-04T10:08:35Z",
        "updatedAt" : "2016-10-04T10:19:16Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8632e22030a2d194c1b8084b519df2d1b6ac5a0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +58,62 @@\n// Returns selectors of services, RCs and RSs matching the given pod.\nfunc getSelectors(pod *api.Pod, sl algorithm.ServiceLister, cl algorithm.ControllerLister, rsl algorithm.ReplicaSetLister) []labels.Selector {\n\tselectors := make([]labels.Selector, 0, 3)\n\tif services, err := sl.GetPodServices(pod); err == nil {"
  },
  {
    "id" : "cec45b3b-442d-4793-8e46-7a95a4368a94",
    "prId" : 32888,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32888#pullrequestreview-1135167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9329ef09-f5d7-429a-b2d5-5b43bf424e7d",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Like above, err == nil => len(services) > 0\n",
        "createdAt" : "2016-09-22T12:57:12Z",
        "updatedAt" : "2016-09-22T12:57:12Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "a18b4370-c6a8-4266-ae50-9652e17c242b",
        "parentId" : "9329ef09-f5d7-429a-b2d5-5b43bf424e7d",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> Like above, err == nil => len(services) > 0\n\nWhy couldn't I match nothing?\n",
        "createdAt" : "2016-09-22T12:59:50Z",
        "updatedAt" : "2016-09-22T12:59:50Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "16fbb47189a2f957178d1971b88a071807289db0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +195,199 @@func (s *ServiceAntiAffinity) CalculateAntiAffinityPriority(pod *api.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*api.Node) (schedulerapi.HostPriorityList, error) {\n\tvar nsServicePods []*api.Pod\n\tif services, err := s.serviceLister.GetPodServices(pod); err == nil && len(services) > 0 {\n\t\t// just use the first service and get the other pods within the service\n\t\t// TODO: a separate predicate can be created that tries to handle all services for the pod"
  },
  {
    "id" : "11b21def-eace-4432-b87e-fd262832e42d",
    "prId" : 28836,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1a50ad0-1453-4662-9710-4c3841c70126",
        "parentId" : null,
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "why set the cap to 3, not some other value? \n",
        "createdAt" : "2016-07-12T13:35:53Z",
        "updatedAt" : "2016-07-13T09:02:36Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      },
      {
        "id" : "8188f771-b91f-4395-90d6-f14c034e7c1a",
        "parentId" : "e1a50ad0-1453-4662-9710-4c3841c70126",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Generally we expect no more than 1 service, no more than 1 RC and no more than 1 RS. (Probably even 2 in heapthy case is enough (either RC or RS)). But this doesn't really change anything...\n",
        "createdAt" : "2016-07-12T13:39:04Z",
        "updatedAt" : "2016-07-13T09:02:36Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "adcf50c9-96c2-46a8-bb38-27c57e24b584",
        "parentId" : "e1a50ad0-1453-4662-9710-4c3841c70126",
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "> But this doesn't really change anything...\n\nI think the goal is to avoid allocation/copying? growing backing slice involving one allocation + len(s) copying. If 3 is enough for the most common case, then great! I do not really care if this is 3 or 6, or 8, since it just several temp bytes difference. Thanks for the explanation. \n",
        "createdAt" : "2016-07-12T13:46:34Z",
        "updatedAt" : "2016-07-13T09:02:36Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      },
      {
        "id" : "d5853554-be5c-4000-b5af-7fa3697a2376",
        "parentId" : "e1a50ad0-1453-4662-9710-4c3841c70126",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "> I think the goal is to avoid allocation/copying?\n\nYes exactly.\n",
        "createdAt" : "2016-07-12T13:49:50Z",
        "updatedAt" : "2016-07-13T09:02:36Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea1d797f9813f2c5fa8103a6a9804627d41189df",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +63,67 @@// Where zone information is included on the nodes, it favors nodes in zones with fewer existing matching pods.\nfunc (s *SelectorSpread) CalculateSpreadPriority(pod *api.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodeLister algorithm.NodeLister) (schedulerapi.HostPriorityList, error) {\n\tselectors := make([]labels.Selector, 0, 3)\n\tif services, err := s.serviceLister.GetPodServices(pod); err == nil {\n\t\tfor _, service := range services {"
  },
  {
    "id" : "49ef8093-59fc-4a5f-9a43-e609cae02488",
    "prId" : 28836,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea76db97-fab0-4594-8e0b-3c920516c4ba",
        "parentId" : null,
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "we avoid copying in range in other places. any reason we do not do that for this for range loop?\n",
        "createdAt" : "2016-07-12T13:39:30Z",
        "updatedAt" : "2016-07-13T09:02:36Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      },
      {
        "id" : "aea448b1-f4f9-434b-918e-c746cab8640f",
        "parentId" : "ea76db97-fab0-4594-8e0b-3c920516c4ba",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Here, Pods() are pointers to pods, which are fine to copy (it's just a copy of pointer, not the underlying struct). If doing it here we would have:\nfor i := range ... {\n  nodePod := nodeNameToInfo[nodeName].Pods()[i]\n  ...\n}\n\nwhich would be more expensive.\n",
        "createdAt" : "2016-07-12T13:42:29Z",
        "updatedAt" : "2016-07-13T09:02:36Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "9f8958c2-82ed-40a7-8772-5c10e2717b82",
        "parentId" : "ea76db97-fab0-4594-8e0b-3c920516c4ba",
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "make sense. thanks.\n",
        "createdAt" : "2016-07-12T13:47:19Z",
        "updatedAt" : "2016-07-13T09:02:36Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea1d797f9813f2c5fa8103a6a9804627d41189df",
    "line" : null,
    "diffHunk" : "@@ -1,1 +97,101 @@\t\t\tnodeName := nodes[i].Name\n\t\t\tcount := float32(0)\n\t\t\tfor _, nodePod := range nodeNameToInfo[nodeName].Pods() {\n\t\t\t\tif pod.Namespace != nodePod.Namespace {\n\t\t\t\t\tcontinue"
  },
  {
    "id" : "b994aade-91ec-49f4-9b57-741acdb8798f",
    "prId" : 23274,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fa82903-3d17-4621-bcc8-b5eafff37f81",
        "parentId" : null,
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "nit\n\n``` go\nfor n := range toProcess {\n    ....\n}\n```\n\nthen we can remove line 133 to line 136\n",
        "createdAt" : "2016-03-22T15:01:54Z",
        "updatedAt" : "2016-03-22T15:01:54Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebcc8f737c4ea7f21ba199657aedd03fb9a17845",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +130,134 @@\t\t\t\tdefer utilruntime.HandleCrash()\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor {\n\t\t\t\t\tnodeName, ok := <-toProcess\n\t\t\t\t\tif !ok {"
  },
  {
    "id" : "4f758e20-47a6-4761-b7a2-bf0e685562d8",
    "prId" : 18493,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8738ef50-d69b-4d15-8c0b-98e4d219c32f",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Seems unused here. Why do you pass it here?\n",
        "createdAt" : "2015-12-17T09:41:07Z",
        "updatedAt" : "2015-12-17T09:41:07Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "89cee9a3-eb1b-482f-b4cb-3622207020bc",
        "parentId" : "8738ef50-d69b-4d15-8c0b-98e4d219c32f",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "OK - I guess it's because of interface.\nnvm\n",
        "createdAt" : "2015-12-17T09:42:52Z",
        "updatedAt" : "2015-12-17T09:42:52Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ee0e16bcd6a48891ae02b439bc2c25e210572ce",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +74,78 @@// pods which match the same service selectors or RC selectors as the pod being scheduled.\n// Where zone information is included on the nodes, it favors nodes in zones with fewer existing matching pods.\nfunc (s *SelectorSpread) CalculateSpreadPriority(pod *api.Pod, machinesToPods map[string][]*api.Pod, podLister algorithm.PodLister, nodeLister algorithm.NodeLister) (schedulerapi.HostPriorityList, error) {\n\tvar nsPods []*api.Pod\n"
  },
  {
    "id" : "3465d2f1-6128-4650-acb9-229bf72a8507",
    "prId" : 18493,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdce0a59-27af-40d5-9eb2-0afb3547b85c",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Seems unused here. Why do you pass it here?\n",
        "createdAt" : "2015-12-17T09:41:50Z",
        "updatedAt" : "2015-12-17T09:41:50Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "6f7826ee-d5ec-4b6e-a801-2a3b9d0fa6ba",
        "parentId" : "fdce0a59-27af-40d5-9eb2-0afb3547b85c",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "OK - I guess it's because of interface.\nnvm\n",
        "createdAt" : "2015-12-17T09:42:58Z",
        "updatedAt" : "2015-12-17T09:42:58Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ee0e16bcd6a48891ae02b439bc2c25e210572ce",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +214,218 @@// on machines with the same value for a particular label.\n// The label to be considered is provided to the struct (ServiceAntiAffinity).\nfunc (s *ServiceAntiAffinity) CalculateAntiAffinityPriority(pod *api.Pod, machinesToPods map[string][]*api.Pod, podLister algorithm.PodLister, nodeLister algorithm.NodeLister) (schedulerapi.HostPriorityList, error) {\n\tvar nsServicePods []*api.Pod\n"
  },
  {
    "id" : "58a2438f-0d9b-4900-a9d0-e17336e7aedd",
    "prId" : 17915,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9540bfed-3664-4eae-9a26-b1ac0e343045",
        "parentId" : null,
        "authorId" : null,
        "body" : "Sins of the past, but the comment below is verging on being incomprehensible.  Worth improving upon now, while it's fresh in your mind?\n",
        "createdAt" : "2015-11-30T15:59:56Z",
        "updatedAt" : "2015-12-14T01:37:27Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "45078095-65d1-4cf6-a894-12d6a504cdb4",
        "parentId" : "9540bfed-3664-4eae-9a26-b1ac0e343045",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "Rewrote it!\n",
        "createdAt" : "2015-12-06T03:24:03Z",
        "updatedAt" : "2015-12-14T01:37:27Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      }
    ],
    "commit" : "541ff002c03eaec8c5f768e0d5c2826568025f48",
    "line" : null,
    "diffHunk" : "@@ -1,1 +66,70 @@\t// As a nice side-benefit, the null character is not printed by fmt.Print or glog\n\treturn region + \":\\x00:\" + failureDomain\n}\n\n// CalculateSpreadPriority spreads pods across hosts and zones, considering pods belonging to the same service or replication controller."
  },
  {
    "id" : "de782a3d-572c-404d-a35f-48683a8f7f9c",
    "prId" : 17915,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d8c5339-0994-47cc-95ed-3aa5a11f99ca",
        "parentId" : null,
        "authorId" : null,
        "body" : "This iterates over all nodes a second time for each pod placement?  Surely we should rather do this as part of the first iteration above rather?\n",
        "createdAt" : "2015-11-30T16:15:38Z",
        "updatedAt" : "2015-12-14T01:37:27Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "0bbaff23-565a-4ebc-92fa-1965cfff9915",
        "parentId" : "6d8c5339-0994-47cc-95ed-3aa5a11f99ca",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "The first iteration was over the pods; now we are iterating over the nodes to find the counts-by-zone.  We could build a map of node-name to node and then combine these two loops, but we'd need a loop to build that map (and I think this is more efficient than building that node-map).\n\nI am making this a little more efficient and perhaps easier to understand though, by computing the aggregates in a separate loop rather than in the body of the main loop.  It does mean more loops, but each loop is much simpler.\n",
        "createdAt" : "2015-12-06T02:46:09Z",
        "updatedAt" : "2015-12-14T01:37:27Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      }
    ],
    "commit" : "541ff002c03eaec8c5f768e0d5c2826568025f48",
    "line" : null,
    "diffHunk" : "@@ -1,1 +144,148 @@\t// Count similar pods by zone, if zone information is present\n\tcountsByZone := map[string]int{}\n\tfor i := range nodes.Items {\n\t\tnode := &nodes.Items[i]\n"
  },
  {
    "id" : "b452c58c-733b-4d27-90f4-953084b7fbf5",
    "prId" : 17915,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a350dbc7-a08d-46aa-8633-5f5204bc10b6",
        "parentId" : null,
        "authorId" : null,
        "body" : "This is a third iteration over all nodes?  It might be more difficult to eliminate this one, but worth trying.\n",
        "createdAt" : "2015-11-30T16:19:52Z",
        "updatedAt" : "2015-12-14T01:37:27Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "bb2f6b6a-b7d7-463f-8c7c-f54dcca6ce71",
        "parentId" : "a350dbc7-a08d-46aa-8633-5f5204bc10b6",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "Well, it's the second.  I think this loop cannot be combined though, because it needs the 'max-pods-by-node' and 'max-pods-by-zone', which can only be known after every pod & every zone have been examined.\n",
        "createdAt" : "2015-12-06T02:47:24Z",
        "updatedAt" : "2015-12-14T01:37:27Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      }
    ],
    "commit" : "541ff002c03eaec8c5f768e0d5c2826568025f48",
    "line" : null,
    "diffHunk" : "@@ -1,1 +173,177 @@\t//score int - scale of 0-maxPriority\n\t// 0 being the lowest priority and maxPriority being the highest\n\tfor i := range nodes.Items {\n\t\tnode := &nodes.Items[i]\n\t\t// initializing to the default/max node score of maxPriority"
  },
  {
    "id" : "1f26cf1a-e97c-4f88-b9fb-0c9e868c1d9a",
    "prId" : 10667,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7f16127-86b3-4013-8576-8132714adf39",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "On line 109 you should change ServiceSpreadPriority to SelectorSpreadPriority\n",
        "createdAt" : "2015-07-25T06:24:16Z",
        "updatedAt" : "2015-07-31T14:29:15Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "180d3085-847e-4da2-bc55-20aa0292eaa7",
        "parentId" : "e7f16127-86b3-4013-8576-8132714adf39",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2015-07-27T14:24:02Z",
        "updatedAt" : "2015-07-31T14:29:15Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "94eb52de33d513958bcdb70cc7595d71ecfa1fe3",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +91,95 @@\t\t\t\tif counts[pod.Spec.NodeName] > maxCount {\n\t\t\t\t\tmaxCount = counts[pod.Spec.NodeName]\n\t\t\t\t}\n\t\t\t}\n\t\t}"
  }
]