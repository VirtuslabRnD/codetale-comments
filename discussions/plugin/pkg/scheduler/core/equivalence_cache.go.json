[
  {
    "id" : "ee3079ae-43db-4f11-aacc-f4e6a509b4d8",
    "prId" : 56577,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56577#pullrequestreview-80553148",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d777853d-2488-4db2-b8df-2148983c9bfe",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Why does this need to be added here?",
        "createdAt" : "2017-12-01T14:25:57Z",
        "updatedAt" : "2017-12-02T14:24:50Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "1762e378-ac9f-4a9c-be5d-aae6782f018a",
        "parentId" : "d777853d-2488-4db2-b8df-2148983c9bfe",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "I noticed `MaxPDVolumeCountChecker` actually relies on volumes count of **specify Pod** to do predicate. So when a pod with PV is add/delete on a node. The `MaxPDVolumeCountChecker` e-cache of this node should be re-calculated for subsequent pods.\r\n\r\nHope I am understanding right :)",
        "createdAt" : "2017-12-01T15:34:20Z",
        "updatedAt" : "2017-12-02T14:24:50Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "aed439fb-aeec-435b-832f-4ba150b36f58",
        "parentId" : "d777853d-2488-4db2-b8df-2148983c9bfe",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "the volume count checker counts the volumes on pods that are assigned on nodes.  I guess for the case where you create the Pod with NodeName already set, then yes, the predicate should be invalidated.",
        "createdAt" : "2017-12-01T16:45:27Z",
        "updatedAt" : "2017-12-02T14:24:50Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3bb74e3a324bd492b3639e4069187c2eb96a82a",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +191,195 @@\n\t// MaxPDVolumeCountPredicate: we check the volumes of pod to make decision.\n\tfor _, vol := range pod.Spec.Volumes {\n\t\tif vol.PersistentVolumeClaim != nil {\n\t\t\tinvalidPredicates.Insert(\"MaxEBSVolumeCount\", \"MaxGCEPDVolumeCount\", \"MaxAzureDiskVolumeCount\")"
  },
  {
    "id" : "53080ab3-014f-4d17-a33f-1436cc0df6cc",
    "prId" : 41541,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41541#pullrequestreview-29065172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Isn't 100 too small? This cache is going to have an entry for each pod controller. I'm not sure what would be a good number to support larger deployments, but I just intuitively feel that 100 may be too small. Raising it to something like 512 shouldn't waste much memory and may be justifiable given the potential performance improvement, unless we are absolutely sure that we would never hit any number like that.",
        "createdAt" : "2017-02-21T02:40:35Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "c35ca0b9-f9fc-4eac-8b4f-540240e6e558",
        "parentId" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Is there a reason why we wouldn't plumb that through as a knob?",
        "createdAt" : "2017-02-21T23:49:49Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "41b590a3-ec0a-468f-8fee-4734fa036319",
        "parentId" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "The ecache data structure in this patch is like:\r\n```\r\neCache[NODE_NAME].Get(“podaffinity”).get(“EQUIVALENCE_HASH”) = {fit, reasons, invalid}\r\n```\r\nSo the `maxCacheEntries` which describe entries of ecache should be less or equal to the total predicates counts of scheduler, it's a relatively fixed and small number. That explains why I did not use a knob.\r\n\r\nI have also benchmarked different item like `EQUIVALENCE_HASH` as ecache's key, but it did not show any performance improvement while will leave `maxCacheEntries` undetermined.",
        "createdAt" : "2017-02-22T02:40:43Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "629aced8-75f7-4c36-a450-4edc7d6c9407",
        "parentId" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Ah, I see. The three level indexing is complex. So, we have N caches, each of which belong to one node and has maxCacheEntries entries (for predicates). We use a predicate key to find an entry in the cache. The entry in the cache is a map. That map is indexed with equivalence hash. This third level confused me.\r\n\r\nWe could have probably dropped predicates from the equation at the cost of losing precision on failure reasons and cache invalidations.",
        "createdAt" : "2017-02-22T23:01:07Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "c49ec63a-8835-4add-af8d-24b500d03907",
        "parentId" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Yes, actually we did this in [the original PR](https://github.com/kubernetes/kubernetes/pull/30844/files#diff-f32e6323e72cadaaa2440fc35266f980R82), while after discussed with wojtek we all agreed to choose invalidation by predicate key over invalidate entirely cache everytime. Since we assume invalidation happens frequently.",
        "createdAt" : "2017-02-23T05:52:38Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "19ad04a1-d1d8-49be-8b74-68df8522a225",
        "parentId" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Makes sense. Thanks!",
        "createdAt" : "2017-02-23T07:30:13Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "e3ed3e16-3bd7-4460-b613-b9d1512721ac",
        "parentId" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Yeah - I think this is the right way to go (though I agree it's complicated from the first glance).\r\nBTW - 100 might even be too much, but we can leave it at least for now.",
        "createdAt" : "2017-03-23T12:31:11Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "38c8410e-272d-45ab-a464-f6da7aa54c25",
        "parentId" : "091c06d4-3298-466b-a51a-8af0d47690d8",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Can we make it configurable? so user can use a bigger cache with more memory.",
        "createdAt" : "2017-03-26T08:35:48Z",
        "updatedAt" : "2017-03-28T11:26:04Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "63197e53a111541ef858d94f0f182102f948d81f",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +31,35 @@\n// we use predicate names as cache's key, its count is limited\nconst maxCacheEntries = 100\n\ntype HostPredicate struct {"
  }
]