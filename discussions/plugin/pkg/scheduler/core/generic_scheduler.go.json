[
  {
    "id" : "3c5ae040-eed3-481a-926b-bd2560e7bb2c",
    "prId" : 56178,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56178#pullrequestreview-78336218",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5dfb305-9d30-4b5c-a1ae-d57d7134f903",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Update the function-level comment to explain how you use PDB.",
        "createdAt" : "2017-11-22T01:35:27Z",
        "updatedAt" : "2017-11-22T17:46:35Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "a2a0b012-e0e3-4795-81c5-fc04cdef0ebf",
        "parentId" : "d5dfb305-9d30-4b5c-a1ae-d57d7134f903",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Done",
        "createdAt" : "2017-11-22T05:01:31Z",
        "updatedAt" : "2017-11-22T17:46:35Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0ef9cd09a3701b4d3901ea9311eef468b1e91ec",
    "line" : 308,
    "diffHunk" : "@@ -1,1 +871,875 @@\tqueue SchedulingQueue,\n\tpdbs []*policy.PodDisruptionBudget,\n) ([]*v1.Pod, int, bool) {\n\tpotentialVictims := util.SortableList{CompFunc: util.HigherPriorityPod}\n\tnodeInfoCopy := nodeInfo.Clone()"
  },
  {
    "id" : "a6036626-f1ae-45fc-adfc-1776e2eaf78d",
    "prId" : 55933,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55933#pullrequestreview-77652704",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57f7efc2-720d-4265-9d9b-46e5f95477c8",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Why would you be trying to schedule a pod that has a nominated node name? If it has a nominated node name, isn't it already essentially scheduled and just waiting?",
        "createdAt" : "2017-11-17T23:56:39Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "d2420596-14bf-40c5-a78e-6aaac6886b3c",
        "parentId" : "57f7efc2-720d-4265-9d9b-46e5f95477c8",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Nominated pods may have to wait minutes before the preempted pods are actually cleaned up and leave the node. If in the meantime other pods terminate, or new nodes are added to the cluster, we would like to schedule the nominated pods in the opened space sooner.",
        "createdAt" : "2017-11-18T03:34:02Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "bb8a1e0a-e237-4f08-914b-57e5e83ca808",
        "parentId" : "57f7efc2-720d-4265-9d9b-46e5f95477c8",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I see. Would it make sense to remove the nominated node name when you move the pod from unschedulable to active, rather than here, since at that point you're trying to schedule it again?",
        "createdAt" : "2017-11-19T00:17:36Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "e3ed257e-0502-4438-9091-08016bfc852c",
        "parentId" : "57f7efc2-720d-4265-9d9b-46e5f95477c8",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "No, we should keep nominated node annotation. When scheduler tries to schedule a nominated pod and fails, it shouldn't preempt more pods if the victims preempted by the pod are still in their grace period. The way scheduler knows that the pod is not eligible for more preemption is by looking at the nominated node annotation of the pod. So, we shouldn't remove it at the time of moving pods to the activeQ.",
        "createdAt" : "2017-11-19T19:40:31Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "eda3df87325f82b8dfa6817c1373d17414982a13",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +207,211 @@\tif len(potentialNodes) == 0 {\n\t\tglog.V(3).Infof(\"Preemption will not help schedule pod %v on any node.\", pod.Name)\n\t\t// In this case, we should clean-up any existing nominated node name of the pod.\n\t\treturn nil, nil, []*v1.Pod{pod}, nil\n\t}"
  },
  {
    "id" : "7514001b-bb52-4f0e-b837-a1154eff7359",
    "prId" : 55933,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55933#pullrequestreview-78008117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2a8a221-b57b-4d84-a908-d10dfe84095d",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "IIUC this \"else if\" clause is enforcing the following:\r\n* If there are no nominated pods for this node, then use the feasibility result from the previous iteration of the loop, which tested the pod against the node with no nominated pods (of course)\r\n* If there are nominated pods for this node, and the pod is infeasible with the nominated pods added (which was determined on the previous iteration of the loop), then declare the pod to be infeasible\r\n\r\nIs that right?\r\n\r\nIf so, I am wondering about the second thing.\r\n\r\nWhat is the high-level rule you are trying to enforce? It seems like a reasonable rule might be \"The pod is feasible if it will be feasible once all of the victims are gone and the preemptors have started running.\" But IIUC the rule you are enforcing is \"The pod is feasible if it is feasible with all preemptors and victims running simultaneously AND it is feasible in the current state (victims running, preemptors not running yet).\" Can you explain the reason behind this rule? This seems to be testing a condition that will never happen (victims and preemptors running simultaneously). I understand you care about the preemptors because of pod affinity but IIUC you are testing against a configuration of the node that will never actually exist. Would testing *just* the pod affinity predicate help? (Though I'm not sure what configuration you'd use it with.)",
        "createdAt" : "2017-11-18T02:01:25Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "bae98f04-f213-4336-b794-023b5fc4e52b",
        "parentId" : "f2a8a221-b57b-4d84-a908-d10dfe84095d",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "> IIUC this \"else if\" clause is enforcing the following:\r\n>If there are no nominated pods for this node, then use the feasibility result from the previous iteration of the loop, which tested the pod against the node with no nominated pods (of course)\r\nIf there are nominated pods for this node, and the pod is infeasible with the nominated pods added (which was determined on the previous iteration of the loop), then declare the pod to be infeasible\r\nIs that right?\r\n\r\nYes, your understanding is right.\r\n\r\n> What is the high-level rule you are trying to enforce? It seems like a reasonable rule might be \"The pod is feasible if it will be feasible once all of the victims are gone and the preemptors have started running.\" But IIUC the rule you are enforcing is \"The pod is feasible if it is feasible with all preemptors and victims running simultaneously AND it is feasible in the current state (victims running, preemptors not running yet).\" Can you explain the reason behind this rule? This seems to be testing a condition that will never happen (victims and preemptors running simultaneously). I understand you care about the preemptors because of pod affinity but IIUC you are testing against a configuration of the node that will never actually exist. Would testing just the pod affinity predicate help? (Though I'm not sure what configuration you'd use it with.)\r\n\r\nPlease note that this function is called from two different places: Schedule and Preempt.\r\nWhen it is called in Schedule, we want to test whether the pod is schedulable on the node with all the existing pods on the node plus higher (and equal) priority pods nominated to run on the node.\r\nWhen it is called from Preempt, we should do what you said. We should remove the victims and add the nominated pods. Removal of the victims is done by [SelectVictimsOnNode()](https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/scheduler/core/generic_scheduler.go#L693). It removes victims from `meta` and `NodeInfo` before calling this function.",
        "createdAt" : "2017-11-18T03:45:08Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "98408ae0-0abf-48b6-9db3-0dd1b614ffea",
        "parentId" : "f2a8a221-b57b-4d84-a908-d10dfe84095d",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I was asking something a bit different, but it would be good to add a comment with that information anyway.\r\n\r\n(The thing I was meaning to ask I've covered in my first new comment in this function.)",
        "createdAt" : "2017-11-19T02:09:54Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "d734d0b6-c4be-4a27-a1f7-69b4331c208c",
        "parentId" : "f2a8a221-b57b-4d84-a908-d10dfe84095d",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Did you add a comment with this (at the top of this function)? If not, please do\r\n\r\n> this function is called from two different places: Schedule and Preempt.\r\nWhen it is called in Schedule, we want to test whether the pod is schedulable on the node with all the existing pods on the node plus higher (and equal) priority pods nominated to run on the node.\r\nWhen it is called from Preempt, we should do what you said. We should remove the victims and add the nominated pods. Removal of the victims is done by SelectVictimsOnNode(). It removes victims from meta and NodeInfo before calling this function.\r\n",
        "createdAt" : "2017-11-21T03:01:52Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "7b77e93d-0113-4f32-89b3-ca3cdc6876fd",
        "parentId" : "f2a8a221-b57b-4d84-a908-d10dfe84095d",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Done",
        "createdAt" : "2017-11-21T06:11:00Z",
        "updatedAt" : "2017-11-21T06:18:00Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "eda3df87325f82b8dfa6817c1373d17414982a13",
    "line" : 242,
    "diffHunk" : "@@ -1,1 +418,422 @@\t\tif i == 0 {\n\t\t\tpodsAdded, metaToUse, nodeInfoToUse = addNominatedPods(util.GetPodPriority(pod), meta, info, queue)\n\t\t} else if !podsAdded || len(failedPredicates) != 0 {\n\t\t\tbreak\n\t\t}"
  },
  {
    "id" : "245966a6-7030-4c2b-b53d-fc06526e1290",
    "prId" : 55933,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55933#pullrequestreview-77652867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2e5a9d3-6e14-4693-aa3e-2c5f99d9516e",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I think this is so we can return all of the fit failure reasons, rather than just the first one that fails. (Unless I'm misunderstanding your question.)",
        "createdAt" : "2017-11-19T02:19:31Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "040978bb-6f68-4ac4-a43b-9079f566eabe",
        "parentId" : "e2e5a9d3-6e14-4693-aa3e-2c5f99d9516e",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "You are probably right, but it costs us a lot of cycles. For example, we run something as sophisticated as affinity/anti-affinity for a node that doesn't have enough memory, just to return failure reasons. I think it is fine to return that this node is not feasible because it didn't have enough memory and leave other predicates.",
        "createdAt" : "2017-11-19T19:47:06Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "eda3df87325f82b8dfa6817c1373d17414982a13",
    "line" : 259,
    "diffHunk" : "@@ -1,1 +431,435 @@\t\t\t}\n\n\t\t\t// TODO(bsalamat): When one predicate fails and fit is false, why do we continue\n\t\t\t// checking other predicates?\n\t\t\tif !eCacheAvailable || invalid {"
  },
  {
    "id" : "3549c3fd-7b9e-42bb-8cc1-97d7a228365d",
    "prId" : 55933,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55933#pullrequestreview-77682324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0f70374-ce7e-4131-ae86-9e7d18e0a6b5",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "IIUC the user will see (in the why pending information, e.g. the event that gets generated) the union of the reasons it didn't fit when the nominated pods were assumed to be running on the node, and the reasons it didn't fit when the nominated pods were not assumed to be running on the node. I could imagine that first part could be confusing if they try to match up the reasons to what they see running on the node. There will be an endless stream of questions/complaints that the scheduler is broken. :)\r\n\r\nA couple of possible solutions:\r\n1) Create a new failure reason called \"preemption in progress\" and return that as the only failure reason, if it fails during the first pass with the nominated pods. This obviously loses information but if the number of active preemptions in the cluster at any particular time is small, you don't lose that much information.\r\nor\r\n2) Don't change anything here, but add to the failure event a list of the nominated pods for the node you'e reporting that the pod failed on, so the user can make sense of the failure reasons.\r\n\r\nIt's probably fine to addressed this later, but it definitely seems that it needs to be addressed.\r\n\r\nOn a related note: I don't know how the eCache works in detail. Will caching failure reasons taking into account nominated pods (rather than just what is running on the node), as you're doing here, break anything? It's easy to see why you can end up with extra failure reasons when you take nominated pods into account, compared to if you just take into account what's running on the node. But you can also end up missing failure reasons. For example, say the pending pod has inter-pod affinity for one of the nominated pods but the first iteration of the loop fails (for some random reason, like resources), and the pending pod is not feasible without the nominated pod due to inter-pod affinity, you will never detect this second condition since you will only run the first iteration of the loop. (This will also be missing from the why pending information reported to the user, but that's probably OK since it failed for other reasons. I'm more worried about the eCache.)",
        "createdAt" : "2017-11-19T03:14:51Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "eafcbb1f-22ed-4b7f-a2f3-cbd29fb07950",
        "parentId" : "f0f70374-ce7e-4131-ae86-9e7d18e0a6b5",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I think adding the nominated pods to the events is a good idea. I will do that.\r\n\r\nYour second point about eCache is valid. Let me investigate it more. We probably need to invalidate the eCache in a couple of more situations.",
        "createdAt" : "2017-11-19T21:04:15Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "f816ca9d-3d79-4137-b07e-67f7010c1676",
        "parentId" : "f0f70374-ce7e-4131-ae86-9e7d18e0a6b5",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "This is not addressed yet. I'm working on it...",
        "createdAt" : "2017-11-20T01:10:13Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "e86fa752-1dbf-4d45-8dd3-994c67fb83ec",
        "parentId" : "f0f70374-ce7e-4131-ae86-9e7d18e0a6b5",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "My guess is that the simplest thing to do is to not read or write the eCache when there are any nominated pods for the node.",
        "createdAt" : "2017-11-20T04:44:15Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "03e1773c-8864-471d-9ec2-bf89e694410c",
        "parentId" : "f0f70374-ce7e-4131-ae86-9e7d18e0a6b5",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Actually, that's a good idea. Today, I tried to understand the current mechanism of updating eCache more deeply, but either I am missing something or there are bugs in the current implementation. Since eCache is an alpha feature disabled by default, and also we don't expect to have nominated pods for a large number of nodes, I think your suggestion makes a lot of sense. We can revisit this again in the future and when moving eCache to beta.",
        "createdAt" : "2017-11-20T04:58:54Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "eda3df87325f82b8dfa6817c1373d17414982a13",
    "line" : 280,
    "diffHunk" : "@@ -1,1 +452,456 @@\t\t\tif !fit {\n\t\t\t\t// eCache is available and valid, and predicates result is unfit, record the fail reasons\n\t\t\t\tfailedPredicates = append(failedPredicates, reasons...)\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "ffe6148b-f732-43cc-891f-175c416ef206",
    "prId" : 55933,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55933#pullrequestreview-78006699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09a017c7-c5f2-48ef-9508-327338ca1fd1",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I think it should be lower or equal priority pods? (In the comment and the implementation.)\r\n",
        "createdAt" : "2017-11-21T03:00:02Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "c9f59d72-5e21-46fd-a0f3-8f81bfe1ee13",
        "parentId" : "09a017c7-c5f2-48ef-9508-327338ca1fd1",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "it should be lower priority pods only. Equal priority pods are scheduled in the order received. A pod cannot preempt another pod with equal priority. Similarly, it cannot take the space emptied for another equal priority nominated pod.",
        "createdAt" : "2017-11-21T05:58:29Z",
        "updatedAt" : "2017-11-21T06:17:25Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "eda3df87325f82b8dfa6817c1373d17414982a13",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +221,225 @@\t\tpasses, pErr := nodePassesExtendersForPreemption(pod, node.Name, nodeToPods[node], g.cachedNodeInfoMap, g.extenders)\n\t\tif passes && pErr == nil {\n\t\t\t// Lower priority pods nominated to run on this node, may no longer fit on\n\t\t\t// this node. So, we should remove their nomination. Removing their\n\t\t\t// nomination updates these pods and moves them to the active queue. It"
  },
  {
    "id" : "953d5e21-79e5-483b-91cf-ff3de0b7bed4",
    "prId" : 50949,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50949#pullrequestreview-61135314",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59708579-4d9a-4065-af50-751e9e3c88a4",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "If we are interested only in nodes with min highest priority, let's do:\r\n```\r\nnodeScores = nil \r\n```\r\nhere and append to nodeScores only if highestPodPriority is equal to minHighestPriority.\r\n\r\nWith this, at the end you will only nodes which are interesting for you (and all of them).",
        "createdAt" : "2017-08-30T10:49:45Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "4403186b-cb6e-4930-add0-71eb6d616b42",
        "parentId" : "59708579-4d9a-4065-af50-751e9e3c88a4",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Also, you can then compute the necessary sum at the same time in this loop.",
        "createdAt" : "2017-08-30T10:50:51Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "7be9bea7-6b8f-442c-b9c2-90311e4dfa04",
        "parentId" : "59708579-4d9a-4065-af50-751e9e3c88a4",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "First part done.\r\nRe: compute sum in the same loop, in the first loop minHighestPriority may change as we go through the nodes. Calculating the sum in the first loop may be wasted work.",
        "createdAt" : "2017-08-30T18:25:15Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "27e621d1-ef4a-47f8-be12-b88cf2439ac1",
        "parentId" : "59708579-4d9a-4065-af50-751e9e3c88a4",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "It's clearly not done - see my comments below.",
        "createdAt" : "2017-08-31T11:13:47Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "876ead38-9f55-40d0-89ce-e53e0f59c5ec",
        "parentId" : "59708579-4d9a-4065-af50-751e9e3c88a4",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This is now done.",
        "createdAt" : "2017-09-07T07:34:02Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0b718373befb2168befc0c4579fd9a02155d5bc",
    "line" : 160,
    "diffHunk" : "@@ -1,1 +523,527 @@\t\thighestPodPriority := util.GetPodPriority(pods[0])\n\t\tif highestPodPriority < minHighestPriority {\n\t\t\tminHighestPriority = highestPodPriority\n\t\t\tminPriorityScores = nil\n\t\t}"
  },
  {
    "id" : "aedaacec-0def-4afe-9810-b854126a58f5",
    "prId" : 50949,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50949#pullrequestreview-60997369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58a82ea8-0c44-4e3c-9669-bc47f4c16f5f",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I started wondering about this.\r\nDo we somehow protect ourselves from the situation that there will be new pods coming all the time that will be smaller, and will effectively blocking our high priority pod from scheduling?\r\n\r\nI'm talking for example about scanario like this:\r\n- there is a single node,\r\n- there is Deployment with 10 replicas (low priority) each of the replica with 30s grace period\r\n- now a high priority pod is coming that requires the whole onde\r\n- so we call preempt on all replicas\r\n- but before last pod is removed, the first one is recreated by RS controller and since there is a place on that node scheduler puts it here\r\n- when preempted pods are removed, there is again no place for the high priority pod\r\n- so we start from the begining.\r\n\r\nAnd at the end of the day, we pretty much starved the high-priority pod with low-priority pods.\r\n\r\nUnless I'm missing something and this problem is not solved, we shouldn't rush for 1.8 with this change.\r\n\r\n@davidopp ",
        "createdAt" : "2017-09-01T12:32:56Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "05567030-660a-4d8a-a94b-b64a532edeac",
        "parentId" : "58a82ea8-0c44-4e3c-9669-bc47f4c16f5f",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "@bsalamat Can you write a separate doc describing this problem, the corner case(s) that the current approach (in this PR) doesn't address, the plan we have for fixing it, and alternatives approaches for fixing it (e.g. mark the entire node as \"can't schedule here\" until all preemption victims have exited and preemptor has started, maybe also the Borg approach though it's super-complicated).",
        "createdAt" : "2017-09-01T22:00:15Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "bb1676c5-e602-46fe-9a98-949b99067ea4",
        "parentId" : "58a82ea8-0c44-4e3c-9669-bc47f4c16f5f",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Sure. I will write a doc.",
        "createdAt" : "2017-09-01T22:49:38Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "f673321f-19c7-49a0-8238-fc2b8511479a",
        "parentId" : "58a82ea8-0c44-4e3c-9669-bc47f4c16f5f",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Can you please add a comment that such starvation is currently possible, it will be solved for Beta and link this:\r\nhttps://github.com/kubernetes/community/blob/master/contributors/design-proposals/pod-preemption.md#preemption-mechanics",
        "createdAt" : "2017-09-06T10:56:18Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "94a646e7-a509-44f1-b6b2-f312f611768a",
        "parentId" : "58a82ea8-0c44-4e3c-9669-bc47f4c16f5f",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "The `podEligibleToPreemptOthers` function already has a TODO. I added another one to `Preempt` as well. ",
        "createdAt" : "2017-09-06T18:03:13Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0b718373befb2168befc0c4579fd9a02155d5bc",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +190,194 @@\t\treturn nil, nil, err\n\t}\n\tif !podEligibleToPreemptOthers(pod, g.cachedNodeInfoMap) {\n\t\tglog.V(5).Infof(\"Pod %v is not eligible for more preemption.\", pod.Name)\n\t\treturn nil, nil, nil"
  },
  {
    "id" : "aade5975-12d7-44d6-930c-632096ff9d93",
    "prId" : 50949,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50949#pullrequestreview-61319103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94e0e6da-efe2-48ca-b88c-b8f8c6d0dee3",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "You are allocating a bunch of slices here of the same type.\r\nIf all nodes will have the same characteristics, this may cause a bunch of allocations.\r\n\r\nInstead of we should try to reuse the same slice.\r\nIn fact optimizations like this reduced kube-proxy memory allocations by 10x or sth IIRC (when we actually use the same slice across different calls - it's safe here too because there is at most one active call to this function at any time).",
        "createdAt" : "2017-09-06T11:03:07Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "12eaa2e2-3ac5-45b1-a6a1-d168729e732f",
        "parentId" : "94e0e6da-efe2-48ca-b88c-b8f8c6d0dee3",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Only the last step can reuse the slice from the first step and even then it should reset the slice. So, I added a line to reset the slice from the first step after we finish the second step.\r\nBTW, each element of the slice is 24Bytes. So, even if we add 1000 nodes, we are talking about 24KB of memory in each step which shouldn't be a problem given that this function should run pretty quickly and upon return the slices can be garbage collected.",
        "createdAt" : "2017-09-06T18:20:06Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "63bdd5a1-e8cc-41d0-adb2-4d6c4c17bd54",
        "parentId" : "94e0e6da-efe2-48ca-b88c-b8f8c6d0dee3",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Yeah, sorry, this wasn't fully baked idea.\r\n\r\nLet me clarify one thing - it's not about amount of data. It's about the fact that garbage collection is slow. For example in apiserver, in the past we were observing Go GC taking 50%+ of CPU before we optimized memory usage. So allocating less memory means lower cpu-usage.\r\n\r\nSo basically, I'm not really suggesting to reset the slice as you did - please remove it.\r\nWhat I'm suggesting is to define those slices once (probably by making this a scheduler method and doing it in scheduler itself) and avoid allocating them every time from scratch.\r\n\r\nTo do that, you need to do:\r\nmySlice = mySlice[:0]\r\n\r\n[BTW this is the trick that bytes.Buffer Reset() is doing to avoid reallocations].\r\n\r\nA TODO for that and followup PR is fine for that.",
        "createdAt" : "2017-09-07T07:43:32Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "ac547c07-6aef-48f4-866c-c925bf77c18a",
        "parentId" : "94e0e6da-efe2-48ca-b88c-b8f8c6d0dee3",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Ah, I see. Good point. I added a TODO, but I should think a bit more about how to do it cleanly. These slices are of type \"nodeScore\" which I have defined in this function. Moving the slices, their type, and the function to \"genericScheduler\" for GC reasons feels a bit unclean to me. ",
        "createdAt" : "2017-09-07T18:47:22Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "c187d857-a63e-49b8-a71a-dc6df385d83f",
        "parentId" : "94e0e6da-efe2-48ca-b88c-b8f8c6d0dee3",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "a TODO is fine for me.",
        "createdAt" : "2017-09-07T18:54:34Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0b718373befb2168befc0c4579fd9a02155d5bc",
    "line" : 173,
    "diffHunk" : "@@ -1,1 +536,540 @@\t// smallest sum of priorities.\n\tminSumPriorities := int64(math.MaxInt64)\n\tminSumPriorityScores := []*nodeScore{}\n\tfor _, nodeScore := range minPriorityScores {\n\t\tvar sumPriorities int64"
  }
]