[
  {
    "id" : "9125223d-cf32-4a11-9d05-351d405acadd",
    "prId" : 21894,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc8a8537-b110-465e-85bc-2836a079996c",
        "parentId" : null,
        "authorId" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "body" : "nit: \"However\"? It seems to me that the same special attention is needed in both cases - what's the difference between the cold and hot standby cases w/ respect to consistency among flags?\n",
        "createdAt" : "2016-02-24T19:11:23Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "tags" : [
        ]
      },
      {
        "id" : "9eb96149-d5f2-4a4e-972b-0830fbcb11cb",
        "parentId" : "bc8a8537-b110-465e-85bc-2836a079996c",
        "authorId" : "9ca65a79-fccf-40d2-8bf4-84a611116ce4",
        "body" : "I'm not comparing cold vs. hot here. I was comparing K8sm vs K8s when setting up HA. If you set up HA in K8s, you don't have to worry about different schedulers might have different executor IDs. Whereas for K8sm schedulers, you need to specify its parameters consistently across so they all generate the same executor ID.\n",
        "createdAt" : "2016-02-24T20:16:27Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "9ca65a79-fccf-40d2-8bf4-84a611116ce4",
        "tags" : [
        ]
      },
      {
        "id" : "7d85b9af-efb2-4bad-ba62-f38f397cfeb7",
        "parentId" : "bc8a8537-b110-465e-85bc-2836a079996c",
        "authorId" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "body" : "understood\n",
        "createdAt" : "2016-02-26T20:41:39Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "tags" : [
        ]
      }
    ],
    "commit" : "3cc729a48296b3ed94bc3ebd444bc90d2365a84f",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +98,102 @@\nSetting up Kubernetes on Mesos in cold-standby mode is similar to Kubernetes in\nstandalone mode described in [Kubernetes HA][1]. However, special attention is\nneeded when setting up K8sm scheduler so that when the currently active\nscheduler crashes/dies, a new one can be instantiated and take over the work."
  },
  {
    "id" : "f1f843da-7c01-4dd9-bbef-26c6190f7bf2",
    "prId" : 21894,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8c03c29-e516-4416-ac94-309d006e6504",
        "parentId" : null,
        "authorId" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "body" : "might be worth adding a note here about additional changes needed for TLS/SPDY for full support of all kubectl commands\n",
        "createdAt" : "2016-02-24T19:48:48Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "tags" : [
        ]
      },
      {
        "id" : "70a6996d-6c15-49c6-9a55-d2ba15258d50",
        "parentId" : "d8c03c29-e516-4416-ac94-309d006e6504",
        "authorId" : "9ca65a79-fccf-40d2-8bf4-84a611116ce4",
        "body" : "Can you give me a pointer on which kubectl commands will not work without TLS/SPDY?\n",
        "createdAt" : "2016-02-24T20:28:45Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "9ca65a79-fccf-40d2-8bf4-84a611116ce4",
        "tags" : [
        ]
      },
      {
        "id" : "871725ac-c07c-4385-9630-42358d2f1c33",
        "parentId" : "d8c03c29-e516-4416-ac94-309d006e6504",
        "authorId" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "body" : "probably similar to this:\nhttps://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/issues.md#kubectl\n\nOn Wed, Feb 24, 2016 at 3:29 PM, Hai Huang notifications@github.com wrote:\n\n> In contrib/mesos/docs/ha.md\n> https://github.com/kubernetes/kubernetes/pull/21894#discussion_r54001270\n> :\n> \n> > -  worker_connections  4096;  ## Default: 1024\n> >   +}\n> >   +\n> >   +http {\n> > -  upstream apiservers {\n> > -    server ${K8S_1_IP}:${K8S_APISERVER_PORT};\n> > -    server ${K8S_2_IP}:${K8S_APISERVER_PORT};\n> > -  }\n> >   +\n> > -  upstream schedulers {\n> > -    server ${K8S_1_IP}:${K8S_SCHEDULER_PORT};\n> > -    server ${K8S_2_IP}:${K8S_SCHEDULER_PORT};\n> > -  }\n> >   +\n> > -  server {\n> > -    listen ${NGINX_APISERVER_PORT};\n> \n> Can you give me a pointer on which kubectl commands will not work without\n> TLS/SPDY?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/21894/files#r54001270.\n",
        "createdAt" : "2016-02-24T21:49:47Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "tags" : [
        ]
      },
      {
        "id" : "ab24f8ea-769d-4b28-907e-46bc02edaa62",
        "parentId" : "d8c03c29-e516-4416-ac94-309d006e6504",
        "authorId" : "9ca65a79-fccf-40d2-8bf4-84a611116ce4",
        "body" : "Ah ok, I looked at `kubectl logs` a while ago, and this turned out to be not related to TLS. I didn't look into the other kubectl commands yet. These kubectl problems seem to exist even for K8sm without HA though. But I'll add a note here just for the sake of completeness.\n",
        "createdAt" : "2016-02-24T22:14:28Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "9ca65a79-fccf-40d2-8bf4-84a611116ce4",
        "tags" : [
        ]
      },
      {
        "id" : "dabf6d90-10e7-4bb9-b640-ce8d3be9b7aa",
        "parentId" : "d8c03c29-e516-4416-ac94-309d006e6504",
        "authorId" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "body" : "oh right, there's some debug flag issues that were causing conflicts with\nsome commands..\n\nOn Wed, Feb 24, 2016 at 5:15 PM, Hai Huang notifications@github.com wrote:\n\n> In contrib/mesos/docs/ha.md\n> https://github.com/kubernetes/kubernetes/pull/21894#discussion_r54016047\n> :\n> \n> > -  worker_connections  4096;  ## Default: 1024\n> >   +}\n> >   +\n> >   +http {\n> > -  upstream apiservers {\n> > -    server ${K8S_1_IP}:${K8S_APISERVER_PORT};\n> > -    server ${K8S_2_IP}:${K8S_APISERVER_PORT};\n> > -  }\n> >   +\n> > -  upstream schedulers {\n> > -    server ${K8S_1_IP}:${K8S_SCHEDULER_PORT};\n> > -    server ${K8S_2_IP}:${K8S_SCHEDULER_PORT};\n> > -  }\n> >   +\n> > -  server {\n> > -    listen ${NGINX_APISERVER_PORT};\n> \n> Ah ok, I looked at kubectl logs a while ago, and this turned out to be\n> not related to TLS. I didn't look into the other kubectl commands yet.\n> These kubectl problems seem to exist even for K8sm without HA though. But\n> I'll add a note here just for the sake of completeness.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/21894/files#r54016047.\n",
        "createdAt" : "2016-02-24T22:16:42Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "tags" : [
        ]
      }
    ],
    "commit" : "3cc729a48296b3ed94bc3ebd444bc90d2365a84f",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +436,440 @@\n  server {\n    listen ${NGINX_APISERVER_PORT};\n    location / {\n      proxy_pass              http://apiservers;"
  },
  {
    "id" : "1a70f143-1590-478a-a41b-275e86b9b50f",
    "prId" : 21894,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75bd1325-2cd2-4fe6-b022-01d74b548303",
        "parentId" : null,
        "authorId" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "body" : "Nice, but better to update the build script (above) that people are going to copy/paste\n",
        "createdAt" : "2016-02-26T20:49:24Z",
        "updatedAt" : "2016-03-01T00:49:42Z",
        "lastEditedBy" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "tags" : [
        ]
      }
    ],
    "commit" : "3cc729a48296b3ed94bc3ebd444bc90d2365a84f",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +174,178 @@node.\n\n**IMPORTANT:** Mesosphere team is currently maintaining the stable K8sm release in\na separate [fork][3]. At the time of this writing, the latest stable release is\n`release-v0.7-v1.1`."
  },
  {
    "id" : "9d335603-4f92-4d68-920a-009b3652faaf",
    "prId" : 10613,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3c126fb-873a-4e28-8dc6-c087d63fc0a0",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Just for my own curiosity, is there any reason why it wouldn't work, given that only one is active at a time?\n",
        "createdAt" : "2015-07-15T22:53:35Z",
        "updatedAt" : "2015-07-19T08:14:10Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fca9b6f095f83bf0d7417fb33c26b7c9e466fcd",
    "line" : null,
    "diffHunk" : "@@ -1,1 +16,20 @@Scheduler leader election is implemented using etcd so it is important to have an HA etcd configuration established for reliable scheduler HA.\n\nIt is currently recommended that no more than 2 scheduler instances be running at the same time.\nRunning more than 2 schedulers at once may work but has not been extensively tested.\nYMMV."
  }
]