[
  {
    "id" : "3a4a0e49-e0ad-4225-bb9a-95036cbed23a",
    "prId" : 2219,
    "prUrl" : "https://github.com/root-project/root/pull/2219#pullrequestreview-132446071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f5ecb71-a4f8-460c-aad7-cd059894474f",
        "parentId" : null,
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "This is fine, but a change to consider is making a separate option for each regulariser (in principle one can use them at the same time).\r\nE.g. `L2_Regularisation=1.5:L1_Regularisation=0.`. The default of =0. would disable the regulariser.\r\nRight now it is unclear to me how you control the strength of the regularisation. (Is this `WeightDecay`?)",
        "createdAt" : "2018-06-27T09:44:27Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      },
      {
        "id" : "7cda8ce8-794c-4e14-89b7-f42c3f5f1f60",
        "parentId" : "0f5ecb71-a4f8-460c-aad7-cd059894474f",
        "authorId" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "body" : "yes weight decay controls the strength !\r\nI am not sure I understand your comment about  L2_Regularisation=1.5:L1_Regularisation=0..\r\nNow you have a string defining the regularisation, can be \"None\",\"L1\",\"L2\"",
        "createdAt" : "2018-06-27T13:54:49Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "tags" : [
        ]
      },
      {
        "id" : "262bc7a1-8532-455f-9f4f-4be1855f3380",
        "parentId" : "0f5ecb71-a4f8-460c-aad7-cd059894474f",
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "I'm just saying that in e.g. xgboost the configuration options are\r\n`xgb.train(..., l2_reg=1., l1_reg=0.2, ...)`, that is they are separate, and maybe we would want to have a similar interface.",
        "createdAt" : "2018-06-27T13:58:43Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "2233f13536c501964a54c9a3be107a331f7eaa15",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +352,356 @@      } else if (regularization == \"L2\") {\n         settings.regularization = DNN::ERegularization::kL2;\n      } else {\n         settings.regularization = DNN::ERegularization::kNone;\n      }"
  },
  {
    "id" : "783533a8-c9a5-41b1-b7db-53590af6e218",
    "prId" : 2219,
    "prUrl" : "https://github.com/root-project/root/pull/2219#pullrequestreview-141977926",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "efe5d375-e5c7-44b5-ac88-0345c78d4dc8",
        "parentId" : null,
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "Is this correct? (Time per loss, what does it mean? Sorry if I'm tired and just don't see it!)",
        "createdAt" : "2018-06-27T09:50:38Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      },
      {
        "id" : "72b7d35d-f6a5-47a1-b91b-4a8b649978d5",
        "parentId" : "efe5d375-e5c7-44b5-ac88-0345c78d4dc8",
        "authorId" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "body" : "It means the time to compute the Loss function for both training and testing data.\r\nFor a fixed interval of epochs (defined in testRepetitions) we compute the test error on both \r\ntest and training data set. Maybe we should call t(s)/Test ? ",
        "createdAt" : "2018-06-27T13:58:15Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "tags" : [
        ]
      },
      {
        "id" : "4a504dd1-6185-4dee-bb7b-e8c33346885c",
        "parentId" : "efe5d375-e5c7-44b5-ac88-0345c78d4dc8",
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "As I understanding: We have 2 columns with timing, the training time per epoch for training and then testing. Is \"Train t(s)/epoch\" and \"Test t(s)/epoch\" too long?",
        "createdAt" : "2018-07-31T14:19:38Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "2233f13536c501964a54c9a3be107a331f7eaa15",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +1210,1214 @@         Log() << std::setw(10) << \"Epoch\"\n               << \" | \" << std::setw(12) << \"Train Err.\" << std::setw(12) << \"Test Err.\" \n               << std::setw(12) << \"t(s)/epoch\" << std::setw(12)  << \"t(s)/Loss\"\n               << std::setw(12) << \"nEvents/s\"\n               << std::setw(12) << \"Conv. Steps\" << Endl;"
  },
  {
    "id" : "93c7ce37-a1b9-4e6a-9400-d2482debac15",
    "prId" : 2309,
    "prUrl" : "https://github.com/root-project/root/pull/2309#pullrequestreview-136847195",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "edd619ec-576f-4431-a010-2d558ba53ab8",
        "parentId" : null,
        "authorId" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "body" : "Else `settings.optimizer` is uninitialized?!",
        "createdAt" : "2018-07-12T12:14:15Z",
        "updatedAt" : "2018-07-16T14:26:45Z",
        "lastEditedBy" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "tags" : [
        ]
      },
      {
        "id" : "e1b462f1-2e2d-42e2-9e07-49b48f9e1ee3",
        "parentId" : "edd619ec-576f-4431-a010-2d558ba53ab8",
        "authorId" : "2ea1e355-3dfa-43cc-8da8-55862e510e81",
        "body" : "I guess here the \"else part\" can never be reached because if the optimizer option is not specified in the training strategy string, the default value \"SGD\" will be taken.",
        "createdAt" : "2018-07-12T13:49:06Z",
        "updatedAt" : "2018-07-16T14:26:45Z",
        "lastEditedBy" : "2ea1e355-3dfa-43cc-8da8-55862e510e81",
        "tags" : [
        ]
      },
      {
        "id" : "126dc3aa-a361-4555-a927-6361f2939c20",
        "parentId" : "edd619ec-576f-4431-a010-2d558ba53ab8",
        "authorId" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "body" : "But what if I misspell the optimizer as `\"SDG\"`? *Please* initialize the member, ideally using a member initializer!",
        "createdAt" : "2018-07-12T22:03:49Z",
        "updatedAt" : "2018-07-16T14:26:45Z",
        "lastEditedBy" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "tags" : [
        ]
      },
      {
        "id" : "2f5fd541-92b2-4226-88e8-166ac482fc56",
        "parentId" : "edd619ec-576f-4431-a010-2d558ba53ab8",
        "authorId" : "2ea1e355-3dfa-43cc-8da8-55862e510e81",
        "body" : "Yeah! That would cause a problem in case of misspelling. I'll change that.",
        "createdAt" : "2018-07-12T22:06:50Z",
        "updatedAt" : "2018-07-16T14:26:45Z",
        "lastEditedBy" : "2ea1e355-3dfa-43cc-8da8-55862e510e81",
        "tags" : [
        ]
      }
    ],
    "commit" : "49ba80007169299576268f162ced299a5843e60c",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +348,352 @@      TString optimizer = fetchValueTmp(block, \"Optimizer\", TString(\"SGD\"));\n      if (optimizer == \"SGD\") {\n         settings.optimizer = DNN::EOptimizer::kSGD;\n      } else {\n         // Since only one optimizer is implemented, make that as default choice for now if the input string is"
  },
  {
    "id" : "5f5f1af7-bcc3-4c03-8295-d5fcc3fdb9e1",
    "prId" : 2309,
    "prUrl" : "https://github.com/root-project/root/pull/2309#pullrequestreview-136846604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f9e744b-7a2f-4091-a883-d3c9b87fa511",
        "parentId" : null,
        "authorId" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "body" : "Note that from here on, every one else writing to `Log()` (which defaults to `std::cerr` or `std::cout`?) will have \"suffer\" from the `setw(10)` call. Please consider capturing the flags and resetting them afterwards. See e.g. https://stackoverflow.com/a/2273352/6182509",
        "createdAt" : "2018-07-12T12:20:43Z",
        "updatedAt" : "2018-07-16T14:26:45Z",
        "lastEditedBy" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "tags" : [
        ]
      },
      {
        "id" : "6208a9c7-bae9-4176-934b-46cd340627d7",
        "parentId" : "7f9e744b-7a2f-4091-a883-d3c9b87fa511",
        "authorId" : "2ea1e355-3dfa-43cc-8da8-55862e510e81",
        "body" : "It was like this previously. I haven't changed that part of the code. And I am not able to get what change I want to do here?",
        "createdAt" : "2018-07-12T15:41:43Z",
        "updatedAt" : "2018-07-16T14:26:45Z",
        "lastEditedBy" : "2ea1e355-3dfa-43cc-8da8-55862e510e81",
        "tags" : [
        ]
      },
      {
        "id" : "ddb93fd8-d824-44dc-bcf8-e18cf47259e8",
        "parentId" : "7f9e744b-7a2f-4091-a883-d3c9b87fa511",
        "authorId" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "body" : "OK fine - we'll fix it when people complain!",
        "createdAt" : "2018-07-12T22:04:18Z",
        "updatedAt" : "2018-07-16T14:26:45Z",
        "lastEditedBy" : "09b7e073-cbeb-42e8-aa22-8b1468de56ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "49ba80007169299576268f162ced299a5843e60c",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +1214,1218 @@            if (testError < minTestError ) {\n               // Copy weights from deepNet to fNet\n               Log() << std::setw(10) << optimizer->GetGlobalStep()\n                     << \" Minimum Test error found - save the configuration \" << Endl;\n               for (size_t i = 0; i < deepNet.GetDepth(); ++i) {"
  }
]