[
  {
    "id" : "3a4a0e49-e0ad-4225-bb9a-95036cbed23a",
    "prId" : 2219,
    "prUrl" : "https://github.com/root-project/root/pull/2219#pullrequestreview-132446071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f5ecb71-a4f8-460c-aad7-cd059894474f",
        "parentId" : null,
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "This is fine, but a change to consider is making a separate option for each regulariser (in principle one can use them at the same time).\r\nE.g. `L2_Regularisation=1.5:L1_Regularisation=0.`. The default of =0. would disable the regulariser.\r\nRight now it is unclear to me how you control the strength of the regularisation. (Is this `WeightDecay`?)",
        "createdAt" : "2018-06-27T09:44:27Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      },
      {
        "id" : "7cda8ce8-794c-4e14-89b7-f42c3f5f1f60",
        "parentId" : "0f5ecb71-a4f8-460c-aad7-cd059894474f",
        "authorId" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "body" : "yes weight decay controls the strength !\r\nI am not sure I understand your comment about  L2_Regularisation=1.5:L1_Regularisation=0..\r\nNow you have a string defining the regularisation, can be \"None\",\"L1\",\"L2\"",
        "createdAt" : "2018-06-27T13:54:49Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "tags" : [
        ]
      },
      {
        "id" : "262bc7a1-8532-455f-9f4f-4be1855f3380",
        "parentId" : "0f5ecb71-a4f8-460c-aad7-cd059894474f",
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "I'm just saying that in e.g. xgboost the configuration options are\r\n`xgb.train(..., l2_reg=1., l1_reg=0.2, ...)`, that is they are separate, and maybe we would want to have a similar interface.",
        "createdAt" : "2018-06-27T13:58:43Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "2233f13536c501964a54c9a3be107a331f7eaa15",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +352,356 @@      } else if (regularization == \"L2\") {\n         settings.regularization = DNN::ERegularization::kL2;\n      } else {\n         settings.regularization = DNN::ERegularization::kNone;\n      }"
  },
  {
    "id" : "783533a8-c9a5-41b1-b7db-53590af6e218",
    "prId" : 2219,
    "prUrl" : "https://github.com/root-project/root/pull/2219#pullrequestreview-141977926",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "efe5d375-e5c7-44b5-ac88-0345c78d4dc8",
        "parentId" : null,
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "Is this correct? (Time per loss, what does it mean? Sorry if I'm tired and just don't see it!)",
        "createdAt" : "2018-06-27T09:50:38Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      },
      {
        "id" : "72b7d35d-f6a5-47a1-b91b-4a8b649978d5",
        "parentId" : "efe5d375-e5c7-44b5-ac88-0345c78d4dc8",
        "authorId" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "body" : "It means the time to compute the Loss function for both training and testing data.\r\nFor a fixed interval of epochs (defined in testRepetitions) we compute the test error on both \r\ntest and training data set. Maybe we should call t(s)/Test ? ",
        "createdAt" : "2018-06-27T13:58:15Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "3cdeea47-0921-4380-9f86-57ac2ba3546a",
        "tags" : [
        ]
      },
      {
        "id" : "4a504dd1-6185-4dee-bb7b-e8c33346885c",
        "parentId" : "efe5d375-e5c7-44b5-ac88-0345c78d4dc8",
        "authorId" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "body" : "As I understanding: We have 2 columns with timing, the training time per epoch for training and then testing. Is \"Train t(s)/epoch\" and \"Test t(s)/epoch\" too long?",
        "createdAt" : "2018-07-31T14:19:38Z",
        "updatedAt" : "2018-08-10T07:51:30Z",
        "lastEditedBy" : "a7291a90-8576-4162-b01b-b44d230129f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "2233f13536c501964a54c9a3be107a331f7eaa15",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +1210,1214 @@         Log() << std::setw(10) << \"Epoch\"\n               << \" | \" << std::setw(12) << \"Train Err.\" << std::setw(12) << \"Test Err.\" \n               << std::setw(12) << \"t(s)/epoch\" << std::setw(12)  << \"t(s)/Loss\"\n               << std::setw(12) << \"nEvents/s\"\n               << std::setw(12) << \"Conv. Steps\" << Endl;"
  }
]