[
  {
    "id" : "079fea6f-45c2-4f24-8bd9-7282180b4e38",
    "prId" : 103484,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/103484#pullrequestreview-700639814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8e72a85-c4bf-4660-beea-55b7897bd69d",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "See earlier comment on \"width\" for a queue.",
        "createdAt" : "2021-07-06T20:27:22Z",
        "updatedAt" : "2021-07-06T20:40:30Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "01c530cd-c238-4fbc-a98b-b03ff23ea616",
        "parentId" : "b8e72a85-c4bf-4660-beea-55b7897bd69d",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "done in the same PR",
        "createdAt" : "2021-07-07T06:55:55Z",
        "updatedAt" : "2021-07-07T07:00:11Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ecc7ba311ab33b16c5d907ebb1120e3e51a947d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +473,477 @@func (qs *queueSet) chooseQueueIndexLocked(hashValue uint64, descr1, descr2 interface{}) int {\n\tbestQueueIdx := -1\n\tbestQueueWidth := int(math.MaxInt32)\n\t// the dealer uses the current desired number of queues, which is no larger than the number in `qs.queues`.\n\tqs.dealer.Deal(hashValue, func(queueIdx int) {"
  },
  {
    "id" : "6f3249a5-4dff-456e-917d-9deb3ab0702b",
    "prId" : 103484,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/103484#pullrequestreview-700639814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c25081c-ed54-417f-84b2-799692a42c35",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Yes, I think the ideal would be to judge based on the current projected E (completion time in the virtual world) of the youngest request in the queue, since that is (the best available estimate of) what the virtual world scheduling of this new request will be based on.",
        "createdAt" : "2021-07-06T20:33:08Z",
        "updatedAt" : "2021-07-06T20:40:30Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "33e706a9-1205-4a82-b8d0-cab67364c9f7",
        "parentId" : "3c25081c-ed54-417f-84b2-799692a42c35",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Extended the comment in that PR too.",
        "createdAt" : "2021-07-07T06:57:21Z",
        "updatedAt" : "2021-07-07T07:00:11Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ecc7ba311ab33b16c5d907ebb1120e3e51a947d",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +477,481 @@\tqs.dealer.Deal(hashValue, func(queueIdx int) {\n\t\t// TODO: Consider taking into account `additional latency` of requests\n\t\t// in addition to their widths.\n\t\tthisWidth := qs.queues[queueIdx].requests.Width()\n\t\tklog.V(7).Infof(\"QS(%s): For request %#+v %#+v considering queue %d of width %d\", qs.qCfg.Name, descr1, descr2, queueIdx, thisWidth)"
  },
  {
    "id" : "3bf52fd6-d800-4383-8819-3097cfe8bb2e",
    "prId" : 103484,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/103484#pullrequestreview-700639814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6aa8e18-0994-4719-ab9e-40bbc2fe50d3",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "https://github.com/kubernetes/api/blob/9f69feafab6a695158ae715cd035656e9d11b61a/flowcontrol/v1beta1/types.go#L529-L534 still declares a limit on queue _length_ , so comparing that with a sum of seats is not appropriate.",
        "createdAt" : "2021-07-06T20:38:01Z",
        "updatedAt" : "2021-07-06T20:40:30Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "0992ec7a-1d92-4e90-8078-ef76e6549974",
        "parentId" : "a6aa8e18-0994-4719-ab9e-40bbc2fe50d3",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Good catch - reverting this line in that PR.",
        "createdAt" : "2021-07-07T06:57:48Z",
        "updatedAt" : "2021-07-07T07:00:11Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ecc7ba311ab33b16c5d907ebb1120e3e51a947d",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +534,538 @@func (qs *queueSet) rejectOrEnqueueLocked(request *request) bool {\n\tqueue := request.queue\n\tcurQueueLength := queue.requests.Width()\n\t// rejects the newly arrived request if resource criteria not met\n\tif qs.totSeatsInUse >= qs.dCfg.ConcurrencyLimit &&"
  },
  {
    "id" : "7163f4de-7654-4208-bdc9-b8d16b0ef223",
    "prId" : 102848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102848#pullrequestreview-684610056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e5adf83-3c1a-4fd4-915e-7f3c0aebbf6b",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "What is described in the KEP is this concept of additional latency. Copying from the KEP:\r\n```\r\nOne more important aspect to resolve is what happens if a given priority level doesn't have enough concurrency units assigned to it. To be on the safe side we should probably implement borrowing across priority levels. However, given we don't want to block introducing the width concept on design and implementation of borrowing, until this is done we have two main options:\r\n\r\ncap the width at the concurrency units assigned to the priority level\r\nreject requests for which we won't be able to allocate enough concurrency units\r\nTo avoid breaking some users, we will proceed with the first option (when computing the cap we should also report requests that we believe are too wide for a given priority level - it would allow operators to adjust configs). That said, to accommodate for the inaccuracy here we will introduce a concept of additional latency for a request. This basically means that after the request finishes in a real world, we still don't mark it as finished in the virtual world for additional latency. Adjusting virtual time of a queue to do that is trivial. The other thing to tweak is to ensure that the concurrency units will not get available for other requests for that time (because currently all actions are triggerred by starting or finishing some request). We will maintain that possibility by wrapping the handler into another one that will be sleeping for additional latence after the request is processed.\r\n\r\nNote that given the estimation for duration of processing the requests is automatically corrected (both up and down), there is no need to change that in the initial version.\r\n```\r\n\r\nI don't necessary need this to be implemented in this PR, but let's just add a TODO to implement this (this isn't that hard to add and borrowing will take us a lot of time).",
        "createdAt" : "2021-06-14T18:06:07Z",
        "updatedAt" : "2021-06-14T18:23:53Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "899266a7-42bc-4896-8593-a08c83222989",
        "parentId" : "0e5adf83-3c1a-4fd4-915e-7f3c0aebbf6b",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "BTW - I prototyped that (and couple other things) in: https://github.com/kubernetes/kubernetes/pull/102875\r\nFeel free to pick up that PR too (though we should probably add additionallatency in a separate PR not in this one).",
        "createdAt" : "2021-06-15T09:06:56Z",
        "updatedAt" : "2021-06-15T09:07:03Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "d6a142bc-fdc9-42c3-b6d9-732909f3a743",
        "parentId" : "0e5adf83-3c1a-4fd4-915e-7f3c0aebbf6b",
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "thanks, I am happy to pick it up, and yes, i will do it in a separate PR.",
        "createdAt" : "2021-06-15T23:54:33Z",
        "updatedAt" : "2021-06-15T23:57:28Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff716cef508f948b50e1026e980e6df5ee475538",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +671,675 @@\t\t// we have picked the queue with the minimum virtual finish time, but\n\t\t// the number of seats this request asks for exceeds the concurrency limit.\n\t\t// TODO: this is a quick fix for now, once we have borrowing in place we will not need it\n\t\tif qs.totRequestsExecuting == 0 {\n\t\t\t// TODO: apply additional lateny associated with this request, as described in the KEP"
  },
  {
    "id" : "8a32c815-926c-425f-9111-72e94061d427",
    "prId" : 102848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102848#pullrequestreview-686508910",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I started wondering about picking the queue by \"VirtualFinish\" time. Up until now, it actually worked fine.\r\n\r\nBut now, it may become extremely penalizing for wider requests.\r\nImagine a case where there are just three queue:\r\n- in first two queues you have a number of extremely fast requests with width=1 (constant incoming flow of them so that the queue always have one)\r\n- in the last queue you have a request with say width=2 (but let's say it will also be really fast)\r\n- say we have concurrency limit = 2\r\nNow - because we have G=60s, what is happening is:\r\nqueue1: finish_time = t0 + G\r\nqueue2: finish_time = t0 + G\r\nqueue2: finish_time = to + 2G\r\n[we pick up queue1 and queue2 to dispatch their requests]\r\nlet's say they finish after 1ms and we course-correct their virtual time and now we have:\r\nqueue1: finish_time = t0 + 1ms + G\r\nqueue2: finish_time = t0 + 1ms + G\r\nqueue3: finish_time = t0 + 2G\r\nand now we again dispatch from queue1 and queue2\r\n\r\nand so on\r\n\r\nSo until we dispatch something from queue3, it will take ages...\r\n\r\n\r\nSo I think we should actually choose the queue based on VirtualStartTime (as opposed to VirtualFinishTime) and just probably resolve conflicts by preferring narrower requests.\r\n\r\n@MikeSpreitzer @tkashem - thoughts?",
        "createdAt" : "2021-06-14T18:22:22Z",
        "updatedAt" : "2021-06-14T18:23:53Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "df93e314-e7ba-4550-b280-0e9fd0cc7fd2",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This PR has to be rebased on top of your last PR (this will cause some of your new tests to start failing). \r\n\r\nBut instead of changing tests - we should in my opinion address this comment.",
        "createdAt" : "2021-06-16T05:51:34Z",
        "updatedAt" : "2021-06-16T05:51:34Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "c82f18bf-fab2-4318-919e-26516c851c8f",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "FWIW - for requests with width=1, this would be a no-op change effectively",
        "createdAt" : "2021-06-16T10:07:29Z",
        "updatedAt" : "2021-06-16T10:07:30Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "6003c563-ada4-4eb2-ba8f-4daf1fb66a7a",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "so i tracked how `queue.virtualStart` changes in order of the following events:\r\n\r\nA: a new request arrives (enqueue)\r\n- if the queue is inactive (no request is currently executing or no other request is waiting in queue) then new request can start immediately (current `R(t)` value):\r\n```\r\nqueue.virtualStart = qs.virtualTime\r\n```\r\n- if the queue is active then we don't change `queue.virtualStart` yet\r\n\r\nB: next round - select a queue with the minimum finish time\r\n- we make sure per-queue virtual time `queue.virtualStart` does not fall behind the global\r\n\r\nC: the request is dequeued, we adjust `queue.virtualStart` as below\r\n```\r\nqueue.virtualStart += G*W\r\n```\r\n\r\nD: request finishes - we do course correcion\r\n```\r\nr.queue.virtualStart -= (G-S)*W\r\n``` \r\n\r\nIf we consider the the above operations, it seems we do take into account the arrival time and implement\r\n```\r\nSi = MAX(R(t), Fi-1)\r\n```\r\n\r\nthe scenario you described, yes it would be a while before q3 is picked and that's fair i think if we consider the bit by bit round robin, at some point virtual finish time for q3 will be minimum\r\n\r\n\r\n\r\n",
        "createdAt" : "2021-06-17T02:06:46Z",
        "updatedAt" : "2021-06-17T02:08:48Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      },
      {
        "id" : "28e3cb59-5de5-4adf-a3bc-77f3b0c111a2",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "At some point it *would* be minimal I agree. The problem is huge how G is (it's 60s).\r\nSo given we're multiplying it by `seats`, it may take minutes until it would happen. Up until then we wll be course-correcting virtualStart (point D) that will still be pushing other queues to have smaller time.\r\n\r\nAnd because of that, at some point we will simply reject that calls (as it would be waiting for too old).\r\n\r\nWhich makes this unfair, because once request with high width reaches the peak of queue, we effectively stop choosing that queue for a long time of there are requests with smaller width in other queue (assuming the wide request didn't arrive minutes earlier).",
        "createdAt" : "2021-06-17T05:25:05Z",
        "updatedAt" : "2021-06-17T05:25:05Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "0d1fe983-5063-4d10-84c2-64ea35460610",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "In other words - if we assume that processing each request takes 1ms (which isn't completely unrealistic), then you have to process 60,000 requests from queue1 and queue2 before you pick the request from queue3. Given that the second request has just 2x higher width, that's not fair at all.\r\n\r\nI strongly believe that just using \"virtual_start\" is the right thing to do:\r\n- for requests with width=1, it's a no-op (because finish_time = start_time + G so finish_time_1 < finish_time_2 <=> start_time_1 < start_time_2)\r\n- it achieves what we want for requests with higher width",
        "createdAt" : "2021-06-17T11:45:10Z",
        "updatedAt" : "2021-06-17T11:45:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "d69be003-6518-4762-ac90-0c1ee6d24415",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "so let's simplify the scenario you described:\r\n- we have two queues `q1` and `q2`\r\n- `q1` has an infinite supply of requests with `W=1`\r\n- `q2` has one request waiting in the queue with the same width `W` = 1\r\n- requests complete really fast `S=1ms` \r\n\r\nIn this scenario we will execute 60,000 requests from `q1` before we pick the request from `q2`. since we are adjusting the virtual start for the `q1` it may look like we are being unfair to `q2` with a request of same width. Yes it gets proportionally worse for request with `W` > 1. \r\n\r\n\r\nSo what you are suggesting would be equivalent to what we have today, not take into account `W` into virtual finish time calculation (basically `W` = 1 for all requests). For now, if we don't use `W` in calculating virtual finish time then we are retaining the current behavior. This feels safer, at least for now.\r\nI think we need unit tests where we can simulate these use cases in a controlled manner and observer/assert on the expected behavior with multiple queues and requests with varying width. This would require some refactor of the `queueSet` and carefully crafted unit tests.\r\n\r\nIn the meantime, we could have the following:\r\n- do not take `W` into account when calculating virtual finish time - in the virtual world all requests are estimated to take the same amount of time.\r\n- we use `W` for seats allocation.\r\n\r\n@wojtek-t @MikeSpreitzer thoughts?\r\n\r\n",
        "createdAt" : "2021-06-17T14:11:25Z",
        "updatedAt" : "2021-06-17T14:17:10Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      },
      {
        "id" : "241286d9-85ea-4c17-8632-34375bd0cfdf",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "> q2 has one request waiting in the queue with the same width W = 1\r\n\r\nYou mean q2 has request with w=2, right?\r\n\r\n> - do not take W into account when calculating virtual finish time - in the virtual world all requests are estimated to take the same amount of time.\r\n>  - we use W for seats allocation.\r\n\r\nSo to rephrase:\r\n- we still use W to adjusting virtualStart as we do now, e.g. here:\r\nhttps://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go#L640\r\n- but we just change the GetNextFinish() to not take `W` into account\r\n\r\nThat translates to exactly the same what I'm proposing (i.e. taking virtualStart, because virtualFinish for every request will be equal to \"virtualStart + G\". So I don't really see how this is different from what I'm proposing.",
        "createdAt" : "2021-06-17T14:33:59Z",
        "updatedAt" : "2021-06-17T14:33:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "37eb03fd-227e-49a2-b37c-cd06b0894be5",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "> You mean q2 has request with w=2, right?\r\n\r\nno, q2 has request with w=1 but I forgot to mention concurrency limit = 1\r\n\r\n> That translates to exactly the same what I'm proposing\r\n\r\nI was referring to:\r\n- A: when we dequeue: `queue.virtualStart += G`\r\n- B: when request finishes: `queue.virtualStart -= G-S`\r\n- C: when we pick: finish time `queue.virtualStart + G`\r\n\r\nThis is the current behavior, what you are suggesting is for `C` we don't use `W`, but otherwise use it for A and B. with your proposal, while there is a currently executing request with much higher width, smaller requests waiting in the said queue won't be picked up since its `virtualStart` is higher because we included `W` and these requests waiting have a chance to timeout as well. \r\nwith my proposal we are equally fair to all requests, `W` matters in only seat allocation. it's practically no change from how we select from queue today. \r\n\r\nwe can start with what you are proposing, we can tweak later if need be. @MikeSpreitzer unless you have any other concerns i want to go ahead and make the changes\r\n",
        "createdAt" : "2021-06-17T15:56:05Z",
        "updatedAt" : "2021-06-17T15:56:06Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      },
      {
        "id" : "89d6047f-c814-40fe-ad8c-20c338243854",
        "parentId" : "431b2094-6012-4ad1-b67f-df53c5a7ce58",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "> no, q2 has request with w=1 but I forgot to mention concurrency limit = 1\r\n\r\nWith width=1 everything works correctly. Because virtualStart will be increasing after every request so it doesn't matter.\r\n\r\n> This is the current behavior\r\n\r\nCurrent where? The current code behavior is already that we increase virtualStart by G*Seats() - see link I pasted above:\r\nhttps://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go#L640\r\n\r\n> with your proposal, while there is a currently executing request with much higher width, smaller requests waiting in the said queue won't be picked up since its virtualStart is higher because we included W and these requests waiting have a chance to timeout as well.\r\n\r\nWhich is exactly what I want. We need to be fair across queue - not fair across requests.\r\nThis has to be adressed by changing how we choose to which queue we add a request - see:\r\nhttps://github.com/kubernetes/kubernetes/pull/102875/files#r653296345\r\n\r\n\r\n> with my proposal we are equally fair to all requests, W matters in only seat allocation. it's practically no change from how we select from queue today.\r\n\r\nBut that's not what we want.\r\nImagine two queues:\r\n- q1 we have a number of short 'get object X' requests (width=1)\r\n- q2 we have a number of large 'get all pods\" requests (e.g. width=10)\r\n\r\nWhat we really want is we want to provide equal capacity across these queue. So if the request is e.g. 10x more expensive we want to proceed 10x less requests from that queue.\r\n\r\nSo to summarize - we want to be fair across queues. What you're proposing doesn't provide that guarantee - wide requests to some extent will be starving small short requests, which is not what we want.\r\nWhat I'm proposing provide the fairness across queues.\r\n\r\n\r\n\r\n\r\n",
        "createdAt" : "2021-06-17T16:07:53Z",
        "updatedAt" : "2021-06-17T16:07:54Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff716cef508f948b50e1026e980e6df5ee475538",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +727,731 @@\t\t}\n\t}\n\n\t// TODO: add a method to fifo that lets us peek at the oldest request\n\tvar oldestReqFromMinQueue *request"
  },
  {
    "id" : "6e9688b4-57af-42f9-a1cc-62c5e5b7e77b",
    "prId" : 102848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102848#pullrequestreview-684746433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa5ba9e1-62db-4f21-bc1e-66e73d069386",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I wrote a long comment why it doesn't work and then I realized it works because we don't modify robinIndex below (we don't modify if we don't dispatch anything).\r\nCan you maybe add an explicit comment above that it's expected and we do that to preserve round-robin behavior?",
        "createdAt" : "2021-06-14T18:23:48Z",
        "updatedAt" : "2021-06-14T18:23:53Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "b6b46cf0-26e4-40d0-9c2a-fc2cfe30da6e",
        "parentId" : "aa5ba9e1-62db-4f21-bc1e-66e73d069386",
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "yes, I added a comment, and also the test I added verifies expected value of robin index with multiple calls to `selectQueueLocked`",
        "createdAt" : "2021-06-15T23:55:37Z",
        "updatedAt" : "2021-06-15T23:55:38Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      },
      {
        "id" : "b9d767ec-81e6-43a9-b48f-663a53a8addc",
        "parentId" : "aa5ba9e1-62db-4f21-bc1e-66e73d069386",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Thanks!",
        "createdAt" : "2021-06-16T05:49:32Z",
        "updatedAt" : "2021-06-16T05:49:33Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff716cef508f948b50e1026e980e6df5ee475538",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +739,743 @@\t\treturn nil\n\t}\n\n\t// we set the round robin indexing to start at the chose queue\n\t// for the next round.  This way the non-selected queues"
  },
  {
    "id" : "d2358b43-1bb3-4ffe-9b13-f9ab0b626524",
    "prId" : 102848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102848#pullrequestreview-686647106",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25f68893-527a-416b-960f-ab156a942004",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Can you please add a TODO to set additional_latency once we add support to it?",
        "createdAt" : "2021-06-17T18:08:34Z",
        "updatedAt" : "2021-06-17T18:15:07Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff716cef508f948b50e1026e980e6df5ee475538",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +674,678 @@\t\tif qs.totRequestsExecuting == 0 {\n\t\t\t// TODO: apply additional lateny associated with this request, as described in the KEP\n\t\t\treturn true\n\t\t}\n\t\t// wait for all \"currently\" executing requests in this queueSet"
  },
  {
    "id" : "f5dcdd6c-bfdb-4cd1-abdc-abe9cdc87a23",
    "prId" : 102848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102848#pullrequestreview-687139489",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3d9857b-ca53-4f50-a41d-96a046356f08",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "one last nit: can you maybe add a specific example that we were discussing (I mean your simplified version with two queues) and discussion about it (e.g. need to process 60,000 requests before we put from the other queue) to ensure we won't forget about this case in the future?",
        "createdAt" : "2021-06-17T18:40:49Z",
        "updatedAt" : "2021-06-17T18:40:50Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "a2cf4ca9-b1f5-4e5f-a0e8-6e717d8b5377",
        "parentId" : "d3d9857b-ca53-4f50-a41d-96a046356f08",
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "done, pls check ",
        "createdAt" : "2021-06-17T19:06:55Z",
        "updatedAt" : "2021-06-17T19:06:56Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      },
      {
        "id" : "61441f43-5f51-4134-9b8e-163b9ea759e3",
        "parentId" : "d3d9857b-ca53-4f50-a41d-96a046356f08",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "I do not think we should attempt a justification in a comment here.  The original fair queuing paper says that the dispatch order is the order of completion in the virtual world (with ties broken by round-robin ordering), and none of our design alternatives change that.",
        "createdAt" : "2021-06-18T04:35:31Z",
        "updatedAt" : "2021-06-18T04:52:25Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "42403d47-5175-4047-b9db-67980e0aab38",
        "parentId" : "d3d9857b-ca53-4f50-a41d-96a046356f08",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "The example below shows that if we didn't change what we had before, it wouldn't work. So I actually claim that this comment is really needed.\r\n\r\nI'm happy to hear how we can frame it differently, but I don't think that's very urgent.",
        "createdAt" : "2021-06-18T08:03:58Z",
        "updatedAt" : "2021-06-18T08:04:52Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff716cef508f948b50e1026e980e6df5ee475538",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +706,710 @@\t\t\t// we are not taking the width of the request into account when\n\t\t\t// we calculate the virtual finish time of the request because\n\t\t\t// it can starve requests with smaller wdith in other queues.\n\t\t\t//\n\t\t\t// so let's draw an example of the starving scenario:"
  },
  {
    "id" : "e4b2b5c2-8fcd-4cb3-889d-9514f937d0db",
    "prId" : 102848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102848#pullrequestreview-687139489",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b07ee5c-1b46-4c92-86f8-019f3f614c25",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "I think this PR should update this comment, perhaps to something like the following.\r\n\r\n> // canAccommodateSeatsLocked decides whether the QueueSet's concurrency constraint allows dispatching a new request that will occupy the given number of seats.",
        "createdAt" : "2021-06-18T04:32:23Z",
        "updatedAt" : "2021-06-18T04:52:25Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "de57ad96-440f-4e93-a5c6-55ce123cf4c2",
        "parentId" : "8b07ee5c-1b46-4c92-86f8-019f3f614c25",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@tkashem - let's do that in a follow up",
        "createdAt" : "2021-06-18T08:04:10Z",
        "updatedAt" : "2021-06-18T08:04:52Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff716cef508f948b50e1026e980e6df5ee475538",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +665,669 @@// canAccommodateSeatsLocked returns true if this queueSet has enough\n// seats available to accommodate a request with the given number of seats,\n// otherwise it returns false.\nfunc (qs *queueSet) canAccommodateSeatsLocked(seats int) bool {\n\tswitch {"
  },
  {
    "id" : "7a588c6a-f611-465f-bd7d-7da936a02c90",
    "prId" : 102843,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102843#pullrequestreview-684060970",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d41d32b7-b64b-4af8-b4d9-0511ad8cb3a6",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "nit: remove the empty line above",
        "createdAt" : "2021-06-15T14:09:20Z",
        "updatedAt" : "2021-06-15T14:13:46Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f7456076e0cb29a95d86cd0f54c34a04b4722ab",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +675,679 @@\t\tqueue := qs.queues[qs.robinIndex]\n\t\tif queue.requests.Length() != 0 {\n\t\t\tcurrentVirtualFinish := queue.GetNextFinish(qs.estimatedServiceTime)\n\t\t\tif currentVirtualFinish < minVirtualFinish {\n\t\t\t\tminVirtualFinish = currentVirtualFinish"
  },
  {
    "id" : "3133c63c-91d1-4b47-b092-1f679187b95d",
    "prId" : 97206,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97206#pullrequestreview-557094978",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6403cfd3-f2fc-458d-8de9-e8b4fcd7e95e",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "thanks",
        "createdAt" : "2020-12-22T13:54:54Z",
        "updatedAt" : "2020-12-22T13:54:55Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "13cedca0eb5337b13e5176983ea5e784ec38df22",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +317,321 @@\t\treturn idle\n\t}\n\tfunc() {\n\t\tdefer func() {\n\t\t\tidle = req.qs.finishRequestAndDispatchAsMuchAsPossible(req)"
  },
  {
    "id" : "f4bb1707-a788-4293-b89d-da2d81c65041",
    "prId" : 85259,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85259#pullrequestreview-317790357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a39e3866-1689-476e-abec-a8b378905ba6",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I am not sure how I missed that `descr1, descr2 interface{}` is being passed around; that is really weird and hard to understand, and neither the comment nor the name of the parameters helps me understand what they are supposed to be such that passing them as an `interface{}` makes sense. Can we fix this in a followup? Worst case, the caller can package their log identity in a single string. I think it is reasonable for the caller to pass some printable request identifier. There may also already be such a thing hidden in the context.",
        "createdAt" : "2019-11-14T22:24:39Z",
        "updatedAt" : "2019-11-15T01:25:58Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "fa56d8e2-80e6-4ad0-9cf1-3b2f6c7f862f",
        "parentId" : "a39e3866-1689-476e-abec-a8b378905ba6",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Note that we already have a trace system that prints out times when requests are slow, it uses a number to identify the request so you can see which trace line goes to which request. I think we should probably reuse that number. I think it is buried in ctx somewhere.",
        "createdAt" : "2019-11-14T22:29:08Z",
        "updatedAt" : "2019-11-15T01:25:58Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "5e012f1f-c282-4c3d-9bca-c342e993064a",
        "parentId" : "a39e3866-1689-476e-abec-a8b378905ba6",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "I am not sure I understand the idea regarding the trace system.  If the logging output gets just one message about a request, and that message shows a number and not the RequestInfo and User details, then it will not be helpful.  Is the number only for the source code, and somehow the details appear in the log message?  Or does something trigger an additional log message that associates the number with details?\r\n\r\nI really do not want to go creating long strings that will not get logged for every request.",
        "createdAt" : "2019-11-15T01:14:00Z",
        "updatedAt" : "2019-11-15T01:25:58Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "914770ed-a629-4f4f-aed7-f716da9dec5b",
        "parentId" : "a39e3866-1689-476e-abec-a8b378905ba6",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "BTW, what is weird and hard to understand about this?  It is passing two opaque values that get logged to explain what the request is.\r\n\r\nThe goal is not to log a request _identifier_ but rather enough details of the request that a reader of log messages can understand which request is involved.",
        "createdAt" : "2019-11-15T01:23:00Z",
        "updatedAt" : "2019-11-15T01:25:58Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "70cd01d8-0d4c-4cf9-b575-2b6a542879ff",
        "parentId" : "a39e3866-1689-476e-abec-a8b378905ba6",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Since there's (potentially) multiple messages logged about a single request, they'll be interspersed in the logs with gajillions of unrelated messages. So, a reader of the logs needs a simple way to show all messages related to the request of interest to them.\r\n\r\nWe already have a similar problem when showing which aspects of a request were slow, in the trace system. There, we print in one message the request details, and in many messages timing information. To keep the log spam down, traces only begin to print once some part of the request has been sufficiently slow.\r\n\r\nNow, we have the additional problem of making sure that the relevant additional request information gets logged when appropriate. It actually makes the log lines harder to read and searches less precise to print out the request description with every related log line rather than printing out a request ID and the relevant information once.\r\n\r\n(I say all this as someone who occasionally does high-stakes spelunking in apiserver logs.)\r\n\r\nI complain about the variable names since it seems really strange to know that you need two pieces of data, but not know what they are enough to give them a name or a type. It makes this code difficult to understand apart from its usage.\r\n\r\nI actually think logging at V(6) isn't going to be super helpful in real life, V(6) is an impractical amount of data. I'd like to find a way to log < 1 message per request, so that we can have the logs on at V2, to give operators some chance at retroactively understanding what the system was doing. Even something rudimentary like occasionally logging the trace IDs of requests that are stuck in hot queues would be useful.\r\n\r\nWe also probably want to make requests that spend time in the queue automatically trigger the trace system; maybe instead we can adjust the top-level per-request line (which is V3) to also log the amount of time spent in queues.\r\n\r\nHere's an example use of the trace code: https://github.com/kubernetes/kubernetes/blob/a5e6ac0a959b059513c1e7908fbb0713467839c4/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/update.go#L48",
        "createdAt" : "2019-11-15T18:24:52Z",
        "updatedAt" : "2019-11-15T18:24:52Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "b123a43e7117e977606bacd31d77f4a30d2ed212",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +209,213 @@// client should not execute the request and afterExecution is\n// irrelevant.\nfunc (qs *queueSet) Wait(ctx context.Context, hashValue uint64, descr1, descr2 interface{}) (tryAnother, execute bool, afterExecution func()) {\n\tvar req *request\n\tdecision := func() requestDecision {"
  },
  {
    "id" : "22bf7da5-23b3-4d75-a4e5-3d79387a3284",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316714453",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0de02f9-96e8-48d6-beda-79152f615118",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I suggest making a Decision type for these, which can be a string or it could be an int if this doesn't need to be printed. (ideal is an int that implements the Stringer interface...)",
        "createdAt" : "2019-11-13T23:07:05Z",
        "updatedAt" : "2019-11-14T01:01:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "dab7037e-f617-4fa9-a2f7-411d5dd5964d",
        "parentId" : "b0de02f9-96e8-48d6-beda-79152f615118",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "OK, switching to `type decision int` in upcoming. PR.",
        "createdAt" : "2019-11-14T05:04:17Z",
        "updatedAt" : "2019-11-14T05:04:17Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 167,
    "diffHunk" : "@@ -1,1 +165,169 @@// Values passed through a request's Decision\nconst (\n\tDecisionExecute    = \"execute\"\n\tDecisionReject     = \"reject\"\n\tDecisionCancel     = \"cancel\""
  },
  {
    "id" : "76ca6f31-aebb-414a-99f0-a2c82b359802",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316714965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89a04b5b-57df-4d3d-8699-809481daac7e",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "nit: `case DecisionTryAnother, DecisionReject, DecisionCancel:`",
        "createdAt" : "2019-11-13T23:21:28Z",
        "updatedAt" : "2019-11-14T01:01:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "dd792108-abea-4a32-bda9-4a3227c97a78",
        "parentId" : "89a04b5b-57df-4d3d-8699-809481daac7e",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Fixing the appropriate two out of three in upcoming PR.",
        "createdAt" : "2019-11-14T05:06:44Z",
        "updatedAt" : "2019-11-14T05:06:44Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +288,292 @@\t}()\n\tswitch decision {\n\tcase DecisionTryAnother:\n\t\treturn true, false, func() {}\n\tcase DecisionReject:"
  },
  {
    "id" : "840cde2d-b918-4eae-afb6-93d312028ba5",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316717170",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d52bef8-b365-4268-a593-dff23e9acf9b",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Personally, I'd probably fail closed...",
        "createdAt" : "2019-11-13T23:22:52Z",
        "updatedAt" : "2019-11-14T01:01:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "0ccee3a7-79d4-40d2-a2c9-6d3043ccbc79",
        "parentId" : "6d52bef8-b365-4268-a593-dff23e9acf9b",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "For a bug in alpha of unproven quality and worth, I think it is right to fail open.  (I presume we are using valve terminology not circuit terminology.)",
        "createdAt" : "2019-11-14T03:59:51Z",
        "updatedAt" : "2019-11-14T03:59:52Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "b5b5da0e-fd2e-44b9-b949-9ef73cfff43d",
        "parentId" : "6d52bef8-b365-4268-a593-dff23e9acf9b",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Fair enough, and yes...",
        "createdAt" : "2019-11-14T05:16:54Z",
        "updatedAt" : "2019-11-14T05:16:55Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 259,
    "diffHunk" : "@@ -1,1 +257,261 @@\t\tdefault:\n\t\t\tklog.Errorf(\"QS(%s): Impossible decision %#+v (of type %T) for request %#+v %#+v\", qs.config.Name, decisionAny, decisionAny, descr1, descr2)\n\t\t\tdecisionStr = DecisionExecute\n\t\t}\n\t\tswitch decisionStr {"
  },
  {
    "id" : "c0cd00f4-f011-4f52-9347-9ba7248dd170",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316716379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83b26d32-7cde-4bcf-919e-3fa574529e25",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "The request parameter is ignored, should the function signature change to not return it?",
        "createdAt" : "2019-11-14T00:18:11Z",
        "updatedAt" : "2019-11-14T01:01:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "b3025131-1e52-41e5-9e0d-668127a118b4",
        "parentId" : "83b26d32-7cde-4bcf-919e-3fa574529e25",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Yes.  Fixing in an upcoming PR.",
        "createdAt" : "2019-11-14T05:12:59Z",
        "updatedAt" : "2019-11-14T05:12:59Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 505,
    "diffHunk" : "@@ -1,1 +503,507 @@\tmetrics.UpdateFlowControlRequestsExecuting(qs.config.Name, queue.RequestsExecuting)\n\trequest.Decision.SetLocked(DecisionExecute)\n\treturn request, ok\n}\n"
  },
  {
    "id" : "7f8b6660-12d9-47aa-814d-8c553324a63b",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316717571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2303130b-34ea-4416-8cf1-aad5fc262170",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "It occurs to me that there's no need to only remove the indexes at the end; you can remove them as they empty, the removal function already renumbers them if needed.",
        "createdAt" : "2019-11-14T00:20:29Z",
        "updatedAt" : "2019-11-14T01:01:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "9a9deed4-b97a-45f2-b83a-f25bed236a78",
        "parentId" : "2303130b-34ea-4416-8cf1-aad5fc262170",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Good idea!  Implementing in upcoming PR.",
        "createdAt" : "2019-11-14T05:18:46Z",
        "updatedAt" : "2019-11-14T05:18:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 564,
    "diffHunk" : "@@ -1,1 +562,566 @@\t// Logic to remove quiesced queues\n\t// >= as Index=25 is out of bounds for DesiredNum=25 [0...24]\n\tif r.Queue.Index >= qs.config.DesiredNumQueues &&\n\t\tlen(r.Queue.Requests) == 0 &&\n\t\tr.Queue.RequestsExecuting == 0 {"
  },
  {
    "id" : "3dcba8f5-4608-49ee-aca1-03bc26aa3cde",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316718932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d61fc400-8391-4cec-8ce5-1b389987a34b",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "It might be worthwhile to export a metric counting blocked goroutines later. Although, I guess what this actually counts is goroutines doing work.",
        "createdAt" : "2019-11-14T00:25:07Z",
        "updatedAt" : "2019-11-14T01:01:04Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "2ac5f46d-8c74-45cf-b8c7-03ead0cadab0",
        "parentId" : "d61fc400-8391-4cec-8ce5-1b389987a34b",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Yeah, this is not a bad kind of blocking (as in locking conflicts), it is normal operation.",
        "createdAt" : "2019-11-14T05:24:50Z",
        "updatedAt" : "2019-11-14T05:24:50Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 614,
    "diffHunk" : "@@ -1,1 +612,616 @@// goroutine associated with this queueSet or when such a goroutine is\n// about to wait on some other goroutine to do something; this is to\n// properly update the accounting used in testing.\nfunc (qs *queueSet) goroutineDoneOrBlocked() {\n\tqs.counter.Add(-1)"
  },
  {
    "id" : "f849e25b-6132-4045-b605-6ec85bef68aa",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316715236",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "896e5b94-8d9e-48ad-a002-b8316a0eff4a",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "nit: timesincelast -> timeSinceLast",
        "createdAt" : "2019-11-14T03:20:11Z",
        "updatedAt" : "2019-11-14T03:40:41Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "ffa75e71-0aa7-4989-bd76-2fe5190e6f5a",
        "parentId" : "896e5b94-8d9e-48ad-a002-b8316a0eff4a",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Fixing in upcoming PR.",
        "createdAt" : "2019-11-14T05:08:09Z",
        "updatedAt" : "2019-11-14T05:08:09Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 320,
    "diffHunk" : "@@ -1,1 +318,322 @@func (qs *queueSet) syncTimeLocked() {\n\trealNow := qs.clock.Now()\n\ttimesincelast := realNow.Sub(qs.lastRealTime).Seconds()\n\tqs.lastRealTime = realNow\n\tqs.virtualTime += timesincelast * qs.getVirtualTimeRatio()"
  },
  {
    "id" : "98a42baf-6650-49f0-8848-b531d98cb9b7",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316701953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ca3b9af-ae42-49ba-a0ca-37d12a00ab53",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Is it possible that qs.queues is empty ?\r\nIn that case -1 would be returned, leading to access error on the next line.",
        "createdAt" : "2019-11-14T03:23:37Z",
        "updatedAt" : "2019-11-14T03:40:41Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "e1d4d10b-8b4c-465c-bad8-cdc6b9c30e6e",
        "parentId" : "4ca3b9af-ae42-49ba-a0ca-37d12a00ab53",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "That is not yet possible.  But when #84771 lands we will need some revisions here, to simply and clearly avoid queuing for a priority level that does limiting but not queuing.",
        "createdAt" : "2019-11-14T04:03:26Z",
        "updatedAt" : "2019-11-14T04:03:26Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 354,
    "diffHunk" : "@@ -1,1 +352,356 @@func (qs *queueSet) timeoutOldRequestsAndRejectOrEnqueueLocked(hashValue uint64, descr1, descr2 interface{}) *request {\n\t//\tStart with the shuffle sharding, to pick a queue.\n\tqueueIdx := qs.chooseQueueIndexLocked(hashValue, descr1, descr2)\n\tqueue := qs.queues[queueIdx]\n\t// The next step is the logic to reject requests that have been waiting too long"
  },
  {
    "id" : "497800b2-30ad-4850-945b-a70017bf7bfd",
    "prId" : 85192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85192#pullrequestreview-316718472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3484583f-95ca-4729-92a0-c89aa40a6d48",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "why negate r.Queue.Index ?",
        "createdAt" : "2019-11-14T03:36:13Z",
        "updatedAt" : "2019-11-14T03:40:41Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "b0c40bae-fa20-4ce1-874f-2035cda75ee6",
        "parentId" : "3484583f-95ca-4729-92a0-c89aa40a6d48",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Oh crud, is that typo still here?!",
        "createdAt" : "2019-11-14T04:03:54Z",
        "updatedAt" : "2019-11-14T04:03:55Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "391f4606-afd6-454c-9b29-6633106730e8",
        "parentId" : "3484583f-95ca-4729-92a0-c89aa40a6d48",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Created #85257 to address this.",
        "createdAt" : "2019-11-14T04:15:36Z",
        "updatedAt" : "2019-11-14T04:15:36Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "a838905c-506c-4073-b4f0-ab0a6b72d891",
        "parentId" : "3484583f-95ca-4729-92a0-c89aa40a6d48",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "I LGTMd that, and am preparing another PR to fix all the other issues here.",
        "createdAt" : "2019-11-14T05:22:51Z",
        "updatedAt" : "2019-11-14T05:22:52Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e10acc78dee5d90d93fc7bc0e76e97bc7bc0b3a3",
    "line" : 571,
    "diffHunk" : "@@ -1,1 +569,573 @@\t\t// decrement here to maintain the invariant that (qs.robinIndex+1) % numQueues\n\t\t// is the index of the next queue after the one last dispatched from\n\t\tif qs.robinIndex >= -r.Queue.Index {\n\t\t\tqs.robinIndex--\n\t\t}"
  },
  {
    "id" : "2b61d88a-7e1c-4da9-8e74-956fbed71c0a",
    "prId" : 103664,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/103664#pullrequestreview-706909637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f20ca69-9f35-4978-8a98-da24ddb4aee4",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "This change is not necessary, the reason this PR is here is because width means seats.\r\nThis change is not a problem.",
        "createdAt" : "2021-07-15T04:17:51Z",
        "updatedAt" : "2021-07-15T04:19:51Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "c79a0a08828ef86e07941e96dafefb7d8bc51990",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +477,481 @@\tqs.dealer.Deal(hashValue, func(queueIdx int) {\n\t\t// TODO: Consider taking into account `additional latency` of requests\n\t\t// in addition to their seats.\n\t\t// Ideally, this should be based on projected completion time in the\n\t\t// virtual world of the youngest request in the queue."
  }
]