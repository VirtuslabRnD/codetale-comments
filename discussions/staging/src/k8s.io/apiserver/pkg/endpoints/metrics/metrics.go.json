[
  {
    "id" : "d4da0442-421b-4f6e-9c45-7293faba7c23",
    "prId" : 95207,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95207#pullrequestreview-520426125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bdadc73-5dc7-4a95-95bb-43fd030dd89f",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Having a 0.0 bucket doesn't make much sense - we should switch that to 0.0001 or sth like that.\r\n@tkashem ",
        "createdAt" : "2020-10-30T07:28:00Z",
        "updatedAt" : "2020-10-30T07:28:00Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "62431047b46c5949eaf592825712098d7280c4c0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +201,205 @@\t\t\tName:           \"apiserver_request_filter_duration_seconds\",\n\t\t\tHelp:           \"Request filter latency distribution in seconds, for each filter type\",\n\t\t\tBuckets:        []float64{0.0, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 5.0},\n\t\t\tStabilityLevel: compbasemetrics.ALPHA,\n\t\t},"
  },
  {
    "id" : "2a4038b3-0331-4daf-9287-77e505f1a56f",
    "prId" : 89451,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89451#pullrequestreview-381353473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89bb448c-6fdf-47f6-9602-e8121a3ae3e8",
        "parentId" : null,
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "If any of the metric types are unknown, should we log the true value, so we know if there's a type that should be added to the list? Same to other labels.",
        "createdAt" : "2020-03-25T01:40:48Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "f71d441c-cc3d-4d08-a199-a740662e26fc",
        "parentId" : "89bb448c-6fdf-47f6-9602-e8121a3ae3e8",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "All requests are already logged in audit logs, so we shouldn't be losing any data.",
        "createdAt" : "2020-03-25T01:47:21Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      },
      {
        "id" : "5c8bd320-1db6-4cda-a795-14d463be3b07",
        "parentId" : "89bb448c-6fdf-47f6-9602-e8121a3ae3e8",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "I don't think we record content type in the audit logs though?",
        "createdAt" : "2020-03-25T16:39:35Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "0b96d585-9c95-4c84-bbf8-d202827ac0d1",
        "parentId" : "89bb448c-6fdf-47f6-9602-e8121a3ae3e8",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "If it's not there, then I'm pretty sure it's in the kube-apiserver logs when verbosity=3 (enabling request logging).",
        "createdAt" : "2020-03-25T17:03:17Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      }
    ],
    "commit" : "1eb6f5240a3756684896ec67bc53ae54ec0aa163",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +359,363 @@\t\treturn normalizedContentType\n\t}\n\treturn OtherContentType\n}\n"
  },
  {
    "id" : "009eb923-9ceb-4c5e-a43a-13121154b49b",
    "prId" : 89451,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89451#pullrequestreview-384882192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3512422-043b-4c6f-bee8-549a57e0acd1",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "is there a reason to report content types other than what we serve?",
        "createdAt" : "2020-03-31T13:45:41Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "7a378c64-1f72-4dd8-85f7-59854d552ba8",
        "parentId" : "e3512422-043b-4c6f-bee8-549a57e0acd1",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "In general, out of a sense of caution about what people may expect.",
        "createdAt" : "2020-03-31T16:02:44Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      }
    ],
    "commit" : "1eb6f5240a3756684896ec67bc53ae54ec0aa163",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +177,181 @@\t// these are the known (e.g. whitelisted/known) content types which we will report for\n\t// request metrics. Any other RFC compliant content types will be aggregated under 'unknown'\n\tknownMetricContentTypes = utilsets.NewString(\n\t\t\"application/apply-patch+yaml\",\n\t\t\"application/json\","
  },
  {
    "id" : "fd3b6683-245e-4ea4-b332-1b2bd666b207",
    "prId" : 89451,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89451#pullrequestreview-413925642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a812b3b-af5e-4f19-b1bb-f56d3f4786f5",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "This seems really expensive for a path that thought it necessary to optimize status code conversion with `codeToString`\r\n\r\nWould it be sufficient to:\r\n1. identify the small number of content-types the API server expects to serve (with no charset)\r\n2. prefix-compare the incoming content-type case-insensitively with those\r\n3. record an unknown content-type if none match\r\n\r\nThe content-type metrics seem useful for judging ratio of yaml to json to proto requests, etc, and this would still allow that",
        "createdAt" : "2020-03-31T16:35:07Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ff468a1e-fe38-4a5f-89f1-421993e5d087",
        "parentId" : "0a812b3b-af5e-4f19-b1bb-f56d3f4786f5",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I can't remember why exactly codeToString was added (was it @smarterclayton who did that?). But I think it might have been because of fmt.Sprintf() which is really expensive. And seems we also have fmt.Sprintf() here, so I agree we may hit the same issues we had 4 years ago (or whenever it was changed).\r\n\r\nI agree with Jordan that the current list of 40 supported content types seems strange. I have never heard of many things in this list. And I don't think we will get anything actionable from here. Does it really matter if it was \"font/woff\" or \"font/woff2\"? [whatever those really are...]",
        "createdAt" : "2020-03-31T18:48:57Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "d1288de1-83a8-4320-ad0d-527fe7aad1db",
        "parentId" : "0a812b3b-af5e-4f19-b1bb-f56d3f4786f5",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "@tallclair WDYT? I'm ambivalent either way.",
        "createdAt" : "2020-03-31T19:42:39Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      },
      {
        "id" : "1c63f511-a617-4b47-bc33-d3caaea7d8c2",
        "parentId" : "0a812b3b-af5e-4f19-b1bb-f56d3f4786f5",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "let's bound this to known mime-types",
        "createdAt" : "2020-05-14T14:09:49Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "cd4bacc2-e48f-48ba-9224-163c709080fc",
        "parentId" : "0a812b3b-af5e-4f19-b1bb-f56d3f4786f5",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "We can memoize the function if performance is the concern, then the call to`fmt.Sprintf` will only happen once per unique content-type string.\r\n\r\nDo you have a list of the known and acceptable mime-types? I am quite happy to constrain this list.",
        "createdAt" : "2020-05-18T20:43:45Z",
        "updatedAt" : "2020-05-28T15:22:29Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      }
    ],
    "commit" : "1eb6f5240a3756684896ec67bc53ae54ec0aa163",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +351,355 @@// cleanContentType binds the contentType (for metrics related purposes) to a\n// bounded set of known/expected content-types.\nfunc cleanContentType(contentType string) string {\n\tnormalizedContentType := strings.ToLower(contentType)\n\tif strings.HasSuffix(contentType, \" stream=watch\") || strings.HasSuffix(contentType, \" charset=utf-8\") {"
  },
  {
    "id" : "6251de0b-79ef-4e96-9dd0-9a05e6884a0a",
    "prId" : 87673,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87673#pullrequestreview-351182837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25cd0926-66c1-449d-b322-71f2dcca3ef9",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I'm happy approving this but an alternative is to fix the cleanUserAgent method such that it can only return N options (e.g., Browser, kubectl, unknown).",
        "createdAt" : "2020-01-30T17:41:38Z",
        "updatedAt" : "2020-01-30T17:41:38Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "1ebc95fb-e058-44e2-912b-fa43a46c160d",
        "parentId" : "25cd0926-66c1-449d-b322-71f2dcca3ef9",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Is it better or worse to not add this label at all, rather than add it with a dummy (empty string) value?\r\n\r\n---\r\n\r\nAlternative approach: Hash it to one of an acceptable number of values (100?) - with a non-secret hash function.  You can't work out what the original client value was, but the metric would still give you _some_ breakdown of query by client.\r\n\r\nNote if we do some sort of cleaning, another useful option is to aggressively truncate the value.  The current code (by experiment on an older cluster) preserves slightly over 1MB of characters here, which is extremely excessive.  100 characters probably preserves all the relevant useful information here.",
        "createdAt" : "2020-01-30T22:14:20Z",
        "updatedAt" : "2020-01-30T22:14:21Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "03b92f13-8e8f-4688-ba7f-7e155fbf4e06",
        "parentId" : "25cd0926-66c1-449d-b322-71f2dcca3ef9",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yup, that'd be cool too, but from talking with Han it seems like this isn't a useful metric anyway.",
        "createdAt" : "2020-01-30T22:20:16Z",
        "updatedAt" : "2020-01-30T22:20:17Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "0034eca30fd4f1d65ed6ad5a6d855f5f300ee43a",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +301,305 @@\treportedVerb := cleanVerb(verb, req)\n\tdryRun := cleanDryRun(req.URL)\n\t// blank out client string here, in order to avoid cardinality issues\n\tclient := \"\"\n\telapsedMicroseconds := float64(elapsed / time.Microsecond)"
  },
  {
    "id" : "5f211745-4d07-47a4-886c-f44d0dd9c223",
    "prId" : 87669,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87669#pullrequestreview-350433076",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00bc9f62-946f-4c88-97fd-42c7dc95fe5f",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@mm4tt - can you please validate if it won't break anything in scalability tests?",
        "createdAt" : "2020-01-29T21:22:27Z",
        "updatedAt" : "2020-01-29T21:32:28Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "75cf4d79f2c52a122d786812eaaed8557e928552",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +72,76 @@\t\t// But changing it would break backwards compatibility. Future label_names\n\t\t// should be all lowercase and separated by underscores.\n\t\t[]string{\"verb\", \"dry_run\", \"group\", \"version\", \"resource\", \"subresource\", \"scope\", \"component\", \"contentType\", \"code\"},\n\t)\n\tlongRunningRequestGauge = compbasemetrics.NewGaugeVec("
  },
  {
    "id" : "a519997b-c571-441e-a931-e08ec7353f76",
    "prId" : 81531,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81531#pullrequestreview-280451611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "70b8082d-8a18-4ad6-8f8b-2455d6fd7511",
        "parentId" : null,
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "Would it make sense to abstract this so that we can have an abstraction between us and the metric system everywhere except cmd?",
        "createdAt" : "2019-08-27T02:49:22Z",
        "updatedAt" : "2019-08-27T19:47:26Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "4b0ea004-0992-4690-a52c-d0558f57701f",
        "parentId" : "70b8082d-8a18-4ad6-8f8b-2455d6fd7511",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "If we intend to move off of Prometheus, it would certainly make sense to do so. There are some other Prometheus references which we did not remove for the initial migration (ie custom Prometheus collectors).",
        "createdAt" : "2019-08-27T03:00:41Z",
        "updatedAt" : "2019-08-27T19:47:26Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      },
      {
        "id" : "24a7196e-a9fd-4b2a-a05a-9ec1075cedcf",
        "parentId" : "70b8082d-8a18-4ad6-8f8b-2455d6fd7511",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "I suppose there could be other reasons as well, but it doesn’t seem strictly necessary for the immediate task at hand.",
        "createdAt" : "2019-08-27T03:01:31Z",
        "updatedAt" : "2019-08-27T19:47:26Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      },
      {
        "id" : "4b53601e-8e26-49fe-991c-320951caa17e",
        "parentId" : "70b8082d-8a18-4ad6-8f8b-2455d6fd7511",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "Were you thinking otherwise?",
        "createdAt" : "2019-08-27T03:03:04Z",
        "updatedAt" : "2019-08-27T19:47:26Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      },
      {
        "id" : "8ff3c104-e760-4b59-b654-924cc44d961a",
        "parentId" : "70b8082d-8a18-4ad6-8f8b-2455d6fd7511",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "It just seems we are getting close to having a better abstraction between our libraries and direct prometheus. It does not have to happen in this PR (probably should be a separate PR) but it would be nice to break the hard dependency.",
        "createdAt" : "2019-08-27T20:20:29Z",
        "updatedAt" : "2019-08-27T20:20:29Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "a04d996b-fcea-4c37-88cc-f22b30c8ffd4",
        "parentId" : "70b8082d-8a18-4ad6-8f8b-2455d6fd7511",
        "authorId" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "body" : "Yeah I agree with the general sentiment. ",
        "createdAt" : "2019-08-27T20:26:08Z",
        "updatedAt" : "2019-08-27T20:26:09Z",
        "lastEditedBy" : "09e31512-b413-47a1-bc83-20b5a77064ee",
        "tags" : [
        ]
      }
    ],
    "commit" : "1700a315c188e4b9d434d1e51de75426aa9d7867",
    "line" : 134,
    "diffHunk" : "@@ -1,1 +131,135 @@\t\t\tHelp: \"Response size distribution in bytes for each group, version, verb, resource, subresource, scope and component.\",\n\t\t\t// Use buckets ranging from 1000 bytes (1KB) to 10^9 bytes (1GB).\n\t\t\tBuckets:        prometheus.ExponentialBuckets(1000, 10.0, 7),\n\t\t\tStabilityLevel: compbasemetrics.ALPHA,\n\t\t},"
  },
  {
    "id" : "bab9b6f4-cdcc-425d-80c8-edaf0ebcd85d",
    "prId" : 74997,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74997#pullrequestreview-210950177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44ccca39-5906-4112-a1bd-afa8511a1f3f",
        "parentId" : null,
        "authorId" : "d4f34d8f-5341-4ac1-b8b2-5e5f11e23a5d",
        "body" : "If I was being malicious, could I pass in a ton of different dryRun values here and explode the carnality of the metrics? Maybe validate dryRun to constrain it to an allowed set of values?",
        "createdAt" : "2019-03-05T22:17:49Z",
        "updatedAt" : "2019-03-07T23:15:31Z",
        "lastEditedBy" : "d4f34d8f-5341-4ac1-b8b2-5e5f11e23a5d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e512eb8758fb32878d7baa33daf3c49561086c0",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +235,239 @@func MonitorRequest(req *http.Request, verb, group, version, resource, subresource, scope, component, contentType string, httpCode, respSize int, elapsed time.Duration) {\n\treportedVerb := cleanVerb(verb, req)\n\tdryRun := cleanDryRun(req.URL.Query()[\"dryRun\"])\n\tclient := cleanUserAgent(utilnet.GetHTTPClient(req))\n\telapsedMicroseconds := float64(elapsed / time.Microsecond)"
  },
  {
    "id" : "4ea50cb2-638e-4d81-bd29-23c68164b8c5",
    "prId" : 74997,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74997#pullrequestreview-212065997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73f4b66b-e01b-4425-863e-a096530cefd6",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "`APPLY` isn't an API server verb, and we're recording content type as well... why do we want to synthesize a verb here?\r\n\r\nedit: the contentType we record isn't the incoming content type... hmm",
        "createdAt" : "2019-03-07T21:47:02Z",
        "updatedAt" : "2019-03-07T23:15:31Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ec11262d-c329-4f59-9b6b-411a5315471b",
        "parentId" : "73f4b66b-e01b-4425-863e-a096530cefd6",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I'm not a huge fan of munging this into the verb, but I am also hesitant to add another label to such a high volume metric. Guard this with the apply feature gate for now, and let's add this to the things to think about before beta",
        "createdAt" : "2019-03-07T22:56:49Z",
        "updatedAt" : "2019-03-07T23:15:31Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e512eb8758fb32878d7baa33daf3c49561086c0",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +327,331 @@\t}\n\tif verb == \"PATCH\" && request.Header.Get(\"Content-Type\") == string(types.ApplyPatchType) && utilfeature.DefaultFeatureGate.Enabled(features.ServerSideApply) {\n\t\treportedVerb = \"APPLY\"\n\t}\n\treturn reportedVerb"
  },
  {
    "id" : "b6cfb2e7-d90e-4fb4-b39a-6d1d7a4b070f",
    "prId" : 73638,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73638#pullrequestreview-199738064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79dced6f-e7ae-4dab-8843-a050979d9a6f",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "We can do that now, since this metric was added in this release.\r\n\r\nJust wanted to clarify one more thing - how I can get histogram of values from say last 5 minutes?",
        "createdAt" : "2019-02-01T15:01:02Z",
        "updatedAt" : "2019-02-04T09:49:53Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "e292e038-9206-45c9-b9a2-bbc7f8f909ce",
        "parentId" : "79dced6f-e7ae-4dab-8843-a050979d9a6f",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "To get the 99th percentile over the past 5min of data, the query in prometheus would be: \r\n\r\nhistogram_quantile(0.99, rate(apiserver_request_latency_seconds_bucket[5m]))",
        "createdAt" : "2019-02-04T09:59:03Z",
        "updatedAt" : "2019-02-04T09:59:03Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "6a13289b-fc7b-4604-8603-bfb5c347ae9c",
        "parentId" : "79dced6f-e7ae-4dab-8843-a050979d9a6f",
        "authorId" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "body" : "@brancz `histogram_quantile(0.99, rate(apiserver_request_latency_seconds_bucket[5m]))` is something that can be calculated by the server, right? The metric endpoint that was discussed here provides only values for the buckets, not a time series?",
        "createdAt" : "2019-02-04T14:38:25Z",
        "updatedAt" : "2019-02-04T14:38:25Z",
        "lastEditedBy" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "tags" : [
        ]
      },
      {
        "id" : "8e636256-33ef-474c-b49b-1937a22f44f9",
        "parentId" : "79dced6f-e7ae-4dab-8843-a050979d9a6f",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "It is calculated by the server, correct. Each bucket is a counter, so each bucket is 1 time-series ingested by Prometheus.",
        "createdAt" : "2019-02-04T18:31:34Z",
        "updatedAt" : "2019-02-04T18:31:35Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0508c7e872f60826d68c58c458cfd865554b486",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +73,77 @@\t\t\tName: \"apiserver_request_latency_seconds\",\n\t\t\tHelp: \"Response latency distribution in seconds for each verb, group, version, resource, subresource, scope and component.\",\n\t\t\t// This metric is used for verifying api call latencies SLO,\n\t\t\t// as well as tracking regressions in this aspects.\n\t\t\t// Thus we customize buckets significantly, to empower both usecases."
  },
  {
    "id" : "9dcc0af7-c994-49db-a755-9f791eeb8a7e",
    "prId" : 72336,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72336#pullrequestreview-199157952",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@danielqsj - this metric was deprecated without adding anything new instead.\r\nWas that intentional or just overlooked?",
        "createdAt" : "2019-01-30T12:00:42Z",
        "updatedAt" : "2019-01-30T12:00:42Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "8e480cab-be56-465b-815b-18cf0f9b96fd",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "The intention was that we felt the latency metrics as summaries are too expensive and not aggregatable across instances so just having the histograms should be sufficient. If there are other voices I'm happy to reconsider. As I mentioned above, I'm aware that scalability tests currently use this metric (or at least were), I recall us having had a discussion a year or so ago that the scalability tests should be moving away from using these (my apologies if I'm mixing up conversations, and this wasn't with you).",
        "createdAt" : "2019-01-30T21:15:56Z",
        "updatedAt" : "2019-01-30T21:15:56Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "0bf56878-c66d-47cb-852d-166c77ce28a8",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "The context is that this PR broke the tests, so we temporarily reverted to using the old metric: https://github.com/kubernetes/kubernetes/pull/73524\r\n\r\nI think I roughly understand the concerns, but I would like to discuss what exactly we are trying to achieve and how we can do that without having a summary.\r\nSo the requirements are:\r\n1. We need to be able to say whether 99th percentile was higher or lower than X (X being 1s, 5s or 30s depending on the calls - the exact SLO is described here: https://github.com/kubernetes/community/blob/master/sig-scalability/slos/api_call_latency.md)\r\n2. As there is defined in SLI, we would like to have the value from last 5 minutes.\r\n3. I would like to be able to spot regressions (at the very least by looking post factum into graphs)\r\n4. Aggregating across few masters is probably also an important thing.\r\n\r\n@brancz - are those reasonable expectations?\r\n\r\nIIUC, the first one we can achieve by appropriately choosing buckets (if we would have buckets that finish at 1s, 5s and 30s, we would be done).\r\nThe third one is probably also doable - we will just show on the graph % of requests handled for a given buckets - if that is growing, then it's probably a regression.\r\nThe question is: what about number (2). Do you have any recommendations?\r\n\r\nAny other thoughts?\r\n\r\n",
        "createdAt" : "2019-01-31T07:47:04Z",
        "updatedAt" : "2019-01-31T07:47:04Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "088d60bc-9f10-42fe-b21f-bc075fa4a785",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "As these are Prometheus metrics, assuming you're collecting these metrics with Prometheus, I see no problem at all for all of the points above:\r\n\r\n1) exactly\r\n2) we can see historic 99th latency with:\r\n\r\n```\r\nhistogram_quantile(0.99, sum without(instance, pod) (rate(apiserver_request_latencies_bucket{job=\"apiserver\"}[5m])))\r\n```\r\n\r\n> This assumes the prometheus config you are using has the `job=\"apiserver\"`, `pod`, and `instance` labels. If you choose to have a different labelling, then those may vary, but the idea is the same.\r\n\r\n3) the above calculates this over time\r\n4) the above already does the latency aggregated over all instances (with today's summaries that's fundamentally not possible which is why we preferred the histogram :) )\r\n\r\nThe only thing that's different right now is that the buckets do not go up to 30s, they go up to 8s. If you have data to back up relevance up to 30s, that would be wonderful and I'd be more than happy to change the bucketing according to real world experience from the scalability tests! :)",
        "createdAt" : "2019-01-31T15:17:32Z",
        "updatedAt" : "2019-01-31T15:17:32Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "d662eb02-201a-4e05-ae8c-66f7b153414f",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@brancz - thanks\r\n\r\nI have more question about buckets. Can we customize buckets more?\r\nIn particular, there is huge difference for us between say 500ms and 900ms (while this is the same bucket).\r\nWhat I would ideally like to see is probably sth like:\r\n- every 100ms (or even a bit more) up to 1s\r\n- then every 500ms up to 5s\r\n- then maybe every 5s or so up to 30s\r\n\r\nRegaring 30s - yes in large clusters having 150k pods, listing all pods may easily take more than 10s.",
        "createdAt" : "2019-01-31T19:40:42Z",
        "updatedAt" : "2019-01-31T19:40:42Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "d8be15fa-808b-4394-b08d-0bded4827a87",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "Buckets are completely free form. Linear, log, or exponential buckets are just standards that are available to be used more easily. If we have a better idea of buckets we want to have, that's absolutely possible, and we should do it.\r\n\r\nThe godoc comment is literally:\r\n\r\n```\r\n\t// Buckets defines the buckets into which observations are counted. Each\r\n\t// element in the slice is the upper inclusive bound of a bucket. The\r\n\t// values must be sorted in strictly increasing order. There is no need\r\n\t// to add a highest bucket with +Inf bound, it will be added\r\n\t// implicitly. The default value is DefBuckets.\r\n\tBuckets []float64\r\n```",
        "createdAt" : "2019-01-31T20:02:55Z",
        "updatedAt" : "2019-01-31T20:08:58Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "ab628143-8c9f-4ec4-b559-53008cd88112",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@brancz - thanks\r\nLet me think a bit more and I will send you a PR tomorrow or early next week that will change buckets in that metric (given that we added it in this release, we can still do that) so that they will be more useful for us.\r\n\r\nProbably last question in this area - what is the reasonable limit for number of buckets? For example, would 50 buckets or so still be ok?",
        "createdAt" : "2019-01-31T21:26:39Z",
        "updatedAt" : "2019-01-31T21:26:39Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "f67428a7-131a-4326-a36f-96e8a9b7de96",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Anyway - I opened https://github.com/kubernetes/kubernetes/pull/73638 for that.",
        "createdAt" : "2019-02-01T15:01:38Z",
        "updatedAt" : "2019-02-01T15:01:38Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "25fda797-7044-4437-8617-2688b76ed558",
        "parentId" : "36b9e8b1-9ffe-4144-a623-bb408acbde35",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "Looking at the cardinality we’re looking at creating roughly 300 histograms and with 50 buckets that’s 15k metrics. That’s a lot, but I feel this is important enough of a metric for it to be worth it.",
        "createdAt" : "2019-02-01T16:33:00Z",
        "updatedAt" : "2019-02-01T16:33:00Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b418631c08cf66ee57c9ec31fe95372a0a3e075",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +87,91 @@\t\t[]string{\"verb\", \"group\", \"version\", \"resource\", \"subresource\", \"scope\", \"component\"},\n\t)\n\tdeprecatedRequestLatenciesSummary = prometheus.NewSummaryVec(\n\t\tprometheus.SummaryOpts{\n\t\t\tName: \"apiserver_request_latencies_summary\","
  },
  {
    "id" : "2b824c62-10c4-4ab9-b055-8c808f1fc88e",
    "prId" : 58342,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58342#pullrequestreview-91977586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbf6d8eb-f07a-4f25-a71a-7c7b47a362ec",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "This metric seems like it belongs in the filter package, not here.  We shouldn't have to register metrics in this package.  This is a weird coupling.",
        "createdAt" : "2018-01-26T21:41:01Z",
        "updatedAt" : "2018-01-26T21:41:02Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "000d7bac29b9239a29531a526d382394d8d60353",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +117,121 @@}\n\nfunc UpdateInflightRequestMetrics(nonmutating, mutating int) {\n\tcurrentInflightRequests.WithLabelValues(ReadOnlyKind).Set(float64(nonmutating))\n\tcurrentInflightRequests.WithLabelValues(MutatingKind).Set(float64(mutating))"
  },
  {
    "id" : "5f0bc1fa-04bb-494d-9912-1e18a492f89e",
    "prId" : 58320,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58320#pullrequestreview-89384809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c50af77-dbc4-4661-bf4d-c9f3dfc1af43",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "not a fan of importing the metrics package autoregistering",
        "createdAt" : "2018-01-16T16:59:07Z",
        "updatedAt" : "2018-01-16T16:59:07Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "bd9e9e75-b8c6-408e-9d2a-9f8a05512d76",
        "parentId" : "0c50af77-dbc4-4661-bf4d-c9f3dfc1af43",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "Previously, autoregistering by importing `k8s.io/apiserver/pkg/endpoints`",
        "createdAt" : "2018-01-17T01:17:20Z",
        "updatedAt" : "2018-01-17T01:17:20Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      },
      {
        "id" : "3417a7fb-122a-4496-9505-418a88067e6c",
        "parentId" : "0c50af77-dbc4-4661-bf4d-c9f3dfc1af43",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Is there an easy way to lazily register when metrics are actually used?",
        "createdAt" : "2018-01-17T08:09:39Z",
        "updatedAt" : "2018-01-17T08:09:39Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "ae3838d9-a28a-45f8-bdba-25af4c57eb01",
        "parentId" : "0c50af77-dbc4-4661-bf4d-c9f3dfc1af43",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "Yes, we can implement lazy register by registering in `Record` and `RecordLongRunning`. But why we need  `lazily register` ?  ",
        "createdAt" : "2018-01-17T08:35:05Z",
        "updatedAt" : "2018-01-17T08:35:28Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      },
      {
        "id" : "ea2d3656-27de-458c-92b8-296d015df52a",
        "parentId" : "0c50af77-dbc4-4661-bf4d-c9f3dfc1af43",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "I agree. pkg/endpoints/metrics is pretty deep in the apiserver. Anybody importing that outside of k8s.io/apiserver?",
        "createdAt" : "2018-01-17T09:50:06Z",
        "updatedAt" : "2018-01-17T09:50:06Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "8717fae5-5b5a-4919-b5aa-000bca860010",
        "parentId" : "0c50af77-dbc4-4661-bf4d-c9f3dfc1af43",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "I donot know.  Maybe user private extended apiservers import it.",
        "createdAt" : "2018-01-17T10:06:35Z",
        "updatedAt" : "2018-01-17T10:06:35Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      },
      {
        "id" : "c2d1c5cc-a7eb-4420-a759-70b859bf8d68",
        "parentId" : "0c50af77-dbc4-4661-bf4d-c9f3dfc1af43",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "But these are not direct import.",
        "createdAt" : "2018-01-17T10:07:16Z",
        "updatedAt" : "2018-01-17T10:07:26Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "631119a7d65e01e48b5d8a46d7300b20c65262e1",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +82,86 @@)\n\nfunc init() {\n\t// Register all metrics.\n\tprometheus.MustRegister(requestCounter)"
  },
  {
    "id" : "6cb16644-1f74-420e-955f-9422725308fe",
    "prId" : 52237,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/52237#pullrequestreview-61774783",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d10aeb21-c193-4b5d-8904-0d7fb1a5d486",
        "parentId" : null,
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "This metric count is already unbounded due to client label, adding scope will now triple them. Discussion with @wojtek-t about this earlier - https://github.com/kubernetes/kubernetes/pull/49117#issuecomment-316702136\r\nDo we still want to do this?",
        "createdAt" : "2017-09-10T20:03:38Z",
        "updatedAt" : "2017-09-12T02:14:19Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "dbbbd140-b1d0-4267-86a2-51d53ce5505c",
        "parentId" : "d10aeb21-c193-4b5d-8904-0d7fb1a5d486",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Only if the client actually invokes all three scopes.  Which is rarely the case.\r\n\r\nI checked on our largest cluster (20k+ unique users and namespaces, several hundred nodes, 15k unique services/apps).  I think this fairly high cardinality generating, but much smaller than the other metrics.  Today:\r\n\r\n```\r\n# sort_desc(count by (__name__)({__name__=~\"apiserver_.*\"}))\r\napiserver_request_latencies_bucket{} | 6184\r\napiserver_request_count{} | 4501\r\napiserver_request_latencies_summary{} | 2319\r\napiserver_request_latencies_summary_count{} | 773\r\napiserver_request_latencies_count{} | 773\r\napiserver_request_latencies_summary_sum{} | 773\r\napiserver_request_latencies_sum{} | 773\r\n```\r\n\r\nThe highest api server metric cardinality is about 60 items down the list:\r\n\r\n```\r\ncontainer_tasks_state{} | 229635\r\ncontainer_cpu_usage_seconds_total{} | 196684\r\ncontainer_memory_failures_total{} | 183708\r\ncontainer_fs_writes_total{} | 159961\r\ncontainer_fs_reads_total{} | 159961\r\ncontainer_fs_writes_bytes_total{} | 142592\r\ncontainer_fs_reads_bytes_total{} | 142592\r\ncontainer_memory_cache{} | 45927\r\ncontainer_memory_rss{} | 45927\r\ncontainer_memory_working_set_bytes{} | 45927\r\ncontainer_start_time_seconds{} | 45927\r\ncontainer_memory_failcnt{} | 45927\r\ncontainer_cpu_system_seconds_total{} | 45927\r\ncontainer_memory_usage_bytes{} | 45927\r\ncontainer_last_seen{} | 45927\r\ncontainer_cpu_user_seconds_total{} | 45927\r\ncontainer_memory_swap{} | 45927\r\ncontainer_spec_cpu_shares{} | 45873\r\ncontainer_spec_cpu_period{} | 45873\r\ncontainer_spec_memory_swap_limit_bytes{} | 45732\r\ncontainer_spec_memory_limit_bytes{} | 45732\r\ncontainer_fs_sector_reads_total{} | 18164\r\ncontainer_fs_write_seconds_total{} | 18164\r\ncontainer_fs_inodes_free{} | 18164\r\ncontainer_fs_io_current{} | 18164\r\ncontainer_fs_io_time_weighted_seconds_total{} | 18164\r\ncontainer_fs_reads_merged_total{} | 18164\r\ncontainer_fs_writes_merged_total{} | 18164\r\ncontainer_fs_inodes_total{} | 18164\r\ncontainer_fs_read_seconds_total{} | 18164\r\ncontainer_fs_sector_writes_total{} | 18164\r\ncontainer_fs_io_time_seconds_total{} | 18164\r\ncontainer_fs_usage_bytes{} | 18164\r\ncontainer_fs_limit_bytes{} | 18164\r\nkubelet_runtime_operations_latency_microseconds{} | 13146\r\ncontainer_network_transmit_bytes_total{} | 11457\r\ncontainer_network_transmit_errors_total{} | 11457\r\ncontainer_network_transmit_packets_total{} | 11457\r\ncontainer_network_transmit_packets_dropped_total{} | 11457\r\ncontainer_network_receive_bytes_total{} | 11457\r\ncontainer_network_receive_packets_dropped_total{} | 11457\r\ncontainer_network_receive_errors_total{} | 11457\r\ncontainer_network_receive_packets_total{} | 11457\r\nkubelet_docker_operations_latency_microseconds{} | 8649\r\ncontainer_cpu_cfs_throttled_periods_total{} | 8245\r\ncontainer_spec_cpu_quota{} | 8245\r\ncontainer_cpu_cfs_throttled_seconds_total{} | 8245\r\ncontainer_cpu_cfs_periods_total{} | 8245\r\nnode_dns_response_size_bytes_bucket{} | 6622\r\n```\r\n\r\nTotal samples scraped\r\n\r\n```\r\n# sum by (job) (scrape_samples_scraped)\r\n{job=\"kubernetes-nodes\"} | 2863747\r\n{job=\"kubernetes-apiservers\"} | 17074\r\n```\r\n\r\nSo yes, it will increase cardinality.  But I think this cardinality is more useful than client cardinality, and in practice we should we be better off limiting that unbounded cardinality than not gathering this metric.  ",
        "createdAt" : "2017-09-10T20:38:48Z",
        "updatedAt" : "2017-09-12T02:14:19Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "140dc6c6-4873-4be1-a93a-4342fb2605c9",
        "parentId" : "d10aeb21-c193-4b5d-8904-0d7fb1a5d486",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "That sounds reasonable.. thanks for the info.",
        "createdAt" : "2017-09-11T10:21:49Z",
        "updatedAt" : "2017-09-12T02:14:19Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      }
    ],
    "commit" : "30a92a8f0a7deb9b54795661d8d6d84234a1089e",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +86,90 @@func Monitor(verb, resource, subresource, scope, client, contentType string, httpCode, respSize int, reqStart time.Time) {\n\telapsed := float64((time.Since(reqStart)) / time.Microsecond)\n\trequestCounter.WithLabelValues(verb, resource, subresource, scope, client, contentType, codeToString(httpCode)).Inc()\n\trequestLatencies.WithLabelValues(verb, resource, subresource, scope).Observe(elapsed)\n\trequestLatenciesSummary.WithLabelValues(verb, resource, subresource, scope).Observe(elapsed)"
  },
  {
    "id" : "27f169c1-7cfc-4638-9484-2cfccf4fbefb",
    "prId" : 48583,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48583#pullrequestreview-48640379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4124c50b-2045-4a74-8b38-a995a42a424e",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "ok, godoc says upper case. then it's fine",
        "createdAt" : "2017-07-07T15:33:58Z",
        "updatedAt" : "2017-07-07T19:50:59Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e33a2f0bc8ac82aecadcb19cf6e41259454d182",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +80,84 @@\n// MonitorRequest handles standard transformations for client and the reported verb and then invokes Monitor to record\n// a request. verb must be uppercase to be backwards compatible with existing monitoring tooling.\nfunc MonitorRequest(request *http.Request, verb, resource, subresource, contentType string, httpCode int, reqStart time.Time) {\n\treportedVerb := verb"
  },
  {
    "id" : "8932b068-1f49-4d02-ae01-5a673f3b2605",
    "prId" : 44421,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/44421#pullrequestreview-32866804",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de9cd124-1a38-409b-a51c-ceaf23520a89",
        "parentId" : null,
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "This can panic based on user input since `strings.ToLower(ua)` is not necessarily the same length in bytes as `ua`:\r\n\r\nhttps://play.golang.org/p/0BV2H6OlKi",
        "createdAt" : "2017-04-14T17:42:30Z",
        "updatedAt" : "2017-04-14T17:42:30Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cff7c3d30f4f6b4377d52ec29ec4866f1d28bb9",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +116,120 @@\t// If an old \"kubectl.exe\" has passed us its full path, we discard the path portion.\n\tif exeIdx := strings.LastIndex(strings.ToLower(ua), \"kubectl.exe\"); exeIdx != -1 {\n\t\treturn ua[exeIdx:]\n\t}\n\treturn ua"
  }
]