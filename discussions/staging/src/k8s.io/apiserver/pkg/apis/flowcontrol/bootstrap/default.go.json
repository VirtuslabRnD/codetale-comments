[
  {
    "id" : "31a54bad-44a7-4869-b85f-a7000e944dbf",
    "prId" : 101151,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/101151#pullrequestreview-637533700",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f195f706-3482-4002-8296-befe82a11824",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "It's confusing to include leases in something named \"node-health\".\r\n\r\nMaybe use \"kubelet-high\" instead?",
        "createdAt" : "2021-04-15T16:52:00Z",
        "updatedAt" : "2021-04-16T12:20:47Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "026e1094-b716-4c3c-b62b-a8afd95662f3",
        "parentId" : "f195f706-3482-4002-8296-befe82a11824",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Naming is hard :)\r\n\r\nI think nodes aren't just about Kubelet - IIRC, NodeProblemDetector is also running under \"NodeGroup\".\r\nSo I think kubelet is not the best.\r\n\r\nI actually like node (but it's in a sense of \"node\" (as a machine/VM where we run pods) not in the context of \"node\" as an API object. But I see where the confusion is coming from.\r\n\r\nI with I have a better proposal (node-heartbeats? [but that doesn't reflect the node status changes]).",
        "createdAt" : "2021-04-15T18:10:34Z",
        "updatedAt" : "2021-04-16T12:20:47Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "6ea82720-ae61-49ec-952b-a1f087e6058f",
        "parentId" : "f195f706-3482-4002-8296-befe82a11824",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Oh, if it's NPD also, then how about \"node-high\"? (but should NPD really be included?)",
        "createdAt" : "2021-04-15T18:16:38Z",
        "updatedAt" : "2021-04-16T12:20:47Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "3cef0891-0b71-4c9c-9df5-0a0f395b569d",
        "parentId" : "f195f706-3482-4002-8296-befe82a11824",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "\"node-high\" sounds fine to me.\r\n\r\nRe NPD - in my opinion it should, because if there is a problem with a node, I think reporting the status (to reflect the actual state and e.g. block scheduling more pods on that in case of issues) is more important than other operations (e.g. reporting new status of pod or fetching secrets to start it or more like that).",
        "createdAt" : "2021-04-15T18:19:51Z",
        "updatedAt" : "2021-04-16T12:20:47Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "b76f07e7-26d3-4ac9-8fc2-a9a80cef50e7",
        "parentId" : "f195f706-3482-4002-8296-befe82a11824",
        "authorId" : "5af3a49e-2ce9-4046-8a13-ee66b8cbca2e",
        "body" : "Thanks for naming suggestions :) I renamed to \"node-high\" :)",
        "createdAt" : "2021-04-16T09:56:58Z",
        "updatedAt" : "2021-04-16T12:20:47Z",
        "lastEditedBy" : "5af3a49e-2ce9-4046-8a13-ee66b8cbca2e",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d6e76f2766e51177ee50a1fba09bc5b04d6ce53",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +299,303 @@\t\t\t\t\t[]string{flowcontrol.VerbAll},\n\t\t\t\t\t[]string{coordinationv1.GroupName},\n\t\t\t\t\t[]string{\"leases\"},\n\t\t\t\t\t[]string{flowcontrol.NamespaceEvery},\n\t\t\t\t\tfalse),"
  },
  {
    "id" : "b42a0d11-d5f8-4f91-ab26-c525f9d97dc4",
    "prId" : 95259,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95259#pullrequestreview-509569118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83d0b49e-73f7-48f3-85e6-d6c3375a3562",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "The belief is that we have many more of these than we do workload-high clients?",
        "createdAt" : "2020-10-15T15:49:44Z",
        "updatedAt" : "2020-10-15T15:50:52Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "e48d5396-b2b9-4990-89ce-c0e64b347f4a",
        "parentId" : "83d0b49e-73f7-48f3-85e6-d6c3375a3562",
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "@deads2k `global-default` matches workloads that are running as non SA accounts. If I am not mistaken they are fewer compared to `service-accounts` which is basically all workloads (minus `workload-high`) running inside the cluster with service accounts.\r\n\r\n`service-accounts` workload matches the `workload-low` priority level.\r\nThe concurrency pool of `workload-low` is `20` whereas that of `global-default` is `100`, This PR swaps the values so `service-accounts` workloads have more concurrency share.\r\n\r\nAs far as `workload-high` is concerned, I have not seen `workload-high` workloads starve in my testing where we had about 13K Pods running at any time and a high namespace and pod churn rate. It was a 250-node cluster. \r\nAt the same time,  I recently came across a CI job where `workload-high` was starving because a certain admission controller `OwnerReferencesPermissionEnforcement` was taking order of seconds to respond and triggering p&f to throttle `workload-high`.\r\n\r\n\r\n",
        "createdAt" : "2020-10-15T16:37:54Z",
        "updatedAt" : "2020-10-15T16:37:54Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd7bf9a5dc3b4a0ba51b041fc721de719d1b2e69",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +211,215 @@\t\t\tType: flowcontrol.PriorityLevelEnablementLimited,\n\t\t\tLimited: &flowcontrol.LimitedPriorityLevelConfiguration{\n\t\t\t\tAssuredConcurrencyShares: 100,\n\t\t\t\tLimitResponse: flowcontrol.LimitResponse{\n\t\t\t\t\tType: flowcontrol.LimitResponseTypeQueue,"
  },
  {
    "id" : "3f2b0168-82e5-456c-9a84-419ba641dfbc",
    "prId" : 85268,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85268#pullrequestreview-341327573",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Currently the catch-all has 100 concurrency shares and all the other priority levels have 30 concurrency shares each.  Is this really what we want?",
        "createdAt" : "2019-11-27T06:58:13Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "017c5f67-2f36-4668-86f8-bc2fda5097d5",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "do you have a better number for recommendation?",
        "createdAt" : "2019-11-27T16:01:17Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      },
      {
        "id" : "d0e8b1bf-4e67-49ce-92b7-f45d1b13defa",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Hm, this is a problem, since we want this object to be immutable but not force people to use it. Consuming 100 shares seems not good. E.g. if the user brings their own config set and doesn't use this PL, and their shares sum to 900, then 10% of the shares would go unused. Unless we are smart enough such that PLs with zero enqueued requests do not get any shares at all?\r\n\r\nUnless we are that smart I'd consider setting this value very small, like to 1. That will also encourage people to fix their config if any requests end up getting routed to this.",
        "createdAt" : "2019-12-02T22:56:08Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "1e160662-69fa-4752-9090-8ad43be7a8aa",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "or preventing users from modifying the catch-all PL is probably not a good idea, just protecting the exempt would be sufficient?",
        "createdAt" : "2019-12-03T12:31:04Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      },
      {
        "id" : "e333d262-5b3b-4fa4-afc0-a3e5e3bfba95",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I don't think we have a choice, since a) the enforcement code pretends this exists and b) we have no way of knowing if a proposed modification is \"reasonable\" or not, since we do probably need this to remain reasonable.\r\n\r\nWhat if we set this to 1 to have the smallest impact possible on the shares? Users can make their own catch-all if that doesn't work for them.",
        "createdAt" : "2020-01-09T20:34:05Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "c726bb44-f821-498a-8038-07f45ad81142",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "body" : "I think we can make a fairly strong case that any number other than 1 here is a mistake. Ideally, no request will ever hit this priority level; it exists only to catch configuration errors, or for cases where people don't want flow control at all (in which case, deleting every non-mandatory object means everything goes into the catch-all bucket, which will get 1/1 total shares, i.e. 100%). We should therefore minimize its possible impact on any other traffic when flow control is enabled.\r\n\r\nIf a system accidentally gets misconfigured so that a lot of traffic is getting through to the catch-all level, we want an administrator to (a) notice that quickly, and (b) still be able to fix it. (a) is best achieved by giving catch-all a very small slice, so that throttling happens quickly. (b) is achieved by the presence of the exempt level.",
        "createdAt" : "2020-01-09T20:40:12Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "tags" : [
        ]
      },
      {
        "id" : "f825c6d7-794b-441b-867b-de5d0013e02d",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "IIRC we discussed this in our last well-attended meeting, and concurred with the following.  Giving the catch-all level a relatively miniscule share is problematic because it will choke off some traffic; even for unexpected traffic, a miniscule share is damn near equivalent to no share, which I think we agree we do not want.  Giving the catch-all priority level a significant share is a waste if it is normally unused.  But the catch-all priority level does not have to be normally unused.  Remember, it is actually the FlowSchema that \"catches\".  We favored using the catch-all priority level for some of the normal traffic and giving it a significant share.  Since we are fixing only one \"share\" quantity, and all the share quantities are relative, the admins can make that fixed share quantity map to any concurrency quantity they desire.",
        "createdAt" : "2020-01-10T00:07:46Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "3cc67107-9685-45da-a3d7-ee1cd071b1cc",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "> (a) is best achieved by giving catch-all a very small slice, so that throttling happens quickly.\r\n\r\nfor the record, in the previous implementation, even 1 concurrencyShare doesnt guarantee the flow-schema will get a share. \r\n\r\nmy point of giving the \"catch-all\" a normal share is based on the fact that \"practically it's tough for admins to define a set of flow-schemas to cover/match all the traffic proactively\". @lavalamp also suggests users to define their own \"catch-all\" to ease the pain of mis-killing unclassified traffic. if so, i suppose the new \"catch-all\" should be included in the suggested configuration, b/c most users will be finally adding it to their cluster.  in terms of UX, i'd expect users can be opt-in this feature w/o pain by simply swtiching on the feature-gate, instead of wasting time diving into the flow-schemas to find out which one is problematic.. the suggested+mandatory set should be sufficient for working w/ most clusters and users are not required to take any extra action unless they feel like more enhancements",
        "createdAt" : "2020-01-10T09:11:27Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      },
      {
        "id" : "744689f8-cead-4715-8702-9db73013d4f1",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "In #85319 and the feature branch before it, the calculation of concurrency from shares and total goes like this:\r\n```\r\n \t\tplState.qsConfig.ConcurrencyLimit = int(math.Ceil(float64(reqMgr.serverConcurrencyLimit) * float64(plState.config.Limited.AssuredConcurrencyShares) / shareSum))\r\n```\r\n\r\n(so a share of even 1 gets a non-zero amount of concurrency).",
        "createdAt" : "2020-01-10T17:50:32Z",
        "updatedAt" : "2020-01-10T17:50:32Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "93a660cd-8291-4437-a51b-f27ab4aade28",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "interesting, does that math work? it looks that the sum of actual shares can be greater than the total share",
        "createdAt" : "2020-01-10T17:54:19Z",
        "updatedAt" : "2020-01-10T17:54:19Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      },
      {
        "id" : "3552a476-0b6f-4745-810a-46dc540e879e",
        "parentId" : "3494a548-6e10-4058-8ed7-72e24b9bc423",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "As discussed in the meeting, the excess (if any) will be negligible.",
        "createdAt" : "2020-01-10T20:26:41Z",
        "updatedAt" : "2020-01-10T20:28:49Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "70dea6e4a8495ff028ccc8dc8e8aec04b93287c3",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +81,85 @@\t\t\tType: flowcontrol.PriorityLevelEnablementLimited,\n\t\t\tLimited: &flowcontrol.LimitedPriorityLevelConfiguration{\n\t\t\t\tAssuredConcurrencyShares: 100,\n\t\t\t\tLimitResponse: flowcontrol.LimitResponse{\n\t\t\t\t\tType: flowcontrol.LimitResponseTypeQueue,"
  },
  {
    "id" : "6cc18ac1-d57b-481e-bbc9-375ccab664c7",
    "prId" : 85268,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85268#pullrequestreview-326104189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "069996df-5596-4c7f-8a4a-77740d74a79d",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "I think the catch-all FlowSchema should have the logically lowest (numerically highest) possible matching precedence.  No FlowSchema with a logically lower matching precedence will have any effect.  We should make it impossible to create such a necessarily ineffective FlowSchema, and we should leave as much of the matching precedence value range available to administrators as possible.",
        "createdAt" : "2019-11-27T07:04:35Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "12fe41f8-c489-4b71-9b21-84e2ea369e69",
        "parentId" : "069996df-5596-4c7f-8a4a-77740d74a79d",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "or can we constraint the precedence to range [1000,10000] by the validation? it's a bit odd to see some confusing number like `(1<<32)-1` in an object. ",
        "createdAt" : "2019-11-27T16:09:03Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      },
      {
        "id" : "c7b7bb8f-2134-4bd9-ae0b-a4d2dd5cddd3",
        "parentId" : "069996df-5596-4c7f-8a4a-77740d74a79d",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "We discussed this value in our last meeting and agreed on using the max `int32`.  Any smaller value creates a booby trap for admins: they can create FlowSchema objects that are guaranteed to have no effect.  Also, this number appears only in a pre-created object --- admins do not have to put it in any of their objeccts.  Also, this number is used elsewhere in the kube API for a similar purpose.",
        "createdAt" : "2019-11-27T22:04:48Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "a015d9b3-8905-4b2f-990a-689efbeac447",
        "parentId" : "069996df-5596-4c7f-8a4a-77740d74a79d",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yeah, since this will match literally every request, we need to make sure that it loses all ties. So, this needs to be the lowest value permitted by validation, whatever that is.\r\n\r\nNote that `(1<<32)-1` is not a positive 32 bit int. ;)\r\n\r\nI'm fine with changing validation to make 10000 the max. I would not make 1000 the min though.",
        "createdAt" : "2019-12-02T23:07:35Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "ea4afa2d-e300-4e67-9c6b-877745165be9",
        "parentId" : "069996df-5596-4c7f-8a4a-77740d74a79d",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "i refactored the model to constraint the range of matching-precedence to [0,10000] by https://github.com/kubernetes/kubernetes/pull/85841",
        "createdAt" : "2019-12-03T12:32:37Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      }
    ],
    "commit" : "70dea6e4a8495ff028ccc8dc8e8aec04b93287c3",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +125,129 @@\t\t\"catch-all\",\n\t\t\"catch-all\",\n\t\t10000, // matchingPrecedence\n\t\tflowcontrol.FlowDistinguisherMethodByUserType, // distinguisherMethodType\n\t\tflowcontrol.PolicyRulesWithSubjects{"
  },
  {
    "id" : "aa90d1e2-9511-4cbc-88c8-de96943733fb",
    "prId" : 85268,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85268#pullrequestreview-340994796",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6844d50e-1ca6-4f5a-90d3-a39db0db4091",
        "parentId" : null,
        "authorId" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "body" : "I think LimitResponseTypeReject might make more sense here, for the reasons I mention above.",
        "createdAt" : "2020-01-09T20:42:58Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "tags" : [
        ]
      },
      {
        "id" : "c8992a59-d48c-4188-9280-c0ec094a0e18",
        "parentId" : "6844d50e-1ca6-4f5a-90d3-a39db0db4091",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "not necessary? the \"catch-all\" doesnt have to be user-hostiling",
        "createdAt" : "2020-01-10T09:13:31Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      }
    ],
    "commit" : "70dea6e4a8495ff028ccc8dc8e8aec04b93287c3",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +83,87 @@\t\t\t\tAssuredConcurrencyShares: 100,\n\t\t\t\tLimitResponse: flowcontrol.LimitResponse{\n\t\t\t\t\tType: flowcontrol.LimitResponseTypeQueue,\n\t\t\t\t\tQueuing: &flowcontrol.QueuingConfiguration{\n\t\t\t\t\t\tQueues:           128,"
  },
  {
    "id" : "6528b954-7ac4-4846-8a65-6eb494248d09",
    "prId" : 85268,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85268#pullrequestreview-340800196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa0a05fc-87e7-48b6-8ea1-6d3533e6a847",
        "parentId" : null,
        "authorId" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "body" : "If I'm reading this right, the suggested schema has ordinary users hitting the catch-all group, contradicting what I said before. I think that might still be OK: if I understand right, giving the catch-all PL one assured share, in combination with the priority levels listed here, would mean that non-admins running kubectl on the command line would get about 1% of server resources, which sounds more or less fine to me (but I'll defer to someone with more experience on that).",
        "createdAt" : "2020-01-09T21:24:38Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "tags" : [
        ]
      }
    ],
    "commit" : "70dea6e4a8495ff028ccc8dc8e8aec04b93287c3",
    "line" : 357,
    "diffHunk" : "@@ -1,1 +355,359 @@\t\t\t},\n\t\t},\n\t)\n)\n"
  },
  {
    "id" : "a888940d-6369-4c0a-93c9-167f9f22e9f5",
    "prId" : 85268,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85268#pullrequestreview-340994796",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bb15ece-fc57-48ff-983e-cbbb73e1c273",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Does leader election involve informers?  If so then wouldn't we need to include the \"list\" verb here?",
        "createdAt" : "2020-01-10T06:48:48Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "1d9fe1e4-5c3f-4287-bdf2-c3fd4df4f515",
        "parentId" : "8bb15ece-fc57-48ff-983e-cbbb73e1c273",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "leader-election doenst involve informer",
        "createdAt" : "2020-01-10T09:13:58Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      }
    ],
    "commit" : "70dea6e4a8495ff028ccc8dc8e8aec04b93287c3",
    "line" : 251,
    "diffHunk" : "@@ -1,1 +249,253 @@\t\t\tResourceRules: []flowcontrol.ResourcePolicyRule{\n\t\t\t\tresourceRule(\n\t\t\t\t\t[]string{\"get\", \"create\", \"update\"},\n\t\t\t\t\t[]string{corev1.GroupName},\n\t\t\t\t\t[]string{\"endpoints\", \"configmaps\"},"
  },
  {
    "id" : "b6fbefa1-71f4-4e5e-ba9f-ce361f20cfee",
    "prId" : 85268,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85268#pullrequestreview-340994796",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a852879-672d-4dbf-b1d4-71eba19a03ad",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Earlier I saw `users(user.KubeControllerManager)` and `kubeSystemServiceAccount(flowcontrol.NameAll)` in the same list of subjects, and this makes sense to me because the cluster operator has a choice for how the controllers in the kube-controller-manager identify themselves.  Why are these two not always appearing together?",
        "createdAt" : "2020-01-10T06:54:32Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "cc15b63c-07f5-4287-b9f0-4b434d36565e",
        "parentId" : "1a852879-672d-4dbf-b1d4-71eba19a03ad",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "we grant the leader-election requests from KCM and scheduler a bit higher precedence than the `kubeSystemServiceAccount(flowcontrol.NameAll)`. given that these are in the suggested set so the users are welcome to modify them. it looks clearer as a dedicated rule to tell the users which flow-schema applies to KCM and scheduler?",
        "createdAt" : "2020-01-10T09:18:20Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      }
    ],
    "commit" : "70dea6e4a8495ff028ccc8dc8e8aec04b93287c3",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +288,292 @@\t\tflowcontrol.FlowDistinguisherMethodByNamespaceType,\n\t\tflowcontrol.PolicyRulesWithSubjects{\n\t\t\tSubjects: users(user.KubeControllerManager),\n\t\t\tResourceRules: []flowcontrol.ResourcePolicyRule{resourceRule(\n\t\t\t\t[]string{flowcontrol.VerbAll},"
  },
  {
    "id" : "b7f0bc71-7ef6-4ebe-abc8-13162cd0b11a",
    "prId" : 85268,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85268#pullrequestreview-341327573",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c51665d2-4ffd-4bec-9e18-3b344b79fc8d",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "I think we wanted to prioritize `kubectl` commands from users above ordinary controller work.  See https://github.com/MikeSpreitzer/kube-enhancements/blob/explain-reqmgmt-config/keps/sig-api-machinery/20190228-priority-and-fairness.md#design-rationale and the default-objects.go file in https://github.com/kubernetes/kubernetes/pull/85319 .",
        "createdAt" : "2020-01-10T06:59:20Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "f710d9f8-7a6b-412c-9d9a-5a8e2bfa7458",
        "parentId" : "c51665d2-4ffd-4bec-9e18-3b344b79fc8d",
        "authorId" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "body" : "yes, it is. the `exempt` rule matches `kubectl`.",
        "createdAt" : "2020-01-10T09:20:35Z",
        "updatedAt" : "2020-01-10T17:46:46Z",
        "lastEditedBy" : "bc182326-9017-48d6-8ee0-4609046c1366",
        "tags" : [
        ]
      },
      {
        "id" : "fe7690e6-70ab-4966-821c-c0bf6af6b573",
        "parentId" : "c51665d2-4ffd-4bec-9e18-3b344b79fc8d",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "`kubectl` can be used by users who are not in the `system:masters` group.",
        "createdAt" : "2020-01-10T20:28:30Z",
        "updatedAt" : "2020-01-10T20:28:49Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "70dea6e4a8495ff028ccc8dc8e8aec04b93287c3",
    "line" : 322,
    "diffHunk" : "@@ -1,1 +320,324 @@\t\t},\n\t)\n\tSuggestedFlowSchemaKubeSystemServiceAccounts = newFlowSchema(\n\t\t\"kube-system-service-accounts\", \"workload-high\", 900,\n\t\tflowcontrol.FlowDistinguisherMethodByNamespaceType,"
  }
]