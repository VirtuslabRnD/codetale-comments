[
  {
    "id" : "69de4674-940d-4c9e-8e3f-d2845e0dfb29",
    "prId" : 87957,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87957#pullrequestreview-355609266",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94ca822c-bfb4-416c-9b31-c6d472c71e3f",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I really didn't want to teach this layer about resource version, but if the impact on existing listeners is as big as I think it is, we may have to",
        "createdAt" : "2020-02-08T21:36:06Z",
        "updatedAt" : "2020-02-08T21:36:07Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "6a78c5e0-dc2b-4051-8e43-3542caa640b0",
        "parentId" : "94ca822c-bfb4-416c-9b31-c6d472c71e3f",
        "authorId" : "5f3d0da0-11c3-49d0-9aa9-f897fad1c961",
        "body" : "What's the alternative? Moving this logic to DeltaFIFO could work, since it also has access to the client's Store (via `knownObjects`), but I suspect that's a concurrency bug waiting to happen (namely, the Store could have an older value, since SharedInformer might still be processing).",
        "createdAt" : "2020-02-09T09:35:28Z",
        "updatedAt" : "2020-02-09T09:35:28Z",
        "lastEditedBy" : "5f3d0da0-11c3-49d0-9aa9-f897fad1c961",
        "tags" : [
        ]
      },
      {
        "id" : "dc5910d8-22e4-4d9a-aebc-e77f1bdde0d8",
        "parentId" : "94ca822c-bfb4-416c-9b31-c6d472c71e3f",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Yeah, if we're going to filter, this is the place",
        "createdAt" : "2020-02-09T14:03:49Z",
        "updatedAt" : "2020-02-09T14:03:49Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6caa0a4726ba97737056175494516367cf98cae",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +499,503 @@\t\t\t\t\t\t\t// Replaced events that didn't change resourceVersion are treated as resync events\n\t\t\t\t\t\t\t// and only propagated to listeners that requested resync\n\t\t\t\t\t\t\tisSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion()\n\t\t\t\t\t\t}\n\t\t\t\t\t}"
  },
  {
    "id" : "7da8aaa4-9b36-4cb8-9ec5-e6ffdf2fb34f",
    "prId" : 87553,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87553#pullrequestreview-370043154",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7468a8f1-709c-44ef-a7f1-03129e334299",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Certainly not for objects of different types. Does the informer break the guarantee for objects of the same type?",
        "createdAt" : "2020-03-05T23:20:38Z",
        "updatedAt" : "2020-03-05T23:20:38Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "470d4f0f-d68e-4e5b-8631-8bda135da5df",
        "parentId" : "7468a8f1-709c-44ef-a7f1-03129e334299",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "Yes.",
        "createdAt" : "2020-03-06T02:13:47Z",
        "updatedAt" : "2020-03-06T02:13:47Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8e2ad5926c3a6872422ad25cf9867e10e052a7d",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +63,67 @@// might never appear in the cache but ordering among the appearing\n// states is correct.  Note, however, that there is no promise about\n// ordering between states seen for different objects.\n//\n// The local cache starts out empty, and gets populated and updated"
  },
  {
    "id" : "cc4481fa-1919-4a7f-945b-69fd3bcc5e25",
    "prId" : 87553,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87553#pullrequestreview-370045837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6cfc78fa-e317-45cf-a8bc-5fbf2911675d",
        "parentId" : null,
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "\"that with\" looks like a typo; I think it should be \"that has\"",
        "createdAt" : "2020-03-06T02:23:20Z",
        "updatedAt" : "2020-03-06T02:23:20Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8e2ad5926c3a6872422ad25cf9867e10e052a7d",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +181,185 @@// The created informer will not do resyncs if the given\n// defaultEventHandlerResyncPeriod is zero.  Otherwise: for each\n// handler that with a non-zero requested resync period, whether added\n// before or after the informer starts, the nominal resync period is\n// the requested resync period rounded up to a multiple of the"
  },
  {
    "id" : "40165c75-c868-4ce6-ac02-a827d395f552",
    "prId" : 87393,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87393#pullrequestreview-346928118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I don't see how this actually causes a delay as claimed by the comment? Maybe the original author intended to have a `return`?\r\n\r\nAlso, doesn't a minute seem overly punitive? It's already skipping the item, why also delay?\r\n\r\nI think the minute on the outer Wait was there to make sure it would lose the race with the `close(stopCh)` call, and the inner wait was intended to do exponential backoffs. But the original author missed a `return` or thought that HandleError panic'd, and as a result there actually was no delay.",
        "createdAt" : "2020-01-21T22:19:14Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "d2154bc7-9970-4445-b1bf-0c8af967fbc5",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "body" : "The original comment seems to correctly describe the old code:\r\n// this gives us a few quick retries before a long pause and then a few more quick retries\r\n\r\nSo this change is changing from retries (where in this context \"retry\" means \"continue with the next notification,\" not \"retry to deliver the failed notification\") at 0.01, 0.02, 0.03, 0.04, 0.05, 60.05, 60.06, 60.07, 60.08, 60.09, 120.09, ... seconds, to retries at 60.0, 120.0, ...\r\n\r\nI have no particular opinion on whether the change is one we want or not.",
        "createdAt" : "2020-01-21T22:45:23Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "tags" : [
        ]
      },
      {
        "id" : "05448ffd-1070-4697-9c52-ffab5ea28b26",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "`wait.Until` does the catch and delay, see the amount of delay is the penultimate parameter.\r\n\r\nSo should the `wait.Until` be replaced with an exponential backoff?  If so, when would it reset?",
        "createdAt" : "2020-01-22T01:05:46Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "ed803ce4-509b-41cc-9beb-14b14e889af5",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "`wait.ExponentialBackoff` does not catch and handle panics, so this PR is making no change in behavior.  But we are getting deep into questions of original intent and (more importantly, since the original code looks confused) what we think _should_ be done.  So let's do this.  We are not talking about retrying a notification, we are talking about proceeding from one notification to the next when processing the first notification panics.  What pattern of delays do we want?  Do we have established principles for answering this question?  A panic indicates a bug (perhaps one of several).  But unless we add significant smarts, this code will not know how often these bugs cause a panic.  I am inclined to suggest a constant short delay, perhaps one second.  Short should be OK because each attempt makes progress (consumes at least one item from the queue).  Longer could be bad because it introduces unhelpful delay when a bug bites.  But the delay should be long enough that something in the context can plausibly have changed, because the bugs may depend on something that changes with time.",
        "createdAt" : "2020-01-22T20:05:18Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      },
      {
        "id" : "da729b2d-564d-40b4-82ef-e23e238712c9",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "body" : "Ah, didn't realize ExponentialBackoff doesn't catch panics. Yeah, this inner func can never return an error, so the inner backoff really was useless.\r\n\r\nI agree that a minute seems uselessly long here. ISTM there are two reasonably likely cases:\r\n1. a single notification tickles some bug in one-off fashion\r\n1. some incompatibility means *every* notification will cause a panic, leading to a flood of panic-recover cycles\r\n\r\nI think you're right that once a second strikes a reasonable balance between not suppressing problems that need to be addressed, and not overwhelming the client program with a potentially-infinite flood of error logging.",
        "createdAt" : "2020-01-22T20:29:20Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "tags" : [
        ]
      },
      {
        "id" : "106e2604-9589-4c6f-8bb9-977813f98725",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "body" : "(BTW, this is likely moot since `runtime.ReallyCrash` defaults to true these days, so it looks like the first panic will just kill the program most of the time.)",
        "createdAt" : "2020-01-22T20:31:02Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "tags" : [
        ]
      },
      {
        "id" : "a2624658-3694-4e88-9118-cfb959c0fd59",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "HandleError doesn't panic. And it already does backoff: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/runtime/runtime.go#L88",
        "createdAt" : "2020-01-22T20:48:31Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "0f4b53b0-2b65-4573-b425-664c2ae69b80",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I may have been confused, and/or the original comment wasn't clear.\r\n\r\n* This error delivery line will skip an item. But no panic is involved.\r\n* A panic on an item will not cause this line to be executed. It will also not cause the item to be \"skipped\", since the item was in the middle of processing in order to cause the panic. It won't be re-delivered/retried, but that's not the same as skipping it. It might have gotten 99.9% done and made irreversible state changes for all we know.",
        "createdAt" : "2020-01-22T20:53:29Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "8af921a5-203b-4ccc-8d4c-bc9e78db9704",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I think a fixed delay is fine, since we are not retrying the same item (nor do we want to, since it caused a panic!). I think 60 seconds is too long and is likely to cause the channel to fill up or get too big. I think 1s is enough to avoid a hot loop.\r\n\r\nI also think ReallyCrash should be on for most callers, so this doesn't matter too much.\r\n\r\nWho knows what state the recipient might be in after panicing partway through processing some item!",
        "createdAt" : "2020-01-22T20:57:31Z",
        "updatedAt" : "2020-01-22T21:49:18Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "5f6caeff-07c6-4533-8963-516dc9405b26",
        "parentId" : "0461ad2a-1bc6-425c-ad1f-5ec57433afc5",
        "authorId" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "body" : "While this discussion is attached to the default case of the switch, the subject matter moved to focus on the comment at the start of the function body.\r\nI think we have agreed what to do, and I have pushed that revision.",
        "createdAt" : "2020-01-22T21:50:26Z",
        "updatedAt" : "2020-01-22T21:50:27Z",
        "lastEditedBy" : "7a59a326-58b3-4590-b70a-297d2e27daa5",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ad469abbb1122cbbd772e15767817cd771f9f6",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +711,715 @@\t\t\t\tp.handler.OnDelete(notification.oldObj)\n\t\t\tdefault:\n\t\t\t\tutilruntime.HandleError(fmt.Errorf(\"unrecognized notification: %T\", next))\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "d11c6416-4ce2-48c6-806c-e4203f862f18",
    "prId" : 87329,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87329#pullrequestreview-374666226",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "586920e1-b039-4ad9-b2ad-59b63e86eced",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Shared informers are usually shared :). This interface should attempt to reflect that ownership model by exposing functionality that is safe to consume at multiple sites across potentially large applications. SharedInformers differ a lot from regular informers to make sure that's the case (see cache mutation detection, and shared processor). This function stands out to me because it is:\r\n* Exclusive: last call before starting the informer wins. Previous calls are silently ignored.\r\n* Has potential for unknown/unbounded/hard to notice spillover effects on other users of the same shared informer.",
        "createdAt" : "2020-03-12T22:08:21Z",
        "updatedAt" : "2020-03-14T00:25:47Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "637dc419-8e41-44c4-b6be-1588b8f650ed",
        "parentId" : "586920e1-b039-4ad9-b2ad-59b63e86eced",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I don't think this is the mechanism we'll use to get controllers to e.g. stop operating during a disconnection. This is just for visibility.\r\n\r\nI think we can state these things in the comment and that will be sufficient.",
        "createdAt" : "2020-03-13T15:38:02Z",
        "updatedAt" : "2020-03-14T00:25:47Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "568a8c42-8b4c-457e-ae16-80e9215ad337",
        "parentId" : "586920e1-b039-4ad9-b2ad-59b63e86eced",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Sorry for not being clear, I think we should add to the comment here:\r\n\r\n* There's only one handler, so if you call this multiple times, last one wins; calling after the informer has been started returns an error.\r\n* The handler is intended for visibility, not to e.g. pause the consumers.\r\n\r\n(these points don't need to go in the place where you copied this comment, just here--since they're unique to shared informers, or at least the first one is.)",
        "createdAt" : "2020-03-13T22:22:52Z",
        "updatedAt" : "2020-03-14T00:25:47Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "45026583-cf09-44da-9350-eccf730840f6",
        "parentId" : "586920e1-b039-4ad9-b2ad-59b63e86eced",
        "authorId" : "f06f9568-4974-4d41-9a77-85c5b8610434",
        "body" : "sorry - github aggressively hides conversations marked 'resolved' so i missed that this was still outstanding! I like your suggestions, added!",
        "createdAt" : "2020-03-13T23:22:27Z",
        "updatedAt" : "2020-03-14T00:25:47Z",
        "lastEditedBy" : "f06f9568-4974-4d41-9a77-85c5b8610434",
        "tags" : [
        ]
      }
    ],
    "commit" : "435b40aa1e5c0ae44e0aeb9aa6dbde79838b3390",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +178,182 @@\t// The handler should return quickly - any expensive processing should be\n\t// offloaded.\n\tSetWatchErrorHandler(handler WatchErrorHandler) error\n}\n"
  },
  {
    "id" : "a91a6031-735a-47eb-bfa2-87818fc28851",
    "prId" : 81527,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81527#pullrequestreview-278458685",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e65fdd8-370c-43f5-808f-7b5b5b49025f",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I'd add a comment to this method indicating callers should generally prefer `WaitForNamedCacheSync`",
        "createdAt" : "2019-08-22T14:26:13Z",
        "updatedAt" : "2019-08-22T15:13:57Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e4c3096fe71afc6a23c273b3309ed5db7289d8c",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +207,211 @@\n// WaitForCacheSync waits for caches to populate.  It returns true if it was successful, false\n// if the controller should shutdown\n// callers should prefer WaitForNamedCacheSync()\nfunc WaitForCacheSync(stopCh <-chan struct{}, cacheSyncs ...InformerSynced) bool {"
  },
  {
    "id" : "b6cb2769-166b-42af-8ced-da61f4be4c69",
    "prId" : 47045,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/47045#pullrequestreview-52815899",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c132ccc4-6d61-4b69-b16c-27b4693d80f3",
        "parentId" : null,
        "authorId" : "6314844c-5310-477a-96a1-0fc838ec485f",
        "body" : "Do we really want this unbuffered? I noticed that in the case of just creating the processListener, we created a sync point because we need to hand off the notification to the newly created listener.",
        "createdAt" : "2017-07-27T15:38:09Z",
        "updatedAt" : "2017-08-02T08:58:07Z",
        "lastEditedBy" : "6314844c-5310-477a-96a1-0fc838ec485f",
        "tags" : [
        ]
      },
      {
        "id" : "6ba53b45-52b9-46d0-a25d-8d5b78aef52e",
        "parentId" : "c132ccc4-6d61-4b69-b16c-27b4693d80f3",
        "authorId" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "body" : "1. Previous code acquires the lock and then adds the notification to the slice. New code pushes it via a channel. Both have potential to block.\r\n2. The whole purpose of the `pop` method running concurrently is to receive from `addCh` as quickly as possible (or push to `nextCh`).\r\n\r\nBenchmark shows that it's not a big deal. I think doing buffering in the channel and then re-buffering into a slice is unnecessary.",
        "createdAt" : "2017-07-27T23:01:48Z",
        "updatedAt" : "2017-08-02T08:58:07Z",
        "lastEditedBy" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "tags" : [
        ]
      },
      {
        "id" : "af2d203d-3fc4-45aa-829c-7fb69941cc27",
        "parentId" : "c132ccc4-6d61-4b69-b16c-27b4693d80f3",
        "authorId" : "6314844c-5310-477a-96a1-0fc838ec485f",
        "body" : "It isn't quite the same. The previous code grabbed a lock added it to a slice and unlocked. The new code must wait until you have something pull off the channel.\r\nIf you have a backlog of pending notifications, anything calling add will wait until pop actually gets to it.\r\nI am a bit concerned that the Benchmark is just that and may not simulate reality of all the various ways add() is actually invoked.\r\n",
        "createdAt" : "2017-07-27T23:44:47Z",
        "updatedAt" : "2017-08-02T08:58:07Z",
        "lastEditedBy" : "6314844c-5310-477a-96a1-0fc838ec485f",
        "tags" : [
        ]
      },
      {
        "id" : "227f0735-151d-4989-88fe-57b5eecfc685",
        "parentId" : "c132ccc4-6d61-4b69-b16c-27b4693d80f3",
        "authorId" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "body" : "I completely agree that the benchmark is not perfect. I'll happily implement something in addition to it but nothing comes to my mind. If you have any ideas, please share :)\r\n\r\nI don't think in practice it matters though - events are coming via the network so delays in order of 0.1-10ms should probably be expected. And we are talking about nanoseconds here.",
        "createdAt" : "2017-07-27T23:53:57Z",
        "updatedAt" : "2017-08-02T08:58:07Z",
        "lastEditedBy" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "tags" : [
        ]
      }
    ],
    "commit" : "35e849bff2e4e2e1b08ee183992d620d4e409f54",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +481,485 @@\tret := &processorListener{\n\t\tnextCh:                make(chan interface{}),\n\t\taddCh:                 make(chan interface{}),\n\t\thandler:               handler,\n\t\trequestedResyncPeriod: requestedResyncPeriod,"
  },
  {
    "id" : "f283b049-e800-4bc8-8905-c1fed7cad993",
    "prId" : 46094,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46094#pullrequestreview-39174358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "727e2d92-c49b-4eb4-b15d-20eab6772248",
        "parentId" : null,
        "authorId" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "body" : "`run` is a blocking method now. It waits for all listeners it started to finish.",
        "createdAt" : "2017-05-19T11:48:26Z",
        "updatedAt" : "2017-07-18T04:07:42Z",
        "lastEditedBy" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "tags" : [
        ]
      }
    ],
    "commit" : "6464774a9b94f6e8376e11d015fd55e98457e74c",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +398,402 @@}\n\nfunc (p *sharedProcessor) run(stopCh <-chan struct{}) {\n\tvar wg wait.Group\n\tfunc() {"
  }
]