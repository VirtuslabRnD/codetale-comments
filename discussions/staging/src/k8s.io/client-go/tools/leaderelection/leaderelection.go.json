[
  {
    "id" : "5972575a-5d1d-4809-942d-70c5209fc759",
    "prId" : 87899,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87899#pullrequestreview-356211770",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6562d4a6-19f6-4d6c-bce3-654586f927d6",
        "parentId" : null,
        "authorId" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "body" : "I think we need to pass the context down to resourcelock.Interface.Get() as well as Update() or risk changing the timeout behavior of this code. @mikedanese you can probably figure out easier than I can whether the difference is significant (i.e. is there any time bound on client requests if no context is passed in?)",
        "createdAt" : "2020-02-07T22:51:05Z",
        "updatedAt" : "2020-02-10T20:31:51Z",
        "lastEditedBy" : "b451f401-2153-49b2-a815-0e09fafa590b",
        "tags" : [
        ]
      },
      {
        "id" : "6b026eb8-1238-4d44-9f60-febf81713ec5",
        "parentId" : "6562d4a6-19f6-4d6c-bce3-654586f927d6",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "There's a default timeout on the client that is probably not set. I think the timeout behavior change here is desirable. There's also server side timeout enforcement that gets applied to these requests.",
        "createdAt" : "2020-02-10T19:48:06Z",
        "updatedAt" : "2020-02-10T20:31:51Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "c049f30ef2a83172f46a587ddaf2104b39df8301",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +305,309 @@// else it tries to renew the lease if it has already been acquired. Returns true\n// on success else returns false.\nfunc (le *LeaderElector) tryAcquireOrRenew(ctx context.Context) bool {\n\tnow := metav1.Now()\n\tleaderElectionRecord := rl.LeaderElectionRecord{"
  },
  {
    "id" : "9f5400a6-aa2e-4753-955e-ece52d676af5",
    "prId" : 71731,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71731#pullrequestreview-189941849",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89b05f9d-9d55-4b42-b49a-768f2179ef56",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "This is really ugly.  What are we gaining by making prometheus metrics not the default?  Who is adding code level metrics plugins to k/k that we would approve?\r\n\r\nThis feels like a pointless abstraction - would like more clarity about why we are polluting the code with it.",
        "createdAt" : "2019-01-04T15:19:09Z",
        "updatedAt" : "2019-01-04T15:19:17Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "6e2e63e8-cf14-49b1-be57-ae93eb204a17",
        "parentId" : "89b05f9d-9d55-4b42-b49a-768f2179ef56",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> This is really ugly. What are we gaining by making prometheus metrics not the default? Who is adding code level metrics plugins to k/k that we would approve?\r\n> \r\n> This feels like a pointless abstraction - would like more clarity about why we are polluting the code with it.\r\n\r\nShort answer: fewer deps from client-go.\r\n\r\nBack when we started the project, prometheus wasn't as wide spread and we wanted to avoid the dependency. I think we should revisit that choice, but I would not choose to revisit it in this pull.",
        "createdAt" : "2019-01-07T13:33:47Z",
        "updatedAt" : "2019-01-07T13:33:47Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "c5c40592-8636-4a75-8a5a-4563779e691b",
        "parentId" : "89b05f9d-9d55-4b42-b49a-768f2179ef56",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "As David says this is about reducing how many extra dependencies you have to pull in when you pull in client-go. The original version of the PR did not include the dependency breaking back flips but I was asked to add it in the standard manner, we do for other client-go metrics)",
        "createdAt" : "2019-01-07T18:48:36Z",
        "updatedAt" : "2019-01-07T18:48:36Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1926573804ed2c172c91d1022203d0699210138",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +93,97 @@\t\tconfig:  lec,\n\t\tclock:   clock.RealClock{},\n\t\tmetrics: globalMetricsFactory.newLeaderMetrics(),\n\t}\n\tle.metrics.leaderOff(le.config.Name)"
  },
  {
    "id" : "618273df-a28b-4425-89f1-ee9146e30ac4",
    "prId" : 71490,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71490#pullrequestreview-191282162",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "440a552b-f5bf-46c4-a4c8-093777aacfeb",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "tryAcquireOrRenew calls Get() which refreshes resource version. We might want to refresh here as well to avoid conflict errors.",
        "createdAt" : "2018-12-05T01:04:48Z",
        "updatedAt" : "2019-01-11T17:50:16Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "97339402-e399-496d-b728-8eb1eb89adac",
        "parentId" : "440a552b-f5bf-46c4-a4c8-093777aacfeb",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "So when writing this I was biasing to \"best effort\" and \"step down as quickly as possible before we get nuked by the sig kill\".  If we refresh, we could potentially have to wait 2 round trips vs one, with the potential for longer behavior.\r\n\r\nSince we only step down if we're the leader, shouldn't we have an updated resource version in the lock already as the last writer?",
        "createdAt" : "2018-12-09T15:35:19Z",
        "updatedAt" : "2019-01-11T17:50:16Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "54547e85-e78d-4d4f-a68a-b22129442d99",
        "parentId" : "440a552b-f5bf-46c4-a4c8-093777aacfeb",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "> shouldn't we have an updated resource version in the lock already as the last writer?\r\n\r\nUsually. Tradeoff SGTM.",
        "createdAt" : "2019-01-10T16:03:52Z",
        "updatedAt" : "2019-01-11T17:50:16Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe74efb1f90826b1903d2908ff9e528329bebea0",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +272,276 @@\t\tLeaderTransitions: le.observedRecord.LeaderTransitions,\n\t}\n\tif err := le.config.Lock.Update(leaderElectionRecord); err != nil {\n\t\tklog.Errorf(\"Failed to release lock: %v\", err)\n\t\treturn false"
  },
  {
    "id" : "68df88cd-481f-4947-964f-f567f1bba332",
    "prId" : 71490,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71490#pullrequestreview-182983034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29f2ac22-1925-4d5e-ab1d-62225acc9781",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "What are the compatibility implications here?",
        "createdAt" : "2018-12-05T01:07:23Z",
        "updatedAt" : "2019-01-11T17:50:16Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "5bbd698b-1e14-4297-8b08-2d005ff7d0bf",
        "parentId" : "29f2ac22-1925-4d5e-ab1d-62225acc9781",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I'll add a test to prove behavior for this, but older clients watching observe the lease as still held (they don't refresh).  So they assume the holder still holds.  If a new client comes up (N+1) while old clients (N) are still running, the new client would have to acquire the lease, then step down, and the old clients would simply be less aggressive in acquiring.",
        "createdAt" : "2018-12-09T15:32:38Z",
        "updatedAt" : "2019-01-11T17:50:16Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe74efb1f90826b1903d2908ff9e528329bebea0",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +314,318 @@\t\tle.observedTime = le.clock.Now()\n\t}\n\tif len(oldLeaderElectionRecord.HolderIdentity) > 0 &&\n\t\tle.observedTime.Add(le.config.LeaseDuration).After(now.Time) &&\n\t\t!le.IsLeader() {"
  },
  {
    "id" : "60c8b20b-125f-4c4c-84ac-25bb671cec28",
    "prId" : 70971,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70971#pullrequestreview-175098097",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28cb58ce-0af2-44b9-af02-4be82aaf5e65",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Document that it may be nil if it's not needed?",
        "createdAt" : "2018-11-14T19:22:55Z",
        "updatedAt" : "2018-11-15T00:57:54Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "aad354dc-a49c-4dad-a20f-351232a72ae4",
        "parentId" : "28cb58ce-0af2-44b9-af02-4be82aaf5e65",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "Done",
        "createdAt" : "2018-11-14T21:55:52Z",
        "updatedAt" : "2018-11-15T00:57:54Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c43ee6d6ec6a159b960381af906c130027bc716",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +115,119 @@\tCallbacks LeaderCallbacks\n\n\t// WatchDog is the associated health checker\n\t// WatchDog may be null if its not needed/configured.\n\tWatchDog *HealthzAdaptor"
  },
  {
    "id" : "05b3a5de-c4eb-4b9c-9337-2ca95c77759e",
    "prId" : 70971,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70971#pullrequestreview-175126388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c1ddea0-a76f-4864-b4df-8daa96378f98",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "maybe calling it \"maxTolerableExpiredLease\" or something would be clearer?\r\n\r\nAlternatively, you could make a function that just returns the lease age and folks could do whatever they wanted.",
        "createdAt" : "2018-11-14T19:29:54Z",
        "updatedAt" : "2018-11-15T00:57:54Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "b9f75ce4-12d5-40c6-aa60-22893b04483f",
        "parentId" : "0c1ddea0-a76f-4864-b4df-8daa96378f98",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "Would rather not expose more internals than are needed for the use case.",
        "createdAt" : "2018-11-14T23:17:30Z",
        "updatedAt" : "2018-11-15T00:57:54Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c43ee6d6ec6a159b960381af906c130027bc716",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +320,324 @@}\n\n// Check will determine if the current lease is expired by more than timeout.\nfunc (le *LeaderElector) Check(maxTolerableExpiredLease time.Duration) error {\n\tif !le.IsLeader() {"
  },
  {
    "id" : "567ddaa5-4292-40f5-bbee-48efba368519",
    "prId" : 64218,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64218#pullrequestreview-122798690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbd8e13f-e443-4389-911d-db081c0804b9",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "nit: should we print the RetryPeriod current value in error ?",
        "createdAt" : "2018-05-23T23:08:56Z",
        "updatedAt" : "2018-05-23T23:08:56Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      }
    ],
    "commit" : "7288e8828f5bf659d41e71760f9d9923e405e2e4",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +82,86 @@\t}\n\tif lec.RetryPeriod < 1 {\n\t\treturn nil, fmt.Errorf(\"retryPeriod must be greater than zero\")\n\t}\n"
  },
  {
    "id" : "0c3757b1-e2b7-4644-ad14-5bb4beacea5c",
    "prId" : 57932,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/57932#pullrequestreview-96344609",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8523a2a5-6669-4b34-ab15-b4f35f5e91df",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Why are you defering a cancel the cancel should only occur then they've lost leadership.",
        "createdAt" : "2018-02-13T15:11:08Z",
        "updatedAt" : "2018-06-09T03:06:35Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "56bbc2e9-006d-4441-879c-3dd120aae773",
        "parentId" : "8523a2a5-6669-4b34-ab15-b4f35f5e91df",
        "authorId" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "body" : "This is a sub context, it does not influence the parent context. This code is just following the best practice patten where the resource is closed/freed when it falls out of scope.\r\nActually, just inspected how child cancel contexts are implemented - child contexts are referenced by parent contexts if both are cancel contexts. References are only cleaned up once the that `cancel()` function is called. So it is a good idea to clean things up.",
        "createdAt" : "2018-02-13T23:51:27Z",
        "updatedAt" : "2018-06-09T03:06:35Z",
        "lastEditedBy" : "06a18a1b-6ca5-44b8-9cdc-5bb944ae4e29",
        "tags" : [
        ]
      }
    ],
    "commit" : "e458cfe02ccb3b816f86d8025c03da67eae3ff0d",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +186,190 @@func (le *LeaderElector) acquire(ctx context.Context) bool {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tsucceeded := false\n\tdesc := le.config.Lock.Describe()"
  },
  {
    "id" : "659a9d1d-111a-475b-a8b0-f2b77b686c82",
    "prId" : 57932,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/57932#pullrequestreview-96174491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bec362b3-a5d9-46af-b569-ee4559c2acd1",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "same",
        "createdAt" : "2018-02-13T15:12:01Z",
        "updatedAt" : "2018-06-09T03:06:35Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "e458cfe02ccb3b816f86d8025c03da67eae3ff0d",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +207,211 @@func (le *LeaderElector) renew(ctx context.Context) {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\twait.Until(func() {\n\t\ttimeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline)"
  }
]