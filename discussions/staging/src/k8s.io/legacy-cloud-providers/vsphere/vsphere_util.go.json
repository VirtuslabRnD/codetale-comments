[
  {
    "id" : "de2bf5cb-14cd-402a-b939-af861c3cf421",
    "prId" : 96689,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96689#pullrequestreview-534035958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f430aae0-38e4-4d90-af12-618cf7db3564",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "I'm trying to understand where/how we would learn the name of an unknown node. It doesn't look to me like you are listing all VMs in a datacenter (that would be a lot!)",
        "createdAt" : "2020-11-19T01:41:57Z",
        "updatedAt" : "2020-11-19T11:32:03Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "9cded0ea-8e27-4f5e-b0da-ec1b4bec71f1",
        "parentId" : "f430aae0-38e4-4d90-af12-618cf7db3564",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I will add some additional details and I updated PR description. Basically this is a follow up of https://github.com/kubernetes/kubernetes/pull/96224, because I found a bug where if a node has no pods with volumes scheduled on it, we don't run `VerifyVolumesAreAttached` on that node and as a result any dangling volume left on such node is never detached.\r\n\r\nSo strictly speaking this PR isn't detaching volumes from \"unknown\" nodes but more like nodes which aren't in attach-detach controller's cache (because there are no pods with volume on those nodes).\r\n",
        "createdAt" : "2020-11-19T02:22:38Z",
        "updatedAt" : "2020-11-19T11:32:03Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "5447dde0-3074-4b61-bdf4-8eacac540340",
        "parentId" : "f430aae0-38e4-4d90-af12-618cf7db3564",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Also, I am only listing VMs that are still part of k8s cluster.",
        "createdAt" : "2020-11-19T02:34:26Z",
        "updatedAt" : "2020-11-19T11:32:03Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a323279a287fdd73c6f3424c4ea3ca3b999ed1f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +623,627 @@\t\t// if given node is not in node volume map\n\t\tif !vs.vsphereVolumeMap.CheckForNode(nodeName) {\n\t\t\tnodeInfo, err := vs.nodeManager.GetNodeInfo(nodeName)\n\t\t\tif err != nil {\n\t\t\t\tklog.V(4).Infof(\"Failed to get node info: %+v. err: %+v\", nodeInfo.vm, err)"
  },
  {
    "id" : "bc29b8ef-7d85-4107-bb88-341ad364de2a",
    "prId" : 96689,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96689#pullrequestreview-552312673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2135c46-0ce7-4a7a-a2bd-930597f1f7c5",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "I don't like throwing away errors like this, it should get propagated to the caller.",
        "createdAt" : "2020-11-19T10:19:31Z",
        "updatedAt" : "2020-11-19T11:32:03Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "04db712b-527e-43d4-a098-71ea69820526",
        "parentId" : "a2135c46-0ce7-4a7a-a2bd-930597f1f7c5",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "you mean propagate upto ADC reconciler? But since this reconciliation is asynchronous and is not part of a user action, we can't report this as event. So even ADC is going to just log the error and nothing else.  ",
        "createdAt" : "2020-11-19T11:27:47Z",
        "updatedAt" : "2020-11-19T11:32:03Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "5551fa84-bc10-4eb3-b8c3-770765b83ff9",
        "parentId" : "a2135c46-0ce7-4a7a-a2bd-930597f1f7c5",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Yes, up to ADC (or whoever calls `DisksAreAttached`), so if we want to report errors later, we just process `DisksAreAttached` errors. BTW, `DisksAreAttached` itself already collects / reports errors from goroutines when checking the \"known\" `nodeVolumes` so you can copy it from there.",
        "createdAt" : "2020-12-03T10:12:34Z",
        "updatedAt" : "2020-12-03T10:12:34Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "3f7e56a4-34a9-4ac9-8bae-6431d05b052e",
        "parentId" : "a2135c46-0ce7-4a7a-a2bd-930597f1f7c5",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "I can see the point that the called did not ask for check of unrelated nodes and is not interested in their errors.\r\n\r\n",
        "createdAt" : "2020-12-15T10:32:08Z",
        "updatedAt" : "2020-12-15T10:32:08Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a323279a287fdd73c6f3424c4ea3ca3b999ed1f",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +641,645 @@\t\t\terr := vs.checkNodeDisks(ctx, nodeNames)\n\t\t\tif err != nil {\n\t\t\t\tklog.Errorf(\"Failed to check disk attached for nodes: %+v. err: %+v\", nodes, err)\n\t\t\t}\n\t\t\twg.Done()"
  }
]