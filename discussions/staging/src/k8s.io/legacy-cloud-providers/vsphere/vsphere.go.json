[
  {
    "id" : "e5578204-442e-4cb0-885b-981e44bee1d5",
    "prId" : 102892,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102892#pullrequestreview-686593782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3a45665-8539-465f-bb24-d430e167e68f",
        "parentId" : null,
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "Does it matter what the error was? At least one of the possible errors seems to be a failure to parse the vmDiskPath. Would you still want to throw an error in that case?",
        "createdAt" : "2021-06-16T19:50:02Z",
        "updatedAt" : "2021-06-16T19:50:03Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "230a1dcf-2ff5-4aa6-bf0b-871201d8af74",
        "parentId" : "a3a45665-8539-465f-bb24-d430e167e68f",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Validating anything here could be a regression and hence I am leaving the error alone and using original disk path present in PV if we can't get canonical path. It is upto backend then to decide if it can attach the disk.  This still gets us the fix we wanted in original PR.",
        "createdAt" : "2021-06-17T17:09:20Z",
        "updatedAt" : "2021-06-17T17:09:21Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "eadfe46e036b43c4326f4d5ad07b4123feff132b",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +940,944 @@\t\tcanonicalPath, pathFetchErr := getcanonicalVolumePath(ctx, vm.Datacenter, vmDiskPath)\n\t\tif canonicalPath != \"\" && pathFetchErr == nil {\n\t\t\tvmDiskPath = canonicalPath\n\t\t}\n"
  },
  {
    "id" : "b15d571a-968e-4b30-adde-de7c4e56e34e",
    "prId" : 100054,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100054#pullrequestreview-611086108",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b2c917e-2853-4f97-bfa6-958fd5e78f09",
        "parentId" : null,
        "authorId" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "body" : "`found` is true already? ",
        "createdAt" : "2021-03-11T00:24:10Z",
        "updatedAt" : "2021-03-16T16:21:51Z",
        "lastEditedBy" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "tags" : [
        ]
      },
      {
        "id" : "48e29d07-e5f3-4902-8fec-79bd93a49059",
        "parentId" : "6b2c917e-2853-4f97-bfa6-958fd5e78f09",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "yes, we found a candiate datastore that matches. How is that a problem?",
        "createdAt" : "2021-03-11T16:08:19Z",
        "updatedAt" : "2021-03-16T16:21:52Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "b6144838-be99-4fa8-a993-edafdf9e4545",
        "parentId" : "6b2c917e-2853-4f97-bfa6-958fd5e78f09",
        "authorId" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "body" : "What i meant is `found` is already true by line 1418",
        "createdAt" : "2021-03-12T17:29:31Z",
        "updatedAt" : "2021-03-16T16:21:52Z",
        "lastEditedBy" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "tags" : [
        ]
      },
      {
        "id" : "b09fbeda-b244-4afa-8753-376401bc406d",
        "parentId" : "6b2c917e-2853-4f97-bfa6-958fd5e78f09",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "ahh. okay I just copied this from old code. :-) I think it should have no impact on final outcome. ",
        "createdAt" : "2021-03-12T19:26:42Z",
        "updatedAt" : "2021-03-16T16:21:52Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3cc5a437ffb55329ac7a847a3a66aa528f5c383",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1415,1419 @@\t\t\t\t\t\tif datastoreInfo, found = candidateDatastores[sharedDs.Info.Url]; found {\n\t\t\t\t\t\t\tklog.V(4).Infof(\"Datastore validation succeeded\")\n\t\t\t\t\t\t\tfound = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}"
  },
  {
    "id" : "870fc181-2635-4913-9fc9-0bf1710a8d66",
    "prId" : 98546,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98546#pullrequestreview-586967858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "70f632b5-8742-4e21-a19f-1e7689bff853",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "So we no longer support multiple vcenters but we still support multiple datacenters spread multiple \"zones\" right? ",
        "createdAt" : "2021-02-09T20:16:14Z",
        "updatedAt" : "2021-02-20T00:31:21Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "cb61a99e-cf35-40fd-98d0-f5126abe939f",
        "parentId" : "70f632b5-8742-4e21-a19f-1e7689bff853",
        "authorId" : "c13045f9-cfc0-48e5-80df-ee48ddaa9fdc",
        "body" : "Yes, we do support multiple datacenters. We have validated migration to CSI with zone/region based deployment.\r\n",
        "createdAt" : "2021-02-09T20:20:18Z",
        "updatedAt" : "2021-02-20T00:31:21Z",
        "lastEditedBy" : "c13045f9-cfc0-48e5-80df-ee48ddaa9fdc",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e91ac26fbda2a2e82e56f809edfc8136384e83d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +414,418 @@\t\tif len(cfg.VirtualCenter) > 1 {\n\t\t\tklog.Warning(\"Multi vCenter support is deprecated. vSphere CSI Driver does not support Kubernetes nodes spread across multiple vCenter servers. Please consider moving all Kubernetes nodes to single vCenter server\")\n\t\t}\n\n\t\tfor vcServer, vcConfig := range cfg.VirtualCenter {"
  },
  {
    "id" : "e247f491-a946-4935-be92-ab3c1eccc7c7",
    "prId" : 96224,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96224#pullrequestreview-525645904",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e25825e-d28d-4aed-85ec-96af9722b3ec",
        "parentId" : null,
        "authorId" : "47dfb7ab-db92-411d-a70b-1dc70d2a9420",
        "body" : "if !ok, we may want to update the vsphereVolumeMap? (or) are we leaving for the next reconciler loop to update?",
        "createdAt" : "2020-11-06T18:39:11Z",
        "updatedAt" : "2020-11-09T20:39:34Z",
        "lastEditedBy" : "47dfb7ab-db92-411d-a70b-1dc70d2a9420",
        "tags" : [
        ]
      },
      {
        "id" : "b70c0781-6adf-4e5e-aa79-78274a1573a2",
        "parentId" : "9e25825e-d28d-4aed-85ec-96af9722b3ec",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Yeah we do not want every detach to scan all VMs in the cluster, so we are leaving this for next reconciler loop to update the volumemap.",
        "createdAt" : "2020-11-07T11:12:31Z",
        "updatedAt" : "2020-11-09T20:39:34Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c4c5ab691a41a29283cf72ae0d17c6f202cd755",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +961,965 @@\t\t// where it is not needed.\n\t\texistingNode, ok := vs.vsphereVolumeMap.CheckForVolume(vmDiskPath)\n\t\tif ok {\n\t\t\tattached, newVolumePath, diskAttachedError := vs.DiskIsAttached(vmDiskPath, existingNode)\n\t\t\t// if disk is attached somewhere else then we can throw a dangling error"
  },
  {
    "id" : "10195a74-62f1-4483-a94c-413f9c8ae61a",
    "prId" : 96224,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96224#pullrequestreview-526636957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0ec38ae-1b3a-47d9-8ddd-260a4953d0a9",
        "parentId" : null,
        "authorId" : "c13045f9-cfc0-48e5-80df-ee48ddaa9fdc",
        "body" : "it would be good if we can move logic in this code block in the `if err != nil {` block written on top of this code block.",
        "createdAt" : "2020-11-09T18:44:17Z",
        "updatedAt" : "2020-11-09T20:39:34Z",
        "lastEditedBy" : "c13045f9-cfc0-48e5-80df-ee48ddaa9fdc",
        "tags" : [
        ]
      },
      {
        "id" : "a80f3848-abcd-4afe-bd39-28a9b8c3022a",
        "parentId" : "a0ec38ae-1b3a-47d9-8ddd-260a4953d0a9",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "it can go either way but I somewhat prefer the current style so I have left it as it is. ",
        "createdAt" : "2020-11-09T20:41:20Z",
        "updatedAt" : "2020-11-09T20:41:20Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c4c5ab691a41a29283cf72ae0d17c6f202cd755",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +956,960 @@\t}\n\tklog.V(4).Infof(\"AttachDisk executed for node %s and volume %s with diskUUID %s. Err: %s\", convertToString(nodeName), vmDiskPath, diskUUID, err)\n\tif err != nil {\n\t\t// if attach failed, we should check if disk is attached somewhere else. This can happen for several reasons\n\t\t// and throwing a dangling volume error here will allow attach-detach controller to detach disk from a node"
  },
  {
    "id" : "297fd430-d412-49db-8d7f-7da63cdfb7f7",
    "prId" : 95447,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95447#pullrequestreview-505978979",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7dabdf6b-f2fc-408d-973e-f1e41c97bddb",
        "parentId" : null,
        "authorId" : "47dfb7ab-db92-411d-a70b-1dc70d2a9420",
        "body" : "Doesn't kubevols/disk.vmdk require similar canonical volumepath conversion? How is that handled today?",
        "createdAt" : "2020-10-09T20:32:59Z",
        "updatedAt" : "2020-10-13T01:58:51Z",
        "lastEditedBy" : "47dfb7ab-db92-411d-a70b-1dc70d2a9420",
        "tags" : [
        ]
      },
      {
        "id" : "871da910-5b9d-450a-b199-18d540200333",
        "parentId" : "7dabdf6b-f2fc-408d-973e-f1e41c97bddb",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "for dynamically provisioned volumes - canonical path is saved in PV and hence gets used for detach and attach. ",
        "createdAt" : "2020-10-09T20:45:35Z",
        "updatedAt" : "2020-10-13T01:58:51Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "562777160190e424c28a48e3987e9e6b20e6d90e",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +1045,1049 @@\n\t\tvolPath = vclib.RemoveStorageClusterORFolderNameFromVDiskPath(volPath)\n\t\tcanonicalPath, pathFetchErr := getcanonicalVolumePath(ctx, vm.Datacenter, volPath)\n\t\t// if canonicalPath is not empty string and pathFetchErr is nil then we can use canonical path to perform detach\n\t\tif canonicalPath != \"\" && pathFetchErr == nil {"
  },
  {
    "id" : "92ffa683-06e1-4e64-8d7a-07046b218a12",
    "prId" : 95447,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95447#pullrequestreview-506791187",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd2faece-6d61-4e2c-bb1e-742834f148d5",
        "parentId" : null,
        "authorId" : "c13045f9-cfc0-48e5-80df-ee48ddaa9fdc",
        "body" : "Can we fail the attach operation if we fail to get canonicalVolumePath and pathFetchErr is no nil?",
        "createdAt" : "2020-10-12T17:14:18Z",
        "updatedAt" : "2020-10-13T01:58:51Z",
        "lastEditedBy" : "c13045f9-cfc0-48e5-80df-ee48ddaa9fdc",
        "tags" : [
        ]
      }
    ],
    "commit" : "562777160190e424c28a48e3987e9e6b20e6d90e",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +1047,1051 @@\t\tcanonicalPath, pathFetchErr := getcanonicalVolumePath(ctx, vm.Datacenter, volPath)\n\t\t// if canonicalPath is not empty string and pathFetchErr is nil then we can use canonical path to perform detach\n\t\tif canonicalPath != \"\" && pathFetchErr == nil {\n\t\t\tvolPath = canonicalPath\n\t\t}"
  },
  {
    "id" : "cab20cb7-e713-47cc-b2be-12d315689c04",
    "prId" : 90836,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90836#pullrequestreview-409297523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c304431e-8bc2-4b59-87a0-13db18f18e80",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "all informers will get an added event at startup, which means we'll always trigger RediscoverNodes... is that intentional?",
        "createdAt" : "2020-05-11T13:29:32Z",
        "updatedAt" : "2020-06-16T13:14:18Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "5fea64ce-6474-4c39-ad32-9722de42d66f",
        "parentId" : "c304431e-8bc2-4b59-87a0-13db18f18e80",
        "authorId" : "f0d74c44-88a9-4517-b04d-d11da56c6036",
        "body" : "That's the issue faced by QE at some point. If some of the configuration is dedicated to the secret content, and there is another manager resource updating cluster configuration on config's content change, this part was left over, as nothing would recognize the secret being `added/updated/deleted`",
        "createdAt" : "2020-05-11T15:45:13Z",
        "updatedAt" : "2020-06-16T13:14:18Z",
        "lastEditedBy" : "f0d74c44-88a9-4517-b04d-d11da56c6036",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e0555446238b2dfe45805babc2b6982565c293d",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +1518,1522 @@\n// Notification handler when credentials secret is added.\nfunc (vs *VSphere) SecretAdded(obj interface{}) {\n\tsecret, ok := obj.(*v1.Secret)\n\tif secret == nil || !ok {"
  },
  {
    "id" : "a9dad889-f26b-4745-b923-6f8058ac4aa2",
    "prId" : 90836,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90836#pullrequestreview-409398781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "218c2336-cb64-4fe6-ab91-9ab00a1b64b8",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "returning here without retry means a single node lookup failure will leave the remaining nodes in the list with stale credentials, right?",
        "createdAt" : "2020-05-11T14:59:12Z",
        "updatedAt" : "2020-06-16T13:14:18Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "d4245aa1-d88e-4ee2-86c9-5aa6abe0227a",
        "parentId" : "218c2336-cb64-4fe6-ab91-9ab00a1b64b8",
        "authorId" : "f0d74c44-88a9-4517-b04d-d11da56c6036",
        "body" : "Having and error here means that it failed during couple of retries while trying to (re)discover node, and update VM state, so this logic is already handled.",
        "createdAt" : "2020-05-11T17:54:54Z",
        "updatedAt" : "2020-06-16T13:14:18Z",
        "lastEditedBy" : "f0d74c44-88a9-4517-b04d-d11da56c6036",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e0555446238b2dfe45805babc2b6982565c293d",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +1561,1565 @@\terr := vs.nodeManager.refreshNodes()\n\tif err != nil {\n\t\tklog.Errorf(\"failed to rediscover nodes: %v\", err)\n\t}\n}"
  }
]