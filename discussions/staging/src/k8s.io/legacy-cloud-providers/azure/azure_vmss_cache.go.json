[
  {
    "id" : "f7367c39-9bed-461b-b993-06a7163df0ce",
    "prId" : 100110,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100110#pullrequestreview-630326248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50263c1e-897c-4018-bac3-20b45252a0da",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Unit tests for this specific case would be useful I think",
        "createdAt" : "2021-04-07T17:59:18Z",
        "updatedAt" : "2021-04-07T17:59:18Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      }
    ],
    "commit" : "8850c8c7d90608abf1a4b456db1f995923a18c48",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +332,336 @@\tcachedNodes := cached.(availabilitySetEntry).nodeNames\n\t// if the node is not in the cache, assume the node has joined after the last cache refresh and attempt to refresh the cache.\n\tif !cachedNodes.Has(nodeName) {\n\t\tklog.V(2).Infof(\"Node %s has joined the cluster since the last VM cache refresh, refreshing the cache\", nodeName)\n\t\tcached, err = ss.availabilitySetNodesCache.Get(availabilitySetNodesKey, azcache.CacheReadTypeForceRefresh)"
  },
  {
    "id" : "3bd3046c-e083-41fc-83a6-d0bc5fe43fa0",
    "prId" : 100110,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100110#pullrequestreview-635116103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9764e315-da62-4f9c-a614-21c46e5de1c3",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Would this mean we refresh the whole node cache for every new VM in the cluster? Is that expected?",
        "createdAt" : "2021-04-07T18:16:13Z",
        "updatedAt" : "2021-04-07T18:16:16Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "a1d73e7a-45d8-48fb-ad59-fe6826b1573c",
        "parentId" : "9764e315-da62-4f9c-a614-21c46e5de1c3",
        "authorId" : "c3c31161-e4c7-4bbd-ba7d-17ef3c134c5b",
        "body" : "tldr: yes. It's a trade-off we can't get both accuracy and performance but this minimizes the damage by adding a node cache.\r\n\r\nSee discussion in https://github.com/kubernetes-sigs/cloud-provider-azure/pull/537#discussion_r589134547",
        "createdAt" : "2021-04-14T00:06:45Z",
        "updatedAt" : "2021-04-14T00:06:46Z",
        "lastEditedBy" : "c3c31161-e4c7-4bbd-ba7d-17ef3c134c5b",
        "tags" : [
        ]
      }
    ],
    "commit" : "8850c8c7d90608abf1a4b456db1f995923a18c48",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +334,338 @@\tif !cachedNodes.Has(nodeName) {\n\t\tklog.V(2).Infof(\"Node %s has joined the cluster since the last VM cache refresh, refreshing the cache\", nodeName)\n\t\tcached, err = ss.availabilitySetNodesCache.Get(availabilitySetNodesKey, azcache.CacheReadTypeForceRefresh)\n\t\tif err != nil {\n\t\t\treturn false, err"
  },
  {
    "id" : "3e8e5a5e-b877-40b8-b8f0-79c1bfae2935",
    "prId" : 89002,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89002#pullrequestreview-372486240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39ccb31d-4713-4746-a2b3-798a9cd7be44",
        "parentId" : null,
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "in this loop, `scaleSet ` does not change, it's read-only",
        "createdAt" : "2020-03-10T11:55:05Z",
        "updatedAt" : "2020-03-10T12:01:17Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "4feec9fa-3487-457c-8209-50f7473f79ac",
        "parentId" : "39ccb31d-4713-4746-a2b3-798a9cd7be44",
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "looks like you mean this code:\r\n```\r\nvmss:       &scaleSet,\r\n```",
        "createdAt" : "2020-03-10T11:56:45Z",
        "updatedAt" : "2020-03-10T11:56:46Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "a2038c02-d048-4eb5-b0a5-6920e99b6b9b",
        "parentId" : "39ccb31d-4713-4746-a2b3-798a9cd7be44",
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "you are correct:\r\nhttps://github.com/golang/go/wiki/CommonMistakes#using-reference-to-loop-iterator-variable",
        "createdAt" : "2020-03-10T11:59:27Z",
        "updatedAt" : "2020-03-10T12:00:00Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "ea7ed2a2-89dc-4e29-860b-78dc7a96f419",
        "parentId" : "39ccb31d-4713-4746-a2b3-798a9cd7be44",
        "authorId" : "b1268fde-49e8-4791-9493-3255e7dba7ac",
        "body" : "yes, you get it.",
        "createdAt" : "2020-03-10T12:00:20Z",
        "updatedAt" : "2020-03-10T12:00:20Z",
        "lastEditedBy" : "b1268fde-49e8-4791-9493-3255e7dba7ac",
        "tags" : [
        ]
      },
      {
        "id" : "6c62b84c-9590-45ad-98ce-914b3168d3ec",
        "parentId" : "39ccb31d-4713-4746-a2b3-798a9cd7be44",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "Good catch, thanks for the fix",
        "createdAt" : "2020-03-11T05:38:26Z",
        "updatedAt" : "2020-03-11T05:38:27Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce3644ca593db0d8f87babfc29804add1c9f05dc",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +75,79 @@\n\t\t\tfor i := range allScaleSets {\n\t\t\t\tscaleSet := allScaleSets[i]\n\t\t\t\tif scaleSet.Name == nil || *scaleSet.Name == \"\" {\n\t\t\t\t\tklog.Warning(\"failed to get the name of VMSS\")"
  },
  {
    "id" : "5f0a595b-ba59-42af-a6d8-a0c20b67f36a",
    "prId" : 87635,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87635#pullrequestreview-351662787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c860e47-e989-41e4-a80c-61797b27a71f",
        "parentId" : null,
        "authorId" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "body" : "What happens if the vm that's in deleting state goes to failed state? Is that a possibility?\r\n\r\nIf that's possible wouldn't we want to keep the node as `NotReady` in the cluster, so the user is aware of the issue with the node? ",
        "createdAt" : "2020-01-29T18:59:21Z",
        "updatedAt" : "2020-01-29T18:59:21Z",
        "lastEditedBy" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "tags" : [
        ]
      },
      {
        "id" : "9bfa5d58-0115-4164-84dc-6418d20aa507",
        "parentId" : "1c860e47-e989-41e4-a80c-61797b27a71f",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "the deleting VM won't go to running again in any cases, hence it's safe to remove it from the cluster.",
        "createdAt" : "2020-01-30T02:10:18Z",
        "updatedAt" : "2020-01-30T02:10:18Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      },
      {
        "id" : "5a7b1202-464f-45ae-9745-921c72ba8b11",
        "parentId" : "1c860e47-e989-41e4-a80c-61797b27a71f",
        "authorId" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "body" : "ok, thanks! ",
        "createdAt" : "2020-01-31T17:34:24Z",
        "updatedAt" : "2020-01-31T17:34:24Z",
        "lastEditedBy" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d70e650ce36704544ed48dd88a753141ae47f1f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +166,170 @@\t\t\t\t\t// set cache entry to nil when the VM is under deleting.\n\t\t\t\t\tif vm.VirtualMachineScaleSetVMProperties != nil &&\n\t\t\t\t\t\tstrings.EqualFold(to.String(vm.VirtualMachineScaleSetVMProperties.ProvisioningState), string(compute.ProvisioningStateDeleting)) {\n\t\t\t\t\t\tklog.V(4).Infof(\"VMSS virtualMachine %q is under deleting, setting its cache to nil\", computerName)\n\t\t\t\t\t\tvmssVMCacheEntry.virtualMachine = nil"
  },
  {
    "id" : "bbcd03fa-fdd0-4d25-854f-535e3f333224",
    "prId" : 87531,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87531#pullrequestreview-348314520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd330246-b914-4839-9481-2b635cc08a09",
        "parentId" : null,
        "authorId" : "405e6d07-e29d-4d39-a352-7a5b887f25b5",
        "body" : "ðŸ‘ ",
        "createdAt" : "2020-01-25T07:33:13Z",
        "updatedAt" : "2020-01-27T21:41:53Z",
        "lastEditedBy" : "405e6d07-e29d-4d39-a352-7a5b887f25b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e2d7153a133b44053d3c244e7ac4ddde3d63b9e6",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +166,170 @@\n\t\t\t\t\tif _, exists := oldCache[computerName]; exists {\n\t\t\t\t\t\tdelete(oldCache, computerName)\n\t\t\t\t\t}\n\t\t\t\t}"
  },
  {
    "id" : "fd72a97b-4962-49e7-a5cd-41783aba262b",
    "prId" : 86049,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86049#pullrequestreview-329896404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "312be0fb-3de4-4bb7-afdb-d2f647b91dd0",
        "parentId" : null,
        "authorId" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "body" : "With the changes that have gone in to [allow unsafe read](https://github.com/kubernetes/kubernetes/pull/83685), the number of ARM calls are reduced. This means we could experiment here by reducing the TTL back to 1 or 2min instead of keeping at 10min. Can we run a test to see what's the impact in reducing this to 1 or 2min instead of keeping it at 10min?",
        "createdAt" : "2019-12-09T17:52:57Z",
        "updatedAt" : "2019-12-09T17:52:58Z",
        "lastEditedBy" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "tags" : [
        ]
      },
      {
        "id" : "84d24762-85d3-4a22-8da3-b0edc635d20b",
        "parentId" : "312be0fb-3de4-4bb7-afdb-d2f647b91dd0",
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "> With the changes that have gone in to [allow unsafe read](https://github.com/kubernetes/kubernetes/pull/83685), the number of ARM calls are reduced. This means we could experiment here by reducing the TTL back to 1 or 2min instead of keeping at 10min. Can we run a test to see what's the impact in reducing this to 1 or 2min instead of keeping it at 10min?\r\n\r\n@aramase this is a good idea, since you have done such tests before, could you do that on your env again? And we could reduce the TTL back to 1min in master branch first, if necessary, then we could decide whether to cherry-pick.\r\nCurrent TTL is 10min, as I could find out, it could cause attach disk time cost to 10min in 1.13.12+, 1.14.8+, change back to 1min could mitigate such potential issue. thanks.",
        "createdAt" : "2019-12-10T13:39:56Z",
        "updatedAt" : "2019-12-10T13:39:56Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "fa1bdfb5-9647-474c-9629-0a9248964a46",
        "parentId" : "312be0fb-3de4-4bb7-afdb-d2f647b91dd0",
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "filed a issue: https://github.com/kubernetes/kubernetes/issues/86125 to track",
        "createdAt" : "2019-12-10T14:56:02Z",
        "updatedAt" : "2019-12-10T14:56:02Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf32a1a5f3d857ca50222d7676f2f71bd2cfd3ae",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +41,45 @@\n\tavailabilitySetNodesCacheTTL = 15 * time.Minute\n\tvmssTTL                      = 10 * time.Minute\n\tvmssVirtualMachinesTTL       = 10 * time.Minute\n)"
  }
]