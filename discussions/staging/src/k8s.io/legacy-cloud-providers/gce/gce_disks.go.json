[
  {
    "id" : "e6e41a59-79c3-45cb-abba-6d8ec4012c3c",
    "prId" : 98700,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98700#pullrequestreview-581835599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53627db8-beb7-4cb6-95ac-eb6ca64808ab",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "This is used by the admission controller? I think the pv object could be user-created, so we may need to still handle beta labels.",
        "createdAt" : "2021-02-02T22:38:12Z",
        "updatedAt" : "2021-02-03T03:14:13Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e76f60d2b82529c4ce26be99a424ebe6c7b361a3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +516,520 @@\t// If the zone is already labeled, honor the hint\n\tname := pv.Spec.GCEPersistentDisk.PDName\n\tzone := pv.Labels[v1.LabelTopologyZone]\n\tif zone == \"\" {\n\t\tzone = pv.Labels[v1.LabelFailureDomainBetaZone]"
  },
  {
    "id" : "c2215d80-0b7e-4eab-9f82-7cc55285ef51",
    "prId" : 98700,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98700#pullrequestreview-582879339",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29153b5d-d697-4025-bb0a-aa135d833e50",
        "parentId" : null,
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "FYI, We are no longer accepting enhancements on legacy-cloud-providers. Any such enhancements must be done out of tree. As this is clearly just a bug fix going to let it through.",
        "createdAt" : "2021-02-03T22:58:08Z",
        "updatedAt" : "2021-02-03T22:58:08Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "e76f60d2b82529c4ce26be99a424ebe6c7b361a3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +516,520 @@\t// If the zone is already labeled, honor the hint\n\tname := pv.Spec.GCEPersistentDisk.PDName\n\tzone := pv.Labels[v1.LabelTopologyZone]\n\tif zone == \"\" {\n\t\tzone = pv.Labels[v1.LabelFailureDomainBetaZone]"
  },
  {
    "id" : "98cb1e72-2c45-4521-b3b6-7f1de0b2b040",
    "prId" : 79897,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79897#pullrequestreview-259204484",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "102db3d5-d04a-4371-ba9f-13c591589c12",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "there are quite some loop functions  in this BulkDisksAreAttached function, I am wondering whether we can reduce some to improve the performance.",
        "createdAt" : "2019-07-08T22:44:59Z",
        "updatedAt" : "2019-07-19T17:41:59Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "b09ca2a3-1c51-4ec5-969c-351bb39eaa12",
        "parentId" : "102db3d5-d04a-4371-ba9f-13c591589c12",
        "authorId" : "b0cf1b2c-d8eb-4f06-a458-6bb60906f8cf",
        "body" : "While there are a lot of for loops, the run time should still be O(numNodes + numDisks). Some of the loops are a bit tricky - despite being a double for loop it still looks over only as many items as there are disks or nodes. \r\n\r\nAssuming that the runtime for getInstancesByName is constant. What's really going to slow it down is probably making API queries if getInstancesByName's heuristic is not good.",
        "createdAt" : "2019-07-08T23:25:52Z",
        "updatedAt" : "2019-07-19T17:41:59Z",
        "lastEditedBy" : "b0cf1b2c-d8eb-4f06-a458-6bb60906f8cf",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ee5861fb93902cec1a1beeff4f70338ee9bbeaf",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +625,629 @@}\n\n// BulkDisksAreAttached is a batch function to check if all corresponding disks are attached to the\n// nodes specified with nodeName.\nfunc (g *Cloud) BulkDisksAreAttached(diskByNodes map[types.NodeName][]string) (map[types.NodeName]map[string]bool, error) {"
  },
  {
    "id" : "9e6b55e3-1641-48ab-9fda-cd120feb9b75",
    "prId" : 79897,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79897#pullrequestreview-264539627",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecdcd4d6-9a8e-43ab-90e9-4aba207fa249",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "It seems we only use disk names of the return value from Cloud#getFoundInstanceByNames .\r\nInstead of mapping to []*compute.AttachedDisk, listedInstanceNamesToDisks can map to Set of disk names which verifyDisksAttachedToNode can directly utilize .\r\n\r\n@jingxu97 @hantaowang what do you think ?",
        "createdAt" : "2019-07-21T14:22:16Z",
        "updatedAt" : "2019-07-21T14:24:59Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ee5861fb93902cec1a1beeff4f70338ee9bbeaf",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +635,639 @@\t// List all instances with the given instance names\n\t// Then for each instance listed, add the disks attached to that instance to a map\n\tlistedInstances, err := g.getFoundInstanceByNames(instanceNames)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error listing instances: %v\", err)"
  }
]