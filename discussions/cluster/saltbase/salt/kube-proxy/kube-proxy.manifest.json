[
  {
    "id" : "a85894c1-f86e-4a5e-99d4-86790f50bfcf",
    "prId" : 46266,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46266#pullrequestreview-39643046",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40960490-48d6-4ffd-9934-4264bad21c0b",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This will not work in GCI. You also need to add appropriate if in:\r\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh#L822\r\nand\r\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/gce/container-linux/configure-helper.sh#L615",
        "createdAt" : "2017-05-23T05:47:34Z",
        "updatedAt" : "2017-05-25T03:33:25Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "2856fde23b509700eacd73ddfded5d6c617f3244",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +33,37 @@\n# test_args should always go last to overwrite prior configuration\n{% set params = log_level + \" \" + throttles + \" \" + feature_gates + \" \" + test_args -%}\n\n{% set container_env = \"\" -%}"
  },
  {
    "id" : "55011021-7186-4835-8c25-42fc213cb79d",
    "prId" : 46266,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46266#pullrequestreview-40222697",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5557b11d-4f0f-4820-9faf-4f5e5909e3e0",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Can we start with smaller min-sync-period now? E.g. 5s?\r\nAfter thinking about it, i think this change may surprise people with small cluster where everything worked really fast.\r\n\r\nIf we decide to increase it later, it would be super trivial change to do.",
        "createdAt" : "2017-05-23T05:51:46Z",
        "updatedAt" : "2017-05-25T03:33:25Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "60e6301d-a669-4370-a914-714603be2191",
        "parentId" : "5557b11d-4f0f-4820-9faf-4f5e5909e3e0",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "In general the bursts should allow changes to happen very quickly most of the time, and throttling kicks in only in larger clusters.  At least, that's what I expect...",
        "createdAt" : "2017-05-24T03:34:39Z",
        "updatedAt" : "2017-05-25T03:33:25Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "a1231066-b89e-411a-ac10-8ba0258efbc0",
        "parentId" : "5557b11d-4f0f-4820-9faf-4f5e5909e3e0",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "The scenario I was afraid about is that you are setting bursts=2.\r\n\r\nSo if we are creating a replication controller with 3 replicas, if endpoint controller will be fast enough that it will be adding pods to Endpoints object one by one, the first two will be added quickly, but for the last we will need to wait 10s more.\r\n\r\nI'm not sure how big problem it is though, so maybe it fine to leave now. We can tune later.",
        "createdAt" : "2017-05-24T10:17:31Z",
        "updatedAt" : "2017-05-25T03:33:25Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "916de3e3-9e2f-4344-aebd-c592531364e7",
        "parentId" : "5557b11d-4f0f-4820-9faf-4f5e5909e3e0",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "we can make this even more complex and add a short holdoff (e.g. 1sec).  I'd rather do that as a followup, if needed.  Or simpler, make bursts higher.  Here's my thinking - if the iptables update is fast, a short burst isn't a big deal.  If the update is slow, there will be a lot of time between runs anyway to accumulate new pods.  So maybe setting burst to 4 or is better - we could do it as a follow up to make that a flag...",
        "createdAt" : "2017-05-25T03:29:29Z",
        "updatedAt" : "2017-05-25T03:33:25Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "ae5e1a7e-91dd-4eae-97ee-42a01c288f61",
        "parentId" : "5557b11d-4f0f-4820-9faf-4f5e5909e3e0",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I agree that bumping bursts sounds like a good option. I'm obviously fine with doing it in a followup - we will see what will be the impact of this change.",
        "createdAt" : "2017-05-25T07:23:16Z",
        "updatedAt" : "2017-05-25T07:23:16Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "2856fde23b509700eacd73ddfded5d6c617f3244",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +30,34 @@{% endif -%}\n\n{% set throttles = \"--iptables-sync-period=1m --iptables-min-sync-period=10s\" -%}\n\n# test_args should always go last to overwrite prior configuration"
  },
  {
    "id" : "a5736c81-932a-4686-ae74-5c3ccd35061a",
    "prId" : 46259,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46259#pullrequestreview-41672158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1360e94f-e8b8-45a9-920e-43a4fc06df01",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "you should not need 2 volumes, just use subPath in the volumeMount",
        "createdAt" : "2017-06-02T00:51:29Z",
        "updatedAt" : "2017-06-04T06:53:48Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a380e8831b42e88cbd15fc31efddcd287098d88",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +110,114 @@      path: /run\n    name: run\n  - hostPath:\n      path: /run/xtables.lock\n    name: iptableslock"
  },
  {
    "id" : "c6079792-5940-43d5-8939-ec7b1c4be580",
    "prId" : 41700,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41700#pullrequestreview-23334379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3aa3eb70-a2eb-4585-b34d-5fc24a8c5ff7",
        "parentId" : null,
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "I just tried this. No sure why it failed for me. `$$` works for me tough. But e2e is not complaining. \r\n\r\n```\r\n$ cat  /proc/$$$/oom_score_adj\r\ncat: /proc/29756$/oom_score: No such file or directory\r\n```\r\n\r\n\r\n",
        "createdAt" : "2017-02-21T23:44:43Z",
        "updatedAt" : "2017-02-21T23:50:57Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      },
      {
        "id" : "7cdbed00-e677-4f98-921f-121ac69ac31d",
        "parentId" : "3aa3eb70-a2eb-4585-b34d-5fc24a8c5ff7",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Kubelet evaluates the command and that's why there is the extra '$'. ",
        "createdAt" : "2017-02-22T17:28:19Z",
        "updatedAt" : "2017-02-22T17:28:19Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "a812a037-d74e-4cb5-b1a8-88baacea2c78",
        "parentId" : "3aa3eb70-a2eb-4585-b34d-5fc24a8c5ff7",
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "I see. ",
        "createdAt" : "2017-02-22T21:14:30Z",
        "updatedAt" : "2017-02-22T21:14:30Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      }
    ],
    "commit" : "80cbb1be5d407150b67bb6c601ddc42cd64d40b4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +60,64 @@    - /bin/sh\n    - -c\n    - echo -998 > /proc/$$$/oom_score_adj && kube-proxy {{api_servers_with_port}} {{kubeconfig}} {{cluster_cidr}} --resource-container=\"\" {{params}} 1>>/var/log/kube-proxy.log 2>&1\n    securityContext:\n      privileged: true"
  },
  {
    "id" : "7880cec6-8143-4065-a70a-adc55f550ab1",
    "prId" : 38836,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38836#pullrequestreview-13254724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29c95147-8302-4e9b-b4f6-049123ab65f4",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Can you change the memory pressure to the resource pressure / starvation? The node eviction manager works for both memory and disk today. ",
        "createdAt" : "2016-12-15T23:44:14Z",
        "updatedAt" : "2016-12-16T03:01:28Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "0250e195-97ba-4420-8335-89278a044c7b",
        "parentId" : "29c95147-8302-4e9b-b4f6-049123ab65f4",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Hmm, I don't think anything in the kubelet reads the disk pressure node condition, does it? \r\n\r\nif we're out of disk we will still not admit it (https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet.go#L1677) and if we admit it in that case it'll probably make the disk situation worse? \r\n\r\nThe current fix is only around the admit method of the eviction manager, and only based on the NodeMemoryPressure condition. \r\n",
        "createdAt" : "2016-12-15T23:52:56Z",
        "updatedAt" : "2016-12-16T03:01:28Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "fc0e2630-34d9-4c9c-84cf-1ed8b1a024d4",
        "parentId" : "29c95147-8302-4e9b-b4f6-049123ab65f4",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "This implementation looks like it just considers critical pods for memory pressure, though.",
        "createdAt" : "2016-12-16T00:04:35Z",
        "updatedAt" : "2016-12-16T03:01:28Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "c175e713-25a7-45b9-bdb4-6576ba491fad",
        "parentId" : "29c95147-8302-4e9b-b4f6-049123ab65f4",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "I agree with @bprashanth, let's focus on the memory situation first.",
        "createdAt" : "2016-12-16T00:38:27Z",
        "updatedAt" : "2016-12-16T03:01:28Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "c0ab20c8-5c9b-4988-9e5e-e7cc7d078700",
        "parentId" : "29c95147-8302-4e9b-b4f6-049123ab65f4",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "@bprashanth Node eviction manager take actions on both memory and disk (space and inode). I know your current implementation only consider memory pressure condition, not include disk. I am ok as is here. ",
        "createdAt" : "2016-12-16T01:01:33Z",
        "updatedAt" : "2016-12-16T03:01:28Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7409e00380b349ec971d089363ad7ff22b37c76",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +40,44 @@  namespace: kube-system\n  # This annotation lowers the possibility that kube-proxy gets evicted when the\n  # node is under memory pressure, and prioritizes it for admission, even if\n  # the node is under memory pressure.\n  # Note that kube-proxy runs as a static pod so this annotation does NOT have"
  },
  {
    "id" : "6d508543-fb18-420e-907f-fd37d6a3bfb5",
    "prId" : 24429,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e7253f5-a948-4408-8e2b-0cfecf4c3caf",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "/cc @andyzheng0831 - does this need a matching trusty change (that would also need to be cherry picked)?\n",
        "createdAt" : "2016-04-19T14:03:39Z",
        "updatedAt" : "2016-04-20T04:39:51Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "3c5a3704-3353-424f-97b6-35316ad7181c",
        "parentId" : "3e7253f5-a948-4408-8e2b-0cfecf4c3caf",
        "authorId" : "34f7b744-302a-4493-8e2d-95681077e4a8",
        "body" : "Yes, as it is a new variable in this manifest.\n",
        "createdAt" : "2016-04-19T16:58:08Z",
        "updatedAt" : "2016-04-20T04:39:51Z",
        "lastEditedBy" : "34f7b744-302a-4493-8e2d-95681077e4a8",
        "tags" : [
        ]
      }
    ],
    "commit" : "760568796f97613f4c538299525f88bb586ffb39",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +15,19 @@  {% set test_args=pillar['kubeproxy_test_args'] %}\n{% endif -%}\n{% set cluster_cidr = \"\" -%}\n{% if pillar['cluster_cidr'] is defined -%}\n  {% set cluster_cidr=\" --cluster-cidr=\" + pillar['cluster_cidr'] %}"
  },
  {
    "id" : "70ec235c-c3f9-40b9-9fc5-094ad37191fd",
    "prId" : 22022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5257d089-35ac-4041-be8f-ef08a8bd6144",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "We should set `limits` ideally. \n",
        "createdAt" : "2016-02-25T23:20:56Z",
        "updatedAt" : "2016-02-25T23:39:39Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "a753a643-cf6c-4bb3-8e32-bd36b29540b1",
        "parentId" : "5257d089-35ac-4041-be8f-ef08a8bd6144",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "I don't want to curtail kube-proxy, I want it to get what it needs (plus I'm not 100% sure how much it will need). \n\nBut I guess it doesn't make sense giving unlimited cpu to the proxy if nothing can run on the node, and if it bumps into the limit it's because something else with higher priority came along, and it'll just be slow, unlike memory. \n",
        "createdAt" : "2016-02-25T23:25:00Z",
        "updatedAt" : "2016-02-25T23:39:39Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "db35c478-3a46-49c8-8951-0cce647d1e4a",
        "parentId" : "5257d089-35ac-4041-be8f-ef08a8bd6144",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "`requests` are OK for now I guess.\nDepending on where we go with the QoS policies, this might make kube-proxy\nhave less priority. But given that it is a DaemonSet, I guess the policies\nwill be thwarted anyways.\n\nOn Thu, Feb 25, 2016 at 3:25 PM, Prashanth B notifications@github.com\nwrote:\n\n> In cluster/saltbase/salt/kube-proxy/kube-proxy.manifest\n> https://github.com/kubernetes/kubernetes/pull/22022#discussion_r54182925\n> :\n> \n> > @@ -31,6 +31,9 @@ spec:\n> >    containers:\n> > - name: kube-proxy\n> >    image: {{pillar['kube_docker_registry']}}/kube-proxy:{{pillar['kube-proxy_docker_tag']}}\n> >   -    resources:\n> >   -      requests:\n> \n> I don't want to curtail kube-proxy, I want it to get what it needs (plus\n> I'm not 100% sure how much it will need).\n> \n> But I guess it doesn't make sense giving unlimited cpu to the proxy if\n> nothing can run on the node, and if it bumps into the limit it's because\n> something else with higher priority came along, and it'll just be slow,\n> unlike memory.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/22022/files#r54182925.\n",
        "createdAt" : "2016-02-25T23:30:41Z",
        "updatedAt" : "2016-02-25T23:39:39Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "5eeea914-0631-4dc8-8b89-f99fe97d688f",
        "parentId" : "5257d089-35ac-4041-be8f-ef08a8bd6144",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Yeah none of the master components set limits, for the same reason. If kube-proxy on a node is really slow it could impact a cluster spanning service, so I think we should leave it as request for now and revisit if required.\n",
        "createdAt" : "2016-02-25T23:40:15Z",
        "updatedAt" : "2016-02-25T23:40:15Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d47d2dcd854bf5778678ae004798ea148600a7e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +33,37 @@    image: {{pillar['kube_docker_registry']}}/kube-proxy:{{pillar['kube-proxy_docker_tag']}}\n    resources:\n      requests:\n        cpu: {{ cpurequest }}\n    command:"
  }
]