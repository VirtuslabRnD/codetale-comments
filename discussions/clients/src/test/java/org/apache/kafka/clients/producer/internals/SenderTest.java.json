[
  {
    "id" : "6b673331-7547-4117-bec5-4875714ff571",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-133335457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98348165-e7fc-4da7-bb57-ca7019f9d020",
        "parentId" : null,
        "authorId" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "body" : "We should add some sort of `accumulator.hasInflightBatches` method, and then check that it returns false here. This would check that expired batches are not reenqueued, which is logic added in this patch.",
        "createdAt" : "2018-06-29T06:00:58Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "tags" : [
        ]
      },
      {
        "id" : "5b9fcac8-3703-43ea-ad54-e1b554650fe4",
        "parentId" : "98348165-e7fc-4da7-bb57-ca7019f9d020",
        "authorId" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "body" : "added the method `public List<ProducerBatch> inFlightBatches(TopicPartition tp)`  in `RecordAccumulator`, and updated the test with two assertions: \r\n\r\n       line 1912: assertEquals(\"Expect one in-flight batch in accumulator\", 1, accumulator.inFlightBatches(tp0).size());\r\n        .....\r\n       line 1920: assertEquals(\"Expect zero in-flight batch in accumulator\", 0, accumulator.inFlightBatches(tp0).size());\r\n",
        "createdAt" : "2018-06-29T18:36:38Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 463,
    "diffHunk" : "@@ -1,1 +1958,1962 @@            fail(\"The expired batch should throw a TimeoutException\");\n        } catch (ExecutionException e) {\n            assertTrue(e.getCause() instanceof TimeoutException);\n        }\n    }"
  },
  {
    "id" : "967f486f-075e-411a-a2c4-597513b51a3a",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-133388419",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2313699a-f42c-413a-a5c6-2514833a7570",
        "parentId" : null,
        "authorId" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "body" : "How is this different from the test `testExpiryOfFirstBatchShouldNotCauseUnresolvedSequencesIfFutureBatchesSucceed`?",
        "createdAt" : "2018-06-29T06:03:16Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "tags" : [
        ]
      },
      {
        "id" : "0a42eae0-4864-4400-a121-f400dc802ba4",
        "parentId" : "2313699a-f42c-413a-a5c6-2514833a7570",
        "authorId" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "body" : "`testExpiryOfFirstBatchShouldNotCauseUnresolvedSequencesIfFutureBatchesSucceed` initialize `sender` with `guaranteeMessageOrder = false`, while `testWhenFirstBatchExpireNoSendSecondBatchIfGuaranteeOrder` initialize sender with `guaranteeMessageOrder = true`.  The `inflightBatches` size is different when we set the parameter to true/false. ",
        "createdAt" : "2018-06-29T21:53:35Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 468,
    "diffHunk" : "@@ -1,1 +1963,1967 @@\n    @Test\n    public void testWhenFirstBatchExpireNoSendSecondBatchIfGuaranteeOrder() throws InterruptedException {\n        long deliveryTimeoutMs = 1500L;\n        setupWithTransactionState(null, true, null);"
  },
  {
    "id" : "c8dda5e8-739c-4689-bf18-afe98dda6aef",
    "prId" : 6388,
    "prUrl" : "https://github.com/apache/kafka/pull/6388#pullrequestreview-211964916",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da048431-6559-4376-b7f8-a6bdc57791ca",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Could we have a separate test case which covers the `forceClose()` path. We can assert that we do not wait for the producerId and all pending sends are aborted.",
        "createdAt" : "2019-03-07T17:55:46Z",
        "updatedAt" : "2019-03-07T19:34:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "01709308-73dd-4d99-84d0-71de1b58e417",
        "parentId" : "da048431-6559-4376-b7f8-a6bdc57791ca",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "@hachikuji Thanks for the review. Addressed the review comment.",
        "createdAt" : "2019-03-07T19:05:16Z",
        "updatedAt" : "2019-03-07T19:34:57Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      }
    ],
    "commit" : "6130ee8b623e1331cfddd92eb9ce3c7447ccd019",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1199,1203 @@\n    @Test\n    public void testCloseWithProducerIdReset() throws Exception {\n        final long producerId = 343434L;\n        TransactionManager transactionManager = new TransactionManager();"
  },
  {
    "id" : "5508e03b-671c-4a93-b19a-56d32e0e9009",
    "prId" : 6388,
    "prUrl" : "https://github.com/apache/kafka/pull/6388#pullrequestreview-211979149",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6026e049-8aa1-4608-b054-210621e6d821",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we assert that this future completed below?",
        "createdAt" : "2019-03-07T19:22:31Z",
        "updatedAt" : "2019-03-07T19:34:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "bdef0b20-0133-4131-9a7c-05cee89aaaea",
        "parentId" : "6026e049-8aa1-4608-b054-210621e6d821",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "Done.",
        "createdAt" : "2019-03-07T19:35:37Z",
        "updatedAt" : "2019-03-07T19:35:37Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      }
    ],
    "commit" : "6130ee8b623e1331cfddd92eb9ce3c7447ccd019",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +1252,1256 @@        Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), \"key\".getBytes(),\n            \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n        Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), \"key\".getBytes(),\n            \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n        sender.run(time.milliseconds());  // connect and send."
  },
  {
    "id" : "4434be1f-ac26-40a6-b5f1-804e29f571a0",
    "prId" : 7389,
    "prUrl" : "https://github.com/apache/kafka/pull/7389#pullrequestreview-357663129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22083a2b-3179-4b64-aad2-8b277e42a115",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Do we also want to check the status of `tp0` again for the new append?",
        "createdAt" : "2020-02-07T00:25:49Z",
        "updatedAt" : "2020-02-15T22:01:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "31d8b75c-ad58-42bf-a959-64381b7957ba",
        "parentId" : "22083a2b-3179-4b64-aad2-8b277e42a115",
        "authorId" : "e88252f3-8879-452a-8098-afb39cb614dc",
        "body" : "I don't think there's anything meaningful to check. We could look at the sequence number, but we're going to reset that when we abort anyway. This test is just to check that failing to resolve sequences sends us to an abortable state, not a fatal one. But if you have something specific in mind, I can add it.",
        "createdAt" : "2020-02-12T17:54:03Z",
        "updatedAt" : "2020-02-15T22:01:39Z",
        "lastEditedBy" : "e88252f3-8879-452a-8098-afb39cb614dc",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe88cc1ea6addd6c8597829bdfa41d9d1d68cef2",
    "line" : 195,
    "diffHunk" : "@@ -1,1 +1167,1171 @@        // Loop once and confirm that the transaction manager does not enter a fatal error state\n        sender.runOnce();\n        assertTrue(txnManager.hasAbortableError());\n    }\n"
  },
  {
    "id" : "9c1f5d43-fd0c-4930-8d7f-2097ecb4105d",
    "prId" : 7920,
    "prUrl" : "https://github.com/apache/kafka/pull/7920#pullrequestreview-343631352",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a82c3ef-3ced-4a0b-8593-e1266b6f3a3b",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "qq: Why is this necessary?",
        "createdAt" : "2020-01-15T22:59:49Z",
        "updatedAt" : "2020-01-23T00:20:39Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "bcf82d5d-7d1c-49e3-9f8f-54489254a892",
        "parentId" : "6a82c3ef-3ced-4a0b-8593-e1266b6f3a3b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Note that we blackout the node for 10ms above. Previously we were relying on the backoff logic in `maybeWaitForProducerId` for the node to be ready again. Now the test needs time to be advanced externally since we cannot rely on `client.poll` advancing it.",
        "createdAt" : "2020-01-15T23:50:15Z",
        "updatedAt" : "2020-01-23T00:20:39Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "7db17cf6-cbc5-4893-ad9c-49ef93570dd2",
        "parentId" : "6a82c3ef-3ced-4a0b-8593-e1266b6f3a3b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "SG.",
        "createdAt" : "2020-01-16T01:45:41Z",
        "updatedAt" : "2020-01-23T00:20:39Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e45a5aec8150398eaec3a4aefaa37a07c65239f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1158,1162 @@\n        // We should now clear the old producerId and get a new one in a single run loop.\n        time.sleep(10);\n        prepareAndReceiveInitProducerId(producerId + 1, Errors.NONE);\n        assertTrue(transactionManager.hasProducerId(producerId + 1));"
  }
]