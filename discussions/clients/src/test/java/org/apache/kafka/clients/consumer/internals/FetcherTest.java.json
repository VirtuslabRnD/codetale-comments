[
  {
    "id" : "834fb771-26ae-4ccf-8801-a46c394294c6",
    "prId" : 5495,
    "prUrl" : "https://github.com/apache/kafka/pull/5495#pullrequestreview-155527369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24f223f4-c5b2-419f-840b-58e76c103359",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Might be useful to have some assertions which verify fetch progress. Like perhaps we can assert the last fetched offset after we complete `fetchesRemaining`?",
        "createdAt" : "2018-09-13T19:01:09Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a0108f50-7427-4981-a7ea-995743014cd0",
        "parentId" : "24f223f4-c5b2-419f-840b-58e76c103359",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@hachikuji Thanks for the review, updated the test.",
        "createdAt" : "2018-09-14T14:44:05Z",
        "updatedAt" : "2018-09-14T14:44:05Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b379bf19b941c9e97d243f8fc2175147a9fab6e8",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +2622,2626 @@            }\n            if (fetcher.hasCompletedFetches()) {\n                Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetcher.fetchedRecords();\n                if (!fetchedRecords.isEmpty()) {\n                    fetchesRemaining.decrementAndGet();"
  },
  {
    "id" : "4406030b-c7d7-491d-9b57-eff49236c064",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa0a1958-15b0-4a0b-8b45-09bb0532cd39",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Use `recordsByPartition.values().forEach`?",
        "createdAt" : "2019-02-12T10:07:38Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 740,
    "diffHunk" : "@@ -1,1 +1116,1120 @@        List<ConsumerRecord<byte[], byte[]>> fetchedRecords = new ArrayList<>();\n        Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> recordsByPartition = fetchedRecords();\n        for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n            fetchedRecords.addAll(records);\n"
  },
  {
    "id" : "b1f6f939-f518-45a0-8ee1-6b3cfdc20320",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acadb4a5-aaf7-44a9-a907-0be7dd7bd6a8",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "As above, use forEach?",
        "createdAt" : "2019-02-12T10:07:55Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 753,
    "diffHunk" : "@@ -1,1 +1126,1130 @@        try {\n            recordsByPartition = fetchedRecords();\n            for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n                fetchedRecords.addAll(records);\n        } catch (OffsetOutOfRangeException oor) {"
  },
  {
    "id" : "c563f7ee-9e29-4dc3-ad76-cb5b0d8d8430",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d22b1710-96f6-4714-8470-0d5b494df3f6",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "As above, use forEach?",
        "createdAt" : "2019-02-12T10:08:27Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 777,
    "diffHunk" : "@@ -1,1 +1151,1155 @@            try {\n                recordsByPartition = fetchedRecords();\n                for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n                    fetchedRecords.addAll(records);\n            } catch (KafkaException e) {"
  },
  {
    "id" : "70505ab6-04b9-4f1d-a592-5d9959e1260a",
    "prId" : 6582,
    "prUrl" : "https://github.com/apache/kafka/pull/6582#pullrequestreview-240260974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2abd7ad4-64e9-485d-a574-22f20be18811",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess there was no choice but to carefully tailor this test in order to hit the bug. We have to do it, but the downside is that its scope is narrow and may be difficult to keep it relevant as the code evolves. Anyway, hopefully at some point we'll get the time to move all network IO to the background thread and then we can simplify a lot of this.",
        "createdAt" : "2019-05-21T19:44:29Z",
        "updatedAt" : "2019-05-21T19:44:40Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6690a7ea679ddf3e186b7facb20ca074ec2757f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2942,2946 @@\n    @Test\n    public void testFetcherSessionEpochUpdate() throws Exception {\n        buildFetcher(2);\n"
  },
  {
    "id" : "af6f138b-ecef-4f9c-934a-3bf052475f9d",
    "prId" : 6988,
    "prUrl" : "https://github.com/apache/kafka/pull/6988#pullrequestreview-269724484",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "623988b2-0f32-417d-a730-80f6dc03b660",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do we have a test case which covers the case where the user seeks to a new offset while a partition is paused with data available to return? In this case, we expect the data to be discarded when the partition is resumed.",
        "createdAt" : "2019-08-01T06:38:13Z",
        "updatedAt" : "2019-08-01T16:03:51Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ecf3dbca-cdf4-4888-877c-dc2465a0fba1",
        "parentId" : "623988b2-0f32-417d-a730-80f6dc03b660",
        "authorId" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "body" : "I did a pass over `FetcherTest` and didn't see this scenario exactly.  I added another test called `testFetchDiscardedAfterPausedPartitionResumedAndSeekedToNewOffset` (a bit of a mouthful).  Does it create the scenario you were thinking?",
        "createdAt" : "2019-08-01T16:04:53Z",
        "updatedAt" : "2019-08-01T16:04:53Z",
        "lastEditedBy" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9b47190841af24dc79beed51837946c119995d5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +898,902 @@    }\n\n    @Test\n    public void testFetchOnCompletedFetchesForPausedAndResumedPartitions() {\n        buildFetcher();"
  },
  {
    "id" : "e8bae282-46c2-4057-ad3f-880fa2d3216b",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-356899065",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90fcbcea-e6d5-4971-8872-114d1a89ab1c",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "This is the new test being added, other changes are just side cleanup.",
        "createdAt" : "2020-02-11T18:40:13Z",
        "updatedAt" : "2020-02-14T18:32:29Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +2414,2418 @@    }\n\n    @Test\n    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,"
  },
  {
    "id" : "961cb5cb-8325-4851-97b3-c3549c31f61f",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-358554974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "201945ef-d770-43a6-b425-62df369aa590",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.",
        "createdAt" : "2020-02-13T07:26:18Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e459b9ca-4f85-4ddf-819f-01250b4318b6",
        "parentId" : "201945ef-d770-43a6-b425-62df369aa590",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I double checked, the reason was because the second try will not match the newLeader so that it actually disconnects and ask for a metadata update. I have injected a fatal error if the client reconnects to the original leader.",
        "createdAt" : "2020-02-13T17:07:29Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "9898c7bc-da23-4dac-9646-1fca9d36ae4b",
        "parentId" : "201945ef-d770-43a6-b425-62df369aa590",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, maybe we need a separate test for the partial failure case? I am interested in verifying 1) that metadata update gets triggered after a partial failure, and 2) the retry does not request partitions that were fetched successfully. ",
        "createdAt" : "2020-02-13T19:23:24Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d7978b69-373e-47a7-bf92-2fe8b784d543",
        "parentId" : "201945ef-d770-43a6-b425-62df369aa590",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "For 1), as long as it's partition level error, it's a partial failure. For 2) I could check to see if there is a way.",
        "createdAt" : "2020-02-13T21:13:37Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +2415,2419 @@\n    @Test\n    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,"
  },
  {
    "id" : "9a4b2e6d-5749-483d-9e3b-b1155d81ad1c",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-358403810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f94a9b16-7165-45db-8d64-f6a0f08246c9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we add an assertion like the following to ensure that both requests were sent?\r\n```java\r\n            assertFalse(client.hasPendingResponses());\r\n```",
        "createdAt" : "2020-02-13T07:32:23Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b1158a54-d639-44ca-87db-137f8fab976f",
        "parentId" : "f94a9b16-7165-45db-8d64-f6a0f08246c9",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Add check of pending responses.",
        "createdAt" : "2020-02-13T17:17:35Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +2494,2498 @@\n            fetcher.close();\n        }\n    }\n"
  },
  {
    "id" : "b9912864-af2e-47e5-8532-124db5bb3463",
    "prId" : 8295,
    "prUrl" : "https://github.com/apache/kafka/pull/8295#pullrequestreview-465225360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81c5f0a0-bb79-4dc6-9940-29c8d5a68200",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "both partitions",
        "createdAt" : "2020-08-11T17:30:31Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78cb96fc13e5c337372b24b5ea50d7ade30485fc",
    "line" : 270,
    "diffHunk" : "@@ -1,1 +3623,3627 @@        // Unknown Offset\n        client.reset();\n        // Ensure metadata has both partition.\n        MetadataResponse initialMetadataUpdate = TestUtils.metadataUpdateWith(1, singletonMap(topicName, 1));\n        client.updateMetadata(initialMetadataUpdate);"
  },
  {
    "id" : "1749176c-d6fc-4888-b550-c3e5e0c53cd5",
    "prId" : 8295,
    "prUrl" : "https://github.com/apache/kafka/pull/8295#pullrequestreview-466778101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "474d90bf-41bf-4512-8591-ea33f1f45481",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Is this expected before the change in this PR?",
        "createdAt" : "2020-08-11T17:36:09Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "7afa45a9-337c-410b-8a92-ed5f7a46bb10",
        "parentId" : "474d90bf-41bf-4512-8591-ea33f1f45481",
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "I think so. I used the following test on trunk and it passes:\r\n```\r\n    @Test\r\n    public void testGetOffsetsForTimesWithUnknownOffsetV0() {\r\n        buildFetcher();\r\n        client.reset();\r\n        // Ensure metadata has both partition.\r\n        MetadataResponse initialMetadataUpdate = TestUtils.metadataUpdateWith(1, singletonMap(topicName, 1));\r\n        client.updateMetadata(initialMetadataUpdate);\r\n\r\n        Map<TopicPartition, ListOffsetResponse.PartitionData> partitionData = new HashMap<>();\r\n        partitionData.put(tp0, new ListOffsetResponse.PartitionData(Errors.NONE,\r\n                Collections.emptyList()));\r\n\r\n        client.prepareResponseFrom(new ListOffsetResponse(0, partitionData),\r\n                metadata.fetch().leaderFor(tp0));\r\n\r\n        Map<TopicPartition, Long> timestampToSearch = new HashMap<>();\r\n        timestampToSearch.put(tp0, 0L);\r\n        Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\r\n                fetcher.offsetsForTimes(timestampToSearch, time.timer(Long.MAX_VALUE));\r\n\r\n        assertTrue(offsetAndTimestampMap.containsKey(tp0));\r\n        assertNull(offsetAndTimestampMap.get(tp0));\r\n    }\r\n```",
        "createdAt" : "2020-08-13T13:48:02Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      }
    ],
    "commit" : "78cb96fc13e5c337372b24b5ea50d7ade30485fc",
    "line" : 264,
    "diffHunk" : "@@ -1,1 +3617,3621 @@\n    @Test\n    public void testGetOffsetsForTimesWithUnknownOffsetV0() {\n        buildFetcher();\n        // Empty map"
  },
  {
    "id" : "435ee64d-cade-46f7-9b7c-4f0ca1bf4b43",
    "prId" : 8457,
    "prUrl" : "https://github.com/apache/kafka/pull/8457#pullrequestreview-392522360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0107b208-3d42-479b-a0ab-c777ff73853e",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "So here we are checking that the request sent from validateOffsetsIfNeeded only includes the partitions whose leader is the current node? If the match fails do we get a nice junit assertion failure, or some funky mockito stack trace?",
        "createdAt" : "2020-04-13T20:31:04Z",
        "updatedAt" : "2020-04-13T20:31:21Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "476f630b-5627-4d0e-b334-34d84a5bec32",
        "parentId" : "0107b208-3d42-479b-a0ab-c777ff73853e",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "You would get a message like this:\r\n```\r\nRequest matcher did not match next-in-line request {replica_id=-1,topics=[{topic=test,partitions=[{partition=2,current_leader_epoch=5,leader_epoch=4}]}]} with prepared response (type=OffsetsForLeaderEpochResponse, , throttleTimeMs=0, epochEndOffsetsByPartition={test-2=EpochEndOffset{error=NONE, leaderEpoch=4, endOffset=0}})\r\n```\r\nwhich I think is reasonable.",
        "createdAt" : "2020-04-14T00:18:05Z",
        "updatedAt" : "2020-04-14T00:18:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "201a4ec4651149aa0150f622915da4fe760628ab",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +3597,3601 @@                public boolean matches(AbstractRequest body) {\n                    OffsetsForLeaderEpochRequest request = (OffsetsForLeaderEpochRequest) body;\n                    return expectedPartitions.equals(request.epochsByTopicPartition().keySet());\n                }\n            }, response, node);"
  },
  {
    "id" : "0836c0bb-8b1c-4ece-b8b6-210f169a072c",
    "prId" : 8486,
    "prUrl" : "https://github.com/apache/kafka/pull/8486#pullrequestreview-423195478",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e00d4e13-ee3b-4e06-8fa2-3a4b506f852c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. Can we assert the raised exception somehow? It's not clear to me that it is getting raised appropriately and we don't have any tests for it.",
        "createdAt" : "2020-06-03T02:29:43Z",
        "updatedAt" : "2020-06-05T01:54:11Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "520061dc-415e-4c1b-a996-39ec9a9fad1d",
        "parentId" : "e00d4e13-ee3b-4e06-8fa2-3a4b506f852c",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "As discussed offline, we plan to address this gap in a separate PR for https://issues.apache.org/jira/browse/KAFKA-10087 as some other bug to fix for log truncation exception propagation.",
        "createdAt" : "2020-06-03T03:33:50Z",
        "updatedAt" : "2020-06-05T01:54:11Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "87ca234c395d1d0b0566269c4cd299da936d2cb3",
    "line" : 312,
    "diffHunk" : "@@ -1,1 +3867,3871 @@        } else {\n            fetcher.validateOffsetsIfNeeded();\n            assertFalse(subscriptions.awaitingValidation(tp0));\n        }\n    }"
  },
  {
    "id" : "b75b6b5f-7fc4-4584-b0a0-b5e268df4d75",
    "prId" : 9630,
    "prUrl" : "https://github.com/apache/kafka/pull/9630#pullrequestreview-542621826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6387e10c-068f-4676-b405-80b4935d5c66",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Not for this patch, but all of this boilerplate we need to build the topic groupings gets annoying. It is such a common case that it might be worth writing a special type that lets the parser construct `Map<TopicPartition, Data>` directly since that is really what the code wants. Alternatively, maybe we could flatten the schemas and introduce compression.",
        "createdAt" : "2020-12-01T16:27:22Z",
        "updatedAt" : "2020-12-03T11:58:01Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "04fb84e4-9e91-4dd7-bece-fc60e8429344",
        "parentId" : "6387e10c-068f-4676-b405-80b4935d5c66",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "That would be nice, indeed! I have been thinking about this as well but did not have the time to tackle this yet. I have opened a JIRA to track this: https://issues.apache.org/jira/browse/KAFKA-10795.",
        "createdAt" : "2020-12-02T09:03:21Z",
        "updatedAt" : "2020-12-03T11:58:01Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a9f87a118c051bfc6e6a944ef21db79db9b5068",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +3737,3741 @@            expectedPartitions.forEach(tp -> {\n                OffsetForLeaderTopicResult topic = data.topics().find(tp.topic());\n                if (topic == null) {\n                    topic = new OffsetForLeaderTopicResult().setTopic(tp.topic());\n                    data.topics().add(topic);"
  },
  {
    "id" : "e0430cde-f751-4bd7-9ae6-1e68669a213e",
    "prId" : 9836,
    "prUrl" : "https://github.com/apache/kafka/pull/9836#pullrequestreview-563008200",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1e08d28-7cf2-4dc5-880b-3f80d301f842",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Just a consequence of moving the records inside of a response struct.",
        "createdAt" : "2021-01-06T22:29:02Z",
        "updatedAt" : "2021-01-27T20:08:58Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc3ec40f25900d2c25d7c1de1de6271cd0180cd5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +525,529 @@\n        // the first fetchedRecords() should return the first valid message\n        assertEquals(1, fetcher.fetchedRecords().records().get(tp0).size());\n        assertEquals(1, subscriptions.position(tp0).offset);\n"
  },
  {
    "id" : "713339d1-b41d-4c69-8994-074aff8fbb33",
    "prId" : 10006,
    "prUrl" : "https://github.com/apache/kafka/pull/10006#pullrequestreview-579700589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47d71956-1792-40bd-b3e3-3175b0e62096",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Why we don't need to set reset strategy here?",
        "createdAt" : "2021-01-30T00:12:58Z",
        "updatedAt" : "2021-02-01T17:37:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "a00dd347-f37f-460c-b0c7-59d301f4b4f0",
        "parentId" : "47d71956-1792-40bd-b3e3-3175b0e62096",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Because the test case sets it explicitly below.",
        "createdAt" : "2021-01-30T00:25:43Z",
        "updatedAt" : "2021-02-01T17:37:39Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c0deea2af77cb774fcb14a927c9058a9cf94d07",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +1769,1773 @@    @Test\n    public void testEarlierOffsetResetArrivesLate() {\n        buildFetcher();\n        assignFromUser(singleton(tp0));\n"
  }
]