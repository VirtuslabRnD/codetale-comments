[
  {
    "id" : "834fb771-26ae-4ccf-8801-a46c394294c6",
    "prId" : 5495,
    "prUrl" : "https://github.com/apache/kafka/pull/5495#pullrequestreview-155527369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24f223f4-c5b2-419f-840b-58e76c103359",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Might be useful to have some assertions which verify fetch progress. Like perhaps we can assert the last fetched offset after we complete `fetchesRemaining`?",
        "createdAt" : "2018-09-13T19:01:09Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a0108f50-7427-4981-a7ea-995743014cd0",
        "parentId" : "24f223f4-c5b2-419f-840b-58e76c103359",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@hachikuji Thanks for the review, updated the test.",
        "createdAt" : "2018-09-14T14:44:05Z",
        "updatedAt" : "2018-09-14T14:44:05Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b379bf19b941c9e97d243f8fc2175147a9fab6e8",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +2622,2626 @@            }\n            if (fetcher.hasCompletedFetches()) {\n                Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetcher.fetchedRecords();\n                if (!fetchedRecords.isEmpty()) {\n                    fetchesRemaining.decrementAndGet();"
  },
  {
    "id" : "4406030b-c7d7-491d-9b57-eff49236c064",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa0a1958-15b0-4a0b-8b45-09bb0532cd39",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Use `recordsByPartition.values().forEach`?",
        "createdAt" : "2019-02-12T10:07:38Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 740,
    "diffHunk" : "@@ -1,1 +1116,1120 @@        List<ConsumerRecord<byte[], byte[]>> fetchedRecords = new ArrayList<>();\n        Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> recordsByPartition = fetchedRecords();\n        for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n            fetchedRecords.addAll(records);\n"
  },
  {
    "id" : "b1f6f939-f518-45a0-8ee1-6b3cfdc20320",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acadb4a5-aaf7-44a9-a907-0be7dd7bd6a8",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "As above, use forEach?",
        "createdAt" : "2019-02-12T10:07:55Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 753,
    "diffHunk" : "@@ -1,1 +1126,1130 @@        try {\n            recordsByPartition = fetchedRecords();\n            for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n                fetchedRecords.addAll(records);\n        } catch (OffsetOutOfRangeException oor) {"
  },
  {
    "id" : "c563f7ee-9e29-4dc3-ad76-cb5b0d8d8430",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d22b1710-96f6-4714-8470-0d5b494df3f6",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "As above, use forEach?",
        "createdAt" : "2019-02-12T10:08:27Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 777,
    "diffHunk" : "@@ -1,1 +1151,1155 @@            try {\n                recordsByPartition = fetchedRecords();\n                for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n                    fetchedRecords.addAll(records);\n            } catch (KafkaException e) {"
  },
  {
    "id" : "70505ab6-04b9-4f1d-a592-5d9959e1260a",
    "prId" : 6582,
    "prUrl" : "https://github.com/apache/kafka/pull/6582#pullrequestreview-240260974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2abd7ad4-64e9-485d-a574-22f20be18811",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess there was no choice but to carefully tailor this test in order to hit the bug. We have to do it, but the downside is that its scope is narrow and may be difficult to keep it relevant as the code evolves. Anyway, hopefully at some point we'll get the time to move all network IO to the background thread and then we can simplify a lot of this.",
        "createdAt" : "2019-05-21T19:44:29Z",
        "updatedAt" : "2019-05-21T19:44:40Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6690a7ea679ddf3e186b7facb20ca074ec2757f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2942,2946 @@\n    @Test\n    public void testFetcherSessionEpochUpdate() throws Exception {\n        buildFetcher(2);\n"
  },
  {
    "id" : "af6f138b-ecef-4f9c-934a-3bf052475f9d",
    "prId" : 6988,
    "prUrl" : "https://github.com/apache/kafka/pull/6988#pullrequestreview-269724484",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "623988b2-0f32-417d-a730-80f6dc03b660",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do we have a test case which covers the case where the user seeks to a new offset while a partition is paused with data available to return? In this case, we expect the data to be discarded when the partition is resumed.",
        "createdAt" : "2019-08-01T06:38:13Z",
        "updatedAt" : "2019-08-01T16:03:51Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ecf3dbca-cdf4-4888-877c-dc2465a0fba1",
        "parentId" : "623988b2-0f32-417d-a730-80f6dc03b660",
        "authorId" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "body" : "I did a pass over `FetcherTest` and didn't see this scenario exactly.  I added another test called `testFetchDiscardedAfterPausedPartitionResumedAndSeekedToNewOffset` (a bit of a mouthful).  Does it create the scenario you were thinking?",
        "createdAt" : "2019-08-01T16:04:53Z",
        "updatedAt" : "2019-08-01T16:04:53Z",
        "lastEditedBy" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9b47190841af24dc79beed51837946c119995d5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +898,902 @@    }\n\n    @Test\n    public void testFetchOnCompletedFetchesForPausedAndResumedPartitions() {\n        buildFetcher();"
  }
]