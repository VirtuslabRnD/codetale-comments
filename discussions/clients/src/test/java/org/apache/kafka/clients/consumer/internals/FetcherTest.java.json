[
  {
    "id" : "834fb771-26ae-4ccf-8801-a46c394294c6",
    "prId" : 5495,
    "prUrl" : "https://github.com/apache/kafka/pull/5495#pullrequestreview-155527369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24f223f4-c5b2-419f-840b-58e76c103359",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Might be useful to have some assertions which verify fetch progress. Like perhaps we can assert the last fetched offset after we complete `fetchesRemaining`?",
        "createdAt" : "2018-09-13T19:01:09Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a0108f50-7427-4981-a7ea-995743014cd0",
        "parentId" : "24f223f4-c5b2-419f-840b-58e76c103359",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@hachikuji Thanks for the review, updated the test.",
        "createdAt" : "2018-09-14T14:44:05Z",
        "updatedAt" : "2018-09-14T14:44:05Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b379bf19b941c9e97d243f8fc2175147a9fab6e8",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +2622,2626 @@            }\n            if (fetcher.hasCompletedFetches()) {\n                Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetcher.fetchedRecords();\n                if (!fetchedRecords.isEmpty()) {\n                    fetchesRemaining.decrementAndGet();"
  },
  {
    "id" : "4406030b-c7d7-491d-9b57-eff49236c064",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa0a1958-15b0-4a0b-8b45-09bb0532cd39",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Use `recordsByPartition.values().forEach`?",
        "createdAt" : "2019-02-12T10:07:38Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 740,
    "diffHunk" : "@@ -1,1 +1116,1120 @@        List<ConsumerRecord<byte[], byte[]>> fetchedRecords = new ArrayList<>();\n        Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> recordsByPartition = fetchedRecords();\n        for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n            fetchedRecords.addAll(records);\n"
  },
  {
    "id" : "b1f6f939-f518-45a0-8ee1-6b3cfdc20320",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acadb4a5-aaf7-44a9-a907-0be7dd7bd6a8",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "As above, use forEach?",
        "createdAt" : "2019-02-12T10:07:55Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 753,
    "diffHunk" : "@@ -1,1 +1126,1130 @@        try {\n            recordsByPartition = fetchedRecords();\n            for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n                fetchedRecords.addAll(records);\n        } catch (OffsetOutOfRangeException oor) {"
  },
  {
    "id" : "c563f7ee-9e29-4dc3-ad76-cb5b0d8d8430",
    "prId" : 6221,
    "prUrl" : "https://github.com/apache/kafka/pull/6221#pullrequestreview-202565530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d22b1710-96f6-4714-8470-0d5b494df3f6",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "As above, use forEach?",
        "createdAt" : "2019-02-12T10:08:27Z",
        "updatedAt" : "2019-03-07T21:10:38Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "546247c314070a8e7ce30a9131eac2b1e9fe1139",
    "line" : 777,
    "diffHunk" : "@@ -1,1 +1151,1155 @@            try {\n                recordsByPartition = fetchedRecords();\n                for (List<ConsumerRecord<byte[], byte[]>> records : recordsByPartition.values())\n                    fetchedRecords.addAll(records);\n            } catch (KafkaException e) {"
  },
  {
    "id" : "70505ab6-04b9-4f1d-a592-5d9959e1260a",
    "prId" : 6582,
    "prUrl" : "https://github.com/apache/kafka/pull/6582#pullrequestreview-240260974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2abd7ad4-64e9-485d-a574-22f20be18811",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess there was no choice but to carefully tailor this test in order to hit the bug. We have to do it, but the downside is that its scope is narrow and may be difficult to keep it relevant as the code evolves. Anyway, hopefully at some point we'll get the time to move all network IO to the background thread and then we can simplify a lot of this.",
        "createdAt" : "2019-05-21T19:44:29Z",
        "updatedAt" : "2019-05-21T19:44:40Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6690a7ea679ddf3e186b7facb20ca074ec2757f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2942,2946 @@\n    @Test\n    public void testFetcherSessionEpochUpdate() throws Exception {\n        buildFetcher(2);\n"
  },
  {
    "id" : "af6f138b-ecef-4f9c-934a-3bf052475f9d",
    "prId" : 6988,
    "prUrl" : "https://github.com/apache/kafka/pull/6988#pullrequestreview-269724484",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "623988b2-0f32-417d-a730-80f6dc03b660",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do we have a test case which covers the case where the user seeks to a new offset while a partition is paused with data available to return? In this case, we expect the data to be discarded when the partition is resumed.",
        "createdAt" : "2019-08-01T06:38:13Z",
        "updatedAt" : "2019-08-01T16:03:51Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ecf3dbca-cdf4-4888-877c-dc2465a0fba1",
        "parentId" : "623988b2-0f32-417d-a730-80f6dc03b660",
        "authorId" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "body" : "I did a pass over `FetcherTest` and didn't see this scenario exactly.  I added another test called `testFetchDiscardedAfterPausedPartitionResumedAndSeekedToNewOffset` (a bit of a mouthful).  Does it create the scenario you were thinking?",
        "createdAt" : "2019-08-01T16:04:53Z",
        "updatedAt" : "2019-08-01T16:04:53Z",
        "lastEditedBy" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9b47190841af24dc79beed51837946c119995d5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +898,902 @@    }\n\n    @Test\n    public void testFetchOnCompletedFetchesForPausedAndResumedPartitions() {\n        buildFetcher();"
  },
  {
    "id" : "e8bae282-46c2-4057-ad3f-880fa2d3216b",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-356899065",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90fcbcea-e6d5-4971-8872-114d1a89ab1c",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "This is the new test being added, other changes are just side cleanup.",
        "createdAt" : "2020-02-11T18:40:13Z",
        "updatedAt" : "2020-02-14T18:32:29Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +2414,2418 @@    }\n\n    @Test\n    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,"
  },
  {
    "id" : "961cb5cb-8325-4851-97b3-c3549c31f61f",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-358554974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "201945ef-d770-43a6-b425-62df369aa590",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.",
        "createdAt" : "2020-02-13T07:26:18Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e459b9ca-4f85-4ddf-819f-01250b4318b6",
        "parentId" : "201945ef-d770-43a6-b425-62df369aa590",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I double checked, the reason was because the second try will not match the newLeader so that it actually disconnects and ask for a metadata update. I have injected a fatal error if the client reconnects to the original leader.",
        "createdAt" : "2020-02-13T17:07:29Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "9898c7bc-da23-4dac-9646-1fca9d36ae4b",
        "parentId" : "201945ef-d770-43a6-b425-62df369aa590",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, maybe we need a separate test for the partial failure case? I am interested in verifying 1) that metadata update gets triggered after a partial failure, and 2) the retry does not request partitions that were fetched successfully. ",
        "createdAt" : "2020-02-13T19:23:24Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d7978b69-373e-47a7-bf92-2fe8b784d543",
        "parentId" : "201945ef-d770-43a6-b425-62df369aa590",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "For 1), as long as it's partition level error, it's a partial failure. For 2) I could check to see if there is a way.",
        "createdAt" : "2020-02-13T21:13:37Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +2415,2419 @@\n    @Test\n    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,"
  },
  {
    "id" : "9a4b2e6d-5749-483d-9e3b-b1155d81ad1c",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-358403810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f94a9b16-7165-45db-8d64-f6a0f08246c9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we add an assertion like the following to ensure that both requests were sent?\r\n```java\r\n            assertFalse(client.hasPendingResponses());\r\n```",
        "createdAt" : "2020-02-13T07:32:23Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b1158a54-d639-44ca-87db-137f8fab976f",
        "parentId" : "f94a9b16-7165-45db-8d64-f6a0f08246c9",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Add check of pending responses.",
        "createdAt" : "2020-02-13T17:17:35Z",
        "updatedAt" : "2020-02-14T18:32:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +2494,2498 @@\n            fetcher.close();\n        }\n    }\n"
  },
  {
    "id" : "b9912864-af2e-47e5-8532-124db5bb3463",
    "prId" : 8295,
    "prUrl" : "https://github.com/apache/kafka/pull/8295#pullrequestreview-465225360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81c5f0a0-bb79-4dc6-9940-29c8d5a68200",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "both partitions",
        "createdAt" : "2020-08-11T17:30:31Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78cb96fc13e5c337372b24b5ea50d7ade30485fc",
    "line" : 270,
    "diffHunk" : "@@ -1,1 +3623,3627 @@        // Unknown Offset\n        client.reset();\n        // Ensure metadata has both partition.\n        MetadataResponse initialMetadataUpdate = TestUtils.metadataUpdateWith(1, singletonMap(topicName, 1));\n        client.updateMetadata(initialMetadataUpdate);"
  },
  {
    "id" : "1749176c-d6fc-4888-b550-c3e5e0c53cd5",
    "prId" : 8295,
    "prUrl" : "https://github.com/apache/kafka/pull/8295#pullrequestreview-466778101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "474d90bf-41bf-4512-8591-ea33f1f45481",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Is this expected before the change in this PR?",
        "createdAt" : "2020-08-11T17:36:09Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "7afa45a9-337c-410b-8a92-ed5f7a46bb10",
        "parentId" : "474d90bf-41bf-4512-8591-ea33f1f45481",
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "I think so. I used the following test on trunk and it passes:\r\n```\r\n    @Test\r\n    public void testGetOffsetsForTimesWithUnknownOffsetV0() {\r\n        buildFetcher();\r\n        client.reset();\r\n        // Ensure metadata has both partition.\r\n        MetadataResponse initialMetadataUpdate = TestUtils.metadataUpdateWith(1, singletonMap(topicName, 1));\r\n        client.updateMetadata(initialMetadataUpdate);\r\n\r\n        Map<TopicPartition, ListOffsetResponse.PartitionData> partitionData = new HashMap<>();\r\n        partitionData.put(tp0, new ListOffsetResponse.PartitionData(Errors.NONE,\r\n                Collections.emptyList()));\r\n\r\n        client.prepareResponseFrom(new ListOffsetResponse(0, partitionData),\r\n                metadata.fetch().leaderFor(tp0));\r\n\r\n        Map<TopicPartition, Long> timestampToSearch = new HashMap<>();\r\n        timestampToSearch.put(tp0, 0L);\r\n        Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\r\n                fetcher.offsetsForTimes(timestampToSearch, time.timer(Long.MAX_VALUE));\r\n\r\n        assertTrue(offsetAndTimestampMap.containsKey(tp0));\r\n        assertNull(offsetAndTimestampMap.get(tp0));\r\n    }\r\n```",
        "createdAt" : "2020-08-13T13:48:02Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      }
    ],
    "commit" : "78cb96fc13e5c337372b24b5ea50d7ade30485fc",
    "line" : 264,
    "diffHunk" : "@@ -1,1 +3617,3621 @@\n    @Test\n    public void testGetOffsetsForTimesWithUnknownOffsetV0() {\n        buildFetcher();\n        // Empty map"
  }
]