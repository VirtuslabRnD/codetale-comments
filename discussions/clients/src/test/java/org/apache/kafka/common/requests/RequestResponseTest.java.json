[
  {
    "id" : "31eb071e-879b-4bcf-ad13-af502159d766",
    "prId" : 5582,
    "prUrl" : "https://github.com/apache/kafka/pull/5582#pullrequestreview-163913547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ed992c0-2059-44bc-9f6b-827939fadd2d",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "why?",
        "createdAt" : "2018-10-10T10:56:47Z",
        "updatedAt" : "2018-10-25T19:47:00Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "bc9a5ff7-e8ff-469a-9f66-6618e7529967",
        "parentId" : "8ed992c0-2059-44bc-9f6b-827939fadd2d",
        "authorId" : "e0554c25-f6f3-4e49-a325-bcc5d4dc5fb2",
        "body" : "Good question!  There was a comment above that method that stated:\r\n```\r\n        // Check that we can serialize, deserialize and serialize again\r\n        // We don't check for equality or hashCode because it is likely to fail for any response containing a HashMap\r\n```\r\n I didn't think about it much; I just read that comment and figured that since I'm making a change to `SaslAuthenticateRequest` and `SaslAuthenticateResponse` and they don't contain a `HashMap` I could -- and should -- test for equality and hashCode.\r\n\r\nBut now that you ask, and I do spend the time to think about it, it seems that testing equality and hashCode doesn't provide the value I thought it would (and that the comment seemed to imply that it would except for the annoying tendency of a HashMap to screw up the results)!  All we would be testing for is to make sure the result of serializing a request to a `Struct` can be deserialized back to a request and then serialized again to an equivalent `Struct`.  In other words, it doesn't actually test that the serialization code (i.e. `SaslAuthenticateResponse.toStruct()`) is working perfectly -- the equality and hashCode tests will still succeed even if that code serializes a field incorrectly because the same field will be serialized incorrectly both times (for example).\r\n\r\nNote that incorrect serialization would presumably be caught indirectly via failure of other unit or integration tests.\r\n\r\nWhat maybe has to change here is the original comment.  Should I adjusted it?",
        "createdAt" : "2018-10-11T16:49:37Z",
        "updatedAt" : "2018-10-25T19:47:00Z",
        "lastEditedBy" : "e0554c25-f6f3-4e49-a325-bcc5d4dc5fb2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c30b80b2b29b7f5c86af21fd19fc520cfe798db",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +355,359 @@    private void checkRequest(AbstractRequest req, boolean checkEqualityAndHashCode) throws Exception {\n        // Check that we can serialize, deserialize and serialize again\n        // Check for equality and hashCode only if indicated\n        Struct struct = req.toStruct();\n        AbstractRequest deserialized = (AbstractRequest) deserialize(req, struct, req.version());"
  },
  {
    "id" : "aa17e73c-a9f3-40f3-b4aa-4a15d1eef4ed",
    "prId" : 6247,
    "prUrl" : "https://github.com/apache/kafka/pull/6247#pullrequestreview-205881739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23b9dbf8-d98a-43ab-9157-daf7f88c4c94",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "nit: Is there any reason to not use what you normally would when building the request - e.g `AlterConfigOp.OpType.APPEND.id()` ?",
        "createdAt" : "2019-02-18T13:26:57Z",
        "updatedAt" : "2019-04-13T16:04:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "4ba2551e-8403-41aa-ab19-552d886cea70",
        "parentId" : "23b9dbf8-d98a-43ab-9157-daf7f88c4c94",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "Thus is just to verify request serialization . Also we need to update import-control .xml to use AlterConfigOp here.",
        "createdAt" : "2019-02-19T16:23:02Z",
        "updatedAt" : "2019-04-13T16:04:28Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "7390e313-ddda-4441-9dc4-75b0bb84fbe1",
        "parentId" : "23b9dbf8-d98a-43ab-9157-daf7f88c4c94",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Makes sense",
        "createdAt" : "2019-02-20T17:00:55Z",
        "updatedAt" : "2019-04-13T16:04:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf81f6c6e266fe058527609c60fb163035b70509",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1491,1495 @@        AlterableConfig alterableConfig = new AlterableConfig()\n                .setName(\"retention.ms\")\n                .setConfigOperation((byte) 0)\n                .setValue(\"100\");\n        AlterableConfigSet alterableConfigs = new AlterableConfigSet();"
  },
  {
    "id" : "fee9f1bc-7ecd-4379-a339-dc24ae8eb527",
    "prId" : 7062,
    "prUrl" : "https://github.com/apache/kafka/pull/7062#pullrequestreview-264340123",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7db39d62-2274-45f4-af2e-b59fa2776fd5",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do you think this test case has much value? I'd be inclined to get rid of it.",
        "createdAt" : "2019-07-17T18:29:08Z",
        "updatedAt" : "2019-07-30T01:51:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "bd5996f9-20c4-4c5b-b7a5-370eb1e0d73a",
        "parentId" : "7db39d62-2274-45f4-af2e-b59fa2776fd5",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I think it's ok to maintain a test on the `toString()` behavior",
        "createdAt" : "2019-07-19T17:32:07Z",
        "updatedAt" : "2019-07-30T01:51:57Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8dd41a80739afc0801faae02e70aecedfbc0b75",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +734,738 @@        String allTopicPartitionsString = OffsetFetchRequest.Builder.allTopicPartitions(\"someGroup\").toString();\n\n        assertTrue(allTopicPartitionsString.contains(\"groupId='someGroup', topics=null\"));\n        String string = new OffsetFetchRequest.Builder(\"group1\",\n            Collections.singletonList(new TopicPartition(\"test11\", 1))).toString();"
  },
  {
    "id" : "b0f034bc-2674-45fd-b86c-820ace3af0fd",
    "prId" : 8295,
    "prUrl" : "https://github.com/apache/kafka/pull/8295#pullrequestreview-465225360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c38da23e-850b-40aa-8d9a-6183346ee6db",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Seems that this data construction logic has been reused in elsewhere (`FetcherTest`), we could get a helper like \r\n```\r\nListOffsetResponseData getSingletonResponseV0(TopicPartition, Errors, OldStyleOffsets);\r\nListOffsetResponseData getSingletonResponseV0(TopicPartition, Errors, Timestamp, Offset, leaderEpoch);\r\n```\r\nin the ListOffsetResponse to reuse.",
        "createdAt" : "2020-07-31T15:54:05Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "653a417b-7b5e-4b0b-93bc-4d60c61f67b9",
        "parentId" : "c38da23e-850b-40aa-8d9a-6183346ee6db",
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "I'm not sure about adding extra methods to `ListOffsetResponse` just to remove a few lines in tests.",
        "createdAt" : "2020-08-07T18:09:02Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      },
      {
        "id" : "3093e965-48c0-40c2-aa51-a98fff23aabf",
        "parentId" : "c38da23e-850b-40aa-8d9a-6183346ee6db",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "As my previous comment suggests, for the sake of encapsulation and reusability.",
        "createdAt" : "2020-08-11T17:44:41Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78cb96fc13e5c337372b24b5ea50d7ade30485fc",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +1268,1272 @@    private ListOffsetResponse createListOffsetResponse(int version) {\n        if (version == 0) {\n            ListOffsetResponseData data = new ListOffsetResponseData()\n                    .setTopics(Collections.singletonList(new ListOffsetTopicResponse()\n                            .setName(\"test\")"
  },
  {
    "id" : "3a55390e-838e-4740-8042-c66cb255016a",
    "prId" : 8295,
    "prUrl" : "https://github.com/apache/kafka/pull/8295#pullrequestreview-466834991",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d56c6de0-391b-4fc7-bf31-03ccae881c87",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Similar for topic request topic construction, let me know if you think we could refactor out a helper like `singletonRequestData(...)` in `ListOffsetRequest`",
        "createdAt" : "2020-07-31T15:54:55Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "9ce1b3b2-19a9-44d7-bac8-0c0464a81913",
        "parentId" : "d56c6de0-391b-4fc7-bf31-03ccae881c87",
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "Not entirely sure. It's only used 3 times so we're not going to save very much.",
        "createdAt" : "2020-08-07T18:11:26Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      },
      {
        "id" : "8af117fe-d5da-4927-9d5e-1b75bf982979",
        "parentId" : "d56c6de0-391b-4fc7-bf31-03ccae881c87",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Well, the purpose is more about encapsulation to reduce the import paths in this test class.",
        "createdAt" : "2020-08-11T17:43:53Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "137d5353-a472-4a54-bd42-26de565a0675",
        "parentId" : "d56c6de0-391b-4fc7-bf31-03ccae881c87",
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "I had a look but all 3 cases set different fields on `ListOffsetPartition`. So it's not very useful here.",
        "createdAt" : "2020-08-13T14:47:21Z",
        "updatedAt" : "2020-09-24T09:26:37Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      }
    ],
    "commit" : "78cb96fc13e5c337372b24b5ea50d7ade30485fc",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1227,1231 @@    private ListOffsetRequest createListOffsetRequest(int version) {\n        if (version == 0) {\n            ListOffsetTopic topic = new ListOffsetTopic()\n                    .setName(\"test\")\n                    .setPartitions(Arrays.asList(new ListOffsetPartition()"
  },
  {
    "id" : "65f862e1-2a75-43a7-ae11-b5cc51156a32",
    "prId" : 9433,
    "prUrl" : "https://github.com/apache/kafka/pull/9433#pullrequestreview-527162016",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ddf8d92-378c-4a7c-b884-64fd65262835",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "pardon me, why to make this change?",
        "createdAt" : "2020-11-10T12:17:48Z",
        "updatedAt" : "2020-11-19T17:01:23Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "f217e0b8-46ce-4dbf-b297-cd24b90a84cd",
        "parentId" : "5ddf8d92-378c-4a7c-b884-64fd65262835",
        "authorId" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "body" : "I wanted to ensure that for error code fields in all RPCs we were including NONE in the count. I would have factored out a method taking the error code to use if any of the rest of the tests depended on the actual error code used, but they don't. We'd need to revisit this if [KIP-636](https://cwiki.apache.org/confluence/display/KAFKA/KIP-636%3A+Make+RPC+error+codes+and+messages+tagged+fields) were ever accepted (since it would interfere with coverage of the serialization test), but it seems harmless for the time being.",
        "createdAt" : "2020-11-10T12:42:00Z",
        "updatedAt" : "2020-11-19T17:01:23Z",
        "lastEditedBy" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "tags" : [
        ]
      }
    ],
    "commit" : "468d16f26c0f2ec32fb10f1e1bb003bcc62612d4",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1995,1999 @@            new CreateAclsResponseData.AclCreationResult(),\n            new CreateAclsResponseData.AclCreationResult()\n                .setErrorCode(Errors.NONE.code())\n                .setErrorMessage(\"Foo bar\"))));\n    }"
  },
  {
    "id" : "768c8c0d-126b-4cb8-91a8-6db4b08c298c",
    "prId" : 9626,
    "prUrl" : "https://github.com/apache/kafka/pull/9626#pullrequestreview-549258642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3df6572c-a234-4f18-9ff5-a5611186330c",
        "parentId" : null,
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "It would be good to verify that all versions are tested in `testSerialization`.",
        "createdAt" : "2020-12-10T15:12:09Z",
        "updatedAt" : "2020-12-18T21:37:05Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ff7840a592464cb18931b27d4ff1d6157a86b93",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +19,23 @@import org.apache.kafka.common.ConsumerGroupState;\nimport org.apache.kafka.common.ElectionType;\nimport org.apache.kafka.common.IsolationLevel;\nimport org.apache.kafka.common.Node;\nimport org.apache.kafka.common.TopicPartition;"
  },
  {
    "id" : "9f67d026-6d48-4c1e-9b5e-f25979533024",
    "prId" : 9758,
    "prUrl" : "https://github.com/apache/kafka/pull/9758#pullrequestreview-600866405",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84ed7271-abfa-4b71-8eb4-37888f91d757",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Do we need to set the partition id here? There are a few other cases in this file that are similar.",
        "createdAt" : "2021-03-01T15:39:13Z",
        "updatedAt" : "2021-03-04T07:33:22Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae25551171fd4e3b889ca94d494e3207545320e5",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +837,841 @@                        .setRecords(records));\n        responseData.put(new TopicPartition(\"bar\", 1),\n                new FetchResponseData.PartitionData()\n                        .setPartitionIndex(1)\n                        .setHighWatermark(900000)"
  },
  {
    "id" : "47950e14-a93b-4c27-a656-e828237d2e34",
    "prId" : 9758,
    "prUrl" : "https://github.com/apache/kafka/pull/9758#pullrequestreview-601937750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5dac7cff-e868-43ab-a997-f8a766d989fb",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Set partition id.",
        "createdAt" : "2021-03-02T15:22:21Z",
        "updatedAt" : "2021-03-04T07:33:22Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae25551171fd4e3b889ca94d494e3207545320e5",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +807,811 @@        MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n        responseData.put(new TopicPartition(\"test\", 0),\n                new FetchResponseData.PartitionData()\n                        .setPartitionIndex(0)\n                        .setHighWatermark(1000000)"
  },
  {
    "id" : "84b33ae6-d6ea-40ad-8733-33febd99cfad",
    "prId" : 9758,
    "prUrl" : "https://github.com/apache/kafka/pull/9758#pullrequestreview-601937750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63ba8c71-0adb-4c9e-9d97-fa9bce89ea33",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Set partition id.",
        "createdAt" : "2021-03-02T15:22:43Z",
        "updatedAt" : "2021-03-04T07:33:22Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae25551171fd4e3b889ca94d494e3207545320e5",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +1182,1186 @@        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n                        .setPartitionIndex(0)\n                        .setHighWatermark(1000000)\n                        .setLogStartOffset(0)\n                        .setRecords(records));"
  },
  {
    "id" : "92849621-9d5e-4071-89c0-0b5236aab174",
    "prId" : 9758,
    "prUrl" : "https://github.com/apache/kafka/pull/9758#pullrequestreview-601937750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93f12285-783f-49d0-b44b-6682108f2477",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Set partition id.",
        "createdAt" : "2021-03-02T15:22:52Z",
        "updatedAt" : "2021-03-04T07:33:22Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae25551171fd4e3b889ca94d494e3207545320e5",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +1189,1193 @@        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n                        .setPartitionIndex(1)\n                        .setHighWatermark(1000000)\n                        .setLogStartOffset(0)\n                        .setAbortedTransactions(abortedTransactions));"
  },
  {
    "id" : "3a349e0f-76b0-4b29-87af-8b321dc32118",
    "prId" : 9758,
    "prUrl" : "https://github.com/apache/kafka/pull/9758#pullrequestreview-601937750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1fac4e6-ef88-4c8f-ae40-905985205ee6",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Set partition id.",
        "createdAt" : "2021-03-02T15:23:08Z",
        "updatedAt" : "2021-03-04T07:33:22Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae25551171fd4e3b889ca94d494e3207545320e5",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +1200,1204 @@        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n                        .setPartitionIndex(0)\n                        .setHighWatermark(1000000)\n                        .setLogStartOffset(0)\n                        .setRecords(records));"
  },
  {
    "id" : "a071ed59-b84c-4c6d-89a2-97747e816719",
    "prId" : 9758,
    "prUrl" : "https://github.com/apache/kafka/pull/9758#pullrequestreview-601937750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e989229-325b-4fab-a43d-6ceea65a4282",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Set partition id.",
        "createdAt" : "2021-03-02T15:23:20Z",
        "updatedAt" : "2021-03-04T07:33:22Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae25551171fd4e3b889ca94d494e3207545320e5",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +1211,1215 @@        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n                        .setPartitionIndex(1)\n                        .setHighWatermark(1000000)\n                        .setLogStartOffset(0)\n                        .setAbortedTransactions(abortedTransactions));"
  }
]