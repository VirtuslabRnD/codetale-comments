[
  {
    "id" : "7562ff92-5c5c-4b09-80ad-f276a84f83dd",
    "prId" : 4842,
    "prUrl" : "https://github.com/apache/kafka/pull/4842#pullrequestreview-110612031",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6525936-db8c-45b7-be11-506b16d42ec9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The problem was basically that `initiateClose` set `running` to true, which prevented the transactional request from being sent. That in turn kept batched records stuck in the accumulator and prevented shutdown from completing.",
        "createdAt" : "2018-04-09T20:35:18Z",
        "updatedAt" : "2018-04-09T20:36:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "91db41a6-e9e8-4310-9448-2282227d1284",
        "parentId" : "c6525936-db8c-45b7-be11-506b16d42ec9",
        "authorId" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "body" : "nit (for documentation): `initiateClose` would set running to `false`.",
        "createdAt" : "2018-04-09T20:51:03Z",
        "updatedAt" : "2018-04-09T20:51:04Z",
        "lastEditedBy" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "tags" : [
        ]
      }
    ],
    "commit" : "838bd87e8ba5954bc0db115f5c8ae7be7d03bf87",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +330,334 @@\n        AbstractRequest.Builder<?> requestBuilder = nextRequestHandler.requestBuilder();\n        while (!forceClose) {\n            Node targetNode = null;\n            try {"
  },
  {
    "id" : "17b5ceb1-c886-4e50-a931-d7b60619c600",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-140441821",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e6a491a-a43c-41a1-af27-1aecc2637a82",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If `batches` is empty, should we remove the list from `inFlightBatches`?",
        "createdAt" : "2018-07-25T06:10:12Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "4bd040ed-1e1e-49e2-883b-40940dce3a31",
        "parentId" : "6e6a491a-a43c-41a1-af27-1aecc2637a82",
        "authorId" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "body" : "updated the change to remove the list from `inFlightBatches` when it is empty. ",
        "createdAt" : "2018-07-25T18:25:05Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +164,168 @@    public void maybeRemoveFromInflightBatches(ProducerBatch batch) {\n        List<ProducerBatch> batches = inFlightBatches.get(batch.topicPartition);\n        if (batches != null) {\n            batches.remove(batch);\n            if (batches.isEmpty()) {"
  },
  {
    "id" : "8456e9cf-de9b-4532-b224-d664e9443eb7",
    "prId" : 5971,
    "prUrl" : "https://github.com/apache/kafka/pull/5971#pullrequestreview-226519142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a5539ec-b9f5-4678-8da2-151458fa1453",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I feel `hasOngoingTransaction` might be the main thing we care about. The only request that could come outside of a transaction is the FindCoordinator request (I think) and we wouldn't want to block shutdown on that. Does that make sense?",
        "createdAt" : "2019-03-07T21:50:20Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "98effaf1-4aaf-43ff-ab2c-3f61d01cc6ee",
        "parentId" : "2a5539ec-b9f5-4678-8da2-151458fa1453",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Yep, it does make sense. Will update the code to reflect this.",
        "createdAt" : "2019-03-11T12:33:31Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "214d16b4-d77f-41ac-ae07-e1eb5c136251",
        "parentId" : "2a5539ec-b9f5-4678-8da2-151458fa1453",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "On the other hand we'll block infinitely in `run()` if `hasPendingRequests` is not included here and there's no closure to the ongoing transaction. My plan here really was that after flipping the `running` flag to false to send out any transactional messages that are in the queue. There may be a commit (or abort) among them. If the user is blocking on one and there is no response then we'll fail those in forceClose as you suggested below.  ",
        "createdAt" : "2019-03-12T08:39:00Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "04596f4f-c1f8-4d44-8510-b042f6b6a5cb",
        "parentId" : "2a5539ec-b9f5-4678-8da2-151458fa1453",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm not sure I follow. If there is a commit or abort that needs to be sent, then wouldn't `hasOngoingTransaction` return true?",
        "createdAt" : "2019-04-04T22:05:39Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f0ab93a7-eb59-4975-aae0-ff9f31d724ef",
        "parentId" : "2a5539ec-b9f5-4678-8da2-151458fa1453",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "So we're not interested in sending out FindCoordinator or InitProducerId requests when closing. Moreover `KafkaProducerTest.testInitTransactionTimeout` demonstrates an edge case where the execution is stuck in the \r\n```\r\n        while (!forceClose && ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() > 0) || hasPendingTransactionalRequests())) {\r\n            try {\r\n                run(time.milliseconds());\r\n            } catch (Exception e) {\r\n                log.error(\"Uncaught error in kafka producer I/O thread: \", e);\r\n            }\r\n        }\r\n```\r\nloop as `TxnRequestHandler.onComplete` would reenqueue it. I was thinking about introducing an `isShuttingDown` flag in `TxnRequestHandler` to work this around but I then thought we wouldn't need to send out those requests anyway.",
        "createdAt" : "2019-04-15T08:46:20Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e9e027171422c2f5af6d9602429702a58939036",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +225,229 @@\n    private boolean hasPendingTransactionalRequests() {\n        return transactionManager != null && transactionManager.hasPendingRequests() && transactionManager.hasOngoingTransaction();\n    }\n"
  },
  {
    "id" : "1e689e76-dbbc-48d7-9c23-55e374976007",
    "prId" : 5971,
    "prUrl" : "https://github.com/apache/kafka/pull/5971#pullrequestreview-213306191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca08c396-8c4e-4825-9066-ec53b6a0cbd3",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this:\r\n\r\n1. Check whether the sender thread is closed and raise an error if it is\r\n2. Refresh metadata if needed\r\n3. Check if the accumulator is closed\r\n\r\nTo be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. \r\n\r\nTo make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.",
        "createdAt" : "2019-03-07T22:21:32Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "cfefb57f-e8ad-40a6-a6a4-2f83be0c5eb8",
        "parentId" : "ca08c396-8c4e-4825-9066-ec53b6a0cbd3",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "I think I'll make that change with checking if the producer is closed. `throwIfProducerClosed` is currently only used in doSend and as I see changing it to check the sender won't have very dramatic changes in behavior. Besides I could reuse it in the transactional methods.",
        "createdAt" : "2019-03-12T10:18:25Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "16655d07-6705-485d-9539-e0f70dd1a4bd",
        "parentId" : "ca08c396-8c4e-4825-9066-ec53b6a0cbd3",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Regarding the second part I'll put in a close method in TransactionManager what you were referring to:\r\n```\r\n    synchronized void abortPendingTransactionalRequests() {\r\n        pendingRequests.forEach(handler ->\r\n                handler.fatalError(new KafkaException(\"The producer closed forcefully\")));\r\n    }\r\n```\r\nThis would fail the requests. The name or the exception thrown might be debatable :). I would apply this in the forceClose part of the sender. So if aborting the transaction normally wasn't successful we just fail them on force close.",
        "createdAt" : "2019-03-12T10:33:22Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e9e027171422c2f5af6d9602429702a58939036",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +260,264 @@            if (!transactionManager.isCompleting()) {\n                log.info(\"Aborting incomplete transaction due to shutdown\");\n                transactionManager.beginAbort();\n            }\n            try {"
  },
  {
    "id" : "15c80929-b15a-49a3-9068-10ead07fd942",
    "prId" : 5971,
    "prUrl" : "https://github.com/apache/kafka/pull/5971#pullrequestreview-226516259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e86eba9-d050-4af6-a379-f87597f8a812",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "No need to fix it in this patch, but these null checks get a bit vexing. Should we create a NoOpTransactionManager or something?",
        "createdAt" : "2019-04-04T22:10:26Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "39ef4f10-b3b0-4d0b-adbd-15ae095e2144",
        "parentId" : "2e86eba9-d050-4af6-a379-f87597f8a812",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Indeed. Searched for it and there are 24 checks for this, so probably worth to do it sometime.",
        "createdAt" : "2019-04-15T08:40:26Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e9e027171422c2f5af6d9602429702a58939036",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +272,276 @@            // We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on\n            // the futures.\n            if (transactionManager != null) {\n                log.debug(\"Aborting incomplete transactional requests due to forced shutdown\");\n                transactionManager.close();"
  },
  {
    "id" : "12a3f83e-c3be-458b-8be2-cfe0cbca9d52",
    "prId" : 5971,
    "prUrl" : "https://github.com/apache/kafka/pull/5971#pullrequestreview-223022128",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57a419d1-6187-4849-9078-beedccdecced",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "By the way, we should probably document this behavior in `KafkaProducer.close()`.",
        "createdAt" : "2019-04-04T22:11:28Z",
        "updatedAt" : "2019-04-15T13:56:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e9e027171422c2f5af6d9602429702a58939036",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +260,264 @@            if (!transactionManager.isCompleting()) {\n                log.info(\"Aborting incomplete transaction due to shutdown\");\n                transactionManager.beginAbort();\n            }\n            try {"
  },
  {
    "id" : "8dd8cbda-dfc2-43e1-87b0-b46f8c7ddff7",
    "prId" : 6613,
    "prUrl" : "https://github.com/apache/kafka/pull/6613#pullrequestreview-232477693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "176c3f77-5d2d-4cae-bd9f-0d401c5d3ad9",
        "parentId" : null,
        "authorId" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "body" : "It seems to me that this is the main fix. Do we really want to redefine 'now' in this method as you do on L455? It is used in multiple places in `run` and I wonder if there will be side effects of using a different value here. ",
        "createdAt" : "2019-04-22T18:20:56Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "tags" : [
        ]
      },
      {
        "id" : "51e34992-1169-4b35-adc5-ff9cf36ec7ee",
        "parentId" : "176c3f77-5d2d-4cae-bd9f-0d401c5d3ad9",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. We definitely have to update the time after the blocking operations above, but it's a fair point that the old `now` continues to be used in `run` afterwards. We actually have the same problem in `maybeWaitForProducerId`, which is also a blocking operation. I don't see a way around it except to change the logic in `run` to treat `now` as more of a `startTimeMs`. ",
        "createdAt" : "2019-04-22T18:46:24Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "17b1e606-2e5b-4ff7-9115-d7b4452d9a4c",
        "parentId" : "176c3f77-5d2d-4cae-bd9f-0d401c5d3ad9",
        "authorId" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "body" : "You're right that accounting for the blocking operations is key. \r\n\r\nInitially, I thought the main issue with recomputing the `currentTime` in every down stream call is that it will likely affect the validity of a bunch of the test cases in `SenderTest`. But I looked at the cases they should be OK with such a change. \r\n\r\nI think standardizing on treating `now` as more of a `startTimeMs` makes sense.",
        "createdAt" : "2019-04-22T20:27:35Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "tags" : [
        ]
      },
      {
        "id" : "cc2a48ab-9a4f-481c-8fea-e6f9509b2cd0",
        "parentId" : "176c3f77-5d2d-4cae-bd9f-0d401c5d3ad9",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Thanks for reviewing. I ended up changing the signature so that it removes the timing assumption. Literally every call to this method was `run(time.milliseconds())`, so I got rid of the argument and renamed to `runOnce`.",
        "createdAt" : "2019-04-22T22:51:26Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "82fe1d98-d56b-46cd-b571-9dc48cfd5aaa",
        "parentId" : "176c3f77-5d2d-4cae-bd9f-0d401c5d3ad9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I like the refactoring too: we had similar pattern of passing `now` in Streamthread loop but later I refactored it to `advanceNowAndComputeLatency` whenever a blocking call is made for the same reason.",
        "createdAt" : "2019-05-01T02:10:36Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfb67287553fb24daf3e7e252379a05d0ef08860",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +458,462 @@                        targetNode.idString(), requestBuilder, currentTimeMs, true, requestTimeoutMs, nextRequestHandler);\n                    log.debug(\"Sending transactional request {} to node {}\", requestBuilder, targetNode);\n                    client.send(clientRequest, currentTimeMs);\n                    transactionManager.setInFlightCorrelationId(clientRequest.correlationId());\n                    return true;"
  },
  {
    "id" : "032bb1c0-a97c-4892-a414-d4122174ec03",
    "prId" : 6613,
    "prUrl" : "https://github.com/apache/kafka/pull/6613#pullrequestreview-229311111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d621f73f-6ac2-4dfd-84d3-f94dbc3d2eca",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Should this also be `now` instead of `currentTimeMs` ?",
        "createdAt" : "2019-04-23T00:43:43Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "a21493e3-32e6-4300-a13a-3ce3c0c511af",
        "parentId" : "d621f73f-6ac2-4dfd-84d3-f94dbc3d2eca",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I actually favor `currentTimeMs`, but perhaps it is unnecessarily pedantic.",
        "createdAt" : "2019-04-23T00:47:50Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b2411a6b-0c0b-41b3-98e5-07a8cdf6bac7",
        "parentId" : "d621f73f-6ac2-4dfd-84d3-f94dbc3d2eca",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I personally don't care too much. Was just wondering about keeping the code consistent. If `now` is the current way, we might want to keep it this way -- or rename it everywhere (but maybe in a separate PR). Just my 2 cents. Feel free to ignore.",
        "createdAt" : "2019-04-23T01:03:04Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfb67287553fb24daf3e7e252379a05d0ef08860",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +331,335 @@        }\n\n        long currentTimeMs = time.milliseconds();\n        long pollTimeout = sendProducerData(currentTimeMs);\n        client.poll(pollTimeout, currentTimeMs);"
  },
  {
    "id" : "6786b035-92f8-4524-91ed-3efc71cc49f9",
    "prId" : 6613,
    "prUrl" : "https://github.com/apache/kafka/pull/6613#pullrequestreview-229308364",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51de4927-a569-47c7-9593-1bfee7f74e43",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Should this also be `now` instead of `currentTimeMs` ?\r\n\r\n",
        "createdAt" : "2019-04-23T00:46:33Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfb67287553fb24daf3e7e252379a05d0ef08860",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +454,458 @@                    if (nextRequestHandler.isRetry())\n                        time.sleep(nextRequestHandler.retryBackoffMs());\n                    long currentTimeMs = time.milliseconds();\n                    ClientRequest clientRequest = client.newClientRequest(\n                        targetNode.idString(), requestBuilder, currentTimeMs, true, requestTimeoutMs, nextRequestHandler);"
  },
  {
    "id" : "b13fb653-afa3-4ab6-827b-bda0a630120a",
    "prId" : 6613,
    "prUrl" : "https://github.com/apache/kafka/pull/6613#pullrequestreview-229898809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6ed8486-9dd3-4cfb-83e7-6945db46eb69",
        "parentId" : null,
        "authorId" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "body" : "(This is a rewrite of the comment I thought I had left with my previous review)\r\n\r\nWith the changes in this patch, the path for the regular produce diverges from the transactional path: the former takes in a value of `now` from `runOnce` and uses it all the way through while the latter recomputes it. This is fine because the regular producer path has no blocking calls. It would be good add a comment explaining this at the top level.",
        "createdAt" : "2019-04-24T04:55:26Z",
        "updatedAt" : "2019-05-01T20:59:05Z",
        "lastEditedBy" : "0e2392fc-5c22-4c17-aa96-3025239a3098",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfb67287553fb24daf3e7e252379a05d0ef08860",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +336,340 @@    }\n\n    private long sendProducerData(long now) {\n        Cluster cluster = metadata.fetch();\n        // get the list of partitions with data ready to send"
  },
  {
    "id" : "10dbb649-1f76-4b2d-8b6c-7d867546b787",
    "prId" : 7389,
    "prUrl" : "https://github.com/apache/kafka/pull/7389#pullrequestreview-354845237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aba9736f-2179-4d7b-a170-3803129d6fe8",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "nice refactoring!",
        "createdAt" : "2020-02-07T05:41:46Z",
        "updatedAt" : "2020-02-15T22:01:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe88cc1ea6addd6c8597829bdfa41d9d1d68cef2",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +688,692 @@            batch.attempts() < this.retries &&\n            !batch.isDone() &&\n            (transactionManager == null ?\n                    response.error.exception() instanceof RetriableException :\n                    transactionManager.canRetry(response, batch));"
  },
  {
    "id" : "04d97b63-ea6f-42dd-b789-e00637dd9d6a",
    "prId" : 7498,
    "prUrl" : "https://github.com/apache/kafka/pull/7498#pullrequestreview-534235755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "277d364f-ec18-464a-9e12-015037968257",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "I'm wondering if we should print an error message even if one is not provided - we could get the default error message from the `ApiException` linked to the `Errors`. An example would be for the OUT_OF_ORDER_SEQUENCE_NUMBER case:\r\n\r\nWithout the change:\r\n```\r\n[2020-11-12 11:26:26,775] WARN Got error produce response with correlation id 2 on topic-partition test-0, retrying (2147483646 attempts left). Error: OUT_OF_ORDER_SEQUENCE_NUMBER.(org.apache.kafka.clients.producer.internals.Sender:610)\r\n```\r\n\r\n\r\n```\r\n[2020-11-12 11:26:26,775] WARN Got error produce response with correlation id 2 on topic-partition test-0, retrying (2147483646 attempts left). Error: OUT_OF_ORDER_SEQUENCE_NUMBER. Error Message: The broker received an out of order sequence number. (org.apache.kafka.clients.producer.internals.Sender:610)\r\n```",
        "createdAt" : "2020-11-12T10:28:59Z",
        "updatedAt" : "2020-11-12T10:28:59Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "b18b3451-c4f2-4644-b001-06a6bb695dd4",
        "parentId" : "277d364f-ec18-464a-9e12-015037968257",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "Indeed, we could. I am not sure that it brings much more information though so I am fine with keeping it as it is.",
        "createdAt" : "2020-11-19T09:37:47Z",
        "updatedAt" : "2020-11-19T09:37:47Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      }
    ],
    "commit" : "57d4b5d6910e744067b0bc962ec10ee9569f04bd",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +660,664 @@     * e.g \"NETWORK_EXCEPTION. Error Message: Disconnected from node 0\"\n     */\n    private String formatErrMsg(ProduceResponse.PartitionResponse response) {\n        String errorMessageSuffix = (response.errorMessage == null || response.errorMessage.isEmpty()) ?\n                \"\" : String.format(\". Error Message: %s\", response.errorMessage);"
  },
  {
    "id" : "8cb0bbd5-5b7c-4837-b078-481b473b4f3e",
    "prId" : 8177,
    "prUrl" : "https://github.com/apache/kafka/pull/8177#pullrequestreview-367400201",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "938d2b3e-7b83-4be0-88c5-9f8d154b29eb",
        "parentId" : null,
        "authorId" : "e88252f3-8879-452a-8098-afb39cb614dc",
        "body" : "ðŸ‘ ",
        "createdAt" : "2020-03-02T17:50:23Z",
        "updatedAt" : "2020-03-02T23:12:23Z",
        "lastEditedBy" : "e88252f3-8879-452a-8098-afb39cb614dc",
        "tags" : [
        ]
      }
    ],
    "commit" : "5798cee17a381dcf4154cb665f2610fd4a56ce2c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +741,745 @@        ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout,\n                produceRecordsByPartition, transactionalId);\n        RequestCompletionHandler callback = response -> handleProduceResponse(response, recordsByPartition, time.milliseconds());\n\n        String nodeId = Integer.toString(destination);"
  },
  {
    "id" : "9220b8dd-77cb-461d-9517-e17c20cb8916",
    "prId" : 9401,
    "prUrl" : "https://github.com/apache/kafka/pull/9401#pullrequestreview-533046230",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e75f0296-718e-4d56-9cd0-8174a98723c2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: since we have the jira for tracking, can we remove the TODO? A few more of these in the PR.",
        "createdAt" : "2020-11-17T23:11:05Z",
        "updatedAt" : "2020-11-18T18:18:40Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "6cb83489-328c-4f65-8f11-bd573a13ccb7",
        "parentId" : "e75f0296-718e-4d56-9cd0-8174a98723c2",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "copy that",
        "createdAt" : "2020-11-18T02:49:53Z",
        "updatedAt" : "2020-11-18T18:18:40Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffbe9a3ee73bc65253c0b319cf08b923dcce6138",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +564,568 @@            if (response.hasResponse()) {\n                // Sender should exercise PartitionProduceResponse rather than ProduceResponse.PartitionResponse\n                // https://issues.apache.org/jira/browse/KAFKA-10696\n                ProduceResponse produceResponse = (ProduceResponse) response.responseBody();\n                produceResponse.data().responses().forEach(r -> r.partitionResponses().forEach(p -> {"
  },
  {
    "id" : "32003617-3c3d-4e0b-90a4-344e46dbc368",
    "prId" : 9401,
    "prUrl" : "https://github.com/apache/kafka/pull/9401#pullrequestreview-533431039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b73413a0-cb77-4b56-82e0-bfa8518d9df6",
        "parentId" : null,
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "nit: As we got rid of the streaming api in this section, would it make sense to also remove this one?",
        "createdAt" : "2020-11-18T10:11:13Z",
        "updatedAt" : "2020-11-18T18:18:40Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      },
      {
        "id" : "f14ca1b2-3e41-44b2-b342-7277b1bb477b",
        "parentId" : "b73413a0-cb77-4b56-82e0-bfa8518d9df6",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "The reason we got rid of streaming APIs is because it produces extra collection (groupBy). However, in this case we have to create a new collection to carry non-auto-generated data (and https://issues.apache.org/jira/browse/KAFKA-10696 will eliminate such conversion) even if we get rid of stream APIs. Hence, it should be fine to keep stream APIs here.",
        "createdAt" : "2020-11-18T11:55:30Z",
        "updatedAt" : "2020-11-18T18:18:40Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "dbca2823-cd8f-4b08-9714-6008fce4d983",
        "parentId" : "b73413a0-cb77-4b56-82e0-bfa8518d9df6",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "Yeah, I do agree. It won't change much from a performance point of view. I was more thinking about this from a code consistency point of view. I don't feel strong about this at all. It is just that I usually prefer not to mix paradigms. I recognise that this is a personal taste :).",
        "createdAt" : "2020-11-18T13:12:27Z",
        "updatedAt" : "2020-11-18T18:18:40Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffbe9a3ee73bc65253c0b319cf08b923dcce6138",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +576,580 @@                                .stream()\n                                .map(e -> new ProduceResponse.RecordError(e.batchIndex(), e.batchIndexErrorMessage()))\n                                .collect(Collectors.toList()),\n                            p.errorMessage());\n                    ProducerBatch batch = batches.get(tp);"
  }
]