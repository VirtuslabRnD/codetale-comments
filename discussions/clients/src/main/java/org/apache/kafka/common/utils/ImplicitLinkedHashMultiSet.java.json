[
  {
    "id" : "a8ef8285-405e-4bae-952c-fd6e8b0e3761",
    "prId" : 5893,
    "prUrl" : "https://github.com/apache/kafka/pull/5893#pullrequestreview-188871093",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a1b4b35-1e79-4339-b304-bb5080830519",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: We're not using `seen`, so this could be replace with `foreach`. Same in `findAll`.",
        "createdAt" : "2019-01-02T21:36:15Z",
        "updatedAt" : "2019-01-11T21:21:35Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "3c013801-0842-42c3-8c6a-d36d1625574a",
        "parentId" : "8a1b4b35-1e79-4339-b304-bb5080830519",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I'm not sure `foreach` is really better here, since the variable that we use would be equally unused.  This loop itself is kind of a bit of paranoia on my part.  It prevents an infinite loop if the hash table is 100% full.  But the hash table should never get more than 50% full anyway...",
        "createdAt" : "2019-01-02T23:57:12Z",
        "updatedAt" : "2019-01-11T21:21:35Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d8de7a5c2839b05e6d3143f9ef0e31832234abf",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +97,101 @@        int slot = slot(elements, key);\n        int bestSlot = INVALID_INDEX;\n        for (int seen = 0; seen < elements.length; seen++) {\n            Element element = elements[slot];\n            if (element == null) {"
  },
  {
    "id" : "7a089fbb-830a-4891-afac-c44d2215a066",
    "prId" : 5893,
    "prUrl" : "https://github.com/apache/kafka/pull/5893#pullrequestreview-188872404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee195e82-cf8b-4796-8c97-1dc36e75fa55",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Wondering if this should be the exceptional behavior and not the rule. Which APIs currently expect multiset semantics? I realize that there is a difficulty enforcing set semantics when parsing requests, but it might be nice to catch violations when building a response.",
        "createdAt" : "2019-01-02T22:17:37Z",
        "updatedAt" : "2019-01-11T21:21:35Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d08a600f-335e-4aa8-872b-ca4e1f87b18f",
        "parentId" : "ee195e82-cf8b-4796-8c97-1dc36e75fa55",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I think we need multisets here in order to avoid breaking compatibility.  One example is CreateTopics -- the KIP specifies what should happen when there are duplicate elements, and it's not a disconnect.  I would expect a similar story for all of the RPCs, unfortunately.",
        "createdAt" : "2019-01-03T00:05:24Z",
        "updatedAt" : "2019-01-11T21:21:35Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d8de7a5c2839b05e6d3143f9ef0e31832234abf",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +27,31 @@ * See org.apache.kafka.common.utils.ImplicitLinkedHashSet for implementation details.\n *\n * This class is a multi-set because it allows multiple elements to be inserted that are\n * equal to each other.\n *"
  }
]