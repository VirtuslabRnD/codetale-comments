[
  {
    "id" : "16476884-f4d9-4543-b5d5-275d990d7d62",
    "prId" : 4485,
    "prUrl" : "https://github.com/apache/kafka/pull/4485#pullrequestreview-164240341",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "155253bd-e9c2-4f09-88a6-8e5f5e852679",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "remove unnecessary newline",
        "createdAt" : "2018-10-12T13:31:57Z",
        "updatedAt" : "2018-10-13T16:40:58Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "85cae7d6538b8cd8c4ecd485bec14485ae10a1c4",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +498,502 @@\n    private static TransactionManager configureTransactionState(ProducerConfig config, LogContext logContext, Logger log) {\n\n        TransactionManager transactionManager = null;\n"
  },
  {
    "id" : "eb750a4c-8535-4c47-bc91-fd1dcf364a5e",
    "prId" : 4485,
    "prUrl" : "https://github.com/apache/kafka/pull/4485#pullrequestreview-164240341",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ff0418c-b494-4001-beaf-8843b00e35da",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "remove unnecessary newline",
        "createdAt" : "2018-10-12T13:32:07Z",
        "updatedAt" : "2018-10-13T16:40:58Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "85cae7d6538b8cd8c4ecd485bec14485ae10a1c4",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +500,504 @@\n        TransactionManager transactionManager = null;\n\n        boolean userConfiguredIdempotence = false;\n        if (config.originals().containsKey(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG))"
  },
  {
    "id" : "5b0d29ba-70ac-478f-920b-5d83dee9213e",
    "prId" : 4563,
    "prUrl" : "https://github.com/apache/kafka/pull/4563#pullrequestreview-104772957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd4e5f55-477d-40f0-b77e-b8d9da23ff38",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can you add a comment to the javadoc mentioning that this method may be retried if a `TimeoutException` or an `InterruptException` is raised?",
        "createdAt" : "2018-03-20T15:42:29Z",
        "updatedAt" : "2018-03-27T15:59:59Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "754249e243260d32faea0ca9d27ff043563988c2",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +568,572 @@     *         transactional.id is not authorized. See the exception for more details\n     * @throws KafkaException if the producer has encountered a previous fatal error or for any other unexpected error\n     * @throws TimeoutException if the time taken for initialize the transaction has surpassed <code>max.block.ms</code>.\n     * @throws InterruptException if the thread is interrupted while blocked\n     */"
  },
  {
    "id" : "5fb7126b-41a6-4925-b60a-3c41039de3ad",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-137190980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63f679c1-ce51-4bcb-b558-b628729217ae",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "Why changing linger to int ?",
        "createdAt" : "2018-07-12T04:14:37Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "a230cf55-a4b1-4ebb-b55e-b6a6f5236463",
        "parentId" : "63f679c1-ce51-4bcb-b558-b628729217ae",
        "authorId" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "body" : "we have `request.timeout.ms` and `delivery.timeout.ms` of `int` type. this is to make the type of `linger.ms` be consistent with other timeout related settings. ",
        "createdAt" : "2018-07-13T21:09:45Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +400,404 @@                    config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),\n                    this.compressionType,\n                    config.getInt(ProducerConfig.LINGER_MS_CONFIG),\n                    retryBackoffMs,\n                    deliveryTimeoutMs,"
  },
  {
    "id" : "9a050ab2-169c-4995-a748-369a1fa62f8a",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-138475262",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d134dcf8-a211-4f14-a31f-e60a17f1e8de",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "Should this be of type long ?\r\nWith long, there is no overflow on line 478\r\n\r\nIn ProducerBatch, deliveryTimeoutMs is long in hasReachedDeliveryTimeout",
        "createdAt" : "2018-07-12T04:15:18Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "b724e85e-9b90-4e9a-8d81-137f326ffa1e",
        "parentId" : "d134dcf8-a211-4f14-a31f-e60a17f1e8de",
        "authorId" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "body" : "`ProducerBatch.hasReachedDeliveryTimeout` is called by RecordAccumulator. In RecordAccumulator's construct, we have had using `long` type for `lingerMs`, and `retryBackoffMs`. It will be inconsistent to use `int` for `deliveryTimeoutMs`. And it will require changes at many places (especially in the test cases) if we use `int` type for `lingerMs` and `retryBackoffMs`.  I thought that it would be better to have another PR for data type related changes for `lingerMs` etc.\r\n\r\n        public RecordAccumulator(LogContext logContext,\r\n                                 int batchSize,\r\n                                 CompressionType compression,\r\n                                 long lingerMs,\r\n                                 long retryBackoffMs,\r\n                                 ...\r\n",
        "createdAt" : "2018-07-18T23:16:51Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +465,469 @@\n    private static int configureDeliveryTimeout(ProducerConfig config, Logger log) {\n        int deliveryTimeoutMs = config.getInt(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG);\n        int lingerMs = config.getInt(ProducerConfig.LINGER_MS_CONFIG);\n        int requestTimeoutMs = config.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);"
  },
  {
    "id" : "44e46224-f481-46c4-8021-e8c06ec5090b",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-137192562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d193231f-2783-402c-9587-08aaaa4706b5",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "Doesn't need to be warn.\r\nCan be info since there is no action from user",
        "createdAt" : "2018-07-12T04:17:42Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "ea8d7a93-7dec-4bca-8e2e-34575062afef",
        "parentId" : "d193231f-2783-402c-9587-08aaaa4706b5",
        "authorId" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "body" : "This is try to get the user's attention as we are overriding the default  delivery.timeout.ms setting.  Previously the user may set a long `request.timeout.ms` as a work around. The user may want to explicitly set `delivery.timeout.ms` and give a smaller value for `request.timeout.ms`. ",
        "createdAt" : "2018-07-13T21:15:57Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +478,482 @@                // override deliveryTimeoutMs default value to lingerMs + requestTimeoutMs for backward compatibility\n                deliveryTimeoutMs = lingerMs + requestTimeoutMs;\n                log.warn(\"{} should be equal to or larger than {} + {}. Setting it to {}.\",\n                    ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, ProducerConfig.LINGER_MS_CONFIG,\n                    ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, deliveryTimeoutMs);"
  },
  {
    "id" : "80dc6a40-89fa-4585-8d52-f6491cadbc8e",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-191921116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5e89e6a-3180-4212-9b8c-72540a634d0b",
        "parentId" : null,
        "authorId" : "6f96e7ef-7198-42b1-9445-532b9b5a1aab",
        "body" : "(deliveryTimeoutMs < lingerMs + requestTimeoutMs) implies (deliveryTimeoutMs < Integer.MAX_VALUE), why do we need to check (deliveryTimeoutMs < Integer.MAX_VALUE), logically it can be removed.",
        "createdAt" : "2019-01-12T01:46:44Z",
        "updatedAt" : "2019-01-12T01:46:44Z",
        "lastEditedBy" : "6f96e7ef-7198-42b1-9445-532b9b5a1aab",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +469,473 @@        int requestTimeoutMs = config.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);\n\n        if (deliveryTimeoutMs < Integer.MAX_VALUE && deliveryTimeoutMs < lingerMs + requestTimeoutMs) {\n            if (config.originals().containsKey(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG)) {\n                // throw an exception if the user explicitly set an inconsistent value"
  },
  {
    "id" : "455bccde-dbd3-4788-97dd-d0c13d32ace5",
    "prId" : 5383,
    "prUrl" : "https://github.com/apache/kafka/pull/5383#pullrequestreview-151046149",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c88f8042-e54e-4983-9442-627500ab6188",
        "parentId" : null,
        "authorId" : "2d677cb0-7f58-4f02-8104-880b46eb7fb3",
        "body" : "nit: please add protected keyword",
        "createdAt" : "2018-08-23T23:09:26Z",
        "updatedAt" : "2018-10-01T22:09:56Z",
        "lastEditedBy" : "2d677cb0-7f58-4f02-8104-880b46eb7fb3",
        "tags" : [
        ]
      },
      {
        "id" : "d3cb548f-0d44-4cd5-99ab-a2591e340cc6",
        "parentId" : "c88f8042-e54e-4983-9442-627500ab6188",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "`protected` gives more visibility than what's there now (`package visibility`) so not sure why we'd want to do that?",
        "createdAt" : "2018-08-30T15:15:15Z",
        "updatedAt" : "2018-10-01T22:09:56Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ee59e6985e64c8109b71f4501d83dd5ed4d38158",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1205,1209 @@\n    // Visible for testing\n    String getClientId() {\n        return clientId;\n    }"
  },
  {
    "id" : "0c8d6e2a-80b6-48a1-bcd7-e44839c87f30",
    "prId" : 5667,
    "prUrl" : "https://github.com/apache/kafka/pull/5667#pullrequestreview-178676687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a962d2a9-dd70-4b3c-8d57-b5ee9a064d1c",
        "parentId" : null,
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "Do we need to add a negative timeout check like we did in [consumer](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L2101)?",
        "createdAt" : "2018-11-27T08:51:22Z",
        "updatedAt" : "2019-01-15T06:10:36Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "0e184e2f-1016-4330-b0a9-ae1fe6f24ab5",
        "parentId" : "a962d2a9-dd70-4b3c-8d57-b5ee9a064d1c",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "add the check and related test (see KafkaProducerTest.testNegativeTimeout)",
        "createdAt" : "2018-11-27T09:23:05Z",
        "updatedAt" : "2019-01-15T06:10:36Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4c9f2e4cac38bb7094099c44170c9ce595571b0",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +1134,1138 @@\n    private void close(Duration timeout, boolean swallowException) {\n        long timeoutMs = timeout.toMillis();\n        if (timeoutMs < 0)\n            throw new IllegalArgumentException(\"The timeout cannot be negative.\");"
  },
  {
    "id" : "709627bc-51e0-4291-8670-89f3e873998c",
    "prId" : 6066,
    "prUrl" : "https://github.com/apache/kafka/pull/6066#pullrequestreview-188383163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "daec6865-bbc9-4b93-b4f1-020c9475bc22",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think we should add some documentation here to make it clear what the user should be expected to do if either of these exceptions are raised. There are really only two options: 1) retry the operation, and 2) close the producer. ",
        "createdAt" : "2018-12-27T19:22:03Z",
        "updatedAt" : "2019-02-20T01:55:39Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b7108b3a-ff00-4e28-8adf-da093f09e6c9",
        "parentId" : "daec6865-bbc9-4b93-b4f1-020c9475bc22",
        "authorId" : "2b8ddac3-3f74-403c-9e9d-62dc37cb6655",
        "body" : "Once a transaction is in committing or aborting state, can user still retry it? Seems the only valid state from COMMITTING_TRANSACTION is to ABORTABLE_ERROR. Do we need to reset the state to IN_TRANSACTION after committing or aborting failed or cache the transaction result to retry later?",
        "createdAt" : "2018-12-28T00:35:03Z",
        "updatedAt" : "2019-02-20T01:55:39Z",
        "lastEditedBy" : "2b8ddac3-3f74-403c-9e9d-62dc37cb6655",
        "tags" : [
        ]
      },
      {
        "id" : "094b8a5e-766b-42a0-8b51-ed16a3d2b813",
        "parentId" : "daec6865-bbc9-4b93-b4f1-020c9475bc22",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It should be possible to retry the operation that was already begun. So if a call to `commitTransaction` timed out, the user ought to be able to retry it, in which case we would just continue waiting on the commit that was already begun. However, it should not be possible to do an `abortTransaction` instead.",
        "createdAt" : "2018-12-28T23:22:34Z",
        "updatedAt" : "2019-02-20T01:55:39Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc8d51b84cab62a769351909a27566cc4672fff",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +684,688 @@     * @throws KafkaException if the producer has encountered a previous fatal or abortable error, or for any\n     *         other unexpected error\n     * @throws TimeoutException if the time taken for committing the transaction has surpassed <code>max.block.ms</code>.\n     * @throws InterruptException if the thread is interrupted while blocked\n     */"
  },
  {
    "id" : "675daebc-3d53-4e7c-a4ad-2b8d5aa90072",
    "prId" : 6066,
    "prUrl" : "https://github.com/apache/kafka/pull/6066#pullrequestreview-191473073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09e21526-6127-4c3d-85f3-54dc26e38fe4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we make this comment match the one above for commitTransaction()?",
        "createdAt" : "2019-01-11T00:13:51Z",
        "updatedAt" : "2019-02-20T01:55:39Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc8d51b84cab62a769351909a27566cc4672fff",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +700,704 @@     *\n     * Note that this method will raise {@link TimeoutException} if the transaction cannot be aborted before expiration\n     * of {@code max.block.ms}. Additionally, it will raise {@link InterruptException} if interrupted.\n     * It is safe to retry in either case, but it is not possible to attempt a different operation (such as commitTransaction)\n     * since the abort may already be in the progress of completing. If not retrying, the only option is to close the producer."
  },
  {
    "id" : "5513bb93-fa1a-49c2-a84e-db0286263fe3",
    "prId" : 6502,
    "prUrl" : "https://github.com/apache/kafka/pull/6502#pullrequestreview-225278426",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b61a22e8-7900-4522-a774-c774aec631d1",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "It seems like we didn't enforce this too well if `lingerMs + requestTimeoutMs` overflowed. It would have been possible to misconfigure the producer with specifying lower `deliveryTimeoutMs` and higher `lingerMs + requestTimeoutMs`. We will now raise an exception if that is the case.\r\n\r\nThe test fixes needed in this patch are for the same reason, where `lingerMs` was set to `Integer.MAX_VALUE` and `deliveryTimeoutMs` was set to the default value of 2 minutes.",
        "createdAt" : "2019-04-11T00:10:21Z",
        "updatedAt" : "2019-04-11T00:10:21Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3fed80a47baf100462e962e4c18a476ce971077",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +486,490 @@        int lingerAndRequestTimeoutMs = (int) Math.min((long) lingerMs + requestTimeoutMs, Integer.MAX_VALUE);\n\n        if (deliveryTimeoutMs < Integer.MAX_VALUE && deliveryTimeoutMs < lingerAndRequestTimeoutMs) {\n            if (config.originals().containsKey(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG)) {\n                // throw an exception if the user explicitly set an inconsistent value"
  },
  {
    "id" : "e81c7ef2-98b4-4d45-9cb1-ada7e2bdf86d",
    "prId" : 6997,
    "prUrl" : "https://github.com/apache/kafka/pull/6997#pullrequestreview-269295555",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c53e08e6-3193-43ad-a36d-e396fc760163",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "We need to call `transactionManager#failIfNotReadyForSend` here, so that we don't try to append to the batch when we are not ready to send.  Also, we should remove `failIfNotReadyForSend` from `TransactionManager#maybeAddPartitionToTransaction`",
        "createdAt" : "2019-07-31T21:33:48Z",
        "updatedAt" : "2019-08-01T17:34:42Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "93f350844be9c391ab375fc9b0c207b4ea4335be",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +913,917 @@            // producer callback will make sure to call both 'callback' and interceptor callback\n            Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);\n\n            if (transactionManager != null && transactionManager.isTransactional()) {\n                transactionManager.failIfNotReadyForSend();"
  },
  {
    "id" : "eb148101-523d-4f61-b431-f42beb575e96",
    "prId" : 6997,
    "prUrl" : "https://github.com/apache/kafka/pull/6997#pullrequestreview-269873842",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2b0b676-af65-4cba-a4f8-639fe32480a7",
        "parentId" : null,
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "Would \"Retrying append due to new batch creation\" be better?",
        "createdAt" : "2019-08-01T20:47:00Z",
        "updatedAt" : "2019-08-01T20:47:00Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      },
      {
        "id" : "4e20a273-5084-46ef-a63e-7a6ee43402dc",
        "parentId" : "b2b0b676-af65-4cba-a4f8-639fe32480a7",
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "This log message will have the most clarity possible! ðŸ™‚",
        "createdAt" : "2019-08-01T20:55:08Z",
        "updatedAt" : "2019-08-01T20:55:08Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      }
    ],
    "commit" : "93f350844be9c391ab375fc9b0c207b4ea4335be",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +926,930 @@                tp = new TopicPartition(record.topic(), partition);\n                if (log.isTraceEnabled()) {\n                    log.trace(\"Retrying because of a new batch, sending the record to topic {} partition {}. The old partition was {}\", record.topic(), partition, prevPartition);\n                }\n                // producer callback will make sure to call both 'callback' and interceptor callback"
  },
  {
    "id" : "ffe97ada-d0a9-4c77-bb02-1a8b75837066",
    "prId" : 7952,
    "prUrl" : "https://github.com/apache/kafka/pull/7952#pullrequestreview-344359583",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "096227f7-cd94-46b0-a30c-0d957842602d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: if you want a new paragraph you need to add `<p>`",
        "createdAt" : "2020-01-16T18:40:49Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "8385e294-0733-410f-92a5-a712f66a50ac",
        "parentId" : "096227f7-cd94-46b0-a30c-0d957842602d",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sounds good",
        "createdAt" : "2020-01-17T04:34:59Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81e0daaec396ec5be7cc3d3cc2adae42f242e95",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +652,656 @@     * also not commit offsets manually (via {@link KafkaConsumer#commitSync(Map) sync} or\n     * {@link KafkaConsumer#commitAsync(Map, OffsetCommitCallback) async} commits).\n     *\n     * @throws IllegalStateException if no transactional.id has been configured or no transaction has been started.\n     * @throws ProducerFencedException fatal error indicating another producer with the same transactional.id is active"
  },
  {
    "id" : "541542b6-6373-41b3-9518-ce784e0440ca",
    "prId" : 7952,
    "prUrl" : "https://github.com/apache/kafka/pull/7952#pullrequestreview-344925824",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99e9ebc6-545c-4111-9e17-e72e1f9f5971",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess we could also get an auth error for the groupId.",
        "createdAt" : "2020-01-18T05:04:32Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81e0daaec396ec5be7cc3d3cc2adae42f242e95",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +661,665 @@     * @throws org.apache.kafka.common.errors.UnsupportedForMessageFormatException fatal error indicating the message\n     *         format used for the offsets topic on the broker does not support transactions\n     * @throws org.apache.kafka.common.errors.AuthorizationException fatal error indicating that the configured\n     *         transactional.id is not authorized, or the consumer group id is not authorized.\n     * @throws org.apache.kafka.clients.consumer.CommitFailedException if the commit failed and cannot be retried"
  },
  {
    "id" : "156f3c2b-70a2-4b3a-a0c1-6c2af04fb949",
    "prId" : 7952,
    "prUrl" : "https://github.com/apache/kafka/pull/7952#pullrequestreview-344969337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8653fac6-9b94-4033-92bb-f8b77dfafad1",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It seems like we don't need to mention 0.11 here since the requirement for 2.5 is stricter.",
        "createdAt" : "2020-01-18T05:15:04Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e6a97080-21f2-468b-aca7-785789ab375d",
        "parentId" : "8653fac6-9b94-4033-92bb-f8b77dfafad1",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "The 0.11 and 2.5 unsupported version exceptions are different IMHO.",
        "createdAt" : "2020-01-19T00:38:41Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81e0daaec396ec5be7cc3d3cc2adae42f242e95",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +656,660 @@     * @throws ProducerFencedException fatal error indicating another producer with the same transactional.id is active\n     * @throws org.apache.kafka.common.errors.UnsupportedVersionException fatal error indicating the broker\n     *         does not support transactions (i.e. if its version is lower than 0.11.0.0) or\n     *         the broker doesn't support latest version of transactional API with consumer group metadata (i.e. if its version is\n     *         lower than 2.5.0)."
  },
  {
    "id" : "eee30bd3-0203-4e19-96a0-7227eddf529a",
    "prId" : 7952,
    "prUrl" : "https://github.com/apache/kafka/pull/7952#pullrequestreview-346251534",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d78b16c4-380b-4e55-9bce-b51276f936f4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.",
        "createdAt" : "2020-01-19T18:49:44Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "749f40af-d764-4620-9de9-16c09dca7582",
        "parentId" : "d78b16c4-380b-4e55-9bce-b51276f936f4",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Why do we need a null check here? The groupMetadata itself will through NPE if it is not defined on `TransactionManager#sendOffsetsToTransaction`",
        "createdAt" : "2020-01-20T20:12:08Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "4547d155-ebc1-4e44-8640-0a719ccce602",
        "parentId" : "d78b16c4-380b-4e55-9bce-b51276f936f4",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We can give a clear message saying null is not supported. If it's an NPE somewhere down the stack then the user doesn't know if it's a bug or not.",
        "createdAt" : "2020-01-21T17:30:09Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9e286a3a-7e20-4742-8b48-2e778446f5ae",
        "parentId" : "d78b16c4-380b-4e55-9bce-b51276f936f4",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "sg",
        "createdAt" : "2020-01-21T23:00:57Z",
        "updatedAt" : "2020-01-22T18:43:33Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81e0daaec396ec5be7cc3d3cc2adae42f242e95",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +671,675 @@     */\n    public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offsets,\n                                         ConsumerGroupMetadata groupMetadata) throws ProducerFencedException {\n        throwIfInvalidGroupMetadata(groupMetadata);\n        throwIfNoTransactionManager();"
  },
  {
    "id" : "cb227602-4e5f-4c0d-9edc-4309f16c8665",
    "prId" : 8375,
    "prUrl" : "https://github.com/apache/kafka/pull/8375#pullrequestreview-383204812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b345f4a-78f6-494d-951e-d99c16558b94",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "`which only sends with consumer group id` -- seems obvious from the API. Should we remove this part?",
        "createdAt" : "2020-03-27T20:17:49Z",
        "updatedAt" : "2020-04-01T01:44:58Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "4448dd7a-723a-44a7-a470-046c6a38c837",
        "parentId" : "6b345f4a-78f6-494d-951e-d99c16558b94",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I feel keeping it is fine, just a more explicit explanation to the `stronger fencing`",
        "createdAt" : "2020-03-27T21:16:37Z",
        "updatedAt" : "2020-04-01T01:44:58Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0a4a0a2bf7f08a6f1fb7c1221118246f9180a36",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +651,655 @@     * {@code groupMetadata} should be extracted from the used {@link KafkaConsumer consumer} via\n     * {@link KafkaConsumer#groupMetadata()} to leverage consumer group metadata for stronger fencing than\n     * {@link #sendOffsetsToTransaction(Map, String)} which only sends with consumer group id.\n     *\n     * <p>"
  },
  {
    "id" : "888986c9-8034-424a-9a30-42c9788534d8",
    "prId" : 8375,
    "prUrl" : "https://github.com/apache/kafka/pull/8375#pullrequestreview-383172906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a83a0a0c-35f6-4251-879a-e25d391510fa",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "missing `<p>`",
        "createdAt" : "2020-03-27T20:19:54Z",
        "updatedAt" : "2020-04-01T01:44:58Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0a4a0a2bf7f08a6f1fb7c1221118246f9180a36",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +652,656 @@     * {@link KafkaConsumer#groupMetadata()} to leverage consumer group metadata for stronger fencing than\n     * {@link #sendOffsetsToTransaction(Map, String)} which only sends with consumer group id.\n     *\n     * <p>\n     * Note, that the consumer should have {@code enable.auto.commit=false} and should"
  },
  {
    "id" : "780c70e9-a774-4a67-a889-5faa9bf99d55",
    "prId" : 9081,
    "prUrl" : "https://github.com/apache/kafka/pull/9081#pullrequestreview-456243108",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cbbd402d-7fab-4244-8e3c-d8ad85fabe56",
        "parentId" : null,
        "authorId" : "2b8ddac3-3f74-403c-9e9d-62dc37cb6655",
        "body" : "javadoc for this method should be updated as well.",
        "createdAt" : "2020-07-27T01:52:53Z",
        "updatedAt" : "2020-07-28T16:03:20Z",
        "lastEditedBy" : "2b8ddac3-3f74-403c-9e9d-62dc37cb6655",
        "tags" : [
        ]
      },
      {
        "id" : "a5ecf51e-5caf-4cae-9484-11c0f3bd1b03",
        "parentId" : "cbbd402d-7fab-4244-8e3c-d8ad85fabe56",
        "authorId" : "2b8255f4-64f1-4086-92a5-5af202b271d7",
        "body" : "I wrote some description related to TimeoutException and InterruptedException",
        "createdAt" : "2020-07-28T01:34:29Z",
        "updatedAt" : "2020-07-28T16:03:20Z",
        "lastEditedBy" : "2b8255f4-64f1-4086-92a5-5af202b271d7",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c20bcab28c5f75d8d380e4f7008e0282f790b49",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +692,696 @@        TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, groupMetadata);\n        sender.wakeup();\n        result.await(maxBlockTimeMs, TimeUnit.MILLISECONDS);\n    }\n"
  }
]