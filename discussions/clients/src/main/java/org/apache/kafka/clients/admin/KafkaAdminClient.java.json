[
  {
    "id" : "b3ffc042-e206-477e-8390-39e725536b3a",
    "prId" : 4263,
    "prUrl" : "https://github.com/apache/kafka/pull/4263#pullrequestreview-85096900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7790e3cf-e653-418f-90dc-4bf93f93aff9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe `DescribeConfigsResponse.ConfigSource` could have a method to get the corresponding `ConfigEntry.ConfigSource`?",
        "createdAt" : "2017-12-20T00:27:48Z",
        "updatedAt" : "2018-01-19T13:34:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d9b2c1a7-12c9-4ea1-89ed-6acd144057ce",
        "parentId" : "7790e3cf-e653-418f-90dc-4bf93f93aff9",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Don't know about that. `ConfigEntry.ConfigSource` is in the admin client's package, so not sure we want to refer to that from the `requests` package. At the moment, it uses the same pattern as `ConfigResource`.",
        "createdAt" : "2017-12-21T15:36:27Z",
        "updatedAt" : "2018-01-19T13:34:10Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d7e19562c3db9e224ad0933b90f58446641c527",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +1622,1626 @@    private ConfigEntry.ConfigSource configSource(DescribeConfigsResponse.ConfigSource source) {\n        ConfigEntry.ConfigSource configSource;\n        switch (source) {\n            case TOPIC_CONFIG:\n                configSource = ConfigEntry.ConfigSource.DYNAMIC_TOPIC_CONFIG;"
  },
  {
    "id" : "7da83bfb-fb6a-4730-bc2f-1deef503f9e3",
    "prId" : 4295,
    "prUrl" : "https://github.com/apache/kafka/pull/4295#pullrequestreview-118087722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eb19d48-3b0f-4dc9-83ba-d33051816760",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "What is the expected behavior if the user ignores the auth error and continues to use the AdminClient? ",
        "createdAt" : "2018-05-04T15:39:54Z",
        "updatedAt" : "2018-05-09T17:37:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "1aecf13a-fec7-4791-abed-491472d3fa63",
        "parentId" : "6eb19d48-3b0f-4dc9-83ba-d33051816760",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "It will continue to throw `AuthenticationException`.\r\n\r\nAfter enough time, another metadata request may be made which may succeed, which would allow future requests to go through.  But we don't spam metadata requests or anything-- if the auth exception is cleared, it will be because of a timeout.",
        "createdAt" : "2018-05-07T17:54:25Z",
        "updatedAt" : "2018-05-09T17:37:08Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e52241619be28f6f5e072cc4f084e0901109c30",
    "line" : 588,
    "diffHunk" : "@@ -1,1 +1177,1181 @@                        log.info(\"Unable to fetch cluster metadata from node {} because of \" +\n                            \"authentication error\", curNode(), e);\n                        metadataManager.update(Cluster.empty(), time.milliseconds(), (AuthenticationException) e);\n                    } else {\n                        log.info(\"Unable to fetch cluster metadata from node {}\","
  },
  {
    "id" : "fe6762eb-8764-41fb-bc9d-0ca2e843441f",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112181287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "For backwards compatibility, the coordinator still supports offset commits for the empty groupId. Maybe we need to weaken this check a little for this API and for `deleteConsumerGroups`?",
        "createdAt" : "2018-04-12T23:28:13Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "665775ed-f673-4789-a8cf-3a61ee7e8a48",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good point.",
        "createdAt" : "2018-04-13T04:47:54Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d337222d-5822-49d7-ae96-f192f070a6ba",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Hmm, interesting... so people can actually use an empty string as a group ID?\r\n\r\nIf I understand correctly, it seems like this point has been fixed, since `KafkaAdminClient#groupIdIsUnrepresentable` now allows empty strings (it only disallows null.)",
        "createdAt" : "2018-04-13T23:04:57Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "a57ccf88-f7bf-4422-9493-78e0d60ed3c2",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Speaking of filtering... as much as possible, we want to have decisions about what names are or are not valid be a server-side decision.  That way it's easy to change the policy without rolling out new clients.  AdminClient has to filter out empty topic names only because the empty string has a special meaning in the RPC.  If we could, we'd let the server do it.  (Obviously null topic names also can't be represented on the wire either, and so must be filtered.)",
        "createdAt" : "2018-04-13T23:07:48Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +2243,2247 @@        final Map<String, KafkaFutureImpl<ConsumerGroupDescription>> futures = new HashMap<>(groupIds.size());\n        for (String groupId: groupIds) {\n            if (groupIdIsUnrepresentable(groupId)) {\n                KafkaFutureImpl<ConsumerGroupDescription> future = new KafkaFutureImpl<>();\n                future.completeExceptionally(new InvalidGroupIdException(\"The given group id '\" +"
  },
  {
    "id" : "3d82f07e-fbfe-470a-a26b-a6ea296b6dc2",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112182310",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Leaving this comment here for lack of a better location. I would have expected that we would have used a `NodeProvider` to handle lookup of the coordinator. Any reason not to?",
        "createdAt" : "2018-04-12T23:47:43Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "1760f7e7-1e23-4d37-b54a-20dd7e16c116",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Not sure what do you mean? We are using `LeastLoadedNodeProvider` for finding the coordinator, and then with the found coordinator we use `ConstantNodeIdProvider(nodeId)` for the follow-up request.",
        "createdAt" : "2018-04-13T05:02:32Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "3aade263-0418-4bf3-b084-deb4bbbc0daa",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "What I meant is that we could have a `CoordinatorProvider`, which does the work of finding the coordinator. We need not do this here, I just thought it seemed like the natural way given the AdminClient abstractions.",
        "createdAt" : "2018-04-13T15:23:25Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "294b522e-eabe-4786-a41b-0dc51b111a2c",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I see what you mean now. I think that is doable when the groupId is given, but 1) with this provider interface it is almost not possible to batch multiple groupIds per coordinator, 2) for list consumers, group ids are not known beforehand and hence we cannot use this provider as well.\r\n\r\nI'll create a JIRA for this for follow-up work, just sharing my thoughts about that here.",
        "createdAt" : "2018-04-13T15:59:13Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8ac55751-b665-4b1c-8bfd-66d85ef4dd0e",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Thanks @hachikuji -- this is a very perceptive comment.  @guozhangwang is right that the NodeProvider is a little too limited to do this correctly at the moment.  There has to be a little more refactoring to make it work.  I have some patches that should help to fix this, but I want to get in the incremental changes first if possible",
        "createdAt" : "2018-04-13T23:16:48Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +2334,2338 @@                @Override\n                void handleFailure(Throwable throwable) {\n                    KafkaFutureImpl<ConsumerGroupDescription> future = futures.get(groupId);\n                    future.completeExceptionally(throwable);\n                }"
  },
  {
    "id" : "71ad752f-8d72-41af-8efd-3b2660571921",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-111826461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff67c597-0942-427b-92bb-820f65fb353b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the error is not `NONE`, do we log it somewhere?",
        "createdAt" : "2018-04-13T00:03:22Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 340,
    "diffHunk" : "@@ -1,1 +2506,2510 @@                                final Errors error = entry.getValue().error;\n\n                                if (error == Errors.NONE) {\n                                    final Long offset = entry.getValue().offset;\n                                    final String metadata = entry.getValue().metadata;"
  },
  {
    "id" : "cde6d58e-13fc-4ab6-a13a-e3b15f2e231a",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112071677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0caa861-1542-476f-9b77-400298a36547",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a little annoying that the API doesn't give us a way to surface these errors somehow. The only way a user can know that the returned offsets are complete is by inspecting the logs. Since the only partition-level error code at the moment seems to be `UNKNOWN_TOPIC_OR_PARTITION`, it may be fine for now, but I'm wondering if we should add something to the API now in case we have more partition-level errors in the future.",
        "createdAt" : "2018-04-13T15:47:04Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "10217b2c-c6ff-4bf7-acac-025328611da6",
        "parentId" : "c0caa861-1542-476f-9b77-400298a36547",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I could change the `OffsetAndMetadata` to `PartitionData` inside `ListOffsetResponse` directly though I'm not a fan of it since we are not technically exposing `ListOffsetResponse` to end users. LMK.",
        "createdAt" : "2018-04-13T16:11:46Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 345,
    "diffHunk" : "@@ -1,1 +2511,2515 @@                                    groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, metadata));\n                                } else {\n                                    log.warn(\"Skipping return offset for {} due to error {}.\", topicPartition, error);\n                                }\n                            }"
  },
  {
    "id" : "b69fd067-ad56-4dd7-ba3c-ada809165835",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112244038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1eebf41-47ff-44bf-8e75-20a4b2165ed9",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I don't think we need a new API `copyWith`. We can just use the standard `thenApply` API, right?",
        "createdAt" : "2018-04-13T23:23:38Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "0ace62ce-b1e0-456a-b1f0-d0d4cd93332a",
        "parentId" : "a1eebf41-47ff-44bf-8e75-20a4b2165ed9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Unfortunately we cannot, because we need to construct a final future variable at the beginning that can be returned at the end of the call, and this future can only be \"initialized\" with the underlying map after the first round trip. `thenApply` will return a new future, which cannot be used here.",
        "createdAt" : "2018-04-15T16:44:45Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +2390,2394 @@                // we have to flatten the future here instead in the result, because we need to wait until the map of nodes\n                // are known from the listNode request.\n                flattenFuture.copyWith(\n                        KafkaFuture.allOf(futuresMap.values().toArray(new KafkaFuture[0])),\n                        new KafkaFuture.BaseFunction<Void, Collection<ConsumerGroupListing>>() {"
  },
  {
    "id" : "a3a6bda7-ffc6-4684-8626-aafabc11ed2f",
    "prId" : 4884,
    "prUrl" : "https://github.com/apache/kafka/pull/4884#pullrequestreview-113029805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9119a9e8-20d6-4fa7-b35e-80e288cda6d4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "By the way, one of the downsides to using the __consumer_offsets topic, is that it effectively makes the `listConsumerGroups` API dependent on having Describe access to this topic.",
        "createdAt" : "2018-04-17T22:56:54Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2125b1902a6bd257a5c73da28b02337aebb3a584",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +2408,2412 @@                        metadataExceptions.add(metadata.error().exception(\"Unable to locate \" +\n                            Topic.GROUP_METADATA_TOPIC_NAME));\n                    } else if (!metadata.topic().equals(Topic.GROUP_METADATA_TOPIC_NAME)) {\n                        metadataExceptions.add(new UnknownServerException(\"Server returned unrequested \" +\n                            \"information about unexpected topic \" + metadata.topic()));"
  },
  {
    "id" : "73a4285b-6e4d-422f-8d50-51ca150bec8e",
    "prId" : 4884,
    "prUrl" : "https://github.com/apache/kafka/pull/4884#pullrequestreview-115400059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's not too clear to me why the synchronization is needed. Are either of `handleResponse` or `handleFailure` called from anywhere except the background thread?",
        "createdAt" : "2018-04-25T23:48:42Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "67f23fd5-4384-43a8-8663-ad7b5d307a0d",
        "parentId" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Good question.  I believe it can... `handleFailure` could be called from `KafkaAdminClient#call`, if the AdminClient has just been shut down.  Theoretically this could happen concurrently with the background thread completing one of the other Call objects that was just created, giving us concurrent modification.\r\n\r\nPart of the reason why there is so little locking elsewhere in AdminClient function bodies is that KafkaFutureImpl instances handle it for us, so this issue doesn't come up.",
        "createdAt" : "2018-04-26T00:17:24Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "927d5c96-dc6b-4a42-bd2f-d61b5740727a",
        "parentId" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Thanks, makes sense. ",
        "createdAt" : "2018-04-26T00:40:22Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2125b1902a6bd257a5c73da28b02337aebb3a584",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +2442,2446 @@                        void handleResponse(AbstractResponse abstractResponse) {\n                            final ListGroupsResponse response = (ListGroupsResponse) abstractResponse;\n                            synchronized (results) {\n                                if (response.error() != Errors.NONE) {\n                                    results.addError(response.error().exception(), node);"
  },
  {
    "id" : "008ea897-04a6-4a46-8dac-496737b245e8",
    "prId" : 4980,
    "prUrl" : "https://github.com/apache/kafka/pull/4980#pullrequestreview-121361455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3405956b-c02a-4cbe-9cd0-631a9ef94077",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Not related to this patch, but we don't seem to be checking the error code here? Maybe we can implement a simple behavior for now and fail the future for this group if the error is not NONE?",
        "createdAt" : "2018-05-17T17:49:12Z",
        "updatedAt" : "2018-05-18T21:31:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "5ef5539e-b869-45a3-8dcc-6c3b32bab025",
        "parentId" : "3405956b-c02a-4cbe-9cd0-631a9ef94077",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "That's a good point.  I will add some basic error handling here.  Another thing we missed in the original patch... :disappointed: ",
        "createdAt" : "2018-05-17T23:55:37Z",
        "updatedAt" : "2018-05-18T21:31:06Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "3b7b0ca5-dd96-4b38-8ef5-67fc12ddbecd",
        "parentId" : "3405956b-c02a-4cbe-9cd0-631a9ef94077",
        "authorId" : "d418e163-e0f8-4dec-9432-7f0eb6df2a1b",
        "body" : "If there is an error like COORDINATOR_NOT_AVAILABLE, we should not send a describeConsumerGroups request. I ran into this issue when I was working on [KAFKA-6884](https://github.com/apache/kafka/pull/5032): ``testResetOffsetsNotExistingGroup`` in ``ResetConsumerGroupOffsetTest`` failed with a ``TimeoutException``. I believe the reason for this is that  ``LeaderNotAvailableException`` is a retriable exception. ",
        "createdAt" : "2018-05-18T10:09:07Z",
        "updatedAt" : "2018-05-18T21:31:06Z",
        "lastEditedBy" : "d418e163-e0f8-4dec-9432-7f0eb6df2a1b",
        "tags" : [
        ]
      }
    ],
    "commit" : "47c37faba10b9efd117803ad709047ebd7aa3e55",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +2347,2351 @@                @Override\n                void handleResponse(AbstractResponse abstractResponse) {\n                    final FindCoordinatorResponse fcResponse = (FindCoordinatorResponse) abstractResponse;\n                    Errors error = fcResponse.error();\n                    if (error == Errors.COORDINATOR_NOT_AVAILABLE) {"
  },
  {
    "id" : "828b6c7c-08ba-4746-b979-a26f3055083a",
    "prId" : 5050,
    "prUrl" : "https://github.com/apache/kafka/pull/5050#pullrequestreview-123435697",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This is a good find. What happens if the metadata request itself is queued up to be sent to a node which is no longer online? Will it be stuck in `callsToSend` until it times out? I am wondering if we should check `NetworkClient.connectionFailed` after every poll() for all requests in `callsToSend` and reenqueue them as we are doing here. This is what we do in `ConsumerNetworkClient`.",
        "createdAt" : "2018-05-24T22:46:51Z",
        "updatedAt" : "2018-05-25T16:24:02Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "c58dfbc7-f344-4302-8fe9-fe8931447da1",
        "parentId" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@hachikuji Thanks for the review. I think metadata requests are handled differently because they use `LeastLoadedNodeProvider` which assigns a node only if a ready node is available. If no ready node is available, then the call stays in `pendingRequests` and gets moved to `callsToSend` only when a node becomes ready after a subsequent poll. If a ready node is available, then it gets moved to `callsToSend` and is removed from `callsToSend` in the same iteration when the request is queued for send. If disconnection is processed in a subsequent poll, then the call is failed and retried using the retry path. We would never expect to see a metadata request in `callsToSend` at this point. Does that make sense?",
        "createdAt" : "2018-05-25T10:40:54Z",
        "updatedAt" : "2018-05-25T16:24:02Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "94d6bb1c-3b99-4ba3-ac6c-aad0b05c6110",
        "parentId" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm, I'm not sure about that. The metadata request uses `MetadataUpdateNodeIdProvider`, which just calls `leastLoadedNode` directly. But the node chosen may not have an established connection, or am I missing something?",
        "createdAt" : "2018-05-25T14:57:09Z",
        "updatedAt" : "2018-05-25T16:24:02Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "cbac399b-f20d-48c3-a991-3fffa749c124",
        "parentId" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@hachikuji Sorry, that was my mistake. I thought that `leastNodedNode` returned ready nodes, but that is not the case. Have updated the code.",
        "createdAt" : "2018-05-25T16:25:09Z",
        "updatedAt" : "2018-05-25T16:25:10Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f55693a81276f03392c3e59f94761f625e909328",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +1164,1168 @@                    MetadataResponse response = (MetadataResponse) abstractResponse;\n                    long now = time.milliseconds();\n                    metadataManager.update(response.cluster(), now);\n                    reassignUnsentCalls(now, false);\n                }"
  },
  {
    "id" : "83a05e59-5e16-4623-8de8-1ae54e17c14e",
    "prId" : 5050,
    "prUrl" : "https://github.com/apache/kafka/pull/5050#pullrequestreview-123455281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3911970f-78e2-49dd-bc29-ec560fcab7a6",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Now that we can use java 8 lambdas, I wonder if we can do this with a Predicate?",
        "createdAt" : "2018-05-25T17:32:57Z",
        "updatedAt" : "2018-05-25T17:32:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f55693a81276f03392c3e59f94761f625e909328",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +981,985 @@         *                         in the last poll\n         */\n        private void reassignUnsentCalls(long now, boolean disconnectedOnly) {\n            ArrayList<Call> pendingCallsToSend = new ArrayList<>();\n            for (Iterator<Map.Entry<Node, List<Call>>> iter = callsToSend.entrySet().iterator(); iter.hasNext(); ) {"
  },
  {
    "id" : "9d6e1add-73e1-488c-abf7-f2ddf6d07dd6",
    "prId" : 5055,
    "prUrl" : "https://github.com/apache/kafka/pull/5055#pullrequestreview-122038197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71abb265-1e02-4129-8a93-7341c99b2ced",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "There's no need for this conditional, right?",
        "createdAt" : "2018-05-21T23:01:01Z",
        "updatedAt" : "2018-05-21T23:01:01Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "c37cbbc2-d0fc-40ef-8132-ad0273c52486",
        "parentId" : "71abb265-1e02-4129-8a93-7341c99b2ced",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "A bit unfortunate that this comment was ignored, it caused a checkstyle error with the new checkstyle:\r\n\r\nhttps://github.com/apache/kafka/pull/5058",
        "createdAt" : "2018-05-22T06:53:09Z",
        "updatedAt" : "2018-05-22T06:53:10Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d92748c47b3ea4894b7ac7994c6205ddb026d16c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +578,582 @@            if ((throwable instanceof UnsupportedVersionException) &&\n                     handleUnsupportedVersionException((UnsupportedVersionException) throwable)) {\n                if (log.isDebugEnabled()) {\n                    log.debug(\"{} attempting protocol downgrade and then retry.\", this);\n                }"
  },
  {
    "id" : "5606eb05-e581-4d00-a9d0-2fa8a8f889cd",
    "prId" : 5112,
    "prUrl" : "https://github.com/apache/kafka/pull/5112#pullrequestreview-125380690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad29dbf8-2def-44c2-98b4-37c271ff2995",
        "parentId" : null,
        "authorId" : "220f032c-6592-42d9-9042-aed276632816",
        "body" : "Not sure this optimization is useful. If there is pending calls, it means these calls can not find nodes to send to and we need to wait for the metadata update before trying again. We already have the logic for reducing pollTimeout based on `metadataFetchDelayMs`. And client.poll() will return immediately after metadata response is received even if we use large pollTimeout. Did I miss something here?",
        "createdAt" : "2018-06-02T19:21:11Z",
        "updatedAt" : "2018-06-02T19:37:23Z",
        "lastEditedBy" : "220f032c-6592-42d9-9042-aed276632816",
        "tags" : [
        ]
      },
      {
        "id" : "c5d13a5b-c85c-4e5e-a3c4-995c1f9a0936",
        "parentId" : "ad29dbf8-2def-44c2-98b4-37c271ff2995",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's less of an optimization and more of a safety net. The intent was to ensure we are not stuck in poll() with a long timeout while we have pending requests waiting to be sent. It may be unnecessary if we're convinced that the poll timeout is computed correctly.",
        "createdAt" : "2018-06-02T23:26:07Z",
        "updatedAt" : "2018-06-02T23:26:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a3d37556-1622-4b2d-935d-8bea3e2b95f9",
        "parentId" : "ad29dbf8-2def-44c2-98b4-37c271ff2995",
        "authorId" : "220f032c-6592-42d9-9042-aed276632816",
        "body" : "I see. It is reasonable to have a safety net.",
        "createdAt" : "2018-06-03T01:09:11Z",
        "updatedAt" : "2018-06-03T01:09:11Z",
        "lastEditedBy" : "220f032c-6592-42d9-9042-aed276632816",
        "tags" : [
        ]
      }
    ],
    "commit" : "88739849e842f3f64e402a6f5aa06c5fa2edd140",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +1106,1110 @@\n                // Ensure that we use a small poll timeout if there are pending calls which need to be sent\n                if (!pendingCalls.isEmpty())\n                    pollTimeout = Math.min(pollTimeout, retryBackoffMs);\n"
  },
  {
    "id" : "87105dac-bdff-4fd6-99af-1ce731b1cef0",
    "prId" : 5578,
    "prUrl" : "https://github.com/apache/kafka/pull/5578#pullrequestreview-150089235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93a3f99d-5b91-47d6-a59a-0dd85f9fa9de",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Can't we just say `return future.completeExceptionally(error.exception());`? As I see `completeExceptionally` returns `false` only in the case when the future has been completed before. Is it a realistic scenario that we complete this future for the second time?",
        "createdAt" : "2018-08-28T12:07:59Z",
        "updatedAt" : "2019-05-09T18:36:41Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d60ed8b759ca9c141bd47f42bc2eefc9c9629c4",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +2646,2650 @@        } else if (error != Errors.NONE) {\n            future.completeExceptionally(error.exception());\n            return true;\n        }\n        return false;"
  },
  {
    "id" : "2c535c9a-4df7-4b88-8b99-9fc7529c15ed",
    "prId" : 5578,
    "prUrl" : "https://github.com/apache/kafka/pull/5578#pullrequestreview-235717914",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd363571-161c-463a-9e13-af7e35f4756d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm guessing the reason you changed this is that NOT_COORDINATOR is considered a retriable exception?",
        "createdAt" : "2019-05-09T02:03:34Z",
        "updatedAt" : "2019-05-09T18:36:41Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b782e01c-bb8e-4759-8d13-022a7d19064b",
        "parentId" : "bd363571-161c-463a-9e13-af7e35f4756d",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "yeah. As you mentioned in the KAFKA-8341 JIRA, we may need  additional logic (find new coordinator?) to handle NOT_COORDINATOR error. For now, NOT_COORDINATOR treated as non-retriable and fail with exception.",
        "createdAt" : "2019-05-09T17:19:18Z",
        "updatedAt" : "2019-05-09T18:36:41Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d60ed8b759ca9c141bd47f42bc2eefc9c9629c4",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +2642,2646 @@\n    private boolean handleGroupRequestError(Errors error, KafkaFutureImpl<?> future) {\n        if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {\n            throw error.exception();\n        } else if (error != Errors.NONE) {"
  },
  {
    "id" : "fa0e17ee-a02a-4c7a-ba5a-7c4f89b2b021",
    "prId" : 5667,
    "prUrl" : "https://github.com/apache/kafka/pull/5667#pullrequestreview-178676810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76db26b1-170d-434f-aa30-af11d68d7dba",
        "parentId" : null,
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "Do we need to add a negative timeout check like we did in [consumer](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L2101)?",
        "createdAt" : "2018-11-27T08:51:49Z",
        "updatedAt" : "2019-01-15T06:10:36Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "52de6f75-6417-492f-a790-ce77dc50cb3d",
        "parentId" : "76db26b1-170d-434f-aa30-af11d68d7dba",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "add the check and related test (see KafkaAdminClientTest.testNegativeTimeout)",
        "createdAt" : "2018-11-27T09:23:22Z",
        "updatedAt" : "2019-01-15T06:10:36Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4c9f2e4cac38bb7094099c44170c9ce595571b0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +442,446 @@    @Override\n    public void close(Duration timeout) {\n        long waitTimeMs = timeout.toMillis();\n        if (waitTimeMs < 0)\n            throw new IllegalArgumentException(\"The timeout cannot be negative.\");"
  },
  {
    "id" : "75a3804b-637b-41c0-8640-1f469c96e6ac",
    "prId" : 6723,
    "prUrl" : "https://github.com/apache/kafka/pull/6723#pullrequestreview-241391496",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89ebe7ea-8093-4e69-ba28-10482dcb34a4",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "nit: also add java doc for type `T, O` here",
        "createdAt" : "2019-05-23T17:59:16Z",
        "updatedAt" : "2019-05-23T18:54:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "af7058b2-797b-490f-b84f-d8e6ab4f2fd2",
        "parentId" : "89ebe7ea-8093-4e69-ba28-10482dcb34a4",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Done.",
        "createdAt" : "2019-05-23T18:59:08Z",
        "updatedAt" : "2019-05-23T18:59:08Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8add37cbf9d01fe3b46d36b295e5bca2a4eacbf9",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +2625,2629 @@     * parameter to schedule action that need to be taken using the coordinator. The param is a Supplier\n     * so that it can be lazily created, so that it can use the results of find coordinator call in its\n     * construction.\n     *\n     * @param <T> The type of return value of the KafkaFuture, like ConsumerGroupDescription, Void etc."
  },
  {
    "id" : "4dc1d773-acfa-4b44-9185-b43a6159d309",
    "prId" : 6723,
    "prUrl" : "https://github.com/apache/kafka/pull/6723#pullrequestreview-241391411",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46ab4908-bad0-4800-b38c-f03f34c5e823",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Do we have NPE risk here?",
        "createdAt" : "2019-05-23T17:59:52Z",
        "updatedAt" : "2019-05-23T18:54:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "2751dfc7-27e1-4010-835b-98f61cbf6763",
        "parentId" : "46ab4908-bad0-4800-b38c-f03f34c5e823",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "findCoordinator request should take care of this and we should not have a null/invalid node.",
        "createdAt" : "2019-05-23T18:58:59Z",
        "updatedAt" : "2019-05-23T18:59:00Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8add37cbf9d01fe3b46d36b295e5bca2a4eacbf9",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +2664,2668 @@        return new Call(\"describeConsumerGroups\",\n                context.getDeadline(),\n                new ConstantNodeIdProvider(context.getNode().get().id())) {\n            @Override\n            AbstractRequest.Builder createRequest(int timeoutMs) {"
  },
  {
    "id" : "00545c37-d886-4ede-9893-829e29da7b16",
    "prId" : 7108,
    "prUrl" : "https://github.com/apache/kafka/pull/7108#pullrequestreview-266307634",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "072fca10-2683-4256-81cd-f6061fc85d58",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "nit: just import entire `ConsumerPartitionAssignor.Assignment` to make the code concise.",
        "createdAt" : "2019-07-25T06:07:33Z",
        "updatedAt" : "2019-07-25T18:05:47Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "20e8f3bc5998d975a2888493d114914fe101d291",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2725,2729 @@                        Set<TopicPartition> partitions = Collections.emptySet();\n                        if (groupMember.memberAssignment().length > 0) {\n                            final ConsumerPartitionAssignor.Assignment assignment = ConsumerProtocol.\n                                deserializeAssignment(ByteBuffer.wrap(groupMember.memberAssignment()));\n                            partitions = new HashSet<>(assignment.partitions());"
  },
  {
    "id" : "135f4162-b246-4866-a17c-6a503eb79147",
    "prId" : 7120,
    "prUrl" : "https://github.com/apache/kafka/pull/7120#pullrequestreview-268474424",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f32db2bc-1cab-43c6-b5b9-624a14442cc5",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "This could be more concise:\r\n```\r\ntopicsToReassignments.getOrDefault(topicPartition.topic(), new TreeMap<>()).put(partition, reassignment);\r\n```",
        "createdAt" : "2019-07-29T15:35:43Z",
        "updatedAt" : "2019-08-13T18:40:44Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "47fbac2c-a895-4d22-9ed3-5cc3349ebbb6",
        "parentId" : "f32db2bc-1cab-43c6-b5b9-624a14442cc5",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "That wouldn't store the `new TreeMap` in `topicsToReassignments` though, right?",
        "createdAt" : "2019-07-30T13:22:51Z",
        "updatedAt" : "2019-08-13T18:40:44Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "f191501f-ba76-4c57-ba1a-063ae677babb",
        "parentId" : "f32db2bc-1cab-43c6-b5b9-624a14442cc5",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Oh, indeed, I missed that. And in this case you'll need to have an extra line with put.",
        "createdAt" : "2019-07-30T15:22:22Z",
        "updatedAt" : "2019-08-13T18:40:44Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "c09a739c9aee8248f5330619de4c98ebb2855675",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +3126,3130 @@                }\n\n                partitionReassignments.put(partition, reassignment);\n            }\n        }"
  },
  {
    "id" : "2d8d795f-7751-4eec-b06f-81970523d97f",
    "prId" : 7120,
    "prUrl" : "https://github.com/apache/kafka/pull/7120#pullrequestreview-274444639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9692e0e7-c642-4c09-9e3b-927a46fa19e9",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "The unrepresentable or invalid topics still get sent later, right?  We need a way of making sure that they don't (perhaps by checking whether the future has already been completed?)",
        "createdAt" : "2019-08-13T16:49:32Z",
        "updatedAt" : "2019-08-13T18:40:44Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "d832d08b-96f0-41a4-9499-e07fee1081c1",
        "parentId" : "9692e0e7-c642-4c09-9e3b-927a46fa19e9",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Oops, nice catch",
        "createdAt" : "2019-08-13T17:38:07Z",
        "updatedAt" : "2019-08-13T18:40:44Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "c09a739c9aee8248f5330619de4c98ebb2855675",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +3260,3264 @@                    partitionReassignmentsFuture.completeExceptionally(new InvalidTopicException(\"The given partition index \" +\n                            partition + \" is not valid.\"));\n                }\n                if (partitionReassignmentsFuture.isCompletedExceptionally())\n                    return new ListPartitionReassignmentsResult(partitionReassignmentsFuture);"
  },
  {
    "id" : "705d9fe9-31a0-4f66-9900-6901a33da9cd",
    "prId" : 7122,
    "prUrl" : "https://github.com/apache/kafka/pull/7122#pullrequestreview-269381037",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31533237-f556-4085-aa1b-6b100d5f8e04",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We should retry find-coordinator first?",
        "createdAt" : "2019-07-30T22:05:53Z",
        "updatedAt" : "2019-09-06T23:52:55Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "c058b60b-fa3b-492a-912e-f30589acc83c",
        "parentId" : "31533237-f556-4085-aa1b-6b100d5f8e04",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Good question, maybe we should do that for all cases.",
        "createdAt" : "2019-08-01T03:35:28Z",
        "updatedAt" : "2019-09-06T23:52:55Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "82afe957-42fe-4c56-b180-ecfc8e6070ff",
        "parentId" : "31533237-f556-4085-aa1b-6b100d5f8e04",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Well since we could have override for `errorCounts()` in subclass, we still need to cast response first to be safe IMO",
        "createdAt" : "2019-08-01T03:40:01Z",
        "updatedAt" : "2019-09-06T23:52:55Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecefd620404778b0643377f2e409920efb9aa2e9",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +3379,3383 @@                // If coordinator changed since we fetched it, retry\n                if (context.hasCoordinatorMoved(response)) {\n                    rescheduleTask(context, () -> getRemoveMembersFromGroupCall(context));\n                    return;\n                }"
  },
  {
    "id" : "f680615b-dc38-4591-8f8c-6c4975b40ffe",
    "prId" : 7122,
    "prUrl" : "https://github.com/apache/kafka/pull/7122#pullrequestreview-268686925",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab33a963-6024-4305-bb6f-1b4bf89205de",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead.\r\n\r\nI think it's better just setting the exception in the future directly.",
        "createdAt" : "2019-07-30T22:11:24Z",
        "updatedAt" : "2019-09-06T23:52:55Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "3d578535-f032-4557-822f-284d09b908f3",
        "parentId" : "ab33a963-6024-4305-bb6f-1b4bf89205de",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Also, we should handle all errors as well so that when constructing the result we know it's successful and hence we do not need to set any errors below.",
        "createdAt" : "2019-07-30T22:13:50Z",
        "updatedAt" : "2019-09-06T23:52:55Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecefd620404778b0643377f2e409920efb9aa2e9",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +3387,3391 @@                if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {\n                    throw error.exception();\n                }\n\n                final RemoveMemberFromGroupResult membershipChangeResult ="
  },
  {
    "id" : "d877ade5-e316-44a3-8932-fe5129685d28",
    "prId" : 7296,
    "prUrl" : "https://github.com/apache/kafka/pull/7296#pullrequestreview-297648479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b42b63b-22af-4977-a4d5-441860c0b842",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If you haven't do so already, can you file a jira to update the `deleteRecords` api to use this?",
        "createdAt" : "2019-10-04T16:47:03Z",
        "updatedAt" : "2019-10-19T14:50:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0e4ae91a-252f-4dbc-9b70-2fc9f6ca30bc",
        "parentId" : "1b42b63b-22af-4977-a4d5-441860c0b842",
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "Good point, I've opened https://issues.apache.org/jira/browse/KAFKA-8982",
        "createdAt" : "2019-10-04T18:33:07Z",
        "updatedAt" : "2019-10-19T14:50:21Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      }
    ],
    "commit" : "adb3377352be6336d1586a4a160eedbcd5a9a021",
    "line" : 465,
    "diffHunk" : "@@ -1,1 +2783,2787 @@\n    /**\n     * Returns a {@code Call} object to fetch the cluster metadata. Takes a List of Calls\n     * parameter to schedule actions that need to be taken using the metadata. The param is a Supplier\n     * so that it can be lazily created, so that it can use the results of the metadata call in its"
  },
  {
    "id" : "ca93b85d-4015-48dd-a69e-0df90f47a627",
    "prId" : 7296,
    "prUrl" : "https://github.com/apache/kafka/pull/7296#pullrequestreview-297666182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc502361-c084-4825-b580-b4242e26a204",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think I'm missing something here. If there is an error for a topic, when do we complete the future for the associated partitions?",
        "createdAt" : "2019-10-04T17:39:24Z",
        "updatedAt" : "2019-10-19T14:50:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "80a74909-c9da-4c2c-98ee-1f5dfe07da60",
        "parentId" : "cc502361-c084-4825-b580-b4242e26a204",
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "Oops! Yes indeed these need to be failed",
        "createdAt" : "2019-10-04T19:08:43Z",
        "updatedAt" : "2019-10-19T14:50:21Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      }
    ],
    "commit" : "adb3377352be6336d1586a4a160eedbcd5a9a021",
    "line" : 896,
    "diffHunk" : "@@ -1,1 +3680,3684 @@            } else {\n                future.completeExceptionally(mr.errors().get(tp.topic()).exception());\n            }\n        }\n"
  },
  {
    "id" : "d2e954fc-b406-46ff-a288-620e740db18b",
    "prId" : 7478,
    "prUrl" : "https://github.com/apache/kafka/pull/7478#pullrequestreview-304853794",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42d779a4-a42d-4b48-ab06-00eec966695c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I don't feel too strongly about it, but I'm not sure this utility is bringing a lot of value. I think the code would be clearer if we inlined these checks.",
        "createdAt" : "2019-10-21T20:39:27Z",
        "updatedAt" : "2019-10-25T02:13:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b2b1ed34-167f-4a3e-8c77-fe702b9b3247",
        "parentId" : "42d779a4-a42d-4b48-ab06-00eec966695c",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "We extracted it for easier unit test purpose.",
        "createdAt" : "2019-10-21T21:11:39Z",
        "updatedAt" : "2019-10-25T02:13:46Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e00990aa16e167cae6d17125063f4a54f778b05",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +3743,3747 @@     * return an {@link IllegalArgumentException}.\n     */\n    static <K> Throwable getSubLevelError(Map<K, Errors> subLevelErrors, K subKey, String keyNotFoundMsg) {\n        if (!subLevelErrors.containsKey(subKey)) {\n            return new IllegalArgumentException(keyNotFoundMsg);"
  },
  {
    "id" : "1949de32-84ec-4900-98b8-0075b80b1bf7",
    "prId" : 7478,
    "prUrl" : "https://github.com/apache/kafka/pull/7478#pullrequestreview-306219021",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4908d602-48c0-4e0e-a6e1-e58ef05422da",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I have a question which is orthogonal to this PR. In the event that this API is used on an older broker, we allow the request to silently downgrade to an older version if there is exactly one member that needs to be removed. However, in this case, we lose the groupInstanceId, which is actually the only field that can be provided with the current API. It seems like we might want to just be strict and require min api version of 3 when LeaveGroup is called through the AdminClient.",
        "createdAt" : "2019-10-21T20:54:46Z",
        "updatedAt" : "2019-10-25T02:13:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "6deec606-8e06-473b-9b48-e94e6cc59ecc",
        "parentId" : "4908d602-48c0-4e0e-a6e1-e58ef05422da",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Tried offline and discovered that the unsupported version exception crashes the network thread instead of populating to the end user. The current conclusion is that we will plan to address this in a separate PR where we also want to add functionality to admin client to surface the exception",
        "createdAt" : "2019-10-23T22:04:05Z",
        "updatedAt" : "2019-10-25T02:13:46Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e00990aa16e167cae6d17125063f4a54f778b05",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +3469,3473 @@\n    @Override\n    public RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroup(String groupId,\n                                                                               RemoveMembersFromConsumerGroupOptions options) {\n        final long startFindCoordinatorMs = time.milliseconds();"
  },
  {
    "id" : "6cca860e-5fb4-4a00-9794-a9e5b67fe0a5",
    "prId" : 7493,
    "prUrl" : "https://github.com/apache/kafka/pull/7493#pullrequestreview-337813496",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36bff2bf-a7c7-4852-bd02-4e0139fff4cb",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Leaving this here for lack of an alternative. I was looking at the defaults used in the schema definitions. We do not override them in any cases, but I wonder if it would make sense in the case of `validateOnly`. If we do not override, the default would be `false`, but maybe `true` is a safer value?\r\n\r\nAnother thing I was looking at is how we handle the `ErrorMessage` field in the response. The previous serialization logic used `ApiError` which attempts to avoid serialization when the error message matches the standard value. I suspect that we might have broken this optimization in other cases as well, though I'm not sure how much it matters. Also, the old code used null as the default instead of the empty string. Perhaps we should do the same?",
        "createdAt" : "2019-12-03T23:48:54Z",
        "updatedAt" : "2020-01-07T19:58:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "403acbe7-3a50-4f7a-81d8-3fabab5faee3",
        "parentId" : "36bff2bf-a7c7-4852-bd02-4e0139fff4cb",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "I agree that its safer to have `validateOnly` default to `true`, but I think the common case is to have it `false`. If I am user of this api, I will assume the api call to create partitions by default. And if there is a need for validation only, only then I will look deeper into api to find a flag for doing that. So in that sense it matches user expectation.\r\n\r\nInteresting comment about the `ErrorMessage`. But when creating `CreatePartitionsResponse` in `CreatePartitionsRequest::getErrorResponse`, we use `apiError.message` to populate the error message, and the method returns null if the message is default. It seems if we use `apiError.messageWithFallback` then we will get into situation that you are describing. I may be missing something here.\r\n\r\nUpdated json to set `null` for `ErrorMessage` field. Thanks for pointing that out.",
        "createdAt" : "2020-01-02T19:20:19Z",
        "updatedAt" : "2020-01-07T19:58:36Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0128e030e3559cde3be5395c63b4b055768546ab",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +2341,2345 @@                CreatePartitionsRequestData requestData = new CreatePartitionsRequestData()\n                        .setTopics(topics)\n                        .setValidateOnly(options.validateOnly())\n                        .setTimeoutMs(timeoutMs);\n"
  },
  {
    "id" : "12587655-34dc-41b8-a49f-2a4d3cdbf906",
    "prId" : 7674,
    "prUrl" : "https://github.com/apache/kafka/pull/7674#pullrequestreview-334999510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84a7569f-0f1a-4bf2-b8d7-a7743f2c4131",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "How does this work with prefixes?  For example, Kafka Connect passes all config variables with a certain prefix to its admin client.  Does `originals` show those variables as prefixed or not here?",
        "createdAt" : "2019-12-19T22:18:23Z",
        "updatedAt" : "2020-02-12T01:34:56Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "b2d18c33-5c0c-48f9-a47c-9fe3543e408a",
        "parentId" : "84a7569f-0f1a-4bf2-b8d7-a7743f2c4131",
        "authorId" : "73896ffa-873a-4115-b79b-370ca722bd60",
        "body" : "my understanding is that originals are the values as originally passed to the admin client. Those should already have prefixes stripped already, since the line above (`config.getString(AdminClientConfig.METRICS_RECORDING_LEVEL_CONFIG)`) does not add any kind of prefixes.",
        "createdAt" : "2019-12-19T23:36:58Z",
        "updatedAt" : "2020-02-12T01:34:56Z",
        "lastEditedBy" : "73896ffa-873a-4115-b79b-370ca722bd60",
        "tags" : [
        ]
      }
    ],
    "commit" : "01208fd218286d2cd318a891f2cb5883422283b1",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +450,454 @@                .tags(metricTags);\n            JmxReporter jmxReporter = new JmxReporter(JMX_PREFIX);\n            jmxReporter.configure(config.originals());\n            reporters.add(jmxReporter);\n            metrics = new Metrics(metricConfig, reporters, time);"
  },
  {
    "id" : "ae18ea5a-1846-4b62-ba0d-057d67241a04",
    "prId" : 7763,
    "prUrl" : "https://github.com/apache/kafka/pull/7763#pullrequestreview-326478631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04697341-f82f-47d4-94b7-dcd282ea09e6",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "What are your thoughts on having `InvalidGroupIdException` here?\r\nIn all regards the KafkaFuture will wrap it in an `ExecutionException` - I'm just wondering if user code could be checking whether the cause is a `KafkaException`.",
        "createdAt" : "2019-11-29T20:13:05Z",
        "updatedAt" : "2019-12-02T10:43:06Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "dfd979dd-5e92-4f67-8f5b-2ea918c1ac71",
        "parentId" : "04697341-f82f-47d4-94b7-dcd282ea09e6",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "I thought about this and I felt like `InvalidGroupIdException` was inappropriate because the group is is valid but the type of the group is not accepted. Thus, `IllegalArgumentException` sounds better to me in this case. Does it make sense?",
        "createdAt" : "2019-12-02T10:05:32Z",
        "updatedAt" : "2019-12-02T10:43:06Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      },
      {
        "id" : "f1d8a27b-88c0-4254-9256-e86e10b744aa",
        "parentId" : "04697341-f82f-47d4-94b7-dcd282ea09e6",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Yeah it's not straightforward. Let's see what others think",
        "createdAt" : "2019-12-02T17:41:01Z",
        "updatedAt" : "2019-12-02T17:41:02Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "c7706f47-f16a-48fb-a0f0-38a7bfa84872",
        "parentId" : "04697341-f82f-47d4-94b7-dcd282ea09e6",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm ok with `IllegalArgumentException`. I think `InvalidGroupIdException` is typically used to indicate a groupId which is structurally invalid in some way (e.g. null). ",
        "createdAt" : "2019-12-03T22:01:36Z",
        "updatedAt" : "2019-12-03T22:01:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ede68327425a9c327d491be0ff73da51da39177",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2776,2780 @@                    context.future().complete(consumerGroupDescription);\n                } else {\n                    context.future().completeExceptionally(new IllegalArgumentException(\n                        String.format(\"GroupId {} is not a consumer group ({}).\",\n                            context.groupId(), protocolType)));"
  },
  {
    "id" : "31f69f96-72b8-4169-9544-01c600532253",
    "prId" : 7878,
    "prUrl" : "https://github.com/apache/kafka/pull/7878#pullrequestreview-342002147",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a550c6c0-8c6a-4ca3-abc9-c9476b922c55",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think it's worth adding a comment about why we set `false` here.",
        "createdAt" : "2020-01-13T17:13:09Z",
        "updatedAt" : "2020-01-14T03:19:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "444fc571446e41ecc8941df7df2de8ef936d73c0",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +3031,3035 @@                // Set the flag to false as for admin client request,\n                // we don't need to wait for any pending offset state to clear.\n                return new OffsetFetchRequest.Builder(context.groupId(), false, context.options().topicPartitions());\n            }\n"
  },
  {
    "id" : "89e41488-7787-48dc-90a9-de92c105d95f",
    "prId" : 7957,
    "prUrl" : "https://github.com/apache/kafka/pull/7957#pullrequestreview-374270485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30b20b41-3c3f-46a7-bc50-ba80e19082ce",
        "parentId" : null,
        "authorId" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "body" : "Same here, I think `computeIfAbsent` would simplify the logic",
        "createdAt" : "2020-03-13T12:47:03Z",
        "updatedAt" : "2020-03-13T14:00:05Z",
        "lastEditedBy" : "d31db46e-de6d-4fea-8dc5-6f7b17b636be",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b037108c621fc4cdc76ad2c64d376fc1bde6d97",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +2489,2493 @@                            Map<String, DeleteRecordsTopic> deletionsForLeader = leaders.computeIfAbsent(\n                                    node, key -> new HashMap<>());\n                            DeleteRecordsTopic deleteRecords = deletionsForLeader.get(topicPartition.topic());\n                            if (deleteRecords == null) {\n                                deleteRecords = new DeleteRecordsTopic()"
  }
]