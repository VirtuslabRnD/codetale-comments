[
  {
    "id" : "b3ffc042-e206-477e-8390-39e725536b3a",
    "prId" : 4263,
    "prUrl" : "https://github.com/apache/kafka/pull/4263#pullrequestreview-85096900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7790e3cf-e653-418f-90dc-4bf93f93aff9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe `DescribeConfigsResponse.ConfigSource` could have a method to get the corresponding `ConfigEntry.ConfigSource`?",
        "createdAt" : "2017-12-20T00:27:48Z",
        "updatedAt" : "2018-01-19T13:34:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d9b2c1a7-12c9-4ea1-89ed-6acd144057ce",
        "parentId" : "7790e3cf-e653-418f-90dc-4bf93f93aff9",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Don't know about that. `ConfigEntry.ConfigSource` is in the admin client's package, so not sure we want to refer to that from the `requests` package. At the moment, it uses the same pattern as `ConfigResource`.",
        "createdAt" : "2017-12-21T15:36:27Z",
        "updatedAt" : "2018-01-19T13:34:10Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d7e19562c3db9e224ad0933b90f58446641c527",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +1622,1626 @@    private ConfigEntry.ConfigSource configSource(DescribeConfigsResponse.ConfigSource source) {\n        ConfigEntry.ConfigSource configSource;\n        switch (source) {\n            case TOPIC_CONFIG:\n                configSource = ConfigEntry.ConfigSource.DYNAMIC_TOPIC_CONFIG;"
  },
  {
    "id" : "7da83bfb-fb6a-4730-bc2f-1deef503f9e3",
    "prId" : 4295,
    "prUrl" : "https://github.com/apache/kafka/pull/4295#pullrequestreview-118087722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eb19d48-3b0f-4dc9-83ba-d33051816760",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "What is the expected behavior if the user ignores the auth error and continues to use the AdminClient? ",
        "createdAt" : "2018-05-04T15:39:54Z",
        "updatedAt" : "2018-05-09T17:37:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "1aecf13a-fec7-4791-abed-491472d3fa63",
        "parentId" : "6eb19d48-3b0f-4dc9-83ba-d33051816760",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "It will continue to throw `AuthenticationException`.\r\n\r\nAfter enough time, another metadata request may be made which may succeed, which would allow future requests to go through.  But we don't spam metadata requests or anything-- if the auth exception is cleared, it will be because of a timeout.",
        "createdAt" : "2018-05-07T17:54:25Z",
        "updatedAt" : "2018-05-09T17:37:08Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e52241619be28f6f5e072cc4f084e0901109c30",
    "line" : 588,
    "diffHunk" : "@@ -1,1 +1177,1181 @@                        log.info(\"Unable to fetch cluster metadata from node {} because of \" +\n                            \"authentication error\", curNode(), e);\n                        metadataManager.update(Cluster.empty(), time.milliseconds(), (AuthenticationException) e);\n                    } else {\n                        log.info(\"Unable to fetch cluster metadata from node {}\","
  },
  {
    "id" : "fe6762eb-8764-41fb-bc9d-0ca2e843441f",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112181287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "For backwards compatibility, the coordinator still supports offset commits for the empty groupId. Maybe we need to weaken this check a little for this API and for `deleteConsumerGroups`?",
        "createdAt" : "2018-04-12T23:28:13Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "665775ed-f673-4789-a8cf-3a61ee7e8a48",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good point.",
        "createdAt" : "2018-04-13T04:47:54Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d337222d-5822-49d7-ae96-f192f070a6ba",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Hmm, interesting... so people can actually use an empty string as a group ID?\r\n\r\nIf I understand correctly, it seems like this point has been fixed, since `KafkaAdminClient#groupIdIsUnrepresentable` now allows empty strings (it only disallows null.)",
        "createdAt" : "2018-04-13T23:04:57Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "a57ccf88-f7bf-4422-9493-78e0d60ed3c2",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Speaking of filtering... as much as possible, we want to have decisions about what names are or are not valid be a server-side decision.  That way it's easy to change the policy without rolling out new clients.  AdminClient has to filter out empty topic names only because the empty string has a special meaning in the RPC.  If we could, we'd let the server do it.  (Obviously null topic names also can't be represented on the wire either, and so must be filtered.)",
        "createdAt" : "2018-04-13T23:07:48Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +2243,2247 @@        final Map<String, KafkaFutureImpl<ConsumerGroupDescription>> futures = new HashMap<>(groupIds.size());\n        for (String groupId: groupIds) {\n            if (groupIdIsUnrepresentable(groupId)) {\n                KafkaFutureImpl<ConsumerGroupDescription> future = new KafkaFutureImpl<>();\n                future.completeExceptionally(new InvalidGroupIdException(\"The given group id '\" +"
  },
  {
    "id" : "3d82f07e-fbfe-470a-a26b-a6ea296b6dc2",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112182310",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Leaving this comment here for lack of a better location. I would have expected that we would have used a `NodeProvider` to handle lookup of the coordinator. Any reason not to?",
        "createdAt" : "2018-04-12T23:47:43Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "1760f7e7-1e23-4d37-b54a-20dd7e16c116",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Not sure what do you mean? We are using `LeastLoadedNodeProvider` for finding the coordinator, and then with the found coordinator we use `ConstantNodeIdProvider(nodeId)` for the follow-up request.",
        "createdAt" : "2018-04-13T05:02:32Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "3aade263-0418-4bf3-b084-deb4bbbc0daa",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "What I meant is that we could have a `CoordinatorProvider`, which does the work of finding the coordinator. We need not do this here, I just thought it seemed like the natural way given the AdminClient abstractions.",
        "createdAt" : "2018-04-13T15:23:25Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "294b522e-eabe-4786-a41b-0dc51b111a2c",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I see what you mean now. I think that is doable when the groupId is given, but 1) with this provider interface it is almost not possible to batch multiple groupIds per coordinator, 2) for list consumers, group ids are not known beforehand and hence we cannot use this provider as well.\r\n\r\nI'll create a JIRA for this for follow-up work, just sharing my thoughts about that here.",
        "createdAt" : "2018-04-13T15:59:13Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8ac55751-b665-4b1c-8bfd-66d85ef4dd0e",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Thanks @hachikuji -- this is a very perceptive comment.  @guozhangwang is right that the NodeProvider is a little too limited to do this correctly at the moment.  There has to be a little more refactoring to make it work.  I have some patches that should help to fix this, but I want to get in the incremental changes first if possible",
        "createdAt" : "2018-04-13T23:16:48Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +2334,2338 @@                @Override\n                void handleFailure(Throwable throwable) {\n                    KafkaFutureImpl<ConsumerGroupDescription> future = futures.get(groupId);\n                    future.completeExceptionally(throwable);\n                }"
  },
  {
    "id" : "71ad752f-8d72-41af-8efd-3b2660571921",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-111826461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff67c597-0942-427b-92bb-820f65fb353b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the error is not `NONE`, do we log it somewhere?",
        "createdAt" : "2018-04-13T00:03:22Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 340,
    "diffHunk" : "@@ -1,1 +2506,2510 @@                                final Errors error = entry.getValue().error;\n\n                                if (error == Errors.NONE) {\n                                    final Long offset = entry.getValue().offset;\n                                    final String metadata = entry.getValue().metadata;"
  },
  {
    "id" : "cde6d58e-13fc-4ab6-a13a-e3b15f2e231a",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112071677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0caa861-1542-476f-9b77-400298a36547",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a little annoying that the API doesn't give us a way to surface these errors somehow. The only way a user can know that the returned offsets are complete is by inspecting the logs. Since the only partition-level error code at the moment seems to be `UNKNOWN_TOPIC_OR_PARTITION`, it may be fine for now, but I'm wondering if we should add something to the API now in case we have more partition-level errors in the future.",
        "createdAt" : "2018-04-13T15:47:04Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "10217b2c-c6ff-4bf7-acac-025328611da6",
        "parentId" : "c0caa861-1542-476f-9b77-400298a36547",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I could change the `OffsetAndMetadata` to `PartitionData` inside `ListOffsetResponse` directly though I'm not a fan of it since we are not technically exposing `ListOffsetResponse` to end users. LMK.",
        "createdAt" : "2018-04-13T16:11:46Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 345,
    "diffHunk" : "@@ -1,1 +2511,2515 @@                                    groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, metadata));\n                                } else {\n                                    log.warn(\"Skipping return offset for {} due to error {}.\", topicPartition, error);\n                                }\n                            }"
  },
  {
    "id" : "b69fd067-ad56-4dd7-ba3c-ada809165835",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112244038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1eebf41-47ff-44bf-8e75-20a4b2165ed9",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I don't think we need a new API `copyWith`. We can just use the standard `thenApply` API, right?",
        "createdAt" : "2018-04-13T23:23:38Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "0ace62ce-b1e0-456a-b1f0-d0d4cd93332a",
        "parentId" : "a1eebf41-47ff-44bf-8e75-20a4b2165ed9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Unfortunately we cannot, because we need to construct a final future variable at the beginning that can be returned at the end of the call, and this future can only be \"initialized\" with the underlying map after the first round trip. `thenApply` will return a new future, which cannot be used here.",
        "createdAt" : "2018-04-15T16:44:45Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +2390,2394 @@                // we have to flatten the future here instead in the result, because we need to wait until the map of nodes\n                // are known from the listNode request.\n                flattenFuture.copyWith(\n                        KafkaFuture.allOf(futuresMap.values().toArray(new KafkaFuture[0])),\n                        new KafkaFuture.BaseFunction<Void, Collection<ConsumerGroupListing>>() {"
  },
  {
    "id" : "a3a6bda7-ffc6-4684-8626-aafabc11ed2f",
    "prId" : 4884,
    "prUrl" : "https://github.com/apache/kafka/pull/4884#pullrequestreview-113029805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9119a9e8-20d6-4fa7-b35e-80e288cda6d4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "By the way, one of the downsides to using the __consumer_offsets topic, is that it effectively makes the `listConsumerGroups` API dependent on having Describe access to this topic.",
        "createdAt" : "2018-04-17T22:56:54Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2125b1902a6bd257a5c73da28b02337aebb3a584",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +2408,2412 @@                        metadataExceptions.add(metadata.error().exception(\"Unable to locate \" +\n                            Topic.GROUP_METADATA_TOPIC_NAME));\n                    } else if (!metadata.topic().equals(Topic.GROUP_METADATA_TOPIC_NAME)) {\n                        metadataExceptions.add(new UnknownServerException(\"Server returned unrequested \" +\n                            \"information about unexpected topic \" + metadata.topic()));"
  },
  {
    "id" : "73a4285b-6e4d-422f-8d50-51ca150bec8e",
    "prId" : 4884,
    "prUrl" : "https://github.com/apache/kafka/pull/4884#pullrequestreview-115400059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's not too clear to me why the synchronization is needed. Are either of `handleResponse` or `handleFailure` called from anywhere except the background thread?",
        "createdAt" : "2018-04-25T23:48:42Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "67f23fd5-4384-43a8-8663-ad7b5d307a0d",
        "parentId" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Good question.  I believe it can... `handleFailure` could be called from `KafkaAdminClient#call`, if the AdminClient has just been shut down.  Theoretically this could happen concurrently with the background thread completing one of the other Call objects that was just created, giving us concurrent modification.\r\n\r\nPart of the reason why there is so little locking elsewhere in AdminClient function bodies is that KafkaFutureImpl instances handle it for us, so this issue doesn't come up.",
        "createdAt" : "2018-04-26T00:17:24Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "927d5c96-dc6b-4a42-bd2f-d61b5740727a",
        "parentId" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Thanks, makes sense. ",
        "createdAt" : "2018-04-26T00:40:22Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2125b1902a6bd257a5c73da28b02337aebb3a584",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +2442,2446 @@                        void handleResponse(AbstractResponse abstractResponse) {\n                            final ListGroupsResponse response = (ListGroupsResponse) abstractResponse;\n                            synchronized (results) {\n                                if (response.error() != Errors.NONE) {\n                                    results.addError(response.error().exception(), node);"
  },
  {
    "id" : "008ea897-04a6-4a46-8dac-496737b245e8",
    "prId" : 4980,
    "prUrl" : "https://github.com/apache/kafka/pull/4980#pullrequestreview-121361455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3405956b-c02a-4cbe-9cd0-631a9ef94077",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Not related to this patch, but we don't seem to be checking the error code here? Maybe we can implement a simple behavior for now and fail the future for this group if the error is not NONE?",
        "createdAt" : "2018-05-17T17:49:12Z",
        "updatedAt" : "2018-05-18T21:31:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "5ef5539e-b869-45a3-8dcc-6c3b32bab025",
        "parentId" : "3405956b-c02a-4cbe-9cd0-631a9ef94077",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "That's a good point.  I will add some basic error handling here.  Another thing we missed in the original patch... :disappointed: ",
        "createdAt" : "2018-05-17T23:55:37Z",
        "updatedAt" : "2018-05-18T21:31:06Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "3b7b0ca5-dd96-4b38-8ef5-67fc12ddbecd",
        "parentId" : "3405956b-c02a-4cbe-9cd0-631a9ef94077",
        "authorId" : "d418e163-e0f8-4dec-9432-7f0eb6df2a1b",
        "body" : "If there is an error like COORDINATOR_NOT_AVAILABLE, we should not send a describeConsumerGroups request. I ran into this issue when I was working on [KAFKA-6884](https://github.com/apache/kafka/pull/5032): ``testResetOffsetsNotExistingGroup`` in ``ResetConsumerGroupOffsetTest`` failed with a ``TimeoutException``. I believe the reason for this is that  ``LeaderNotAvailableException`` is a retriable exception. ",
        "createdAt" : "2018-05-18T10:09:07Z",
        "updatedAt" : "2018-05-18T21:31:06Z",
        "lastEditedBy" : "d418e163-e0f8-4dec-9432-7f0eb6df2a1b",
        "tags" : [
        ]
      }
    ],
    "commit" : "47c37faba10b9efd117803ad709047ebd7aa3e55",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +2347,2351 @@                @Override\n                void handleResponse(AbstractResponse abstractResponse) {\n                    final FindCoordinatorResponse fcResponse = (FindCoordinatorResponse) abstractResponse;\n                    Errors error = fcResponse.error();\n                    if (error == Errors.COORDINATOR_NOT_AVAILABLE) {"
  },
  {
    "id" : "828b6c7c-08ba-4746-b979-a26f3055083a",
    "prId" : 5050,
    "prUrl" : "https://github.com/apache/kafka/pull/5050#pullrequestreview-123435697",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This is a good find. What happens if the metadata request itself is queued up to be sent to a node which is no longer online? Will it be stuck in `callsToSend` until it times out? I am wondering if we should check `NetworkClient.connectionFailed` after every poll() for all requests in `callsToSend` and reenqueue them as we are doing here. This is what we do in `ConsumerNetworkClient`.",
        "createdAt" : "2018-05-24T22:46:51Z",
        "updatedAt" : "2018-05-25T16:24:02Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "c58dfbc7-f344-4302-8fe9-fe8931447da1",
        "parentId" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@hachikuji Thanks for the review. I think metadata requests are handled differently because they use `LeastLoadedNodeProvider` which assigns a node only if a ready node is available. If no ready node is available, then the call stays in `pendingRequests` and gets moved to `callsToSend` only when a node becomes ready after a subsequent poll. If a ready node is available, then it gets moved to `callsToSend` and is removed from `callsToSend` in the same iteration when the request is queued for send. If disconnection is processed in a subsequent poll, then the call is failed and retried using the retry path. We would never expect to see a metadata request in `callsToSend` at this point. Does that make sense?",
        "createdAt" : "2018-05-25T10:40:54Z",
        "updatedAt" : "2018-05-25T16:24:02Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "94d6bb1c-3b99-4ba3-ac6c-aad0b05c6110",
        "parentId" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm, I'm not sure about that. The metadata request uses `MetadataUpdateNodeIdProvider`, which just calls `leastLoadedNode` directly. But the node chosen may not have an established connection, or am I missing something?",
        "createdAt" : "2018-05-25T14:57:09Z",
        "updatedAt" : "2018-05-25T16:24:02Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "cbac399b-f20d-48c3-a991-3fffa749c124",
        "parentId" : "ccbc105f-6b05-4dfd-b92c-cfcc2e4cea27",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@hachikuji Sorry, that was my mistake. I thought that `leastNodedNode` returned ready nodes, but that is not the case. Have updated the code.",
        "createdAt" : "2018-05-25T16:25:09Z",
        "updatedAt" : "2018-05-25T16:25:10Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f55693a81276f03392c3e59f94761f625e909328",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +1164,1168 @@                    MetadataResponse response = (MetadataResponse) abstractResponse;\n                    long now = time.milliseconds();\n                    metadataManager.update(response.cluster(), now);\n                    reassignUnsentCalls(now, false);\n                }"
  },
  {
    "id" : "83a05e59-5e16-4623-8de8-1ae54e17c14e",
    "prId" : 5050,
    "prUrl" : "https://github.com/apache/kafka/pull/5050#pullrequestreview-123455281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3911970f-78e2-49dd-bc29-ec560fcab7a6",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Now that we can use java 8 lambdas, I wonder if we can do this with a Predicate?",
        "createdAt" : "2018-05-25T17:32:57Z",
        "updatedAt" : "2018-05-25T17:32:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f55693a81276f03392c3e59f94761f625e909328",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +981,985 @@         *                         in the last poll\n         */\n        private void reassignUnsentCalls(long now, boolean disconnectedOnly) {\n            ArrayList<Call> pendingCallsToSend = new ArrayList<>();\n            for (Iterator<Map.Entry<Node, List<Call>>> iter = callsToSend.entrySet().iterator(); iter.hasNext(); ) {"
  },
  {
    "id" : "9d6e1add-73e1-488c-abf7-f2ddf6d07dd6",
    "prId" : 5055,
    "prUrl" : "https://github.com/apache/kafka/pull/5055#pullrequestreview-122038197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71abb265-1e02-4129-8a93-7341c99b2ced",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "There's no need for this conditional, right?",
        "createdAt" : "2018-05-21T23:01:01Z",
        "updatedAt" : "2018-05-21T23:01:01Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "c37cbbc2-d0fc-40ef-8132-ad0273c52486",
        "parentId" : "71abb265-1e02-4129-8a93-7341c99b2ced",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "A bit unfortunate that this comment was ignored, it caused a checkstyle error with the new checkstyle:\r\n\r\nhttps://github.com/apache/kafka/pull/5058",
        "createdAt" : "2018-05-22T06:53:09Z",
        "updatedAt" : "2018-05-22T06:53:10Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d92748c47b3ea4894b7ac7994c6205ddb026d16c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +578,582 @@            if ((throwable instanceof UnsupportedVersionException) &&\n                     handleUnsupportedVersionException((UnsupportedVersionException) throwable)) {\n                if (log.isDebugEnabled()) {\n                    log.debug(\"{} attempting protocol downgrade and then retry.\", this);\n                }"
  },
  {
    "id" : "5606eb05-e581-4d00-a9d0-2fa8a8f889cd",
    "prId" : 5112,
    "prUrl" : "https://github.com/apache/kafka/pull/5112#pullrequestreview-125380690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad29dbf8-2def-44c2-98b4-37c271ff2995",
        "parentId" : null,
        "authorId" : "220f032c-6592-42d9-9042-aed276632816",
        "body" : "Not sure this optimization is useful. If there is pending calls, it means these calls can not find nodes to send to and we need to wait for the metadata update before trying again. We already have the logic for reducing pollTimeout based on `metadataFetchDelayMs`. And client.poll() will return immediately after metadata response is received even if we use large pollTimeout. Did I miss something here?",
        "createdAt" : "2018-06-02T19:21:11Z",
        "updatedAt" : "2018-06-02T19:37:23Z",
        "lastEditedBy" : "220f032c-6592-42d9-9042-aed276632816",
        "tags" : [
        ]
      },
      {
        "id" : "c5d13a5b-c85c-4e5e-a3c4-995c1f9a0936",
        "parentId" : "ad29dbf8-2def-44c2-98b4-37c271ff2995",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's less of an optimization and more of a safety net. The intent was to ensure we are not stuck in poll() with a long timeout while we have pending requests waiting to be sent. It may be unnecessary if we're convinced that the poll timeout is computed correctly.",
        "createdAt" : "2018-06-02T23:26:07Z",
        "updatedAt" : "2018-06-02T23:26:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a3d37556-1622-4b2d-935d-8bea3e2b95f9",
        "parentId" : "ad29dbf8-2def-44c2-98b4-37c271ff2995",
        "authorId" : "220f032c-6592-42d9-9042-aed276632816",
        "body" : "I see. It is reasonable to have a safety net.",
        "createdAt" : "2018-06-03T01:09:11Z",
        "updatedAt" : "2018-06-03T01:09:11Z",
        "lastEditedBy" : "220f032c-6592-42d9-9042-aed276632816",
        "tags" : [
        ]
      }
    ],
    "commit" : "88739849e842f3f64e402a6f5aa06c5fa2edd140",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +1106,1110 @@\n                // Ensure that we use a small poll timeout if there are pending calls which need to be sent\n                if (!pendingCalls.isEmpty())\n                    pollTimeout = Math.min(pollTimeout, retryBackoffMs);\n"
  },
  {
    "id" : "87105dac-bdff-4fd6-99af-1ce731b1cef0",
    "prId" : 5578,
    "prUrl" : "https://github.com/apache/kafka/pull/5578#pullrequestreview-150089235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93a3f99d-5b91-47d6-a59a-0dd85f9fa9de",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Can't we just say `return future.completeExceptionally(error.exception());`? As I see `completeExceptionally` returns `false` only in the case when the future has been completed before. Is it a realistic scenario that we complete this future for the second time?",
        "createdAt" : "2018-08-28T12:07:59Z",
        "updatedAt" : "2019-05-09T18:36:41Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d60ed8b759ca9c141bd47f42bc2eefc9c9629c4",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +2646,2650 @@        } else if (error != Errors.NONE) {\n            future.completeExceptionally(error.exception());\n            return true;\n        }\n        return false;"
  },
  {
    "id" : "2c535c9a-4df7-4b88-8b99-9fc7529c15ed",
    "prId" : 5578,
    "prUrl" : "https://github.com/apache/kafka/pull/5578#pullrequestreview-235717914",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd363571-161c-463a-9e13-af7e35f4756d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm guessing the reason you changed this is that NOT_COORDINATOR is considered a retriable exception?",
        "createdAt" : "2019-05-09T02:03:34Z",
        "updatedAt" : "2019-05-09T18:36:41Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b782e01c-bb8e-4759-8d13-022a7d19064b",
        "parentId" : "bd363571-161c-463a-9e13-af7e35f4756d",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "yeah. As you mentioned in the KAFKA-8341 JIRA, we may need  additional logic (find new coordinator?) to handle NOT_COORDINATOR error. For now, NOT_COORDINATOR treated as non-retriable and fail with exception.",
        "createdAt" : "2019-05-09T17:19:18Z",
        "updatedAt" : "2019-05-09T18:36:41Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d60ed8b759ca9c141bd47f42bc2eefc9c9629c4",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +2642,2646 @@\n    private boolean handleGroupRequestError(Errors error, KafkaFutureImpl<?> future) {\n        if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {\n            throw error.exception();\n        } else if (error != Errors.NONE) {"
  },
  {
    "id" : "fa0e17ee-a02a-4c7a-ba5a-7c4f89b2b021",
    "prId" : 5667,
    "prUrl" : "https://github.com/apache/kafka/pull/5667#pullrequestreview-178676810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76db26b1-170d-434f-aa30-af11d68d7dba",
        "parentId" : null,
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "Do we need to add a negative timeout check like we did in [consumer](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L2101)?",
        "createdAt" : "2018-11-27T08:51:49Z",
        "updatedAt" : "2019-01-15T06:10:36Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "52de6f75-6417-492f-a790-ce77dc50cb3d",
        "parentId" : "76db26b1-170d-434f-aa30-af11d68d7dba",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "add the check and related test (see KafkaAdminClientTest.testNegativeTimeout)",
        "createdAt" : "2018-11-27T09:23:22Z",
        "updatedAt" : "2019-01-15T06:10:36Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4c9f2e4cac38bb7094099c44170c9ce595571b0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +442,446 @@    @Override\n    public void close(Duration timeout) {\n        long waitTimeMs = timeout.toMillis();\n        if (waitTimeMs < 0)\n            throw new IllegalArgumentException(\"The timeout cannot be negative.\");"
  },
  {
    "id" : "75a3804b-637b-41c0-8640-1f469c96e6ac",
    "prId" : 6723,
    "prUrl" : "https://github.com/apache/kafka/pull/6723#pullrequestreview-241391496",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89ebe7ea-8093-4e69-ba28-10482dcb34a4",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "nit: also add java doc for type `T, O` here",
        "createdAt" : "2019-05-23T17:59:16Z",
        "updatedAt" : "2019-05-23T18:54:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "af7058b2-797b-490f-b84f-d8e6ab4f2fd2",
        "parentId" : "89ebe7ea-8093-4e69-ba28-10482dcb34a4",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Done.",
        "createdAt" : "2019-05-23T18:59:08Z",
        "updatedAt" : "2019-05-23T18:59:08Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8add37cbf9d01fe3b46d36b295e5bca2a4eacbf9",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +2625,2629 @@     * parameter to schedule action that need to be taken using the coordinator. The param is a Supplier\n     * so that it can be lazily created, so that it can use the results of find coordinator call in its\n     * construction.\n     *\n     * @param <T> The type of return value of the KafkaFuture, like ConsumerGroupDescription, Void etc."
  },
  {
    "id" : "4dc1d773-acfa-4b44-9185-b43a6159d309",
    "prId" : 6723,
    "prUrl" : "https://github.com/apache/kafka/pull/6723#pullrequestreview-241391411",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46ab4908-bad0-4800-b38c-f03f34c5e823",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Do we have NPE risk here?",
        "createdAt" : "2019-05-23T17:59:52Z",
        "updatedAt" : "2019-05-23T18:54:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "2751dfc7-27e1-4010-835b-98f61cbf6763",
        "parentId" : "46ab4908-bad0-4800-b38c-f03f34c5e823",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "findCoordinator request should take care of this and we should not have a null/invalid node.",
        "createdAt" : "2019-05-23T18:58:59Z",
        "updatedAt" : "2019-05-23T18:59:00Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8add37cbf9d01fe3b46d36b295e5bca2a4eacbf9",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +2664,2668 @@        return new Call(\"describeConsumerGroups\",\n                context.getDeadline(),\n                new ConstantNodeIdProvider(context.getNode().get().id())) {\n            @Override\n            AbstractRequest.Builder createRequest(int timeoutMs) {"
  }
]