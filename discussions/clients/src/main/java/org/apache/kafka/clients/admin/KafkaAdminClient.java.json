[
  {
    "id" : "b3ffc042-e206-477e-8390-39e725536b3a",
    "prId" : 4263,
    "prUrl" : "https://github.com/apache/kafka/pull/4263#pullrequestreview-85096900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7790e3cf-e653-418f-90dc-4bf93f93aff9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe `DescribeConfigsResponse.ConfigSource` could have a method to get the corresponding `ConfigEntry.ConfigSource`?",
        "createdAt" : "2017-12-20T00:27:48Z",
        "updatedAt" : "2018-01-19T13:34:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d9b2c1a7-12c9-4ea1-89ed-6acd144057ce",
        "parentId" : "7790e3cf-e653-418f-90dc-4bf93f93aff9",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Don't know about that. `ConfigEntry.ConfigSource` is in the admin client's package, so not sure we want to refer to that from the `requests` package. At the moment, it uses the same pattern as `ConfigResource`.",
        "createdAt" : "2017-12-21T15:36:27Z",
        "updatedAt" : "2018-01-19T13:34:10Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d7e19562c3db9e224ad0933b90f58446641c527",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +1622,1626 @@    private ConfigEntry.ConfigSource configSource(DescribeConfigsResponse.ConfigSource source) {\n        ConfigEntry.ConfigSource configSource;\n        switch (source) {\n            case TOPIC_CONFIG:\n                configSource = ConfigEntry.ConfigSource.DYNAMIC_TOPIC_CONFIG;"
  },
  {
    "id" : "7da83bfb-fb6a-4730-bc2f-1deef503f9e3",
    "prId" : 4295,
    "prUrl" : "https://github.com/apache/kafka/pull/4295#pullrequestreview-118087722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eb19d48-3b0f-4dc9-83ba-d33051816760",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "What is the expected behavior if the user ignores the auth error and continues to use the AdminClient? ",
        "createdAt" : "2018-05-04T15:39:54Z",
        "updatedAt" : "2018-05-09T17:37:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "1aecf13a-fec7-4791-abed-491472d3fa63",
        "parentId" : "6eb19d48-3b0f-4dc9-83ba-d33051816760",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "It will continue to throw `AuthenticationException`.\r\n\r\nAfter enough time, another metadata request may be made which may succeed, which would allow future requests to go through.  But we don't spam metadata requests or anything-- if the auth exception is cleared, it will be because of a timeout.",
        "createdAt" : "2018-05-07T17:54:25Z",
        "updatedAt" : "2018-05-09T17:37:08Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e52241619be28f6f5e072cc4f084e0901109c30",
    "line" : 588,
    "diffHunk" : "@@ -1,1 +1177,1181 @@                        log.info(\"Unable to fetch cluster metadata from node {} because of \" +\n                            \"authentication error\", curNode(), e);\n                        metadataManager.update(Cluster.empty(), time.milliseconds(), (AuthenticationException) e);\n                    } else {\n                        log.info(\"Unable to fetch cluster metadata from node {}\","
  },
  {
    "id" : "fe6762eb-8764-41fb-bc9d-0ca2e843441f",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112181287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "For backwards compatibility, the coordinator still supports offset commits for the empty groupId. Maybe we need to weaken this check a little for this API and for `deleteConsumerGroups`?",
        "createdAt" : "2018-04-12T23:28:13Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "665775ed-f673-4789-a8cf-3a61ee7e8a48",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good point.",
        "createdAt" : "2018-04-13T04:47:54Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d337222d-5822-49d7-ae96-f192f070a6ba",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Hmm, interesting... so people can actually use an empty string as a group ID?\r\n\r\nIf I understand correctly, it seems like this point has been fixed, since `KafkaAdminClient#groupIdIsUnrepresentable` now allows empty strings (it only disallows null.)",
        "createdAt" : "2018-04-13T23:04:57Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "a57ccf88-f7bf-4422-9493-78e0d60ed3c2",
        "parentId" : "2a6cbbf4-2a5e-4512-ba8e-2796070b414b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Speaking of filtering... as much as possible, we want to have decisions about what names are or are not valid be a server-side decision.  That way it's easy to change the policy without rolling out new clients.  AdminClient has to filter out empty topic names only because the empty string has a special meaning in the RPC.  If we could, we'd let the server do it.  (Obviously null topic names also can't be represented on the wire either, and so must be filtered.)",
        "createdAt" : "2018-04-13T23:07:48Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +2243,2247 @@        final Map<String, KafkaFutureImpl<ConsumerGroupDescription>> futures = new HashMap<>(groupIds.size());\n        for (String groupId: groupIds) {\n            if (groupIdIsUnrepresentable(groupId)) {\n                KafkaFutureImpl<ConsumerGroupDescription> future = new KafkaFutureImpl<>();\n                future.completeExceptionally(new InvalidGroupIdException(\"The given group id '\" +"
  },
  {
    "id" : "3d82f07e-fbfe-470a-a26b-a6ea296b6dc2",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112182310",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Leaving this comment here for lack of a better location. I would have expected that we would have used a `NodeProvider` to handle lookup of the coordinator. Any reason not to?",
        "createdAt" : "2018-04-12T23:47:43Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "1760f7e7-1e23-4d37-b54a-20dd7e16c116",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Not sure what do you mean? We are using `LeastLoadedNodeProvider` for finding the coordinator, and then with the found coordinator we use `ConstantNodeIdProvider(nodeId)` for the follow-up request.",
        "createdAt" : "2018-04-13T05:02:32Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "3aade263-0418-4bf3-b084-deb4bbbc0daa",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "What I meant is that we could have a `CoordinatorProvider`, which does the work of finding the coordinator. We need not do this here, I just thought it seemed like the natural way given the AdminClient abstractions.",
        "createdAt" : "2018-04-13T15:23:25Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "294b522e-eabe-4786-a41b-0dc51b111a2c",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I see what you mean now. I think that is doable when the groupId is given, but 1) with this provider interface it is almost not possible to batch multiple groupIds per coordinator, 2) for list consumers, group ids are not known beforehand and hence we cannot use this provider as well.\r\n\r\nI'll create a JIRA for this for follow-up work, just sharing my thoughts about that here.",
        "createdAt" : "2018-04-13T15:59:13Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8ac55751-b665-4b1c-8bfd-66d85ef4dd0e",
        "parentId" : "561c1436-de93-4ccb-806d-6d891d978f7a",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Thanks @hachikuji -- this is a very perceptive comment.  @guozhangwang is right that the NodeProvider is a little too limited to do this correctly at the moment.  There has to be a little more refactoring to make it work.  I have some patches that should help to fix this, but I want to get in the incremental changes first if possible",
        "createdAt" : "2018-04-13T23:16:48Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +2334,2338 @@                @Override\n                void handleFailure(Throwable throwable) {\n                    KafkaFutureImpl<ConsumerGroupDescription> future = futures.get(groupId);\n                    future.completeExceptionally(throwable);\n                }"
  },
  {
    "id" : "71ad752f-8d72-41af-8efd-3b2660571921",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-111826461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff67c597-0942-427b-92bb-820f65fb353b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the error is not `NONE`, do we log it somewhere?",
        "createdAt" : "2018-04-13T00:03:22Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 340,
    "diffHunk" : "@@ -1,1 +2506,2510 @@                                final Errors error = entry.getValue().error;\n\n                                if (error == Errors.NONE) {\n                                    final Long offset = entry.getValue().offset;\n                                    final String metadata = entry.getValue().metadata;"
  },
  {
    "id" : "cde6d58e-13fc-4ab6-a13a-e3b15f2e231a",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112071677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0caa861-1542-476f-9b77-400298a36547",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a little annoying that the API doesn't give us a way to surface these errors somehow. The only way a user can know that the returned offsets are complete is by inspecting the logs. Since the only partition-level error code at the moment seems to be `UNKNOWN_TOPIC_OR_PARTITION`, it may be fine for now, but I'm wondering if we should add something to the API now in case we have more partition-level errors in the future.",
        "createdAt" : "2018-04-13T15:47:04Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "10217b2c-c6ff-4bf7-acac-025328611da6",
        "parentId" : "c0caa861-1542-476f-9b77-400298a36547",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I could change the `OffsetAndMetadata` to `PartitionData` inside `ListOffsetResponse` directly though I'm not a fan of it since we are not technically exposing `ListOffsetResponse` to end users. LMK.",
        "createdAt" : "2018-04-13T16:11:46Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 345,
    "diffHunk" : "@@ -1,1 +2511,2515 @@                                    groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, metadata));\n                                } else {\n                                    log.warn(\"Skipping return offset for {} due to error {}.\", topicPartition, error);\n                                }\n                            }"
  },
  {
    "id" : "b69fd067-ad56-4dd7-ba3c-ada809165835",
    "prId" : 4856,
    "prUrl" : "https://github.com/apache/kafka/pull/4856#pullrequestreview-112244038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1eebf41-47ff-44bf-8e75-20a4b2165ed9",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I don't think we need a new API `copyWith`. We can just use the standard `thenApply` API, right?",
        "createdAt" : "2018-04-13T23:23:38Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "0ace62ce-b1e0-456a-b1f0-d0d4cd93332a",
        "parentId" : "a1eebf41-47ff-44bf-8e75-20a4b2165ed9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Unfortunately we cannot, because we need to construct a final future variable at the beginning that can be returned at the end of the call, and this future can only be \"initialized\" with the underlying map after the first round trip. `thenApply` will return a new future, which cannot be used here.",
        "createdAt" : "2018-04-15T16:44:45Z",
        "updatedAt" : "2018-04-15T16:54:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50a73806f6ac000c598fb1b8958d67c2be39bd16",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +2390,2394 @@                // we have to flatten the future here instead in the result, because we need to wait until the map of nodes\n                // are known from the listNode request.\n                flattenFuture.copyWith(\n                        KafkaFuture.allOf(futuresMap.values().toArray(new KafkaFuture[0])),\n                        new KafkaFuture.BaseFunction<Void, Collection<ConsumerGroupListing>>() {"
  },
  {
    "id" : "a3a6bda7-ffc6-4684-8626-aafabc11ed2f",
    "prId" : 4884,
    "prUrl" : "https://github.com/apache/kafka/pull/4884#pullrequestreview-113029805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9119a9e8-20d6-4fa7-b35e-80e288cda6d4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "By the way, one of the downsides to using the __consumer_offsets topic, is that it effectively makes the `listConsumerGroups` API dependent on having Describe access to this topic.",
        "createdAt" : "2018-04-17T22:56:54Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2125b1902a6bd257a5c73da28b02337aebb3a584",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +2408,2412 @@                        metadataExceptions.add(metadata.error().exception(\"Unable to locate \" +\n                            Topic.GROUP_METADATA_TOPIC_NAME));\n                    } else if (!metadata.topic().equals(Topic.GROUP_METADATA_TOPIC_NAME)) {\n                        metadataExceptions.add(new UnknownServerException(\"Server returned unrequested \" +\n                            \"information about unexpected topic \" + metadata.topic()));"
  },
  {
    "id" : "73a4285b-6e4d-422f-8d50-51ca150bec8e",
    "prId" : 4884,
    "prUrl" : "https://github.com/apache/kafka/pull/4884#pullrequestreview-115400059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's not too clear to me why the synchronization is needed. Are either of `handleResponse` or `handleFailure` called from anywhere except the background thread?",
        "createdAt" : "2018-04-25T23:48:42Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "67f23fd5-4384-43a8-8663-ad7b5d307a0d",
        "parentId" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Good question.  I believe it can... `handleFailure` could be called from `KafkaAdminClient#call`, if the AdminClient has just been shut down.  Theoretically this could happen concurrently with the background thread completing one of the other Call objects that was just created, giving us concurrent modification.\r\n\r\nPart of the reason why there is so little locking elsewhere in AdminClient function bodies is that KafkaFutureImpl instances handle it for us, so this issue doesn't come up.",
        "createdAt" : "2018-04-26T00:17:24Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "927d5c96-dc6b-4a42-bd2f-d61b5740727a",
        "parentId" : "267b706d-07e8-437e-b41e-2ce3e938808e",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Thanks, makes sense. ",
        "createdAt" : "2018-04-26T00:40:22Z",
        "updatedAt" : "2018-04-26T00:47:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2125b1902a6bd257a5c73da28b02337aebb3a584",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +2442,2446 @@                        void handleResponse(AbstractResponse abstractResponse) {\n                            final ListGroupsResponse response = (ListGroupsResponse) abstractResponse;\n                            synchronized (results) {\n                                if (response.error() != Errors.NONE) {\n                                    results.addError(response.error().exception(), node);"
  }
]