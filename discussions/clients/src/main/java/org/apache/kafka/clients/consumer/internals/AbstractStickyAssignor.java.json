[
  {
    "id" : "7de4672b-06c8-4a6d-819c-ea2404551531",
    "prId" : 7130,
    "prUrl" : "https://github.com/apache/kafka/pull/7130#pullrequestreview-274578503",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c651222b-da4b-47ca-99db-bcf1d36c42a7",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "@guozhangwang Just to confirm: this case should not happen for cooperative mode given the new `onPartitionsLost` protocol, right? ie this corresponds to a metadata change where the partition no longer exists, and after Pt. 3 it will be removed from `ownedPartitions`?",
        "createdAt" : "2019-08-07T19:41:33Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "db3d2f8f-8493-4a4b-8ac7-6cebc672b908",
        "parentId" : "c651222b-da4b-47ca-99db-bcf1d36c42a7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Actually I'm thinking about not triggering `onPartitionsLost` on metadata change aggressively since we do not have a good way to track metadata freshness, and upon stale metadata we could incorrectly give away partitions as lost..",
        "createdAt" : "2019-08-08T05:46:56Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "1ae85cdd-f9a2-4e56-96e3-4e22202bd176",
        "parentId" : "c651222b-da4b-47ca-99db-bcf1d36c42a7",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Yeah, as discussed we may trigger unnecessary follow-up rebalances on metadata change because of this. What if `ConsumerCoordinator` checks metadata and only sets `needsRejoin` if it is revoking partitions that actually exist and must be getting reassigned? ",
        "createdAt" : "2019-08-08T21:49:42Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "77309ede-9183-4dd3-9c17-98eb502810e7",
        "parentId" : "c651222b-da4b-47ca-99db-bcf1d36c42a7",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "We don't want to actually call `onPartitionsLost` instead of `onPartitionsRevoked` but we should still distinguish the two cases w.r.t triggering another rebalance.",
        "createdAt" : "2019-08-08T21:50:39Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "dc10a911-7c0c-440c-8fa9-ae4630794034",
        "parentId" : "c651222b-da4b-47ca-99db-bcf1d36c42a7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`rejoinNeededOrPending` logic is a bit different for leader and other members: for normal members we would only return true if it has been requested, or if our subscription has changed (either new topics added, or existing topics removed). In this case we would not revoke anything.\r\n\r\nFor leader though, it can additionally returns true if the metadata has been changed since its last assign procedure. And also we are relying on leader's metadata as the source-of-truth (which may actually not always be up-to-date, but we do this to avoid split-brain) to determine which partitions should be revoked. And then normal members upon receiving the assignment would call `revoke`.\r\n\r\nWhat I originally added the `onPartitionsLost` is only to the leader's additional check on metadata, but as we discussed, calling it aggressively is not safe given that our metadata is not guaranteed to be out-of-date.\r\n\r\nBut I think even with this, we COULD still avoid the extra rebalance, if we add back the error code from leader. I.e. if leader realized that some assignment would be taken away but due to the fact that they no longer exist, it can set the error code still as `none` and the members would then not trigger another rebalance even after they have revoked non-empty partition set. This is more aligned with our current semantics I think: since we are relying on the leader as the sole source-of-truth, we should probably let the leader to indicate whether new rebalance should be needed rather than letting each member to decide? cc @hachikuji ",
        "createdAt" : "2019-08-08T22:13:51Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "32e07a83-063f-43ee-9c08-52709558ce20",
        "parentId" : "c651222b-da4b-47ca-99db-bcf1d36c42a7",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Let's just create a ticket for this for now, and we can revisit the best way to avoid unnecessary rebalances later?",
        "createdAt" : "2019-08-13T21:56:30Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "2353e5a29fdd36b86160a8e63de90cd54f0d0825",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +124,128 @@                    TopicPartition partition = partitionIter.next();\n                    if (!partition2AllPotentialConsumers.containsKey(partition)) {\n                        // if this topic partition of this consumer no longer exists remove it from currentAssignment of the consumer\n                        partitionIter.remove();\n                        currentPartitionConsumer.remove(partition);"
  },
  {
    "id" : "e631a386-77e4-4ab4-8510-86455f930789",
    "prId" : 7130,
    "prUrl" : "https://github.com/apache/kafka/pull/7130#pullrequestreview-272873936",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e509a76-7654-4d6c-9fe6-eb42686e6ad8",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "This is the one change to the sticky assignment algorithm -- in some rare cases the assignment logic will sacrifice stickiness for balance unnecessarily, triggering a third rebalance under cooperative mode (see [KAFKA-8767](https://issues.apache.org/jira/browse/KAFKA-8767) for more context).\r\nThis doesn't fix this entirely, but did reduce the occurrences by a factor of more than 2. I feel this is fine for now as the situation should be rare (mostly random subscription changes) and not too costly, and we can further optimize later.",
        "createdAt" : "2019-08-07T21:54:34Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "49bf81ac-067b-4ed2-b6ff-f585a78ecfa4",
        "parentId" : "7e509a76-7654-4d6c-9fe6-eb42686e6ad8",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Changing subscriptions in a random fashion should be rare, but I'm more curious of why the algorithm would fail to find the optimal solution, and if it is could be triggered by other even more common situations?\r\n\r\nBy reading the description of the sticky assignor from KIP-441, I think the root cause may be in the iterative balancing phase:\r\n\r\n```\r\nWhen considering the best move for a partition, it first checks to see if that partition is currently hosted on a consumer that is unbalanced with respect to the prior host of that partition. In this case, it just moves the partition back to the prior host. This is essentially a short-circuit for the case where a partition has become \"unstuck\" and restoring stickiness could actually improve balance. If we get past that short-circuit, then we just propose to move the partition to the consumer that can host it and has the smallest current assignment.\r\n```\r\n\r\nSuch random walks algorithm can only achieve sub-optimal solution indeed; maybe we can change this behavior to consider \"switching two partitions so that the total balance of the cluster does not change, but we can achieve better stickiness\" as a valid move before considering for convergence? Just a thought.",
        "createdAt" : "2019-08-08T05:57:02Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f59b5801-4e1c-49a5-a1b8-6dccf2452bc2",
        "parentId" : "7e509a76-7654-4d6c-9fe6-eb42686e6ad8",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Yeah, I didn't see substantial improvement trying that but perhaps there is a more clever way to do so. I opened a ticket to look into this further but consider it an optimization as future work. Personally, I feel it's an interesting problem -- but doesn't merit blocking progress on 429.\r\n\r\nDoes that sound reasonable? I also think most use cases will not hit this -- and if they do, it's not incorrect just annoying :) ",
        "createdAt" : "2019-08-08T22:11:17Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "a488ee44-65de-4e64-bda7-d07e33b20284",
        "parentId" : "7e509a76-7654-4d6c-9fe6-eb42686e6ad8",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yeah as long as you feel that no other more common situations would hit this, then it should not block KIP-429.",
        "createdAt" : "2019-08-08T22:41:57Z",
        "updatedAt" : "2019-08-22T22:29:11Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2353e5a29fdd36b86160a8e63de90cd54f0d0825",
    "line" : 474,
    "diffHunk" : "@@ -1,1 +472,476 @@\n        // if we don't already need to revoke something due to subscription changes, first try to balance by only moving newly added partitions\n        if (!revocationRequired) {\n            performReassignments(unassignedPartitions, currentAssignment, prevAssignment, sortedCurrentSubscriptions,\n                consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer);"
  },
  {
    "id" : "99af712e-1b97-4cd0-9e43-33c0418cebba",
    "prId" : 8668,
    "prUrl" : "https://github.com/apache/kafka/pull/8668#pullrequestreview-421217820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "607485ad-3d8b-4128-98e4-ffde994fca61",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Is it possible that this unfilled consumer has N+1 remaining capacity, while there's only N max consumer only?",
        "createdAt" : "2020-05-29T19:27:51Z",
        "updatedAt" : "2020-05-30T02:34:19Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "10bc6c60-2c3f-4ca5-9f67-c47637382d5e",
        "parentId" : "607485ad-3d8b-4128-98e4-ffde994fca61",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "NVM, I realized it should never happen.",
        "createdAt" : "2020-05-29T19:31:05Z",
        "updatedAt" : "2020-05-30T02:34:19Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25a323d7e21aa5a554e43d811ac6eb3e3894055",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +225,229 @@            List<TopicPartition> consumerAssignment = assignment.get(consumer);\n            int remainingCapacity = minQuota - consumerAssignment.size();\n            while (remainingCapacity > 0) {\n                String overloadedConsumer = maxCapacityMembers.poll();\n                if (overloadedConsumer == null) {"
  },
  {
    "id" : "17ee795e-6c62-4bdd-b40a-06101bc27998",
    "prId" : 8668,
    "prUrl" : "https://github.com/apache/kafka/pull/8668#pullrequestreview-421370476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1051944-29a5-4434-a3b1-c3b3a7178f91",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "This is just an optimization for the cooperative case: I found that the assignment time for the eager and cooperative assignor began to diverge once you reached partition counts in the millions. At 10 million partitions for example, the eager assignor hovered around 30s but the cooperative assignor was upwards of 5-6 minutes.\r\nThe discrepancy was entirely due to the `adjustAssignment` method needing to compute the set of partitions transferring ownership  in the completed assignment. But we can build up this map during assignment much more efficiently, by taking advantage of the additional context we have at various steps in the algorithm. Tracking and exposing this set to the cooperative assignor cut the assignment time for large partition numbers pretty drastically, putting the cooperative assignor  on par with the eager assignor. ",
        "createdAt" : "2020-05-30T02:29:09Z",
        "updatedAt" : "2020-05-30T02:34:19Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25a323d7e21aa5a554e43d811ac6eb3e3894055",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +48,52 @@    // Keep track of the partitions being migrated from one consumer to another during assignment\n    // so the cooperative assignor can adjust the assignment\n    protected Map<TopicPartition, String> partitionsTransferringOwnership = new HashMap<>();\n\n    static final class ConsumerGenerationPair {"
  },
  {
    "id" : "3adef16f-ea02-4419-9a30-45e86231f2be",
    "prId" : 8668,
    "prUrl" : "https://github.com/apache/kafka/pull/8668#pullrequestreview-421370570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e0b82b1-e058-47ff-be0c-54f1a7e89433",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I didn't bother to include this optimization for the general case. We know that the assignment algorithm itself becomes a bottleneck at only 2,000 partitions, so there's no point optimizing something that only becomes a bottleneck in the millions of partitions",
        "createdAt" : "2020-05-30T02:30:31Z",
        "updatedAt" : "2020-05-30T02:34:19Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25a323d7e21aa5a554e43d811ac6eb3e3894055",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +82,86 @@            log.debug(\"Detected that all not consumers were subscribed to same set of topics, falling back to the \"\n                          + \"general case assignment algorithm\");\n            partitionsTransferringOwnership = null;\n            return generalAssign(partitionsPerTopic, subscriptions);\n        }"
  },
  {
    "id" : "d494ff1f-ce40-4488-b3d8-a2f6b21b78bb",
    "prId" : 8668,
    "prUrl" : "https://github.com/apache/kafka/pull/8668#pullrequestreview-422865488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da8fdca7-d9c8-40b8-89f4-c7de6b8e4f0e",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Just FYI, I introduced this bug right before merging. Luckily the tests caught it -- fix is https://github.com/apache/kafka/pull/8777",
        "createdAt" : "2020-06-02T16:42:49Z",
        "updatedAt" : "2020-06-02T16:42:50Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25a323d7e21aa5a554e43d811ac6eb3e3894055",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +133,137 @@                if (memberData.generation.isPresent() && memberData.generation.get() > maxGeneration) {\n                    membersWithOldGeneration.addAll(membersOfCurrentHighestGeneration);\n                    membersOfCurrentHighestGeneration.clear();\n                    maxGeneration = memberData.generation.get();\n                }"
  },
  {
    "id" : "e69f02f9-d87a-40c6-902d-4bd3e2fc85df",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-632010547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b68049a8-5d18-48ec-87ee-a883e56bc5bb",
        "parentId" : null,
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "we should make the capacity to maxQuota to avoid memory reallocation.",
        "createdAt" : "2021-04-09T03:48:26Z",
        "updatedAt" : "2021-05-05T01:22:25Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +186,190 @@        // initialize the assignment map with an empty array of size maxQuota for all members\n        Map<String, List<TopicPartition>> assignment = new HashMap<>(\n            consumerToOwnedPartitions.keySet().stream().collect(Collectors.toMap(c -> c, c -> new ArrayList<>(maxQuota))));\n\n        List<TopicPartition> assignedPartitions = new ArrayList<>();"
  },
  {
    "id" : "02136b90-47fe-4cae-b937-b3ce6909edc9",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-650069089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9943ab54-ae8a-4109-948a-f6d740a02ecc",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I feel like the compiler _should_ know to garbage collect this since it's unused after this point, but you just never know. Better to be sure",
        "createdAt" : "2021-04-29T21:46:58Z",
        "updatedAt" : "2021-05-05T01:22:25Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "2f5406d3-5e5c-47a6-ad66-aaa8f2b4039c",
        "parentId" : "9943ab54-ae8a-4109-948a-f6d740a02ecc",
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "I agree! Anyway, I'll improve that part in another ticket soon. :)",
        "createdAt" : "2021-05-03T03:53:36Z",
        "updatedAt" : "2021-05-05T01:22:25Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +82,86 @@            partitionsTransferringOwnership = null;\n            // we don't need consumerToOwnedPartitions in general assign case\n            consumerToOwnedPartitions = null;\n            return generalAssign(partitionsPerTopic, subscriptions);\n        }"
  },
  {
    "id" : "ee8dfd80-d6a2-4ad4-bbd6-e88f5d162dbf",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-652883380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b44aa395-bdef-4d38-a5ee-96be3b69329a",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Just a nit -- and to clarify up front, if you agree with this let's still hold off on doing it here so this PR can finally be merged, as I figure any nits can be addressed in your general assign PR:\r\n\r\nIt's still a bit unclear what this value will be sued for when you first see it, maybe we can work in the word `minQuota` somewhere in the name? Eg `expectedNumMembersWithMoreThanMinQuotaPartitions`, or for a slightly shorter example `numConsumersAssignedOverMinQuota`, or something between or similar to those\r\n\r\nFYI I'm also ok with it as-is if you prefer the current name -- just wanted to throw out some other suggestions. I'll trust you to pick whatever name feels right 🙂 ",
        "createdAt" : "2021-05-06T01:07:05Z",
        "updatedAt" : "2021-05-06T01:07:06Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +180,184 @@        int maxQuota = (int) Math.ceil(((double) totalPartitionsCount) / numberOfConsumers);\n        // the expected number of members with maxQuota assignment\n        int expectedNumMembersHavingMorePartitions = totalPartitionsCount % numberOfConsumers;\n        // the number of members with exactly maxQuota partitions assigned\n        int numMembersHavingMorePartitions = 0;"
  },
  {
    "id" : "62ec44ca-bcee-4455-9fe2-db241301755f",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-652888276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22ac1b42-5633-44b8-bd03-37229c7df765",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "nit (again, please address this in the other PR so I can merge this one): as Guozhang pointed out in another comment, in the case minQuota == maxQuota, this comment is a bit misleading as the number of expected max capacity members is technically all of them, but the variable `expectedNumMembersHavingMorePartitions` refers to the number of members who have more than the minQuota number of partitions, which in that case would actually be zero.",
        "createdAt" : "2021-05-06T01:15:35Z",
        "updatedAt" : "2021-05-06T01:15:35Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "46d24371-fa1c-43a1-896d-a61523b9c187",
        "parentId" : "22ac1b42-5633-44b8-bd03-37229c7df765",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "maybe something like\r\n```suggestion\r\n                // since we're still under the number of expected members with more than the minQuota partitions, this consumer may be assigned one more partition \r\n```",
        "createdAt" : "2021-05-06T01:18:17Z",
        "updatedAt" : "2021-05-06T01:22:54Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "b47d3e6a-a481-4d8f-9336-f416cbfd5927",
        "parentId" : "22ac1b42-5633-44b8-bd03-37229c7df765",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Just a thought: technically it's not even a \"potential maxQuota\" member, since as you pointed out in another comment \"the unassignedPartitions size will always >= unfilledMembers size\" -- therefore anything in `unfilledMembers` will in fact need to receive at least one partition. Does that sound right to you? (this is just a followup question to make sure we're on the same page, no need to do anything for this one)",
        "createdAt" : "2021-05-06T01:21:56Z",
        "updatedAt" : "2021-05-06T01:21:57Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +219,223 @@                assignedPartitions.addAll(minQuotaPartitions);\n                allRevokedPartitions.addAll(ownedPartitions.subList(minQuota, ownedPartitions.size()));\n                // this consumer is potential maxQuota candidate since we're still under the number of expected max capacity members\n                if (numMembersHavingMorePartitions < expectedNumMembersHavingMorePartitions) {\n                    unfilledMembers.add(consumer);"
  },
  {
    "id" : "ff416582-b12a-44f7-9319-66aa8ec371ed",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-652889085",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9476d87c-f3d6-4b70-9df7-9505d973d68b",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Thanks for cleaning this part of the logic up, it's much clearer now (not to mention the nice savings memory-wise)",
        "createdAt" : "2021-05-06T01:24:22Z",
        "updatedAt" : "2021-05-06T01:24:22Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +226,230 @@        }\n\n        List<TopicPartition> unassignedPartitions = getUnassignedPartitions(totalPartitionsCount, partitionsPerTopic, assignedPartitions);\n        assignedPartitions = null;\n"
  },
  {
    "id" : "6e71e15e-b1b6-462a-8f4a-1a6e30ac1478",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-652892130",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a21b969d-0b57-4cda-aeb6-9e9245cdeca9",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "nit (for next PR): can you log an error before throwing the exception and include the set of unassigned partitions? Either just print out the `unassignedPartitions` along with the current partition being processed so you can figure out which partitions are remaining after that, or else by actually computing the remaining partitions that have yet to be assigned. Since it's an error case, I think it's ok to spend a little extra time computing that for better debuggability ",
        "createdAt" : "2021-05-06T01:33:24Z",
        "updatedAt" : "2021-05-06T01:33:24Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 172,
    "diffHunk" : "@@ -1,1 +242,246 @@                if (unfilledMembers.isEmpty()) {\n                    // Should not enter here since we have calculated the exact number to assign to each consumer\n                    // There might be issues in the assigning algorithm, or maybe assigning the same partition to two owners.\n                    throw new IllegalStateException(\"No more unfilled consumers to be assigned.\");\n                }"
  },
  {
    "id" : "60257b0e-a18a-4ef2-bf3a-4407e61ed04a",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-652893035",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c4b76c9-9173-403c-a41f-acccffddcabb",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "nit: same here, can you log an error with the remaining `unfilledMembers`? I know you already do that in the exception message, but imo it would be better to print in a log message instead of an exception, as it may be long",
        "createdAt" : "2021-05-06T01:36:16Z",
        "updatedAt" : "2021-05-06T01:36:17Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 222,
    "diffHunk" : "@@ -1,1 +269,273 @@            // of max capacity members. Otherwise, there must be error here.\n            if (numMembersHavingMorePartitions != expectedNumMembersHavingMorePartitions) {\n                throw new IllegalStateException(String.format(\"We haven't reached the allowed number of max capacity members, \" +\n                    \"but no more partitions to be assigned to unfilled consumers: %s\", unfilledMembers));\n            } else {"
  },
  {
    "id" : "4f2ee522-ae79-4964-9bfd-4f7c7dfe78e5",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-652893759",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33d4537c-a944-4e4f-90d9-e379f1097833",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "nit: the exception here looks good, but once again let's also log an error (it just makes it easier to debug when you have something concrete in the place you encountered the error, whereas exceptions are not always printed right away). Should probably just log any info that could be useful, such as all remaining `unfilledMembers`",
        "createdAt" : "2021-05-06T01:38:40Z",
        "updatedAt" : "2021-05-06T01:38:41Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +275,279 @@                    int assignedPartitionsCount = assignment.get(unfilledMember).size();\n                    if (assignedPartitionsCount != minQuota) {\n                        throw new IllegalStateException(String.format(\"Consumer: [%s] should have %d partitions, but got %d partitions, \" +\n                            \"and no more partitions to be assigned\", unfilledMember, minQuota, assignedPartitionsCount));\n                    }"
  },
  {
    "id" : "d0e9fee2-d50a-481c-b8b2-ca55f403e651",
    "prId" : 10509,
    "prUrl" : "https://github.com/apache/kafka/pull/10509#pullrequestreview-652894404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46ba58d6-8f97-4ea1-a308-d7676ba320dc",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "nit: can we add an `else` case that just logs that we skipped over this member because we reached max capacity and it was still at min? Not sure if debug or trace is more appropriate, might be worth just running the tests with this log in place to see how often it gets printed",
        "createdAt" : "2021-05-06T01:40:28Z",
        "updatedAt" : "2021-05-06T01:40:29Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2ac7d537e968079b360ede161ddc6cb1a566750",
    "line" : 227,
    "diffHunk" : "@@ -1,1 +274,278 @@                for (String unfilledMember : unfilledMembers) {\n                    int assignedPartitionsCount = assignment.get(unfilledMember).size();\n                    if (assignedPartitionsCount != minQuota) {\n                        throw new IllegalStateException(String.format(\"Consumer: [%s] should have %d partitions, but got %d partitions, \" +\n                            \"and no more partitions to be assigned\", unfilledMember, minQuota, assignedPartitionsCount));"
  },
  {
    "id" : "f9b2f09e-ada9-4575-a159-ab768b6a6966",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-650775127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c28ea02-3251-44a8-ae28-e275735e74cd",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: the following comment needs to be updated as well.",
        "createdAt" : "2021-05-03T22:47:58Z",
        "updatedAt" : "2021-05-11T09:05:03Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 211,
    "diffHunk" : "@@ -1,1 +393,397 @@                for (Iterator<TopicPartition> partitionIter = entry.getValue().iterator(); partitionIter.hasNext();) {\n                    TopicPartition partition = partitionIter.next();\n                    if (!topic2AllPotentialConsumers.containsKey(partition.topic())) {\n                        // if this topic partition of this consumer no longer exists, remove it from currentAssignment of the consumer\n                        partitionIter.remove();"
  },
  {
    "id" : "c4087c6d-8199-47cb-b5a1-d8289d86d5d2",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-656465325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f40e08a6-08d8-4511-becd-07bf3f7cfaec",
        "parentId" : null,
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "put the `maxGeneration` into class scope, so we can re-use it in `prepopulateCurrentAssignments`.",
        "createdAt" : "2021-05-11T09:06:35Z",
        "updatedAt" : "2021-05-11T09:06:35Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +40,44 @@\n    public static final int DEFAULT_GENERATION = -1;\n    public int maxGeneration = DEFAULT_GENERATION;\n\n    private PartitionMovements partitionMovements;"
  },
  {
    "id" : "96ce17f9-5ac0-4c36-aa3e-befaf964930e",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-656468077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4be92967-4b71-49b7-b05f-a72433044b12",
        "parentId" : null,
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "Now, we'll run through all the `subscriptions` since the data `consumerToOwnedPartitions` will also passed into `generalAssign`",
        "createdAt" : "2021-05-11T09:09:21Z",
        "updatedAt" : "2021-05-11T09:09:22Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +107,111 @@            } else if (isAllSubscriptionsEqual && !(subscription.topics().size() == subscribedTopics.size()\n                && subscribedTopics.containsAll(subscription.topics()))) {\n                isAllSubscriptionsEqual = false;\n            }\n"
  },
  {
    "id" : "585a64ec-20b0-42b9-b8de-e1a5633b056e",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-656468977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78a5c284-a141-4c96-aa6c-a3df5208a2fe",
        "parentId" : null,
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "reuse the `getAllTopicPartitions` in `constrainedAssign`",
        "createdAt" : "2021-05-11T09:10:18Z",
        "updatedAt" : "2021-05-11T09:10:19Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +376,380 @@        List<String> sortedAllTopics = new ArrayList<>(topic2AllPotentialConsumers.keySet());\n        Collections.sort(sortedAllTopics, new TopicComparator(topic2AllPotentialConsumers));\n        List<TopicPartition> sortedAllPartitions = getAllTopicPartitions(partitionsPerTopic, sortedAllTopics, totalPartitionsCount);\n\n        // the partitions already assigned in current assignment"
  },
  {
    "id" : "da4660a7-2381-4f49-9cd8-0ef11d2fb529",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-656470392",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a70d3d61-9669-4187-9b1c-5c0b9f1fa082",
        "parentId" : null,
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "put the 2 `getUnassignedPartitions` (this one and the following one) overloading method together for readability",
        "createdAt" : "2021-05-11T09:11:44Z",
        "updatedAt" : "2021-05-11T09:11:58Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +452,456 @@     * @return                              partitions that aren't assigned to any current consumer\n     */\n    private List<TopicPartition> getUnassignedPartitions(List<TopicPartition> sortedAllPartitions,\n                                                         List<TopicPartition> sortedAssignedPartitions,\n                                                         Map<String, List<String>> topic2AllPotentialConsumers) {"
  },
  {
    "id" : "8820253d-a43e-4e33-858d-7f437c69a074",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-667643913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4a7745b-0eeb-4745-8561-bb2d4de70ec9",
        "parentId" : null,
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "Is this `null` assignment needed? Don't see the variable used after this.",
        "createdAt" : "2021-05-23T04:49:30Z",
        "updatedAt" : "2021-05-24T04:21:35Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      },
      {
        "id" : "17ba4d37-8bc0-4f90-8225-075318637628",
        "parentId" : "a4a7745b-0eeb-4745-8561-bb2d4de70ec9",
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "Yes, it just tells the GC that this memory can be freed, to avoid OOM. I know in this step, we should already allocated all memories we need, but it's just in case. What do you think?",
        "createdAt" : "2021-05-24T13:16:43Z",
        "updatedAt" : "2021-05-24T13:16:43Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      },
      {
        "id" : "11f99bb5-fe2d-4d5b-a4d1-d9727b461436",
        "parentId" : "a4a7745b-0eeb-4745-8561-bb2d4de70ec9",
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "So this is assuming the following `balance()` call could run beyond the next GC?\r\nIn that case imho `assignedPartitions.clear()` would look better (having almost the same impact).\r\n",
        "createdAt" : "2021-05-25T03:33:25Z",
        "updatedAt" : "2021-05-25T04:13:01Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      },
      {
        "id" : "8c0c22d8-bf04-46d3-a904-3df66bbec0ec",
        "parentId" : "a4a7745b-0eeb-4745-8561-bb2d4de70ec9",
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "Yes, `assignedPartitions.clear()` would have the same impact, but it'll loop through all the arrayList and nullify them one by one. I think we can either `null` it, or remove this line. What do you think?\r\n\r\n```java\r\n/**\r\n     * Removes all of the elements from this list.  The list will\r\n     * be empty after this call returns.\r\n     */\r\n    public void clear() {\r\n        modCount++;\r\n        final Object[] es = elementData;\r\n        for (int to = size, i = size = 0; i < to; i++)\r\n            es[i] = null;\r\n    }```\r\n```",
        "createdAt" : "2021-05-25T10:13:02Z",
        "updatedAt" : "2021-05-25T10:13:02Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 236,
    "diffHunk" : "@@ -1,1 +413,417 @@        // all partitions that needed to be assigned\n        List<TopicPartition> unassignedPartitions = getUnassignedPartitions(sortedAllPartitions, assignedPartitions, topic2AllPotentialConsumers);\n        assignedPartitions = null;\n\n        if (log.isDebugEnabled()) {"
  },
  {
    "id" : "4d41d8c7-5cfd-4a9f-9ca2-dfe02ba07254",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-667360154",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb056c6d-8fe7-474a-b768-6e6876471e23",
        "parentId" : null,
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "Isn't `topic` already added in line 357 above?",
        "createdAt" : "2021-05-24T03:46:30Z",
        "updatedAt" : "2021-05-24T04:21:35Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      },
      {
        "id" : "ab6314ec-2f76-46f6-bab6-5ee3e6d67f14",
        "parentId" : "cb056c6d-8fe7-474a-b768-6e6876471e23",
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "No, it just create a List with the capacity: `topics().size()`. We cannot just create a List with all topics directly, because we need to filter out topics not in `partitionsPerTopic`.",
        "createdAt" : "2021-05-24T13:21:36Z",
        "updatedAt" : "2021-05-24T13:21:36Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      },
      {
        "id" : "dc177450-2674-4ba0-8026-8995ca167c32",
        "parentId" : "cb056c6d-8fe7-474a-b768-6e6876471e23",
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "Right, sorry I misread that line.",
        "createdAt" : "2021-05-25T03:43:03Z",
        "updatedAt" : "2021-05-25T04:13:01Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +358,362 @@            consumer2AllPotentialTopics.put(consumerId, subscribedTopics);\n            entry.getValue().topics().stream().filter(topic -> partitionsPerTopic.get(topic) != null).forEach(topic -> {\n                subscribedTopics.add(topic);\n                topic2AllPotentialConsumers.get(topic).add(consumerId);\n            });"
  },
  {
    "id" : "c33a8123-8dd4-4367-a0ef-72c3ab202cc9",
    "prId" : 10552,
    "prUrl" : "https://github.com/apache/kafka/pull/10552#pullrequestreview-667648588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "893be795-73e4-4025-be41-b62ed2012173",
        "parentId" : null,
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "This block didn't exist before, why is it needed now?",
        "createdAt" : "2021-05-25T03:57:54Z",
        "updatedAt" : "2021-05-25T04:13:01Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      },
      {
        "id" : "4f694160-c930-4f8d-8e42-e059e3671f78",
        "parentId" : "893be795-73e4-4025-be41-b62ed2012173",
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "This is actually a bug after `constrainedAssign` implemented. After `constrainedAssign` implemented, we'll do `allSubscriptionsEqual` to decide if we want to use `constrainedAssign` or `generalAssign`. In `allSubscriptionsEqual`, we not only check if subscription equal, but also deserialize the user data. So, if it is deserialized once, the position of userData (ByteBuffer) will be moved to the end of the buffer, so that we have to rewind here.",
        "createdAt" : "2021-05-25T10:18:11Z",
        "updatedAt" : "2021-05-25T10:18:11Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0023ec1227d279ff35a09d91b4e4ffdd4af194",
    "line" : 426,
    "diffHunk" : "@@ -1,1 +582,586 @@                // since this is our 2nd time to deserialize memberData, rewind userData is necessary\n                subscription.userData().rewind();\n            }\n            MemberData memberData = memberData(subscriptionEntry.getValue());\n"
  }
]