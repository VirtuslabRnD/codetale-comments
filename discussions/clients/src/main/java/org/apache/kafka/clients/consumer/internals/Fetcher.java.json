[
  {
    "id" : "2ffead69-4cec-4d60-9b76-8b272c629c7e",
    "prId" : 4557,
    "prUrl" : "https://github.com/apache/kafka/pull/4557#pullrequestreview-95859553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bae74676-eb5b-4c79-b180-e234dfc20389",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "Is it possible that offsetResetTimestamps is empty ?",
        "createdAt" : "2018-02-12T03:24:42Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "608ab8d6-48c8-4faa-aeae-d7cb002a7099",
        "parentId" : "bae74676-eb5b-4c79-b180-e234dfc20389",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It is possible, but seems fine? We could even get rid of the empty check a couple lines above.",
        "createdAt" : "2018-02-12T16:46:00Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "76c796ca128c3c97231f3ebda994a07bb06b26aa",
    "line" : 217,
    "diffHunk" : "@@ -1,1 +375,379 @@        }\n\n        resetOffsetsAsync(offsetResetTimestamps);\n    }\n"
  },
  {
    "id" : "7dee546e-3db7-4fbc-a004-ffc212cd2cc0",
    "prId" : 4557,
    "prUrl" : "https://github.com/apache/kafka/pull/4557#pullrequestreview-96404008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd70a793-b531-4e5e-9f47-98478d13a824",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "What do we do if one of the exceptions is not retriable (e.g UNSUPPORTED_FOR_MESSAGE_FORMAT)? It seems to be handled differently from `TOPIC_AUTHORIZATION_FAILED`, but I wasn't sure why.",
        "createdAt" : "2018-02-13T23:06:03Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "c429894e-0e14-4692-af64-9a599d0d0b9a",
        "parentId" : "dd70a793-b531-4e5e-9f47-98478d13a824",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We treat `UNSUPPORTED_FOR_MESSAGE_FORMAT` as equivalent to not finding an offset for the timestamp being searched. It makes sense because timestamps didn't exist in the old format. I can clarify this in the comment.",
        "createdAt" : "2018-02-14T07:31:28Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "76c796ca128c3c97231f3ebda994a07bb06b26aa",
    "line" : 614,
    "diffHunk" : "@@ -1,1 +788,792 @@            future.raise(new TopicAuthorizationException(unauthorizedTopics));\n        else\n            future.complete(new ListOffsetResult(fetchedOffsets, partitionsToRetry));\n    }\n"
  },
  {
    "id" : "6f0dcd8e-5a1e-43e6-ade3-998b8def62e0",
    "prId" : 5495,
    "prUrl" : "https://github.com/apache/kafka/pull/5495#pullrequestreview-146008624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14d0e4a8-bf79-4bc5-9e88-b53543bf50f5",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Wouldn't it be enough to lock on the given instance of the FetchSessionHandler given that's the object that runs into a concurrent modification? This way as I understand we completely evict concurrency from the sendFetches method for a given instance.\r\nAlso I would generally prefer using a lock object instead of locking on `this`. The reason is that this way the synchronized is externalized to the public API (well in this case it is arguable since it's an _internals_ class), + it enables accidental lock stealing, ie. a different class locking on Fetcher.this. I don't know if this is a concern now though.",
        "createdAt" : "2018-08-13T13:41:41Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "17a8ce92-820e-4599-9f52-9222d1eb2e54",
        "parentId" : "14d0e4a8-bf79-4bc5-9e88-b53543bf50f5",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@viktorsomogyi Thanks for the review. There is also a `sessionHandlers` HashMap. That would need to become a `ConcurrentHashMap`. I wasn't sure if there was any other state. We do broad locking of the coordinator for thread-safety, I thought the same for `Fetcher` would be the simplest and safest fix. Since this code is generally single-threaded and locking is only to avoid concurrent access in the heartbeat thread, I am not sure it matters so much. Will wait for @hachikuji 's review and then update if required.",
        "createdAt" : "2018-08-14T10:10:02Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "47080302-621e-4678-b1f7-d33361c71070",
        "parentId" : "14d0e4a8-bf79-4bc5-9e88-b53543bf50f5",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "I see, then it's probably ok. I was missing the detail about the `sessionHandlers` map, but thanks for the heads-up :).",
        "createdAt" : "2018-08-14T10:45:29Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "b379bf19b941c9e97d243f8fc2175147a9fab6e8",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +206,210 @@     * @return number of fetches sent\n     */\n    public synchronized int sendFetches() {\n        Map<Node, FetchSessionHandler.FetchRequestData> fetchRequestMap = prepareFetchRequests();\n        for (Map.Entry<Node, FetchSessionHandler.FetchRequestData> entry : fetchRequestMap.entrySet()) {"
  },
  {
    "id" : "5279ad4b-5c8c-4e68-ad6c-3f03ebd09e67",
    "prId" : 5627,
    "prUrl" : "https://github.com/apache/kafka/pull/5627#pullrequestreview-153980869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fae6ccd-875e-450c-94b2-53b3ca2cad30",
        "parentId" : null,
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "I moved the logic for getting the list of partitions that do not have available leader to `groupListOffsetRequests` . I was contemplating whether we should add partition for which we have a failed connection to leader to `partitionsToRetry`. Seems like we should, because we may be able to reconnect before the list offsets timeout. ",
        "createdAt" : "2018-09-10T22:02:42Z",
        "updatedAt" : "2018-09-10T23:39:54Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f8fbd8a0a66124cffd3a95a15fe8464dd3ae92b",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +695,699 @@                log.debug(\"Leader {} for partition {} is unavailable for fetching offset until reconnect backoff expires\",\n                          info.leader(), tp);\n                partitionsToRetry.add(tp);\n            } else {\n                Node node = info.leader();"
  },
  {
    "id" : "9ea10ce5-eaa6-45ea-b071-fa3479cf42a5",
    "prId" : 5991,
    "prUrl" : "https://github.com/apache/kafka/pull/5991#pullrequestreview-182458401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b546855b-aecb-47bc-b2fd-f7d970d6040a",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Might be nice to add a couple test cases to make sure the expected behavior won't regress.",
        "createdAt" : "2018-12-06T22:00:00Z",
        "updatedAt" : "2018-12-14T16:34:23Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "54d8dd54100b18957db07485d9ee0c37ad4476c8",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +813,817 @@                       error == Errors.REPLICA_NOT_AVAILABLE ||\n                       error == Errors.KAFKA_STORAGE_ERROR ||\n                       error == Errors.OFFSET_NOT_AVAILABLE ||\n                       error == Errors.LEADER_NOT_AVAILABLE) {\n                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\","
  },
  {
    "id" : "d3862562-44a7-4276-b1d6-a5fea7becd38",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-214244487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I am sure I am missing something but how can this be true when due to line 560 `if (partitionRecords.nextFetchOffset == position.offset) {`?",
        "createdAt" : "2019-03-12T22:19:20Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "9a3f0246-7007-467d-9d48-a7d53336759b",
        "parentId" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "On a side note, unless the local variable and the field are marked as `final`, it is basically impossible for javac to catch this statically because Java's concurrency model. E.g. any thread can modify both `partitionRecords.nextFetchOffset` and `position.offset`",
        "createdAt" : "2019-03-12T22:23:49Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "99e150ef-f7fa-401e-81d1-379446c6cb04",
        "parentId" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Good catch. Let me look into this... might be something I missed during the merge from trunk. ",
        "createdAt" : "2019-03-13T13:28:04Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "2affcb55-ce9e-463e-ac5b-2d86cd5dac33",
        "parentId" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I think the code is correct as-is. The call to `partitionRecords.fetchRecords(maxRecords)` updates the nextFetchOffset to the offset of the last record returned plus one.",
        "createdAt" : "2019-03-13T22:31:55Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +604,608 @@                List<ConsumerRecord<K, V>> partRecords = partitionRecords.fetchRecords(maxRecords);\n\n                if (partitionRecords.nextFetchOffset > position.offset) {\n                    SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition(\n                            partitionRecords.nextFetchOffset,"
  },
  {
    "id" : "ed8635b8-c3bc-4a90-8e1a-35f75e84d27f",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-217995699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3c431b4-767f-4e7e-8286-25cc33db7adc",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe worth adding a comment here. If `currentPosition.lastFetchEpoch` is equal to `respEndOffset.leaderEpoch`, then the truncation is precise.",
        "createdAt" : "2019-03-22T23:11:21Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +739,743 @@                            }\n\n                            if (respEndOffset.endOffset() < currentPosition.offset) {\n                                if (subscriptions.hasDefaultOffsetResetPolicy()) {\n                                    SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition("
  },
  {
    "id" : "7c0106f5-8888-4e64-a9b3-d981258b1ea1",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-217995699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84461157-7476-475f-9266-061f17b30920",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe the method should be `completeValidation` or something like that?",
        "createdAt" : "2019-03-22T23:12:16Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 241,
    "diffHunk" : "@@ -1,1 +752,756 @@                            } else {\n                                // Offset is fine, clear the validation state\n                                subscriptions.validate(respTopicPartition);\n                            }\n                        }"
  },
  {
    "id" : "c40fbfd6-372b-4b8e-8d41-d6e910956eb6",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-221802445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "551267ad-1fd6-4420-875f-297092d05a08",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the user ignores this exception and calls `poll()`, will we repeat the validation? That may be reasonable, just making sure I understand.",
        "createdAt" : "2019-03-22T23:13:19Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "084a2542-466e-4de5-b99b-ff74432dd91d",
        "parentId" : "551267ad-1fd6-4420-875f-297092d05a08",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "At this point, the subscription state is still in AWAITING_VALIDATION, so yes the next `poll()` call should attempt to revalidate asynchronously and these partitions will be excluded for the time being. ",
        "createdAt" : "2019-04-02T17:34:37Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 247,
    "diffHunk" : "@@ -1,1 +758,762 @@\n                    if (!truncationWithoutResetPolicy.isEmpty()) {\n                        throw new LogTruncationException(truncationWithoutResetPolicy);\n                    }\n                }"
  },
  {
    "id" : "95676a89-2b3a-4790-a6ee-f133ce4c201a",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-228385288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05432830-124b-4714-a001-2e58f0255a94",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we save the second pass over the partitions by doing this collection in the loop above? I'm just thinking about MM-like use cases where the number of partitions could be quite large. A possible optimization is to to cache the metadata update version so that we only bother redoing this check if there has actually been a metadata update.",
        "createdAt" : "2019-04-18T01:59:13Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "95d2d284-4ad3-40a8-931b-c31aa7b3419c",
        "parentId" : "05432830-124b-4714-a001-2e58f0255a94",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "The first pass through all the partitions covers the case of metadata changing, but the second pass through is also used to resubmit the async request with backoff. We could remember the last metadata version seen and avoid unnecessary calls to the first loop.",
        "createdAt" : "2019-04-18T15:34:11Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "b1fa148f-c8ea-4c32-aca9-e2f36919789a",
        "parentId" : "05432830-124b-4714-a001-2e58f0255a94",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "How about we leave this for a follow-up?",
        "createdAt" : "2019-04-18T16:11:18Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "5544c8d2-c9e2-4330-b8c2-a5cc711ced56",
        "parentId" : "05432830-124b-4714-a001-2e58f0255a94",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Works for me 👍 ",
        "createdAt" : "2019-04-18T16:29:07Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +433,437 @@\n        // Collect positions needing validation, with backoff\n        Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate = subscriptions\n                .partitionsNeedingValidation(time.milliseconds())\n                .stream()"
  },
  {
    "id" : "89fc6d0e-83c8-4829-b446-ae53aac502aa",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-228371955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f16e3a2-d2ca-4d70-aa66-31ea9f3bd964",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The checks above ensure that we are still in the validating phase and that the current leader epoch hasn't changed. I guess it is still possible that both of these are true, but the user has seeked to a different position. Perhaps we can add position to the cached data above?",
        "createdAt" : "2019-04-18T15:01:54Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "4ef674dd-3576-4fd1-820e-a014c5e12499",
        "parentId" : "6f16e3a2-d2ca-4d70-aa66-31ea9f3bd964",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I think it's still okay as long as the position's epoch hasn't changed. What's the side effect if you seek to offset 10 (FETCHING), do validation (AWAIT_VALIDATION), seek to offset 30 (FETCHING), do validation again (AWAIT_VALIDATION), and then get back the OffsetsForLeader response from the first async validation? I think as long as the position's epoch is the same, there isn't a problem. When the second response comes back it will get ignored since we won't be in the right state. WDYT?",
        "createdAt" : "2019-04-18T15:57:49Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "432cca33-8112-4e6e-bf8d-0aeed186aed5",
        "parentId" : "6f16e3a2-d2ca-4d70-aa66-31ea9f3bd964",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I agree that the important thing is that the position's epoch hasn't changed. That and the current leader epoch are the only input to the OffsetsForLeaderEpoch API.",
        "createdAt" : "2019-04-18T16:01:11Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +735,739 @@                            SubscriptionState.FetchPosition currentPosition = subscriptions.position(respTopicPartition);\n                            Metadata.LeaderAndEpoch currentLeader = currentPosition.currentLeader;\n                            if (!currentLeader.equals(cachedLeaderAndEpochs.get(respTopicPartition))) {\n                                return;\n                            }"
  },
  {
    "id" : "d908e820-68e8-4608-98db-ec0ee7fdda68",
    "prId" : 6427,
    "prUrl" : "https://github.com/apache/kafka/pull/6427#pullrequestreview-214224025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Any reason why we didn't use String.format here?",
        "createdAt" : "2019-03-13T05:55:19Z",
        "updatedAt" : "2019-03-13T05:55:23Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "f40377bc-5ecf-4d8f-9e09-d922b7783419",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "No technical reason. I see that `String.format` is used in a few places already. Let me know if you would like me to change this.",
        "createdAt" : "2019-03-13T16:52:17Z",
        "updatedAt" : "2019-03-13T16:52:17Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "e834d69b-6963-4829-9c45-9d563fc7fabb",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "@ijuma, Actually, I now remember why I did it. `MessageFormatter` supports Java arrays while `String.format` does not.",
        "createdAt" : "2019-03-13T19:16:59Z",
        "updatedAt" : "2019-03-13T19:17:00Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "63aab6fa-d5cb-4a00-bae8-d80e3ec8d992",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Why would you want arrays instead of varargs though?",
        "createdAt" : "2019-03-13T20:48:46Z",
        "updatedAt" : "2019-03-13T20:48:46Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "37cff944-9d45-4680-84fa-73d8fee2c839",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "```\r\nString.format(\"array=%s\", new Object[]{1,2,3})\r\narray=[I@76f78e3a\r\n\r\nMessageFormatter.arrayFormat(\"array={}\", new Object[]{new Object[]{1,2,3}}).getMessage()\r\narray=[1,2,3]\r\n```\r\nRight?\r\n",
        "createdAt" : "2019-03-13T21:34:13Z",
        "updatedAt" : "2019-03-13T21:34:13Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f78e775c00a00235d853b09de0146797dc2fa8b",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +249,253 @@                                            message = MessageFormatter.arrayFormat(\n                                                    \"Response for missing full request partition: partition={}; metadata={}\",\n                                                    new Object[]{partition, data.metadata()}).getMessage();\n                                        } else {\n                                            message = MessageFormatter.arrayFormat("
  },
  {
    "id" : "64696b8b-43db-4ac1-bb29-fb36c5c5ccf9",
    "prId" : 6559,
    "prUrl" : "https://github.com/apache/kafka/pull/6559#pullrequestreview-225689114",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8f43aa5-1e73-428d-915d-7af019424e89",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "I first questioned in myself if an int is enough for this or not but we're not using any kind of ordering property so if it overflows then we just end up with a funny ID but nothing more. (So I'm just noting this.)\r\nAlso it seems to be more effective than other methods. I was just thinking about whether we can solve this by reference comparison of the return value of `subscription.assignedPartitions()` and a weak reference stored here but the first one is a final map, so that doesn't help in this case.",
        "createdAt" : "2019-04-11T16:52:53Z",
        "updatedAt" : "2019-04-11T17:17:20Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "4ac53417-65b5-4a2e-9d15-dcbb38b152e9",
        "parentId" : "e8f43aa5-1e73-428d-915d-7af019424e89",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I debated this myself. Probably no harm using a long, but we currently use ints in similar situations (e.g. metadata updates), so I thought we may as well be consistent. 2 billion reassignments is probably enough for anyone, right? 😝 ",
        "createdAt" : "2019-04-11T17:47:49Z",
        "updatedAt" : "2019-04-11T17:47:49Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3ea8405045f4cd312261a6db71963bd1f9b56a2",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1425,1429 @@        private final Sensor recordsFetchLead;\n\n        private int assignmentId = 0;\n        private Set<TopicPartition> assignedPartitions = Collections.emptySet();\n"
  },
  {
    "id" : "15f2f16a-40bc-4aca-a702-c0832e48974b",
    "prId" : 6559,
    "prUrl" : "https://github.com/apache/kafka/pull/6559#pullrequestreview-225891878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5098217c-7174-4349-af50-104a8eae2e9c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: add a debug level logging on the `if` condition below triggered?",
        "createdAt" : "2019-04-12T04:26:51Z",
        "updatedAt" : "2019-04-12T04:26:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3ea8405045f4cd312261a6db71963bd1f9b56a2",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +1490,1494 @@\n        private void maybeUpdateAssignment(SubscriptionState subscription) {\n            int newAssignmentId = subscription.assignmentId();\n            if (this.assignmentId != newAssignmentId) {\n                Set<TopicPartition> newAssignedPartitions = new HashSet<>(subscription.assignedPartitions());"
  },
  {
    "id" : "19bc0db8-238b-4b0d-8512-8458a473407c",
    "prId" : 6582,
    "prUrl" : "https://github.com/apache/kafka/pull/6582#pullrequestreview-240243080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8444edc8-1947-4d25-9e1e-6f0878f19a80",
        "parentId" : null,
        "authorId" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "body" : "One idea that I had was to make this a `Map<Integer, Long>`, with the value being `System.currentTimeMillis()` at the time the fetch request is sent.\r\n\r\nThat would allow the \"Skipping fetch for partition\" log message to include the duration that the previous request has been pending for (possibly adjusting the log level based on how long ago that previous request was sent), and also enable a fetch request time metric to be easily collected if someone wishes to add that enhancement in the future.",
        "createdAt" : "2019-05-21T16:28:27Z",
        "updatedAt" : "2019-05-21T16:28:28Z",
        "lastEditedBy" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "tags" : [
        ]
      },
      {
        "id" : "6986eb22-c3bc-4a5c-badb-6b886e9a0cbf",
        "parentId" : "8444edc8-1947-4d25-9e1e-6f0878f19a80",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "FYI, there is already a metric for fetch request latency.",
        "createdAt" : "2019-05-21T18:28:15Z",
        "updatedAt" : "2019-05-21T18:28:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9407381a-05a1-4d0f-bf00-0cd875aa23a7",
        "parentId" : "8444edc8-1947-4d25-9e1e-6f0878f19a80",
        "authorId" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "body" : "oh, ok",
        "createdAt" : "2019-05-21T19:05:39Z",
        "updatedAt" : "2019-05-21T19:05:39Z",
        "lastEditedBy" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6690a7ea679ddf3e186b7facb20ca074ec2757f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +144,148 @@    private final AtomicReference<RuntimeException> cachedOffsetForLeaderException = new AtomicReference<>();\n    private final OffsetsForLeaderEpochClient offsetsForLeaderEpochClient;\n    private final Set<Integer> nodesWithPendingFetchRequests;\n\n    private PartitionRecords nextInLineRecords = null;"
  },
  {
    "id" : "0a95f3c1-0366-4aa6-8593-d347a55d3224",
    "prId" : 6731,
    "prUrl" : "https://github.com/apache/kafka/pull/6731#pullrequestreview-239005707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "931bf2db-988a-44b0-b655-689ad7e8cf00",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Does this assume that the preferred read replica cannot be the leader?",
        "createdAt" : "2019-05-17T15:57:29Z",
        "updatedAt" : "2019-05-17T21:55:02Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c7c0bdc761dc4bb68e0ec59c113abb4a834901d",
    "line" : 140,
    "diffHunk" : "@@ -1,1 +1187,1191 @@                Optional<Integer> clearedReplicaId = subscriptions.clearPreferredReadReplica(tp);\n                if (!clearedReplicaId.isPresent()) {\n                    // If there's no preferred replica to clear, we're fetching from the leader so handle this error normally\n                    if (fetchOffset != subscriptions.position(tp).offset) {\n                        log.debug(\"Discarding stale fetch response for partition {} since the fetched offset {} \" +"
  },
  {
    "id" : "0ad49f88-c2c7-4f6f-9f32-5725fc770b2d",
    "prId" : 6731,
    "prUrl" : "https://github.com/apache/kafka/pull/6731#pullrequestreview-239169896",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5452a1e-c62b-4160-9968-a04224c78c67",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "For OFFSET_NOT_AVAILABLE, we do not need to refresh metadata. We can just retry.",
        "createdAt" : "2019-05-17T23:41:46Z",
        "updatedAt" : "2019-05-17T23:46:45Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c7c0bdc761dc4bb68e0ec59c113abb4a834901d",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +1178,1182 @@                       error == Errors.KAFKA_STORAGE_ERROR ||\n                       error == Errors.FENCED_LEADER_EPOCH ||\n                       error == Errors.OFFSET_NOT_AVAILABLE) {\n                log.debug(\"Error in fetch for partition {}: {}\", tp, error.exceptionName());\n                this.metadata.requestUpdate();"
  }
]