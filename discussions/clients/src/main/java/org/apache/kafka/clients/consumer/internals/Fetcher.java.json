[
  {
    "id" : "2ffead69-4cec-4d60-9b76-8b272c629c7e",
    "prId" : 4557,
    "prUrl" : "https://github.com/apache/kafka/pull/4557#pullrequestreview-95859553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bae74676-eb5b-4c79-b180-e234dfc20389",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "Is it possible that offsetResetTimestamps is empty ?",
        "createdAt" : "2018-02-12T03:24:42Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "608ab8d6-48c8-4faa-aeae-d7cb002a7099",
        "parentId" : "bae74676-eb5b-4c79-b180-e234dfc20389",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It is possible, but seems fine? We could even get rid of the empty check a couple lines above.",
        "createdAt" : "2018-02-12T16:46:00Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "76c796ca128c3c97231f3ebda994a07bb06b26aa",
    "line" : 217,
    "diffHunk" : "@@ -1,1 +375,379 @@        }\n\n        resetOffsetsAsync(offsetResetTimestamps);\n    }\n"
  },
  {
    "id" : "7dee546e-3db7-4fbc-a004-ffc212cd2cc0",
    "prId" : 4557,
    "prUrl" : "https://github.com/apache/kafka/pull/4557#pullrequestreview-96404008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd70a793-b531-4e5e-9f47-98478d13a824",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "What do we do if one of the exceptions is not retriable (e.g UNSUPPORTED_FOR_MESSAGE_FORMAT)? It seems to be handled differently from `TOPIC_AUTHORIZATION_FAILED`, but I wasn't sure why.",
        "createdAt" : "2018-02-13T23:06:03Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "c429894e-0e14-4692-af64-9a599d0d0b9a",
        "parentId" : "dd70a793-b531-4e5e-9f47-98478d13a824",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We treat `UNSUPPORTED_FOR_MESSAGE_FORMAT` as equivalent to not finding an offset for the timestamp being searched. It makes sense because timestamps didn't exist in the old format. I can clarify this in the comment.",
        "createdAt" : "2018-02-14T07:31:28Z",
        "updatedAt" : "2018-02-14T15:55:04Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "76c796ca128c3c97231f3ebda994a07bb06b26aa",
    "line" : 614,
    "diffHunk" : "@@ -1,1 +788,792 @@            future.raise(new TopicAuthorizationException(unauthorizedTopics));\n        else\n            future.complete(new ListOffsetResult(fetchedOffsets, partitionsToRetry));\n    }\n"
  },
  {
    "id" : "6f0dcd8e-5a1e-43e6-ade3-998b8def62e0",
    "prId" : 5495,
    "prUrl" : "https://github.com/apache/kafka/pull/5495#pullrequestreview-146008624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14d0e4a8-bf79-4bc5-9e88-b53543bf50f5",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Wouldn't it be enough to lock on the given instance of the FetchSessionHandler given that's the object that runs into a concurrent modification? This way as I understand we completely evict concurrency from the sendFetches method for a given instance.\r\nAlso I would generally prefer using a lock object instead of locking on `this`. The reason is that this way the synchronized is externalized to the public API (well in this case it is arguable since it's an _internals_ class), + it enables accidental lock stealing, ie. a different class locking on Fetcher.this. I don't know if this is a concern now though.",
        "createdAt" : "2018-08-13T13:41:41Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "17a8ce92-820e-4599-9f52-9222d1eb2e54",
        "parentId" : "14d0e4a8-bf79-4bc5-9e88-b53543bf50f5",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@viktorsomogyi Thanks for the review. There is also a `sessionHandlers` HashMap. That would need to become a `ConcurrentHashMap`. I wasn't sure if there was any other state. We do broad locking of the coordinator for thread-safety, I thought the same for `Fetcher` would be the simplest and safest fix. Since this code is generally single-threaded and locking is only to avoid concurrent access in the heartbeat thread, I am not sure it matters so much. Will wait for @hachikuji 's review and then update if required.",
        "createdAt" : "2018-08-14T10:10:02Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "47080302-621e-4678-b1f7-d33361c71070",
        "parentId" : "14d0e4a8-bf79-4bc5-9e88-b53543bf50f5",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "I see, then it's probably ok. I was missing the detail about the `sessionHandlers` map, but thanks for the heads-up :).",
        "createdAt" : "2018-08-14T10:45:29Z",
        "updatedAt" : "2018-09-14T14:43:43Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "b379bf19b941c9e97d243f8fc2175147a9fab6e8",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +206,210 @@     * @return number of fetches sent\n     */\n    public synchronized int sendFetches() {\n        Map<Node, FetchSessionHandler.FetchRequestData> fetchRequestMap = prepareFetchRequests();\n        for (Map.Entry<Node, FetchSessionHandler.FetchRequestData> entry : fetchRequestMap.entrySet()) {"
  },
  {
    "id" : "5279ad4b-5c8c-4e68-ad6c-3f03ebd09e67",
    "prId" : 5627,
    "prUrl" : "https://github.com/apache/kafka/pull/5627#pullrequestreview-153980869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fae6ccd-875e-450c-94b2-53b3ca2cad30",
        "parentId" : null,
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "I moved the logic for getting the list of partitions that do not have available leader to `groupListOffsetRequests` . I was contemplating whether we should add partition for which we have a failed connection to leader to `partitionsToRetry`. Seems like we should, because we may be able to reconnect before the list offsets timeout. ",
        "createdAt" : "2018-09-10T22:02:42Z",
        "updatedAt" : "2018-09-10T23:39:54Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f8fbd8a0a66124cffd3a95a15fe8464dd3ae92b",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +695,699 @@                log.debug(\"Leader {} for partition {} is unavailable for fetching offset until reconnect backoff expires\",\n                          info.leader(), tp);\n                partitionsToRetry.add(tp);\n            } else {\n                Node node = info.leader();"
  },
  {
    "id" : "9ea10ce5-eaa6-45ea-b071-fa3479cf42a5",
    "prId" : 5991,
    "prUrl" : "https://github.com/apache/kafka/pull/5991#pullrequestreview-182458401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b546855b-aecb-47bc-b2fd-f7d970d6040a",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Might be nice to add a couple test cases to make sure the expected behavior won't regress.",
        "createdAt" : "2018-12-06T22:00:00Z",
        "updatedAt" : "2018-12-14T16:34:23Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "54d8dd54100b18957db07485d9ee0c37ad4476c8",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +813,817 @@                       error == Errors.REPLICA_NOT_AVAILABLE ||\n                       error == Errors.KAFKA_STORAGE_ERROR ||\n                       error == Errors.OFFSET_NOT_AVAILABLE ||\n                       error == Errors.LEADER_NOT_AVAILABLE) {\n                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\","
  },
  {
    "id" : "d3862562-44a7-4276-b1d6-a5fea7becd38",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-214244487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I am sure I am missing something but how can this be true when due to line 560 `if (partitionRecords.nextFetchOffset == position.offset) {`?",
        "createdAt" : "2019-03-12T22:19:20Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "9a3f0246-7007-467d-9d48-a7d53336759b",
        "parentId" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "On a side note, unless the local variable and the field are marked as `final`, it is basically impossible for javac to catch this statically because Java's concurrency model. E.g. any thread can modify both `partitionRecords.nextFetchOffset` and `position.offset`",
        "createdAt" : "2019-03-12T22:23:49Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "99e150ef-f7fa-401e-81d1-379446c6cb04",
        "parentId" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Good catch. Let me look into this... might be something I missed during the merge from trunk. ",
        "createdAt" : "2019-03-13T13:28:04Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "2affcb55-ce9e-463e-ac5b-2d86cd5dac33",
        "parentId" : "d8dcfa3c-78e9-4d92-b843-ba6c13bd9255",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I think the code is correct as-is. The call to `partitionRecords.fetchRecords(maxRecords)` updates the nextFetchOffset to the offset of the last record returned plus one.",
        "createdAt" : "2019-03-13T22:31:55Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +604,608 @@                List<ConsumerRecord<K, V>> partRecords = partitionRecords.fetchRecords(maxRecords);\n\n                if (partitionRecords.nextFetchOffset > position.offset) {\n                    SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition(\n                            partitionRecords.nextFetchOffset,"
  },
  {
    "id" : "ed8635b8-c3bc-4a90-8e1a-35f75e84d27f",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-217995699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3c431b4-767f-4e7e-8286-25cc33db7adc",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe worth adding a comment here. If `currentPosition.lastFetchEpoch` is equal to `respEndOffset.leaderEpoch`, then the truncation is precise.",
        "createdAt" : "2019-03-22T23:11:21Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +739,743 @@                            }\n\n                            if (respEndOffset.endOffset() < currentPosition.offset) {\n                                if (subscriptions.hasDefaultOffsetResetPolicy()) {\n                                    SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition("
  },
  {
    "id" : "7c0106f5-8888-4e64-a9b3-d981258b1ea1",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-217995699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84461157-7476-475f-9266-061f17b30920",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe the method should be `completeValidation` or something like that?",
        "createdAt" : "2019-03-22T23:12:16Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 241,
    "diffHunk" : "@@ -1,1 +752,756 @@                            } else {\n                                // Offset is fine, clear the validation state\n                                subscriptions.validate(respTopicPartition);\n                            }\n                        }"
  },
  {
    "id" : "c40fbfd6-372b-4b8e-8d41-d6e910956eb6",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-221802445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "551267ad-1fd6-4420-875f-297092d05a08",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the user ignores this exception and calls `poll()`, will we repeat the validation? That may be reasonable, just making sure I understand.",
        "createdAt" : "2019-03-22T23:13:19Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "084a2542-466e-4de5-b99b-ff74432dd91d",
        "parentId" : "551267ad-1fd6-4420-875f-297092d05a08",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "At this point, the subscription state is still in AWAITING_VALIDATION, so yes the next `poll()` call should attempt to revalidate asynchronously and these partitions will be excluded for the time being. ",
        "createdAt" : "2019-04-02T17:34:37Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 247,
    "diffHunk" : "@@ -1,1 +758,762 @@\n                    if (!truncationWithoutResetPolicy.isEmpty()) {\n                        throw new LogTruncationException(truncationWithoutResetPolicy);\n                    }\n                }"
  },
  {
    "id" : "95676a89-2b3a-4790-a6ee-f133ce4c201a",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-228385288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05432830-124b-4714-a001-2e58f0255a94",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we save the second pass over the partitions by doing this collection in the loop above? I'm just thinking about MM-like use cases where the number of partitions could be quite large. A possible optimization is to to cache the metadata update version so that we only bother redoing this check if there has actually been a metadata update.",
        "createdAt" : "2019-04-18T01:59:13Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "95d2d284-4ad3-40a8-931b-c31aa7b3419c",
        "parentId" : "05432830-124b-4714-a001-2e58f0255a94",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "The first pass through all the partitions covers the case of metadata changing, but the second pass through is also used to resubmit the async request with backoff. We could remember the last metadata version seen and avoid unnecessary calls to the first loop.",
        "createdAt" : "2019-04-18T15:34:11Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "b1fa148f-c8ea-4c32-aca9-e2f36919789a",
        "parentId" : "05432830-124b-4714-a001-2e58f0255a94",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "How about we leave this for a follow-up?",
        "createdAt" : "2019-04-18T16:11:18Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "5544c8d2-c9e2-4330-b8c2-a5cc711ced56",
        "parentId" : "05432830-124b-4714-a001-2e58f0255a94",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Works for me üëç ",
        "createdAt" : "2019-04-18T16:29:07Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +433,437 @@\n        // Collect positions needing validation, with backoff\n        Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate = subscriptions\n                .partitionsNeedingValidation(time.milliseconds())\n                .stream()"
  },
  {
    "id" : "89fc6d0e-83c8-4829-b446-ae53aac502aa",
    "prId" : 6371,
    "prUrl" : "https://github.com/apache/kafka/pull/6371#pullrequestreview-228371955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f16e3a2-d2ca-4d70-aa66-31ea9f3bd964",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The checks above ensure that we are still in the validating phase and that the current leader epoch hasn't changed. I guess it is still possible that both of these are true, but the user has seeked to a different position. Perhaps we can add position to the cached data above?",
        "createdAt" : "2019-04-18T15:01:54Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "4ef674dd-3576-4fd1-820e-a014c5e12499",
        "parentId" : "6f16e3a2-d2ca-4d70-aa66-31ea9f3bd964",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I think it's still okay as long as the position's epoch hasn't changed. What's the side effect if you seek to offset 10 (FETCHING), do validation (AWAIT_VALIDATION), seek to offset 30 (FETCHING), do validation again (AWAIT_VALIDATION), and then get back the OffsetsForLeader response from the first async validation? I think as long as the position's epoch is the same, there isn't a problem. When the second response comes back it will get ignored since we won't be in the right state. WDYT?",
        "createdAt" : "2019-04-18T15:57:49Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "432cca33-8112-4e6e-bf8d-0aeed186aed5",
        "parentId" : "6f16e3a2-d2ca-4d70-aa66-31ea9f3bd964",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I agree that the important thing is that the position's epoch hasn't changed. That and the current leader epoch are the only input to the OffsetsForLeaderEpoch API.",
        "createdAt" : "2019-04-18T16:01:11Z",
        "updatedAt" : "2019-04-19T22:50:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0934f5ac0211e742b1c5b77b1b5c067ed5ff9a6e",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +735,739 @@                            SubscriptionState.FetchPosition currentPosition = subscriptions.position(respTopicPartition);\n                            Metadata.LeaderAndEpoch currentLeader = currentPosition.currentLeader;\n                            if (!currentLeader.equals(cachedLeaderAndEpochs.get(respTopicPartition))) {\n                                return;\n                            }"
  },
  {
    "id" : "d908e820-68e8-4608-98db-ec0ee7fdda68",
    "prId" : 6427,
    "prUrl" : "https://github.com/apache/kafka/pull/6427#pullrequestreview-214224025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Any reason why we didn't use String.format here?",
        "createdAt" : "2019-03-13T05:55:19Z",
        "updatedAt" : "2019-03-13T05:55:23Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "f40377bc-5ecf-4d8f-9e09-d922b7783419",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "No technical reason. I see that `String.format` is used in a few places already. Let me know if you would like me to change this.",
        "createdAt" : "2019-03-13T16:52:17Z",
        "updatedAt" : "2019-03-13T16:52:17Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "e834d69b-6963-4829-9c45-9d563fc7fabb",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "@ijuma, Actually, I now remember why I did it. `MessageFormatter` supports Java arrays while `String.format` does not.",
        "createdAt" : "2019-03-13T19:16:59Z",
        "updatedAt" : "2019-03-13T19:17:00Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "63aab6fa-d5cb-4a00-bae8-d80e3ec8d992",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Why would you want arrays instead of varargs though?",
        "createdAt" : "2019-03-13T20:48:46Z",
        "updatedAt" : "2019-03-13T20:48:46Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "37cff944-9d45-4680-84fa-73d8fee2c839",
        "parentId" : "455792e7-6d22-4135-b640-42b0024b10d3",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "```\r\nString.format(\"array=%s\", new Object[]{1,2,3})\r\narray=[I@76f78e3a\r\n\r\nMessageFormatter.arrayFormat(\"array={}\", new Object[]{new Object[]{1,2,3}}).getMessage()\r\narray=[1,2,3]\r\n```\r\nRight?\r\n",
        "createdAt" : "2019-03-13T21:34:13Z",
        "updatedAt" : "2019-03-13T21:34:13Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f78e775c00a00235d853b09de0146797dc2fa8b",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +249,253 @@                                            message = MessageFormatter.arrayFormat(\n                                                    \"Response for missing full request partition: partition={}; metadata={}\",\n                                                    new Object[]{partition, data.metadata()}).getMessage();\n                                        } else {\n                                            message = MessageFormatter.arrayFormat("
  },
  {
    "id" : "64696b8b-43db-4ac1-bb29-fb36c5c5ccf9",
    "prId" : 6559,
    "prUrl" : "https://github.com/apache/kafka/pull/6559#pullrequestreview-225689114",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8f43aa5-1e73-428d-915d-7af019424e89",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "I first questioned in myself if an int is enough for this or not but we're not using any kind of ordering property so if it overflows then we just end up with a funny ID but nothing more. (So I'm just noting this.)\r\nAlso it seems to be more effective than other methods. I was just thinking about whether we can solve this by reference comparison of the return value of `subscription.assignedPartitions()` and a weak reference stored here but the first one is a final map, so that doesn't help in this case.",
        "createdAt" : "2019-04-11T16:52:53Z",
        "updatedAt" : "2019-04-11T17:17:20Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "4ac53417-65b5-4a2e-9d15-dcbb38b152e9",
        "parentId" : "e8f43aa5-1e73-428d-915d-7af019424e89",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I debated this myself. Probably no harm using a long, but we currently use ints in similar situations (e.g. metadata updates), so I thought we may as well be consistent. 2 billion reassignments is probably enough for anyone, right? üòù ",
        "createdAt" : "2019-04-11T17:47:49Z",
        "updatedAt" : "2019-04-11T17:47:49Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3ea8405045f4cd312261a6db71963bd1f9b56a2",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1425,1429 @@        private final Sensor recordsFetchLead;\n\n        private int assignmentId = 0;\n        private Set<TopicPartition> assignedPartitions = Collections.emptySet();\n"
  },
  {
    "id" : "15f2f16a-40bc-4aca-a702-c0832e48974b",
    "prId" : 6559,
    "prUrl" : "https://github.com/apache/kafka/pull/6559#pullrequestreview-225891878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5098217c-7174-4349-af50-104a8eae2e9c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: add a debug level logging on the `if` condition below triggered?",
        "createdAt" : "2019-04-12T04:26:51Z",
        "updatedAt" : "2019-04-12T04:26:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3ea8405045f4cd312261a6db71963bd1f9b56a2",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +1490,1494 @@\n        private void maybeUpdateAssignment(SubscriptionState subscription) {\n            int newAssignmentId = subscription.assignmentId();\n            if (this.assignmentId != newAssignmentId) {\n                Set<TopicPartition> newAssignedPartitions = new HashSet<>(subscription.assignedPartitions());"
  },
  {
    "id" : "19bc0db8-238b-4b0d-8512-8458a473407c",
    "prId" : 6582,
    "prUrl" : "https://github.com/apache/kafka/pull/6582#pullrequestreview-240243080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8444edc8-1947-4d25-9e1e-6f0878f19a80",
        "parentId" : null,
        "authorId" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "body" : "One idea that I had was to make this a `Map<Integer, Long>`, with the value being `System.currentTimeMillis()` at the time the fetch request is sent.\r\n\r\nThat would allow the \"Skipping fetch for partition\" log message to include the duration that the previous request has been pending for (possibly adjusting the log level based on how long ago that previous request was sent), and also enable a fetch request time metric to be easily collected if someone wishes to add that enhancement in the future.",
        "createdAt" : "2019-05-21T16:28:27Z",
        "updatedAt" : "2019-05-21T16:28:28Z",
        "lastEditedBy" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "tags" : [
        ]
      },
      {
        "id" : "6986eb22-c3bc-4a5c-badb-6b886e9a0cbf",
        "parentId" : "8444edc8-1947-4d25-9e1e-6f0878f19a80",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "FYI, there is already a metric for fetch request latency.",
        "createdAt" : "2019-05-21T18:28:15Z",
        "updatedAt" : "2019-05-21T18:28:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9407381a-05a1-4d0f-bf00-0cd875aa23a7",
        "parentId" : "8444edc8-1947-4d25-9e1e-6f0878f19a80",
        "authorId" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "body" : "oh, ok",
        "createdAt" : "2019-05-21T19:05:39Z",
        "updatedAt" : "2019-05-21T19:05:39Z",
        "lastEditedBy" : "fe197fc8-ce23-45ee-9a80-f73d50e5b450",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6690a7ea679ddf3e186b7facb20ca074ec2757f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +144,148 @@    private final AtomicReference<RuntimeException> cachedOffsetForLeaderException = new AtomicReference<>();\n    private final OffsetsForLeaderEpochClient offsetsForLeaderEpochClient;\n    private final Set<Integer> nodesWithPendingFetchRequests;\n\n    private PartitionRecords nextInLineRecords = null;"
  },
  {
    "id" : "0a95f3c1-0366-4aa6-8593-d347a55d3224",
    "prId" : 6731,
    "prUrl" : "https://github.com/apache/kafka/pull/6731#pullrequestreview-239005707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "931bf2db-988a-44b0-b655-689ad7e8cf00",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Does this assume that the preferred read replica cannot be the leader?",
        "createdAt" : "2019-05-17T15:57:29Z",
        "updatedAt" : "2019-05-17T21:55:02Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c7c0bdc761dc4bb68e0ec59c113abb4a834901d",
    "line" : 140,
    "diffHunk" : "@@ -1,1 +1187,1191 @@                Optional<Integer> clearedReplicaId = subscriptions.clearPreferredReadReplica(tp);\n                if (!clearedReplicaId.isPresent()) {\n                    // If there's no preferred replica to clear, we're fetching from the leader so handle this error normally\n                    if (fetchOffset != subscriptions.position(tp).offset) {\n                        log.debug(\"Discarding stale fetch response for partition {} since the fetched offset {} \" +"
  },
  {
    "id" : "0ad49f88-c2c7-4f6f-9f32-5725fc770b2d",
    "prId" : 6731,
    "prUrl" : "https://github.com/apache/kafka/pull/6731#pullrequestreview-239169896",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5452a1e-c62b-4160-9968-a04224c78c67",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "For OFFSET_NOT_AVAILABLE, we do not need to refresh metadata. We can just retry.",
        "createdAt" : "2019-05-17T23:41:46Z",
        "updatedAt" : "2019-05-17T23:46:45Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c7c0bdc761dc4bb68e0ec59c113abb4a834901d",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +1178,1182 @@                       error == Errors.KAFKA_STORAGE_ERROR ||\n                       error == Errors.FENCED_LEADER_EPOCH ||\n                       error == Errors.OFFSET_NOT_AVAILABLE) {\n                log.debug(\"Error in fetch for partition {}: {}\", tp, error.exceptionName());\n                this.metadata.requestUpdate();"
  },
  {
    "id" : "3482a8e8-6f28-41c9-a940-f97ba2d167e3",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-248455347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be0b7829-1351-4253-974b-5a247bd1f967",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Not a big deal, but could potentially skip this check since we are checking for metric existence below anyway.",
        "createdAt" : "2019-06-11T22:11:25Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1714,1718 @@\n                for (TopicPartition tp : newAssignedPartitions) {\n                    if (!this.assignedPartitions.contains(tp)) {\n                        MetricName metricName = partitionPreferredReadReplicaMetricName(tp);\n                        if (metrics.metric(metricName) == null) {"
  },
  {
    "id" : "e27c3747-4ac2-4a78-8eb4-52b2205fda8c",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-248455347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "547976e0-ecce-4921-b756-6cae8797fb55",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: We have this logic in a couple other places. Maybe sufficient cause to turn it into a method?",
        "createdAt" : "2019-06-11T22:13:18Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +1775,1779 @@\n        private Map<String, String> topicPartitionTags(TopicPartition tp) {\n            Map<String, String> metricTags = new HashMap<>(2);\n            metricTags.put(\"topic\", tp.topic().replace('.', '_'));\n            metricTags.put(\"partition\", String.valueOf(tp.partition()));"
  },
  {
    "id" : "a9018c45-e61d-486e-85ab-cea9336442f9",
    "prId" : 6988,
    "prUrl" : "https://github.com/apache/kafka/pull/6988#pullrequestreview-269719194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "005caa8b-e8c0-4291-b3b4-773d8e7127ef",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Just one more thing I thought of when I was looking at the test cases. With paused partitions in `completedFetches`, I think we will never block in `KafkaConsumer.pollForFetches` since `hasCompletedFetches` would continue returning true even if there is no data that can be returned. This would result in a busy loop consuming cpu. We could either change the logic to `hasCompletedFetches` or we could introduce a new api (e.g. `hasAvailableFetches`).",
        "createdAt" : "2019-08-01T06:38:20Z",
        "updatedAt" : "2019-08-01T16:03:51Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "124a881e-4aa5-4ca7-bcfe-3a3e5ada3a1b",
        "parentId" : "005caa8b-e8c0-4291-b3b4-773d8e7127ef",
        "authorId" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "body" : "I'll implement `hasAvailableFetches` to check if a fetchable partition exists in `completedFetches`.  `hasCompletedFetches` only has the single call site in the `KafkaConsumer`.  We could make it make it package protected if we wanted to so tests can still use it.  Since this is part of clients internals should we do that?",
        "createdAt" : "2019-08-01T14:48:19Z",
        "updatedAt" : "2019-08-01T16:03:51Z",
        "lastEditedBy" : "27ab30e2-edf7-4e09-a032-8cc89f132093",
        "tags" : [
        ]
      },
      {
        "id" : "7fbe46af-6541-4c99-9a17-657ca9edf2c0",
        "parentId" : "005caa8b-e8c0-4291-b3b4-773d8e7127ef",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, that sounds fine to me. It does seem useful for testing, so package access makes sense.",
        "createdAt" : "2019-08-01T15:56:11Z",
        "updatedAt" : "2019-08-01T16:03:51Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9b47190841af24dc79beed51837946c119995d5",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +590,594 @@     * @throws TopicAuthorizationException If there is TopicAuthorization error in fetchResponse.\n     */\n    public Map<TopicPartition, List<ConsumerRecord<K, V>>> fetchedRecords() {\n        Map<TopicPartition, List<ConsumerRecord<K, V>>> fetched = new HashMap<>();\n        Queue<PartitionRecords> pausedCompletedFetches = new ArrayDeque<>();"
  },
  {
    "id" : "a71b2505-f503-40a6-8924-3e15e227b649",
    "prId" : 7222,
    "prUrl" : "https://github.com/apache/kafka/pull/7222#pullrequestreview-277491582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4d978c7-ebb8-49ae-bad6-8531b07867b5",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The `fetchablePartitions` method used below is probably another nice opportunity to use something like `forEachAssignedPartition`.",
        "createdAt" : "2019-08-19T18:09:34Z",
        "updatedAt" : "2020-08-12T14:05:10Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ce148385-e80e-4032-bcb1-2b1906e07023",
        "parentId" : "d4d978c7-ebb8-49ae-bad6-8531b07867b5",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I'm not sure it saves anything in this case. The main benefit of `forEachAssignedPartition` is avoiding making a copy of the assignment set. Since `fetchablePartitions` iterates across the internal set directly I don't think it would help",
        "createdAt" : "2019-08-19T19:16:43Z",
        "updatedAt" : "2020-08-12T14:05:10Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "1f933d66-b275-4832-99ba-56dc5e9f9652",
        "parentId" : "d4d978c7-ebb8-49ae-bad6-8531b07867b5",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "We can avoid a copy in the case @hachikuji mentions as well, right? See below:\r\n\r\n```java\r\nsynchronized List<TopicPartition> fetchablePartitions(Predicate<TopicPartition> isAvailable) {\r\n        return assignment.stream()\r\n                .filter(tpState -> isAvailable.test(tpState.topicPartition()) && tpState.value().isFetchable())\r\n                .map(PartitionStates.PartitionState::topicPartition)\r\n                .collect(Collectors.toList());\r\n    }\r\n```",
        "createdAt" : "2019-08-20T05:23:46Z",
        "updatedAt" : "2020-08-12T14:05:10Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "f542ae84-cd65-45dc-8b1c-5965c651f63c",
        "parentId" : "d4d978c7-ebb8-49ae-bad6-8531b07867b5",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Ah ok, I see what he means. I'll look into this",
        "createdAt" : "2019-08-20T15:39:36Z",
        "updatedAt" : "2020-08-12T14:05:10Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "d2511b37-5fa9-44cd-970b-4c174df4e410",
        "parentId" : "d4d978c7-ebb8-49ae-bad6-8531b07867b5",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I made a pass at this and it wasn't so simple. Deferring for now",
        "createdAt" : "2019-08-20T23:26:08Z",
        "updatedAt" : "2020-08-12T14:05:10Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f3ed50b0c0557a3e31b0b6c2b13188ceab38fd8",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +1142,1146 @@     * that have no existing requests in flight.\n     */\n    private Map<Node, FetchSessionHandler.FetchRequestData> prepareFetchRequests() {\n        Map<Node, FetchSessionHandler.Builder> fetchable = new LinkedHashMap<>();\n"
  },
  {
    "id" : "8708d67d-b14f-422b-b628-0a1249f8150c",
    "prId" : 7222,
    "prUrl" : "https://github.com/apache/kafka/pull/7222#pullrequestreview-278168144",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a24c939f-c222-444f-a138-17a995c12b90",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Mentioned offline, but there is actually a behavioral change here. Previously we didn't validate after an offset reset in any case, but now I think we might if the offset returned from the ListOffset request has an epoch. I think the simple thing to do is to use `Optional.empty()` above when defining `position`.",
        "createdAt" : "2019-08-22T00:16:13Z",
        "updatedAt" : "2020-08-12T14:05:10Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f95a87c5-5f61-4d93-8213-7b8d81bfa68c",
        "parentId" : "a24c939f-c222-444f-a138-17a995c12b90",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "+1 for not introducing a behavior change here. Fixed. Let's see how the next build goes.",
        "createdAt" : "2019-08-22T02:55:25Z",
        "updatedAt" : "2020-08-12T14:05:10Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f3ed50b0c0557a3e31b0b6c2b13188ceab38fd8",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +727,731 @@            metadata.currentLeader(partition));\n        offsetData.leaderEpoch.ifPresent(epoch -> metadata.updateLastSeenEpochIfNewer(partition, epoch));\n        subscriptions.maybeSeekUnvalidated(partition, position, requestedResetStrategy);\n    }\n"
  },
  {
    "id" : "4a779b75-82f7-45fd-bd0f-7acfc1bc914d",
    "prId" : 7511,
    "prUrl" : "https://github.com/apache/kafka/pull/7511#pullrequestreview-302059096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ada9cda-9d2b-4fe9-b76b-a0314d5a4a2d",
        "parentId" : null,
        "authorId" : "94fc4266-bae8-4f22-9f5e-0f69c68f4c29",
        "body" : "Wondering if the subtleties of this fairly benign-looking code warrant a comment here or in the test?",
        "createdAt" : "2019-10-14T20:36:04Z",
        "updatedAt" : "2019-10-16T14:21:20Z",
        "lastEditedBy" : "94fc4266-bae8-4f22-9f5e-0f69c68f4c29",
        "tags" : [
        ]
      },
      {
        "id" : "f2ea63dc-e3c1-4951-a6e9-e812353086d1",
        "parentId" : "9ada9cda-9d2b-4fe9-b76b-a0314d5a4a2d",
        "authorId" : "d7542c21-256b-4d05-bb4a-3071f938b86e",
        "body" : "Was hoping the test would catch the issues with it - but I guess it's possible in the future some change causes the test to be less useful. Could stick a comment to the effect of \"ensure `nodesWithPendingFetchRequests` is always present before finalizer could be called\" or something.",
        "createdAt" : "2019-10-14T20:50:21Z",
        "updatedAt" : "2019-10-16T14:21:20Z",
        "lastEditedBy" : "d7542c21-256b-4d05-bb4a-3071f938b86e",
        "tags" : [
        ]
      },
      {
        "id" : "4ab66915-d69d-49e6-b33c-ad440cb4ba65",
        "parentId" : "9ada9cda-9d2b-4fe9-b76b-a0314d5a4a2d",
        "authorId" : "d7542c21-256b-4d05-bb4a-3071f938b86e",
        "body" : "Was also wondering if there could ever be an exception thrown by `addListener` which would cause the listener to not be added or the completion handler to not be called?",
        "createdAt" : "2019-10-14T20:51:02Z",
        "updatedAt" : "2019-10-16T14:21:20Z",
        "lastEditedBy" : "d7542c21-256b-4d05-bb4a-3071f938b86e",
        "tags" : [
        ]
      },
      {
        "id" : "d3f95fd3-c4e9-470c-95cd-241b92545285",
        "parentId" : "9ada9cda-9d2b-4fe9-b76b-a0314d5a4a2d",
        "authorId" : "94fc4266-bae8-4f22-9f5e-0f69c68f4c29",
        "body" : "> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called?\r\n\r\nHm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.",
        "createdAt" : "2019-10-14T21:06:53Z",
        "updatedAt" : "2019-10-16T14:21:20Z",
        "lastEditedBy" : "94fc4266-bae8-4f22-9f5e-0f69c68f4c29",
        "tags" : [
        ]
      },
      {
        "id" : "de5950dd-788c-46d6-9d65-b21657bc3012",
        "parentId" : "9ada9cda-9d2b-4fe9-b76b-a0314d5a4a2d",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "We rely on `addListener` always resuling in listener being invoked in various places. So the code should be ok. But a comment that explains the sequence and exception scenarios would be useful.",
        "createdAt" : "2019-10-15T11:46:25Z",
        "updatedAt" : "2019-10-16T14:21:20Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "0bcd42ad-4836-4b58-aa34-51b224fb8f19",
        "parentId" : "9ada9cda-9d2b-4fe9-b76b-a0314d5a4a2d",
        "authorId" : "d7542c21-256b-4d05-bb4a-3071f938b86e",
        "body" : "Ok I've added a comment describing the rationale behind the sequence. If this looks good, what are the next steps for getting this merged?",
        "createdAt" : "2019-10-15T16:58:59Z",
        "updatedAt" : "2019-10-16T14:21:20Z",
        "lastEditedBy" : "d7542c21-256b-4d05-bb4a-3071f938b86e",
        "tags" : [
        ]
      }
    ],
    "commit" : "3bd1bd390d4b3ca9a5667ca376cd03d845fca97c",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +265,269 @@            // will be invoked synchronously.\n            this.nodesWithPendingFetchRequests.add(entry.getKey().id());\n            future.addListener(new RequestFutureListener<ClientResponse>() {\n                @Override\n                public void onSuccess(ClientResponse resp) {"
  },
  {
    "id" : "39207b23-d971-4a64-b8f4-3f1bec6b48bc",
    "prId" : 8077,
    "prUrl" : "https://github.com/apache/kafka/pull/8077#pullrequestreview-376328015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "666bb037-c736-474f-abeb-6636fa760a48",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Good catch! This was missed during the initial implementation",
        "createdAt" : "2020-03-16T18:32:32Z",
        "updatedAt" : "2020-03-16T18:32:32Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "76ea01a6-d12b-455e-bf59-da5ec87d8649",
        "parentId" : "666bb037-c736-474f-abeb-6636fa760a48",
        "authorId" : "1079dabd-8107-41dd-b9fa-436b982e8b23",
        "body" : "Thanks for the review! I‚Äôm not sure what are the next steps, but how should I get this PR merged?",
        "createdAt" : "2020-03-17T19:09:13Z",
        "updatedAt" : "2020-03-17T19:09:14Z",
        "lastEditedBy" : "1079dabd-8107-41dd-b9fa-436b982e8b23",
        "tags" : [
        ]
      }
    ],
    "commit" : "a89c06804cc7232988615d385b22ebd80899c39c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +801,805 @@            subscriptions.setNextAllowedRetry(fetchPostitions.keySet(), time.milliseconds() + requestTimeoutMs);\n\n            RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future = offsetsForLeaderEpochClient.sendAsyncRequest(node, fetchPostitions);\n            future.addListener(new RequestFutureListener<OffsetsForLeaderEpochClient.OffsetForEpochResult>() {\n                @Override"
  },
  {
    "id" : "6c3520b4-b2a9-4e26-83ae-3856c7617326",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-356882160",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41c120e4-8cc3-4491-ba3e-dc4813397cc5",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I have to follow @ijuma suggestion to convert the check into a case switch, as the checkstyle won't let me merge 7 boolean statements XD",
        "createdAt" : "2020-02-11T18:14:49Z",
        "updatedAt" : "2020-02-14T18:32:29Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +1013,1017 @@                                  \"is before 0.10.0\", topicPartition);\n                    break;\n                case NOT_LEADER_FOR_PARTITION:\n                case REPLICA_NOT_AVAILABLE:\n                case KAFKA_STORAGE_ERROR:"
  },
  {
    "id" : "885a5fe6-e292-40fb-ae53-5998ad1073fa",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-356994684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0efb0118-d000-456b-a255-1741943a5337",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We can consolidate this with the logic below. Something like this:\r\n```scala\r\nif (remainingToSearch.isEmpty()) {\r\n  return result;\r\n} else {\r\n  client.awaitMetadataUpdate(timer);\r\n}\r\n```",
        "createdAt" : "2020-02-11T21:07:09Z",
        "updatedAt" : "2020-02-14T18:32:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +547,551 @@            }\n\n            if (remainingToSearch.isEmpty()) {\n                return result;\n            } else {"
  },
  {
    "id" : "4d952642-5279-4763-a46e-be5b2f2a1e90",
    "prId" : 8088,
    "prUrl" : "https://github.com/apache/kafka/pull/8088#pullrequestreview-357051459",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2fc14d23-9356-4a82-af5c-e968ce1378d3",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Since we're refactoring from if/else to a switch/case as well as changing the logic, it would be useful to include the before/after of errors that cause retry in the PR description",
        "createdAt" : "2020-02-11T21:59:07Z",
        "updatedAt" : "2020-02-14T18:32:29Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "c8c71b51-898a-480b-82a7-8c949f178035",
        "parentId" : "2fc14d23-9356-4a82-af5c-e968ce1378d3",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sure, done!",
        "createdAt" : "2020-02-11T22:42:28Z",
        "updatedAt" : "2020-02-14T18:32:29Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "78820ca0b3d6bde44409634dae6a78900838870f",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +1019,1023 @@                case LEADER_NOT_AVAILABLE:\n                case FENCED_LEADER_EPOCH:\n                case UNKNOWN_LEADER_EPOCH:\n                    log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n                        topicPartition, error);"
  },
  {
    "id" : "e818f6a7-fe43-4ee9-b92b-ee9b3a70382c",
    "prId" : 8111,
    "prUrl" : "https://github.com/apache/kafka/pull/8111#pullrequestreview-360598275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b93b9a50-66b1-49c9-a30d-974c70c17443",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "While we're at it, could we add the fetch offset to the IllegalStateException message below and the `UNKNOWN_SERVER_ERROR` log message above?",
        "createdAt" : "2020-02-16T23:28:22Z",
        "updatedAt" : "2020-02-20T19:57:45Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "954dedd1-3b7f-4be2-9b5e-562e2d289882",
        "parentId" : "b93b9a50-66b1-49c9-a30d-974c70c17443",
        "authorId" : "7389ee75-6fd7-4f0a-a83c-b6e47290d43f",
        "body" : "Done.",
        "createdAt" : "2020-02-18T19:08:44Z",
        "updatedAt" : "2020-02-20T19:57:45Z",
        "lastEditedBy" : "7389ee75-6fd7-4f0a-a83c-b6e47290d43f",
        "tags" : [
        ]
      }
    ],
    "commit" : "238eced39a01154d09718ec0b0536a3a468037ab",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1275,1279 @@                        fetchOffset, tp);\n            } else if (error == Errors.CORRUPT_MESSAGE) {\n                throw new KafkaException(\"Encountered corrupt message when fetching offset \"\n                        + fetchOffset\n                        + \" for topic-partition \""
  },
  {
    "id" : "58c7c170-6a78-4dc5-86f5-33e350c0a7c0",
    "prId" : 8486,
    "prUrl" : "https://github.com/apache/kafka/pull/8486#pullrequestreview-423169797",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5215cde4-a616-43ba-ad64-624c78ef673d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "As below, should we log an event here to make sure we will have it in the logs even if the user discards it?",
        "createdAt" : "2020-06-03T02:20:11Z",
        "updatedAt" : "2020-06-05T01:54:11Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "87ca234c395d1d0b0566269c4cd299da936d2cb3",
    "line" : 137,
    "diffHunk" : "@@ -1,1 +843,847 @@                    });\n\n                    if (!truncationWithoutResetPolicy.isEmpty()) {\n                        setFatalOffsetForLeaderException(new LogTruncationException(truncationWithoutResetPolicy));\n                    }"
  },
  {
    "id" : "4f66d062-3dbe-49c2-b95b-0173f730f703",
    "prId" : 8486,
    "prUrl" : "https://github.com/apache/kafka/pull/8486#pullrequestreview-423809318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25f6036d-d178-4787-b626-b740f78ae61c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we fix the LogTruncationException case below as well?",
        "createdAt" : "2020-06-03T18:12:22Z",
        "updatedAt" : "2020-06-05T01:54:11Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "87ca234c395d1d0b0566269c4cd299da936d2cb3",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +829,833 @@                            } catch (OffsetOutOfRangeException e) {\n                                // Catch the exception here to ensure finishing other partitions validation.\n                                setFatalOffsetForLeaderException(e);\n                            }\n                        } else {"
  },
  {
    "id" : "15c3b5f5-c4e8-422b-956c-1b57262c15c3",
    "prId" : 8822,
    "prUrl" : "https://github.com/apache/kafka/pull/8822#pullrequestreview-425840504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dda7c020-a371-4e7d-b3b8-f7d276cc8170",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Why do we move this check out of `setFatalOffsetForLeaderException`?",
        "createdAt" : "2020-06-07T02:40:19Z",
        "updatedAt" : "2020-06-18T05:40:32Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "9dce4733-696b-4d41-8edf-34be0330df2b",
        "parentId" : "dda7c020-a371-4e7d-b3b8-f7d276cc8170",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Seemed inconsistent to have a method named `setFatal` which checks for retriable exceptions.",
        "createdAt" : "2020-06-07T17:56:24Z",
        "updatedAt" : "2020-06-18T05:40:32Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fa803c0899f0e678f52eb7bc4d438e8b33bd70a",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +842,846 @@                    metadata.requestUpdate();\n\n                    if (!(e instanceof RetriableException)) {\n                        maybeSetOffsetForLeaderException(e);\n                    }"
  },
  {
    "id" : "7732074e-afb0-4eab-b9f3-5406db70c0ca",
    "prId" : 8822,
    "prUrl" : "https://github.com/apache/kafka/pull/8822#pullrequestreview-425782489",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4431867-5e5c-4333-b9ed-84c6e64756d9",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Should still mention `no configured reset policy` here IMHO.",
        "createdAt" : "2020-06-07T02:53:34Z",
        "updatedAt" : "2020-06-18T05:40:32Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fa803c0899f0e678f52eb7bc4d438e8b33bd70a",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +1341,1345 @@        } else {\n            log.info(\"{}, raising error to the application since no reset policy is configured\", errorMessage);\n            throw new OffsetOutOfRangeException(errorMessage,\n                Collections.singletonMap(topicPartition, fetchPosition.offset));\n        }"
  },
  {
    "id" : "34b5d813-1cb5-4684-bd53-953a12f3fb37",
    "prId" : 8822,
    "prUrl" : "https://github.com/apache/kafka/pull/8822#pullrequestreview-425843830",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee67bb96-e972-43e5-a6de-9414177195ff",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I feel we could still share `handleOffsetOutOfRange` in two places by letting it return a struct of `Optional<LogTruncation>` and decide when to throw it by the caller.",
        "createdAt" : "2020-06-07T02:55:40Z",
        "updatedAt" : "2020-06-18T05:40:32Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "d34c1f2e-c73b-465d-bc46-b7f0f722dc48",
        "parentId" : "ee67bb96-e972-43e5-a6de-9414177195ff",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "After thinking about it, it seemed simpler to always use LogTruncationException for validation failures, even if the divergent offset is not known. Then direct OffsetOutOfRange errors are reserved for fetch responses which indicate the OFFSET_OUT_OF_RANGE error.",
        "createdAt" : "2020-06-07T18:44:09Z",
        "updatedAt" : "2020-06-18T05:40:32Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fa803c0899f0e678f52eb7bc4d438e8b33bd70a",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +1334,1338 @@    }\n\n    private void handleOffsetOutOfRange(FetchPosition fetchPosition, TopicPartition topicPartition) {\n        String errorMessage = \"Fetch position \" + fetchPosition + \" is out of range for partition \" + topicPartition;\n        if (subscriptions.hasDefaultOffsetResetPolicy()) {"
  },
  {
    "id" : "92820e85-48a6-42f5-b4f4-06682a505e87",
    "prId" : 8841,
    "prUrl" : "https://github.com/apache/kafka/pull/8841#pullrequestreview-432897990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7422391-0270-4d98-9ec8-ffca207195f2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm, if the position is null, we raise out of range?",
        "createdAt" : "2020-06-17T16:29:12Z",
        "updatedAt" : "2020-06-17T19:13:00Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "2cad6957-4680-4e7f-9c2d-36c799378413",
        "parentId" : "e7422391-0270-4d98-9ec8-ffca207195f2",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "What should we do here for null position? This can happen if we get OOOR while we're in the middle of a reset. Maybe we should just log a warning? Or maybe just change the condition to\r\n\r\n```java\r\nif (position == null || fetchOffset != position.offset)\r\n```",
        "createdAt" : "2020-06-17T18:25:08Z",
        "updatedAt" : "2020-06-17T19:13:00Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "bc291153-8f2d-4276-a57f-35b38d012b45",
        "parentId" : "e7422391-0270-4d98-9ec8-ffca207195f2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.",
        "createdAt" : "2020-06-17T19:15:07Z",
        "updatedAt" : "2020-06-17T19:15:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "5ba5a2ea-2ecc-45a7-b595-074a64f7152f",
        "parentId" : "e7422391-0270-4d98-9ec8-ffca207195f2",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I'm inclined to keep the same behavior as we previously did and just warn (not that we expect this case to get hit anymore)",
        "createdAt" : "2020-06-18T02:28:19Z",
        "updatedAt" : "2020-06-18T02:28:20Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "332aeffedcbe435eafab63be4f51e7f4449e9891",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +1297,1301 @@                                \"does not match the current offset {}\", tp, fetchOffset, position);\n                    } else {\n                        handleOffsetOutOfRange(position, tp, \"error response in offset fetch\");\n                    }\n                } else {"
  },
  {
    "id" : "9dc691d2-da6b-4949-bb7f-0d74e7c5adc8",
    "prId" : 9008,
    "prUrl" : "https://github.com/apache/kafka/pull/9008#pullrequestreview-450191488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94fc5e59-b0fe-4a02-befb-da5656554795",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: could probably change this to use `ifPresent`",
        "createdAt" : "2020-07-16T20:27:58Z",
        "updatedAt" : "2020-07-29T17:20:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "507eb047ba0f652f7781d80e35c8d8c262ccb9f5",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +1265,1269 @@                }\n\n                if (partition.preferredReadReplica().isPresent()) {\n                    subscriptions.updatePreferredReadReplica(completedFetch.partition, partition.preferredReadReplica().get(), () -> {\n                        long expireTimeMs = time.milliseconds() + metadata.metadataExpireMs();"
  },
  {
    "id" : "a7694b96-05aa-4cab-8cc5-2ee479372e4a",
    "prId" : 9836,
    "prUrl" : "https://github.com/apache/kafka/pull/9836#pullrequestreview-563008200",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d1cd424-7f3b-4986-8251-d5f5e5c7091d",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Capture the received timestamp.",
        "createdAt" : "2021-01-06T20:58:03Z",
        "updatedAt" : "2021-01-27T20:08:58Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc3ec40f25900d2c25d7c1de1de6271cd0180cd5",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +319,323 @@\n                                    completedFetches.add(new CompletedFetch(partition, partitionData,\n                                            metricAggregator, batches, fetchOffset, responseVersion, resp.receivedTimeMs()));\n                                }\n                            }"
  },
  {
    "id" : "75b2f55d-66b0-4f72-8595-cda40a5cac92",
    "prId" : 9836,
    "prUrl" : "https://github.com/apache/kafka/pull/9836#pullrequestreview-573457870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d50633ea-4e34-4294-ba43-f86ea49ca8a4",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Could it ever happen that this condition failed except mocking tests?",
        "createdAt" : "2021-01-21T01:08:42Z",
        "updatedAt" : "2021-01-27T20:08:58Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "994ef277-33d7-4015-aa52-e81b48d20fa6",
        "parentId" : "d50633ea-4e34-4294-ba43-f86ea49ca8a4",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I copied this check from fetchRecords, which says \"this can happen when a rebalance happened before fetched records are returned to the consumer's poll call\". I.e., it seems like it can actually happen, but a comment is called for, at least. I'll add it.",
        "createdAt" : "2021-01-21T15:38:36Z",
        "updatedAt" : "2021-01-27T20:08:58Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc3ec40f25900d2c25d7c1de1de6271cd0180cd5",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +641,645 @@                    // This can be false when a rebalance happened before fetched records\n                    // are returned to the consumer's poll call\n                    if (subscriptions.isAssigned(partition)) {\n\n                        // initializeCompletedFetch, above, has already persisted the metadata from the fetch in the"
  },
  {
    "id" : "aab51d6d-ed0d-4d09-8cc3-bf19c524c849",
    "prId" : 11057,
    "prUrl" : "https://github.com/apache/kafka/pull/11057#pullrequestreview-713357902",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2999b3f-c935-44a4-816f-6c04674a3afb",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Hm..I wonder if deduplicating like this within the Fetcher itself is too low-level, ie there may be other callers of `sendListOffsetsRequests` that actually do want to issue a new request. I think there are arguments to be made for doing this for all requests, but maybe also some arguments against it -- this is a more drastic change that means APIs like `Consumer#endOffsets` can actually return old/stale results (by up to the configured `request.timeout` at most).\r\n\r\nSince this is a last-minute blocker fix I'd prefer to keep the changes to a minimum and scoped to the specific bug, if at all possible. Can we do the deduplication in another layer, so that we only avoid re-sending the listOffsets request in the specific case of `currentLag`, where we know it's acceptable to report a slightly-out-of-date value because the alternative is to report no value at all? ",
        "createdAt" : "2021-07-23T02:18:39Z",
        "updatedAt" : "2021-07-23T02:18:39Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "dea2825250187e5c59aa95b63908e948bdd78f62",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +943,947 @@        for (Map.Entry<Node, Map<TopicPartition, ListOffsetsPartition>> entry : timestampsToSearchByNode.entrySet()) {\n            // we skip sending the list off request only if there's already one with the exact\n            // requested offsets for the destination node\n            RequestFuture<ListOffsetResult> future = sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);\n            future.addListener(new RequestFutureListener<ListOffsetResult>() {"
  }
]