[
  {
    "id" : "fff0ec4b-5ac9-4fc9-aaa3-0f1c89fd709d",
    "prId" : 5270,
    "prUrl" : "https://github.com/apache/kafka/pull/5270#pullrequestreview-134065084",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9c537b8-09b3-47cc-be27-53bdcc885291",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This will be very spammy if the user has set a large delivery timeout. I don't think the log message has much value to be honest, but perhaps we should at least reduce the level to TRACE?",
        "createdAt" : "2018-07-02T21:43:10Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "c5fc109d-9fdc-4918-83c8-f2ec0cae4e15",
        "parentId" : "a9c537b8-09b3-47cc-be27-53bdcc885291",
        "authorId" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "body" : "`batch.createdMs + deliveryTimeoutMs ` should be positive.  we log an warning here in case of addition overflow. we have defined `deliverTimeoutMs` as `int` when we initialize the producer. Given that, i am not expecting that that log line will be trigged often. \r\n\r\n       private static int configureDeliveryTimeout(ProducerConfig config, Logger log) {\r\n            int deliveryTimeoutMs = config.getInt(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG);\r\n            int lingerMs = config.getInt(ProducerConfig.LINGER_MS_CONFIG);\r\n            int requestTimeoutMs = config.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);\r\n\r\nCurrently I keep RecordAccumulator constructor signature unchanged. If we change the type of 'deliveryTimeoutMs', it is better to also change the type of `lingerMs` and `retryBackoffMs` to be consistent. As changing the types of those paramters will require changes in many test cases to pass `int` instead of `long`,  I'd rather to put the RecordAccumulator constructor signature change in other PR. \r\n\r\n\r\n        public RecordAccumulator(LogContext logContext,\r\n                                 int batchSize,\r\n                                 CompressionType compression,\r\n                                 long lingerMs,\r\n                                 long retryBackoffMs,\r\n                                 long deliveryTimeoutMs,\r\n                                  ...\r\n.",
        "createdAt" : "2018-07-03T00:36:49Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "a962e9bb-bf94-4294-b23c-d279e1e69019",
        "tags" : [
        ]
      },
      {
        "id" : "3dfd5a6a-7cac-4ff5-a5b3-bb76b6879a10",
        "parentId" : "a9c537b8-09b3-47cc-be27-53bdcc885291",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Fair enough. I missed the long coercion. I think the log message has dubious value, but I agree we shouldn't ever actually see it.",
        "createdAt" : "2018-07-03T15:59:59Z",
        "updatedAt" : "2018-07-26T15:53:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9aa6b1706e2e374c20d710567a64b0328fe3119",
    "line" : 134,
    "diffHunk" : "@@ -1,1 +280,284 @@            nextBatchExpiryTimeMs = Math.min(nextBatchExpiryTimeMs, batch.createdMs + deliveryTimeoutMs);\n        } else {\n            log.warn(\"Skipping next batch expiry time update due to addition overflow: \"\n                + \"batch.createMs={}, deliveryTimeoutMs={}\", batch.createdMs, deliveryTimeoutMs);\n        }"
  },
  {
    "id" : "4d31b6e4-2729-4292-90a5-b2289b7320f8",
    "prId" : 5531,
    "prUrl" : "https://github.com/apache/kafka/pull/5531#pullrequestreview-147847001",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a8f1ed5-70b8-4384-93be-58624616215b",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "@rodesai @guozhangwang For my own education: why do we reset to MAX_VALUE?",
        "createdAt" : "2018-08-20T22:30:26Z",
        "updatedAt" : "2018-08-20T22:30:26Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "79b8f704-f0fb-4428-b0eb-1af505994b8b",
        "parentId" : "5a8f1ed5-70b8-4384-93be-58624616215b",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Nevermind. The PR description explains it.",
        "createdAt" : "2018-08-20T22:32:15Z",
        "updatedAt" : "2018-08-20T22:32:15Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f8d86dcdda9ab6ebc55d8c59c2c248da5ad9ac8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +275,279 @@\n    public void resetNextBatchExpiryTime() {\n        nextBatchExpiryTimeMs = Long.MAX_VALUE;\n    }\n"
  },
  {
    "id" : "b805e984-504c-4dc2-92db-7bd316327a8a",
    "prId" : 7672,
    "prUrl" : "https://github.com/apache/kafka/pull/7672#pullrequestreview-315708920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0556755-9b3d-48d4-8553-65ed11e03469",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I don't think it's safe to reuse `nowMs` here since we may have blocked on `free.allocate`.",
        "createdAt" : "2019-11-12T11:53:46Z",
        "updatedAt" : "2019-11-19T19:30:15Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "fb5d32bf-dcc8-40de-a04b-6972790cb519",
        "parentId" : "c0556755-9b3d-48d4-8553-65ed11e03469",
        "authorId" : "98b12f1a-2624-4608-85a1-ec49503fd316",
        "body" : "Done. Common case should be that there's already an in-progress batch above, so shouldn't be a big deal to always call time.milliseconds() here even if there's no memory pressure.",
        "createdAt" : "2019-11-12T17:25:06Z",
        "updatedAt" : "2019-11-19T19:30:15Z",
        "lastEditedBy" : "98b12f1a-2624-4608-85a1-ec49503fd316",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab43e8ecb46522aea51f7880768d722afc0d873d",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +228,232 @@                    throw new KafkaException(\"Producer closed while send in progress\");\n\n                RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);\n                if (appendResult != null) {\n                    // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often..."
  },
  {
    "id" : "ad6b2370-d733-4695-a5ed-62e264d9fbb8",
    "prId" : 10620,
    "prUrl" : "https://github.com/apache/kafka/pull/10620#pullrequestreview-659958496",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01ff1726-10cf-4e76-9847-d92cbdf45c55",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I think I'd mention this bit from your message: `the sender will remove the producer batches from the original incomplete collection`. This explains why we should not hold to any batches.",
        "createdAt" : "2021-05-10T21:43:25Z",
        "updatedAt" : "2021-05-10T21:43:25Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "267f7c26-e8aa-4ffa-97dd-f2c962605107",
        "parentId" : "01ff1726-10cf-4e76-9847-d92cbdf45c55",
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "Done.",
        "createdAt" : "2021-05-14T15:35:35Z",
        "updatedAt" : "2021-05-14T15:35:35Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      }
    ],
    "commit" : "78e84373bd73c2e70209d75a4f09c7239cdbcd28",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +713,717 @@            // Obtain a copy of all of the incomplete ProduceRequestResult(s) at the time of the flush.\n            // We must be careful not to hold a reference to the ProduceBatch(s) so that garbage\n            // collection can occur on the contents.\n            // The sender will remove ProducerBatch(s) from the original incomplete collection.\n            for (ProduceRequestResult result : this.incomplete.requestResults())"
  }
]