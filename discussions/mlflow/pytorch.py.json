[
  {
    "id" : "ae431588-2477-4b41-8d82-f3921896f425",
    "prId" : 842,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/842#pullrequestreview-196303323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5807a5ba-4d06-4212-8378-b2ba7bb8a139",
        "parentId" : null,
        "authorId" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "body" : "pyfunc calls this argument ``code_path``. It's a bit confusing since you can pass in list of files, but maybe we should keep the name consistent across mlflow?",
        "createdAt" : "2019-01-24T23:27:12Z",
        "updatedAt" : "2019-01-25T01:59:34Z",
        "lastEditedBy" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "tags" : [
        ]
      },
      {
        "id" : "b218db2c-33d1-4d7c-bd9f-b9a6f46531e8",
        "parentId" : "5807a5ba-4d06-4212-8378-b2ba7bb8a139",
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "I'd prefer to keep this `code_paths` and eventually rename `code_path` in the `mlflow.pyfunc` API. `mlflow.pyfunc.save_model` also refers to `dst_path`, unlike the `save_model` methods of the remaining flavors that refer to `path`, so it seems to deviate from naming conventions already",
        "createdAt" : "2019-01-24T23:40:58Z",
        "updatedAt" : "2019-01-25T01:59:34Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "8cba81b9b436f8b1f62b95b5df679ab0c7181906",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +45,49 @@\n\ndef log_model(pytorch_model, artifact_path, conda_env=None, code_paths=None, **kwargs):\n    \"\"\"\n    Log a PyTorch model as an MLflow artifact for the current run."
  },
  {
    "id" : "891d00cf-a771-4492-b490-511c7731f4bb",
    "prId" : 842,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/842#pullrequestreview-196318279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a87a22a4-da4b-4c9e-b395-e88564b24074",
        "parentId" : null,
        "authorId" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "body" : "This adds a dependency on pyfunc flavor. I guess it makes sense in this case to add the dependency rather than store the code path data twice. but since we already have the dependency would it make sense to just call _load_pyfunc here and return the model from the wrapper? Should we move the version check from load_model to _load_model?\r\n\r\n\r\n ",
        "createdAt" : "2019-01-24T23:57:13Z",
        "updatedAt" : "2019-01-25T01:59:34Z",
        "lastEditedBy" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "tags" : [
        ]
      },
      {
        "id" : "17988f27-16aa-45ff-babf-906b9dd23477",
        "parentId" : "a87a22a4-da4b-4c9e-b395-e88564b24074",
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "After discussing offline, version validation cannot currently be performed via the `_load_pyfunc` code path because the path to the PyTorch model directory (the **data path**) is passed to `_load_pyfunc`, instead of the model root directory. This prevents us from accessing and parsing the `MLmodel` configuration and should be addressed in the future.",
        "createdAt" : "2019-01-25T00:46:20Z",
        "updatedAt" : "2019-01-25T01:59:34Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "8cba81b9b436f8b1f62b95b5df679ab0c7181906",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +246,250 @@\n    try:\n        pyfunc_conf = _get_flavor_configuration(model_path=path, flavor_name=pyfunc.FLAVOR_NAME)\n    except MlflowException:\n        pyfunc_conf = {}"
  },
  {
    "id" : "9d31a588-ad8d-4ddd-8bb9-b65fea967268",
    "prId" : 748,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/748#pullrequestreview-182888393",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c432443-2c8f-4f17-9c80-05e8708ba4f5",
        "parentId" : null,
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "BTW, it seems like we could also implement dict-handling logic in `pyfunc.add_to_model` (i.e. allow `env` here to be a dictionary or a path to a file), which would allow us to avoid duplicating the logic across different flavor implementations. Seems like we still need to update the docs across the flavors, but it'd make it easier to update the logic later on - let me know what you think.",
        "createdAt" : "2018-12-07T22:06:27Z",
        "updatedAt" : "2018-12-09T04:20:12Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      }
    ],
    "commit" : "f266a37c90d4560624abc71089982f5fd53a01a9",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +176,180 @@\n    mlflow_model.add_flavor(FLAVOR_NAME, model_data=model_file, pytorch_version=torch.__version__)\n    pyfunc.add_to_model(mlflow_model, loader_module=\"mlflow.pytorch\",\n                        data=model_file, env=conda_env_subpath)\n    mlflow_model.save(os.path.join(path, \"MLmodel\"))"
  },
  {
    "id" : "6833f82f-01b1-4003-9bc1-26267cf59f26",
    "prId" : 264,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/264#pullrequestreview-147319016",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a9aa57a-4a1e-4afb-9523-5d6f641a3e40",
        "parentId" : null,
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "Sorry I'm noticing this late, wanted to get your opinion: currently users can only predict on models with a single input tensor, which precludes using MLflow with models that take input tensors of different types (e.g. when predicting on images, maybe we'd have a float tensor containing pixel values & integer values corresponding to image height/width). IMO this restriction is fine as long as we can extend this implementation to support multi-input models in the future.\r\n\r\nMy only concern with the current implementation is that we convert the entire DataFrame to a single tensor & pass that as the sole input to the model, which requires us to make breaking changes down the line if we want to instead pass each column as an input tensor to the model. Maybe we should instead require that the input DataFrame `df` contains a single column of numpy arrays & convert that column to an input tensor (e.g. via `np.hstack(data[data.columns[0]])`?",
        "createdAt" : "2018-08-17T07:40:29Z",
        "updatedAt" : "2018-08-17T20:49:53Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      },
      {
        "id" : "7605e497-d611-4b1b-8045-cffd7e9b1f37",
        "parentId" : "6a9aa57a-4a1e-4afb-9523-5d6f641a3e40",
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "Discussed offline: let's keep this issue in mind but move forward with the current implementation for now :)",
        "createdAt" : "2018-08-17T17:42:19Z",
        "updatedAt" : "2018-08-17T20:49:53Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f9d2edd5eab3f09fae7241e8d16188714e2be3b",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +136,140 @@        self.pytorch_model.eval()\n        with torch.no_grad():\n            input_tensor = torch.from_numpy(data.values.astype(np.float32)).to(device)\n            preds = self.pytorch_model(input_tensor)\n            if not isinstance(preds, torch.Tensor):"
  }
]