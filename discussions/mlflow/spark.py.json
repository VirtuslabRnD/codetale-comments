[
  {
    "id" : "59cf78d5-21e8-45d2-9e40-d804ea7b8701",
    "prId" : 3860,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3860#pullrequestreview-554785290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a50f6c8-8fb8-4898-a602-8930edb04321",
        "parentId" : null,
        "authorId" : "c7dda55d-9e2b-478f-9c63-d6cdec83a883",
        "body" : "@dbczumar \r\n\r\nkeeping with previous style. I left imports relevant to `autolog()` inside of the method as opposed to in the beginning of the file. let me know if `mlflow` standard practice is otherwise.\r\n\r\nAlso I saw that everywhere `SparkContext` is used in this file it is locally imported.",
        "createdAt" : "2020-12-17T16:42:17Z",
        "updatedAt" : "2020-12-17T20:34:50Z",
        "lastEditedBy" : "c7dda55d-9e2b-478f-9c63-d6cdec83a883",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab8610860f0ccae7231185886ab8dd9c8fda10d1",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +692,696 @@    from mlflow._spark_autologging import _listen_for_spark_activity\n    from pyspark.sql import SparkSession\n    from pyspark import SparkContext\n\n    def __init__(original, self, *args, **kwargs):"
  },
  {
    "id" : "194c8682-647d-4f53-ab3a-eabd8819e9fe",
    "prId" : 3844,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3844#pullrequestreview-552185958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb1ebbbe-fe02-4e4d-be12-7915d71a16f0",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Missed this when I approved https://github.com/mlflow/mlflow/pull/3838 via GitHub mobile UI. Lesson learned!\r\n\r\n\r\ncc @mohamad-arabi ",
        "createdAt" : "2020-12-15T06:44:53Z",
        "updatedAt" : "2020-12-15T08:37:32Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      },
      {
        "id" : "6b701698-2e05-4cc6-9020-23ecd49a1b5a",
        "parentId" : "eb1ebbbe-fe02-4e4d-be12-7915d71a16f0",
        "authorId" : "c7dda55d-9e2b-478f-9c63-d6cdec83a883",
        "body" : "Good catch, weird that black didn't run on my PR, or it did but didn't arise due to other linting errors",
        "createdAt" : "2020-12-15T06:55:42Z",
        "updatedAt" : "2020-12-15T08:37:32Z",
        "lastEditedBy" : "c7dda55d-9e2b-478f-9c63-d6cdec83a883",
        "tags" : [
        ]
      },
      {
        "id" : "f5e3c87b-41a0-4a0f-a3c9-f6145d557904",
        "parentId" : "eb1ebbbe-fe02-4e4d-be12-7915d71a16f0",
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Looks like it did: https://github.com/mlflow/mlflow/runs/1553912990. I just didn't catch it while reviewing via a mobile phone web browser :/",
        "createdAt" : "2020-12-15T08:16:12Z",
        "updatedAt" : "2020-12-15T08:37:32Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "040e2fc3f53388eb30e3647f12e756368f1dab68",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +632,636 @@def autolog(disable=False):  # pylint: disable=unused-argument\n    \"\"\"\n    Enables (or disables) and configures logging of Spark datasource paths, versions\n    (if applicable), and formats when they are read. This method is not threadsafe and assumes a\n    `SparkSession"
  },
  {
    "id" : "310b90c0-caec-4114-b717-ce00cf6b5883",
    "prId" : 3443,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3443#pullrequestreview-555745595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9616a13-dc77-4dda-8964-42b82449fb40",
        "parentId" : null,
        "authorId" : "e3833790-b6ba-4cb1-825a-8cefd426ea29",
        "body" : "When does the file get written to tmp_path fuse so we can move it? Are we assuming that the FUSE mount has been synced with HDFS? What if the sync hasn't finished yet?",
        "createdAt" : "2020-12-17T23:00:47Z",
        "updatedAt" : "2021-01-04T17:31:38Z",
        "lastEditedBy" : "e3833790-b6ba-4cb1-825a-8cefd426ea29",
        "tags" : [
        ]
      },
      {
        "id" : "bfa21840-92a2-4948-824e-f656033e230f",
        "parentId" : "e9616a13-dc77-4dda-8964-42b82449fb40",
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "In this save_model method, the goal is to save the SparkML model to a local filesystem path\r\n\r\nThe flow here is:\r\n* Save SparkML model to a temporary dbfs location (`dbfs:/tmp/my-model`)\r\n* Move that persisted Spark model to the local filesystem using FUSE and a file move (move `/dbs/tmp/my-model` to the `/my/desired/local/path`)\r\n\r\nWe leverage FUSE to copy from DBFS -> local as there's no allowlisted JVM-side, HDFS API for copying from dbfs:/... to the local filesystem\r\n",
        "createdAt" : "2020-12-18T19:06:04Z",
        "updatedAt" : "2021-01-04T17:31:38Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      },
      {
        "id" : "a3be2fab-f162-4cba-99e4-ad39db1075fd",
        "parentId" : "e9616a13-dc77-4dda-8964-42b82449fb40",
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "The FUSE access has the same consistency properties as standard HDFS-style access to the same DBFS path. That is, writing to `dbfs:/tmp/my-model` and reading `/dbfs/tmp/my-model` is equivalent from a blob-storage-eventual-consistency perspective to writing to `dbfs:/tmp/my-model` and subsequently reading `dbfs:/tmp/my-model`",
        "createdAt" : "2020-12-18T19:09:11Z",
        "updatedAt" : "2021-01-04T17:31:38Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bbd1fbbd6bdd375e679535f77c034148a2d9608",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +518,522 @@    if copying_from_dbfs:\n        tmp_path_fuse = dbfs_hdfs_uri_to_fuse_path(tmp_path)\n        shutil.move(src=tmp_path_fuse, dst=sparkml_data_path)\n    else:\n        _HadoopFileSystem.copy_to_local_file(tmp_path, sparkml_data_path, remove_src=True)"
  },
  {
    "id" : "9a2c1be2-cac6-41a5-9f04-c328a0fd9a97",
    "prId" : 3443,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3443#pullrequestreview-555046764",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19496a6e-b181-4c61-8ef1-018e759a579b",
        "parentId" : null,
        "authorId" : "e3833790-b6ba-4cb1-825a-8cefd426ea29",
        "body" : "Can we factor out this copytree functionality into a more generic helper function?",
        "createdAt" : "2020-12-17T23:31:03Z",
        "updatedAt" : "2021-01-04T17:31:38Z",
        "lastEditedBy" : "e3833790-b6ba-4cb1-825a-8cefd426ea29",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bbd1fbbd6bdd375e679535f77c034148a2d9608",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +563,567 @@    os.mkdir(fuse_dfs_tmpdir)\n    # Workaround for inability to use shutil.copytree with DBFS FUSE due to permission-denied\n    # errors on passthrough-enabled clusters when attempting to copy permission bits for directories\n    _shutil_copytree_without_file_permissions(src_dir=local_model_path, dst_dir=fuse_dfs_tmpdir)\n    return PipelineModel.load(dfs_tmpdir)"
  },
  {
    "id" : "38bd31ef-3b30-4e4a-912d-23663add67b8",
    "prId" : 3443,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3443#pullrequestreview-561202084",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcd54630-d3dd-4b3c-bea2-d507b1f0c9bf",
        "parentId" : null,
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "Note: should be a posixpath join since HDFS URIs are posixpath style (e.g. we don't want to include backward slashes `\\` here on platforms like Windows)",
        "createdAt" : "2021-01-04T17:22:36Z",
        "updatedAt" : "2021-01-04T17:31:39Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bbd1fbbd6bdd375e679535f77c034148a2d9608",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +212,216 @@    # to persist the model\n    try:\n        spark_model.save(posixpath.join(model_dir, _SPARK_MODEL_PATH_SUB))\n    except Py4JJavaError:\n        return Model.log("
  },
  {
    "id" : "37504571-85a1-4066-a9ef-9bfa6439ab1f",
    "prId" : 3193,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3193#pullrequestreview-457670808",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c876398-1fad-4d25-9307-4a98d0ab34d0",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Can we add an inline comment explaining why we're doing this?",
        "createdAt" : "2020-07-29T15:59:09Z",
        "updatedAt" : "2020-07-29T16:14:02Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      },
      {
        "id" : "4ec61fea-04f2-43e2-a236-a09d926ba493",
        "parentId" : "8c876398-1fad-4d25-9307-4a98d0ab34d0",
        "authorId" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "body" : "Yeah, makes sense!",
        "createdAt" : "2020-07-29T16:04:30Z",
        "updatedAt" : "2020-07-29T16:14:02Z",
        "lastEditedBy" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "tags" : [
        ]
      }
    ],
    "commit" : "83b841d8d31b422219bf05f1f12588e3adfdf77e",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +524,528 @@        spark = pyspark.sql.SparkSession.builder\\\n            .config(\"spark.python.worker.reuse\", True) \\\n            .config(\"spark.databricks.io.cache.enabled\", False) \\\n            .master(\"local[1]\").getOrCreate()\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))"
  },
  {
    "id" : "6d6d25bb-a338-42ea-97d2-39a58061dcb1",
    "prId" : 808,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/808#pullrequestreview-192454286",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "723e2643-5e3a-4263-b9e4-b27e30d0fde3",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "It's worth noting that `_save_model_metadata` may inefficiently log large MLeap model contents that are comparable in size to the PySpark pipeline being logged. Does this PR provide any optimization for the logging of large MLeap-flavored models?",
        "createdAt" : "2019-01-15T01:29:20Z",
        "updatedAt" : "2019-01-16T23:55:29Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff2bdbf09c7ebb81d700c7eddb0ee0d13994898e",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +135,139 @@    with TempDir() as tmp:\n        tmp_model_metadata_dir = tmp.path()\n        _save_model_metadata(\n            tmp_model_metadata_dir, spark_model, mlflow_model, sample_input, conda_env)\n        mlflow.tracking.fluent.log_artifacts(tmp_model_metadata_dir, artifact_path)"
  },
  {
    "id" : "b2146cf4-e9d9-49ca-9723-3466d4bbdfff",
    "prId" : 808,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/808#pullrequestreview-193404687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6beb2755-50eb-41fe-8619-917fe57f20a4",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Could we use the optimized logging process for local artifact URIs as well? This would save users an extra filesystem copy and simplify the logical flow of this method.",
        "createdAt" : "2019-01-16T21:08:24Z",
        "updatedAt" : "2019-01-16T23:55:29Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      },
      {
        "id" : "9cb8299d-4d38-47b9-9080-fe35f8390d89",
        "parentId" : "6beb2755-50eb-41fe-8619-917fe57f20a4",
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "Discussed offline, will add a comment explaining why we use Model.log() for local artifact URIs",
        "createdAt" : "2019-01-16T23:38:39Z",
        "updatedAt" : "2019-01-16T23:55:29Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff2bdbf09c7ebb81d700c7eddb0ee0d13994898e",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +117,121 @@    # be incorrect on multi-node clusters - to avoid such issues we just use the Model.log() path\n    # here.\n    if mlflow.tracking.utils._is_local_uri(run_root_artifact_uri):\n        return Model.log(artifact_path=artifact_path, flavor=mlflow.spark, spark_model=spark_model,\n                         jars=jars, conda_env=conda_env, dfs_tmpdir=dfs_tmpdir,"
  },
  {
    "id" : "42ed9f1d-ac1d-4e1c-a104-32f6a06413d4",
    "prId" : 402,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/402#pullrequestreview-150655455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7511b069-8195-4983-85bc-123da70b23d7",
        "parentId" : null,
        "authorId" : "6e958077-8d60-494d-8079-e77af3a2951f",
        "body" : "doc warning",
        "createdAt" : "2018-08-29T16:46:42Z",
        "updatedAt" : "2018-08-30T19:28:49Z",
        "lastEditedBy" : "6e958077-8d60-494d-8079-e77af3a2951f",
        "tags" : [
        ]
      }
    ],
    "commit" : "150bfb4198cd39a2e87af813fab80c630ac7feff",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2,6 @@MLflow integration for Spark MLlib models.\nThis module enables the exporting of Spark MLlib models with the following flavors (formats):\n\n    1. Spark MLlib (native) format - Allows models to be loaded as Spark Transformers for scoring\n                                     in a Spark session. Models with this flavor can be loaded"
  },
  {
    "id" : "2e3708f3-f5ad-4151-979f-ac8900daf59c",
    "prId" : 324,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/324#pullrequestreview-147752852",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f23a9efe-1e2b-4179-af60-27fa8afe12b8",
        "parentId" : null,
        "authorId" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "body" : "I would also explicitly mention that this format lets you read the same model back. Also make it eplxicit that this is the main format which is always produced.\r\n",
        "createdAt" : "2018-08-20T17:01:30Z",
        "updatedAt" : "2018-08-28T05:59:34Z",
        "lastEditedBy" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "tags" : [
        ]
      },
      {
        "id" : "d7407e28-ae22-4b15-92cc-1912724312f2",
        "parentId" : "f23a9efe-1e2b-4179-af60-27fa8afe12b8",
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Done!",
        "createdAt" : "2018-08-20T17:51:01Z",
        "updatedAt" : "2018-08-28T05:59:34Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b3ecb5ee44cc7b206365effedd11fc8508eb457",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2,6 @@MLflow integration for Spark MLlib models.\nThis module enables the exporting of Spark MLlib models with the following flavors (formats):\n    1. Spark MLlib (native) format - Allows models to be loaded as Spark Transformers for scoring\n                                     in a Spark session. Models with this flavor can be loaded\n                                     back as PySpark PipelineModel objects in Python. This"
  }
]