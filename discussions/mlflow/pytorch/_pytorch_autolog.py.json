[
  {
    "id" : "d996035a-97a1-4d12-af7c-249218c582a6",
    "prId" : 4398,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/4398#pullrequestreview-669778525",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65ad1283-fe8c-450b-92ba-7367e78443c2",
        "parentId" : null,
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "The current dev version of `pytorch-lightning` is `1.4.0dev`. To enable the fix in this version, use `1.4.0dev` instead of `1.4.0`.",
        "createdAt" : "2021-05-27T06:17:05Z",
        "updatedAt" : "2021-05-27T06:17:05Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b7ec8131215bd366c1356bfb0f82295764764e1",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +115,119 @@            # current training epoch when `on_train_epoch_end` is called:\n            # https://github.com/PyTorchLightning/pytorch-lightning/pull/7357\n            if _pl_version >= Version(\"1.4.0dev\"):\n\n                def on_train_epoch_end("
  },
  {
    "id" : "2e0ac843-12a5-4564-95e6-7e166868d167",
    "prId" : 4398,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/4398#pullrequestreview-669778947",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30a10aa1-2bc7-42dc-8919-60ead1df81e3",
        "parentId" : null,
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Assigned `Version(pl.__version__)` into a variable to avoid parsing multiple times (not a big issue though)",
        "createdAt" : "2021-05-27T06:17:49Z",
        "updatedAt" : "2021-05-27T06:17:54Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b7ec8131215bd366c1356bfb0f82295764764e1",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +109,113 @@                        self._early_stop_check(callback)\n\n            _pl_version = Version(pl.__version__)\n\n            # In pytorch-lightning >= 1.4.0, validation is run inside the training epoch and"
  },
  {
    "id" : "d962216f-70d3-43d4-9bdb-5b5ea9cb752b",
    "prId" : 3901,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3901#pullrequestreview-557868689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3162ffe-4ea5-4cc7-a1ff-4a12465fa214",
        "parentId" : null,
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Introduced this function to log the original optimizer name regardless of pytorch-lightning version.",
        "createdAt" : "2020-12-23T13:30:43Z",
        "updatedAt" : "2020-12-23T13:35:00Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f9e7b7f7033464003ed385b37d0243b89a721a",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +53,57 @@            if isinstance(optimizer, LightningOptimizer)\n            else optimizer.__class__.__name__\n        )\n\n"
  },
  {
    "id" : "e8b52eed-4d9e-435e-a563-afc4ac6b9709",
    "prId" : 3723,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3723#pullrequestreview-542502127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "426ebb7a-7bde-464e-a7b4-a495e78b3efd",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Should we use `record_metrics` here? (Probably not too significant since this is presumably called at the end of training)",
        "createdAt" : "2020-12-02T04:14:40Z",
        "updatedAt" : "2020-12-03T08:37:47Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      },
      {
        "id" : "03cabe3c-d710-445a-aefb-0d14f62eca6c",
        "parentId" : "426ebb7a-7bde-464e-a7b4-a495e78b3efd",
        "authorId" : "c7dda55d-9e2b-478f-9c63-d6cdec83a883",
        "body" : "No, because after extensive  reading of pytorch docs, from my understanding: testing stage which invokes `on_test_end ` is not part of the training stage whether it be `fit` or `train`. Therefore the batch metrics logger context would have been torn down, and recording metrics using it would not make sense. I'm sure you would know better, if I can guarantee that `on_test_end()` will be called before the context is teared down then definitely we should use `record_metrics()` instead of `log_metric `. ",
        "createdAt" : "2020-12-02T05:06:56Z",
        "updatedAt" : "2020-12-03T08:37:47Z",
        "lastEditedBy" : "c7dda55d-9e2b-478f-9c63-d6cdec83a883",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffc8b8e2ec950e7532a387cc7f49dc05851bc6c3",
    "line" : 253,
    "diffHunk" : "@@ -1,1 +147,151 @@                try_mlflow_log(mlflow.set_tag, \"Mode\", \"testing\")\n                for key, value in trainer.callback_metrics.items():\n                    try_mlflow_log(mlflow.log_metric, key, float(value))\n\n            @staticmethod"
  },
  {
    "id" : "c019002e-4ed0-4415-8526-5d23047cf20a",
    "prId" : 3638,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3638#pullrequestreview-523321884",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a86fafac-9d46-4386-9485-b720764c4ce8",
        "parentId" : null,
        "authorId" : "005113e6-923f-4238-88d9-8c097ed155e1",
        "body" : "@shrinath-suresh Please add some detailed comments for the logic we are putting to only log for the 1st optimizer, so that we can evaluate in future if the behaviour needs to be changed based on user feedback",
        "createdAt" : "2020-11-04T12:39:13Z",
        "updatedAt" : "2020-11-06T14:27:27Z",
        "lastEditedBy" : "005113e6-923f-4238-88d9-8c097ed155e1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3834d16d4eeffccc543c28a0e0df1a7c40d4ce",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +100,104 @@                try_mlflow_log(mlflow.log_param, \"optimizer_name\", type(optimizer).__name__)\n\n                if hasattr(optimizer, \"defaults\"):\n                    try_mlflow_log(mlflow.log_params, optimizer.defaults)\n"
  },
  {
    "id" : "cb36f78b-61f7-4371-817d-5dda0542ef30",
    "prId" : 3638,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3638#pullrequestreview-523338738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "502be27e-d13e-4765-a664-20798927a188",
        "parentId" : null,
        "authorId" : "5ea9649b-c37c-4aab-88b3-7bfcd507163e",
        "body" : "Currently, all the default parameters in the optimizer are logged. \r\n\r\nWe need to revisit the following scenarios\r\n\r\n1. Matching fixed keys as below, restricts the number of parameters. For ex: if user chooses a different optimizer and the optimizer dictionary contains extra keys (apart from `learning_rate`, `betas`, `weight_decay`, `epsilon`) such parameters would be missed to log.\r\n\r\n``` \r\nif \"lr\" in optmizer.defaults:\r\n        try_mlflow_log(mlflow.log_param, \"learning_rate\", optimzer.defaults[\"lr\"])\r\n```\r\n\r\nHence the entire default dictionary parameters are stored into mlflow (however , the names are too short `lr` - `learning_rate`)\r\n\r\n2. Handling multiple optimizers - As per the current approach only the first optimizer parameters are logged. In case of multiple optimizers, we might need to iterate and find an effective way of logging the parameters (and differentiating the parameters among the optimizers)",
        "createdAt" : "2020-11-04T13:03:23Z",
        "updatedAt" : "2020-11-06T14:27:27Z",
        "lastEditedBy" : "5ea9649b-c37c-4aab-88b3-7bfcd507163e",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3834d16d4eeffccc543c28a0e0df1a7c40d4ce",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +101,105 @@\n                if hasattr(optimizer, \"defaults\"):\n                    try_mlflow_log(mlflow.log_params, optimizer.defaults)\n\n            summary = str(ModelSummary(pl_module, mode=\"full\"))"
  },
  {
    "id" : "f382a52f-08d0-4b3f-922c-1936bc88c785",
    "prId" : 3601,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3601#pullrequestreview-521696109",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca6ce786-3410-412c-86b1-201c7111ad90",
        "parentId" : null,
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Should we log the checkpoint in the `\"model\"` directory?\r\n\r\n```\r\n- model\r\n  - restored_model_checkpoint\r\n    - ...\r\n```",
        "createdAt" : "2020-11-02T04:39:28Z",
        "updatedAt" : "2020-11-03T03:56:58Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      },
      {
        "id" : "2ad720ad-e05c-456a-811b-c28033a8cbe4",
        "parentId" : "ca6ce786-3410-412c-86b1-201c7111ad90",
        "authorId" : "5ea9649b-c37c-4aab-88b3-7bfcd507163e",
        "body" : "checkpoints are saved as `.ckpt`. I guess it would be confusing to have both `pytorch` and `pytorch_lightning` model under same folder `model`. ",
        "createdAt" : "2020-11-02T11:15:30Z",
        "updatedAt" : "2020-11-03T03:56:58Z",
        "lastEditedBy" : "5ea9649b-c37c-4aab-88b3-7bfcd507163e",
        "tags" : [
        ]
      },
      {
        "id" : "3f65849e-e4fb-4e09-a779-dfba631b9a7b",
        "parentId" : "ca6ce786-3410-412c-86b1-201c7111ad90",
        "authorId" : "005113e6-923f-4238-88d9-8c097ed155e1",
        "body" : "@harupy This will get confusing for user's. The Checkpoints reside in their own separate folder",
        "createdAt" : "2020-11-02T13:34:03Z",
        "updatedAt" : "2020-11-03T03:56:58Z",
        "lastEditedBy" : "005113e6-923f-4238-88d9-8c097ed155e1",
        "tags" : [
        ]
      },
      {
        "id" : "79855950-ecbe-40ed-84aa-f55ef6e30a9c",
        "parentId" : "ca6ce786-3410-412c-86b1-201c7111ad90",
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Makes sense. Let's just leave this as it is.",
        "createdAt" : "2020-11-02T14:49:19Z",
        "updatedAt" : "2020-11-03T03:56:58Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a0d0785d3d5162e9596922749f606ccd523a724",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +146,150 @@                    local_path=trainer.checkpoint_callback.best_model_path,\n                    artifact_path=\"restored_model_checkpoint\",\n                )\n\n        def on_test_end(self, trainer, pl_module):"
  },
  {
    "id" : "0817299b-4e3d-4479-aa7b-1d95f7e34967",
    "prId" : 3601,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3601#pullrequestreview-521721399",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8aca7716-c85f-4a4c-bbfc-e505398f4c04",
        "parentId" : null,
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "I found the tensorflow does the same thing (defining a constant and use it with `global` in the autolog function). @smurching Do you happen to know why we need to do this?\r\n\r\nhttps://github.com/mlflow/mlflow/blob/master/mlflow/tensorflow.py#L50",
        "createdAt" : "2020-11-02T15:14:30Z",
        "updatedAt" : "2020-11-03T03:56:58Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      },
      {
        "id" : "e94f946e-578e-4107-be17-5afcf7b356b4",
        "parentId" : "8aca7716-c85f-4a4c-bbfc-e505398f4c04",
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "ah I got it. `_LOG_EVERY_N_STEPS` is used in `_log_event`.\r\n\r\nhttps://github.com/mlflow/mlflow/blob/master/mlflow/tensorflow.py#L601",
        "createdAt" : "2020-11-02T15:15:58Z",
        "updatedAt" : "2020-11-03T03:56:58Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a0d0785d3d5162e9596922749f606ccd523a724",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +13,17 @@logging.basicConfig(level=logging.ERROR)\n\nevery_n_epoch = 1\n\n"
  },
  {
    "id" : "89335223-51fe-41ff-9eab-3eb949b5fe73",
    "prId" : 3601,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3601#pullrequestreview-522123083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ca0d613-68bc-4e57-8a5b-aad01ad7319f",
        "parentId" : null,
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "@shrinath-suresh could we remove this comment in favor of filing an issue against PytorchLightning & linking to the filed issue? I worry that we may lose track of these details / not take action otherwise :) ",
        "createdAt" : "2020-11-03T00:21:57Z",
        "updatedAt" : "2020-11-03T03:56:58Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a0d0785d3d5162e9596922749f606ccd523a724",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +25,29 @@# In case of MlflowLogger, Run management is completely controlled by the class and\n# hence mlflow object needs to be reinstantiated by setting\n# tracking uri, experiment_id and run_id which may lead to a race condition.\n# TODO: Replace __MlflowPLCallback with Pytorch Lightning's built-in MlflowLogger\n# once the above mentioned issues have been addressed"
  },
  {
    "id" : "ddd609b5-70cf-41f7-bc4b-715b97fcc7dd",
    "prId" : 3601,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3601#pullrequestreview-522180629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e2d9ec0-cd4c-47dd-af7b-ab75833093d5",
        "parentId" : null,
        "authorId" : "5ea9649b-c37c-4aab-88b3-7bfcd507163e",
        "body" : "used the doc string from [fastai autolog](https://github.com/mlflow/mlflow/blob/89fb38ec122a1c489488ab1fb0d35fe70c2be649/mlflow/fastai.py#L282) method with minor changes.",
        "createdAt" : "2020-11-03T04:14:22Z",
        "updatedAt" : "2020-11-03T04:14:22Z",
        "lastEditedBy" : "5ea9649b-c37c-4aab-88b3-7bfcd507163e",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a0d0785d3d5162e9596922749f606ccd523a724",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +34,38 @@def _autolog(log_every_n_epoch=1):\n    \"\"\"\n    Enable automatic logging from pytorch to MLflow.\n    Logs loss and any other metrics specified in the fit\n    function, and optimizer data as parameters. Model checkpoints"
  },
  {
    "id" : "09b26961-df9a-4abf-aef8-5262d765edbf",
    "prId" : 3601,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3601#pullrequestreview-533012131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef092443-c546-4287-bc17-9e77e30740f4",
        "parentId" : null,
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Hi @shrinath-suresh,\r\n\r\nWhen training is early stopped, does `trainer` restore `model`'s weights to the best values?\r\n\r\ncc @SeanNaren ",
        "createdAt" : "2020-11-17T06:21:14Z",
        "updatedAt" : "2020-11-17T06:47:14Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      },
      {
        "id" : "9b9f9b07-232a-4ba9-b345-6f6983567489",
        "parentId" : "ef092443-c546-4287-bc17-9e77e30740f4",
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Looks like it doesn't?\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/4018237c309b7d9d6978da73132003615341e04a/pytorch_lightning/trainer/training_loop.py#L591",
        "createdAt" : "2020-11-17T06:52:25Z",
        "updatedAt" : "2020-11-17T07:04:27Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      },
      {
        "id" : "7af526be-7724-45d2-af7b-c4adf0b38f68",
        "parentId" : "ef092443-c546-4287-bc17-9e77e30740f4",
        "authorId" : "cef5079b-ed96-43c3-b090-283017e0249e",
        "body" : "Not entirely sure what the question is, when does the trainer restore the model's weights?",
        "createdAt" : "2020-11-17T16:55:47Z",
        "updatedAt" : "2020-11-17T16:55:47Z",
        "lastEditedBy" : "cef5079b-ed96-43c3-b090-283017e0249e",
        "tags" : [
        ]
      },
      {
        "id" : "ee802fd2-8207-41b8-be5c-bb7bfda6b1b9",
        "parentId" : "ef092443-c546-4287-bc17-9e77e30740f4",
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "@SeanNaren Thanks for the reply. Let's say you train your model using early stopping with `patience=3` and training early stops at epoch 6.\r\n\r\n```md\r\nEarly Stopping patience: 3\r\n\r\n| Epoch | Validation loss | comment                |\r\n| :---- | :-------------- | ---------------------- |\r\n| 1     | 9               |                        |\r\n| 2     | 8               |                        |\r\n| 3     | 7               |                        | <- best\r\n| 4     | 7.1             | loss starts increasing |\r\n| 5     | 7.2             |                        |\r\n| 6     | 7.3             | training early-stops   | <- stopped\r\n| 7     | -               |                        |\r\n| 8     | -               |                        |\r\n| 9     | -               |                        |\r\n```\r\n\r\nIn this case, does `trainer` restore the weights at the best epoch or not?",
        "createdAt" : "2020-11-18T01:59:21Z",
        "updatedAt" : "2020-11-18T02:10:06Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      },
      {
        "id" : "8aa6ba97-2b7b-49f8-91ad-a24ef988b7f1",
        "parentId" : "ef092443-c546-4287-bc17-9e77e30740f4",
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Keras' early-stopping has this feature (`restore_best_weights`):\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping",
        "createdAt" : "2020-11-18T02:11:31Z",
        "updatedAt" : "2020-11-18T02:20:58Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      },
      {
        "id" : "9455cac2-27da-4570-847f-4696c48d18ff",
        "parentId" : "ef092443-c546-4287-bc17-9e77e30740f4",
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Found a related discussion: https://github.com/PyTorchLightning/pytorch-lightning/issues/1395",
        "createdAt" : "2020-11-18T02:31:13Z",
        "updatedAt" : "2020-11-18T02:31:13Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a0d0785d3d5162e9596922749f606ccd523a724",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +146,150 @@                    local_path=trainer.checkpoint_callback.best_model_path,\n                    artifact_path=\"restored_model_checkpoint\",\n                )\n\n        def on_test_end(self, trainer, pl_module):"
  }
]