[
  {
    "id" : "4821bc25-2106-4b0e-84bc-fc1e49701c63",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-691408591",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b352f87f-e27f-4884-a053-961db9430bdb",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Waiting for each event to be consumed reduces throughput. Could we handle the expected metadata load with this?",
        "createdAt" : "2021-05-26T21:55:54Z",
        "updatedAt" : "2021-05-28T01:23:55Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "a360ee53-ae32-41b2-bd90-d594fd39b43f",
        "parentId" : "b352f87f-e27f-4884-a053-961db9430bdb",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "This method is invoked from multiple threads for different topic partitions. This will not be a bottleneck as each partition's segments will be uploaded in a sequential manner.",
        "createdAt" : "2021-05-30T15:53:40Z",
        "updatedAt" : "2021-05-30T15:53:40Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      },
      {
        "id" : "002386d0-a4c9-486d-805b-e1cea61af077",
        "parentId" : "b352f87f-e27f-4884-a053-961db9430bdb",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "I agree with the question here. This can become more expensive than it seems. The alternative is to pursue an asynchronous notification model to improve the throughput.",
        "createdAt" : "2021-06-02T08:05:53Z",
        "updatedAt" : "2021-06-02T08:15:16Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "cdc4cfbe-0500-4ed3-8aaa-148c64d17538",
        "parentId" : "b352f87f-e27f-4884-a053-961db9430bdb",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "As we discussed offlime, this may not become a bottleneck but we will make respective RLMM APIs asynchronous so that the APIs are extensible and implementors can provide async behavior.FIled [KAFKA-12988](https://issues.apache.org/jira/browse/KAFKA-12988) to track this issue.  ",
        "createdAt" : "2021-06-24T07:10:29Z",
        "updatedAt" : "2021-06-24T07:10:30Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +160,164 @@            // Wait until the consumer catches up with this offset. This will ensure read-after-write consistency\n            // semantics.\n            consumerManager.waitTillConsumptionCatchesUp(recordMetadata);\n        } catch (KafkaException e) {\n            if (e instanceof RetriableException) {"
  },
  {
    "id" : "fbe5ced9-35a0-41dc-9f42-412e485f0ba7",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-691396477",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fffdc907-e1ca-4b0e-86f2-336ff4bfc856",
        "parentId" : null,
        "authorId" : "1e1e52a7-6513-46fd-99c3-3d01eee59d17",
        "body" : "function names `addRemoteLogSegmentMetadata`, `updateRemoteLogSegmentMetadata`, and `putRemotePartitionDeleteMetadata` don't seem very consistent. Is there a way to improve it?",
        "createdAt" : "2021-06-23T05:03:18Z",
        "updatedAt" : "2021-06-23T05:16:21Z",
        "lastEditedBy" : "1e1e52a7-6513-46fd-99c3-3d01eee59d17",
        "tags" : [
        ]
      },
      {
        "id" : "b7f87842-dd8c-4d52-8520-1d2ac47040f4",
        "parentId" : "fffdc907-e1ca-4b0e-86f2-336ff4bfc856",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "Javadocs explain the behaviorr in detail.\r\n`addRemoteLogSegmentMetadata` - adds a new entry. \r\n`updateRemoteLogSegmentMetadata ` - updates an existing entry. \r\n`putRemotePartitionDeleteMetadata ` - adds or updates an existing entry, put is generally used for that purpose. If this is not so clear, another option can be `addOrUpdateRemotePartitionDeleteMetadata`.\r\n",
        "createdAt" : "2021-06-24T06:53:57Z",
        "updatedAt" : "2021-06-25T03:12:56Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 139,
    "diffHunk" : "@@ -1,1 +137,141 @@\n    @Override\n    public void putRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)\n            throws RemoteStorageException {\n        Objects.requireNonNull(remotePartitionDeleteMetadata, \"remotePartitionDeleteMetadata can not be null\");"
  },
  {
    "id" : "ff2b4170-0748-4175-a936-b40cfeb208c3",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-699033296",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e7d2c7c-94c7-4ff7-9dec-667064d31518",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Since we have a lock, could we just make closing a boolean?",
        "createdAt" : "2021-06-29T22:22:17Z",
        "updatedAt" : "2021-06-30T18:10:06Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "a4b38897-3603-4acc-98b0-14ad7180b810",
        "parentId" : "8e7d2c7c-94c7-4ff7-9dec-667064d31518",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "`closing` is accessed in `initializeResources` and we do not need to take a lock there.  I would like to keep this as `AtomicBoolean` which addresses that and it is easy to understand the semantics. ",
        "createdAt" : "2021-07-05T10:26:46Z",
        "updatedAt" : "2021-07-05T10:26:47Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +68,72 @@    // Using AtomicBoolean instead of volatile as it may encounter http://findbugs.sourceforge.net/bugDescriptions.html#SP_SPIN_ON_FIELD\n    // if the field is read but not updated in a spin loop like in #initializeResources() method.\n    private final AtomicBoolean closing = new AtomicBoolean(false);\n    private final AtomicBoolean initialized = new AtomicBoolean(false);\n    private final Time time = Time.SYSTEM;"
  },
  {
    "id" : "fc4cef8c-5676-45b0-874a-ed8060b4e140",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-703550721",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfe65d97-655e-4d7d-96c3-8065d5c6743f",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "> It was required to be retried until the topic is successfully created. I added the logic to check for topic creation too.\r\n\r\nI am still not sure why we need to initialize in a separate thread. If we can't create the metadata topic or instantiate the producer/consumer due to wrong configurations, we want to fail fast by throwing an error to shut down the broker.",
        "createdAt" : "2021-07-06T22:00:41Z",
        "updatedAt" : "2021-07-06T22:33:01Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "7d5175e8-0d59-4607-b602-18cec26ea72e",
        "parentId" : "dfe65d97-655e-4d7d-96c3-8065d5c6743f",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "@junrao The reason why we need to initialize in a different thread here is that RLMM will be created and `configure()` will be called before the broker starts accepting the requests. So, we can not call topic creation requests in the same thread as the brokers are not yet up. \r\n\r\nAnother way to do this is to have this topic as an internal topic and it will be auto created whenever it is accessed. afaik, creating producer and consumer instances can be done without the brokers up and running and they will not trigger a request to auto creation of remote log metadata topic. Consumer assignment will send a metadata request which will trigger auto creation of topics. This assignment on consumer is done only when RLMM receives callback thorough the broker about leader and isr updates from the controller. This is what we had in pre-2.7 implementation but we saw an intermittent deadlock issue but I do not see that happening on trunk.  \r\n\r\nIn the current PR, I will make the existing producer manager and consumer manager in configure() and have the topic creation done in the tests. \r\n\r\nI will have a quick follow-up PR with internal topic changes and remove the topic creation code from tests. \r\n",
        "createdAt" : "2021-07-07T10:50:18Z",
        "updatedAt" : "2021-07-07T15:11:55Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      },
      {
        "id" : "e62cce83-3e89-42e7-9c01-afc83afc910d",
        "parentId" : "dfe65d97-655e-4d7d-96c3-8065d5c6743f",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "I made the mentioned changes for this PR in the latest commit. ",
        "createdAt" : "2021-07-07T16:29:41Z",
        "updatedAt" : "2021-07-08T04:43:14Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      },
      {
        "id" : "d0fa3cf1-b410-456f-a8fc-505281be7943",
        "parentId" : "dfe65d97-655e-4d7d-96c3-8065d5c6743f",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@satishd : To create a topic, you don't need a particular broker to be up. You just need to be able to access the bootstrap brokers. Auto creating this topic on access is a bit weird since it's not truly an internal topic and it is just an implementation detail of RLMM. So, it makes more sense for topic-based RLMM to create it.",
        "createdAt" : "2021-07-08T17:03:35Z",
        "updatedAt" : "2021-07-08T17:03:35Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "33b5741a-bcd4-49ec-aac9-ba8c841c54f1",
        "parentId" : "dfe65d97-655e-4d7d-96c3-8065d5c6743f",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "@junrao \r\nAs I mentioned earlier, RLMM is created before broker starts accepting the requests. So, when RLMM is getting initialized and tries to create a topic in the same thread, then none of the brokers(including the brokers related to the bootstrap-servers config) will be available for taking any admin client requests for topic creation. That is why I was doing this in a different thread as it allows the broker/controller to comeup and accept the admin client request for topic creation.\r\n",
        "createdAt" : "2021-07-09T06:15:00Z",
        "updatedAt" : "2021-07-09T06:15:00Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      },
      {
        "id" : "c5f34fa4-2754-4faf-9692-33bed14ce1a5",
        "parentId" : "dfe65d97-655e-4d7d-96c3-8065d5c6743f",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@satishd : My understanding is that when we enable remote storage, we will do that through a rolling upgrade. So, at any given time, there is at most a single broker being down. Therefore, as long as the bootstrap broker list contains more than one broker, operations like creating topics can still be done while a single broker is starting.",
        "createdAt" : "2021-07-09T17:21:12Z",
        "updatedAt" : "2021-07-09T17:21:12Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "7cc14b93-de41-427d-9dde-085e5fe82dfe",
        "parentId" : "dfe65d97-655e-4d7d-96c3-8065d5c6743f",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "@junrao: That is a good point. But that is valid only for upgrade scenario.\r\n\r\nThere are two scenarios here.\r\n1) Upgrade path\r\n2) Fresh install/deploy with the release containing this feature.\r\n\r\nWhat you said makes sense for upgrade path. But \"bootstrap.servers\" list is configured with the local broker endpoint only in which RLMM is getting initialized. So, if we try to initialize RLMM in the same thread, it wont be able to connect to the local broker as it is not yet started to accept the broker API requests. One way to address this is to give `bootstrap.servers` with more than one broker's endpoint. This will also put a limitation to create a cluster with one broker instance for test/demo environments. \r\n\r\nWhen users install/deploy a fresh environment with the release containing this feature, there should not be a restriction to do rolling restarts with the configuration enabled. Please let me know if I am missing anything here.",
        "createdAt" : "2021-07-11T06:04:21Z",
        "updatedAt" : "2021-07-12T00:34:28Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 313,
    "diffHunk" : "@@ -1,1 +311,315 @@            // not yet be available now. This thread makes sure that it is retried at regular intervals until it is\n            // successful.\n            initializationThread = KafkaThread.nonDaemon(\"RLMMInitializationThread\", () -> initializeResources());\n            initializationThread.start();\n        } finally {"
  },
  {
    "id" : "28bdebb5-f915-45a3-a4cd-30eca64f0280",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-700367031",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "001e4233-0aeb-4f32-9775-83db0456d78e",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should we verify that the number of partitions in the existing topic matches the configuration?",
        "createdAt" : "2021-07-06T22:06:20Z",
        "updatedAt" : "2021-07-06T22:33:01Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 436,
    "diffHunk" : "@@ -1,1 +434,438 @@            if (e.getCause() instanceof TopicExistsException) {\n                log.info(\"Topic [{}] already exists\", topic.name());\n                topicCreated = true;\n            } else {\n                log.error(\"Encountered error while creating remote log metadata topic.\", e);"
  },
  {
    "id" : "9e835873-49f3-410e-aaa3-97fcabf55c7f",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-707985062",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "796b3cc6-ac7f-47c1-ab34-565e76e21fc7",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should halt the jvm in this case? Ditto below when the partition count doesn't match.",
        "createdAt" : "2021-07-15T20:58:17Z",
        "updatedAt" : "2021-07-15T21:49:42Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "265d3685-440b-4c4f-8ae9-9772ef27eef2",
        "parentId" : "796b3cc6-ac7f-47c1-ab34-565e76e21fc7",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "Yes, we are already doing that [here](https://github.com/apache/kafka/pull/10579/files#diff-50bf08ff4e92c16dfbea9239c89467a3ebf5fa38e5f4ea44ed4acff80013136eR452) whenever this class is accessed with/for remote log metadata operations.",
        "createdAt" : "2021-07-16T03:59:51Z",
        "updatedAt" : "2021-07-16T03:59:51Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 336,
    "diffHunk" : "@@ -1,1 +334,338 @@                    log.error(\"Timed out in initializing the resources, retried to initialize the resource for [{}] ms.\",\n                              rlmmConfig.initializationRetryMaxTimeoutMs());\n                    initializationFailed = true;\n                    return;\n                }"
  },
  {
    "id" : "e47424b3-ef6b-4a3d-9f26-f43150fbd0f2",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-708169798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46ff96f1-2ca3-4628-96c2-02e3874b6699",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Does pendingAssignPartitions need to be synchronizedSet since it's accessed under a lock?",
        "createdAt" : "2021-07-15T21:40:10Z",
        "updatedAt" : "2021-07-15T21:49:42Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "edb1b2f6-4f0d-456f-b24d-0c7a39aa0abf",
        "parentId" : "46ff96f1-2ca3-4628-96c2-02e3874b6699",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "It is needed because `pendingAssignPartitions` is updated in both `onPartitionLeadershipChanges` and `onStopPartitions` methods which can happen concurrently as both of them take read lock.",
        "createdAt" : "2021-07-16T09:13:18Z",
        "updatedAt" : "2021-07-16T09:13:19Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +83,87 @@    private volatile TopicBasedRemoteLogMetadataManagerConfig rlmmConfig;\n    private volatile RemoteLogMetadataTopicPartitioner rlmmTopicPartitioner;\n    private final Set<TopicIdPartition> pendingAssignPartitions = Collections.synchronizedSet(new HashSet<>());\n    private volatile boolean initializationFailed;\n"
  },
  {
    "id" : "7ac52ad0-317e-4bab-8f99-4c69e083c839",
    "prId" : 10579,
    "prUrl" : "https://github.com/apache/kafka/pull/10579#pullrequestreview-708606163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d1c14b0-f5e8-4f59-b03f-b78ad3d55aff",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Since producerManager is initialized asynchronously, how do we deal with the case when the producerManager is not ready when an event needs to be published?",
        "createdAt" : "2021-07-16T16:27:07Z",
        "updatedAt" : "2021-07-16T16:30:29Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0ca43e37-f567-4286-bcbf-c8acc7a6bdbd",
        "parentId" : "2d1c14b0-f5e8-4f59-b03f-b78ad3d55aff",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "That is a good point. We plan to improve the semantics here. \r\n\r\nEarlier, we plan to introduce RetriableException for RLMM and RSM so that callers can have an option to know whether they can retry or not. In the case of initialization is not yet complete, RetriableException can be thrown caller can retry based on backoff. RLMM can send non retriable exception if it is in closing state and there will not be any retries.  \r\n\r\nAnother way to handle this is to take these events and store them in in-memory queue and return Future. These Futures will be considered successful if initialization is successful and the events are published to the topic.\r\n\r\nI plan to address these in a followup PR while these APIs are integrated with RLM, filed https://issues.apache.org/jira/browse/KAFKA-13097. ",
        "createdAt" : "2021-07-16T17:25:57Z",
        "updatedAt" : "2021-07-18T16:15:10Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "99edb5d679feaaa72df291c00e73de4511208daa",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +130,134 @@\n            // Publish the message to the topic.\n            doPublishMetadata(segmentMetadataUpdate.remoteLogSegmentId().topicIdPartition(), segmentMetadataUpdate);\n        } finally {\n            lock.readLock().unlock();"
  }
]