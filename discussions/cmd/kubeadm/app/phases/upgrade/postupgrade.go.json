[
  {
    "id" : "eb373eaf-e5bb-4d1f-89b4-d8500cd6c8bb",
    "prId" : 95382,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95382#pullrequestreview-527102670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d1dda48-5601-4d2b-87c1-4cf1be484a82",
        "parentId" : null,
        "authorId" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "body" : "nit. Might be patch nodes only if the control-plane label is missing",
        "createdAt" : "2020-11-10T11:36:45Z",
        "updatedAt" : "2020-11-10T20:10:26Z",
        "lastEditedBy" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb7ddf88e907100409b64a158fbf9f3444aa186d",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +273,277 @@\t\t\tcontinue\n\t\t}\n\t\terr = apiclient.PatchNode(client, n.Name, func(n *v1.Node) {\n\t\t\tn.ObjectMeta.Labels[kubeadmconstants.LabelNodeRoleControlPlane] = \"\"\n\t\t})"
  },
  {
    "id" : "16f5c3a8-e247-4a1a-8349-5708d80ed046",
    "prId" : 89593,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89593#pullrequestreview-390946444",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "770a366a-a17d-4178-8429-e8302e5d278b",
        "parentId" : null,
        "authorId" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "body" : "I can remember, that this can also be a \"forbidden\" error here (due to RBACs not getting created as well). This is true for all config maps here. You might have to use `apierrors.IsForbidden(err)` too.",
        "createdAt" : "2020-04-09T16:19:05Z",
        "updatedAt" : "2020-04-09T16:21:42Z",
        "lastEditedBy" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "tags" : [
        ]
      },
      {
        "id" : "5421c360-092e-4db0-9232-6f2da2a7f640",
        "parentId" : "770a366a-a17d-4178-8429-e8302e5d278b",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "hm, if the CM is forbidden then the kubeadm client has no privileges to access it.\r\ngiven this is using the existing admin.conf i don't see how it can happen unless the API server is behaving. me and @fabriziopandini saw something similar in kinder (WRT kubelet RBAC), yet not sure why it would happen here.\r\n",
        "createdAt" : "2020-04-09T16:26:27Z",
        "updatedAt" : "2020-04-09T16:27:56Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "d4de1a571ab9a1fab429902702c1619ea8e6f921",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +112,116 @@\t\tkubeadmconstants.CoreDNSConfigMap,\n\t\tmetav1.GetOptions{},\n\t); err != nil && apierrors.IsNotFound(err) {\n\t\tmissingCoreDNSConfigMap = true\n\t}"
  },
  {
    "id" : "e5daa525-ab1a-4012-93c8-be622c648f07",
    "prId" : 89593,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89593#pullrequestreview-390947153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "751d3293-cc90-4554-ab71-4503b1f6da03",
        "parentId" : null,
        "authorId" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "body" : "With the resiliency problems here in mind, shouldn't these be `apiclient.GetConfigMapWithRetry`?",
        "createdAt" : "2020-04-09T16:20:13Z",
        "updatedAt" : "2020-04-09T16:21:42Z",
        "lastEditedBy" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "tags" : [
        ]
      },
      {
        "id" : "f316db1f-20aa-41aa-a12f-6a03fa991281",
        "parentId" : "751d3293-cc90-4554-ab71-4503b1f6da03",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "i was under the impression that we are only improving `join` for now. none of the above API calls have retries.",
        "createdAt" : "2020-04-09T16:27:26Z",
        "updatedAt" : "2020-04-09T16:27:26Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "d4de1a571ab9a1fab429902702c1619ea8e6f921",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +115,119 @@\t\tmissingCoreDNSConfigMap = true\n\t}\n\tif _, err := client.CoreV1().ConfigMaps(metav1.NamespaceSystem).Get(\n\t\tcontext.TODO(),\n\t\tkubeadmconstants.KubeDNSConfigMap,"
  },
  {
    "id" : "5bc8c888-e0ae-4512-8b1f-51a6a356570c",
    "prId" : 88434,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88434#pullrequestreview-363323808",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ab2d85b-db42-44da-a1e2-2f4a2e555c3a",
        "parentId" : null,
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "i do not recall observing nodes being unschedulable while the CoreDNS addon is being upgraded during \"kubeadm upgrade\". is this something you have seen @SataQiu ?",
        "createdAt" : "2020-02-23T17:54:05Z",
        "updatedAt" : "2020-02-23T17:54:06Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      },
      {
        "id" : "2a97b467-e04b-431b-9711-13648f42f78f",
        "parentId" : "1ab2d85b-db42-44da-a1e2-2f4a2e555c3a",
        "authorId" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "body" : "According to this guide https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrade-the-first-control-plane-node, `kubectl drain <cp-node-name> --ignore-daemonsets` in step 2 will make the control plane node unschedulable.\r\nIf we are using a single node cluster, the only node will be marked as unschedulable. In this case, new DNS deployment will never be ready. That's why the program is stuck. ",
        "createdAt" : "2020-02-24T02:29:00Z",
        "updatedAt" : "2020-02-24T02:29:00Z",
        "lastEditedBy" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "tags" : [
        ]
      },
      {
        "id" : "4d877bde-4949-4e58-9fad-6d9e0f5ac411",
        "parentId" : "1ab2d85b-db42-44da-a1e2-2f4a2e555c3a",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "that is true.",
        "createdAt" : "2020-02-24T11:29:37Z",
        "updatedAt" : "2020-02-24T11:29:37Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "8067dd84700e5ab1310a4decf22abf2b25980c50",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +122,126 @@\n\t\tnodes, err := client.CoreV1().Nodes().List(context.TODO(), metav1.ListOptions{\n\t\t\tFieldSelector: fields.Set{\"spec.unschedulable\": \"false\"}.AsSelector().String(),\n\t\t})\n\t\tif err != nil {"
  },
  {
    "id" : "971f4251-6668-4137-bd6e-370784d32cfd",
    "prId" : 60359,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60359#pullrequestreview-99950925",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc72728c-9b56-4f24-9505-92469ff314e0",
        "parentId" : null,
        "authorId" : "3c1422a0-6358-4857-8f56-961979171514",
        "body" : "This var is not used. Should be removed?",
        "createdAt" : "2018-02-28T05:46:32Z",
        "updatedAt" : "2018-03-07T01:56:34Z",
        "lastEditedBy" : "3c1422a0-6358-4857-8f56-961979171514",
        "tags" : [
        ]
      },
      {
        "id" : "147b3d2b-8b38-49c2-86e7-0e38c335bdd0",
        "parentId" : "cc72728c-9b56-4f24-9505-92469ff314e0",
        "authorId" : "a09f2d48-fec0-4c0d-bd81-6de7378a6ebf",
        "body" : "It's still used (in line 147: https://github.com/kubernetes/kubernetes/pull/60359/files#diff-668202917ffe6a202af0eea0d7bcf87aR147) and necessary to keep it for now. It can be removed in 1.11 cycle.",
        "createdAt" : "2018-02-28T05:58:43Z",
        "updatedAt" : "2018-03-07T01:56:34Z",
        "lastEditedBy" : "a09f2d48-fec0-4c0d-bd81-6de7378a6ebf",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d167d27948d5f99fc572120325fe7e7bad5efe8",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +44,48 @@)\n\nvar v190alpha3 = version.MustParseSemantic(\"v1.9.0-alpha.3\")\nvar expiry = 180 * 24 * time.Hour\n"
  },
  {
    "id" : "d0b90eea-34dc-4932-924d-348692237868",
    "prId" : 56513,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56513#pullrequestreview-79867444",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fa89b20-ab0d-4912-a013-ac39f8ce158d",
        "parentId" : null,
        "authorId" : "a09f2d48-fec0-4c0d-bd81-6de7378a6ebf",
        "body" : "Are there other things of KubeDNS we should cleanup? e.g., KubeDNS service?",
        "createdAt" : "2017-11-29T07:20:09Z",
        "updatedAt" : "2017-12-01T22:27:29Z",
        "lastEditedBy" : "a09f2d48-fec0-4c0d-bd81-6de7378a6ebf",
        "tags" : [
        ]
      },
      {
        "id" : "c3cbec9f-5958-4723-81b8-2ee9105b4f89",
        "parentId" : "7fa89b20-ab0d-4912-a013-ac39f8ce158d",
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "no, only the Deployment. The Service is shared and preserved between kube-dns and coredns",
        "createdAt" : "2017-11-29T14:40:55Z",
        "updatedAt" : "2017-12-01T22:27:29Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7c494fe5b1a07d243e778ba29e511eb53e9821d",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +125,129 @@\t\t\t\treturn fmt.Errorf(\"the CodeDNS deployment isn't ready yet\")\n\t\t\t}\n\t\t\treturn apiclient.DeleteDeploymentForeground(client, metav1.NamespaceSystem, kubeadmconstants.KubeDNS)\n\t\t}, 10)\n\t}"
  },
  {
    "id" : "c5b39125-79d3-4011-8d62-91c91ea9f372",
    "prId" : 53338,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53338#pullrequestreview-67007538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb9bead2-56d8-4f3a-ac82-ae9d642dc362",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this is incorrect... we still need this permission set up for 1.7 installs. this version constant just controls whether we use the `--enable-bootstrap-token-auth` or `--experimental-bootstrap-token-auth` flag. In either case, we require the RBAC permissions.",
        "createdAt" : "2017-10-03T16:28:01Z",
        "updatedAt" : "2017-10-03T16:28:49Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ed138651-1906-4c5e-97bd-fafbe00dd9f3",
        "parentId" : "eb9bead2-56d8-4f3a-ac82-ae9d642dc362",
        "authorId" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "body" : "It exists if 1.7.x was initialized with kubeadm. Only with different roleRef than expected in 1.8.\r\nNot running it, means cluster will be in the same state as it was expected within 1.7.x (no role/bindings migrations to 1.8 naming schema).\r\n\r\nThat's what @luxas suggested. First version of that PR had migration to 1.8 naming schema even in cases of 1.7.x->1.7.x upgrade.",
        "createdAt" : "2017-10-03T16:34:18Z",
        "updatedAt" : "2017-10-03T16:34:18Z",
        "lastEditedBy" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "tags" : [
        ]
      },
      {
        "id" : "ae8e3c81-1d18-4e38-a8bd-649ba63b5e1d",
        "parentId" : "eb9bead2-56d8-4f3a-ac82-ae9d642dc362",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "ah, this is just the upgrade path. this still seems like a fragile way to determine what changes are needed, but I'll defer to @luxas.\r\n\r\nwere all currently supported upgrade paths tested with this change?\r\n1.7.x -> 1.7.y upgrade\r\n1.7.x -> 1.8.x upgrade\r\n1.8.0 -> 1.8.0 \"upgrade\"",
        "createdAt" : "2017-10-04T04:56:34Z",
        "updatedAt" : "2017-10-04T04:56:34Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "02e36904-3cff-41a2-8593-400bacb023d8",
        "parentId" : "eb9bead2-56d8-4f3a-ac82-ae9d642dc362",
        "authorId" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "body" : "Yes, I've tested following on my development cluster:\r\n- install fresh cluster version 1.7.3 using kubeadm 1.7.5\r\n  - old roles/bindings naming schema\r\n- upgrade 1.7.3 to 1.7.7 using modified kubeadm 1.8.0 \r\n  - no roles/binding changes, old names/values\r\n- upgrade 1.7.7 to 1.8.0-rc.1 using modified kubeadm 1.8.0 \r\n  - roles/bindings migrated to 1.8 schema\r\n- upgrade 1.8.0-rc.1 to 1.8.0 using modified kubeadm 1.8.0\r\n  - roles/bindings in 1.8 naming schema\r\n",
        "createdAt" : "2017-10-04T09:33:02Z",
        "updatedAt" : "2017-10-04T09:33:02Z",
        "lastEditedBy" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "tags" : [
        ]
      }
    ],
    "commit" : "c24087a676efaa8c821f839da49ebebff17e57c3",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +78,82 @@\t//}\n\t// Not needed for 1.7 upgrades\n\tif k8sVersion.AtLeast(constants.UseEnableBootstrapTokenAuthFlagVersion) {\n\t\t// Create/update RBAC rules that makes the cluster-info ConfigMap reachable\n\t\tif err := clusterinfo.CreateClusterInfoRBACRules(client); err != nil {"
  }
]