[
  {
    "id" : "37e1ba26-e64b-4a6f-b52b-f6421b0b8572",
    "prId" : 94479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94479#pullrequestreview-482204350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4403ef2f-f69a-4a0e-8d2e-3260a2281ae9",
        "parentId" : null,
        "authorId" : "9e6ce3ed-e2f7-40ea-b6d7-b1b04fc48f70",
        "body" : "@neolit123 looks like we don't need to use `staticpodutil.ComponentResources` here, WDYT?",
        "createdAt" : "2020-09-03T17:00:53Z",
        "updatedAt" : "2020-09-04T03:54:31Z",
        "lastEditedBy" : "9e6ce3ed-e2f7-40ea-b6d7-b1b04fc48f70",
        "tags" : [
        ]
      },
      {
        "id" : "93348c3a-950c-40d1-99ad-28501161b695",
        "parentId" : "4403ef2f-f69a-4a0e-8d2e-3260a2281ae9",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "your are correct.\r\nwe don't need the utility function and the method i outlined in the issue.\r\n",
        "createdAt" : "2020-09-03T21:08:27Z",
        "updatedAt" : "2020-09-04T03:54:31Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ebd2937806ae957df67ae8138f62a5ad9b39c8d",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +249,253 @@\t\t\t\tstaticpodutil.NewVolumeMount(certsVolumeName, cfg.CertificatesDir+\"/etcd\", false),\n\t\t\t},\n\t\t\tResources: v1.ResourceRequirements{\n\t\t\t\tRequests: v1.ResourceList{\n\t\t\t\t\tv1.ResourceCPU:              resource.MustParse(\"100m\"),"
  },
  {
    "id" : "0da96b3d-b90c-4ea7-a942-8d8b4afa6b4e",
    "prId" : 94479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94479#pullrequestreview-482205162",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed233c12-cee2-45e2-95f9-34d35a4fd9ad",
        "parentId" : null,
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "~this import should remain on top under pkg/errors.~\r\n\r\nnever mind. your adjustment is better.",
        "createdAt" : "2020-09-03T21:09:46Z",
        "updatedAt" : "2020-09-04T03:54:31Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ebd2937806ae957df67ae8138f62a5ad9b39c8d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +30,34 @@\t\"k8s.io/apimachinery/pkg/api/resource\"\n\tclientset \"k8s.io/client-go/kubernetes\"\n\t\"k8s.io/klog/v2\"\n\tutilsnet \"k8s.io/utils/net\"\n"
  },
  {
    "id" : "477a79d6-3270-4d11-b566-1afda0e0c6c0",
    "prId" : 91145,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91145#pullrequestreview-418416337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0dc327c-2b5f-49ed-8d99-5f54f3e52daf",
        "parentId" : null,
        "authorId" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "body" : "Should we check etcdClientAddress == member.clientURLs instead of relying on endpoints?",
        "createdAt" : "2020-05-24T12:07:57Z",
        "updatedAt" : "2020-05-24T12:07:58Z",
        "lastEditedBy" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "tags" : [
        ]
      },
      {
        "id" : "548fcf5f-4e98-4742-8870-7545b45115cd",
        "parentId" : "f0dc327c-2b5f-49ed-8d99-5f54f3e52daf",
        "authorId" : "f8030d76-6069-40c7-9c21-93f9f4b262ad",
        "body" : "Thanks @fabriziopandini for the review.\r\n`etcdutil.NewFromCluster` has read all etcd endpoints from etcd and the `etcdClient.Endpoints` are [got from ClientURLs of all members](https://github.com/etcd-io/etcd/blob/747ff75c96df87530bcd8b6b02d1160c5500bf4e/clientv3/client.go#L170-L181).\r\nhttps://github.com/kubernetes/kubernetes/blob/2f38e1b130ea75704bf17316a9c5e89bb36e2d54/cmd/kubeadm/app/util/etcd/etcd.go#L113-L118\r\n\r\nAnd currently the Member struct returned here has only a few information and doesn't have clientURLs: https://github.com/kubernetes/kubernetes/blob/2f38e1b130ea75704bf17316a9c5e89bb36e2d54/cmd/kubeadm/app/util/etcd/etcd.go#L254-L259\r\nSo checking `etcdClientAddress` in `etcdClient.Endpoints` should have the same result as your suggestion and doesn't require struct change?\r\n\r\n",
        "createdAt" : "2020-05-25T17:46:54Z",
        "updatedAt" : "2020-05-25T17:46:54Z",
        "lastEditedBy" : "f8030d76-6069-40c7-9c21-93f9f4b262ad",
        "tags" : [
        ]
      },
      {
        "id" : "720191d1-fb22-4548-9874-77e4ea5ef154",
        "parentId" : "f0dc327c-2b5f-49ed-8d99-5f54f3e52daf",
        "authorId" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "body" : "ok, given that the goal of this check is to check this is actually the last member in the cluster.",
        "createdAt" : "2020-05-26T15:28:34Z",
        "updatedAt" : "2020-05-26T15:28:34Z",
        "lastEditedBy" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cc416e7df0c9fdce7b78a7868b4345bc2cca621",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +112,116 @@\tif len(members) == 1 {\n\t\tetcdClientAddress := etcdutil.GetClientURL(&cfg.LocalAPIEndpoint)\n\t\tfor _, endpoint := range etcdClient.Endpoints {\n\t\t\tif endpoint == etcdClientAddress {\n\t\t\t\tklog.V(1).Info(\"[etcd] This is the only remaining etcd member in the etcd cluster, skip removing it\")"
  },
  {
    "id" : "1d179e6c-935e-4281-940a-61af5b47bdee",
    "prId" : 87656,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87656#pullrequestreview-356059674",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64bf5015-de67-4252-9c28-f3bc92278c87",
        "parentId" : null,
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "the larger diff here could have been avoided if `v1.Container` remained on the previous line.",
        "createdAt" : "2020-02-10T14:24:25Z",
        "updatedAt" : "2020-02-20T11:19:18Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      },
      {
        "id" : "96352482-5d97-46b0-bde4-c07c75746e99",
        "parentId" : "64bf5015-de67-4252-9c28-f3bc92278c87",
        "authorId" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "body" : "If I move the `v1.Container` one line up, the return statement looks weird with the last argument `map[string]string{kubeadmconstants.EtcdAdvertiseClientUrlsAnnotationKey: etcdutil.GetClientURL(endpoint)},`",
        "createdAt" : "2020-02-10T16:05:08Z",
        "updatedAt" : "2020-02-20T11:19:18Z",
        "lastEditedBy" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e59a0651f168331dfc51a056ee3f5b25be431bd",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +183,187 @@\tprobeHostname, probePort, probeScheme := staticpodutil.GetEtcdProbeEndpoint(&cfg.Etcd, utilsnet.IsIPv6String(endpoint.AdvertiseAddress))\n\treturn staticpodutil.ComponentPod(\n\t\tv1.Container{\n\t\t\tName:            kubeadmconstants.Etcd,\n\t\t\tCommand:         getEtcdCommand(cfg, endpoint, nodeName, initialCluster),"
  },
  {
    "id" : "ef5c9c9f-d6f4-4e0b-9c31-98c943457dd0",
    "prId" : 80905,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80905#pullrequestreview-270963577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "744a3106-ff87-48bf-b1e4-acde9e44f251",
        "parentId" : null,
        "authorId" : "0de88c3b-1f1a-401f-9fd1-cd63adc590fd",
        "body" : "Why we are renaming this import?",
        "createdAt" : "2019-08-05T18:28:08Z",
        "updatedAt" : "2019-08-12T12:24:13Z",
        "lastEditedBy" : "0de88c3b-1f1a-401f-9fd1-cd63adc590fd",
        "tags" : [
        ]
      },
      {
        "id" : "d2abe524-322a-479e-a25c-bf61518aa7d7",
        "parentId" : "744a3106-ff87-48bf-b1e4-acde9e44f251",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "goimports does it automatically if i recall correctly (e.g. on vscode).\r\nthere was a discussion about this in k/k but overall it's fine, as long as we don't sent huge PRs that modify all of these at once.\r\n",
        "createdAt" : "2019-08-05T19:34:15Z",
        "updatedAt" : "2019-08-12T12:24:13Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "5eca04955797e4e3e9cbf804bac64f82b7719b00",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +26,30 @@\t\"k8s.io/klog\"\n\n\tv1 \"k8s.io/api/core/v1\"\n\tclientset \"k8s.io/client-go/kubernetes\"\n\tkubeadmapi \"k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm\""
  },
  {
    "id" : "7f537996-a5a4-4c18-b3da-e3859be08587",
    "prId" : 72984,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72984#pullrequestreview-194125368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "parentId" : null,
        "authorId" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "body" : "I don't like this function printing\r\n```\r\n    [util/etcd] Attempt timed out\r\n    [util/etcd] Waiting 5s until next retry\r\n    [util/etcd] Attempt timed out\r\n    [util/etcd] Waiting 5s until next retry\r\n```\r\nIMO those output should be removed (or converted into log messages) in order to be consistent with all the other waiters in kubeadm.\r\n\r\nHowever, considering that this requires to add \"This can take up to ...\" in every place where `WaitForClusterAvailable` is used, this goes out of the scope of this PR, so please open an issue to track this as a todo/good first issue\r\n",
        "createdAt" : "2019-01-17T21:31:00Z",
        "updatedAt" : "2019-01-18T11:04:54Z",
        "lastEditedBy" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "tags" : [
        ]
      },
      {
        "id" : "fa13cca9-56bb-45eb-b089-b12c4ac586b6",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "body" : "I proposed to use `klog` here too but @rosti didn't want to address this change on this PR, only changing the one potentially long with the endpoints. I agree with your point of view though @fabriziopandini.\r\n\r\n@rosti, wdyt? Should I change this now that @fabriziopandini also raised this issue?",
        "createdAt" : "2019-01-17T21:48:31Z",
        "updatedAt" : "2019-01-18T11:04:54Z",
        "lastEditedBy" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "tags" : [
        ]
      },
      {
        "id" : "3a3c92e6-03c2-4146-ac53-3d18a806ed38",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "body" : "Marked as resolved as per discussion with @fabriziopandini, leaving it as it was as @rosti proposed.",
        "createdAt" : "2019-01-18T09:47:42Z",
        "updatedAt" : "2019-01-18T11:04:54Z",
        "lastEditedBy" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "tags" : [
        ]
      },
      {
        "id" : "da7c96d8-7bd7-426d-b3ec-a097d0e7fd5f",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "body" : "I do think, that we need some sort of indication about the reason we wait another 5 seconds. This is tightly coupled with the UX of end users, that run kubeadm directly on command line. For that matter I am not a fan of klogging this. In my opinion it should go out via print.\r\nOn the other hand, we can certainly reduce the output here to say a single, more descriptive message per retry.\r\nHowever, as @fabriziopandini mentioned, this will require changes in a few more places, thus it may better be done in another PR. We can file a backlog issue for now.",
        "createdAt" : "2019-01-18T11:12:36Z",
        "updatedAt" : "2019-01-18T11:12:36Z",
        "lastEditedBy" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "tags" : [
        ]
      },
      {
        "id" : "6e9157da-adf5-4c19-9bd6-c3ed2be5358b",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "i wanted to get more feedback on the 5 seconds and the interval of 20 tries.\r\nif we can get a check faster than 5 on the average, possibly we can reduce the value?\r\nalso 20 tries is a lot. in reality, we might get the failed state much sooner.\r\n",
        "createdAt" : "2019-01-18T11:47:23Z",
        "updatedAt" : "2019-01-18T11:47:23Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      },
      {
        "id" : "a392f78e-4f83-4f52-b770-5804b9609da4",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "body" : "On my environment that is fresh everytime it took 3 retries when it was a 1 second interval, and right now with 5 seconds it retries none or once.\r\n\r\nWithout feedback on how much this takes on different environments it's hard to know (I guess it depends on the uptime of the cluster and the I/O throughput). That being said, my concern is that if we lower the timeout too much we might call it a failure when a long living one-master cluster (weeks, months?) gets a new master.\r\n\r\nJust my 2 cents, but of course it would be great to have real data on this.",
        "createdAt" : "2019-01-18T12:04:03Z",
        "updatedAt" : "2019-01-18T12:04:03Z",
        "lastEditedBy" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "tags" : [
        ]
      },
      {
        "id" : "b7398042-a492-4146-bd3c-7a52867231b9",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "body" : "In the last run it took 4 retries (of 5 seconds interval), this one was way off-charts and with a clean environment :(",
        "createdAt" : "2019-01-18T12:24:30Z",
        "updatedAt" : "2019-01-18T12:24:30Z",
        "lastEditedBy" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "tags" : [
        ]
      },
      {
        "id" : "dd54dd96-b5c4-45f3-bcb5-6d7a5fdfb3be",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "i've mentioned this on slack:\r\n> we may want to keep the overall time under 40seconds to match the kubelet timeout.\r\nhow about 2 seconds rate with 20 retries.",
        "createdAt" : "2019-01-18T14:10:23Z",
        "updatedAt" : "2019-01-18T14:10:23Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      },
      {
        "id" : "ee4bea94-e77c-471e-9071-819c25c068ec",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "@fabriziopandini @rosti please give you stamp of approval for the above comment.\r\n",
        "createdAt" : "2019-01-18T14:12:17Z",
        "updatedAt" : "2019-01-18T14:12:18Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      },
      {
        "id" : "ac762acf-298b-4ee6-b27c-073ad6dd58a8",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "body" : "I like the 40 seconds idea, but let's keep the steps at 5 sec. Bear in mind, that we have just written out the static pod spec, so the kubelet needs to detect it, spin it up and for etcd to become responsive. On some systems it's easy for this to come above 2 seconds.",
        "createdAt" : "2019-01-18T14:37:56Z",
        "updatedAt" : "2019-01-18T14:43:42Z",
        "lastEditedBy" : "cccc7bed-95f4-42a9-83ef-6ba1a4dca7ec",
        "tags" : [
        ]
      },
      {
        "id" : "2f40a59c-ac40-4f6d-bc0c-c06e289d7d1f",
        "parentId" : "fd9dbec5-cf13-4053-a438-6d6e7fb53560",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "ok, @rosti is voting for 5 sec / 8 retries.\r\n@fabriziopandini ?\r\n",
        "createdAt" : "2019-01-18T14:41:37Z",
        "updatedAt" : "2019-01-18T14:41:37Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4cb3fd37c8f36c653b551ea5f962c2cac1aba9b",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +127,131 @@\n\tfmt.Printf(\"[etcd] Waiting for the new etcd member to join the cluster. This can take up to %v\\n\", etcdHealthyCheckInterval*etcdHealthyCheckRetries)\n\tif _, err := etcdClient.WaitForClusterAvailable(etcdHealthyCheckRetries, etcdHealthyCheckInterval); err != nil {\n\t\treturn err\n\t}"
  },
  {
    "id" : "bee74bbc-8dc8-48ef-b268-4398b1a1b383",
    "prId" : 69486,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69486#pullrequestreview-166778131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c601c7d-5c4c-4515-b8ca-ebd1792f45be",
        "parentId" : null,
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "Is this method used during upgrade? If so, it seems odd to call 'AddMember()' for an instance that is already a member of the etcd cluster.",
        "createdAt" : "2018-10-11T13:56:31Z",
        "updatedAt" : "2018-10-27T16:06:32Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "3cc12a59-a3ce-499b-8907-d2e9b05019cd",
        "parentId" : "5c601c7d-5c4c-4515-b8ca-ebd1792f45be",
        "authorId" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "body" : "No, this is used only when adding a new control plane instance",
        "createdAt" : "2018-10-21T09:52:59Z",
        "updatedAt" : "2018-10-27T16:06:32Z",
        "lastEditedBy" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbd6d2d68a36c07082a3d41492e3adcccf83cd25",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +94,98 @@\n\tglog.V(1).Infof(\"Adding etcd member: %s\", etcdPeerAddress)\n\tinitialCluster, err := etcdClient.AddMember(cfg.NodeRegistration.Name, etcdPeerAddress)\n\tif err != nil {\n\t\treturn err"
  },
  {
    "id" : "c2efc1ad-027c-48e1-b0ff-70604c9ccb9f",
    "prId" : 69420,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69420#pullrequestreview-161900346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a05312d-0715-42dc-9ec6-e8a0d6733758",
        "parentId" : null,
        "authorId" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "body" : "Small nit\r\nPlease use fmt.Errorf. it is the standard in kubeadm codebase",
        "createdAt" : "2018-10-04T16:01:27Z",
        "updatedAt" : "2018-10-05T05:55:29Z",
        "lastEditedBy" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "tags" : [
        ]
      },
      {
        "id" : "70fcf3a2-f3e0-4ad5-93b1-866d3917cbc7",
        "parentId" : "9a05312d-0715-42dc-9ec6-e8a0d6733758",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "but errors.New() makes good sense too if the are no arguments.\r\nit's much cheaper on the CPU compared to fmt.Errorf().\r\n\r\nin fact someone made a big PR to fix this across k8s, but he got tired of rebasing and closed.",
        "createdAt" : "2018-10-04T21:25:03Z",
        "updatedAt" : "2018-10-05T05:55:29Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      },
      {
        "id" : "19f32b24-47c0-435e-a750-c94304d96f88",
        "parentId" : "9a05312d-0715-42dc-9ec6-e8a0d6733758",
        "authorId" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "body" : "I don't have an opinion here because I am missing context on how the project wants to make error reporting homogeneous.",
        "createdAt" : "2018-10-05T05:45:10Z",
        "updatedAt" : "2018-10-05T05:55:29Z",
        "lastEditedBy" : "67bc6085-2cd9-4674-9dbc-d2a266f478be",
        "tags" : [
        ]
      },
      {
        "id" : "d918a2e7-d119-4819-b94d-ad118d9b9f49",
        "parentId" : "9a05312d-0715-42dc-9ec6-e8a0d6733758",
        "authorId" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "body" : "@neolit123 \r\nI'm in favour of errors too, but if we have to switch we should do this across all the kubeadm codebase.\r\nIn the meantime fmt.Errorf is the standard to use",
        "createdAt" : "2018-10-05T06:15:01Z",
        "updatedAt" : "2018-10-05T06:15:02Z",
        "lastEditedBy" : "8a27151d-3530-4221-90e8-48b3a85cba37",
        "tags" : [
        ]
      }
    ],
    "commit" : "503c6c7b85e3261eb004120a0b46cd8a371c7cd8",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +38,42 @@// CreateLocalEtcdStaticPodManifestFile will write local etcd static pod manifest file.\nfunc CreateLocalEtcdStaticPodManifestFile(manifestDir string, cfg *kubeadmapi.InitConfiguration) error {\n\tif cfg.ClusterConfiguration.Etcd.External != nil {\n\t\treturn fmt.Errorf(\"etcd static pod manifest cannot be generated for cluster using external etcd\")\n\t}"
  },
  {
    "id" : "c3610f40-07c3-4a3e-aca8-e139ae160320",
    "prId" : 64988,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64988#pullrequestreview-127723255",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a47f0834-043d-449a-84cc-c3f3424da548",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "I'd be pretty surprised that this would cause an issue.",
        "createdAt" : "2018-06-11T20:13:41Z",
        "updatedAt" : "2018-06-12T01:29:28Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "793a51cef0dfd9976b32936badb8f138d4d4cffa",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +81,85 @@\t\t\"listen-client-urls\":          \"https://127.0.0.1:2379\",\n\t\t\"advertise-client-urls\":       \"https://127.0.0.1:2379\",\n\t\t\"listen-peer-urls\":            \"https://127.0.0.1:2380\",\n\t\t\"initial-advertise-peer-urls\": \"https://127.0.0.1:2380\",\n\t\t\"data-dir\":                    cfg.Etcd.Local.DataDir,"
  },
  {
    "id" : "b52efeeb-30b6-4f55-bb40-86aa045445fb",
    "prId" : 57415,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/57415#pullrequestreview-97993181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc5a5c18-cd36-4c44-8b84-b5c42c6063c5",
        "parentId" : null,
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "Should this just mount in the etcd certs here, instead of all of the certs?",
        "createdAt" : "2018-02-20T18:42:13Z",
        "updatedAt" : "2018-02-24T00:05:51Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "0e32143f-b4f3-4368-8dc8-08552a2a8b17",
        "parentId" : "bc5a5c18-cd36-4c44-8b84-b5c42c6063c5",
        "authorId" : "bf66f693-b705-46d8-a378-3750b55f033c",
        "body" : "This will be easy for this PR if we have a separate etcd CA cert in the same subdirectory directory.\r\n\r\nI think we should do this for all of the static pods, though.\r\nIt's a little weirder when we think about the apiserver pod needing the etcd CA cert.\r\n\r\nThis seems doable with individual HostPath Volumes per File -- WDYT?\r\n\r\n",
        "createdAt" : "2018-02-20T19:53:17Z",
        "updatedAt" : "2018-02-24T00:05:51Z",
        "lastEditedBy" : "bf66f693-b705-46d8-a378-3750b55f033c",
        "tags" : [
        ]
      },
      {
        "id" : "f9c9652c-2e3b-4604-aa26-2749e25c7220",
        "parentId" : "bc5a5c18-cd36-4c44-8b84-b5c42c6063c5",
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "I think it all depends on the end-user experience when managing the certs post-install. I'm a bit torn on whether referencing single files across different paths makes the most sense here or if duplicating files into component based subdirectories makes the most sense.",
        "createdAt" : "2018-02-20T21:20:36Z",
        "updatedAt" : "2018-02-24T00:05:51Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      }
    ],
    "commit" : "509e9af5223de6849b7d99dbb53796f794876752",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +54,58 @@\tetcdMounts := map[string]v1.Volume{\n\t\tetcdVolumeName:  staticpodutil.NewVolume(etcdVolumeName, cfg.Etcd.DataDir, &pathType),\n\t\tcertsVolumeName: staticpodutil.NewVolume(certsVolumeName, cfg.CertificatesDir, &pathType),\n\t}\n\treturn staticpodutil.ComponentPod(v1.Container{"
  },
  {
    "id" : "690c1b4c-c443-42c3-8e9f-39e631cdeb0d",
    "prId" : 50967,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50967#pullrequestreview-57545636",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f28b22c2-9ac1-4fbd-be7a-f50e2721b76a",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "So this sentence is confusing to read.  \r\n[etcd-installation-phase] vs. [etcd]  Otherwise it reads like [etcd] is the actor when kubeadm is the actor. ",
        "createdAt" : "2017-08-21T16:04:35Z",
        "updatedAt" : "2017-08-21T16:08:47Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "667fb96f-31d4-4475-b0b7-17352c6f1ff9",
        "parentId" : "f28b22c2-9ac1-4fbd-be7a-f50e2721b76a",
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "I'll open an issue in kubeadm for that. This makes things consistent, which is better than not, but we might want to revisit this in future releases indeed",
        "createdAt" : "2017-08-21T16:10:38Z",
        "updatedAt" : "2017-08-21T16:10:39Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4855bd1af4f7f4cee2be715888df41d0a8e2127",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +42,46 @@\t}\n\n\tfmt.Printf(\"[etcd] Wrote Static Pod manifest for a local etcd instance to %q\\n\", kubeadmconstants.GetStaticPodFilepath(kubeadmconstants.Etcd, manifestDir))\n\treturn nil\n}"
  }
]