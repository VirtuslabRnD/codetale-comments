[
  {
    "id" : "c2e1feae-059f-4e87-85a4-7e15943f6332",
    "prId" : 99358,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99358#pullrequestreview-597679743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0365cb42-b3aa-4b10-ba1a-dbcf0b6ed05a",
        "parentId" : null,
        "authorId" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "body" : "FYI, this would put `timeout=70s` to the request URI and the apiserver would truncate it to `60s` :)\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/request.go#L494-L497",
        "createdAt" : "2021-02-23T14:12:04Z",
        "updatedAt" : "2021-02-23T14:12:05Z",
        "lastEditedBy" : "3e6e337f-0beb-4609-abc3-11b8e8cf5688",
        "tags" : [
        ]
      },
      {
        "id" : "c849a00a-5481-4524-a32f-458f7b53e52c",
        "parentId" : "0365cb42-b3aa-4b10-ba1a-dbcf0b6ed05a",
        "authorId" : "13c01cb8-6052-4a98-b6fd-20d3acd6332f",
        "body" : "yeah, so the requests should be timed out after 60s if not then we know there was something that was holding up requests, for example, a proxy.",
        "createdAt" : "2021-02-24T16:32:04Z",
        "updatedAt" : "2021-02-24T16:32:04Z",
        "lastEditedBy" : "13c01cb8-6052-4a98-b6fd-20d3acd6332f",
        "tags" : [
        ]
      }
    ],
    "commit" : "662cc70c70a0f2b269188b9b2192eeee0e1a2ab4",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +436,440 @@\tkubeconfig.QPS = s.Generic.ClientConnection.QPS\n\tkubeconfig.Burst = int(s.Generic.ClientConnection.Burst)\n\tkubeconfig.Timeout = 70 * time.Second // slightly bigger than the default server timeout which is 60 seconds\n\n\tclient, err := clientset.NewForConfig(restclient.AddUserAgent(kubeconfig, KubeControllerManagerUserAgent))"
  },
  {
    "id" : "8fb57961-2dbc-46c6-922d-17adab9b31e7",
    "prId" : 96216,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96216#pullrequestreview-526352814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccc39e6d-2f16-4243-805b-f46005c34772",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "we keep the flags but remove the logic? How strange.",
        "createdAt" : "2020-11-09T15:08:02Z",
        "updatedAt" : "2021-02-26T10:30:23Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "44474e6a-474b-430c-b4c5-db044eb343b3",
        "parentId" : "ccc39e6d-2f16-4243-805b-f46005c34772",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "We keep the flags that people have been using for years to turn off insecure serving and allow them to continue setting `--port=0` for a few releases. Any other value causes an error.\r\n\r\nThe reason is to avoid accidents in multi-version deployers (like kubeadm) which could lead to dropping the `--port` flag in 1.20 because it was no longer allowed, but then accidentally re-enabling insecure serving when using those manifests to deploy a 1.19 cluster",
        "createdAt" : "2020-11-09T15:10:26Z",
        "updatedAt" : "2021-02-26T10:30:23Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ef034be6-1d3b-4661-b557-494d680caee1",
        "parentId" : "ccc39e6d-2f16-4243-805b-f46005c34772",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "got it",
        "createdAt" : "2020-11-09T15:11:38Z",
        "updatedAt" : "2021-02-26T10:30:23Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "97b5d2a3003ef1a36363d46b037fe2a8726691b2",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +230,234 @@\tfs.IntVar(&bindPort, \"port\", bindPort, \"The port on which to serve unsecured, unauthenticated access. Set to 0 to disable.\")\n\tfs.MarkDeprecated(\"port\", \"This flag has no effect now and will be removed in v1.24.\")\n}\n\n// Flags returns flags for a specific APIServer by section name"
  },
  {
    "id" : "5a853ba0-4f1f-4041-a8fb-d34791ed8958",
    "prId" : 91521,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91521#pullrequestreview-441095881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca94509b-b328-444f-be46-f93ee8a01dce",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Does this fit better into generic? Is it just one flag? Where are the other klog flags?",
        "createdAt" : "2020-07-01T14:41:58Z",
        "updatedAt" : "2020-07-01T14:41:59Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "8aa45090-a62e-4a0b-b5af-f82bfb71f79a",
        "parentId" : "ca94509b-b328-444f-be46-f93ee8a01dce",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "Other klog flags are Global flags. I think this flag should be in same flagSet `logs`  as in other control plane components. ",
        "createdAt" : "2020-07-01T14:49:56Z",
        "updatedAt" : "2020-07-01T14:50:05Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      },
      {
        "id" : "439b0c66-1aaa-48d2-ae99-1d81eab8baf4",
        "parentId" : "ca94509b-b328-444f-be46-f93ee8a01dce",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "https://gist.github.com/fd93b5d6044d7c256d2e70fcabf503ae does it. All logging flags will be under the logs section.",
        "createdAt" : "2020-07-01T15:27:09Z",
        "updatedAt" : "2020-07-01T15:27:10Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "0b973b52-8863-4d33-857c-7773180c8fba",
        "parentId" : "ca94509b-b328-444f-be46-f93ee8a01dce",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "My gist does not move `log-flush-frequency` yet. That's also defined in component-base/logs and has to go to the named flag set.",
        "createdAt" : "2020-07-01T15:30:24Z",
        "updatedAt" : "2020-07-01T15:30:24Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "e912245f-e20d-4aa6-845f-db79d09f004e",
        "parentId" : "ca94509b-b328-444f-be46-f93ee8a01dce",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "As the proposed change is bigger (removes the init func behaviour of the package), I am fine to postpone this to 1.20. But would like to see a PR started with the gist, tagged with the relevant people **before** approving this PR here.",
        "createdAt" : "2020-07-01T16:18:20Z",
        "updatedAt" : "2020-07-01T16:18:20Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "bf9ce481-4ae7-434e-93e8-afb09b80fe4b",
        "parentId" : "ca94509b-b328-444f-be46-f93ee8a01dce",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "Done https://github.com/kubernetes/kubernetes/pull/92707",
        "createdAt" : "2020-07-01T17:00:03Z",
        "updatedAt" : "2020-07-01T17:00:03Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      },
      {
        "id" : "e1ed6832-fb7b-496b-9f41-ef9b30537b09",
        "parentId" : "ca94509b-b328-444f-be46-f93ee8a01dce",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "We agreed to keep this new logs section for now, and move klog flags over in a follow-up (in 1.19 or 1.20 if we miss code freeze).",
        "createdAt" : "2020-07-01T18:15:01Z",
        "updatedAt" : "2020-07-01T18:15:01Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "17f3cd48a54483b4c6b7dc1d742194a1f41daf0a",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +253,257 @@\ts.TTLAfterFinishedController.AddFlags(fss.FlagSet(\"ttl-after-finished controller\"))\n\ts.Metrics.AddFlags(fss.FlagSet(\"metrics\"))\n\ts.Logs.AddFlags(fss.FlagSet(\"logs\"))\n\n\tfs := fss.FlagSet(\"misc\")"
  },
  {
    "id" : "75f425b0-b636-41ae-959f-d6877dceb1f1",
    "prId" : 68068,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/68068#pullrequestreview-151156024",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83c1c7e1-8898-4dd0-baaf-4e992f78ba18",
        "parentId" : null,
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "Should this say CPU in it?  Doesn't it apply to other metrics as well?",
        "createdAt" : "2018-08-30T14:43:12Z",
        "updatedAt" : "2018-08-30T21:14:13Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      },
      {
        "id" : "bf42aaa9-0ba6-49ca-9923-437e678986d5",
        "parentId" : "83c1c7e1-8898-4dd0-baaf-4e992f78ba18",
        "authorId" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "body" : "+mwielgus\r\nCurrently this is only for CPU and I belive it will be only used for CPU.",
        "createdAt" : "2018-08-30T15:53:25Z",
        "updatedAt" : "2018-08-30T21:14:13Z",
        "lastEditedBy" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "tags" : [
        ]
      },
      {
        "id" : "16a2864c-e034-4594-98ad-6559d1f22a12",
        "parentId" : "83c1c7e1-8898-4dd0-baaf-4e992f78ba18",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "it got changed a PR or two ago.  It used to be universal across all metrics, so we probably need to decide if we really want to change it.",
        "createdAt" : "2018-08-30T18:36:02Z",
        "updatedAt" : "2018-08-30T21:14:13Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      },
      {
        "id" : "30211de5-862c-421e-920c-c868dd2763d6",
        "parentId" : "83c1c7e1-8898-4dd0-baaf-4e992f78ba18",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "@mwielgus ",
        "createdAt" : "2018-08-30T18:36:09Z",
        "updatedAt" : "2018-08-30T21:14:13Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      },
      {
        "id" : "bf42d6d9-5a28-47ba-8a75-8e577bc8a3f8",
        "parentId" : "83c1c7e1-8898-4dd0-baaf-4e992f78ba18",
        "authorId" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "body" : "We didn't have such flag before and we want to change behavior only for CPU as it is kind of special.",
        "createdAt" : "2018-08-30T19:53:13Z",
        "updatedAt" : "2018-08-30T21:14:13Z",
        "lastEditedBy" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "tags" : [
        ]
      }
    ],
    "commit" : "62f771fbd4abc8561b9406823c7fb2e2667ba291",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +136,140 @@\t\t\tHorizontalPodAutoscalerUpscaleForbiddenWindow:   componentConfig.HPAController.HorizontalPodAutoscalerUpscaleForbiddenWindow,\n\t\t\tHorizontalPodAutoscalerDownscaleForbiddenWindow: componentConfig.HPAController.HorizontalPodAutoscalerDownscaleForbiddenWindow,\n\t\t\tHorizontalPodAutoscalerCPUInitializationPeriod:  componentConfig.HPAController.HorizontalPodAutoscalerCPUInitializationPeriod,\n\t\t\tHorizontalPodAutoscalerInitialReadinessDelay:    componentConfig.HPAController.HorizontalPodAutoscalerInitialReadinessDelay,\n\t\t\tHorizontalPodAutoscalerTolerance:                componentConfig.HPAController.HorizontalPodAutoscalerTolerance,"
  },
  {
    "id" : "564d597a-b15a-430f-b447-feafaf8440eb",
    "prId" : 67362,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67362#pullrequestreview-147157679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08043f03-fbc3-4153-83bc-9d331db57c6c",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "maybe CamelCase those strings?",
        "createdAt" : "2018-08-17T07:12:06Z",
        "updatedAt" : "2018-08-20T12:04:05Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "d3902284-77d3-48c7-b1bb-169ba33591e6",
        "parentId" : "08043f03-fbc3-4153-83bc-9d331db57c6c",
        "authorId" : "05637862-b60e-403e-8519-09d1b3f0c9c2",
        "body" : "fixed, and add judgement for one option section has no flags\r\n",
        "createdAt" : "2018-08-17T09:30:31Z",
        "updatedAt" : "2018-08-20T12:04:05Z",
        "lastEditedBy" : "05637862-b60e-403e-8519-09d1b3f0c9c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0eb92cc5292c7bebb6bad66b3e7a045d5e4ee5e",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +246,250 @@\ts.EndPointController.AddFlags(fss.FlagSet(\"endpoint controller\"))\n\ts.GarbageCollectorController.AddFlags(fss.FlagSet(\"garbagecollector controller\"))\n\ts.HPAController.AddFlags(fss.FlagSet(\"horizontalpodautoscaling controller\"))\n\ts.JobController.AddFlags(fss.FlagSet(\"job controller\"))\n\ts.NamespaceController.AddFlags(fss.FlagSet(\"namespace controller\"))"
  },
  {
    "id" : "dbaa1f5a-6809-4bde-9ff4-87285edc223b",
    "prId" : 67060,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67060#pullrequestreview-151395524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0fd1ce6-6322-4136-9246-7cc53e69de02",
        "parentId" : null,
        "authorId" : "695b6860-569c-4e63-a215-7342e2279a94",
        "body" : "nit: why do we change the order of L344 and L347?",
        "createdAt" : "2018-08-28T23:06:37Z",
        "updatedAt" : "2018-08-28T23:06:37Z",
        "lastEditedBy" : "695b6860-569c-4e63-a215-7342e2279a94",
        "tags" : [
        ]
      },
      {
        "id" : "0bde7900-f027-4dd2-90c6-923eef13894c",
        "parentId" : "e0fd1ce6-6322-4136-9246-7cc53e69de02",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Like in kube-apiserver we prefer a loopback connection through insecure serving. Not really super important, but more uniform.",
        "createdAt" : "2018-08-31T13:32:11Z",
        "updatedAt" : "2018-08-31T13:32:11Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2724793e8ff47fc5961762a874ac74b9169bed1",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +345,349 @@\t\treturn err\n\t}\n\tif err := s.SecureServing.ApplyTo(&c.SecureServing); err != nil {\n\t\treturn err\n\t}"
  },
  {
    "id" : "7bd3a2de-489e-4374-8731-1735c47c1170",
    "prId" : 66840,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66840#pullrequestreview-152248316",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "104fbe76-2f54-4726-96e2-5d1b464313b7",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "I'm a proponent of YAGNI when it comes to configuration like parallel syncs. Is there a reason we want it explicitly for this feature as compared to other loops? An operator guessed value might be slighter better than a developer guessed value in an edge situation but the cost is extra code, maintainablity and suportability complexity. This was @cheftako's request",
        "createdAt" : "2018-09-04T20:50:00Z",
        "updatedAt" : "2018-09-04T21:21:33Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "59b6a2a0-c095-4755-b27b-2a91734074e6",
        "parentId" : "104fbe76-2f54-4726-96e2-5d1b464313b7",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "I'd prefer adding workqueue latency and handletime metrics and explore adaptive worker counts then hand knobs to operators as a last resort.",
        "createdAt" : "2018-09-04T20:51:22Z",
        "updatedAt" : "2018-09-04T21:21:33Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "13b76d5fb4eb680a78eb9760f8bf0b2dd4d75d9c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +175,179 @@\t\t},\n\t\tTTLAfterFinishedController: &TTLAfterFinishedControllerOptions{\n\t\t\tConcurrentTTLSyncs: componentConfig.TTLAfterFinishedController.ConcurrentTTLSyncs,\n\t\t},\n\t\tSecureServing: apiserveroptions.NewSecureServingOptions().WithLoopback(),"
  },
  {
    "id" : "cdc7949d-8f37-4f11-bd13-77366a700f5a",
    "prId" : 65094,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65094#pullrequestreview-131847432",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f49389d-9bd3-4592-96dd-ad365f5de446",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Would it be sufficient to set this to RetryPeriod? I think then we can get away without the other timeout.",
        "createdAt" : "2018-06-22T18:28:13Z",
        "updatedAt" : "2018-06-30T02:39:18Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "e42c0097-5d93-45d4-b8d3-3ac5c3ceffd6",
        "parentId" : "5f49389d-9bd3-4592-96dd-ad365f5de446",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "No, this timeout is to set for http client, used to prevent blocking forever.",
        "createdAt" : "2018-06-25T01:55:00Z",
        "updatedAt" : "2018-06-30T02:39:18Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      },
      {
        "id" : "7c7e97db-28b6-455d-8715-97b181d2055d",
        "parentId" : "5f49389d-9bd3-4592-96dd-ad365f5de446",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "I understand what it is used for. If we retry every RetryPeriod anyway, why would we want a client to block for longer than RetryPeriod?",
        "createdAt" : "2018-06-25T17:49:32Z",
        "updatedAt" : "2018-06-30T02:39:18Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "d7fc5fe9-b271-47f9-befd-3e448db34f9d",
        "parentId" : "5f49389d-9bd3-4592-96dd-ad365f5de446",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "In general, the timeout less than `RetryPeriod` can work well. But if the network is unstable, and it takes longer to finish the renew. During this period, this client loses leadership. ",
        "createdAt" : "2018-06-26T02:34:20Z",
        "updatedAt" : "2018-06-30T02:39:18Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      },
      {
        "id" : "3d742e25-8922-4e91-9eb2-de13824d1bf4",
        "parentId" : "5f49389d-9bd3-4592-96dd-ad365f5de446",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "This is my own thought. Would not mind different voice.",
        "createdAt" : "2018-06-26T02:36:19Z",
        "updatedAt" : "2018-06-30T02:39:18Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "90b287c12d921e4e08e522dbe214f5587f147956",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +435,439 @@\t// shallow copy, do not modify the kubeconfig.Timeout.\n\tconfig := *kubeconfig\n\tconfig.Timeout = s.GenericComponent.LeaderElection.RenewDeadline.Duration\n\tleaderElectionClient := clientset.NewForConfigOrDie(restclient.AddUserAgent(&config, \"leader-election\"))\n"
  },
  {
    "id" : "7debf158-c21d-4114-a2e5-dc0ee4474ec6",
    "prId" : 64142,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64142#pullrequestreview-128049662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eaef35fe-f96b-44a3-b7b7-a80b53289d0e",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "@luxas out of curiosity: is this disabled somehow when an external cloud ctrl manager is running?",
        "createdAt" : "2018-05-29T07:34:52Z",
        "updatedAt" : "2018-06-19T10:00:06Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "c323c321-c5d4-412c-b80a-1f9efcfd5e26",
        "parentId" : "eaef35fe-f96b-44a3-b7b7-a80b53289d0e",
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "cc @andrewsykim \r\n@sttts IIRC, yes it is\r\n@sttts might be interesting reading: https://docs.google.com/document/d/1YqsPzeG10E59vea5StKFM6W_xZq034eTaf1n649JQvs/edit?ouid=100233907167775907499&usp=docs_home",
        "createdAt" : "2018-05-29T08:16:19Z",
        "updatedAt" : "2018-06-19T10:00:06Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      },
      {
        "id" : "595f8609-9da6-4fcb-9a17-f6ff56db3a7f",
        "parentId" : "eaef35fe-f96b-44a3-b7b7-a80b53289d0e",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Service controller does nothing if cloud provider is not set or set to `external`",
        "createdAt" : "2018-06-12T16:31:42Z",
        "updatedAt" : "2018-06-19T10:00:06Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "d2cc3731-5fb9-418e-b2b5-96a71b74e987",
        "parentId" : "eaef35fe-f96b-44a3-b7b7-a80b53289d0e",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "If you're curious https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/service/service_controller.go#L225-L227 ",
        "createdAt" : "2018-06-12T16:32:46Z",
        "updatedAt" : "2018-06-19T10:00:06Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "d52ae386-a524-403a-85f5-5272e1b6a0d2",
        "parentId" : "eaef35fe-f96b-44a3-b7b7-a80b53289d0e",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Also: https://github.com/kubernetes/kubernetes/blob/master/cmd/kube-controller-manager/app/controllermanager.go#L345-L350 ",
        "createdAt" : "2018-06-12T16:34:46Z",
        "updatedAt" : "2018-06-19T10:00:06Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      }
    ],
    "commit" : "440a616644f16db35cf30457d609fe5a553889ca",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +61,65 @@\tGenericComponent  *cmoptions.GenericComponentConfigOptions\n\tKubeCloudShared   *cmoptions.KubeCloudSharedOptions\n\tServiceController *cmoptions.ServiceControllerOptions\n\n\tAttachDetachController           *AttachDetachControllerOptions"
  },
  {
    "id" : "7102cc7b-f605-47a4-998c-4c4c6d2de157",
    "prId" : 49215,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49215#pullrequestreview-54682731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "495bab97-3770-49fe-9d18-345952c77f6a",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Should we mark this flag as deprecated immediately?",
        "createdAt" : "2017-07-20T20:17:34Z",
        "updatedAt" : "2017-08-10T13:42:29Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "a712f0c4-f392-42ab-8c53-5808bfecc470",
        "parentId" : "495bab97-3770-49fe-9d18-345952c77f6a",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "This flag seems pointless if it defaults to false. People either have to change their config to pass --allow-untagged-cloud or change their config to pass clusterID. If you are going to break compatibility, just force people to get to where they need to go. By allowing them to pass --allow-untagged-cloud and later removing the flag, aren't you just breaking compatibility twice?",
        "createdAt" : "2017-07-20T20:49:24Z",
        "updatedAt" : "2017-08-10T13:42:29Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "f924fcdb-a3e6-4f9a-8450-ba773ae27204",
        "parentId" : "495bab97-3770-49fe-9d18-345952c77f6a",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Whether to set the default to false or true is still being discussed in the AWS sig.  However, imo, setting the flag to false forces the issue immediately and gives users time to do the labeling before the option goes away.  The intent is for this option to only exist for a few releases.  Having it default to false will inform the user while still allowing for an easy change to function in the old, but broken, behavior while they plan to do the required labeling.",
        "createdAt" : "2017-07-25T18:22:32Z",
        "updatedAt" : "2017-08-10T13:42:29Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "14736fce-63d2-4366-a406-a14c1f863ada",
        "parentId" : "495bab97-3770-49fe-9d18-345952c77f6a",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "@roberthbailey I believe that is the plan, but would prefer aws-sig members weigh in.",
        "createdAt" : "2017-07-25T18:23:24Z",
        "updatedAt" : "2017-08-10T13:42:29Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "e4b9a121-466f-429c-9246-7bfba2edc1f9",
        "parentId" : "495bab97-3770-49fe-9d18-345952c77f6a",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "The thought behind the flag is that changing your infrastructure deployment tool to add the tags may be non-trivial.\r\n\r\nSo we are forcing people to add the tag, but we're also giving them a reasonable path to upgrade if they can't easily add that to their deployment tool.\r\n\r\nI agree we can mark the flag deprecated right away.",
        "createdAt" : "2017-08-04T17:22:10Z",
        "updatedAt" : "2017-08-10T13:42:29Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "59d98b7f-372e-4d4a-bc46-83a140e1a2df",
        "parentId" : "495bab97-3770-49fe-9d18-345952c77f6a",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "I marked the new flag as deprecated",
        "createdAt" : "2017-08-07T15:08:33Z",
        "updatedAt" : "2017-08-10T13:42:29Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "926f070719f98a1fd3a811898cbcb90c919b3d56",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +135,139 @@\tfs.StringVar(&s.CloudProvider, \"cloud-provider\", s.CloudProvider, \"The provider for cloud services.  Empty string for no provider.\")\n\tfs.StringVar(&s.CloudConfigFile, \"cloud-config\", s.CloudConfigFile, \"The path to the cloud provider configuration file.  Empty string for no configuration file.\")\n\tfs.BoolVar(&s.AllowUntaggedCloud, \"allow-untagged-cloud\", false, \"Allow the cluster to run without the cluster-id on cloud instances.  This is a legacy mode of operation and a cluster-id will be required in the future.\")\n\tfs.MarkDeprecated(\"allow-untagged-cloud\", \"This flag is deprecated and will be removed in a future release.  A cluster-id will be required on cloud instances\")\n\tfs.Int32Var(&s.ConcurrentEndpointSyncs, \"concurrent-endpoint-syncs\", s.ConcurrentEndpointSyncs, \"The number of endpoint syncing operations that will be done concurrently. Larger number = faster endpoint updating, but more CPU (and network) load\")"
  },
  {
    "id" : "56567b13-7269-4e2c-ba3e-01d0edac4a51",
    "prId" : 46796,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46796#pullrequestreview-43626737",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc51e8a1-cc0b-480a-92d4-f82504d4124e",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Is 10 enough?",
        "createdAt" : "2017-06-13T06:06:41Z",
        "updatedAt" : "2017-06-13T06:06:41Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "76493fcb7d702a94fe2e5a8ffa1fe57425d2cddb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +68,72 @@\t\t\tConcurrentResourceQuotaSyncs:                    5,\n\t\t\tConcurrentDeploymentSyncs:                       5,\n\t\t\tConcurrentNamespaceSyncs:                        10,\n\t\t\tConcurrentSATokenSyncs:                          5,\n\t\t\tLookupCacheSizeForRC:                            4096,"
  },
  {
    "id" : "32afe05f-4aac-4c5d-b0a3-760dbe6c5eea",
    "prId" : 40355,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40355#pullrequestreview-21397638",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "if alpha, should default off. specific deployments (CI, etc) can enable it if they want to",
        "createdAt" : "2017-02-01T13:51:18Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "44dc1700-9187-43df-befc-7b5e6fee549a",
        "parentId" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "This was false in a previous commit iirc, and we decided it has to be disabled this release. \r\n\r\n/cc @davidopp ",
        "createdAt" : "2017-02-01T15:26:29Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "fe666768-e355-4e38-a6c5-e795a30a238d",
        "parentId" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "OK - I'm really confused now. Is this going to be alpha or beta? I.e. we plan on moving Taints to beta as a field, will 'NoExecute' effect be an alpha value for that field? How do we gate/mark this?",
        "createdAt" : "2017-02-06T10:25:31Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "10b7cc99-433d-4cac-a81e-f3dac94323e2",
        "parentId" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "@davidopp ",
        "createdAt" : "2017-02-06T10:25:38Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "cb645dd4-fcb5-4be1-bb42-f61ef1126402",
        "parentId" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Here is what my plan is for 1.6 (sorry this was probably not communicated widely):\r\n\r\n1. taints and tolerations in the form we had in 1.5 goes to beta (including moving annotations to fields)\r\n2. tolerationSeconds and support for NoExecute taint effect is introduced as beta (without going through alpha), thus they can use fields\r\n3. use of taints (instead of node conditions) to trigger evictions due to node problems is ALPHA and is disabled by default\r\n\r\nRationale:\r\n\r\n* Introducing tolerationSeconds as alpha would be extremely painful because we don't really have \"alpha fields,\" so it would need to be an alpha annotation that gets moved into a field in 1.7. \r\n* Introducing NoExecute taint as alpha (assuming that also means disabled by default) doesn't make sense if tolerationSeconds is beta (and enabled by default), since tolerationSeconds only makes sense with NoExecute taint.\r\n* tolerationSeconds and NoExecute only have an impact if user decides to use them, so having them enabled by default in the first release where they are introduced is not dangerous (obviously if people using them find bugs, we will need to do a patch release). OTOH the \"use taints for eviction in NodeController\" behavior IS dangerous to enable by default in the first release where it appears, because nobody \"decides\" to use it, the system just uses it internally. So we need to make that behavior be disabled by default, and use the \"old\" NodeController codepath for evictions in 1.6. We can have people experiment with enabling it over the next few months, and then enable it by default in 1.7 if there are no problems (or after fixing problems we find).\r\n\r\nDoes that make sense?\r\n",
        "createdAt" : "2017-02-06T20:10:43Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "a4636da0-e64e-4ac9-a703-d6a56feca8d5",
        "parentId" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "> use of taints (instead of node conditions) to trigger evictions due to node problems is ALPHA and is disabled by default \r\n\r\nSo false...?",
        "createdAt" : "2017-02-07T20:58:02Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "8b24ae25-e570-4854-b982-f0d488dbe754",
        "parentId" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Nope - true here, false in the third PR that's in the making. This PR adds a beta feature, and IIUC beta is enabled by default.",
        "createdAt" : "2017-02-08T09:23:57Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "3c183757-59c5-480a-ae9d-db86c3369327",
        "parentId" : "3cabcaa9-af3f-4724-89a7-f3861136df59",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Right, this should be true.\r\n\r\n@gmarek What are you referring to as the first of the three PRs -- is it #41068 ?",
        "createdAt" : "2017-02-12T01:00:04Z",
        "updatedAt" : "2017-02-12T06:19:09Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "004552f8a43e0bfd138c166f586bb4686fac559c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +105,109 @@\t\t\tClusterSigningKeyFile:    \"/etc/kubernetes/ca/ca.key\",\n\t\t\tReconcilerSyncLoopPeriod: metav1.Duration{Duration: 5 * time.Second},\n\t\t\tEnableTaintManager:       true,\n\t\t},\n\t}"
  },
  {
    "id" : "5960f064-275d-4499-bc7c-059a077c5a32",
    "prId" : 40355,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40355#pullrequestreview-21397638",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f79ad5b-eabe-4a2b-b23f-0a542be62f9e",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Is it really necessary to make this be a flag? The code would be simpler without it, and beta is enabled by default... I think if this feature is broken we would do a patch release, not just disable it.",
        "createdAt" : "2017-02-12T01:18:11Z",
        "updatedAt" : "2017-02-12T06:19:09Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "004552f8a43e0bfd138c166f586bb4686fac559c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +198,202 @@\tfs.BoolVar(&s.DisableAttachDetachReconcilerSync, \"disable-attach-detach-reconcile-sync\", false, \"Disable volume attach detach reconciler sync. Disabling this may cause volumes to be mismatched with pods. Use wisely.\")\n\tfs.DurationVar(&s.ReconcilerSyncLoopPeriod.Duration, \"attach-detach-reconcile-sync-period\", s.ReconcilerSyncLoopPeriod.Duration, \"The reconciler sync wait time between volume attach detach. This duration must be larger than one second, and increasing this value from the default may allow for volumes to be mismatched with pods.\")\n\tfs.BoolVar(&s.EnableTaintManager, \"enable-taint-manager\", s.EnableTaintManager, \"WARNING: Beta feature. If set to true enables NoExecute Taints and will evict all not-tolerating Pod running on Nodes tainted with this kind of Taints.\")\n\n\tleaderelection.BindFlags(&s.LeaderElection, fs)"
  },
  {
    "id" : "4524b624-4cc3-46b1-b330-1953c2147b0b",
    "prId" : 39740,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39740#pullrequestreview-16228129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc655ca6-4a2d-40d1-9993-553ba5690d99",
        "parentId" : null,
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "ValidateControllers?",
        "createdAt" : "2017-01-11T20:28:19Z",
        "updatedAt" : "2017-01-12T13:46:36Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9b75ed82bf5b80716d9d1c4682a725b165314b3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +200,204 @@\n// Validate is used to validate the options and config before launching the controller manager\nfunc (s *CMServer) Validate(allControllers []string, disabledByDefaultControllers []string) error {\n\tvar errs []error\n"
  },
  {
    "id" : "eaabd52c-6dbc-4e80-b625-5a484edfb919",
    "prId" : 30138,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ebc9a398-c0fd-456c-91d7-0ff3d99eb2d3",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I agree with Daniel that 0.01 is probably too large, especially if --large-cluster-size-threshold is 20. \n\nI think maybe we should change the default for --unhealthy-zone-threshold to 0.55 (this is a rough approximation for \"can't reschedule anywhere\" if the pods are evenly thread), and change --secondary-node-eviction-rate to 0.002.\n\nWe should also mention this new default behavior very prominently in the release notes so we don't get a ton of questions from people about why their pods aren't being evicted.\n\nActually, it would be great to add some kind of NodeCondition (generated by node controller) that indicates when evictions from that node are being rate-limited due to this new feature. Borg has something like this and it's really helpful for understandability. I would say it's not critical if you can't finish it by Friday (certainly I would not suggest to put it in this PR).\n",
        "createdAt" : "2016-08-15T05:08:56Z",
        "updatedAt" : "2016-08-17T08:43:38Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "fce2d72b-7bfc-48a1-88a7-fa95d223a75a",
        "parentId" : "ebc9a398-c0fd-456c-91d7-0ff3d99eb2d3",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "0.002 means that we'll evict ~7 Nodes and hour - I think it's too slow. \n",
        "createdAt" : "2016-08-15T21:54:16Z",
        "updatedAt" : "2016-08-17T08:43:38Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "f82924aa-fe60-4854-971b-75d6271c3095",
        "parentId" : "ebc9a398-c0fd-456c-91d7-0ff3d99eb2d3",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "For 20 Nodes with 1 Node per 100s it will take over 30 minutes to evict whole cluster. If you think it's too little I'd rather increase the large-cluster-threshold to 40 (it'll give an hour for full eviction) than to decrease rate drastically. This is actually a reason why I wanted to have 'time-to-drain' as a second flag here... @davidpp PTAL.\n",
        "createdAt" : "2016-08-16T13:16:43Z",
        "updatedAt" : "2016-08-17T08:43:38Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "8115d9ff-fb24-44e1-a315-91187b66e134",
        "parentId" : "ebc9a398-c0fd-456c-91d7-0ff3d99eb2d3",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I don't know... I mean, you probably never really want to evict the whole cluster, so 30 minutes is probably too short... I'm trying to think in terms of an ops team with a pager, what is a reasonable SLO for them to respond and stop the bleeding... But yeah, if you want to change large-cluster-threshold to 40 (or probably 50 would be better) instead of changing the rate, I think it's OK.\n",
        "createdAt" : "2016-08-17T07:52:20Z",
        "updatedAt" : "2016-08-17T08:43:38Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "e0dce418-7110-41d8-8a16-2fea7b814b70",
        "parentId" : "ebc9a398-c0fd-456c-91d7-0ff3d99eb2d3",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Made it 50.\n",
        "createdAt" : "2016-08-17T08:28:02Z",
        "updatedAt" : "2016-08-17T08:43:38Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cf698ef044ce52b283d620e82f2b99c7403d76c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +176,180 @@\tfs.Int32Var(&s.ConcurrentGCSyncs, \"concurrent-gc-syncs\", s.ConcurrentGCSyncs, \"The number of garbage collector workers that are allowed to sync concurrently.\")\n\tfs.Float32Var(&s.NodeEvictionRate, \"node-eviction-rate\", 0.1, \"Number of nodes per second on which pods are deleted in case of node failure when a zone is healthy (see --unhealthy-zone-threshold for definition of healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.\")\n\tfs.Float32Var(&s.SecondaryNodeEvictionRate, \"secondary-node-eviction-rate\", 0.01, \"Number of nodes per second on which pods are deleted in case of node failure when a zone is unhealthy (see --unhealthy-zone-threshold for definition of healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters. This value is implicitly overridden to 0 if the cluster size is smaller than --large-cluster-size-threshold.\")\n\tfs.Int32Var(&s.LargeClusterSizeThreshold, \"large-cluster-size-threshold\", 50, \"Number of nodes from which NodeController treats the cluster as large for the eviction logic purposes. --secondary-node-eviction-rate is implicitly overridden to 0 for clusters this size or smaller.\")\n\tfs.Float32Var(&s.UnhealthyZoneThreshold, \"unhealthy-zone-threshold\", 0.55, \"Fraction of Nodes in a zone which needs to be not Ready (minimum 3) for zone to be treated as unhealthy. \")"
  },
  {
    "id" : "175da316-f9a0-4997-9595-380f2e3067f9",
    "prId" : 23858,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d86b6ef-4260-4b35-bdec-a1bc0b480394",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "\"Workers\" instead of Syncs?\n",
        "createdAt" : "2016-04-05T18:29:22Z",
        "updatedAt" : "2016-06-27T18:05:30Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "83dd78e8-8384-4d0e-8446-1a2cd5e38045",
        "parentId" : "8d86b6ef-4260-4b35-bdec-a1bc0b480394",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I guess that would be inconsistent and I won't ask you to change them all. Sigh. \"Syncs\" is confusing.\n",
        "createdAt" : "2016-04-05T18:30:11Z",
        "updatedAt" : "2016-06-27T18:05:30Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "f1704af7-bf99-44c3-8230-8d23521f8fc6",
        "parentId" : "8d86b6ef-4260-4b35-bdec-a1bc0b480394",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "agree, and agree\n",
        "createdAt" : "2016-04-05T18:31:10Z",
        "updatedAt" : "2016-06-27T18:05:30Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f45d9dc2f8339e649262ae760fabc1e8486ab2d9",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +54,58 @@\t\t\tConcurrentDeploymentSyncs:         5,\n\t\t\tConcurrentNamespaceSyncs:          2,\n\t\t\tConcurrentSATokenSyncs:            5,\n\t\t\tLookupCacheSizeForRC:              4096,\n\t\t\tLookupCacheSizeForRS:              4096,"
  },
  {
    "id" : "1708f5c1-0509-446f-93ab-0dd1b58a354f",
    "prId" : 21870,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9d9a520-cfbe-49b0-8436-e329b0b94ee2",
        "parentId" : null,
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "Should we rename \"--rc-lookup-cache-size\" as \"--replication-controller-lookup-cache-size\", and rename \"--rs-lookup-cache-size\" as  \"--replicaset-lookup-cache-size\", It seems that we don't use abbreviation  in flag, for example:\n\n```\n\"---concurrent-resource-quota-syncs\"\n\"---concurrent-deployment-syncs\"\n\"---concurrent-replicaset-syncs\"\n```\n\nWe use \"--concurrent_rc_syncs\" for compatibility reason, right?\n",
        "createdAt" : "2016-02-25T06:55:48Z",
        "updatedAt" : "2016-02-25T07:02:57Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      },
      {
        "id" : "b1e9d341-3068-4d75-9586-667e97caaccf",
        "parentId" : "e9d9a520-cfbe-49b0-8436-e329b0b94ee2",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "hmm - yeah, let's rename it until it's not too late :) thanks for noticing it\n",
        "createdAt" : "2016-02-25T07:16:04Z",
        "updatedAt" : "2016-02-25T07:16:04Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "e44e71ca87bb8bc4ec31db37ff67bb2eba85dd4b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +102,106 @@\tfs.IntVar(&s.ConcurrentNamespaceSyncs, \"concurrent-namespace-syncs\", s.ConcurrentNamespaceSyncs, \"The number of namespace objects that are allowed to sync concurrently. Larger number = more responsive namespace termination, but more CPU (and network) load\")\n\tfs.IntVar(&s.LookupCacheSizeForRC, \"rc-lookup-cache-size\", s.LookupCacheSizeForRC, \"The the size of lookup cache for replication controllers. Larger number = more responsive replica management, but more MEM load.\")\n\tfs.IntVar(&s.LookupCacheSizeForRS, \"rs-lookup-cache-size\", s.LookupCacheSizeForRS, \"The the size of lookup cache for replicatsets. Larger number = more responsive replica management, but more MEM load.\")\n\tfs.DurationVar(&s.ServiceSyncPeriod.Duration, \"service-sync-period\", s.ServiceSyncPeriod.Duration, \"The period for syncing services with their external load balancers\")\n\tfs.DurationVar(&s.NodeSyncPeriod.Duration, \"node-sync-period\", s.NodeSyncPeriod.Duration, \"\"+"
  }
]