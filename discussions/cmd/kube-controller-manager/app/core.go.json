[
  {
    "id" : "53a704d2-f450-4bbf-8c0f-68268710cc96",
    "prId" : 90439,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90439#pullrequestreview-446230343",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3569bb17-dffe-45f1-a239-2a4a154a5755",
        "parentId" : null,
        "authorId" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "body" : "`dualStack` will be `true` when the `--cluster-cidr` is configured as dual-stack (at least one from each IPFamily) regardless of the `IPv6DualStack` feature gate is enabled or not. So even `IPv6DualStack` feature gate is enabled by default, if the user does not set `--cluster-cidr` as dual-stack, we will not enter this `if` branch, `--node-cidr-mask-size` is valid as before. ",
        "createdAt" : "2020-07-10T08:47:25Z",
        "updatedAt" : "2020-07-10T08:47:25Z",
        "lastEditedBy" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec1efc3b79cd23ae0f90c065bb8cf7bf43f7d56a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +157,161 @@\n\tvar nodeCIDRMaskSizeIPv4, nodeCIDRMaskSizeIPv6 int\n\tif dualStack {\n\t\t// only --node-cidr-mask-size-ipv4 and --node-cidr-mask-size-ipv6 supported with dual stack clusters.\n\t\t// --node-cidr-mask-size flag is incompatible with dual stack clusters."
  },
  {
    "id" : "6453b55d-1530-4a80-835b-daf02010d150",
    "prId" : 82365,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82365#pullrequestreview-305964008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1671702a-3350-4bd0-8487-7e1c1c616869",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This should go do the podgc-related commit",
        "createdAt" : "2019-10-23T14:41:54Z",
        "updatedAt" : "2019-10-23T14:58:08Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "ce549e8c-c0be-41dd-9882-a8c6ad656996",
        "parentId" : "1671702a-3350-4bd0-8487-7e1c1c616869",
        "authorId" : "26e83e8e-0b56-4418-bc01-d562e5b0ea4b",
        "body" : "Done",
        "createdAt" : "2019-10-23T15:01:34Z",
        "updatedAt" : "2019-10-23T15:01:34Z",
        "lastEditedBy" : "26e83e8e-0b56-4418-bc01-d562e5b0ea4b",
        "tags" : [
        ]
      }
    ],
    "commit" : "39883f08bf02dd703d161400418d47d7b7ee4c32",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +348,352 @@\t\tctx.ClientBuilder.ClientOrDie(\"pod-garbage-collector\"),\n\t\tctx.InformerFactory.Core().V1().Pods(),\n\t\tctx.InformerFactory.Core().V1().Nodes(),\n\t\tint(ctx.ComponentConfig.PodGCController.TerminatedPodGCThreshold),\n\t).Run(ctx.Stop)"
  },
  {
    "id" : "8b6b262c-7270-4a2f-be1a-2323d2984a8c",
    "prId" : 81797,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81797#pullrequestreview-298267845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cd7eea5-621c-4cf5-9b8e-cf38c3754a9f",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "verify is complaining this should be up one line",
        "createdAt" : "2019-10-07T16:50:10Z",
        "updatedAt" : "2019-10-08T12:42:41Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c1487840bc77ffbf4fbbcc84a2c0263c84fd74fe",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +37,41 @@\t\"k8s.io/client-go/metadata\"\n\trestclient \"k8s.io/client-go/rest\"\n\t\"k8s.io/component-base/metrics/prometheus/ratelimiter\"\n\tcsitrans \"k8s.io/csi-translation-lib\"\n\t\"k8s.io/kubernetes/pkg/controller\""
  },
  {
    "id" : "7957c4b9-e375-43a9-95dd-6a2811e233c9",
    "prId" : 79993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79993#pullrequestreview-308643733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ec7baed-be8d-45a2-a132-0cdcf7e8580b",
        "parentId" : null,
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "what happen if the feature gate is not enabled? \r\nhow do I get `int(ctx.ComponentConfig.NodeIPAMController.NodeCIDRMaskSize),' and the cluster family?",
        "createdAt" : "2019-10-29T09:18:12Z",
        "updatedAt" : "2019-11-15T04:04:44Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "9d2604d8-1bc0-4533-a255-e43dbdbd4500",
        "parentId" : "4ec7baed-be8d-45a2-a132-0cdcf7e8580b",
        "authorId" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "body" : "The set node cidr mask sizes only sets the IPv4 mask and IPv6 masks that we'll use. The `getNodeCIDRMaskSizes` returns a slice of node cidr mask sizes to be used based on cluster family.",
        "createdAt" : "2019-10-29T16:13:10Z",
        "updatedAt" : "2019-11-15T04:04:44Z",
        "lastEditedBy" : "62eb404a-5fe6-4b29-afab-583b57ce8f19",
        "tags" : [
        ]
      }
    ],
    "commit" : "796faba4ac96db07194c4811585abe52fd682064",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +158,162 @@\n\tvar nodeCIDRMaskSizeIPv4, nodeCIDRMaskSizeIPv6 int\n\tif utilfeature.DefaultFeatureGate.Enabled(kubefeatures.IPv6DualStack) {\n\t\tnodeCIDRMaskSizeIPv4, nodeCIDRMaskSizeIPv6, err = setNodeCIDRMaskSizesDualStack(ctx.ComponentConfig.NodeIPAMController)\n\t} else {"
  },
  {
    "id" : "73c395fc-e7ae-4a28-9af6-9ccc6066375f",
    "prId" : 79993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79993#pullrequestreview-317581906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce98bcc6-7d2a-4507-88e2-a80f95253c4d",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "nit: this can be presized to the length of clusterCIDRs",
        "createdAt" : "2019-11-15T12:16:28Z",
        "updatedAt" : "2019-11-15T12:16:28Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "796faba4ac96db07194c4811585abe52fd682064",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +608,612 @@// sizes slice based on the cluster cidr slice\nfunc getNodeCIDRMaskSizes(clusterCIDRs []*net.IPNet, maskSizeIPv4, maskSizeIPv6 int) []int {\n\tnodeMaskCIDRs := []int{}\n\tfor _, clusterCIDR := range clusterCIDRs {\n\t\tif netutils.IsIPv6CIDR(clusterCIDR) {"
  },
  {
    "id" : "dc41739a-0c30-4d7c-91bc-46d08c7bfde8",
    "prId" : 78742,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78742#pullrequestreview-260395005",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f547bcb-019b-4037-9770-7b6692a5146d",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this is still passing in `ctx.GenericInformerFactory`, which is still using `dynamicInformers`, not metadata informers",
        "createdAt" : "2019-07-10T22:29:02Z",
        "updatedAt" : "2019-07-11T16:19:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "dc380a3c-8e39-4fbb-97d7-2c927ac842eb",
        "parentId" : "4f547bcb-019b-4037-9770-7b6692a5146d",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Renamed and replaced, added a comment about why the name changed.  No object count quota unit tests it looks like, may add those in a follow up (although the logic is super simple since it just observes existing object counts, doesn't even crack open the object).",
        "createdAt" : "2019-07-10T22:49:22Z",
        "updatedAt" : "2019-07-11T16:19:02Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "d631f9b7e9e9bec131d171a7a859455498fdeb49",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +438,442 @@\t\tctx.RESTMapper,\n\t\tdeletableResources,\n\t\tignoredResources,\n\t\tctx.ObjectOrMetadataInformerFactory,\n\t\tctx.InformersStarted,"
  },
  {
    "id" : "27b20b41-75b4-4dff-b40f-d3a3f8e8a11b",
    "prId" : 77994,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77994#pullrequestreview-239644091",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c49f1ce8-e7fb-4605-b35e-08a9ba63e91f",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Note for reviewers : ExpandController already had rbac permission to list/get storageclasses, so we don't need a authorization policy update.",
        "createdAt" : "2019-05-20T17:59:46Z",
        "updatedAt" : "2019-05-29T15:02:17Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "7563b4d01b5e5dde148f746ccb1ff47c5b5b9272",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +245,249 @@\t\t\tctx.InformerFactory.Core().V1().PersistentVolumeClaims(),\n\t\t\tctx.InformerFactory.Core().V1().PersistentVolumes(),\n\t\t\tctx.InformerFactory.Storage().V1().StorageClasses(),\n\t\t\tctx.Cloud,\n\t\t\tProbeExpandableVolumePlugins(ctx.ComponentConfig.PersistentVolumeBinderController.VolumeConfiguration))"
  },
  {
    "id" : "94bba165-f99a-4e88-bc99-f48a9562c3de",
    "prId" : 73337,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73337#pullrequestreview-199578886",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "291a59f5-5eb7-46cd-adeb-a09f74b9acb4",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "This name isn't obvious to me, but if Walter liked it I won't nit it to death. Next time something more obvious?   If you removed a context dependency for instance it may make it more obvious.  Based on where you split it, I'm guessing you didn't want to deal with the client built from config because you can't mock that.",
        "createdAt" : "2019-02-04T13:16:13Z",
        "updatedAt" : "2019-02-04T13:16:13Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6ab3cfc543bdef0f893cd25e66c74e5bded696e",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +336,340 @@}\n\nfunc startModifiedNamespaceController(ctx ControllerContext, namespaceKubeClient clientset.Interface, nsKubeconfig *restclient.Config) (http.Handler, bool, error) {\n\n\tdynamicClient, err := dynamic.NewForConfig(nsKubeconfig)"
  },
  {
    "id" : "f5a331ab-88e4-4034-ab30-beb91e297c35",
    "prId" : 67803,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67803#pullrequestreview-151571827",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66c3a46d-9130-49ce-8fdd-1836146e1d81",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Is it really a good idea to crashloop controller manager if this fails?",
        "createdAt" : "2018-08-31T20:47:26Z",
        "updatedAt" : "2018-08-31T21:07:59Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "d0043bad-2687-4839-94f4-bded4352a2d5",
        "parentId" : "66c3a46d-9130-49ce-8fdd-1836146e1d81",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "We could log an error here, but eventually when it is needed, it will fail. So I think, it's better to fail loudly at initialization if something is wrong, rather then at some point during runtime. And this seems like the standard pattern for config initialization for controllers. ",
        "createdAt" : "2018-08-31T23:24:19Z",
        "updatedAt" : "2018-08-31T23:24:19Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "fdeb895d25f21ad67ed4db488879532d40aae16e",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +195,199 @@\t\treturn nil, true, fmt.Errorf(\"Duration time must be greater than one second as set via command line option reconcile-sync-loop-period.\")\n\t}\n\tcsiClientConfig := ctx.ClientBuilder.ConfigOrDie(\"attachdetach-controller\")\n\t// csiClient works with CRDs that support json only\n\tcsiClientConfig.ContentType = \"application/json\""
  },
  {
    "id" : "26893bb5-991c-44f8-8a75-c0c832640af3",
    "prId" : 67803,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67803#pullrequestreview-165645528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13dab3b5-b49f-4e4f-a656-2904ce7f38e1",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "This shouldn't have been necessary.  Clients should have priority order content accept types which should result in json being served.  What happened when you didn't set this?",
        "createdAt" : "2018-10-16T16:15:30Z",
        "updatedAt" : "2018-10-16T16:15:30Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "765780c8-bf9a-4d92-b4b9-0aa938312cba",
        "parentId" : "13dab3b5-b49f-4e4f-a656-2904ce7f38e1",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "@liggitt wouldn't have expected this to show up in code using CRD",
        "createdAt" : "2018-10-16T16:15:42Z",
        "updatedAt" : "2018-10-16T16:15:43Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "9d284a12-57ac-422d-b111-ea35170f7390",
        "parentId" : "13dab3b5-b49f-4e4f-a656-2904ce7f38e1",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "looks like \r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/95c99eb052baba498dfb366bb1e43d2af6c51270/cmd/kube-controller-manager/app/options/options.go#L412\r\n\r\ndefaulted at \r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/95c99eb052baba498dfb366bb1e43d2af6c51270/staging/src/k8s.io/apimachinery/pkg/apis/config/v1alpha1/defaults.go#L29-L31\r\n\r\nI'm not seeing a fallback specified\r\n\r\n",
        "createdAt" : "2018-10-16T16:21:56Z",
        "updatedAt" : "2018-10-16T16:21:56Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "65efec5e-be8a-469d-a1cf-a29b2813e44a",
        "parentId" : "13dab3b5-b49f-4e4f-a656-2904ce7f38e1",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "> What happened when you didn't set this?\r\n\r\n`Watch()` did not work. It received protobuf and failed to parse it.",
        "createdAt" : "2018-10-17T11:53:59Z",
        "updatedAt" : "2018-10-17T11:53:59Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "59a096d9-cecb-4d73-a05c-dd54a744362c",
        "parentId" : "13dab3b5-b49f-4e4f-a656-2904ce7f38e1",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "is this related to https://github.com/kubernetes/kubernetes/pull/62175?",
        "createdAt" : "2018-10-17T14:00:09Z",
        "updatedAt" : "2018-10-17T14:00:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "fdeb895d25f21ad67ed4db488879532d40aae16e",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +197,201 @@\tcsiClientConfig := ctx.ClientBuilder.ConfigOrDie(\"attachdetach-controller\")\n\t// csiClient works with CRDs that support json only\n\tcsiClientConfig.ContentType = \"application/json\"\n\n\tcrdClientConfig := ctx.ClientBuilder.ConfigOrDie(\"attachdetach-controller\")"
  },
  {
    "id" : "636c5f8e-349c-41c1-ba74-f9af17ff41ce",
    "prId" : 63049,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63049#pullrequestreview-118125867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d1a3ac6-a307-4aad-ab15-a15e61320f2b",
        "parentId" : null,
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "I think I'm missing something. This seems to be disabling the node ipam controller on the KCM if \"--allocate-node-cidrs\" isn't set (https://github.com/kubernetes/kubernetes/blob/master/cmd/controller-manager/app/options/kubecloudshared.go#L57). GCE and local both explicitly set this flag to true. I also know kubeadm sets it to true based on \"Networking.PodSubnet\" not being null. (Not sure for which cloud providers that will be true).  My understanding is that our goal (eventual) is to have node ipam running on the KCM for all the providers except GCE. For GCE running the CCM, node ipam should be disabled on the KCM. \r\n\r\nI generally like this clean up but I don't think it belongs with re-enabling nodeipam in the KCM. (I would like to be able to revert one without the other) Also I don't think it meets my concerns about being able to properly control whether nodeipam is running in the KCM or CCM. (That can be handled later and is not necessarily blocking)",
        "createdAt" : "2018-04-25T23:33:14Z",
        "updatedAt" : "2018-04-25T23:56:46Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "c534e7ae-406a-4e12-997a-e7749f8254b2",
        "parentId" : "0d1a3ac6-a307-4aad-ab15-a15e61320f2b",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "> My understanding is that our goal (eventual) is to have node ipam running on the KCM for all the providers except GCE. For GCE running the CCM, node ipam should be disabled on the KCM.\r\n\r\nYup, this change would allow us to do that. Set `--allocate-node-cidr` if you want nodeipam on KCM (right now this is broken if you set `--cloud-provider=external` on KCM), if you are running GCE setup with CCM, make sure you set `--cloud-provider=external` and unset `--allocate-node-cidr` on KCM.",
        "createdAt" : "2018-04-26T14:13:46Z",
        "updatedAt" : "2018-04-26T14:13:51Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "85b315c7-84ef-4fc0-8172-0df4f0149238",
        "parentId" : "0d1a3ac6-a307-4aad-ab15-a15e61320f2b",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "> I generally like this clean up but I don't think it belongs with re-enabling nodeipam in the KCM. (I would like to be able to revert one without the other) Also I don't think it meets my concerns about being able to properly control whether nodeipam is running in the KCM or CCM. (That can be handled later and is not necessarily blocking) \r\n\r\nIf you prefer, I can simplify the PR by only including [this](https://github.com/kubernetes/kubernetes/pull/63049/files#diff-2aa9cfe6a01016c1679752b0fcfc9244R333) change. But functionally it would be the same thing since nodeipam will do nothing if `--allocate-node-cidr` is not set and from GCE you would still have to unset `--allocate-node-cidr`. The nice thing about this change is that we would [explicity log that nodeipam controller was skipped](https://github.com/kubernetes/kubernetes/blob/master/cmd/kube-controller-manager/app/controllermanager.go#L445). ",
        "createdAt" : "2018-04-26T14:16:13Z",
        "updatedAt" : "2018-04-26T14:16:36Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "6d1be1db-57c9-4ffc-982b-0532f160012a",
        "parentId" : "0d1a3ac6-a307-4aad-ab15-a15e61320f2b",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "I guess I'm fine for now. However I think we need to get away from conflating meaning on configuration flags. I believe at some point will want to be able to have allocate-node-cidrs set to true and disable this controller on the KCM. Especially if we move forward on component config (https://github.com/kubernetes/features/issues/115).",
        "createdAt" : "2018-05-07T19:48:14Z",
        "updatedAt" : "2018-05-07T19:48:14Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a164760dc64c710c509a2358341eab282095444",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +84,88 @@\tvar serviceCIDR *net.IPNet = nil\n\n\tif !ctx.ComponentConfig.KubeCloudShared.AllocateNodeCIDRs {\n\t\treturn false, nil\n\t}"
  },
  {
    "id" : "d782b5aa-97aa-4737-b458-d340dbc3dd79",
    "prId" : 62990,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/62990#pullrequestreview-117711331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d546c6c-b8f2-4e2c-bd5e-42e19f92fd2c",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "You need to call invalidate in the GCE sync loop because on this because it is a different instance the one used for the RESTMapper.",
        "createdAt" : "2018-04-26T16:25:00Z",
        "updatedAt" : "2018-04-28T01:59:04Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "b4e12576-ab77-48f3-8c81-e5aa5d068fc8",
        "parentId" : "6d546c6c-b8f2-4e2c-bd5e-42e19f92fd2c",
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "I am confused, seems this discoveryClient  is only used once by `\tdeletableResources := garbagecollector.GetDeletableResources(discoveryClient)`.  It is not used in sync loop, so keep it as it is now seems ok. What do I miss?",
        "createdAt" : "2018-04-27T01:38:09Z",
        "updatedAt" : "2018-04-28T01:59:04Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      },
      {
        "id" : "fb42e008-4abc-4c07-ae31-374f7e06403e",
        "parentId" : "6d546c6c-b8f2-4e2c-bd5e-42e19f92fd2c",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "My mistake",
        "createdAt" : "2018-05-04T18:18:09Z",
        "updatedAt" : "2018-05-04T18:18:09Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f93d11f9e202bd91bfcc471f538ee806588e325",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +348,352 @@\n\tgcClientset := ctx.ClientBuilder.ClientOrDie(\"generic-garbage-collector\")\n\tdiscoveryClient := cacheddiscovery.NewMemCacheClient(gcClientset.Discovery())\n\n\tconfig := ctx.ClientBuilder.ConfigOrDie(\"generic-garbage-collector\")"
  },
  {
    "id" : "3948182b-de26-4573-b080-48dd802c8789",
    "prId" : 61324,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61324#pullrequestreview-114050410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7cb1d04-219a-45bb-89a5-dac10c094710",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "Yeah, this is ok for now.  Eventually we'll want to push it back into the context/config, but this a fair point to start.",
        "createdAt" : "2018-04-20T16:49:11Z",
        "updatedAt" : "2018-04-20T17:55:09Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3ddf7eb8b7700b4891dcf69b75f998fd85f3cca",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +400,404 @@\t\tctx.InformerFactory.Core().V1().Pods(),\n\t\tctx.ClientBuilder.ClientOrDie(\"pvc-protection-controller\"),\n\t\tutilfeature.DefaultFeatureGate.Enabled(features.StorageObjectInUseProtection),\n\t).Run(1, ctx.Stop)\n\treturn true, nil"
  },
  {
    "id" : "0b470bed-96d5-41a6-8a0a-133dd8f563ec",
    "prId" : 54320,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54320#pullrequestreview-71664167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f78868fc-8727-4249-9f91-5bfb13f70b56",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "Add yourself to this issue: https://github.com/kubernetes/kubernetes/issues/54084 . We need a shared one on the context.",
        "createdAt" : "2017-10-24T15:11:22Z",
        "updatedAt" : "2017-10-27T15:08:48Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "7f397211-dc7b-42ec-b340-738e4daf01df",
        "parentId" : "f78868fc-8727-4249-9f91-5bfb13f70b56",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "done",
        "createdAt" : "2017-10-24T20:38:31Z",
        "updatedAt" : "2017-10-27T15:08:48Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9765bcebe77ce4e62dcbdacaa5bb0910220cef0",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +241,245 @@func startResourceQuotaController(ctx ControllerContext) (bool, error) {\n\tresourceQuotaControllerClient := ctx.ClientBuilder.ClientOrDie(\"resourcequota-controller\")\n\tdiscoveryFunc := resourceQuotaControllerClient.Discovery().ServerPreferredNamespacedResources\n\tlisterFuncForResource := generic.ListerFuncForResourceFunc(ctx.InformerFactory.ForResource)\n\tquotaConfiguration := quotainstall.NewQuotaConfigurationForControllers(listerFuncForResource)"
  },
  {
    "id" : "62928a07-c3f0-4132-a2b2-7b6d245a01d8",
    "prId" : 49727,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49727#pullrequestreview-60189487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46fe1866-275b-4c83-853e-935df0753dde",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "does this need a peer to `GetDynamicPluginProber(ctx.Options.VolumeConfiguration)`?",
        "createdAt" : "2017-09-01T04:23:32Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "25848e19-2dc8-438e-a357-302c7deb351a",
        "parentId" : "46fe1866-275b-4c83-853e-935df0753dde",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "We don't need `GetDynamicPluginProber` for this controller (at least for now), because currently only flex volumes are dynamically probed and they are not resizable. ",
        "createdAt" : "2017-09-01T12:03:38Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "8f41f159-0b79-4151-ae7b-74415089ff15",
        "parentId" : "46fe1866-275b-4c83-853e-935df0753dde",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "> currently only flex volumes are dynamically probed and they are not resizable.\r\n\r\nwhy not? keeping parity in our out-of-tree volume mechanisms is important",
        "createdAt" : "2017-09-01T14:21:35Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "d40aafce-0d06-4fe1-849b-b3f10800bcde",
        "parentId" : "46fe1866-275b-4c83-853e-935df0753dde",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "For resizing to work for Flex volumes, we will have to add expand/resize API calls to flex protocol. We are yet to be in a place where flex volumes can be called stable. Attach/Detach interface was added recently to flex and has issues that we are trying to workout - https://github.com/kubernetes/kubernetes/issues/44737. sig-storage is also working on making flex plugin deployments saner. \r\n\r\nIn a nutshell, I would like to pursue flex resizing as separate problem. Several community members have expressed an interest in solving it. There is a separate feature request open for it- https://github.com/kubernetes/features/issues/304 ",
        "createdAt" : "2017-09-01T14:33:23Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "1ba24ec3-f185-4d7c-961a-27cb86c5d92b",
        "parentId" : "46fe1866-275b-4c83-853e-935df0753dde",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "But just to clarify current resizing design does not inhibit a future flex resize design. As long as, protocols/API calls laid out by this design are implemented by flex - we will have working resizing. So it all comes down to implementing volume plugin interface necessary for resizing. ",
        "createdAt" : "2017-09-01T14:36:57Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "b64c8bca-54ac-475a-92cc-0c0c7b5a750d",
        "parentId" : "46fe1866-275b-4c83-853e-935df0753dde",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "understood. seems like that should tracked as part of graduating this to beta.",
        "createdAt" : "2017-09-01T14:37:19Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "2254979f-f045-4853-a690-2d7e80cfb523",
        "parentId" : "46fe1866-275b-4c83-853e-935df0753dde",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "ack. I will make a note of this in original design proposal. ",
        "createdAt" : "2017-09-01T16:56:45Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "84029c2c1a798bab26781c0219186b278ad69af7",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +198,202 @@\t\t\tctx.InformerFactory.Core().V1().PersistentVolumes(),\n\t\t\tctx.Cloud,\n\t\t\tProbeExpandableVolumePlugins(ctx.Options.VolumeConfiguration))\n\n\t\tif expandControllerErr != nil {"
  },
  {
    "id" : "7b8dbc43-0d0d-45de-b96c-5c7d03f4599d",
    "prId" : 47731,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/47731#pullrequestreview-45078179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34c6e9ff-4493-46e0-a4da-aa4e9ee9fc10",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I didn't realize we weren't watching endpoints before - given that this controller is expected to know and mutate every endpoint, I think this is exactly what we should be doing.  It will have a memory impact, but will drop a significant chunk of QPS from the apiservers.\r\n\r\n@wojtek-t @gmarek may change our kube-mark profiles, but I would expect it to be in a positive way.",
        "createdAt" : "2017-06-19T17:32:50Z",
        "updatedAt" : "2017-06-27T09:17:15Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "74885ebf-f792-461b-af94-bbfb93de96e0",
        "parentId" : "34c6e9ff-4493-46e0-a4da-aa4e9ee9fc10",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I agree.",
        "createdAt" : "2017-06-19T20:19:56Z",
        "updatedAt" : "2017-06-27T09:17:15Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "8dedcf48-b48f-4147-8806-c4a3abcaf4fa",
        "parentId" : "34c6e9ff-4493-46e0-a4da-aa4e9ee9fc10",
        "authorId" : "c135d2c5-f879-4989-b899-96610cfb9026",
        "body" : "QPS drops substantially in my testing. I also don't see spurious updates I was seeing before (during a resync, about 20% of the endpoints would cause a PUT even though nothing changed - not sure why).",
        "createdAt" : "2017-06-20T09:23:06Z",
        "updatedAt" : "2017-06-27T09:17:15Z",
        "lastEditedBy" : "c135d2c5-f879-4989-b899-96610cfb9026",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fc5a547aeeb552f48a588f59c02eb76fc4d3262",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +179,183 @@\t\tctx.InformerFactory.Core().V1().Pods(),\n\t\tctx.InformerFactory.Core().V1().Services(),\n\t\tctx.InformerFactory.Core().V1().Endpoints(),\n\t\tctx.ClientBuilder.ClientOrDie(\"endpoint-controller\"),\n\t).Run(int(ctx.Options.ConcurrentEndpointSyncs), ctx.Stop)"
  },
  {
    "id" : "4804e1f4-5498-4ebe-8aac-070c81d51211",
    "prId" : 47665,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/47665#pullrequestreview-52652690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "253cdb13-aeaf-4a97-ad48-849b28d92bcf",
        "parentId" : null,
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "Could you open an issue to track it?",
        "createdAt" : "2017-07-27T00:19:12Z",
        "updatedAt" : "2017-07-28T14:00:27Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      },
      {
        "id" : "ebfcacf4-3282-4297-b8a8-36b1eabc940e",
        "parentId" : "253cdb13-aeaf-4a97-ad48-849b28d92bcf",
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "Opened https://github.com/kubernetes/kubernetes/issues/49718",
        "createdAt" : "2017-07-27T13:21:26Z",
        "updatedAt" : "2017-07-28T14:00:27Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      }
    ],
    "commit" : "d08dfb92c71960dba663f7a8e6465a93ddf5c560",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +306,310 @@\tconfig := ctx.ClientBuilder.ConfigOrDie(\"generic-garbage-collector\")\n\tconfig.ContentConfig = dynamic.ContentConfig()\n\t// TODO: Make NewMetadataCodecFactory support arbitrary (non-compiled)\n\t// resource types. Otherwise we'll be storing full Unstructured data in our\n\t// caches for custom resources. Consider porting it to work with"
  },
  {
    "id" : "bf894dc1-0d25-4148-9a19-bb109ddc8e82",
    "prId" : 102981,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102981#pullrequestreview-702729783",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b670e706-534d-4fca-8ce4-498ba568a425",
        "parentId" : null,
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "Thanks for taking care of this TODO. It never made it into tracking list of opens for the feature and then of course was promptly forgotten - my bad.\r\n\r\nMaking it configurable makes sense. The rest seems to be mostly cut-and-paste from how it is done elsewhere and looks good to me.\r\n\r\n/lgtm\r\n",
        "createdAt" : "2021-07-09T06:44:54Z",
        "updatedAt" : "2021-07-09T06:44:55Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      }
    ],
    "commit" : "7fa0b9b6c104d076bc3fe617e2e26baec495ff84",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +403,407 @@\t\t}\n\t\tgo ephemeralController.Run(int(ctx.ComponentConfig.EphemeralVolumeController.ConcurrentEphemeralVolumeSyncs), ctx.Stop)\n\t\treturn nil, true, nil\n\t}\n\treturn nil, false, nil"
  }
]