[
  {
    "id" : "92535434-77ee-4068-bf24-84863d50ccdd",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65f5ebad-f4b0-45ed-895f-b564157212f9",
        "parentId" : null,
        "authorId" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "body" : "not necessarily part of this PR but I think we should have a unified way to deal with textual fields as per declaring it: either varchar/character varying or text.",
        "createdAt" : "2021-05-17T14:09:41Z",
        "updatedAt" : "2021-05-17T15:04:49Z",
        "lastEditedBy" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +50,54 @@\n    -- * transaction metadata\n    command_id text,\n    workflow_id text,\n    application_id text,"
  },
  {
    "id" : "059ed8b3-9fbc-4a0a-b3c0-beaa5bd32856",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a642f7df-1cfa-4754-9f0a-5e122ce7ac3e",
        "parentId" : null,
        "authorId" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "body" : "I am trying to accommodate @gerolf-da -s request regarding observe-ability of the migration here: how can this possibly long going migration be more granual?\r\n* could we put separate sequential steps into subsequent migrations?\r\n* can we somehow emit more logs from Flyway so we see what it is doing?\r\n",
        "createdAt" : "2021-05-17T14:12:17Z",
        "updatedAt" : "2021-05-17T15:04:49Z",
        "lastEditedBy" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "tags" : [
        ]
      },
      {
        "id" : "3536e055-0b20-4568-8563-d93adb02715a",
        "parentId" : "a642f7df-1cfa-4754-9f0a-5e122ce7ac3e",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "> could we put separate sequential steps into subsequent migrations?\r\n\r\nWe probably could, but why? The participant would have to run all of these migrations in a row anyway.\r\n\r\n> can we somehow emit more logs from Flyway so we see what it is doing?\r\n\r\nThe flyway log output is actually quite verbose. When enabled, it prints each individual SQL statement it executes, along with the number of rows updated by that statement. Is there something else we would want to log?",
        "createdAt" : "2021-05-17T20:50:42Z",
        "updatedAt" : "2021-05-17T20:50:42Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      },
      {
        "id" : "d68f2642-ddeb-4664-9ae0-fe58316b22a4",
        "parentId" : "a642f7df-1cfa-4754-9f0a-5e122ce7ac3e",
        "authorId" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "body" : "To be clear: this is exclusively about logging, and how clients can observe a possibly long-running migration.\r\nThe flyway log output is just one line per migrations for me. If we turn that to debug, probably then will print every statement, no? (which is also sufficient for logging purposes I guess, but also wonder whether it is needed)",
        "createdAt" : "2021-05-17T21:28:30Z",
        "updatedAt" : "2021-05-17T21:28:30Z",
        "lastEditedBy" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "tags" : [
        ]
      },
      {
        "id" : "8b650fa0-f3d2-4341-a5d3-ae88c3038d9c",
        "parentId" : "a642f7df-1cfa-4754-9f0a-5e122ce7ac3e",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "> how clients can observe a possibly long-running migration\r\n\r\nIf this is about troubleshooting issues, then IMO we need to log individual SQL statements. If it's about showing progress with only a few lines of log output, then we would have to split the script into at least 6: schema changes, then the 4 statements that do data migration each in its own script (those will be the slow ones), then some more schema changes. Do you think this is worth the effort?",
        "createdAt" : "2021-05-18T21:20:02Z",
        "updatedAt" : "2021-05-18T21:20:02Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      },
      {
        "id" : "0d8dcfe2-89d0-465a-8c7d-b63858c0edc6",
        "parentId" : "a642f7df-1cfa-4754-9f0a-5e122ce7ac3e",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "Left for follow up PR.",
        "createdAt" : "2021-05-20T19:33:49Z",
        "updatedAt" : "2021-05-20T19:33:49Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +317,321 @@-- Insert all create events and use the participant_contracts table\n-- to fill in the create_key_hash for _active_ contracts.\nINSERT INTO participant_events_create\n(\n    event_sequential_id,"
  },
  {
    "id" : "7444aae5-dcac-42d9-8f59-b7fca18223b6",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9374076-233b-4128-a799-173c1358c84f",
        "parentId" : null,
        "authorId" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "body" : "ah nice, you simply repurpose the original, clever ;) ",
        "createdAt" : "2021-05-17T14:28:51Z",
        "updatedAt" : "2021-05-17T15:04:49Z",
        "lastEditedBy" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "tags" : [
        ]
      },
      {
        "id" : "47d36270-1204-4ca2-b554-63fb0bffd3d3",
        "parentId" : "d9374076-233b-4128-a799-173c1358c84f",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "This way all existing queries keep working. We can (and probably should) optimize some of the queries to use `select from participant_events_divulgence` instead of `select from participant_events where event_kind = 0` (and similar for other partitions). ",
        "createdAt" : "2021-05-17T19:50:24Z",
        "updatedAt" : "2021-05-17T19:50:24Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 357,
    "diffHunk" : "@@ -1,1 +555,559 @@-- TODO append-only: EITHER only include columns that are used in queries that use this view OR verify that the query planning\n-- is not negatively affected by a long list of columns that are never used.\nCREATE VIEW participant_events\nAS\nSELECT"
  },
  {
    "id" : "eda985af-e5fa-4e69-b899-b32e57823ef3",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b576d35-8ae0-4446-b438-8fe606082a38",
        "parentId" : null,
        "authorId" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "body" : "I could not find this original TODO here: `TODO append-only: reorder small fields to the end to avoid unnecessary padding.` can it be that it is missing?\r\nHow about adding this TODO to all \"partitions\"?",
        "createdAt" : "2021-05-17T14:32:11Z",
        "updatedAt" : "2021-05-17T15:04:49Z",
        "lastEditedBy" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "tags" : [
        ]
      },
      {
        "id" : "a670deec-d796-447a-af3e-f63c2b4f7c8e",
        "parentId" : "4b576d35-8ae0-4446-b438-8fe606082a38",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "Good catch, I'll add them.",
        "createdAt" : "2021-05-17T20:52:42Z",
        "updatedAt" : "2021-05-17T20:52:42Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +42,46 @@---------------------------------------------------------------------------------------------------\n-- TODO append-only: reorder small fields to the end to avoid unnecessary padding.\nCREATE TABLE participant_events_divulgence (\n    -- * event identification\n    event_sequential_id bigint NOT NULL,"
  },
  {
    "id" : "a8e5ec0d-5677-41b1-9efe-b8f36f347b8a",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cadf7bcb-8b95-4180-84b0-98d800b58b01",
        "parentId" : null,
        "authorId" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "body" : "just to make absolutely sure: my current understanding is that with this on main we will start doing migration tests, but we do not support append-only for production usage/data continuity yet, right? cc @mziolekda @rautenrieth-da \r\nThis is important to know: we can still change this file as going onwards (some refactorings to the schema would be much better looking if we \"squash\" them to this sql script, instead of subsequent ones)",
        "createdAt" : "2021-05-17T15:02:34Z",
        "updatedAt" : "2021-05-17T15:04:49Z",
        "lastEditedBy" : "940afe62-a033-4e56-b2d3-8de3ca6d6d5c",
        "tags" : [
        ]
      },
      {
        "id" : "6c9158f6-5d95-4d57-8164-1afb948c5929",
        "parentId" : "cadf7bcb-8b95-4180-84b0-98d800b58b01",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "The command line flag to enable the append-only schema is still hidden and we have not advertised it. I would not be comfortable declaring the migration production ready right now, without more manual tests. It is my understanding that we can still change this file.",
        "createdAt" : "2021-05-17T19:46:16Z",
        "updatedAt" : "2021-05-17T19:46:16Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +4,8 @@\n---------------------------------------------------------------------------------------------------\n-- V100: Append-only schema\n--\n-- This is a major redesign of the index database schema. Updates from the ReadService are"
  },
  {
    "id" : "f9227cdb-8813-41c7-9592-e72a22aff84f",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "250e2ff2-f2e3-4f17-b78b-3ea27a573838",
        "parentId" : null,
        "authorId" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "body" : "@rautenrieth-da : I recommend dropping the indices before the migration and recreating them afterwards, as for large DB's this is going make a large difference in runtime.",
        "createdAt" : "2021-05-19T06:48:02Z",
        "updatedAt" : "2021-05-19T07:05:57Z",
        "lastEditedBy" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "tags" : [
        ]
      },
      {
        "id" : "ce6127cf-86da-4169-ada9-b8b6eec1033e",
        "parentId" : "250e2ff2-f2e3-4f17-b78b-3ea27a573838",
        "authorId" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "body" : "This holds for all tables.",
        "createdAt" : "2021-05-19T06:48:37Z",
        "updatedAt" : "2021-05-19T07:05:57Z",
        "lastEditedBy" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "tags" : [
        ]
      },
      {
        "id" : "6fcd5c26-20bc-4488-9c88-97abe281aeb0",
        "parentId" : "250e2ff2-f2e3-4f17-b78b-3ea27a573838",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "Great suggestion, left for follow up PR.",
        "createdAt" : "2021-05-20T19:33:33Z",
        "updatedAt" : "2021-05-20T19:33:33Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +367,371 @@FROM participant_events LEFT JOIN participant_contracts USING (contract_id)\nWHERE participant_events.exercise_consuming IS NULL -- create events\nORDER BY participant_events.event_sequential_id;\n\n-- Insert all consuming exercise events"
  },
  {
    "id" : "6174a0a0-c22e-451c-ae4d-b4b77c2723f3",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fb7e5b31-0af3-4c23-a2c2-963c89192bdd",
        "parentId" : null,
        "authorId" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "body" : "@rautenrieth-da : did you check the execution plan that this won't run the inner query for every event?\r\n\r\nYou could move it out to its own definition in the `WITH` clause so it is marked as a Common-Table-Expression.",
        "createdAt" : "2021-05-19T06:52:48Z",
        "updatedAt" : "2021-05-19T07:05:57Z",
        "lastEditedBy" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "tags" : [
        ]
      },
      {
        "id" : "9b14e67f-53c1-437b-a252-3503d781f39b",
        "parentId" : "fb7e5b31-0af3-4c23-a2c2-963c89192bdd",
        "authorId" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "body" : "I did not check this before. The following is the `EXPLAIN ANALYZE` on this statement, run on a database produced by the ledger api test tool (which has very few divulgence events, and is not representative for real index databases):\r\n\r\n```\r\n   CTE divulged_contracts\r\n     ->  HashAggregate  (cost=242.47..247.53 rows=289 width=107) (actual time=2.056..2.071 rows=3 loops=1)\r\n           Group Key: participant_contract_witnesses.contract_id\r\n           InitPlan 2 (returns $1)\r\n             ->  Result  (cost=0.31..0.32 rows=1 width=8) (actual time=0.023..0.024 rows=1 loops=1)\r\n                   InitPlan 1 (returns $0)\r\n                     ->  Limit  (cost=0.28..0.31 rows=1 width=8) (actual time=0.019..0.020 rows=1 loops=1)\r\n                           ->  Index Only Scan Backward using participant_events_event_sequential_id on participant_events  (cost=0.28..37.51 rows=985 width=8) (actual time=0.019..0.019 rows=1 loops=1)\r\n                                 Index Cond: (event_sequential_id IS NOT NULL)\r\n                                 Heap Fetches: 1\r\n           ->  Hash Join  (cost=53.37..240.25 rows=379 width=120) (actual time=0.551..1.973 rows=3 loops=1)\r\n                 Hash Cond: ((participant_contract_witnesses.contract_id)::text = (participant_contracts_1.contract_id)::text)\r\n                 Join Filter: (NOT (participant_contracts_1.create_stakeholders @> ARRAY[participant_contract_witnesses.contract_witness]))\r\n                 ->  Hash Right Join  (cost=22.87..208.73 rows=381 width=120) (actual time=0.281..1.700 rows=3 loops=1)\r\n                       Hash Cond: ((participant_events_1.contract_id)::text = (participant_contract_witnesses.contract_id)::text)\r\n                       Filter: ((participant_events_1.exercise_consuming IS NULL) AND ((participant_events_1.tree_event_witnesses IS NULL) OR (NOT (participant_events_1.tree_event_witnesses @> ARRAY[participant_contract_witnesses.contract_witness]))))\r\n                       Rows Removed by Filter: 538\r\n                       ->  Seq Scan on participant_events participant_events_1  (cost=0.00..171.85 rows=985 width=162) (actual time=0.008..1.132 rows=1041 loops=1)\r\n                       ->  Hash  (cost=17.94..17.94 rows=394 width=120) (actual time=0.214..0.215 rows=424 loops=1)\r\n                             Buckets: 1024  Batches: 1  Memory Usage: 72kB\r\n                             ->  Seq Scan on participant_contract_witnesses  (cost=0.00..17.94 rows=394 width=120) (actual time=0.011..0.090 rows=424 loops=1)\r\n                 ->  Hash  (cost=26.89..26.89 rows=289 width=165) (actual time=0.256..0.256 rows=319 loops=1)\r\n                       Buckets: 1024  Batches: 1  Memory Usage: 70kB\r\n                       ->  Seq Scan on participant_contracts participant_contracts_1  (cost=0.00..26.89 rows=289 width=165) (actual time=0.003..0.168 rows=319 loops=1)\r\n   ->  Hash Join  (cost=30.50..37.05 rows=289 width=358) (actual time=2.257..2.277 rows=3 loops=1)\r\n         Hash Cond: ((divulged_contracts.contract_id)::text = (participant_contracts.contract_id)::text)\r\n         ->  CTE Scan on divulged_contracts  (cost=0.00..5.78 rows=289 width=72) (actual time=2.057..2.072 rows=3 loops=1)\r\n         ->  Hash  (cost=26.89..26.89 rows=289 width=241) (actual time=0.183..0.183 rows=319 loops=1)\r\n               Buckets: 1024  Batches: 1  Memory Usage: 94kB\r\n               ->  Seq Scan on participant_contracts  (cost=0.00..26.89 rows=289 width=241) (actual time=0.006..0.075 rows=319 loops=1)\r\n```\r\n\r\nI don't have enough experience with query plans, but the `InitPlan 1` seems to show that the subquery is only run once.",
        "createdAt" : "2021-05-19T08:03:33Z",
        "updatedAt" : "2021-05-19T08:03:33Z",
        "lastEditedBy" : "c83a17a2-0c0d-4811-9fe5-e560bb3ad87a",
        "tags" : [
        ]
      },
      {
        "id" : "1ce2efe7-03b7-4b99-8da0-6bc9bd193fe4",
        "parentId" : "fb7e5b31-0af3-4c23-a2c2-963c89192bdd",
        "authorId" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "body" : "Cool.Thanks for checking. yes this looks right.",
        "createdAt" : "2021-05-19T11:09:55Z",
        "updatedAt" : "2021-05-19T11:09:55Z",
        "lastEditedBy" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 299,
    "diffHunk" : "@@ -1,1 +490,494 @@WITH divulged_contracts AS (\n    SELECT nextval('temp_divulgence_sequential_id') +\n           (SELECT coalesce(max(event_sequential_id), 0) FROM participant_events) as event_sequential_id,\n           contract_id,\n           array_agg(contract_witness) as divulgees"
  },
  {
    "id" : "16eaaef0-bcb3-4975-8ef9-490d73eb06e2",
    "prId" : 9481,
    "prUrl" : "https://github.com/digital-asset/daml/pull/9481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b210c869-cab1-4ace-9beb-5fb0a4446d97",
        "parentId" : null,
        "authorId" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "body" : "@rautenrieth-da : why do we need to check the `tree_event_witnesses`? Shouldn't a divulgence event for all non-stakeholder witnesses of an active contract suffice?",
        "createdAt" : "2021-05-19T06:59:01Z",
        "updatedAt" : "2021-05-19T07:05:57Z",
        "lastEditedBy" : "ba0b5f2d-78e4-41a5-94a1-d55d6f19f728",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c7762ebfcd194cb99cedfb89235e21d7f28c93",
    "line" : 307,
    "diffHunk" : "@@ -1,1 +498,502 @@    WHERE (NOT create_stakeholders @> array [contract_witness])\n      AND exercise_consuming IS NULL -- create events only\n      AND (tree_event_witnesses IS NULL OR NOT tree_event_witnesses @> array [contract_witness])\n    GROUP BY contract_id\n) INSERT INTO participant_events_divulgence ("
  }
]