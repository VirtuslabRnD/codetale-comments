[
  {
    "id" : "872f026b-fe47-47fa-9238-a6716c16cdd9",
    "prId" : 9352,
    "prUrl" : "https://github.com/apache/kafka/pull/9352#pullrequestreview-506968482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3804d645-bb5d-4738-bfbc-1eba093dab32",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just to clarify this is not a correctness bugfix, but just to optimize away unnecessary purgatory access right?",
        "createdAt" : "2020-10-12T22:35:57Z",
        "updatedAt" : "2020-10-13T00:38:58Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "ed5f35e2-b250-4789-a768-cf39466b8964",
        "parentId" : "3804d645-bb5d-4738-bfbc-1eba093dab32",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, right. Also to avoid the log spam when the high watermark doesn't actually increment.",
        "createdAt" : "2020-10-12T23:08:33Z",
        "updatedAt" : "2020-10-13T00:38:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "bceb07ff9233d969d13108c3d15fefbe665ed873",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +194,198 @@        highWatermarkOpt.ifPresent(highWatermark -> {\n            long newHighWatermark = Math.min(endOffset().offset, highWatermark);\n            if (state.updateHighWatermark(OptionalLong.of(newHighWatermark))) {\n                updateHighWatermark(state, currentTimeMs);\n            }"
  },
  {
    "id" : "7021160c-96d7-481a-9571-30c883bf3efa",
    "prId" : 9418,
    "prUrl" : "https://github.com/apache/kafka/pull/9418#pullrequestreview-514199988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66b24ed4-7f17-4a1b-8b0c-0f65a5697256",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "For now since we only have a single thread processing all incoming req/resp, this is okay; but when we multi-thread processing requests this would no longer be safe, since it is possible that some batches gets replicated and committed while not being flushed locally yet.",
        "createdAt" : "2020-10-21T00:37:10Z",
        "updatedAt" : "2020-10-27T03:04:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "7542e12a-9532-4162-b949-d035159253be",
        "parentId" : "66b24ed4-7f17-4a1b-8b0c-0f65a5697256",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, I agree with you. Of course it is ok if unflushed data gets replicated. The main thing we need to protect is incrementing the high watermark.",
        "createdAt" : "2020-10-21T21:29:00Z",
        "updatedAt" : "2020-10-27T03:04:10Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "fac5c0a9507f9f40f99dac017242d12500a97d5d",
    "line" : 289,
    "diffHunk" : "@@ -1,1 +1533,1537 @@                    appendBatch(state, batch, currentTimeMs);\n                }\n                flushLeaderLog(state, currentTimeMs);\n            } finally {\n                // Release and discard any batches which failed to be appended"
  },
  {
    "id" : "7832ff60-a69b-4f13-a4a5-a9d7fdf7cd19",
    "prId" : 9418,
    "prUrl" : "https://github.com/apache/kafka/pull/9418#pullrequestreview-517316390",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1245a0d3-6281-407f-99ca-2cce95785be8",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why return MAX_VALUE instead of null here? If we want to use `null` to indicate `memory full` and use `MAX_VALUE` to indicate `not leader`, the javadoc should reflecting this.\r\n\r\nAnyways, I think returning sth like a `combo(Offset, ErrorCode, backoffMs)` would be preferred in the end state.",
        "createdAt" : "2020-10-21T00:39:31Z",
        "updatedAt" : "2020-10-27T03:04:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d28d9ad2-6554-41c4-9217-ef0e7dd2f9e3",
        "parentId" : "1245a0d3-6281-407f-99ca-2cce95785be8",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, see my comment above about the handling of `Long.MAX_VALUE`. This is an attempt to reduce the error handling in the state machine. The model that we are working toward here is the following:\r\n\r\n1) the state machine gets notified that the node has become leader in some epoch\r\n2) the state machine can schedule appends with this epoch and it will get back the expected append offset\r\n3) the state machine treats scheduled appends as uncommitted until the call to `handleCommit`\r\n4) if the node resigns its leadership, the state machine will get notified and it will be expected to drop uncommitted data\r\n\r\nBy using a sort of impossible offset sentinel, the state machine just needs to wait for the notification that the leader has resigned.",
        "createdAt" : "2020-10-21T21:35:44Z",
        "updatedAt" : "2020-10-27T03:04:10Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "fd81abb1-bc00-41b0-87cc-998d06028919",
        "parentId" : "1245a0d3-6281-407f-99ca-2cce95785be8",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Sounds good.",
        "createdAt" : "2020-10-27T03:06:21Z",
        "updatedAt" : "2020-10-27T03:06:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fac5c0a9507f9f40f99dac017242d12500a97d5d",
    "line" : 424,
    "diffHunk" : "@@ -1,1 +1713,1717 @@        BatchAccumulator<T> accumulator = this.accumulator;\n        if (accumulator == null) {\n            return Long.MAX_VALUE;\n        }\n"
  },
  {
    "id" : "3bcd3157-a0bc-4960-bd56-00f82e7c43cb",
    "prId" : 9418,
    "prUrl" : "https://github.com/apache/kafka/pull/9418#pullrequestreview-521187510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d966055-82ea-4f89-be43-629405a2cf0c",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "@hachikuji Should we move this state to `LeaderState`? We can delegate `maybeCloseAccumulator` to the `transitionTo...` functions.\r\n\r\nI have a similar requirements for snapshot. I am currently adding a `SnapshotWriter` to `FollowerState` to track this state. I haven't implemented it yet but I am thinking of making `FollowerState` `Closeable` which will handle the cleanup. The `transitionTo...` functions have the additional responsibility of calling `close` if necessary.\r\n\r\nWhat do you think?",
        "createdAt" : "2020-11-01T02:32:40Z",
        "updatedAt" : "2020-11-01T02:32:40Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "fac5c0a9507f9f40f99dac017242d12500a97d5d",
    "line" : 178,
    "diffHunk" : "@@ -1,1 +301,305 @@        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n\n        accumulator = new BatchAccumulator<>(\n            quorum.epoch(),\n            log.endOffset().offset,"
  },
  {
    "id" : "81445f84-8443-446b-a14b-6352fc6f54e4",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-515089413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae6bf6a5-513b-4b7e-a9d5-c34338a30d6b",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Does this mean that in practice, follower will have at most two batches in flight?\r\n1. The one that they are currently processing\r\n2. If they read the last message/record in the batch then the next batch in the log?",
        "createdAt" : "2020-10-22T19:28:11Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "4cb5779f-824e-4d89-8ab3-0fa36128c0ae",
        "parentId" : "ae6bf6a5-513b-4b7e-a9d5-c34338a30d6b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "When catching up from the log, yes. However, I have implemented an optimization for writes from the leader. We save the original batch in memory so that it can be sent back to the state machine after the write is committed. In this case, we know the last offset of the batch, so we can have multiple inflight batches sent to the controller. This is nice because it means the elected controller will not have to read from disk.",
        "createdAt" : "2020-10-22T20:22:12Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 484,
    "diffHunk" : "@@ -1,1 +1835,1839 @@         * inflight data until it has been processed by the state machine. In this case,\n         * we delay sending additional data until the state machine has read to the\n         * end and the last offset is determined.\n         */\n        public synchronized OptionalLong nextExpectedOffset() {"
  },
  {
    "id" : "c8015acd-16f5-4575-8b38-8691a0f60d82",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-520043870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ebec9c34-992e-45fe-8d63-062b0a77811b",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "It looks a bit weird to have two versions of fire handle commit, could we name them differently or comment about their distinctive logics for determining when to fire commit callback?",
        "createdAt" : "2020-10-29T17:48:13Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "bd1f0237-b3b0-4923-b1bf-36eeb56e492e",
        "parentId" : "ebec9c34-992e-45fe-8d63-062b0a77811b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The only difference is the input. I will add some comments to try and clarify the usage.",
        "createdAt" : "2020-10-29T20:12:18Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 173,
    "diffHunk" : "@@ -1,1 +280,284 @@    }\n\n    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {\n        for (ListenerContext listenerContext : listenerContexts) {\n            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();"
  },
  {
    "id" : "fc10849c-e352-4dbf-9d68-8383c7239ea1",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-521078117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e96f4cb-edcf-46ad-9e80-32e3096f4129",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Why would this work? If the flush wasn't successful, could the fetched records be invalidated later?",
        "createdAt" : "2020-10-29T17:54:30Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "241381ab-95aa-4796-9bde-875200e0ba83",
        "parentId" : "4e96f4cb-edcf-46ad-9e80-32e3096f4129",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it's ok for followers to see uncommitted or even unflushed data. The main thing is that we avoid advancing the high watermark until the fsync completes. Note that this is the main reason that we had to do KAFKA-10527. Without this fix, it was possible for the leader to continue in the same epoch after a start, which means that it could lose and overwrite unflushed data.",
        "createdAt" : "2020-10-29T20:16:16Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ee8bc8e3-d452-45c7-aaeb-2f4b73b88a62",
        "parentId" : "4e96f4cb-edcf-46ad-9e80-32e3096f4129",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Yeah. You want to force an epoch change in the case that the old leader stays leader and partially replicated data was lost. This would force followers to truncate to the new leader's log state.",
        "createdAt" : "2020-10-30T23:31:36Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +388,392 @@\n    private void flushLeaderLog(LeaderState state, long currentTimeMs) {\n        // We update the end offset before flushing so that parked fetches can return sooner\n        updateLeaderEndOffsetAndTimestamp(state, currentTimeMs);\n        log.flush();"
  },
  {
    "id" : "84115041-4fc6-46c7-ad60-a92b4c4d24a5",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-519993857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7780d3c2-fa49-419d-8477-72e197c2bf87",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Do we anticipate use cases to add listeners on the fly? Right now I could only see one case in static context from test raft server.",
        "createdAt" : "2020-10-29T17:59:30Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "866a1fdf-8e10-4e66-9d44-fa73876dd720",
        "parentId" : "7780d3c2-fa49-419d-8477-72e197c2bf87",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I doubt we would use it in practice, though I guess it would open the door to changing roles dynamically, which might be interesting in the future. That said, it was simple to add and useful in testing since it gave me an easy way to initialize a state where a listener had not caught up.",
        "createdAt" : "2020-10-29T19:14:10Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 418,
    "diffHunk" : "@@ -1,1 +1694,1698 @@    private void pollListeners() {\n        // Register any listeners added since the last poll\n        while (!pendingListeners.isEmpty()) {\n            Listener<T> listener = pendingListeners.poll();\n            listenerContexts.add(new ListenerContext(listener));"
  },
  {
    "id" : "f9e8f330-9535-40f8-8ae8-295284b2d6a2",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-521189070",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2adeeedb-ede6-425d-b865-d1631c0c5cfc",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Should we call `pollListeners` after `pollCurrentState` to get more recent updates quicker?",
        "createdAt" : "2020-10-29T18:05:17Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "691f65ff-02cb-42ba-b254-6dbb0fd70f8e",
        "parentId" : "2adeeedb-ede6-425d-b865-d1631c0c5cfc",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm, that's a fair question. I think the listeners will tend to get new data in two cases: 1) high watermark advanced, or 2) a previous read completes. In the first case, the high watermark only advances in response to a request, so there should be no delay. In the second case, we call `wakeup()` to take us out of the network poll, so I think there also should be no delay. Can you think of a case where there would be a delay?",
        "createdAt" : "2020-10-29T21:41:03Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "307ad0d8-4b54-4879-9477-76b02c0a502a",
        "parentId" : "2adeeedb-ede6-425d-b865-d1631c0c5cfc",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "That looks correct to me with the clarification that \"in response to a request\" has two cases:\r\n\r\n1. The leader handles a fetch request. This implementation calls \"update high watermark\r\n2. The follower handle a fetch response. This implementation calls \"update high watermark\"\r\n\r\nI think that `pollListeners` should only fire a `Listener::handleCommit` for new listeners in `pendingListeners`.",
        "createdAt" : "2020-10-31T00:36:35Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "009218db-f0c3-4a86-adb9-4f9688f97fd9",
        "parentId" : "2adeeedb-ede6-425d-b865-d1631c0c5cfc",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sounds fair.",
        "createdAt" : "2020-11-01T03:16:27Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 443,
    "diffHunk" : "@@ -1,1 +1719,1723 @@            pollShutdown(gracefulShutdown);\n        } else {\n            pollListeners();\n\n            long currentTimeMs = time.milliseconds();"
  },
  {
    "id" : "6dc69b8c-a31e-4dcd-b6cd-981fd629875e",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-521865650",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e248a22-6fcd-4bbc-8dad-e56c195e8fb2",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Is this needed because users of `KafkaRaftClient` can call `::register` before `::initizalize`? When else would this result on a call to `Listener::handleCommit`?",
        "createdAt" : "2020-10-31T00:10:50Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "49bff349-77cd-41c3-9b90-4eb5136193fc",
        "parentId" : "5e248a22-6fcd-4bbc-8dad-e56c195e8fb2",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Never mind. I think this can happens when the replica changes state from follower to leader.\r\n\r\nI was having an issue if doing this would cause both `appendPurgatory.maybeComplete` and `maybeFireHandleCommit` to fire `Listener.handleCommit` for the same listener.\r\n\r\nI don't think this can happened based on how `ListenerContext` is managing the `nextExpectedOffset`. If `appendPurgator.maybeComplete` fires then that means that `nextExpectedOffset` is greater that the high watermark. Since the `nextExpectedOffset` is greater than the high watermark then `maybeFireHandleCommit` will not fire.\r\n\r\nI actually think that this order is important. Should we write a comment on the code explaining this if you agree with my analysis?",
        "createdAt" : "2020-10-31T00:19:44Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "e92971ee-ba82-4325-bfc9-c3147c74e342",
        "parentId" : "5e248a22-6fcd-4bbc-8dad-e56c195e8fb2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I will add a comment. I agree it is a subtle point.",
        "createdAt" : "2020-11-02T17:51:50Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +257,261 @@            // for the first time following the leader election, so we need\n            // to give lagging listeners an opportunity to catch up as well\n            maybeFireHandleCommit(highWatermark.offset);\n        });\n    }"
  },
  {
    "id" : "b28bd40f-6a67-4122-b5aa-b67a54e0810d",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-521078117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b62ed00f-df79-4ae3-bb5d-0a7f677342cd",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I would document that `synchronized` is protecting `lastSent` and `lastAckedOffset`.\r\n\r\n`claimedEpoch` is okay because it is only used by the thread calling `RaftClient::poll`.",
        "createdAt" : "2020-10-31T00:44:13Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 464,
    "diffHunk" : "@@ -1,1 +1815,1819 @@        // and are protected through synchronization on this `ListenerContext` instance\n        private BatchReader<T> lastSent = null;\n        private long lastAckedOffset = 0;\n\n        private ListenerContext(Listener<T> listener) {"
  },
  {
    "id" : "9bdaefa1-0e53-49a5-8d40-34265f2a5b6a",
    "prId" : 9482,
    "prUrl" : "https://github.com/apache/kafka/pull/9482#pullrequestreview-522110287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72448b36-d595-41bb-b057-87531ffeeb7c",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "This applies to all of the `listener.handle...` on this file.\r\n\r\nWhat are your thoughts on the `Listener` throwing an exception? I think with this implementation it will unwind all the way until `KafakRaftClient::poll`. Should we catch all non-fatal exceptions here and log an error instead?",
        "createdAt" : "2020-10-31T01:21:38Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "71d18c45-42b7-444b-8075-d3e600ee365b",
        "parentId" : "72448b36-d595-41bb-b057-87531ffeeb7c",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm... That's a good question. I guess the issue is that the listener will then be in an unknown state. Should the IO thread keep sending it updates or should it mark it as failed? This comes back to something I have been wondering in the KIP-500 world. Do we want the process to stay active if either the controller or broker listeners have failed or would it be better to shutdown? At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair?",
        "createdAt" : "2020-11-02T18:16:45Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "277bfe9a-0780-4695-888e-7d9899f2a9ad",
        "parentId" : "72448b36-d595-41bb-b057-87531ffeeb7c",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : ">  At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair?\r\n\r\nSounds fair to create a Jira and revisit this later.",
        "createdAt" : "2020-11-02T18:31:50Z",
        "updatedAt" : "2020-11-02T18:54:18Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "290c8766-3bd2-4114-980e-799a525a1eb6",
        "parentId" : "72448b36-d595-41bb-b057-87531ffeeb7c",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Filed https://issues.apache.org/jira/browse/KAFKA-10676.",
        "createdAt" : "2020-11-02T23:45:26Z",
        "updatedAt" : "2020-11-02T23:45:26Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "65494771be58190214d67d2d27bdd3f853fdaa6c",
    "line" : 528,
    "diffHunk" : "@@ -1,1 +1879,1883 @@                this.lastSent = reader;\n            }\n            listener.handleCommit(reader);\n        }\n"
  },
  {
    "id" : "8e074aff-e153-4eb6-8eb3-d55a91da2237",
    "prId" : 9531,
    "prUrl" : "https://github.com/apache/kafka/pull/9531#pullrequestreview-520893802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "776dd1e1-181b-4e28-aae9-e3cb21d95c91",
        "parentId" : null,
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "If the cluster resigns from candidateState, it will also send EndQuorumEpochRequest to all voters?",
        "createdAt" : "2020-10-30T08:43:41Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      },
      {
        "id" : "a81a2b3d-95e4-404b-97c9-f2b7baf4876e",
        "parentId" : "776dd1e1-181b-4e28-aae9-e3cb21d95c91",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, that is why the `EndQuorumEpoch` specifies both a `leaderId` and a `replicaId`. The idea is that this allows a voter who had voted for the candidate to start a new election more quickly. Admittedly, I am not sure how valuable this optimization is in practice. We could consider simplifying the protocol so that it is only sent by leaders.",
        "createdAt" : "2020-10-30T17:36:44Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "631ba91121a26b0171ab490c92991e628ce3658c",
    "line" : 151,
    "diffHunk" : "@@ -1,1 +1517,1521 @@    private long pollResigned(long currentTimeMs) throws IOException {\n        ResignedState state = quorum.resignedStateOrThrow();\n        long endQuorumBackoffMs = maybeSendRequests(\n            currentTimeMs,\n            state.unackedVoters(),"
  },
  {
    "id" : "6d99990b-ced6-48da-9aaa-2370ecfded9e",
    "prId" : 9531,
    "prUrl" : "https://github.com/apache/kafka/pull/9531#pullrequestreview-520897211",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "513ad4d9-844e-4edc-9965-5b516835ec37",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "If we are shutting down, why we still want to care about election at all?",
        "createdAt" : "2020-10-30T17:29:31Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "5a96e672-9976-41ad-928c-eda9d6bc9dad",
        "parentId" : "513ad4d9-844e-4edc-9965-5b516835ec37",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This node can stick around long enough to help another leader get elected.",
        "createdAt" : "2020-10-30T17:41:02Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "631ba91121a26b0171ab490c92991e628ce3658c",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +1527,1531 @@        if (shutdown != null) {\n            // If we are shutting down, then we will remain in the resigned state\n            // until either the shutdown expires or an election bumps the epoch\n            stateTimeoutMs = shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {"
  },
  {
    "id" : "8d0c3197-1efd-4f8e-8e45-168e7bb8c3cc",
    "prId" : 9531,
    "prUrl" : "https://github.com/apache/kafka/pull/9531#pullrequestreview-522144304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd562f78-be02-4ab1-a9a9-61116762f7cf",
        "parentId" : null,
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "should this be `Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs, state.remainingElectionTimeMs(currentTimeMs))`",
        "createdAt" : "2020-10-31T05:25:03Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      },
      {
        "id" : "ef99ea58-c2f9-402b-bf72-05466044c9f6",
        "parentId" : "fd562f78-be02-4ab1-a9a9-61116762f7cf",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The intent is to ignore the election timeout in order to prevent a shutting down broker from becoming a candidate. Does that make sense?",
        "createdAt" : "2020-11-02T19:15:21Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "8de43aa2-74ed-47c7-b1b0-86fcb4ae3c7b",
        "parentId" : "fd562f78-be02-4ab1-a9a9-61116762f7cf",
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "Understand, The candidate will try to complete only the current election when shutting down, so just ignore the election timeout.",
        "createdAt" : "2020-11-03T01:36:59Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "631ba91121a26b0171ab490c92991e628ce3658c",
    "line" : 217,
    "diffHunk" : "@@ -1,1 +1589,1593 @@            //  3) the shutdown timer expires\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs);\n        } else if (state.isBackingOff()) {\n            if (state.isBackoffComplete(currentTimeMs)) {"
  },
  {
    "id" : "36a9465a-8c68-4724-8eac-6fce0efafa34",
    "prId" : 9531,
    "prUrl" : "https://github.com/apache/kafka/pull/9531#pullrequestreview-526461191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1eec22ab-a029-4e5c-9ebd-07f1b8bb1b4b",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why the behavior of `pollFollowerAsVoter` and `pollVoted` are different when shutting down? Could the former case still help in casting and completing a vote as well?",
        "createdAt" : "2020-11-09T04:44:18Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "c6357770-58d9-4f6f-bda1-ee8dfff043b5",
        "parentId" : "1eec22ab-a029-4e5c-9ebd-07f1b8bb1b4b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If we are a follower, then there is no election in progress to help with. ",
        "createdAt" : "2020-11-09T16:58:17Z",
        "updatedAt" : "2020-11-09T17:28:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "631ba91121a26b0171ab490c92991e628ce3658c",
    "line" : 250,
    "diffHunk" : "@@ -1,1 +1623,1627 @@            // If we are a follower, then we can shutdown immediately. We want to\n            // skip the transition to candidate in any case.\n            return 0;\n        } else if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            logger.info(\"Become candidate due to fetch timeout\");"
  },
  {
    "id" : "a3fc0777-4942-46a1-a849-cc5244d59927",
    "prId" : 9539,
    "prUrl" : "https://github.com/apache/kafka/pull/9539#pullrequestreview-530809034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Similar to `LeaderState::nonEndorsingFollower`, I think you want to add a method to `LeaderState` with the following signature `public Set<Integer> endorsingFollower()`.",
        "createdAt" : "2020-10-31T19:25:11Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "1dbe13e1-6c40-4b50-945d-f93f91aa17a9",
        "parentId" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "authorId" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "body" : "Thanks @jsancio . I am slightly confused regarding your comment though(maybe because of my limited understanding of the protocol) so plz help me understand here:\r\nThe block of code where i made. the changes, picks all the followers and adds them as the voters:\r\n\r\n`List<Voter> voters = state.followers().stream()\r\n            .map(follower -> new Voter().setVoterId(follower))\r\n            .collect(Collectors.toList());`\r\n\r\nWhile the LeaderChange message schema, states that voterIds are all those voters who voted for the currently elected leader. \r\n\r\nContinuing on this, from what I understood in the handling of BeginQuorumEpoch response handling section in kIP-595, the moment a leader receives a response from a voter with no errors, it records an endorsement from the voter for itself. It also states that if a Voter accepts a BeginQuorumEpoch(and thereby sending the accepted response), it automatically transitions to a follower state for that leader. Lastly, the new leader will keep sending BeginQuorumEpoch requests to non endorsing voters until it doesn't receive them (or doesn't infer through the Fetch API in case of partitioning).\r\n\r\nSo, the questions I have are:\r\n\r\n1. Since the leader waits for the endorsements from all voters, doesn't it mean that eventually the followers method in LeaderState would return all voters? While the LeaderState message schema mentions that VotedIDs are those voters are voted for this new leader. \r\n2. I had thought that the nonEndorsingFollower method was used to find all followers who haven't yet endorsed for the new leader and keep sending requests to them. In that assumption, I am not able to follow your statement of adding a new method to add endorsingFollower wrt this PR. \r\n\r\nAgain, I apologise if I made some comments which don't make sense. As I said, I am not totally well verse with the internal implementations being done here. \r\n",
        "createdAt" : "2020-11-01T09:20:43Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "tags" : [
        ]
      },
      {
        "id" : "c2348498-a7bd-4f7e-bfd2-8241c38dbafa",
        "parentId" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think the original KIP stated that the LeaderChange message would encode the set of voters that had voted for the leader. We thought this might be useful for debugging. Later on, we had a change of heart and decided it would just be the set of voters. Now I'm thinking it might be useful to have both. The log will always remember who the voters were at the time of the election and which voters had granted the leader's candidacy, which could be helpful in case of misconfigurations.\r\n\r\nFor the set of voters which voted for the current leader, I think what we want is `CandidateState.grantingVoters`. However, by the time `onBecomeLeader` is fired, we have already dropped the `CandidateState`. One option is to carry `grantingVoters` over to `LeaderState`. We might also be able to pass it through `onBecomeLeader`. This will be easier if we get rid of the call to `onBecomeLeader` in `initialize()`. Following KAFKA-10527, it is not possible to initialize as a leader, so we could raise an exception instead.\r\n\r\n",
        "createdAt" : "2020-11-02T19:48:35Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e22d7b06-17a4-441a-a7be-6406763cbf20",
        "parentId" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "authorId" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "body" : "Thanks @hachikuji . So, from the context of this PR, do you suggest to continue using the definition of Voters in the LeaderChange message as all the voters ? Maybe we can create separate issues for:\r\n1) changing LeaderChange message to include both all voters and endorsing voters for that Leader.\r\n2) Make relevant changes to be able to pass the grantingVoters information from CandidateState to LeaderState. These include the things you mentioned like removing `onBecomeLeader` from `initialize()` or throw exception.\r\n\r\nJust curious on the last part. As per KAFKA-10527, a node can't be initialized as a leader. So, this block of code is effectively unused then:\r\n\r\n`if (quorum.isLeader()) {\r\nonBecomeLeader(currentTimeMs);\r\n        } else if (quorum.isCandidate()) {\r\n            onBecomeCandidate(currentTimeMs);\r\n        } `\r\n\r\n\r\nOr should we chase all the above as part of this PR itself? Plz let me know.",
        "createdAt" : "2020-11-03T05:48:59Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "tags" : [
        ]
      },
      {
        "id" : "62c74cb0-3171-48a6-b4cc-338f3e498a2f",
        "parentId" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "@vamossagar12 Up to you I guess. I'm ok doing it all here since the changes seem pretty small.",
        "createdAt" : "2020-11-03T17:22:07Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "3f123be6-d034-47da-a15c-dc6011ba6f76",
        "parentId" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "authorId" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "body" : "Alright. Thanks @hachikuji . So, I need to add voters + voted for the current Leader in the LeaderChange message and see how to pass the latter to LeaderState.",
        "createdAt" : "2020-11-04T16:15:43Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "tags" : [
        ]
      },
      {
        "id" : "964888fe-6ce8-4d14-81a9-4bd978f544f3",
        "parentId" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "authorId" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "body" : "@hachikuji i have made the changes you had suggested. \r\ncc @jsancio ",
        "createdAt" : "2020-11-06T04:12:35Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "tags" : [
        ]
      },
      {
        "id" : "95c0b292-5cc9-4b31-8816-d2edefb43e52",
        "parentId" : "bec842cf-447a-4362-b0df-e4e4a14dfa91",
        "authorId" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "body" : "hey @hachikuji, did you get a chance to review the changes?",
        "createdAt" : "2020-11-15T15:31:22Z",
        "updatedAt" : "2020-12-08T02:56:30Z",
        "lastEditedBy" : "dec739d0-c763-471c-b738-6b6f5b82e58e",
        "tags" : [
        ]
      }
    ],
    "commit" : "16432f8e9a89a11ffd862efe67f9ddc27a85642a",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +383,387 @@\n        // Adding the leader to the voters as any voter always votes for itself.\n        voters.add(new Voter().setVoterId(state.election().leaderId()));\n\n        LeaderChangeMessage leaderChangeMessage = new LeaderChangeMessage()"
  },
  {
    "id" : "93b6def5-3f5d-4a49-a7d9-6fb4d4f15344",
    "prId" : 9553,
    "prUrl" : "https://github.com/apache/kafka/pull/9553#pullrequestreview-546771075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f77110f7-9de8-4d57-9cd9-94edb28d4370",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "The follower needs to update the log start offset and log end offset after it has successfully fetched a snapshot. I want to implement this part in this JIRA: https://issues.apache.org/jira/browse/KAFKA-10820",
        "createdAt" : "2020-12-08T06:08:51Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "22133d1166394ea4ffbf111aeb8eb063c2182cd8",
    "line" : 270,
    "diffHunk" : "@@ -1,1 +1326,1330 @@            // Finished fetching the snapshot.\n            snapshot.freeze();\n            state.setFetchingSnapshot(Optional.empty());\n        }\n"
  },
  {
    "id" : "8cd437a8-00fe-4143-82fe-15c1508ea9de",
    "prId" : 9553,
    "prUrl" : "https://github.com/apache/kafka/pull/9553#pullrequestreview-557268245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "405a71da-38b7-43fd-b9ef-05c0d0587278",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: I guess you could use `leaderValidation.ifPresent`",
        "createdAt" : "2020-12-22T17:10:25Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "07f43904-141c-4117-a3cf-98451a4ea7e1",
        "parentId" : "405a71da-38b7-43fd-b9ef-05c0d0587278",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Hmm. I don't think so. `ifPresent` returns `void` and this part of the code wants to return a `FetchSnapshotResponseData` if there was a validation error. We can do a `map` followed by a `isPresent` and `get` but I think this is more consistent with the rest of the code.",
        "createdAt" : "2020-12-22T18:09:35Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "22133d1166394ea4ffbf111aeb8eb063c2182cd8",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +1183,1187 @@                partitionSnapshot.currentLeaderEpoch()\n        );\n        if (leaderValidation.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),"
  },
  {
    "id" : "ff7d102d-2780-4d76-82f3-8f48ecd86220",
    "prId" : 9553,
    "prUrl" : "https://github.com/apache/kafka/pull/9553#pullrequestreview-557422874",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48929f5a-16c7-4af8-ab8a-10deb5edda05",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Is it worth validating that `partitionSnapshot.position()` is non-negative?",
        "createdAt" : "2020-12-22T17:12:11Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f3fc7dcb-af26-4cdc-847e-35289ea8ed9e",
        "parentId" : "48929f5a-16c7-4af8-ab8a-10deb5edda05",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Good catch. In the KIP we added `POSITION_OUT_OF_RANGE` which I am not using any where in this PR.",
        "createdAt" : "2020-12-22T18:30:45Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "6abbd6ca-c950-477f-a636-ddee62d0a8b5",
        "parentId" : "48929f5a-16c7-4af8-ab8a-10deb5edda05",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Done.",
        "createdAt" : "2020-12-22T23:23:49Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "22133d1166394ea4ffbf111aeb8eb063c2182cd8",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +1227,1231 @@\n            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n            snapshot.read(buffer, partitionSnapshot.position());\n            buffer.flip();\n"
  },
  {
    "id" : "4719e21b-bf27-4a32-8aca-55b49f527d36",
    "prId" : 9553,
    "prUrl" : "https://github.com/apache/kafka/pull/9553#pullrequestreview-557224937",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6899f4f5-652c-4c5d-8030-f19cd4c93a98",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "A log message would probably be helpful. It's probably worth doing one full pass over the logic here to see where we could add extra logging.",
        "createdAt" : "2020-12-22T17:17:36Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "22133d1166394ea4ffbf111aeb8eb063c2182cd8",
    "line" : 232,
    "diffHunk" : "@@ -1,1 +1288,1292 @@            partitionSnapshot.snapshotId().epoch() < 0) {\n\n            /* The leader deleted the snapshot before the follower could download it. Start over by\n             * reseting the fetching snapshot state and sending another fetch request.\n             */"
  },
  {
    "id" : "812ed583-2032-4214-af36-7cbd30dd8eaa",
    "prId" : 9553,
    "prUrl" : "https://github.com/apache/kafka/pull/9553#pullrequestreview-557253734",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eeb679c3-32a1-4c30-a7e2-c8cbc27a2f58",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Just checking my understanding. This patch adds the logic to respond to the snapshot id from a fetch response and to handle send/handle snapshots when needed. However, since it does not contain the logic to set the snapshot id in the fetch request handler, none of this logic will get exercised by the simulation test. Is that right?",
        "createdAt" : "2020-12-22T17:28:15Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b01336e0-a0ed-4553-8637-f26ec80bf37b",
        "parentId" : "eeb679c3-32a1-4c30-a7e2-c8cbc27a2f58",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Correct. I am handling this case in https://issues.apache.org/jira/browse/KAFKA-10761. I think that after implementing that issue we should have an end-to-end working Raft Client with snapshot support that we test in the simulations.",
        "createdAt" : "2020-12-22T17:44:34Z",
        "updatedAt" : "2020-12-23T18:10:34Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "22133d1166394ea4ffbf111aeb8eb063c2182cd8",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +1152,1156 @@    }\n\n    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata\n    ) throws IOException {"
  },
  {
    "id" : "a6cc7657-d7c4-4bf9-859c-0e5addec0d2e",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-563187063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ed0afa4-2352-4017-a65b-1134cf2b137e",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "This is a temporary signature. This will change when we implement the API for loading snapshots. Tracking is work in this issue: https://issues.apache.org/jira/browse/KAFKA-12154",
        "createdAt" : "2021-01-07T03:24:24Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +2308,2312 @@         * when the context last acked end offset is less that then log start offset.\n         */\n        public void fireHandleSnapshot(long logStartOffset) {\n            synchronized (this) {\n                nextOffset = logStartOffset;"
  },
  {
    "id" : "fac8f6e8-cdeb-4197-87fb-88be884c1f49",
    "prId" : 9881,
    "prUrl" : "https://github.com/apache/kafka/pull/9881#pullrequestreview-567819858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83e5366f-7bf7-484a-a646-fcdccdc243e6",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Since we are no longer initializing this in the constructor, we might need a null check in `close`.",
        "createdAt" : "2021-01-13T23:05:25Z",
        "updatedAt" : "2021-01-14T19:06:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "dba11293-134f-4f89-966c-02bf72a478ee",
        "parentId" : "83e5366f-7bf7-484a-a646-fcdccdc243e6",
        "authorId" : "19ee7e44-7168-44d6-980b-a3b07f2ad76a",
        "body" : "Good catch!",
        "createdAt" : "2021-01-14T03:05:55Z",
        "updatedAt" : "2021-01-14T19:06:15Z",
        "lastEditedBy" : "19ee7e44-7168-44d6-980b-a3b07f2ad76a",
        "tags" : [
        ]
      }
    ],
    "commit" : "1735ae340894d16b87eae1c99305bc054ce8a672",
    "line" : 168,
    "diffHunk" : "@@ -1,1 +338,342 @@        quorumState.initialize(new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch()));\n        this.quorum = quorumState;\n        this.kafkaRaftMetrics = new KafkaRaftMetrics(metrics, \"raft\", quorum);\n        kafkaRaftMetrics.updateNumUnknownVoterConnections(quorum.remoteVoters().size());\n"
  },
  {
    "id" : "dba33ed5-6383-4078-ae70-1b194ec3fc98",
    "prId" : 9944,
    "prUrl" : "https://github.com/apache/kafka/pull/9944#pullrequestreview-683418719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d25d6a66-26df-42e2-b0f4-1e85accac658",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "The KIP talks about bootstrapping the topicId for the metadata topic. Is that part done already? I don't see it included in this PR.",
        "createdAt" : "2021-06-14T15:55:51Z",
        "updatedAt" : "2021-06-14T22:25:42Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "bbac2bbb-c229-46d8-9374-3639da3a153d",
        "parentId" : "d25d6a66-26df-42e2-b0f4-1e85accac658",
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "Are you referring to creating a new topic ID for the metadata topic? For now, we are simply using the sentinel ID. ",
        "createdAt" : "2021-06-14T22:53:56Z",
        "updatedAt" : "2021-06-14T22:53:56Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a47290d9a46db70b9ed3273ddafedbc8827e8da",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +936,940 @@        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition(), log.topicId())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INVALID_REQUEST.code()));"
  },
  {
    "id" : "193e8dbb-d50d-4047-9bbe-4464463724a5",
    "prId" : 10085,
    "prUrl" : "https://github.com/apache/kafka/pull/10085#pullrequestreview-620115686",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80fb9984-b2df-4c74-8089-7b1dd5927702",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "When the implementation is a `FileRawSnapshotReader`, the created slice will be `close` before the network client has had a chance to send the bytes. Created https://issues.apache.org/jira/browse/KAFKA-12543 and I will work on this after this PR.",
        "createdAt" : "2021-03-24T18:39:54Z",
        "updatedAt" : "2021-04-29T20:10:59Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbbf95304c34a52855ad56d51561e822d64ecc54",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +1314,1318 @@            }\n\n            UnalignedRecords records = snapshot.slice(partitionSnapshot.position(), Math.min(data.maxBytes(), maxSnapshotSize));\n\n            return FetchSnapshotResponse.singleton("
  },
  {
    "id" : "1b919d4e-7692-44f5-b3a3-955c8f33fd4d",
    "prId" : 10129,
    "prUrl" : "https://github.com/apache/kafka/pull/10129#pullrequestreview-593800953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e1cc21b-744a-4924-bf45-e1322402994a",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "One small detail which is probably ok. The clusterId field in the fetch schema is not currently marked as ignorable. That should be ok since it is only used in the raft implementation which can guarantee that we will have version 12 and above. On the other hand, I don't see any harm making the field ignorable since we are accepting a null value anyway. Is it worth changing that?",
        "createdAt" : "2021-02-19T02:12:35Z",
        "updatedAt" : "2021-02-19T19:37:39Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cb9137f8ad99b9a6452382a4c15b635c6fb3183",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +1784,1788 @@        return request\n            .setMaxWaitMs(fetchMaxWaitMs)\n            .setClusterId(clusterId.toString())\n            .setReplicaId(quorum.localIdOrSentinel());\n    }"
  },
  {
    "id" : "9d6bc336-70bd-4402-b907-573c4a4059f6",
    "prId" : 10129,
    "prUrl" : "https://github.com/apache/kafka/pull/10129#pullrequestreview-603657323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01aaa7fc-f8a8-4f36-a6e7-ad782414ea19",
        "parentId" : null,
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "We only validate `FetchRequest` here? should we also add validation to the other 4 rpcs?",
        "createdAt" : "2021-03-04T02:51:54Z",
        "updatedAt" : "2021-03-04T02:51:54Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cb9137f8ad99b9a6452382a4c15b635c6fb3183",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +973,977 @@        FetchRequestData request = (FetchRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request)) {\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code()));\n        }"
  },
  {
    "id" : "241d4a74-1455-4971-9230-8157d3b0af43",
    "prId" : 10705,
    "prUrl" : "https://github.com/apache/kafka/pull/10705#pullrequestreview-663697581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3458d3e0-a6df-4c51-98ac-35257d0c915a",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "This needs to be supported, because the controller will resign if it detects certain bugs.",
        "createdAt" : "2021-05-17T22:48:05Z",
        "updatedAt" : "2021-05-17T22:48:05Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "bf22d501-0de1-4e6d-9992-a16d871db78c",
        "parentId" : "3458d3e0-a6df-4c51-98ac-35257d0c915a",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Yes. We have an issue for this: https://issues.apache.org/jira/browse/KAFKA-12631",
        "createdAt" : "2021-05-18T00:01:35Z",
        "updatedAt" : "2021-05-18T00:01:35Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "77f962ec-5e77-4cf1-a66b-678c87cebb0d",
        "parentId" : "3458d3e0-a6df-4c51-98ac-35257d0c915a",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "For some reason, I thought this was already implemented. But you're correct, it is not currently implemented. Maybe we should have this for 3.0, since controller bugs do happen sometimes...",
        "createdAt" : "2021-05-19T22:01:53Z",
        "updatedAt" : "2021-05-19T22:01:53Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "2dc57d09cb99c67a452902f07cdcec3b615a88b7",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +2262,2266 @@    @Override\n    public void resign(int epoch) {\n        throw new UnsupportedOperationException();\n    }\n"
  },
  {
    "id" : "8035b39a-1b8c-42bc-8643-2865902d2edb",
    "prId" : 10749,
    "prUrl" : "https://github.com/apache/kafka/pull/10749#pullrequestreview-681124861",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7628aa3a-4d9b-4857-9b27-248ad207ceb9",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "This is semantically different from the previous code but I think it is okay. If `initialize` throws an io exception the JVM process for the broker will terminate as this is called in the `RaftManager` constructor which is indirectly called from `kafka.Kafka`'s `main`.",
        "createdAt" : "2021-06-01T16:57:46Z",
        "updatedAt" : "2021-06-01T17:20:15Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "83196b84-adbd-421c-a3db-87475bf824c0",
        "parentId" : "7628aa3a-4d9b-4857-9b27-248ad207ceb9",
        "authorId" : "6bb534cf-8043-4ddd-8eda-0599d79cebe3",
        "body" : "This change doesn't look like it needs to be changed, does it?",
        "createdAt" : "2021-06-02T02:17:57Z",
        "updatedAt" : "2021-06-02T02:17:57Z",
        "lastEditedBy" : "6bb534cf-8043-4ddd-8eda-0599d79cebe3",
        "tags" : [
        ]
      },
      {
        "id" : "17438e37-2ff8-4c08-a2a7-396a1a1e8ad9",
        "parentId" : "7628aa3a-4d9b-4857-9b27-248ad207ceb9",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Yeah, I think it is fine as is. I was just pointing out that it is technically different from the original code.",
        "createdAt" : "2021-06-02T16:48:00Z",
        "updatedAt" : "2021-06-02T16:48:01Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "a52faec0-416d-4e06-833e-41907f491bfe",
        "parentId" : "7628aa3a-4d9b-4857-9b27-248ad207ceb9",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Leaving this note for future readers. My comments above are not accurate. I misread the diff generated by GitHub. When I wrote the comment, I was under the impression that the old code was handling, wrapping and re-throwing the `IOException`.\r\nInstead the old code wrapped and re-threw the `IOException`; it was not handling the exception.",
        "createdAt" : "2021-06-10T18:33:45Z",
        "updatedAt" : "2021-06-10T18:33:45Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa2348265d8bc668f3c0e75b418c162620218cac",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +382,386 @@            && !quorum.isCandidate()) {\n\n            transitionToCandidate(currentTimeMs);\n        }\n    }"
  },
  {
    "id" : "e8538ce9-80fe-47c2-aca9-3cd31de1afbf",
    "prId" : 10786,
    "prUrl" : "https://github.com/apache/kafka/pull/10786#pullrequestreview-683489619",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31253b96-1ea8-430c-9b64-4ba8f963019b",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "No functional change here. Just a formatting change. Always found this line hard to read and I had to fix it :smile: ",
        "createdAt" : "2021-06-15T01:54:45Z",
        "updatedAt" : "2021-06-15T01:54:49Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "b18ae7abd96ae59d1684e628ef09a8f2215b7d4b",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +2292,2296 @@        } else {\n            return OptionalLong.empty();\n        }\n    }\n"
  },
  {
    "id" : "0ba7a1fd-b8d4-4816-9c30-c32d5fe5d660",
    "prId" : 10909,
    "prUrl" : "https://github.com/apache/kafka/pull/10909#pullrequestreview-719531116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2010f2e-b99c-42e7-9d66-79699b9b34fc",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think it would be nice if we could consolidate `NotLeaderException` and `FencedEpochException`. In the context of `scheduleAppend`, they mean the same thing to the caller. They both say that the raft state has transitioned and the operation is no longer possible. I'm inclined to keep `NotLeaderException` and document that it covers both cases. What do you think?\r\n",
        "createdAt" : "2021-07-30T18:35:49Z",
        "updatedAt" : "2021-07-30T18:36:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "8c945554-e769-43a5-85bd-5b1f6696ce24",
        "parentId" : "c2010f2e-b99c-42e7-9d66-79699b9b34fc",
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "Here I referred to the practices of `KafkaController` in which we have `NOT_CONTROLLER` and `STALE_CONTROLLER_EPOCH` for RPC errors, but here I also think it's better to consolidate `NotLeaderException` and `FencedEpochException` now.\r\n\r\n",
        "createdAt" : "2021-07-31T01:10:33Z",
        "updatedAt" : "2021-07-31T01:10:34Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "7426fc014915feaeaabaaf5099cceae9dd6756f7",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +2261,2265 @@    private long append(int epoch, List<T> records, boolean isAtomic) {\n        LeaderState<T> leaderState = quorum.<T>maybeLeaderState().orElseThrow(\n            () -> new NotLeaderException(\"Append failed because the replication is not the current leader\")\n        );\n"
  },
  {
    "id" : "f31dfc7d-5e25-494e-835b-90aa019bcf07",
    "prId" : 10913,
    "prUrl" : "https://github.com/apache/kafka/pull/10913#pullrequestreview-689821855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b35e4a0c-be4b-460a-934c-133a34435ab4",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "`0` is also an invalid epoch, right? Or does the raft client send `handleLeaderChange(LeaderAndEpoch(OptionalInt.empty(), 0))` to the listener?\r\n\r\nI still think it is fair to say that the raft client will never send `handleLeaderChange(LeaderAndEpoch(OptionalInt.of(...), 0))`",
        "createdAt" : "2021-06-21T23:11:10Z",
        "updatedAt" : "2021-06-22T17:02:34Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "d5841eee-e025-4b06-8cbd-941d4b7dbb1e",
        "parentId" : "b35e4a0c-be4b-460a-934c-133a34435ab4",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I consider epoch 0 to be a valid epoch even though no leader could have been elected in this epoch. Therefore a resignation from epoch 0 is treated as a no-op.",
        "createdAt" : "2021-06-22T17:48:24Z",
        "updatedAt" : "2021-06-22T17:48:24Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88f3e5347adb6eef8ce259b9204f90d4b891909f",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +2250,2254 @@    @Override\n    public void resign(int epoch) {\n        if (epoch < 0) {\n            throw new IllegalArgumentException(\"Attempt to resign from an invalid negative epoch \" + epoch);\n        }"
  },
  {
    "id" : "06bea074-5c42-46aa-8ccc-b3f497b140d8",
    "prId" : 10913,
    "prUrl" : "https://github.com/apache/kafka/pull/10913#pullrequestreview-694278708",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed0b6ec1-6fa0-4a48-a434-ef817121f62e",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Does it make sense to do this check before the epoch validation? If we're not the leader and received an old epoch (which, if i understand, seems likely if we're _not_ the leader anymore), we will silently ignore in the above case.",
        "createdAt" : "2021-06-28T18:00:23Z",
        "updatedAt" : "2021-06-28T18:02:59Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "12b5f83b-a37f-4d2c-98ac-56d273ad25ca",
        "parentId" : "ed0b6ec1-6fa0-4a48-a434-ef817121f62e",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the epoch has moved on, then the leader check is likely to fail, so the current order seems to make sense. We don't keep a history of previous states, so I think the best we can do is catch cases where the passed epoch does not make sense with the current state.",
        "createdAt" : "2021-06-28T18:26:53Z",
        "updatedAt" : "2021-06-28T18:26:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9987c70e-8e14-4706-8bc3-a91326deacb2",
        "parentId" : "ed0b6ec1-6fa0-4a48-a434-ef817121f62e",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "I see what you mean, and yea that is a fair point 👍 ",
        "createdAt" : "2021-06-28T18:40:35Z",
        "updatedAt" : "2021-06-28T18:45:24Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "88f3e5347adb6eef8ce259b9204f90d4b891909f",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +2272,2276 @@                \"current epoch {}\", epoch, currentEpoch);\n            return;\n        } else if (!leaderAndEpoch.isLeader(quorum.localIdOrThrow())) {\n            throw new IllegalArgumentException(\"Cannot resign from epoch \" + epoch +\n                \" since we are not the leader\");"
  },
  {
    "id" : "53a46b82-3627-410f-941b-c515ae5b03d5",
    "prId" : 10913,
    "prUrl" : "https://github.com/apache/kafka/pull/10913#pullrequestreview-694273401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45e6a36c-9150-4612-a11a-a5937ceae2b3",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "unrelated, but maybe worth creating helper method that returns `Optional<GracefulShutdown>` to avoid these null checks throughout",
        "createdAt" : "2021-06-28T18:01:57Z",
        "updatedAt" : "2021-06-28T18:02:59Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "58148b7d-74df-4702-8569-713726d760f4",
        "parentId" : "45e6a36c-9150-4612-a11a-a5937ceae2b3",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "That's fair. Let me see if it's reasonable to do this here or if we should push to a separate PR.",
        "createdAt" : "2021-06-28T18:05:07Z",
        "updatedAt" : "2021-06-28T18:05:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d0e981c7-6e59-4363-b2bc-4aeaec705bb6",
        "parentId" : "45e6a36c-9150-4612-a11a-a5937ceae2b3",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If you don't mind, let's do this refactor separately. There are a fair number of uses that would benefit from having `Optional<GracefulShutdown>`.",
        "createdAt" : "2021-06-28T18:32:18Z",
        "updatedAt" : "2021-06-28T18:32:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "db4107f1-2d6b-4b07-b85a-b3fec85fd7ed",
        "parentId" : "45e6a36c-9150-4612-a11a-a5937ceae2b3",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Works for me 👍 ",
        "createdAt" : "2021-06-28T18:34:00Z",
        "updatedAt" : "2021-06-28T18:34:00Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "88f3e5347adb6eef8ce259b9204f90d4b891909f",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1916,1920 @@        maybeFireLeaderChange(state);\n\n        if (shutdown.get() != null || state.isResignRequested()) {\n            transitionToResigned(state.nonLeaderVotersByDescendingFetchOffset());\n            return 0L;"
  },
  {
    "id" : "f8df9d58-b476-40ae-a63d-7de6fb3c4ae3",
    "prId" : 11084,
    "prUrl" : "https://github.com/apache/kafka/pull/11084#pullrequestreview-710080199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c1f392e-7205-4430-938a-ed17c44d6dfe",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Note that all of the change to the `raft` module are cosmetic mainly to improving logging.",
        "createdAt" : "2021-07-20T00:14:41Z",
        "updatedAt" : "2021-07-20T00:14:46Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "717f4f9c3e11ec782976152eb1552752b95cca8b",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +309,313 @@                        String.format(\n                            \"Snapshot expected since next offset of %s is %s, log start offset is %s and high-watermark is %s\",\n                            listenerContext.listenerName(),\n                            nextExpectedOffset,\n                            log.startOffset(),"
  }
]