[
  {
    "id" : "80cb74db-0a1b-4cef-97b6-6aa12cd12c12",
    "prId" : 9332,
    "prUrl" : "https://github.com/apache/kafka/pull/9332#pullrequestreview-495801315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab50bca6-7e27-409b-ae73-9b91d9e0cc8b",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "So the idea is to wipe out the older epoch (as epoch - 1)'s end offset, and the search for (epoch - 1) would give (epoch - 2) end offset? In the case where we put leader change message in mock log, this should never happen right?",
        "createdAt" : "2020-09-24T06:15:50Z",
        "updatedAt" : "2020-09-24T06:19:03Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "34d06f07-f557-40d8-8731-0be4cb180a1e",
        "parentId" : "ab50bca6-7e27-409b-ae73-9b91d9e0cc8b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "There's no guarantee that the leader change message ever gets committed. It is even possible for the leadership to change multiple times before a leader change message can be committed. There is no correctness problem with the current implementation. I just wanted the behavior to be consistent with `LeaderEpochFileCache`.",
        "createdAt" : "2020-09-24T17:36:02Z",
        "updatedAt" : "2020-09-24T17:36:02Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "14e0cab5e335b2619d8f17c999aac6a1c648b465",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +313,317 @@    public void initializeLeaderEpoch(int epoch) {\n        long startOffset = endOffset().offset;\n        epochStartOffsets.removeIf(epochStartOffset ->\n            epochStartOffset.startOffset >= startOffset || epochStartOffset.epoch >= epoch);\n        epochStartOffsets.add(new EpochStartOffset(epoch, startOffset));"
  },
  {
    "id" : "283c52f6-ba98-4685-9f68-ee27d3dae266",
    "prId" : 10085,
    "prUrl" : "https://github.com/apache/kafka/pull/10085#pullrequestreview-620115686",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72217dfd-1e16-4059-8b30-04b2a6fa736a",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Note that the changes to this method are so that `read`  doesn't return all of the batches from from `startOffset` to `highWatermark`. This was needed for more interested test cases around snapshot loading.",
        "createdAt" : "2021-03-24T19:25:41Z",
        "updatedAt" : "2021-04-29T20:10:59Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbbf95304c34a52855ad56d51561e822d64ecc54",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +364,368 @@        ByteBuffer buffer = ByteBuffer.allocate(512);\n        int batchCount = 0;\n        LogOffsetMetadata batchStartOffset = null;\n\n        for (LogBatch batch : batches) {"
  },
  {
    "id" : "9f1c7a11-9b7c-419a-80a0-4bfe30ed2a81",
    "prId" : 10085,
    "prUrl" : "https://github.com/apache/kafka/pull/10085#pullrequestreview-620115686",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c7036b5-ab2a-4070-be23-7ae7602388a8",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Note that the changes to this method are to relax the log start offset and high-watermark invariant so that we can create more interesting snapshot and log states in the `RaftClientTestContext.Builder`.",
        "createdAt" : "2021-03-24T19:27:28Z",
        "updatedAt" : "2021-04-29T20:10:59Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbbf95304c34a52855ad56d51561e822d64ecc54",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +432,436 @@\n    @Override\n    public boolean deleteBeforeSnapshot(OffsetAndEpoch snapshotId) {\n        if (logStartOffset() > snapshotId.offset) {\n            throw new OffsetOutOfRangeException("
  },
  {
    "id" : "7ce371b9-2b12-410c-955e-0f0c54185539",
    "prId" : 10085,
    "prUrl" : "https://github.com/apache/kafka/pull/10085#pullrequestreview-649549840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c7d02f1-7df0-4794-bba2-683cd000d9ff",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think this is a good change. I wonder if it creates a sort of reverse problem though where we end up not exercising multi-batch paths. Perhaps we could randomly choose 1-3 batches to return or something like that.",
        "createdAt" : "2021-04-28T20:55:57Z",
        "updatedAt" : "2021-04-29T20:11:00Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "259e04b8-fc4f-4915-a746-62714e044914",
        "parentId" : "1c7d02f1-7df0-4794-bba2-683cd000d9ff",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I wanted to keep the tests reproducible. As a comprise I changed this code to read at most 2 batches. We can revisit this in the future and make it random but reproducible.",
        "createdAt" : "2021-04-29T20:18:18Z",
        "updatedAt" : "2021-04-29T20:18:19Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "f8057fa9-03c9-42f7-858a-542ad33b05e2",
        "parentId" : "1c7d02f1-7df0-4794-bba2-683cd000d9ff",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Sounds good.",
        "createdAt" : "2021-04-30T19:22:22Z",
        "updatedAt" : "2021-04-30T19:22:23Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbbf95304c34a52855ad56d51561e822d64ecc54",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +371,375 @@            // filtered, which is effectively the same as having the consumer drop an incomplete\n            // batch returned in a fetch response.\n            if (batch.lastOffset() >= startOffset && batch.lastOffset() < maxOffset && !batch.entries.isEmpty()) {\n                buffer = batch.writeTo(buffer);\n"
  }
]