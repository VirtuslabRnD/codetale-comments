[
  {
    "id" : "efc3f9ce-045b-4bd8-9bcd-1dcb9cf1e86b",
    "prId" : 8307,
    "prUrl" : "https://github.com/apache/kafka/pull/8307#pullrequestreview-378782209",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b29dd829-81b3-4236-b134-68db537836de",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "For my own education: before the fix, this integration test will fail when instance-2 is started?",
        "createdAt" : "2020-03-20T19:02:00Z",
        "updatedAt" : "2020-03-20T19:02:03Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "87cf13b5-5c45-4f9b-9a97-5f67200bf2d4",
        "parentId" : "b29dd829-81b3-4236-b134-68db537836de",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Yes, actually either instance-1 or instance-2 would fail, depending on which box gets standby assignment. There would be a IllegalState + NPE exception sequence happening.",
        "createdAt" : "2020-03-20T20:10:28Z",
        "updatedAt" : "2020-03-20T20:10:28Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f4cfd7503c0959ed2453e88ee9f1c98d280ca71",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +103,107 @@            \"Stream instance one should be up and running by now\");\n\n        streamInstanceOne.close(Duration.ofSeconds(30));\n        streamInstanceTwo.close(Duration.ofSeconds(30));\n"
  },
  {
    "id" : "a7ec0b30-8d71-4af7-8c81-ce0eeab14a19",
    "prId" : 8330,
    "prUrl" : "https://github.com/apache/kafka/pull/8330#pullrequestreview-379066420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b761f708-28e8-4a55-a68e-c2efaa08e03f",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I moved this into a try-with-resources so that the instances would get closed even if the assertions fail.",
        "createdAt" : "2020-03-22T23:23:46Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "06c17350dac09c8b4a35edeb1b3c8e29132dfa17",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +86,90 @@            final KafkaStreams streamInstanceOne = buildStreamWithDirtyStateDir(appId, stateDirPath + \"/\" + appId + \"-1/\", instanceLatch);\n            final KafkaStreams streamInstanceTwo = buildStreamWithDirtyStateDir(appId, stateDirPath + \"/\" + appId + \"-2/\", instanceLatch);\n        ) {\n\n"
  },
  {
    "id" : "6c50313b-0339-408b-98ee-0dc9626ce402",
    "prId" : 8330,
    "prUrl" : "https://github.com/apache/kafka/pull/8330#pullrequestreview-379066420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69fc29c4-5bea-4149-b4f9-52d30e7e1d69",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Close them both asynchronously so we can tear them down in parallel. The try-with-resources will block until they are really closed.",
        "createdAt" : "2020-03-22T23:25:12Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "06c17350dac09c8b4a35edeb1b3c8e29132dfa17",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +102,106 @@\n            streamInstanceOne.close(Duration.ZERO);\n            streamInstanceTwo.close(Duration.ZERO);\n        }\n    }"
  },
  {
    "id" : "26d06aa8-fa00-4edf-904b-07f26686e247",
    "prId" : 8330,
    "prUrl" : "https://github.com/apache/kafka/pull/8330#pullrequestreview-379066420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eeed81b2-fe43-4576-8404-b108703ec607",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This test isn't logically changed, but I noticed a couple of problems with it that I fixed on the side.",
        "createdAt" : "2020-03-22T23:26:08Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "06c17350dac09c8b4a35edeb1b3c8e29132dfa17",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +83,87 @@        final CountDownLatch instanceLatch = new CountDownLatch(1);\n\n        try (\n            final KafkaStreams streamInstanceOne = buildStreamWithDirtyStateDir(appId, stateDirPath + \"/\" + appId + \"-1/\", instanceLatch);\n            final KafkaStreams streamInstanceTwo = buildStreamWithDirtyStateDir(appId, stateDirPath + \"/\" + appId + \"-2/\", instanceLatch);"
  },
  {
    "id" : "4fa35e1d-ceb7-4349-9b7b-307d5af60db8",
    "prId" : 8440,
    "prUrl" : "https://github.com/apache/kafka/pull/8440#pullrequestreview-390417660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cec3c3a9-8f7e-47c3-96a3-033e555ac4e2",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Why do we need to add cleanUp now? Is this missing originally?",
        "createdAt" : "2020-04-08T00:05:34Z",
        "updatedAt" : "2020-04-14T23:37:49Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "1f88691e-c3e9-40f1-9528-19dbfd16f209",
        "parentId" : "cec3c3a9-8f7e-47c3-96a3-033e555ac4e2",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Previously, the test was executed only once. Now it's executed twice. Because we use the same `appId` for both runs it seems better to add the cleanup (even if we might get two different temp directories anyway, so it might not be required)",
        "createdAt" : "2020-04-09T00:26:51Z",
        "updatedAt" : "2020-04-14T23:37:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "15e0b98ef384c15022ee76e6ca88b325567e31cb",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +121,125 @@            streamInstanceTwo.close(Duration.ZERO);\n\n            streamInstanceOne.cleanUp();\n            streamInstanceTwo.cleanUp();\n        }"
  },
  {
    "id" : "0f1c6b6a-fbea-4772-8bc1-489f69d14302",
    "prId" : 8886,
    "prUrl" : "https://github.com/apache/kafka/pull/8886#pullrequestreview-432750161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2832e501-2c15-4ae2-91a1-99adf3759b6e",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "should this be parameterized for both flavors of eos?",
        "createdAt" : "2020-06-17T15:28:45Z",
        "updatedAt" : "2020-06-18T01:53:01Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "ca26558a-4da0-47fe-9004-03d420fd8783",
        "parentId" : "2832e501-2c15-4ae2-91a1-99adf3759b6e",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This PR is for `2.5`, not trunk. I have another branch for `trunk` and there both cases are covered.",
        "createdAt" : "2020-06-17T20:29:23Z",
        "updatedAt" : "2020-06-18T01:53:01Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "67974712873ebf4f20705043d3221a9545bb47be",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +50,54 @@ * task towards a standby task is safe across restarts of the application.\n */\npublic class StandbyTaskEOSIntegrationTest {\n\n    private final static long REBALANCE_TIMEOUT = Duration.ofMinutes(2L).toMillis();"
  },
  {
    "id" : "90cd8860-a713-47bf-ab0e-d613a7808c8a",
    "prId" : 8890,
    "prUrl" : "https://github.com/apache/kafka/pull/8890#pullrequestreview-434282111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed330bcd-c9d7-4372-b506-ddb58cd5b6b1",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Did you mean \"stale\"?",
        "createdAt" : "2020-06-19T18:54:57Z",
        "updatedAt" : "2020-06-19T18:57:21Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "386c42869d23108856cc426effc7e63438043d86",
    "line" : 214,
    "diffHunk" : "@@ -1,1 +263,267 @@\n            // \"restart\" first client and wait for standby recovery\n            // (could actually also be active, but it does not matter as long as we enable \"state stores\"\n            startApplicationAndWaitUntilRunning(\n                Collections.singletonList(streamInstanceOneRecovery),"
  },
  {
    "id" : "f8f953a1-cb52-4251-8923-22231b70245a",
    "prId" : 8890,
    "prUrl" : "https://github.com/apache/kafka/pull/8890#pullrequestreview-434322451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c3d41c5-2c96-4da2-a13a-ab3fdf3db0f5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Do we really need this? It seems like the only thing that depends on knowing which instance would get the active is just waiting for the crash after the poison pill. Could we instead just wait for once of the instances to crash, but not worry about which?",
        "createdAt" : "2020-06-19T18:57:04Z",
        "updatedAt" : "2020-06-19T18:57:21Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "2d2afd39-88b0-4900-abee-2a5a36b511b6",
        "parentId" : "1c3d41c5-2c96-4da2-a13a-ab3fdf3db0f5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes. It's for the first phase of the test. We start the first instance and let it process the first record. As there is not enough capacity, no standby is scheduled. When we start the second instance, with \"lag=0\" setting, we ensure that the standby is placed at instance two. With default setting, we don't know which instance will get the active/standby assigned. -> when we inject the poison pill, we know that instance one will fail as it hosts the active.",
        "createdAt" : "2020-06-19T19:59:42Z",
        "updatedAt" : "2020-06-19T19:59:42Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "61f0d2d4-8eee-46cc-9ca5-5c767677ed68",
        "parentId" : "1c3d41c5-2c96-4da2-a13a-ab3fdf3db0f5",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Right, this was my suggestion:\r\n> Could we instead just wait for once of the instances to crash, but not worry about which?",
        "createdAt" : "2020-06-19T20:02:09Z",
        "updatedAt" : "2020-06-19T20:02:10Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "5585a154-4817-42f0-b810-e328c08d8081",
        "parentId" : "1c3d41c5-2c96-4da2-a13a-ab3fdf3db0f5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Well, it make the test more complex, because all the following code depends on instance one failing.",
        "createdAt" : "2020-06-19T20:09:27Z",
        "updatedAt" : "2020-06-19T20:09:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "030a6270-46bf-4644-b478-d287982873bc",
        "parentId" : "1c3d41c5-2c96-4da2-a13a-ab3fdf3db0f5",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, it's your call. I think this might make the tests flaky, but I guess we can figure that out later.",
        "createdAt" : "2020-06-19T20:21:12Z",
        "updatedAt" : "2020-06-19T20:21:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "386c42869d23108856cc426effc7e63438043d86",
    "line" : 325,
    "diffHunk" : "@@ -1,1 +378,382 @@        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);\n        // need to set to zero to get predictable active/standby task assignments\n        streamsConfiguration.put(StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG, 0);\n        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n"
  }
]