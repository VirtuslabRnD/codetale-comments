[
  {
    "id" : "39f26cc8-9060-4c0f-8bfa-2a34b2006286",
    "prId" : 5163,
    "prUrl" : "https://github.com/apache/kafka/pull/5163#pullrequestreview-127747977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c8e578f-6fda-4f27-9383-c762c6cbd838",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why do we use a delta, here, instead of the actual offset we want to commit (ie, 4000) ? Might be simpler to understand the test?",
        "createdAt" : "2018-06-10T20:47:24Z",
        "updatedAt" : "2018-06-11T17:05:19Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "2f790c19-c869-42e7-9940-c387ae4172bf",
        "parentId" : "5c8e578f-6fda-4f27-9383-c762c6cbd838",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The reason is that in line 150, I need to check exactly how many data were processed, which should be the diff between committed offset to log end offset.\r\n\r\nHowever, for log end offset it is not always numKeys / 2 (num.partitions). In my tests I saw for example 5005 and 4995. So I have to use the limit to say `commit at the log end offset minus that delta`.",
        "createdAt" : "2018-06-11T16:42:17Z",
        "updatedAt" : "2018-06-11T17:05:19Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "696dd680-217b-420f-8fdb-479db9035e08",
        "parentId" : "5c8e578f-6fda-4f27-9383-c762c6cbd838",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ack.",
        "createdAt" : "2018-06-11T21:26:03Z",
        "updatedAt" : "2018-06-11T21:26:03Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca40f17ef652a219d23b27ed11e2e8d9dbb748ca",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +129,133 @@        final int offsetCheckpointed = 1000;\n        createStateForRestoration(INPUT_STREAM);\n        setCommittedOffset(INPUT_STREAM, offsetLimitDelta);\n\n        final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(props), new MockTime());"
  },
  {
    "id" : "99f93d93-f763-42d7-b0dc-bb2dc0978afc",
    "prId" : 5163,
    "prUrl" : "https://github.com/apache/kafka/pull/5163#pullrequestreview-127409417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b5a4e83-a69e-438c-bbd0-323e10635e0f",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: indention should only be 4 spaces?",
        "createdAt" : "2018-06-10T20:48:53Z",
        "updatedAt" : "2018-06-11T17:05:19Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca40f17ef652a219d23b27ed11e2e8d9dbb748ca",
    "line" : 179,
    "diffHunk" : "@@ -1,1 +208,212 @@\n        builder.table(INPUT_STREAM, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.as(\"store\"))\n                .toStream()\n                .foreach(new ForeachAction<Integer, Integer>() {\n                    @Override"
  },
  {
    "id" : "532f1bdd-c7d3-4d7b-8d09-aee01ab82aae",
    "prId" : 8248,
    "prUrl" : "https://github.com/apache/kafka/pull/8248#pullrequestreview-408598738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "159db412-984c-48b6-ac11-a38e5592b590",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Alright I tried to shore up the test to verify that we actually do not restore anything in addition to not closing the store itself. This should be closer to testing the specific behavior pointed out in the ticket",
        "createdAt" : "2020-05-09T01:32:11Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "30ac7b3ccd47063497c17ac148d90f9b29683e82",
    "line" : 242,
    "diffHunk" : "@@ -1,1 +330,334 @@\n    @Test\n    public void shouldRecycleStateFromStandbyTaskPromotedToActiveTaskAndNotRestore() throws Exception {\n        final StreamsBuilder builder = new StreamsBuilder();\n        builder.table("
  },
  {
    "id" : "4c02e46d-5953-431c-8b6d-801a60693ea0",
    "prId" : 8248,
    "prUrl" : "https://github.com/apache/kafka/pull/8248#pullrequestreview-421013104",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d319ccd-24e2-42a6-93dd-20424c3af2a1",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Note for the future: it's not necessary to prefix the temp directory.",
        "createdAt" : "2020-05-29T14:34:35Z",
        "updatedAt" : "2020-05-29T14:39:11Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "30ac7b3ccd47063497c17ac148d90f9b29683e82",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +113,117 @@        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n        streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(appId).getPath());\n        streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());\n        streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());"
  },
  {
    "id" : "cc1dcdff-8735-46a0-af90-f7aac298237e",
    "prId" : 8530,
    "prUrl" : "https://github.com/apache/kafka/pull/8530#pullrequestreview-398407846",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7acad2f-36cf-4308-bce8-adab90448ee5",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Similar here, for this test I reduced the session / heartbeat timeout so that their rebalance timeout could be much smaller. I think it is simpler than changing a bunch of changelogs / source / sink / and app ids.",
        "createdAt" : "2020-04-22T17:21:00Z",
        "updatedAt" : "2020-04-22T17:22:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1bcd1db9aff9c52e0aca5dd788000f748ab477c9",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +105,109 @@        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);\n        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        streamsConfiguration.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 1000);\n        streamsConfiguration.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 300);\n        return streamsConfiguration;"
  },
  {
    "id" : "3553ff7c-08a7-48d0-ac01-92e9cf5f2f57",
    "prId" : 8900,
    "prUrl" : "https://github.com/apache/kafka/pull/8900#pullrequestreview-435426452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c69f377a-120c-4b4c-acd8-88b903b878c0",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Saw this fail locally so just did a minor flaky test fix on the side",
        "createdAt" : "2020-06-23T04:12:27Z",
        "updatedAt" : "2020-06-23T22:09:07Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "20321fd1325d402405a4b5899c22c83a51732b1c",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +362,366 @@        // Sometimes the store happens to have already been closed sometime during startup, so just keep track\n        // of where it started and make sure it doesn't happen more times from there\n        final int initialStoreCloseCount = CloseCountingInMemoryStore.numStoresClosed();\n        assertThat(restoreListener.totalNumRestored(), CoreMatchers.equalTo(0L));\n"
  }
]