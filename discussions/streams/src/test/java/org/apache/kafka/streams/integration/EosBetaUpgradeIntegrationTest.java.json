[
  {
    "id" : "2fda4b01-1e3d-4d8f-bd05-3c10debaeb36",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-395028242",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ac24df3-13ca-4fef-a870-e16b4d71b306",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could we just use a boolean flag as parameter to determine whether to only read committed data?",
        "createdAt" : "2020-04-16T16:40:22Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "287515ed-38e6-4a58-842f-39cf57d737c4",
        "parentId" : "7ac24df3-13ca-4fef-a870-e16b4d71b306",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "+1 It seems we do not need the actual groupId here, just a boolean flag.",
        "createdAt" : "2020-04-16T22:24:32Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 972,
    "diffHunk" : "@@ -1,1 +970,974 @@    }\n\n    private List<KeyValue<Long, Long>> readResult(final int numberOfRecords,\n                                                  final boolean readCommitted) throws Exception {\n        if (readCommitted) {"
  },
  {
    "id" : "a3f4d219-87c4-468b-8ad2-3ba612e4b520",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-395119860",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff3bc3fe-3c66-4a8f-9a51-bb3283893e7f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "In the current settings, with either alpha or beta, we will have one producer per thread since each thread would host one task only, right? Should we have 4 partitions so that under alpha we will have two producers and two txns per thread?",
        "createdAt" : "2020-04-16T22:14:06Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "a5c20438-aa74-42aa-9932-2123b80fdc9b",
        "parentId" : "ff3bc3fe-3c66-4a8f-9a51-bb3283893e7f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "That is correct. I did consider using 4 partitions for the same reason, but was not sure if it would add value to the test? In the end, the \"gap\" between eos-alpha and eos-beta is not the number to open transaction, but the usage of different `transactional.id`s between a task-producer and a thread-producer and this gap is closed via \"fetch offset fencing\". Hence, if the \"fetch offset fencing\" works for one task-producer vs one thread-producer (both using different txId), it also works for two task-producers vs one thread-producer?\r\n\r\nThoughts?",
        "createdAt" : "2020-04-16T23:01:41Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "add4e316-ebf8-4892-a3a3-5e4eea9a46a0",
        "parentId" : "ff3bc3fe-3c66-4a8f-9a51-bb3283893e7f",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "What I'm thinking is when there are two running clients, one with eos-alpha and another with eos-beta (upgraded from eos-alpha) the number of transaction.ids is actually reduced, and hence the number of max in-flight txns, and logically I agree they should not have much impact, but hey without the testing we don't know about what we don't know right? If you think such \"changes of number of txn.ids and hence number of txns\" has been covered in other system tests then probably it's fine. But if using 4 partitions isn't going to make the test more complex / takes much longer, could we be a bit over-cautious here?",
        "createdAt" : "2020-04-17T01:16:44Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "90fe6775-1a3d-4d91-a094-4ff57ad7a4b0",
        "parentId" : "ff3bc3fe-3c66-4a8f-9a51-bb3283893e7f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Sure. Works for me.",
        "createdAt" : "2020-04-17T02:24:45Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 200,
    "diffHunk" : "@@ -1,1 +198,202 @@        // picked up, i.e., GroupCoordinator fencing works correctly.\n        //\n        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n        // control when a commit actually happens. We use an input topic with 4 partitions and each task will request\n        // a commit after processing 10 records."
  },
  {
    "id" : "682d5939-6037-4940-a95b-3427d46b84f7",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-403183253",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87a3be85-626f-4bac-a599-283abc200492",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "The test fails here... (cf. over TODO)",
        "createdAt" : "2020-04-30T03:50:11Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 389,
    "diffHunk" : "@@ -1,1 +387,391 @@                final List<KeyValue<Long, Long>> expectedCommittedResult =\n                    computeExpectedResult(committedInputDataDuringFirstUpgrade, committedState);\n                verifyCommitted(expectedCommittedResult);\n            } else {\n                // retrying TX"
  },
  {
    "id" : "5de65042-fbd4-411a-972e-de187d4a6530",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-403183398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e48d38d-cb14-42ba-812b-4180b3a33242",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is the workaround that make the test (clean run) pass. For the error injection run, the test passed w/ and w/o this partitioner.",
        "createdAt" : "2020-04-30T03:50:51Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 882,
    "diffHunk" : "@@ -1,1 +880,884 @@        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG), 5 * 1000 - 1);\n        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), MAX_POLL_INTERVAL_MS);\n        properties.put(StreamsConfig.producerPrefix(ProducerConfig.PARTITIONER_CLASS_CONFIG), KeyPartitioner.class);\n        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n        properties.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath() + File.separator + appDir);"
  },
  {
    "id" : "fedaee8f-0de0-4062-a4f2-b49bf71e4770",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-403782661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbfb88e4-5eb2-4197-9ea0-92ae0ba03a64",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is just needed to make the partitioner work for writing into input topics and to use within KS to write into output topic.",
        "createdAt" : "2020-04-30T03:51:38Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "8684ef3b-bea5-4728-9331-58f3d848409d",
        "parentId" : "bbfb88e4-5eb2-4197-9ea0-92ae0ba03a64",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hmm, this sounds to me that the StreamProducer's own `partitionsFor` did not return the num.partitions so we ended up calling `send` with `partition == null`, since otherwise we will get the `partition` as\r\n\r\n```\r\npartition = partitioner.partition(topic, key, value, partitions.size());\r\n```\r\n\r\nwhere `partitioner` is the `StreamsPartitioner` and the producer's own partitioner should not be used. ",
        "createdAt" : "2020-04-30T18:25:15Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "e8ba4bda-9aff-4f42-b8d5-afe95c7501a3",
        "parentId" : "bbfb88e4-5eb2-4197-9ea0-92ae0ba03a64",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I don't think so. The original impl (just for the upstream producer to write into the input topics) was:\r\n```\r\nreturn ((Long) key).intValue() % NUM_TOPIC_PARTITIONS;\r\n```\r\n\r\nHowever, this assumes that `key` is of type `Long` what is not true when used within streams, because Streams does serialize all data upfront and `key` and `value` type is `byte[]` -- thus, we need to deserialize  to get the original key object.",
        "createdAt" : "2020-04-30T18:32:32Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "75d4e670-f0e0-47ea-ab1a-4ba7613a2c88",
        "parentId" : "bbfb88e4-5eb2-4197-9ea0-92ae0ba03a64",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "What I was asking is for the necessity of \r\n\r\n```\r\nproperties.put(StreamsConfig.producerPrefix(ProducerConfig.PARTITIONER_CLASS_CONFIG), KeyPartitioner.class);\r\n```\r\n\r\nAs I mentioned, Streams has its own StreamsPartitioner, and if it can get the actual not-null `partition` value passing to the `send` call, then the embedded producer's partitioner would not be used. Maybe I missed something critical here --- did you mean this config is only used for sending data to the source topics? If yes why put it into a streams props?",
        "createdAt" : "2020-04-30T18:41:46Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 1065,
    "diffHunk" : "@@ -1,1 +1063,1067 @@                             final byte[] valueBytes,\n                             final Cluster cluster) {\n            return LONG_DESERIALIZER.deserialize(topic, keyBytes).intValue() % NUM_TOPIC_PARTITIONS;\n        }\n"
  },
  {
    "id" : "8ddffa58-7569-4aeb-8a97-540c611e13ff",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-403184063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99218398-6998-492f-89c8-7df1eaad802e",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "We use 6 clients now, to do some error injection in \"mixed mode\". To avoid JXM warnings, we cannot create all clients at the same time and thus cannot use try-with-resources...",
        "createdAt" : "2020-04-30T03:53:51Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 256,
    "diffHunk" : "@@ -1,1 +254,258 @@        KafkaStreams streams2Beta = null;\n\n        try {\n            // phase 1: start both clients\n            streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);"
  },
  {
    "id" : "238d5195-8fec-46d9-bda5-bebb0787703d",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-403184310",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da6ec8d3-ecf9-48ed-904a-2106f70fcea0",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is new: for the crash case, in inject more errors in mixed mode, after we called `sendOffsetsToTransaction()` but before actually committing.",
        "createdAt" : "2020-04-30T03:55:03Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 442,
    "diffHunk" : "@@ -1,1 +440,444 @@            verifyCommitted(expectedCommittedResultAfterRestartFirstClient);\n\n            // phase 6: (complete second batch of data; crash: let second client fail on commit)\n            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n            //"
  },
  {
    "id" : "9a11a5d9-72b5-4c35-8e55-970d3724915f",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-403184711",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "879a4592-cc9e-45f7-a760-1042e6ceaa64",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is the second part of the \"mixed mode\" test: client1 is on eos-beta and client2 is on eoa-alpha. In the first part above, we crashed the second client and restarted it in eos-alpha mode. In this second part, we crash client1 and restart it in eos-beta mode.\r\n\r\nThe actual upgrade continues in the next phase.",
        "createdAt" : "2020-04-30T03:56:49Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 523,
    "diffHunk" : "@@ -1,1 +521,525 @@            }\n\n            // 7. only for crash case:\n            //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n            //     7b. write third batch of input data"
  },
  {
    "id" : "739243c0-1c57-463e-b6bc-88561f949637",
    "prId" : 8496,
    "prUrl" : "https://github.com/apache/kafka/pull/8496#pullrequestreview-403185649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3fc798a-565c-46b9-80ed-4f5ac5afa25d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "With 4 partitions, we need a custom partitioner to make sure we write the 4 different keys into 4 different partitions -- the default partitioner would only write data to 2 partitions (this behavior, ie, empty partitions vs. non-empty partitions, seems to be related to the bug when the test fails -- ensuring that data is written into all partitions avoids the issue).",
        "createdAt" : "2020-04-30T04:00:48Z",
        "updatedAt" : "2020-05-01T21:39:59Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "346c7b4fe34cf6870cd6f6d28365d10ef39a141d",
    "line" : 953,
    "diffHunk" : "@@ -1,1 +951,955 @@            LongSerializer.class\n        );\n        config.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG, KeyPartitioner.class.getName());\n        IntegrationTestUtils.produceKeyValuesSynchronously(\n            MULTI_PARTITION_INPUT_TOPIC,"
  }
]