[
  {
    "id" : "722d7621-8dfc-4f6a-a698-63f03d6e0dd0",
    "prId" : 4365,
    "prUrl" : "https://github.com/apache/kafka/pull/4365#pullrequestreview-86067703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2b12457-8714-4d0c-b335-843238c1c8da",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "maybe add a private method for adding records to `mockConsumer` for here and below?",
        "createdAt" : "2018-01-01T19:14:52Z",
        "updatedAt" : "2018-01-01T19:14:52Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fc84a53065fce2f4d15418a5f36c8d839ed1ae5",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +856,860 @@\n        long offset = -1;\n        mockConsumer.addRecord(new ConsumerRecord<>(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));\n        mockConsumer.addRecord(new ConsumerRecord<>(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));\n        thread.runOnce(-1);"
  },
  {
    "id" : "1b440d11-c8f0-45e6-8377-3a3b9ae8c93d",
    "prId" : 5428,
    "prUrl" : "https://github.com/apache/kafka/pull/5428#pullrequestreview-148270649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "500ebcc0-d624-4bd0-b7ac-2fa48dfc35a4",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Why did this go from 2 to 1? other than not passing an arg to `runOnce` the test logic to this point hasn't changed",
        "createdAt" : "2018-08-16T18:02:59Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "29f963a5-f4a5-48e7-839c-f23d7d1978fe",
        "parentId" : "500ebcc0-d624-4bd0-b7ac-2fa48dfc35a4",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The logic does have changed: in the old code we will commit twice on producer, one during the rebalance and one from the elapsed time. In the new code, the optimization I added will realize that nothing has been generated since the last commit, and hence we will skip committing in this case. \r\n\r\nThinking about it, this does have a side-effect though since for EOS if commit was not called in a long time then txn will be aborted, and if producer does not talk to txn coordinator even longer it could be removed as well. But personally I think it is okay for such scenario to happen, since really no data was generated, and hence committing an empty txn does not really make sense, and we should rather increase the txn expiration time in this case. WDYT? @mjsax @vvcephei ",
        "createdAt" : "2018-08-21T01:16:54Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0ab3d996-98c5-4ff2-b4f3-96bbed206cae",
        "parentId" : "500ebcc0-d624-4bd0-b7ac-2fa48dfc35a4",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I guess that we always have a transaction open, not just when we have something to commit.\r\n\r\nIt seems like one solution is to open a transaction only when we have data to process. Although this might complicate things.\r\n\r\nAlternatively, is there a way to periodically send a \"keep alive\" message to let the broker know we do still intend to use that transaction? It seems like either this or just abort/close the empty txn and re-open is better than a super-long expiration time. Otherwise, why is there even an expiration time?\r\n\r\nIs there any tradeoff between having one transaction open for a super long time, vs periodically closing empty transactions and starting new ones?",
        "createdAt" : "2018-08-21T21:23:35Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "2e9b3d57-9f83-4b25-bc50-800c6ce89685",
        "parentId" : "500ebcc0-d624-4bd0-b7ac-2fa48dfc35a4",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Completing a txn and starting a new one come with some cost, and hence is what we want to avoid generally.\r\n\r\nOn the other hand, we do not yet have a mechanism for \"keep alive\": with that, I think keeping a long lived EMPTY txn is okay, note that if the txn is not empty, then not committing it in time will increase the latency. Hence I'm only trying to optimize the case when the txn is empty.",
        "createdAt" : "2018-08-21T21:57:21Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "57608d7f-c74f-4e15-9ca0-e114501faee5",
        "parentId" : "500ebcc0-d624-4bd0-b7ac-2fa48dfc35a4",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I just talked to @hachikuji about this. Not committing is actually fine. Note, that beginTx() is a client local state transition -- nothing is written to the log (there are no \"begin tx markers\") and the TC state is also not modified. This implies, that the transaction timeout is not started on beginTx() -- the timeout only starts after the first record was written to the log. Thus, we don't need \"keep alive heartbeats\" and don't need to tell users to increase the tx timeout for low traffic topics that might have longer periods with no data.",
        "createdAt" : "2018-08-21T22:09:46Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "71b2b16d0b0f37a05af3df60ae9b5ff88649a7a4",
    "line" : 209,
    "diffHunk" : "@@ -1,1 +767,771 @@                @Override\n                public boolean conditionMet() {\n                    return producer.commitCount() == 1;\n                }\n            },"
  },
  {
    "id" : "0f669d2e-25bf-4523-8c74-88a97daf760d",
    "prId" : 6055,
    "prUrl" : "https://github.com/apache/kafka/pull/6055#pullrequestreview-187215944",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e67a1558-bf03-4c7c-958e-f5ce24d56f4d",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "very nice cleanup!",
        "createdAt" : "2018-12-20T22:48:31Z",
        "updatedAt" : "2019-01-08T21:31:03Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "786d82d2c2844c681ccb8332f0784f88f22932c3",
    "line" : 540,
    "diffHunk" : "@@ -1,1 +1032,1036 @@            public void init(final ProcessorContext context) {\n                context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, punctuatedStreamTime::add);\n                context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, punctuatedWallClockTime::add);\n            }\n"
  },
  {
    "id" : "67146936-bf94-46bd-8c1b-3f6a24c2d629",
    "prId" : 6055,
    "prUrl" : "https://github.com/apache/kafka/pull/6055#pullrequestreview-188899885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "484221b2-341c-405b-942d-e860b9f4ac55",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "nice!",
        "createdAt" : "2019-01-03T04:48:53Z",
        "updatedAt" : "2019-01-08T21:31:03Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "786d82d2c2844c681ccb8332f0784f88f22932c3",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +352,356 @@        // processed one record, punctuated after the first record, and hence num.iterations is still 1\n        long offset = -1;\n        addRecord(mockConsumer, ++offset, 0L);\n        thread.runOnce();\n"
  },
  {
    "id" : "0f9223a5-88df-4209-9fd1-920c24d61381",
    "prId" : 7021,
    "prUrl" : "https://github.com/apache/kafka/pull/7021#pullrequestreview-259241614",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc28bb23-5bf6-4bce-a5c8-5e7b512922ff",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "To test the race condition, should we call this when `newState == PARTITIONS_REVOKED`, ie, one state transition earlier?",
        "createdAt" : "2019-07-09T01:44:04Z",
        "updatedAt" : "2019-07-10T15:28:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "6e6629cf-f41d-4d12-bbc6-cab092637d53",
        "parentId" : "fc28bb23-5bf6-4bce-a5c8-5e7b512922ff",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "We couldn't reproduce the NPE in `partitionRevoked` because it's not expected to return records from `poll`. We need to trigger shutdown exactly during the call the `partitionAssigned` so that we are expecting records to return to attempt adding records to task manager. \r\nHowever current solution is not stable either, because it's not a correct way to trigger the condition. The natural way should leverage mock consumer to trigger assignment calls. Maybe we could talk in person and see how to reproduce it.",
        "createdAt" : "2019-07-09T02:33:21Z",
        "updatedAt" : "2019-07-10T15:28:46Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c6ec626b0d4a4252209fffa8f705fcda739a2e7",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +725,729 @@                assertNotNull(streamThread);\n                if (shutdownOnPoll) {\n                    streamThread.shutdown();\n                }\n                streamThread.rebalanceListener.onPartitionsAssigned(assignedPartitions);"
  },
  {
    "id" : "69ea5f58-521f-429c-88b8-e0384adbb6a6",
    "prId" : 7382,
    "prUrl" : "https://github.com/apache/kafka/pull/7382#pullrequestreview-292617274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ced369d-5e77-4a54-854c-9ac98a64bdea",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "See the other comment below in `MockClientSupplier.java`",
        "createdAt" : "2019-09-24T18:21:35Z",
        "updatedAt" : "2019-09-27T19:06:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f40f55cb0194c0daccd982aad3e4eebf68a638f",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +224,228 @@\n    private Cluster createCluster() {\n        final Node node = new Node(-1, \"localhost\", 8121);\n        return new Cluster(\n            \"mockClusterId\","
  },
  {
    "id" : "01a49079-4e09-40c5-8145-3ec16a043d70",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353368873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "988c41de-021b-40b7-8828-b9bcfbc08c59",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Did you want to fix this as part of this PR or as a follow-on?",
        "createdAt" : "2020-02-04T22:49:18Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "e563f892-54fc-42c9-8e5a-c799a8069f8e",
        "parentId" : "988c41de-021b-40b7-8828-b9bcfbc08c59",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "As a follow-up.",
        "createdAt" : "2020-02-04T23:11:08Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 1046,
    "diffHunk" : "@@ -1,1 +1369,1373 @@    @Ignore\n    @Test\n    // FIXME: should unblock this test after we added invalid offset handling\n    public void shouldRecoverFromInvalidOffsetExceptionOnRestoreAndFinishRestore() throws Exception {\n        internalStreamsBuilder.stream(Collections.singleton(\"topic\"), consumed)"
  },
  {
    "id" : "f7056cbd-33a8-4adc-bd48-9fcaa2b4356f",
    "prId" : 8220,
    "prUrl" : "https://github.com/apache/kafka/pull/8220#pullrequestreview-370705841",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4b9f7c4-88df-4ee7-bebf-fd07c7de5ca0",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hmm :) it reminds me that the mock consumer's behavior is not exactly the same as the actual consumer (the later would filter, the former would throw), but perhaps this worth a different PR to cleanup. @abbccdda could you file a JIRA for it?",
        "createdAt" : "2020-03-07T00:10:44Z",
        "updatedAt" : "2020-03-07T00:11:03Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0171cba3-0e95-4af9-962c-8d046581bcd2",
        "parentId" : "e4b9f7c4-88df-4ee7-bebf-fd07c7de5ca0",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sounds good to me! https://issues.apache.org/jira/browse/KAFKA-9679",
        "createdAt" : "2020-03-07T01:01:51Z",
        "updatedAt" : "2020-03-07T01:01:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d4c44254016f6c41f3429ab6a66dd7c203e0d482",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +864,868 @@        EasyMock.verify(taskManager);\n\n        // The Mock consumer shall throw as the assignment has been wiped out, but records are assigned.\n        assertEquals(\"No current assignment for partition topic1-1\", thrown.getMessage());\n        assertFalse(consumer.shouldRebalance());"
  }
]