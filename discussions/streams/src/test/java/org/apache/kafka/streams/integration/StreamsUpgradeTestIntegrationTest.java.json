[
  {
    "id" : "d4ccab60-c6ff-4dc2-9056-faf213233a40",
    "prId" : 7248,
    "prUrl" : "https://github.com/apache/kafka/pull/7248#pullrequestreview-306852295",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b9fa80b-813e-4c5c-b051-ed2662d61782",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I accidentally broke the version probing system test when I made the code changes for this PR, and it was super-hard to track down why in system test form, so I added an integration test for the system test code. This seems like a reasonable practice, since we don't seem to be able to run the system tests locally; an integration test gives us the ability to find out right away if we have broken the system test logic, and also gives us a way to verify a fix without waiting a minimum of a half-hour for the system test to run.",
        "createdAt" : "2019-10-24T14:45:39Z",
        "updatedAt" : "2019-10-29T21:04:35Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "5a17d432-692c-4773-a148-5deff4bfbf36",
        "parentId" : "8b9fa80b-813e-4c5c-b051-ed2662d61782",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I think this is ok.   But let's do it on a case by case basis, i.e., not require a likewise integration test for each system test.  I'm don't think that's your intent, but I just wanted to be clear.\r\n\r\nWhat was the change that broke the system test?",
        "createdAt" : "2019-10-24T17:09:14Z",
        "updatedAt" : "2019-10-29T21:04:35Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "2b154a97-6301-4604-b0e5-9c6afe8ab06d",
        "parentId" : "8b9fa80b-813e-4c5c-b051-ed2662d61782",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Sounds wise :)\r\n\r\nThe FutureVersion stuff in the upgrade test previously relied on serialization methods in SubscriptionInfo that now no longer exist. I tried just deleting all the extra fields and just encoding the used and supported versions only in the message, reasoning that these are all that's really required to test version probing.\r\n\r\nWell, it turns out the test does a little more than I thought, and it actually expects the whole cluster to upgrade to the \"future version\", so it actually does need to encode the tasks etc.\r\n\r\nWhat I've done now is just inline all the logic to encode the full \"future version\" subscription the same way we always have, so that the test can continue to work the same way. I think this is reasonable, since the logic of the test is explicitly mocking out a \"future\" message, not intentionally depending on anything about the way that the current messages are encoded.",
        "createdAt" : "2019-10-24T20:44:01Z",
        "updatedAt" : "2019-10-29T21:04:35Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4d6ae101dee1bb04c4a267771e3f9ac6ec347eb",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +41,45 @@\n@Category(IntegrationTest.class)\npublic class StreamsUpgradeTestIntegrationTest {\n    @ClassRule\n    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(3);"
  },
  {
    "id" : "8a69e400-cb9a-4941-a36e-11709140ece9",
    "prId" : 7248,
    "prUrl" : "https://github.com/apache/kafka/pull/7248#pullrequestreview-306853757",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d3e67b9-18e7-4520-9047-af7add8a2184",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Why have`kafkaStreamsN.close(Duration.ZERO)` (which according to JavaDoc blocks forever), followed by `kafkaStreamsN.close()` which blocks for `Long.MAX_VALUE`?",
        "createdAt" : "2019-10-24T16:53:32Z",
        "updatedAt" : "2019-10-29T21:04:35Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "b629f9d8-56bc-4165-b900-615d98203bc7",
        "parentId" : "3d3e67b9-18e7-4520-9047-af7add8a2184",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Does the javadoc say that? (confirmed, yes it does)\r\n\r\n`close(Duration.ZERO)` is async, so the effect of this code is just to send the \"close\" signal to all the instances at the same time, and then wait for them all to shut down.\r\n\r\nI'll send a new PR to fix the javadoc.",
        "createdAt" : "2019-10-24T20:46:43Z",
        "updatedAt" : "2019-10-29T21:04:35Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4d6ae101dee1bb04c4a267771e3f9ac6ec347eb",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +97,101 @@        kafkaStreams4.close();\n        kafkaStreams5.close();\n        kafkaStreams6.close();\n    }\n"
  },
  {
    "id" : "83fcbb63-58b2-46cc-9186-00619fb41eab",
    "prId" : 7248,
    "prUrl" : "https://github.com/apache/kafka/pull/7248#pullrequestreview-308234938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7c832a8-a40e-4614-bad7-39b4f78eff5b",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "ping @ableegoldman to also take a look at this test logic, are we narrowing down the path sufficiently?",
        "createdAt" : "2019-10-29T02:26:51Z",
        "updatedAt" : "2019-10-29T21:04:35Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4d6ae101dee1bb04c4a267771e3f9ac6ec347eb",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +88,92 @@        final KafkaStreams kafkaStreams6 = buildFutureStreams(usedVersion6);\n        startSync(kafkaStreams6);\n        assertThat(usedVersion6.get(), is(LATEST_SUPPORTED_VERSION + 1));\n        assertThat(usedVersion5.get(), is(LATEST_SUPPORTED_VERSION + 1));\n        assertThat(usedVersion4.get(), is(LATEST_SUPPORTED_VERSION + 1));"
  },
  {
    "id" : "bf443ab7-58b2-4fac-b8ff-780f16930893",
    "prId" : 7974,
    "prUrl" : "https://github.com/apache/kafka/pull/7974#pullrequestreview-344119728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d85fdd4e-6177-4f9b-a030-182a0c698739",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Should we consider doing the same for second / first roll as well?",
        "createdAt" : "2020-01-16T17:37:15Z",
        "updatedAt" : "2020-01-16T18:19:46Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "9019982f-880d-44ce-bf13-8720bd3040a1",
        "parentId" : "d85fdd4e-6177-4f9b-a030-182a0c698739",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks, @guozhangwang . It _should_ be unnecessary, since they already block until running (in startSync), and they should have gotten the expected number deterministically before transitioning to running. Time will tell if I am wrong, though...",
        "createdAt" : "2020-01-16T18:21:28Z",
        "updatedAt" : "2020-01-16T18:21:29Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "31a62ed4d626b86e7509073ceb37d6ee5f9d5490",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +89,93 @@        final KafkaStreams kafkaStreams6 = buildFutureStreams(usedVersion6);\n        startSync(kafkaStreams6);\n        retryOnExceptionWithTimeout(() -> assertThat(usedVersion6.get(), is(LATEST_SUPPORTED_VERSION + 1)));\n        retryOnExceptionWithTimeout(() -> assertThat(usedVersion5.get(), is(LATEST_SUPPORTED_VERSION + 1)));\n        retryOnExceptionWithTimeout(() -> assertThat(usedVersion4.get(), is(LATEST_SUPPORTED_VERSION + 1)));"
  }
]