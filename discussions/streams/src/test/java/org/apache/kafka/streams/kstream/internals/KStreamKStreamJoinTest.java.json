[
  {
    "id" : "1c295067-8ea5-46e7-9d7d-2fdecc175546",
    "prId" : 4832,
    "prUrl" : "https://github.com/apache/kafka/pull/4832#pullrequestreview-115690115",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31af4605-83ba-42af-b2d6-dd62dce44f2e",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ditto on removing these before/after methods.",
        "createdAt" : "2018-04-26T18:49:28Z",
        "updatedAt" : "2018-04-26T19:12:59Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc5df4c3ba297c3fe2933b2adcb7ee623da698b3",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +78,82 @@            driver.close();\n        }\n        driver = null;\n    }\n"
  },
  {
    "id" : "70fdcc82-fa4c-4bad-b95e-ba0b6ad77756",
    "prId" : 4832,
    "prUrl" : "https://github.com/apache/kafka/pull/4832#pullrequestreview-115980452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c7d4d75-3b8e-4f19-9114-609396d7e3c7",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "The usage of `time` in this test is a little hard to follow. Can we either explicitly set the scalar time value as in your other tests or use `advanceTimeMs`?  Or is there some other reason for the variable that I missed?",
        "createdAt" : "2018-04-26T18:53:32Z",
        "updatedAt" : "2018-04-26T19:12:59Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "569e4f5f-726a-4f3e-9721-15cfdeb5e3dc",
        "parentId" : "5c7d4d75-3b8e-4f19-9114-609396d7e3c7",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Actually, on closer reading, I think it was like this because the old driver didn't have a way to \"advance\" the time, just to set it, and the test logic wants to \"advance\" the time inside a loop at some points, so I think we should delete the variable and use `advanceTimeMs` instead.",
        "createdAt" : "2018-04-26T18:55:45Z",
        "updatedAt" : "2018-04-26T19:12:59Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "860f6402-6e76-44d6-88ae-f7f6291f3e46",
        "parentId" : "5c7d4d75-3b8e-4f19-9114-609396d7e3c7",
        "authorId" : "796fe0a1-c8b6-4b8f-a416-408b76bf799e",
        "body" : "Yes, using a variable to store the time makes it a bit convoluted. I'll change it to `advanceTimeMs`",
        "createdAt" : "2018-04-27T11:08:28Z",
        "updatedAt" : "2018-04-27T11:08:28Z",
        "lastEditedBy" : "796fe0a1-c8b6-4b8f-a416-408b76bf799e",
        "tags" : [
        ]
      },
      {
        "id" : "2b758f42-2bee-46de-887b-d3cfdaa85584",
        "parentId" : "5c7d4d75-3b8e-4f19-9114-609396d7e3c7",
        "authorId" : "796fe0a1-c8b6-4b8f-a416-408b76bf799e",
        "body" : "While I was changing it I realised that, although it would be cleaner to remove `time` and use `advanceTimeMs` instead, it is actually easier to use `time` because the tests require the time to advance and recede.",
        "createdAt" : "2018-04-27T15:25:42Z",
        "updatedAt" : "2018-04-27T15:25:42Z",
        "lastEditedBy" : "796fe0a1-c8b6-4b8f-a416-408b76bf799e",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc5df4c3ba297c3fe2933b2adcb7ee623da698b3",
    "line" : 242,
    "diffHunk" : "@@ -1,1 +342,346 @@        assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());\n\n        driver = new TopologyTestDriver(builder.build(), props, time);\n\n        // push two items to the primary stream. the other window is empty. this should produce no items."
  },
  {
    "id" : "1c7cf406-7450-4497-a641-faf93ed3b1b1",
    "prId" : 6447,
    "prUrl" : "https://github.com/apache/kafka/pull/6447#pullrequestreview-214787142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7519f7c-6fe1-4367-9fa4-ee0ad3140841",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "No need to set initial wall-clock time of the driver (removed to avoid confusion with event-time). Similar on other places.",
        "createdAt" : "2019-03-14T22:07:21Z",
        "updatedAt" : "2019-03-15T22:47:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc762eb14f8bf66eab01acccb93ebad26b446979",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +198,202 @@        assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());\n\n        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n            final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();\n"
  },
  {
    "id" : "76832918-b5b9-4867-bb96-67a455ad7afc",
    "prId" : 6447,
    "prUrl" : "https://github.com/apache/kafka/pull/6447#pullrequestreview-214787545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49182d84-d06f-4b31-ac14-81168e1fd52c",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This test does modify timestamps to test window boundaries, thus, result timestamps are not zero.",
        "createdAt" : "2019-03-14T22:08:35Z",
        "updatedAt" : "2019-03-15T22:47:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc762eb14f8bf66eab01acccb93ebad26b446979",
    "line" : 243,
    "diffHunk" : "@@ -1,1 +327,331 @@                driver.pipeInput(recordFactory.create(topic2, expectedKey, \"YY\" + expectedKey, time));\n            }\n            processor.checkAndClearProcessResult(\"0:X0+YY0 (ts: 1100)\", \"1:X1+YY1 (ts: 1100)\", \"2:X2+YY2 (ts: 1100)\", \"3:X3+YY3 (ts: 1100)\");\n\n            time += 1L;"
  },
  {
    "id" : "a63c2dc3-3c80-4f2f-be4e-30a15bf8cbb0",
    "prId" : 6447,
    "prUrl" : "https://github.com/apache/kafka/pull/6447#pullrequestreview-214787621",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bdd32217-dc2e-4af8-8931-03715df126fe",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "As above.",
        "createdAt" : "2019-03-14T22:08:49Z",
        "updatedAt" : "2019-03-15T22:47:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc762eb14f8bf66eab01acccb93ebad26b446979",
    "line" : 442,
    "diffHunk" : "@@ -1,1 +503,507 @@                driver.pipeInput(recordFactory.create(topic2, expectedKey, \"YY\" + expectedKey, time));\n            }\n            processor.checkAndClearProcessResult(\"0:X0+YY0 (ts: 1000)\");\n\n            time += 1L;"
  },
  {
    "id" : "369fd814-a61d-4a55-bff7-72bdba3b9cc4",
    "prId" : 6447,
    "prUrl" : "https://github.com/apache/kafka/pull/6447#pullrequestreview-214787680",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec525e0c-9ab9-4b18-9c51-9e8216a2f7ed",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "As above.",
        "createdAt" : "2019-03-14T22:08:59Z",
        "updatedAt" : "2019-03-15T22:47:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc762eb14f8bf66eab01acccb93ebad26b446979",
    "line" : 541,
    "diffHunk" : "@@ -1,1 +601,605 @@                driver.pipeInput(recordFactory.create(topic2, expectedKey, \"YY\" + expectedKey, time));\n            }\n            processor.checkAndClearProcessResult(\"0:X0+YY0 (ts: 900)\");\n\n            time += 1L;"
  },
  {
    "id" : "ae7d06a7-f0aa-4667-9350-fca8e5abc636",
    "prId" : 6933,
    "prUrl" : "https://github.com/apache/kafka/pull/6933#pullrequestreview-260480693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5594e9be-f437-4f6b-aef3-2fdf546c175d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: Avoid unnecessary reformatting.",
        "createdAt" : "2019-07-11T06:14:02Z",
        "updatedAt" : "2019-07-11T09:45:31Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fd06e50eae4c23a88290a3aa6f0c8e5e271a036",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +89,93 @@        final StreamsBuilder builder = new StreamsBuilder();\n\n        final int[] expectedKeys = new int[] {0, 1, 2, 3};\n\n        final KStream<Integer, String> stream1;"
  },
  {
    "id" : "8c779f8a-050c-46aa-8c16-57551007497d",
    "prId" : 6933,
    "prUrl" : "https://github.com/apache/kafka/pull/6933#pullrequestreview-260480693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14039f27-4e9d-457e-bd1e-d4feabee4eb1",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "as above",
        "createdAt" : "2019-07-11T06:14:10Z",
        "updatedAt" : "2019-07-11T09:45:31Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fd06e50eae4c23a88290a3aa6f0c8e5e271a036",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +196,200 @@        final StreamsBuilder builder = new StreamsBuilder();\n\n        final int[] expectedKeys = new int[] {0, 1, 2, 3};\n\n        final KStream<Integer, String> stream1;"
  },
  {
    "id" : "3cc6d28a-f4e8-4cae-a409-ce7c42c29f50",
    "prId" : 7285,
    "prUrl" : "https://github.com/apache/kafka/pull/7285#pullrequestreview-294407510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e32cc82c-1b2c-4000-9533-e48a265f779d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This should be multiple test. One for each case.",
        "createdAt" : "2019-09-26T17:07:03Z",
        "updatedAt" : "2019-09-28T16:01:43Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "0d799c9c-88cb-4169-9e94-195a96c673e1",
        "parentId" : "e32cc82c-1b2c-4000-9533-e48a265f779d",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Ack",
        "createdAt" : "2019-09-27T15:38:37Z",
        "updatedAt" : "2019-09-28T16:01:43Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "a450c43edf7bb2cb5c110cc60136676c73903ca7",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +96,100 @@\n    @Test\n    public void shouldThrowExceptionThisStoreSupplierRetentionDoNotMatchWindowsSizeAndGrace() {\n        // Case where retention of thisJoinStore doesn't match JoinWindows\n        final WindowBytesStoreSupplier thisStoreSupplier = buildWindowBytesStoreSupplier(\"in-memory-join-store\", 500, 100, true);"
  },
  {
    "id" : "1ffe5daa-17ea-4807-9424-6de2d72f025d",
    "prId" : 7285,
    "prUrl" : "https://github.com/apache/kafka/pull/7285#pullrequestreview-294407956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c6fbf0b-9da9-474b-badc-7761cdec7382",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Should this be it's one test?",
        "createdAt" : "2019-09-26T17:07:49Z",
        "updatedAt" : "2019-09-28T16:01:43Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fcc0b855-f5b5-4eeb-913b-95fe8b0d2df1",
        "parentId" : "1c6fbf0b-9da9-474b-badc-7761cdec7382",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Ack",
        "createdAt" : "2019-09-27T15:39:21Z",
        "updatedAt" : "2019-09-28T16:01:43Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "a450c43edf7bb2cb5c110cc60136676c73903ca7",
    "line" : 173,
    "diffHunk" : "@@ -1,1 +211,215 @@\n        //Case with this stream store supplier\n        runJoin(streamJoined.withThisStoreSupplier(thisStoreSupplier), joinWindows);\n\n        //Case with other stream store supplier"
  },
  {
    "id" : "c6c76669-551f-4087-b946-474c99d58377",
    "prId" : 7285,
    "prUrl" : "https://github.com/apache/kafka/pull/7285#pullrequestreview-294408096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "598b0ef1-2ef9-4cf0-a852-d88c08f45b64",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Should this be it's one test?",
        "createdAt" : "2019-09-26T17:07:55Z",
        "updatedAt" : "2019-09-28T16:01:43Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "c770e0e1-42c6-4270-8352-0026d18d1991",
        "parentId" : "598b0ef1-2ef9-4cf0-a852-d88c08f45b64",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Ack",
        "createdAt" : "2019-09-27T15:39:34Z",
        "updatedAt" : "2019-09-28T16:01:43Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "a450c43edf7bb2cb5c110cc60136676c73903ca7",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +214,218 @@\n        //Case with other stream store supplier\n        runJoin(streamJoined.withOtherStoreSupplier(otherStoreSupplier), joinWindows);\n\n"
  },
  {
    "id" : "a8266fd7-2b0e-48b3-8361-440f0bb89cf9",
    "prId" : 8504,
    "prUrl" : "https://github.com/apache/kafka/pull/8504#pullrequestreview-410450128",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22b1ff67-6874-4e12-b5d9-0a8fa1b3cd23",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is the default. Why setting is explicitly?",
        "createdAt" : "2020-05-12T21:53:39Z",
        "updatedAt" : "2020-05-22T02:43:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0861510c468819b94ae6013fe9d655ccac9b6f18",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +85,89 @@        final StreamsBuilder builder = new StreamsBuilder();\n        final Properties props = new Properties();\n        props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.NO_OPTIMIZATION);\n        final KStream<String, String> stream1 = builder.stream(\"topic\", Consumed.with(Serdes.String(), Serdes.String()));\n        final KStream<String, String> stream2 = builder.stream(\"topic2\", Consumed.with(Serdes.String(), Serdes.String()));"
  },
  {
    "id" : "a65f977d-ffd4-4000-b272-65fc4bb2919a",
    "prId" : 8504,
    "prUrl" : "https://github.com/apache/kafka/pull/8504#pullrequestreview-416364585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5b28211-1aab-44aa-9988-60fffc4717df",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Sorry for being undecided... Reading the code now, I am wondering if this behavior may become problematic with regard to topology upgrade. Assume, the first join is removed. Technically, the new topology is compatible, but we would now generate a new repartition topic name, and thus it's not compatible. This could be fixed by inserting a `repartition()` in the new code enforcing the old name -- however, this makes me wonder if we might want to throw a \"naming conflict\" (ie, cannot pick a name) exception based on the original topology for this case when both operators are named, and tell people to insert `repartition()` right away? For this case, if they later remove a join it's clear what is happening to them.\r\n\r\nIe, we should still not create two repartition topics what would be \"bad\" (user could still enforce if by calling `repartition()` twice), but just throw with an informative error message? -- Curious what @vvcephei thinks?",
        "createdAt" : "2020-05-12T22:04:30Z",
        "updatedAt" : "2020-05-22T02:43:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "9a128399-5b94-44b1-8195-0b204aefbb15",
        "parentId" : "d5b28211-1aab-44aa-9988-60fffc4717df",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : ">This could be fixed by inserting a repartition() i the new code enforcing the old name -- however, this make me wonder if we might want to throw a \"naming conflict\" (ie, cannot pick a name) exception based on the original topology for this case when both operators are named, and tell people to insert repartition() right away? For this case, if they later remove a join it's clear what is happening to them.\r\n\r\nI see your point, but I think that is a bad user experience and IMHO leaks too much detail about an operation we want to handle automatically.\r\n\r\nI'm leaning towards the simpler case of what we had before.  With generated names re-use the reputation node, but if the user creates a new join with explicit names, just go ahead and create two repartition topics.\r\n\r\nWDYT?",
        "createdAt" : "2020-05-20T22:26:26Z",
        "updatedAt" : "2020-05-22T02:43:12Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "0a6b4669-a386-4c4b-9c5f-82c47bd45d17",
        "parentId" : "d5b28211-1aab-44aa-9988-60fffc4717df",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for?\r\n\r\n\\cc @vvcephei @guozhangwang ",
        "createdAt" : "2020-05-21T00:11:32Z",
        "updatedAt" : "2020-05-22T02:43:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "a97ea6e0-d88c-44e7-a95f-d903ae33437e",
        "parentId" : "d5b28211-1aab-44aa-9988-60fffc4717df",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks for the discussion, all.\r\n\r\nComing back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named.\r\n\r\nThe purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense.\r\n\r\nWhen the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't \"solve\" it ;)\r\n\r\nIt's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program.\r\n\r\nThe other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way.\r\n\r\nCompatibility is a concern, and it seems like it's satisfied if we follow this path:\r\n1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program\r\n2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility.\r\n3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset.\r\n4. Adding Optimization is well defined to break compatibility, so people will know they have to reset.\r\n\r\nHave I missed some consideration?\r\nThanks,\r\n-John",
        "createdAt" : "2020-05-21T16:22:47Z",
        "updatedAt" : "2020-05-22T02:43:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "fb9d36bf-b7a4-45ae-ab8d-e1dca6503766",
        "parentId" : "d5b28211-1aab-44aa-9988-60fffc4717df",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Thanks @vvcephei -- that is convincing.",
        "createdAt" : "2020-05-21T17:47:46Z",
        "updatedAt" : "2020-05-22T02:43:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "53218186-a5bb-4fe9-bb91-578fdfc6f1e9",
        "parentId" : "d5b28211-1aab-44aa-9988-60fffc4717df",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Thanks for the discussion @vvcephei and @mjsax. I'll revert this PR to its original state which conforms to @vvcephei's comments above. ",
        "createdAt" : "2020-05-21T17:53:30Z",
        "updatedAt" : "2020-05-22T02:43:12Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "0861510c468819b94ae6013fe9d655ccac9b6f18",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +109,113 @@        final Topology topology =  builder.build(props);\n        System.out.println(topology.describe().toString());\n        assertEquals(expectedTopologyWithUserNamedRepartitionTopics, topology.describe().toString());\n    }\n"
  },
  {
    "id" : "5d78aed2-fb9e-4a6c-a493-e2b4c648391f",
    "prId" : 9708,
    "prUrl" : "https://github.com/apache/kafka/pull/9708#pullrequestreview-552306765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99f36d79-71fb-4fd7-ac57-4e869df78f05",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It seems like these would pass by default. Maybe we should check set some log configs and then check that they got propagated ? ",
        "createdAt" : "2020-12-10T17:53:29Z",
        "updatedAt" : "2020-12-15T15:42:52Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "26b0bf5e-5b30-4e39-aa41-8a88f7f2a085",
        "parentId" : "99f36d79-71fb-4fd7-ac57-4e869df78f05",
        "authorId" : "114424ac-2f76-47ba-b653-f85692b08607",
        "body" : "Yeah it does pass by default. I experimented with passing the logs in but I wasn't able to find a good way to confirm that the logs I was passing were getting set somewhere. Do you know where they're exposed for me to check? Materialized doesn't seem to check this either",
        "createdAt" : "2020-12-10T19:17:38Z",
        "updatedAt" : "2020-12-15T15:42:52Z",
        "lastEditedBy" : "114424ac-2f76-47ba-b653-f85692b08607",
        "tags" : [
        ]
      },
      {
        "id" : "da9ccaa3-0049-4b85-9f45-2748d9e29082",
        "parentId" : "99f36d79-71fb-4fd7-ac57-4e869df78f05",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Maybe there is an easier way, but I found the following:\r\n\r\nSet the config to:\r\n\r\n```\r\n        final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined\r\n            .with(Serdes.String(), Serdes.Integer(), Serdes.Integer())\r\n            .withStoreName(\"store\")\r\n            .withLoggingEnabled(Collections.singletonMap(\"test\", \"property\"));\r\n```\r\nand then check it:\r\n\r\n```\r\n        internalTopologyBuilder.buildSubtopology(0);\r\n\r\n        assertThat(internalTopologyBuilder.stateStores().get(\"store-this-join-store\").loggingEnabled(), equalTo(true));\r\n        assertThat(internalTopologyBuilder.stateStores().get(\"store-other-join-store\").loggingEnabled(), equalTo(true));\r\n        assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2));\r\n        for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) {\r\n            assertThat(\r\n                config.getProperties(Collections.emptyMap(), 0).get(\"test\"),\r\n                equalTo(\"property\")\r\n            );\r\n        }\r\n```\r\n\r\nWithout\r\n\r\n```\r\nassertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2));\r\n```\r\n\r\nthe test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.",
        "createdAt" : "2020-12-15T11:40:06Z",
        "updatedAt" : "2020-12-15T15:42:52Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "b43a480aacc197a275e9c8d1a6451cff3c449050",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +210,214 @@\n        assertThat(internalTopologyBuilder.stateStores().get(\"store-this-join-store\").loggingEnabled(), equalTo(true));\n        assertThat(internalTopologyBuilder.stateStores().get(\"store-other-join-store\").loggingEnabled(), equalTo(true));\n        assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2));\n        for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) {"
  }
]