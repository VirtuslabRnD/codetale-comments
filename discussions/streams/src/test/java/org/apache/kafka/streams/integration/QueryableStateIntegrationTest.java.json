[
  {
    "id" : "0141d1d9-b568-422d-95ed-49afd2b08d9b",
    "prId" : 5428,
    "prUrl" : "https://github.com/apache/kafka/pull/5428#pullrequestreview-144924661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5659f197-ab70-4301-8d8a-bdbbad60f5a2",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is also a flaky test that I discovered here.",
        "createdAt" : "2018-08-09T16:33:28Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "71b2b16d0b0f37a05af3df60ae9b5ff88649a7a4",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +585,589 @@        kafkaStreams.start();\n\n        waitUntilAtLeastNumRecordProcessed(outputTopic, 5);\n\n        final ReadOnlyKeyValueStore<String, Long>"
  },
  {
    "id" : "0837d6ca-acb1-4505-b399-0ca993410476",
    "prId" : 5987,
    "prUrl" : "https://github.com/apache/kafka/pull/5987#pullrequestreview-180554837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1238f84e-a703-44f1-a592-8c8180596c57",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is the actual fix.",
        "createdAt" : "2018-12-02T09:57:23Z",
        "updatedAt" : "2018-12-02T09:57:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6c98ff4a8b3029fc32143b64c59de9e493db184",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +194,198 @@\n        stringComparator = Comparator.comparing((KeyValue<String, String> o) -> o.key).thenComparing(o -> o.value);\n        stringLongComparator = Comparator.comparing((KeyValue<String, Long> o) -> o.key).thenComparingLong(o -> o.value);\n        inputValues = getInputValues();\n        inputValuesKeys = new HashSet<>();"
  },
  {
    "id" : "f4134672-e693-4fe0-a2d6-b950482e74b9",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-198402185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c47ef281-ea9f-4c53-adc6-c583a805a1b7",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is the effect of the optimization: we will only sent down one record now, and the second `null` record will be omitted.",
        "createdAt" : "2019-01-31T02:35:14Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +520,524 @@        kafkaStreams.start();\n\n        waitUntilAtLeastNumRecordProcessed(outputTopic, 1);\n\n        final ReadOnlyKeyValueStore<String, Long>"
  },
  {
    "id" : "89f9d619-cb91-424f-8e1b-c621246696c1",
    "prId" : 7519,
    "prUrl" : "https://github.com/apache/kafka/pull/7519#pullrequestreview-302113731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96fd47c0-f3a6-46d4-bf38-320f5423b166",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why not use `startKafkaStreamsAndWaitForRunningState()` here?",
        "createdAt" : "2019-10-15T08:13:35Z",
        "updatedAt" : "2019-10-15T18:31:45Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fcb152c9-3824-4607-909f-27df7045f19d",
        "parentId" : "96fd47c0-f3a6-46d4-bf38-320f5423b166",
        "authorId" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "body" : "+1",
        "createdAt" : "2019-10-15T15:41:42Z",
        "updatedAt" : "2019-10-15T18:31:45Z",
        "lastEditedBy" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "tags" : [
        ]
      },
      {
        "id" : "25ccdac5-5ec9-46cb-aad6-54e0a1888c39",
        "parentId" : "96fd47c0-f3a6-46d4-bf38-320f5423b166",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The streams instance already has a registered state listener above for further testing purposes, so I have to reuse that one than calling `startKafkaStreamsAndWaitForRunningState` to register a new one.",
        "createdAt" : "2019-10-15T16:56:40Z",
        "updatedAt" : "2019-10-15T18:31:45Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "5bd4e170-7181-404a-b662-3d0b2185d094",
        "parentId" : "96fd47c0-f3a6-46d4-bf38-320f5423b166",
        "authorId" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "body" : "Ahh! https://github.com/apache/kafka/pull/7500 allows you to wait for running state while still using an existing listener. But still waiting for it to get merged.",
        "createdAt" : "2019-10-15T17:01:48Z",
        "updatedAt" : "2019-10-15T18:31:45Z",
        "lastEditedBy" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "tags" : [
        ]
      },
      {
        "id" : "e9c90f2c-f20a-466c-b4f7-9f680022419f",
        "parentId" : "96fd47c0-f3a6-46d4-bf38-320f5423b166",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I left a comment on #7500, basically I think we do not need two functions in IntegrationTestUtils and StreamsTestUtils.",
        "createdAt" : "2019-10-15T18:34:10Z",
        "updatedAt" : "2019-10-15T18:34:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc343975fd2e3778e6fc7e0f6f3d757868f5c99a",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +273,277 @@                if (!stateListener.mapStates.containsKey(KafkaStreams.State.RUNNING))\n                    fail(\"Did not start successfully\");\n            }\n        }\n"
  },
  {
    "id" : "c6dc395e-89d7-4fe2-aad7-3703a90e8764",
    "prId" : 7548,
    "prUrl" : "https://github.com/apache/kafka/pull/7548#pullrequestreview-304268475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aba6872a-b7e1-4517-be5f-87356484f94c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good call!",
        "createdAt" : "2019-10-20T17:17:15Z",
        "updatedAt" : "2019-10-23T19:32:22Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2a41f690610f4c6eddfe70afae2acd7c058c76a",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +257,261 @@    }\n\n    private void verifyAllKVKeys(final List<KafkaStreams> streamsList,\n                                 final KafkaStreams streams,\n                                 final KafkaStreamsTest.StateListenerStub stateListener,"
  },
  {
    "id" : "0a16e531-3cba-4698-ae54-a1a0820f51a3",
    "prId" : 8370,
    "prUrl" : "https://github.com/apache/kafka/pull/8370#pullrequestreview-384894246",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acb634a4-614d-4663-8d08-627673e0d7c5",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This test becomes more flaky than before after the main loop refactoring, shown as the following:\r\n\r\n```\r\njava.lang.AssertionError: Did not receive all 48 records from topic output-concurrent-2 within 120000 ms\r\nExpected: is a value equal to or greater than <48>\r\n     but: <38> was less than <48>\r\n```\r\n\r\nEach iteration has 48 words, and 38 unique words, and when it fails it reported only 38 records ever consumed from the output topic. I suspect this is because of the caching effects, we processed all 500000 iterations without sending out any updates to the resulted count table output topic (seems we process too fast?). \r\n\r\nI piggy-backed a fix for this flakiness by reducing the requirement that we start the verification as long as we've seen one record in the output, similar as we do in other tests, and then in verifyGreaterOrEqual we do not require all keys been populated already.",
        "createdAt" : "2020-03-27T21:47:56Z",
        "updatedAt" : "2020-03-31T23:37:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f06e25a9-28e9-4a26-9f2f-5781548e5f5b",
        "parentId" : "acb634a4-614d-4663-8d08-627673e0d7c5",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "prop: I would prefer to have this fix in its own PR, because the commit would be easier to retrieve and to reference on failure tickets. ",
        "createdAt" : "2020-03-31T18:29:58Z",
        "updatedAt" : "2020-03-31T23:37:08Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "32ef38c40728841a720aa75e7c21b193215eea1e",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +646,650 @@\n        try {\n            waitUntilAtLeastNumRecordProcessed(outputTopicConcurrent, 1);\n            waitUntilAtLeastNumRecordProcessed(outputTopicConcurrentWindowed, 1);\n"
  },
  {
    "id" : "9a1a2997-0b51-4d62-958d-b70f913052f8",
    "prId" : 9487,
    "prUrl" : "https://github.com/apache/kafka/pull/9487#pullrequestreview-530218086",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91a263b1-efaf-415b-8434-49e866b6168a",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "ditto here",
        "createdAt" : "2020-11-13T04:49:40Z",
        "updatedAt" : "2020-11-18T03:39:14Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "b03b1ae5-8e61-41b9-b23a-8ed8f02da478",
        "parentId" : "91a263b1-efaf-415b-8434-49e866b6168a",
        "authorId" : "e4e906b3-d01a-4fe6-ad3e-ccca19a6df5c",
        "body" : "same as above",
        "createdAt" : "2020-11-13T18:21:29Z",
        "updatedAt" : "2020-11-18T03:39:14Z",
        "lastEditedBy" : "e4e906b3-d01a-4fe6-ad3e-ccca19a6df5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "35dc69a461be58624c13ce4a252ed483d1f4c65d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1014,1018 @@\n    @Test\n    @Deprecated //A single thread should no longer die\n    public void shouldAllowToQueryAfterThreadDied() throws Exception {\n        final AtomicBoolean beforeFailure = new AtomicBoolean(true);"
  }
]