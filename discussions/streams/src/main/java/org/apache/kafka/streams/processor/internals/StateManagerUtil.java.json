[
  {
    "id" : "e1a20c0f-abb4-4130-b379-c5837e5a8b27",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2337b80-f9c6-47e0-94f8-3e2cdbb4ded2",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This has changed from an abstract class to a static utility class.",
        "createdAt" : "2019-07-03T17:19:38Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +39,43 @@import static org.apache.kafka.streams.state.internals.WrappedStateStore.isTimestamped;\n\nfinal class StateManagerUtil {\n    static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n"
  },
  {
    "id" : "b4bea26b-6b46-4e13-8d95-1ca39b346479",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-263363652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "023fd817-0abe-45cb-bbfc-2200ef959612",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Do we need this constructor explicitly? Would this just be default in java?",
        "createdAt" : "2019-07-17T23:17:56Z",
        "updatedAt" : "2019-07-17T23:40:55Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +42,46 @@    static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n\n    private StateManagerUtil() {}\n\n    static RecordConverter converterForStore(final StateStore store) {"
  },
  {
    "id" : "1ea7f3ab-3dc5-482d-81fc-176d68011871",
    "prId" : 8187,
    "prUrl" : "https://github.com/apache/kafka/pull/8187#pullrequestreview-366053030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f843e54-dbba-4ae2-9d5d-0b3b00813d17",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Add a logging to log the IO exception.",
        "createdAt" : "2020-02-27T22:43:48Z",
        "updatedAt" : "2020-02-28T04:50:31Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "abbb2e38df0ff7c250e3b77588c7cc62b22bb4de",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +85,89 @@        } catch (final IOException fatalException) {\n            // since it is only called under dirty close, we always swallow the exception\n            log.warn(\"Failed to wiping state stores for task {} due to {}\", stateMgr.taskId(), fatalException);\n        }\n    }"
  },
  {
    "id" : "a2e4718a-1dd7-48b2-b9c6-75d38eebb394",
    "prId" : 8248,
    "prUrl" : "https://github.com/apache/kafka/pull/8248#pullrequestreview-404494706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "638d2c6d-afea-4d73-9eb2-dd1d98397780",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It seems like the intent here is to skip initializing the store again if it's already been initialized, but still register it to the changelog reader. Which is ok, since we `unregisterAllStoresWithChangelogReader` in the recycleState, right?",
        "createdAt" : "2020-05-01T22:12:27Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "60a7520a-8eb4-4e7f-97c1-001156013d35",
        "parentId" : "638d2c6d-afea-4d73-9eb2-dd1d98397780",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "That's the idea",
        "createdAt" : "2020-05-02T02:27:41Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9161fb2b-54c0-4c77-b8fa-3628104c28e8",
        "parentId" : "638d2c6d-afea-4d73-9eb2-dd1d98397780",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "We need to unregister and register the changelogs during task type conversion, as standby changelogs are handled differently  than active ones during registration (to disable standby processing during restoration)",
        "createdAt" : "2020-05-02T02:29:24Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "ee0f88a5-098a-4435-97b1-9907b7fc1b60",
        "parentId" : "638d2c6d-afea-4d73-9eb2-dd1d98397780",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ah. Makes sense.",
        "createdAt" : "2020-05-02T03:02:43Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "30ac7b3ccd47063497c17ac148d90f9b29683e82",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +74,78 @@        final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n\n        stateMgr.registerStateStores(topology.stateStores(), processorContext);\n        log.debug(\"Registered state stores\");\n"
  },
  {
    "id" : "11e9d0ce-8aad-4e0d-a7b6-26b5bb6c3248",
    "prId" : 8478,
    "prUrl" : "https://github.com/apache/kafka/pull/8478#pullrequestreview-393338423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bc164d6-18cf-49aa-a82e-998e0760f1c8",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Not clear to me why we need a nested try-catch IOException here?",
        "createdAt" : "2020-04-14T17:35:10Z",
        "updatedAt" : "2020-04-15T18:28:32Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "74818d13-ebe2-42ed-905c-5068ab3b5e07",
        "parentId" : "9bc164d6-18cf-49aa-a82e-998e0760f1c8",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "The only reason would be to distinguish between an `IOException` on `lock`, and one on `unlock`. But I guess that's not actually useful to differentiate, I'll take it out",
        "createdAt" : "2020-04-14T22:13:59Z",
        "updatedAt" : "2020-04-15T18:28:32Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "b816ff8c-a9c7-4452-9483-26473c443745",
        "parentId" : "9bc164d6-18cf-49aa-a82e-998e0760f1c8",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "(or during wiping of the state stores, but presumably all this can be discovered through the state trace anyway)",
        "createdAt" : "2020-04-14T22:25:59Z",
        "updatedAt" : "2020-04-15T18:28:32Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ad8282dd07d2d4b91647f7f94fe235536139227",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +123,127 @@                }\n            }\n        } catch (final IOException e) {\n            final ProcessorStateException exception = new ProcessorStateException(\n                String.format(\"%sFatal error while trying to close the state manager for task %s\", logPrefix, id), e"
  },
  {
    "id" : "5a74647c-9ff7-4b01-8748-102f54dc451a",
    "prId" : 8478,
    "prUrl" : "https://github.com/apache/kafka/pull/8478#pullrequestreview-393997914",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a969e32e-80ed-43c0-a5ce-bd97c0a1dd61",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Nice clean, now we catch the exception in the caller.",
        "createdAt" : "2020-04-15T17:45:45Z",
        "updatedAt" : "2020-04-15T18:28:32Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ad8282dd07d2d4b91647f7f94fe235536139227",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +133,137 @@        final ProcessorStateException exception = firstException.get();\n        if (exception != null) {\n            throw exception;\n        }\n    }"
  },
  {
    "id" : "a20a2cef-b084-4d51-a51b-0956d94d1ca8",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-462117590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca2d8e8d-b228-4330-a0a6-f09776ea753d",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I'm a bit torn about this optimization to avoid double checkpointing, because on the other hand, even if we have not made any progress since loaded the checkpoint, we'd still need to make a checkpoint upon closing if we have never made one before -- and I use emptyMap as an indicator for that.\r\n\r\nGiven that upon suspending we are now less likely to checkpoint, the chance that we would double checkpointing (when transiting to suspended, and when transiting to closed) is smaller, and hence I'm thinking maybe I'd just remove this optimization to make the logic a bit simpler. LMK WDYT.",
        "createdAt" : "2020-07-01T03:23:53Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "3af15648-5fdb-4bfa-b996-0662e28d613b",
        "parentId" : "ca2d8e8d-b228-4330-a0a6-f09776ea753d",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Is this out of date? It seems like we now never checkpoint during suspension so we don't have to bother with this optimization",
        "createdAt" : "2020-07-21T22:59:32Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "1571478c-21f4-47df-81a9-0d985c0585f7",
        "parentId" : "ca2d8e8d-b228-4330-a0a6-f09776ea753d",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Sorry I meant the \"commit\" during suspension: when we suspend a task we would commit the offsets and in `postCommit` we may write the checkpoint file, and then when we close the suspended task we may commit again (note the `prepareCommit` call should return empty offsets since there's nothing new to commit, but in `postCommit` we would checkpoint the file again, i.e. double writing checkpoint.\r\n\r\nSo what I did here is to not enforce checkpointing in `postCommit` during suspension, and only enforce checkpointing in `postCommit` during closing.",
        "createdAt" : "2020-07-24T05:05:27Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "1356f266-56fa-410a-9dc7-22b568ad4169",
        "parentId" : "ca2d8e8d-b228-4330-a0a6-f09776ea753d",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Can you elaborate on why not enforce the checkpoint during suspension in `handleRevocation` (for the tasks to be closed)? It seems like we just re-introduce this problem of potentially double checkpointing and I'm not sure I understand why we need to checkpoint only in `handleAssignment`. Is this an optimization or a correctness issue or a code cleanness thing?",
        "createdAt" : "2020-07-28T17:37:42Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "922c1a44-0c91-4139-a7ca-8568da71b643",
        "parentId" : "ca2d8e8d-b228-4330-a0a6-f09776ea753d",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think we do not need to enforce checkpoint during suspension but only need to do that during closure / recycling; if a suspended task is resumed then we do not need to write checkpoint in between.\r\n\r\nBut admittedly moving forward most suspended tasks would be closed or recycled :slightly_smiling_face: So I can change that back.",
        "createdAt" : "2020-07-29T00:24:31Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d753745d-dccf-4d2c-8ee7-8a9cfe3318e7",
        "parentId" : "ca2d8e8d-b228-4330-a0a6-f09776ea753d",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I would expect that writing the checkpoint file should be cheap anyway (it's just some metadata, how bad can it be?) -- or are you worried about the blocking flush to the file system? Thus, I don't consider double-checkpointing as a real issue? Might be a case of pre-mature optimization? It might simplify our code if we just blindly write the checkpoint data even if it did not change? -- But I am fine either way.",
        "createdAt" : "2020-08-05T23:39:08Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "9132348d-ffe0-4e74-935b-572c65c80d95",
        "parentId" : "ca2d8e8d-b228-4330-a0a6-f09776ea753d",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "In the current code we actually call `postCommit(true)` both during suspension and closure. It's just that in `checkpointNeeded` we would only return true with `enforce` if it has changed --- personally I think this is a general optimization worth applying.",
        "createdAt" : "2020-08-06T00:56:48Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +67,71 @@        // when enforcing checkpoint is required, we should overwrite the checkpoint if it is different from the old one;\n        // otherwise, we only overwrite the checkpoint if it is largely different from the old one\n        return enforceCheckpoint ? totalOffsetDelta > 0 : totalOffsetDelta > OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT;\n    }\n"
  },
  {
    "id" : "2cc7922e-7ee7-4a06-8725-d712d5855499",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-456244877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2559cb9c-eede-4ea2-a491-438691436e04",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is just a placeholder for making less frequent flushing than commits, currently I'm making it num.records based, but it's open for better proposal to decide when flushing should be executed.",
        "createdAt" : "2020-07-07T02:23:55Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "b73790e0-b547-4ecb-ad22-0b9a89b98ea6",
        "parentId" : "2559cb9c-eede-4ea2-a491-438691436e04",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "It would be awesome if we could do it based on the size of the pending data and the configured memtable size. Not sure if that's really feasible, just throwing it out there",
        "createdAt" : "2020-07-24T18:10:25Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "2a3553a7-8898-4e80-9b08-3463eeeb92ba",
        "parentId" : "2559cb9c-eede-4ea2-a491-438691436e04",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "You mean when the accumulated pending data is larger than the memtable, we should checkpoint?",
        "createdAt" : "2020-07-27T00:05:24Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "96ddda55-fc3c-4a0c-bb7f-c81bb99043f8",
        "parentId" : "2559cb9c-eede-4ea2-a491-438691436e04",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Something like that. We know rocksdb will render the memtable immutable once it reaches the configured memtable size, after that it will flush once the number of immutable memtables reaches the configured value. Probably makes sense to align our checkpoint/flushing to the configured rocksdb flushing.\r\n\r\nWould be cool if we could piggy-back on the rocksdb options and avoid a new config in Streams altogether, but obviously not everyone uses rocksdb",
        "createdAt" : "2020-07-27T23:33:10Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "ba045854-9635-45cb-89f1-fb6c1674a38e",
        "parentId" : "2559cb9c-eede-4ea2-a491-438691436e04",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yeah my major concern is to tie the flushing policy with rocksdb -- although it is the default persistent stores now, we should avoid tying with a specific type of stores.",
        "createdAt" : "2020-07-28T01:40:35Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +42,46 @@final class StateManagerUtil {\n    static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n    static final long OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT = 10_000L;\n\n    private StateManagerUtil() {}"
  },
  {
    "id" : "f1e323ea-db13-46c2-8bf1-4fa28b2cfb73",
    "prId" : 8996,
    "prUrl" : "https://github.com/apache/kafka/pull/8996#pullrequestreview-445973926",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33f49759-d6c9-482f-bbf9-7033b9b22d35",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I take it this block can also throw an exception? We shouldn't throw exceptions inside a finally block because it's not defined when the exception will be thrown, or in the case where the first try block threw, which exception is ultimately thrown is also undefined.\r\n\r\nTo make this simpler to grapple with, we added org.apache.kafka.streams.state.internals.ExceptionUtils#executeAll",
        "createdAt" : "2020-07-09T19:11:58Z",
        "updatedAt" : "2020-07-09T19:14:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "0aca226f-ef10-4e4f-bce0-e7a6e480099b",
        "parentId" : "33f49759-d6c9-482f-bbf9-7033b9b22d35",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Well, I figured it didn't matter since these both just throw IOException which we catch in the outer block. The point was to make sure we unlock it. But I'll check out `ExceptionUtils#executeAll`",
        "createdAt" : "2020-07-09T20:43:56Z",
        "updatedAt" : "2020-07-09T20:43:56Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "e0caea0d-5c31-4608-84b2-924e2a4317a7",
        "parentId" : "33f49759-d6c9-482f-bbf9-7033b9b22d35",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I can't use `ExceptionUtils#executeAll` because the compiler complains that we don't handle the `IOException` unless we surround each Runnable with its own try-catch block, at which point `#executeAll` isn't really doing anything",
        "createdAt" : "2020-07-09T21:08:30Z",
        "updatedAt" : "2020-07-09T21:08:31Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "ac623c13-75b4-48c1-9002-5999076997fc",
        "parentId" : "33f49759-d6c9-482f-bbf9-7033b9b22d35",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "IMHO, the code is good as-is.\r\n\r\nThanks for rewriting to a nested `try-final` structure!",
        "createdAt" : "2020-07-09T21:14:44Z",
        "updatedAt" : "2020-07-09T21:14:44Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f6db6d6dea1a2164ee59895e46ccc268e6a477d",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +118,122 @@                    } finally {\n                        stateDirectory.unlock(id);\n                    }\n                }\n            }"
  },
  {
    "id" : "a62be14a-7245-4fc7-a4fe-1cda68c270a7",
    "prId" : 9170,
    "prUrl" : "https://github.com/apache/kafka/pull/9170#pullrequestreview-466245551",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1c33bee-01c1-4fd7-b124-b05157633c09",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could we move this logic to the `StandbyTask` only? It is the only case I have seen which could have null snapshot passed in, which could make this helper assume both snapshots are not null.\r\n```\r\nif (oldOffsetSnapshot == null) {\r\n            return false;\t          \r\n}\t        \r\n```",
        "createdAt" : "2020-08-12T20:11:06Z",
        "updatedAt" : "2020-08-12T20:11:19Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "15973d08-7440-4a1f-af69-21a0f6d8332c",
        "parentId" : "f1c33bee-01c1-4fd7-b124-b05157633c09",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The null snapshot is for both active / standby tasks before the initializeIfNeeded is triggered, in that case, we should not overwrite the checkpoint even if it was enforced.",
        "createdAt" : "2020-08-12T20:24:20Z",
        "updatedAt" : "2020-08-12T20:24:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4288099ff38892fc9b02b9ea2eaa87a50adb64c1",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +59,63 @@        }\n\n        if (enforceCheckpoint)\n            return true;\n"
  },
  {
    "id" : "699312c8-05b7-4027-b489-6148749bdbb4",
    "prId" : 10342,
    "prUrl" : "https://github.com/apache/kafka/pull/10342#pullrequestreview-614860973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5fa84ca-995a-4a1d-8ea8-d46df57f9068",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "No logical changes here (or below in TaskManager), just cleaning up the try-catch block now that we no longer throw IOException",
        "createdAt" : "2021-03-17T23:16:25Z",
        "updatedAt" : "2021-03-30T19:31:35Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "de4dbd1e14ae4345c75c26c6825df6232f310dee",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +88,92 @@        final TaskId id = stateMgr.taskId();\n        if (!stateDirectory.lock(id)) {\n            throw new LockException(String.format(\"%sFailed to lock the state directory for task %s\", logPrefix, id));\n        }\n        log.debug(\"Acquired state directory lock\");"
  }
]