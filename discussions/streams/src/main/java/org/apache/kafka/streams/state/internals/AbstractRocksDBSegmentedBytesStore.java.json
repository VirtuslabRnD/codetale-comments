[
  {
    "id" : "76550449-ee18-408e-9f57-07262dfb4fb0",
    "prId" : 6186,
    "prUrl" : "https://github.com/apache/kafka/pull/6186#pullrequestreview-216484004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26bf1673-f7bf-4c93-9c9a-82699a7bb85c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just to clarify there's no logic changes from the existing RocksDBSegmentedBytesStore, but just extract it out right (if yes I'll skip reviewing this file)? ",
        "createdAt" : "2019-03-18T18:01:46Z",
        "updatedAt" : "2019-03-20T06:33:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "59bc6fb8-a813-426d-ad20-fbf44705ce1d",
        "parentId" : "26bf1673-f7bf-4c93-9c9a-82699a7bb85c",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes. It's the same code.",
        "createdAt" : "2019-03-20T00:39:26Z",
        "updatedAt" : "2019-03-20T06:35:47Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "39578f44d576e1839048f1b2432411729cb50172",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +44,48 @@import static org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.addInvocationRateAndCount;\n\npublic class AbstractRocksDBSegmentedBytesStore<S extends Segment> implements SegmentedBytesStore {\n    private static final Logger LOG = LoggerFactory.getLogger(AbstractRocksDBSegmentedBytesStore.class);\n    private final String name;"
  },
  {
    "id" : "c384f6eb-8b44-4bc4-80d1-3bc45ee45875",
    "prId" : 6186,
    "prUrl" : "https://github.com/apache/kafka/pull/6186#pullrequestreview-216546006",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42faebd2-2c20-4629-a8aa-bcc152328eb3",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is the bug fix -- in the original code, the record was added to the batch directly (ie, to default CF) -- we know delegate this to the segment that can decide to which CF the record is added.",
        "createdAt" : "2019-03-20T06:35:13Z",
        "updatedAt" : "2019-03-20T06:35:13Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "39578f44d576e1839048f1b2432411729cb50172",
    "line" : 262,
    "diffHunk" : "@@ -1,1 +260,264 @@                try {\n                    final WriteBatch batch = writeBatchMap.computeIfAbsent(segment, s -> new WriteBatch());\n                    segment.addToBatch(record, batch);\n                } catch (final RocksDBException e) {\n                    throw new ProcessorStateException(\"Error restoring batch to store \" + this.name, e);"
  },
  {
    "id" : "38f4a8a2-cf97-4f18-a3f2-eadd0ac364b9",
    "prId" : 7050,
    "prUrl" : "https://github.com/apache/kafka/pull/7050#pullrequestreview-261002325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "939d73ad-4440-4007-9bd9-864dd731da4a",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Can we add a test this this?",
        "createdAt" : "2019-07-10T20:49:30Z",
        "updatedAt" : "2019-07-11T22:40:33Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "5eb4e39e-504f-4cc4-a86f-291448d57c64",
        "parentId" : "939d73ad-4440-4007-9bd9-864dd731da4a",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Not really sure what a good way to test this would be...thoughts?",
        "createdAt" : "2019-07-11T22:25:16Z",
        "updatedAt" : "2019-07-11T22:40:33Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "30b558f0-124e-4cb5-b06a-335eca6b5101",
        "parentId" : "939d73ad-4440-4007-9bd9-864dd731da4a",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I would be tricky. We would need to intercept the call to `new WriteBatch` within `getWriteBatches()` using power-mock and later verify if `close()` was called on the mock. Might not be worth it...",
        "createdAt" : "2019-07-11T23:35:00Z",
        "updatedAt" : "2019-07-11T23:35:00Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1203d5c28a8b8db23fb53ce5d3c5cc857b4411f6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +234,238 @@                final WriteBatch batch = entry.getValue();\n                segment.write(batch);\n                batch.close();\n            }\n        } catch (final RocksDBException e) {"
  },
  {
    "id" : "62374017-aa9f-47c2-a559-fa6d8255af83",
    "prId" : 7416,
    "prUrl" : "https://github.com/apache/kafka/pull/7416#pullrequestreview-296093410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41d9c809-82a9-4b1b-845a-4ea338930b65",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "nit: string literals as static finals",
        "createdAt" : "2019-10-01T16:47:07Z",
        "updatedAt" : "2019-10-04T08:23:54Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "baba92fb-9b4e-44ea-bcbd-c1a9f5e88bc6",
        "parentId" : "41d9c809-82a9-4b1b-845a-4ea338930b65",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "This is fixed in PR #7429.",
        "createdAt" : "2019-10-02T09:45:10Z",
        "updatedAt" : "2019-10-04T08:23:54Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab3c0436cf659dd6f76b2b6b5360d327bad385bb",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +188,192 @@            expiredRecordSensor,\n            \"stream-\" + metricScope + \"-metrics\",\n            metrics.tagMap(threadId, \"task-id\", taskName, metricScope + \"-id\", name()),\n            EXPIRED_WINDOW_RECORD_DROP\n        );"
  },
  {
    "id" : "ac39e4ad-bff4-4f1e-ae48-288aa20b0951",
    "prId" : 9138,
    "prUrl" : "https://github.com/apache/kafka/pull/9138#pullrequestreview-473682699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ce80119-71ff-4eb3-a2de-f2296bd49f53",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "This should be a descending iterator for the reverse case (here and the other reverse methods in this class)",
        "createdAt" : "2020-08-21T23:51:52Z",
        "updatedAt" : "2020-09-02T15:24:52Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "361ed328-59f8-4cdb-bb4d-54abf2995cbf",
        "parentId" : "4ce80119-71ff-4eb3-a2de-f2296bd49f53",
        "authorId" : "f58052af-021e-47d1-bcb5-e22f46383a12",
        "body" : "`searchSpace` will be reversed based on the `forward` flag, on `AbstractSegments`. ",
        "createdAt" : "2020-08-24T16:54:20Z",
        "updatedAt" : "2020-09-02T15:24:52Z",
        "lastEditedBy" : "f58052af-021e-47d1-bcb5-e22f46383a12",
        "tags" : [
        ]
      }
    ],
    "commit" : "273f612bfe98c58c40e77ec8a2d77e5eabdd2f18",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +87,91 @@\n        return new SegmentIterator<>(\n            searchSpace.iterator(),\n            keySchema.hasNextCondition(key, key, from, to),\n            binaryFrom,"
  },
  {
    "id" : "d21c3dbf-def5-423e-9cb5-4390486f54b1",
    "prId" : 9388,
    "prUrl" : "https://github.com/apache/kafka/pull/9388#pullrequestreview-503498921",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2fec2793-71b9-4a9a-bc2b-519386a91772",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "There are a handful of these also, just passing the deprecation on to the callers.",
        "createdAt" : "2020-10-07T03:28:59Z",
        "updatedAt" : "2020-10-07T03:47:44Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab093bc0610dfee1ccfa7199de59e1b2c417a3d0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +226,230 @@    }\n\n    @Deprecated\n    @Override\n    public void init(final ProcessorContext context,"
  },
  {
    "id" : "a4b2d98b-a068-469c-a645-6eea27ff4737",
    "prId" : 10537,
    "prUrl" : "https://github.com/apache/kafka/pull/10537#pullrequestreview-637987462",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c029c89f-9b9a-447a-b258-78135a2d52d5",
        "parentId" : null,
        "authorId" : "92abc720-96d9-4067-bc18-9bf1fa4019a1",
        "body" : "I wasn't sure how to name this method. I initially called `removeRange(key, from, to)`, but I don't want to support a time range with a specific key because time-ordered key schema will delete other keys between `from-key` and `to-key`.\r\n\r\nSo I thought of just using one timestamp, to make sure this is not called with a time range. But `removeRange(key, timestamp)` does not look like a range. I ended up just calling it `remove`. Any thoughts?",
        "createdAt" : "2021-04-14T19:52:47Z",
        "updatedAt" : "2021-04-16T20:58:48Z",
        "lastEditedBy" : "92abc720-96d9-4067-bc18-9bf1fa4019a1",
        "tags" : [
        ]
      },
      {
        "id" : "44c045b6-5dd7-4c33-ad42-625c6a64ef49",
        "parentId" : "c029c89f-9b9a-447a-b258-78135a2d52d5",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think just calling `remove` is totally fine :)",
        "createdAt" : "2021-04-16T18:32:05Z",
        "updatedAt" : "2021-04-16T20:58:48Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "28b4206202ac84c96810bf885d7f7c68b815cc0c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +198,202 @@\n    @Override\n    public void remove(final Bytes key, final long timestamp) {\n        final Bytes keyBytes = keySchema.toStoreBinaryKeyPrefix(key, timestamp);\n        final S segment = segments.getSegmentForTimestamp(timestamp);"
  }
]