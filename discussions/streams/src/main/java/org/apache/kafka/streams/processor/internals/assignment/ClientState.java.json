[
  {
    "id" : "a6fdf118-5c9d-4c16-a270-3ae559ac38ec",
    "prId" : 4636,
    "prUrl" : "https://github.com/apache/kafka/pull/4636#pullrequestreview-124611947",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "944c9b12-f628-4055-990a-00d4fa33347f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The added `prevStandbyTasks` seems not set anywhere? I.e. it will always be empty hashset?",
        "createdAt" : "2018-05-30T20:41:46Z",
        "updatedAt" : "2018-05-31T03:24:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "65922893-35f5-4f9b-bbdb-d7810e73b60d",
        "parentId" : "944c9b12-f628-4055-990a-00d4fa33347f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "It is set in \r\n```\r\npublic void addPreviousStandbyTasks(final Set<TaskId> standbyTasks) {\r\n    prevStandbyTasks.addAll(standbyTasks);\r\n    prevAssignedTasks.addAll(standbyTasks);\r\n}\r\n```",
        "createdAt" : "2018-05-30T22:08:56Z",
        "updatedAt" : "2018-05-31T03:24:33Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a87bd5254155a9d60ba479371305ddaae99282d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +41,45 @@    }\n\n    private ClientState(final Set<TaskId> activeTasks,\n                        final Set<TaskId> standbyTasks,\n                        final Set<TaskId> assignedTasks,"
  },
  {
    "id" : "bbbf6f03-f642-4007-aef6-86ad92c9b5c7",
    "prId" : 8330,
    "prUrl" : "https://github.com/apache/kafka/pull/8330#pullrequestreview-379860752",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This is the fix, to return an accurate estimate instead of throwing an exception. I felt a warning was appropriate, given that this does indicate task corruption, or some other unexpected situation.",
        "createdAt" : "2020-03-22T23:29:25Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "475fcc7b-f15c-40ad-a829-1aba90f28609",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Should we report the lag as the whole log in this case? Even if the log is truncated it is not guaranteed to throw the invalid offset exception and hence task-corruption logic would not necessarily triggered.",
        "createdAt" : "2020-03-23T00:18:04Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "12fd4165-4415-44bc-9ca0-7a88862521ce",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Do we know the exact reason, why the offset sum on the client is larger than the end offset sum on the broker? Before we change this invariant we should make sure we understand why it is not satisfied. Is it our code that breaks it or some external influences that we cannot control? ",
        "createdAt" : "2020-03-23T10:21:24Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "fe956383-0adc-44bd-9341-fe3db65993d8",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition?\r\nedit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)",
        "createdAt" : "2020-03-23T17:58:02Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "0727e63f-eacc-439a-937d-ddc63e3e5c51",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "~~Also, @vvcephei , if a single task is corrupted and we do hit the `TaskCorruptedException` don't we close, wipe, and recreate stores of all tasks in the thread? Seems we would need to keep track of the thread ownership, and if we detect this condition then set the lag for all tasks owned by that thread to the `endOffset`~~\r\nedit: nevermind, I checked the `TaskCorruptedExcepion` PR, looks like we ended up just setting the invalid offsets to 0 instead of wiping everything. So, disregard this comment ðŸ™‚ ",
        "createdAt" : "2020-03-23T18:09:52Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "f1707111-2d11-42f6-81ca-eb1e26cf0410",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "A task is corrupted if the store changelogs in the checkpoint file don't match the store changelogs in the assigned task.\r\n\r\nWhen the member sends its lag estimate, it doesn't validate whether the task is valid or corrupted. It just reads the values out of the checkpoint files and sums them. I think this is fine, actually, but if we want to check for task corrupted at that time, we can add that in.\r\n\r\nI don't think it really changes how the assignor should behave, though. I think it's could easily happen that any past, present, or future versioned member reports a task sum that's beyond the end-offset sum of what the leader thinks the task contains. If that happens, it doesn't seem like it's a good idea to just crash the leader's thread. It would be better to handle it gracefully in some way.\r\n\r\nIt seems like assuming the task is corrupted, and therefore essentially that it isn't cached at all, is reasonable. This just de-prioritizes that member to get assigned the task, which is harmless.",
        "createdAt" : "2020-03-23T18:46:56Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "ca6a7e84-62af-4280-8093-a0247f148bf9",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Whoops, edited my previous comment before refreshing & seeing your response. Makes sense, although I still believe we should be checking for this and throwing `TaskCorrupted` if we detect this case (in the owning StreamThread, not in the leader during assignment)",
        "createdAt" : "2020-03-23T18:57:52Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "6a23007a-77c8-44f2-8fbe-0041e6d4e043",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "> I think it's could easily happen that any past, present, or future versioned member reports a task sum that's beyond the end-offset sum of what the leader thinks the task contains.\r\n\r\nIf the corruption comes from our code, we should throw. The leader fetches the end offset shortly before that code, so they are most probably up-to-date. If the corruption is caused by failure we should handle it gracefully.",
        "createdAt" : "2020-03-23T19:15:40Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "4e776dd2-e5ac-4f2a-a790-fe286cf2cfab",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Let's think about cases where this could possibly go wrong. We know it must reach or already be in standby/restoring phase. I will use `logEndOffset` for the actual `endOffset` of the partition, and `endOffset` for the offset a standby/restoring task is allowed to restore up to.\r\n\r\nNewly-assigned: we read the invalid offset from the checkpoint, and determine the endOffset as either `logEndOffset` (normal) or `min(logEndOffset, LCO)` (optimized-source-changelog). In both cases, the `endOffset` <= `logEndOffset` so the invalid offset should also satisfy `offset` > `endOffset` which we can detect and throw as `TaskCorrupted`. \r\nAlready-assigned: in this case, we must have been the ones who wrote the invalid offset and/or have it in our `checkpointableOffsets` map. But we still need to fetch the `endOffset` to know when to stop, so the above applies here as well and we can detect the corruption. \r\n\r\nIn either case, I'd agree the leader should not throw and should just treat that task's offset sum as 0. But, we should make sure that on the other end we actually do detect this case and handle it the way the assignor expects.",
        "createdAt" : "2020-03-23T19:59:46Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "03e63e14-73ac-4dc5-ad75-ef70f8b01f99",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "@ableegoldman If the log is truncated and then immediately more records are appended to go beyond the original log-end-offset, in old versions we would not throw the exception.\r\n\r\nAfter some thoughts I think it makes sense to report the lag as the whole log.",
        "createdAt" : "2020-03-23T21:35:29Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "e2aed314-d63b-4f2c-a185-044f57b2badf",
        "parentId" : "4246db04-0a61-4799-b64e-6f6f29a1acbb",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks for the conversation, all. I think where it nets out for me is that we don't really expect this to happen for non-bugged code, but there are a few edge cases where it could. Thus, logging a warning actually seems reasonable.\r\n\r\nSince the graceful handling option is still safe in any case, and since there do exist edge cases where this condition doesn't indicate a bug in Streams, we'll just keep the proposed graceful handling of reporting the lag as the whole log. Either the member in question won't get assigned the task (because it's de-prioritized wrt to other members who report valid positions), and it winds up cleaning up its state dir on its own, or it gets assigned the task, detects the corruption on its side, and recovers appropriately.",
        "createdAt" : "2020-03-23T22:13:06Z",
        "updatedAt" : "2020-03-23T22:24:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "06c17350dac09c8b4a35edeb1b3c8e29132dfa17",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +205,209 @@                             \" The assignor will de-prioritize returning this task to this member in the hopes that\" +\n                             \" some other member may be able to re-use its state.\");\n                taskLagTotals.put(task, endOffsetSum);\n            } else if (offsetSum == Task.LATEST_OFFSET) {\n                taskLagTotals.put(task, Task.LATEST_OFFSET);"
  },
  {
    "id" : "36cab1cb-28a9-44d5-84be-d24576574f09",
    "prId" : 8541,
    "prUrl" : "https://github.com/apache/kafka/pull/8541#pullrequestreview-401414888",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a984bf0b-b505-4a1c-8b52-6b3fbe4efec5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This constructor is currently only used in tests, but I'm planning a follow-on refactor that would actually use it from the StickyTaskAssignor as well.",
        "createdAt" : "2020-04-23T23:33:50Z",
        "updatedAt" : "2020-04-28T03:20:48Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "738d9240-ad60-42cc-89d0-9329f0cc5af2",
        "parentId" : "a984bf0b-b505-4a1c-8b52-6b3fbe4efec5",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I'm having a failure of imagination to see why any task assignor would ever be creating ClientState objects, but I eagerly wait to be enlightened.",
        "createdAt" : "2020-04-28T00:19:04Z",
        "updatedAt" : "2020-04-28T03:20:48Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +94,98 @@                       final Set<TaskId> previousStandbyTasks,\n                       final Map<TaskId, Long> taskLagTotals,\n                       final int capacity) {\n        activeTasks = new HashSet<>();\n        standbyTasks = new HashSet<>();"
  },
  {
    "id" : "e50e704a-c99e-4f6b-9878-fae0ce7f6c40",
    "prId" : 8541,
    "prUrl" : "https://github.com/apache/kafka/pull/8541#pullrequestreview-400913168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27cbd29c-c682-491c-a254-40a90f55dd4d",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "req: Please add a unit test.",
        "createdAt" : "2020-04-27T13:35:04Z",
        "updatedAt" : "2020-04-28T03:20:48Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +91,95 @@    }\n\n    public ClientState(final Set<TaskId> previousActiveTasks,\n                       final Set<TaskId> previousStandbyTasks,\n                       final Map<TaskId, Long> taskLagTotals,"
  },
  {
    "id" : "82e4d2a9-e76b-4458-8888-a8e221703e21",
    "prId" : 8588,
    "prUrl" : "https://github.com/apache/kafka/pull/8588#pullrequestreview-409795041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc33cdcd-a7da-4c98-97c5-8319194fdd1f",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I made all these collections sorted just to help readability while debugging. What do you think about keeping them sorted going forward (for the same reason)?",
        "createdAt" : "2020-05-08T20:46:50Z",
        "updatedAt" : "2020-05-14T01:51:44Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "4ab49071-2d88-45c9-98fd-149c9b0c8083",
        "parentId" : "fc33cdcd-a7da-4c98-97c5-8319194fdd1f",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Fine with me.",
        "createdAt" : "2020-05-12T08:17:43Z",
        "updatedAt" : "2020-05-14T01:51:44Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cf8316444c205ea04f6fa0874619451f5b94d92",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +60,64 @@\n    ClientState(final int capacity) {\n        activeTasks = new TreeSet<>();\n        standbyTasks = new TreeSet<>();\n        prevActiveTasks = new TreeSet<>();"
  },
  {
    "id" : "3eac56f6-3f3d-4c6b-a3a9-429ed1a21e8e",
    "prId" : 8588,
    "prUrl" : "https://github.com/apache/kafka/pull/8588#pullrequestreview-408512587",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2fb45ae-5d69-411e-bff6-fee335d37663",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Unfortunately, TreeMap doesn't have a constructor that takes both a collection to copy and a comparator.",
        "createdAt" : "2020-05-08T20:48:10Z",
        "updatedAt" : "2020-05-14T01:51:44Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cf8316444c205ea04f6fa0874619451f5b94d92",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +104,108 @@    public ClientState copy() {\n        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n        newOwnedPartitions.putAll(ownedPartitions);\n        return new ClientState(\n            new TreeSet<>(activeTasks),"
  },
  {
    "id" : "61563446-9473-4be1-884b-6ec367335e61",
    "prId" : 8588,
    "prUrl" : "https://github.com/apache/kafka/pull/8588#pullrequestreview-409795041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0ed17ee-fbf7-48a9-8f79-6410442c1ab1",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "In addition to writing tests to verify that assignments are valid at the cluster level, it seemed wise to at least make sure that we can't create an invalid assignment for a single node.",
        "createdAt" : "2020-05-08T20:49:16Z",
        "updatedAt" : "2020-05-14T01:51:44Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "a60a57f5-5548-4fec-842c-53b0acba0027",
        "parentId" : "f0ed17ee-fbf7-48a9-8f79-6410442c1ab1",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "req: Please add unit test to verify `IllegalStateException`.",
        "createdAt" : "2020-05-12T08:58:34Z",
        "updatedAt" : "2020-05-14T01:51:44Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cf8316444c205ea04f6fa0874619451f5b94d92",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +145,149 @@\n    void assignActive(final TaskId task) {\n        assertNotAssigned(task);\n        activeTasks.add(task);\n    }"
  },
  {
    "id" : "dac5ae35-ec0b-46b9-813e-35e430fef3ee",
    "prId" : 8588,
    "prUrl" : "https://github.com/apache/kafka/pull/8588#pullrequestreview-409795041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "108fd1f7-7b7c-4eff-9beb-6342090e64e1",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "req: Please add unit test to verify at least `IllegalStateException`.",
        "createdAt" : "2020-05-12T08:58:51Z",
        "updatedAt" : "2020-05-14T01:51:44Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cf8316444c205ea04f6fa0874619451f5b94d92",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +168,172 @@    }\n\n    void assignStandby(final TaskId task) {\n        assertNotAssigned(task);\n        standbyTasks.add(task);"
  },
  {
    "id" : "e35b303a-be54-41cf-9522-0e33c68d1177",
    "prId" : 8716,
    "prUrl" : "https://github.com/apache/kafka/pull/8716#pullrequestreview-417052308",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37bc1995-882e-4f62-909b-cb25b7a7abaa",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I found this useful while debugging the system test.",
        "createdAt" : "2020-05-22T16:49:19Z",
        "updatedAt" : "2020-05-27T14:44:04Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "94194f2f6cc87a202266fbabfe426f8bc4fb09aa",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +335,339 @@            \") prevOwnedPartitionsByConsumerId: (\" + ownedPartitions.keySet() +\n            \") changelogOffsetTotalsByTask: (\" + taskOffsetSums.entrySet() +\n            \") taskLagTotals: (\" + taskLagTotals.entrySet() +\n            \") capacity: \" + capacity +\n            \" assigned: \" + assignedTaskCount() +"
  }
]