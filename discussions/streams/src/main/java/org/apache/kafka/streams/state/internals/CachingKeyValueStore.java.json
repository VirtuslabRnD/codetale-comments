[
  {
    "id" : "584e5dba-97b8-4ecc-b798-5e967a55d493",
    "prId" : 4508,
    "prUrl" : "https://github.com/apache/kafka/pull/4508#pullrequestreview-94140112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e45b356-b88e-4bb8-a474-f656b25e2f57",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is also intentional but I'd like to have at least a few more careful checks on: I think our current handling of delete, which is to remove from cache and remove from store immediately is not correct, since by doing this, the deletion will not be forwarded to downstream processors (even if the store is flushed later). So I changed it to just putting a null byte arrays into the cache, which will also be picked over the underlying store in the merge-sort iterators.\r\n\r\nBut I want to make sure that there are no other corner cases that may be affected by this change. @dguy @mjsax @bbejeck ?",
        "createdAt" : "2018-02-03T19:49:09Z",
        "updatedAt" : "2018-02-06T01:08:20Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "c8f94ee3-4953-4e49-8591-89e9f9c52ec8",
        "parentId" : "8e45b356-b88e-4bb8-a474-f656b25e2f57",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I believe this to be correct as well, as all the underlying stores handle the put of a `null` as a delete and it seems compatible with the new change to not have `all()` include null values.",
        "createdAt" : "2018-02-05T17:47:52Z",
        "updatedAt" : "2018-02-06T01:08:20Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "7b881f6d-9282-4463-9f85-b04fa462f927",
        "parentId" : "8e45b356-b88e-4bb8-a474-f656b25e2f57",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I am not 100% familiar with the details here, but it sounds reasonable. If you are correct and our old code is incorrect, we should write a test that fails with the old code but passed with the new code.\r\n\r\nOr is this exposed already by a newly added test as mentioned here https://github.com/apache/kafka/pull/4508/files#r165542967",
        "createdAt" : "2018-02-05T19:47:17Z",
        "updatedAt" : "2018-02-06T01:08:20Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "6743c952-c163-4556-a4d3-8b4e6c60fef4",
        "parentId" : "8e45b356-b88e-4bb8-a474-f656b25e2f57",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The reason I think we did not capture this issue earlier, is that `tuple forward in flushing` is only used for DSL, and in our current DSL's KTableXX operators we never explicitly trigger `delete` and hence that's why we never realize this issue.\r\n\r\nBut even without the scenario that could expose this bug, not using write-through caching for delete still have a performance benefits of write-back caching.",
        "createdAt" : "2018-02-05T20:59:36Z",
        "updatedAt" : "2018-02-06T01:08:20Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "98b78ecd73a08d76506318623561b797d1659c49",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +272,276 @@    }\n\n    private byte[] deleteInternal(final Bytes key) {\n        final byte[] v = getInternal(key);\n        putInternal(key, null);"
  },
  {
    "id" : "4d9be8c4-03b8-423d-90fe-7ace479cd6a8",
    "prId" : 4988,
    "prUrl" : "https://github.com/apache/kafka/pull/4988#pullrequestreview-118896585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e15c7e6-5892-449b-98fe-8471a9b1b0c0",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Should we do this change for `CachingSessionStore` as well?",
        "createdAt" : "2018-05-09T20:40:18Z",
        "updatedAt" : "2018-05-09T20:53:52Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d4508cb22d8a270c237c831850939b788e0b577",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +62,66 @@    public void init(final ProcessorContext context, final StateStore root) {\n        initInternal(context);\n        underlying.init(context, root);\n        // save the stream thread as we only ever want to trigger a flush\n        // when the stream thread is the current thread."
  },
  {
    "id" : "f7feca63-0ffe-4e81-aa60-283623d181ba",
    "prId" : 6147,
    "prUrl" : "https://github.com/apache/kafka/pull/6147#pullrequestreview-192831329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7cfbc18-c227-440e-9cf8-0a43d3bfccc5",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "part of the fix: we now also forward the timestamp on eviction",
        "createdAt" : "2019-01-15T19:47:53Z",
        "updatedAt" : "2019-01-18T03:06:14Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "30b8447f5ca42f0be25ab3144de9a68b532e9924",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +101,105 @@                    serdes.valueFrom(entry.newValue()),\n                    oldValue,\n                    entry.entry().context().timestamp());\n            } else {\n                underlying.put(entry.key(), entry.newValue());"
  },
  {
    "id" : "8f295c31-ae45-437a-b3a1-f7f94e0b1a94",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-198401210",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd371460-952a-4d66-ad52-d8ac21ece754",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is an optimization that I did for all three caching stores:\r\n\r\n1. get the new bytes from cache, read the old bytes from underlying store.\r\n2. if either old / new bytes are not null, go to 3) below; otherwise skip so that we do not need to deserialize.\r\n3. deserialize to objects, and apply flush listener to downstream.",
        "createdAt" : "2019-01-31T02:29:42Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +87,91 @@                                    final InternalProcessorContext context) {\n        if (flushListener != null) {\n            final byte[] newValueBytes = entry.newValue();\n            final byte[] oldValueBytes = newValueBytes == null || sendOldValues ? underlying.get(entry.key()) : null;\n"
  },
  {
    "id" : "244f716d-c04d-41ab-8220-45dc03161ebd",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-202434903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38d0cecb-bff2-4661-8e6d-d529079d200a",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Can we apply deMorgan's rule and flip the conditional (with an empty body) here?\r\n\r\nIt seems easier to understand:\r\n```\r\nif (newValueBytes == null && oldValueBytes == null) {\r\n  // no need to flush or write to underlying store\r\n} else {\r\n  ... the rest of the code\r\n}\r\n```\r\n\r\nIn other words, the empty \"skip\" block is more self-documenting than the preceeding comment explaining the algorithm.\r\n\r\nWhich actually makes me wonder: is there a more general optimization that we can skip desserialization, flush, and write any time the new and old values are identical?",
        "createdAt" : "2019-02-01T18:15:58Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "e38c1ac6-7126-4b69-ad73-c08aaf099727",
        "parentId" : "38d0cecb-bff2-4661-8e6d-d529079d200a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think we cannot generally do this optimization if oldBytes == newBytes, since the timestamp may be updated for `flushListener.apply`.",
        "createdAt" : "2019-02-09T00:01:05Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "9fab171b-acc1-4152-ad63-b4e77dd4eb11",
        "parentId" : "38d0cecb-bff2-4661-8e6d-d529079d200a",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I think an empty then-body would be bad code style.",
        "createdAt" : "2019-02-10T08:03:10Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "0014a807-7a8c-4783-a6a2-05fe9110b3cf",
        "parentId" : "38d0cecb-bff2-4661-8e6d-d529079d200a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hey folks, I'm slightly modifying this logic to NOT always blindly read from the underlying store, but do sth. like this:\r\n\r\n```\r\n            final byte[] newValueBytes = entry.newValue();\r\n            final byte[] oldValueBytes = newValueBytes == null || sendOldValues ? underlying.get(entry.key()) : null;\r\n\r\n            if (newValueBytes != null || oldValueBytes != null) {\r\n                    ....\r\n            }\r\n```\r\n\r\nThe main motivation is that, for session stores, the likelihood of `newValueBytes == null` could be high while for other two types, the likelihood would be low. As a result, for other two types we would fail the above condition and if `sendOldValues == false` we would not need to read the old bytes for this optimization since it is doomed to not happen.",
        "createdAt" : "2019-02-12T01:13:11Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +92,96 @@            // this is an optimization: if this key did not exist in underlying store and also not in the cache,\n            // we can skip flushing to downstream as well as writing to underlying store\n            if (newValueBytes != null || oldValueBytes != null) {\n                final K key = serdes.keyFrom(entry.key().get());\n                final V newValue = newValueBytes != null ? serdes.valueFrom(newValueBytes) : null;"
  },
  {
    "id" : "c9c57f50-553f-4f1b-9577-9eb6c087b639",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-202852839",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18c37df1-ba50-4ae5-94d4-14c1ce5ec9f2",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Do we need to check `sendOldValues` here?",
        "createdAt" : "2019-02-12T01:03:10Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "097b5840-c5fc-467a-88f5-9e88f66b8983",
        "parentId" : "18c37df1-ba50-4ae5-94d4-14c1ce5ec9f2",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good point, since I've add the check above I can remove it here. Ditto elsewhere.",
        "createdAt" : "2019-02-12T18:28:55Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f9cf5347-b98e-4e73-9778-eec8ed330cd7",
        "parentId" : "18c37df1-ba50-4ae5-94d4-14c1ce5ec9f2",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "@mjsax Actually, it is not correct: when `sendOldValues` is false, we should never send old values downstreams. So suppose `newValueBytes != null`, and hence we read the underlying store, we still need to have the check here so that we can have `oldValue` as null.",
        "createdAt" : "2019-02-12T19:07:26Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +95,99 @@                final K key = serdes.keyFrom(entry.key().get());\n                final V newValue = newValueBytes != null ? serdes.valueFrom(newValueBytes) : null;\n                final V oldValue = sendOldValues && oldValueBytes != null ? serdes.valueFrom(oldValueBytes) : null;\n                // we need to get the old values if needed, and then put to store, and then flush\n                underlying.put(entry.key(), entry.newValue());"
  },
  {
    "id" : "e0610999-891a-48cf-80e9-9fa16c10cb77",
    "prId" : 6255,
    "prUrl" : "https://github.com/apache/kafka/pull/6255#pullrequestreview-202287857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2913cf3-8bf6-4fa8-9fc9-3ff73c2775a1",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "You'll note in all these methods that we need to say `super.***` to explicitly invoke the operation on the wrapped store when it's a method that the subclass explicitly overrides.",
        "createdAt" : "2019-02-11T20:03:48Z",
        "updatedAt" : "2019-02-13T14:39:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e871f2a26bed0701a074e91e41c4e47f48f6b9e",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +60,64 @@                     final StateStore root) {\n        initInternal(context);\n        super.init(context, root);\n        // save the stream thread as we only ever want to trigger a flush\n        // when the stream thread is the current thread."
  },
  {
    "id" : "0a7142e9-6a63-4784-9163-0820812a40df",
    "prId" : 6331,
    "prUrl" : "https://github.com/apache/kafka/pull/6331#pullrequestreview-208306791",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55ac9f81-2914-4baa-93ff-2cd2cddbef49",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Using `CachedStateStore<byte[], byte[]>` instead of `CachedStateStore< Bytes, byte[]>` because it seems cleaner overall (similar on other caching stores).",
        "createdAt" : "2019-02-27T02:19:00Z",
        "updatedAt" : "2019-02-28T00:49:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d95689529bd3b0aa046c37df59896930b9226bb",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +34,38 @@class CachingKeyValueStore\n    extends WrappedStateStore<KeyValueStore<Bytes, byte[]>, byte[], byte[]>\n    implements KeyValueStore<Bytes, byte[]>, CachedStateStore<byte[], byte[]> {\n\n    private CacheFlushListener<byte[], byte[]> flushListener;"
  },
  {
    "id" : "abc9c73f-923e-4efc-a313-505a60ed506f",
    "prId" : 6331,
    "prUrl" : "https://github.com/apache/kafka/pull/6331#pullrequestreview-208666399",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99adcc15-39b5-4f4c-b2ee-b34562e3ab71",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just to confirm: this line and below do not have any logical changes just due to git diff messing the line numbers right?",
        "createdAt" : "2019-02-27T17:10:49Z",
        "updatedAt" : "2019-02-28T00:49:23Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "daec52c6-463b-4ae6-95a3-45ba0497327b",
        "parentId" : "99adcc15-39b5-4f4c-b2ee-b34562e3ab71",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes.",
        "createdAt" : "2019-02-27T17:40:24Z",
        "updatedAt" : "2019-02-28T00:49:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d95689529bd3b0aa046c37df59896930b9226bb",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +111,115 @@    @Override\n    public void put(final Bytes key,\n                    final byte[] value) {\n        Objects.requireNonNull(key, \"key cannot be null\");\n        validateStoreOpen();"
  }
]