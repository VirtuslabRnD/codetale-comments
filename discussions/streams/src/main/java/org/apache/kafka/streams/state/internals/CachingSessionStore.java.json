[
  {
    "id" : "874e0d83-228a-4364-ae81-09e55bee9643",
    "prId" : 6147,
    "prUrl" : "https://github.com/apache/kafka/pull/6147#pullrequestreview-192831329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ddfd975-278c-426f-b3a9-eabe217010b9",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "part of the fix: we now also forward the timestamp on eviction",
        "createdAt" : "2019-01-15T19:47:56Z",
        "updatedAt" : "2019-01-18T03:06:14Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "30b8447f5ca42f0be25ab3144de9a68b532e9924",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +181,185 @@                        newValue,\n                        oldValue,\n                        entry.entry().context().timestamp());\n                }\n            }"
  },
  {
    "id" : "808f6a1b-c378-4610-af99-3c6e63706051",
    "prId" : 6161,
    "prUrl" : "https://github.com/apache/kafka/pull/6161#pullrequestreview-193785382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a5e534b-bf53-406c-a254-bfd8db70f069",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "nice cleanup!",
        "createdAt" : "2019-01-17T18:53:27Z",
        "updatedAt" : "2019-01-31T01:30:38Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4bcfd8a8650fee88171642e5b5eec9fd96fb27c",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +78,82 @@        cache.addDirtyEntryFlushListener(cacheName, entries -> {\n            for (final ThreadCache.DirtyEntry entry : entries) {\n                putAndMaybeForward(entry, context);\n            }\n        });"
  },
  {
    "id" : "cfb731dc-83a0-43c2-9c9f-327c6e77ae8b",
    "prId" : 6161,
    "prUrl" : "https://github.com/apache/kafka/pull/6161#pullrequestreview-194410868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c997316-23b3-40c7-b95e-435ff925543d",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "nit: move the `cache == null` check two lines up and save creating the `bytesKey` and `cacheKey` operations when the `cache` is `null`.  I know this is a minor point, but IMHO still worth the change.",
        "createdAt" : "2019-01-17T19:00:14Z",
        "updatedAt" : "2019-01-31T01:30:38Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "9202c199-1a10-4ac1-a050-386a70c689d5",
        "parentId" : "1c997316-23b3-40c7-b95e-435ff925543d",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "ack.",
        "createdAt" : "2019-01-20T21:49:56Z",
        "updatedAt" : "2019-01-31T01:30:38Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4bcfd8a8650fee88171642e5b5eec9fd96fb27c",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +150,154 @@        Objects.requireNonNull(key, \"key cannot be null\");\n        validateStoreOpen();\n        if (cache == null) {\n            return bytesStore.fetchSession(key, startTime, endTime);\n        } else {"
  },
  {
    "id" : "9269f5e7-8950-44aa-ac14-34c1c7b9e9dd",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-198401713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd677787-d9f8-4469-bcad-d2d09d300a1c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is another optimization I did for window / session stores: previously we deserialize bytes to `windowed<K>`, and then we serialize it back to `windowed<Bytes>` for underlying.put, which is a bit waste.\r\n\r\nNow what I did is `bytes -> windowed<Bytes> -> windowed<K>` where the first deser is always executed, while the second deser is executed only if new / old bytes are not null. Note that second deser actually only does `Bytes -> K` and we just wrap with the same `window`.",
        "createdAt" : "2019-01-31T02:32:25Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +191,195 @@                final AGG oldValue = sendOldValues && oldValueBytes != null ? serdes.valueFrom(oldValueBytes) : null;\n                // we need to get the old values if needed, and then put to store, and then flush\n                bytesStore.put(bytesKey, entry.newValue());\n\n                final ProcessorRecordContext current = context.recordContext();"
  },
  {
    "id" : "ba918484-1d9e-4ca0-904a-521c865a5aad",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-198402064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a90e2047-7bc7-4aa4-955c-5a25640eec19",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The actual fix here: we need to 1) read the old bytes, and 2) put the new bytes to underlying, and 3) apply flush listener.\r\n\r\nPreviously the ordering was 1) -> 3) -> 2), which has an issue that if the flushed downstream access the store, it does not have the new data yet which is incorrect. This fix was merged into KVStore some time ago, but we did not do the same for window / session store here.",
        "createdAt" : "2019-01-31T02:34:28Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +186,190 @@            // this is an optimization: if this key did not exist in underlying store and also not in the cache,\n            // we can skip flushing to downstream as well as writing to underlying store\n            if (newValueBytes != null || oldValueBytes != null) {\n                final Windowed<K> key = SessionKeySchema.from(bytesKey, serdes.keyDeserializer(), topic);\n                final AGG newValue = newValueBytes != null ? serdes.valueFrom(newValueBytes) : null;"
  },
  {
    "id" : "3730d4f6-80c4-4506-baaf-9f20245f9b57",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-202426327",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "529e6068-45ee-4ac0-af49-33b1c4e83581",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Do we need to check `sendOldValues` here?",
        "createdAt" : "2019-02-12T01:05:05Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +189,193 @@                final Windowed<K> key = SessionKeySchema.from(bytesKey, serdes.keyDeserializer(), topic);\n                final AGG newValue = newValueBytes != null ? serdes.valueFrom(newValueBytes) : null;\n                final AGG oldValue = sendOldValues && oldValueBytes != null ? serdes.valueFrom(oldValueBytes) : null;\n                // we need to get the old values if needed, and then put to store, and then flush\n                bytesStore.put(bytesKey, entry.newValue());"
  },
  {
    "id" : "bd713634-5b7e-4591-96b4-6d6693ca6bda",
    "prId" : 6331,
    "prUrl" : "https://github.com/apache/kafka/pull/6331#pullrequestreview-208307118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69498666-dc73-4664-84e0-763dd7aa9b6f",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "move this method only.",
        "createdAt" : "2019-02-27T02:20:44Z",
        "updatedAt" : "2019-02-28T00:49:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d95689529bd3b0aa046c37df59896930b9226bb",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +66,70 @@    }\n\n    private void putAndMaybeForward(final ThreadCache.DirtyEntry entry, final InternalProcessorContext context) {\n        final Bytes binaryKey = cacheFunction.key(entry.key());\n        final Windowed<Bytes> bytesKey = SessionKeySchema.from(binaryKey);"
  },
  {
    "id" : "add2b08e-e5e9-4074-849a-e9e96965edb1",
    "prId" : 6331,
    "prUrl" : "https://github.com/apache/kafka/pull/6331#pullrequestreview-208307143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "286e3348-2ede-4731-bf49-8d58960bedb5",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Below is just some method reordering within the class -- no actual code change.",
        "createdAt" : "2019-02-27T02:20:53Z",
        "updatedAt" : "2019-02-28T00:49:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d95689529bd3b0aa046c37df59896930b9226bb",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +106,110 @@    }\n\n    @Override\n    public void put(final Windowed<Bytes> key, final byte[] value) {\n        validateStoreOpen();"
  },
  {
    "id" : "ae1a328a-810e-4691-a85c-add39720f2b3",
    "prId" : 6448,
    "prUrl" : "https://github.com/apache/kafka/pull/6448#pullrequestreview-216495875",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b872753-220f-42bf-bac2-2d0804fb2850",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why we cannot use `MAX(earliestSessionEndTime, currentSegmentBeginTime())` here but directly `currentSegmentBeginTime()`?",
        "createdAt" : "2019-03-19T00:34:33Z",
        "updatedAt" : "2019-04-10T20:22:17Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "dcf5c422-aaac-452f-bb90-82139970167d",
        "parentId" : "8b872753-220f-42bf-bac2-2d0804fb2850",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just to follow the question above, could we directly restrict the range at this caller as:\r\n\r\n```\r\n(Math.max(earliestSessionEndTime, currentSegmentBeginTime()),\r\n Math.min(latestSessionStartTime, segmentEndTime))\r\n```",
        "createdAt" : "2019-03-19T00:36:34Z",
        "updatedAt" : "2019-04-10T20:22:17Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "b09b5982-cbc0-41d6-b3d9-8f20b01f12d3",
        "parentId" : "8b872753-220f-42bf-bac2-2d0804fb2850",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "The 2nd argument to setCacheKeyRange is the largest end timestamp of that segment --latestSessionStartTime is actually used in the method to set the upper range for every segment. And currentSegmentBeginTime() is always >= earliestSessionEndTime",
        "createdAt" : "2019-03-20T01:45:36Z",
        "updatedAt" : "2019-04-10T20:22:17Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "dcdc8866e0ff0293f2b974bc65cdfb3fc6618138",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +331,335 @@            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();"
  },
  {
    "id" : "32f160bc-97e6-4c92-a54d-2404a77b9587",
    "prId" : 9139,
    "prUrl" : "https://github.com/apache/kafka/pull/9139#pullrequestreview-502572221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4eb28b97-9ee0-49fa-8264-7b3c0648e7ce",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Can you fix the `keyFrom == keyTo` to use `.equals` on the side (down on line 370)",
        "createdAt" : "2020-10-06T04:18:53Z",
        "updatedAt" : "2020-10-08T04:16:47Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "14fce521346b290407c971e42bd684f55b512785",
    "line" : 223,
    "diffHunk" : "@@ -1,1 +478,482 @@        }\n\n        private void setCacheKeyRange(final long lowerRangeEndTime, final long upperRangeEndTime) {\n            if (cacheFunction.segmentId(lowerRangeEndTime) != cacheFunction.segmentId(upperRangeEndTime)) {\n                throw new IllegalStateException(\"Error iterating over segments: segment interval has changed\");"
  }
]