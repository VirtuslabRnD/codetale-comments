[
  {
    "id" : "874e0d83-228a-4364-ae81-09e55bee9643",
    "prId" : 6147,
    "prUrl" : "https://github.com/apache/kafka/pull/6147#pullrequestreview-192831329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ddfd975-278c-426f-b3a9-eabe217010b9",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "part of the fix: we now also forward the timestamp on eviction",
        "createdAt" : "2019-01-15T19:47:56Z",
        "updatedAt" : "2019-01-18T03:06:14Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "30b8447f5ca42f0be25ab3144de9a68b532e9924",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +181,185 @@                        newValue,\n                        oldValue,\n                        entry.entry().context().timestamp());\n                }\n            }"
  },
  {
    "id" : "808f6a1b-c378-4610-af99-3c6e63706051",
    "prId" : 6161,
    "prUrl" : "https://github.com/apache/kafka/pull/6161#pullrequestreview-193785382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a5e534b-bf53-406c-a254-bfd8db70f069",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "nice cleanup!",
        "createdAt" : "2019-01-17T18:53:27Z",
        "updatedAt" : "2019-01-31T01:30:38Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4bcfd8a8650fee88171642e5b5eec9fd96fb27c",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +78,82 @@        cache.addDirtyEntryFlushListener(cacheName, entries -> {\n            for (final ThreadCache.DirtyEntry entry : entries) {\n                putAndMaybeForward(entry, context);\n            }\n        });"
  },
  {
    "id" : "cfb731dc-83a0-43c2-9c9f-327c6e77ae8b",
    "prId" : 6161,
    "prUrl" : "https://github.com/apache/kafka/pull/6161#pullrequestreview-194410868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c997316-23b3-40c7-b95e-435ff925543d",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "nit: move the `cache == null` check two lines up and save creating the `bytesKey` and `cacheKey` operations when the `cache` is `null`.  I know this is a minor point, but IMHO still worth the change.",
        "createdAt" : "2019-01-17T19:00:14Z",
        "updatedAt" : "2019-01-31T01:30:38Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "9202c199-1a10-4ac1-a050-386a70c689d5",
        "parentId" : "1c997316-23b3-40c7-b95e-435ff925543d",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "ack.",
        "createdAt" : "2019-01-20T21:49:56Z",
        "updatedAt" : "2019-01-31T01:30:38Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4bcfd8a8650fee88171642e5b5eec9fd96fb27c",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +150,154 @@        Objects.requireNonNull(key, \"key cannot be null\");\n        validateStoreOpen();\n        if (cache == null) {\n            return bytesStore.fetchSession(key, startTime, endTime);\n        } else {"
  },
  {
    "id" : "9269f5e7-8950-44aa-ac14-34c1c7b9e9dd",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-198401713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd677787-d9f8-4469-bcad-d2d09d300a1c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is another optimization I did for window / session stores: previously we deserialize bytes to `windowed<K>`, and then we serialize it back to `windowed<Bytes>` for underlying.put, which is a bit waste.\r\n\r\nNow what I did is `bytes -> windowed<Bytes> -> windowed<K>` where the first deser is always executed, while the second deser is executed only if new / old bytes are not null. Note that second deser actually only does `Bytes -> K` and we just wrap with the same `window`.",
        "createdAt" : "2019-01-31T02:32:25Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +191,195 @@                final AGG oldValue = sendOldValues && oldValueBytes != null ? serdes.valueFrom(oldValueBytes) : null;\n                // we need to get the old values if needed, and then put to store, and then flush\n                bytesStore.put(bytesKey, entry.newValue());\n\n                final ProcessorRecordContext current = context.recordContext();"
  },
  {
    "id" : "ba918484-1d9e-4ca0-904a-521c865a5aad",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-198402064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a90e2047-7bc7-4aa4-955c-5a25640eec19",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The actual fix here: we need to 1) read the old bytes, and 2) put the new bytes to underlying, and 3) apply flush listener.\r\n\r\nPreviously the ordering was 1) -> 3) -> 2), which has an issue that if the flushed downstream access the store, it does not have the new data yet which is incorrect. This fix was merged into KVStore some time ago, but we did not do the same for window / session store here.",
        "createdAt" : "2019-01-31T02:34:28Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +186,190 @@            // this is an optimization: if this key did not exist in underlying store and also not in the cache,\n            // we can skip flushing to downstream as well as writing to underlying store\n            if (newValueBytes != null || oldValueBytes != null) {\n                final Windowed<K> key = SessionKeySchema.from(bytesKey, serdes.keyDeserializer(), topic);\n                final AGG newValue = newValueBytes != null ? serdes.valueFrom(newValueBytes) : null;"
  },
  {
    "id" : "3730d4f6-80c4-4506-baaf-9f20245f9b57",
    "prId" : 6191,
    "prUrl" : "https://github.com/apache/kafka/pull/6191#pullrequestreview-202426327",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "529e6068-45ee-4ac0-af49-33b1c4e83581",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Do we need to check `sendOldValues` here?",
        "createdAt" : "2019-02-12T01:05:05Z",
        "updatedAt" : "2019-02-13T06:01:27Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fef77ba1a948117f4218a8ae8b96a8cd924e8e7",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +189,193 @@                final Windowed<K> key = SessionKeySchema.from(bytesKey, serdes.keyDeserializer(), topic);\n                final AGG newValue = newValueBytes != null ? serdes.valueFrom(newValueBytes) : null;\n                final AGG oldValue = sendOldValues && oldValueBytes != null ? serdes.valueFrom(oldValueBytes) : null;\n                // we need to get the old values if needed, and then put to store, and then flush\n                bytesStore.put(bytesKey, entry.newValue());"
  }
]