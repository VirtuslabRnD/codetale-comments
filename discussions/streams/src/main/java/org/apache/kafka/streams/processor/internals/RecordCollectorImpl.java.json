[
  {
    "id" : "468bc459-f1a9-4e3d-83fa-ce52a8509376",
    "prId" : 5279,
    "prUrl" : "https://github.com/apache/kafka/pull/5279#pullrequestreview-131379868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Isn't `toString()` called implicitly?",
        "createdAt" : "2018-06-22T19:32:22Z",
        "updatedAt" : "2018-06-22T19:32:22Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fd3fab72-470a-468f-9f22-6f48be3a30b3",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We've observed from the actual logs that it's not actually.. and the reason is this: \r\n\r\nhttps://stackoverflow.com/questions/6371638/slf4j-how-to-log-formatted-message-object-array-exception",
        "createdAt" : "2018-06-23T00:24:42Z",
        "updatedAt" : "2018-06-23T00:24:42Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0f211f1d-3177-4c82-8da1-139d4efa4b2f",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Note this is indeed fixed in trunk but not in older versions.",
        "createdAt" : "2018-06-23T00:25:07Z",
        "updatedAt" : "2018-06-23T00:25:07Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "4c4d3e3f-75b6-425a-af34-7875279c07b3",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Thanks for clarification, @guozhangwang !",
        "createdAt" : "2018-06-23T01:14:58Z",
        "updatedAt" : "2018-06-23T01:14:58Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "867f3c36-d4ad-4e23-965c-1b99385a67b2",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Is only `1.0` affected? What about `1.1` or `0.11.0` and older?",
        "createdAt" : "2018-06-23T01:15:59Z",
        "updatedAt" : "2018-06-23T01:16:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e27ce2a851508e7093ab0f7ebf2b107c3f7b1306",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +111,115 @@                                log.error(\"Error sending record (key {} value {} timestamp {}) to topic {} due to {}; \" +\n                                                \"No more records will be sent and no more offsets will be recorded for this task.\",\n                                        key, value, timestamp, topic, exception.toString());\n                                if (exception instanceof ProducerFencedException) {\n                                    sendException = new ProducerFencedException(String.format(\"%sAbort sending since producer got fenced with a previous record (key %s value %s timestamp %d) to topic %s, error message: %s\","
  },
  {
    "id" : "734d6d33-50b8-46bf-86f7-5a0b6a1170e0",
    "prId" : 5613,
    "prUrl" : "https://github.com/apache/kafka/pull/5613#pullrequestreview-152962987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3059fbc-a045-482b-8d03-472184863ea7",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "this import was missing.",
        "createdAt" : "2018-09-05T13:47:58Z",
        "updatedAt" : "2018-09-06T14:24:33Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "43548a53-6f14-443d-90eb-34c4071ebaac",
        "parentId" : "c3059fbc-a045-482b-8d03-472184863ea7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why it did not fail before?",
        "createdAt" : "2018-09-05T17:44:36Z",
        "updatedAt" : "2018-09-06T14:24:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "5557845b-f6bc-465f-8904-0ba570fcc535",
        "parentId" : "c3059fbc-a045-482b-8d03-472184863ea7",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "don't know, maybe something was missed in porting back? compared against upstream/0.11.0 and the import is not there. The lines (128-137) requiring the import were introduced in #5520 so maybe in cherry-picking, it was missed",
        "createdAt" : "2018-09-06T14:37:46Z",
        "updatedAt" : "2018-09-06T14:50:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "34402b501d00d4f2af8b421a63004c2af3c3bd34",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +24,28 @@import org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.kafka.common.TopicPartition;"
  },
  {
    "id" : "1789054d-313d-451d-926b-0c65666656c2",
    "prId" : 6280,
    "prUrl" : "https://github.com/apache/kafka/pull/6280#pullrequestreview-257109026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9740ab8e-1a89-4e93-84c3-ab00030be6de",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: missing whitespace before `retries` (or after `and/or` the line above)",
        "createdAt" : "2019-07-02T19:40:52Z",
        "updatedAt" : "2019-07-02T20:29:25Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "82d8e33a52559274540b2dbebe61f35bcf1baeb3",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +61,65 @@    private final static String EXCEPTION_MESSAGE = \"%sAbort sending since %s with a previous record (timestamp %d) to topic %s due to %s\";\n    private final static String PARAMETER_HINT = \"\\nYou can increase the producer configs `delivery.timeout.ms` and/or \" +\n        \"`retries` to avoid this error. Note that `retries` is set to infinite by default.\";\n\n    private volatile KafkaException sendException;"
  },
  {
    "id" : "f346cc3e-55fc-4415-8bc0-adcc6e293915",
    "prId" : 6372,
    "prUrl" : "https://github.com/apache/kafka/pull/6372#pullrequestreview-210467895",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c050e54-b8cb-42af-9d3f-3b9306727450",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I added the `TimeoutException` to the error message and as the cause of the `StreamsException`. This made the lines too long, so I reformatted them.\r\n\r\nI also added some more failure modes to the log message; I felt the existing message could be misleading if the problem was actually just a network interruption.",
        "createdAt" : "2019-03-05T02:35:27Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9136d41ec244506da0739e2c2213fdaab4629c7",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +232,236 @@                String.format(\"%sFailed to send record to topic %s due to timeout.\", logPrefix, topic),\n                e\n            );\n        } catch (final Exception uncaughtException) {\n            if (uncaughtException instanceof KafkaException &&"
  },
  {
    "id" : "db355c6f-4598-4400-926e-28a2b9ebecf1",
    "prId" : 6372,
    "prUrl" : "https://github.com/apache/kafka/pull/6372#pullrequestreview-212067786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d2be8c6-f1d5-4ac1-957c-6c3e0355e22c",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: why one more indent ?",
        "createdAt" : "2019-03-07T02:08:04Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "7551ed35-1f63-49a1-9ebe-7fe1b1042a36",
        "parentId" : "3d2be8c6-f1d5-4ac1-957c-6c3e0355e22c",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It's just how Idea seems to format multiline expressions... e.g.,\r\n```\r\n1 +\r\n    2\r\n```\r\n\r\ninstead of \r\n```\r\n1 + \r\n2\r\n```\r\n\r\nIs it undesirable?",
        "createdAt" : "2019-03-07T18:08:32Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "218a276e-d3c8-4d12-84e7-4153a270403e",
        "parentId" : "3d2be8c6-f1d5-4ac1-957c-6c3e0355e22c",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Not important -- just look funky to me. We concatenate multiple strings and thus all should have the same indent -- why would we indent the first differently? I would format as follows (even if I know that you don't like that the plus goes into the next line):\r\n```\r\n\"string1\"\r\n+ \"string2\"\r\n+ \"string3\"\r\n```\r\n\r\nThis avoid the \"ambiguity\", of multiple string parameters, vs one concatenated parameter:\r\n```\r\nmethod(\r\n    \"param1\",\r\n    \"param2\",\r\n    \"param3\",\r\n    \"param4\");\r\n\r\n// vs\r\n\r\nmethod(\r\n    \"param-part-1\" +\r\n    \"param-part-2\" +\r\n    \"param-part-3\",\r\n    \"new-param\");\r\n\r\n// vs\r\n\r\nmethod(\r\n    \"param-part-1\"\r\n    + \"param-part-2\"\r\n    + \"param-part-3\",\r\n    \"new-param\");\r\n```\r\n\r\nThirst and second hard hard to distinguish (where do parameters start/stop), but third makes it clear, that it's two parameters but not one or four, what is hard to tell in the middle case. Of course, double indent also fixes this but it's weird to me:\r\n```\r\nmethod(\r\n    \"param-part-1\" +\r\n        \"param-part-2\" +\r\n        \"param-part-3\",\r\n    \"new-param\");\r\n```",
        "createdAt" : "2019-03-07T18:41:35Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "66e0db31-ddfa-404e-9e44-f404a5de9c91",
        "parentId" : "3d2be8c6-f1d5-4ac1-957c-6c3e0355e22c",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I remember a while back someone on the internet trying to get everyone to always put operators (and commas) at the beginning of the line for this reason. I think it makes sense. I don't think it caught on in general because it creates syntactic ambiguity in Javascript, but since Java requires semicolons to end a line, it should be fine.\r\n\r\nDo you have your IDE set up to create this formatting? Maybe it sounds lazy, but the reason I've formatted it this way is that that's what IDEA does by default. I don't want to spend time curating the number of indent spaces by hand on every code change. I couldn't figure out how to get rid of the extra indent in the multi-line string concatenation. Eg, it even does this:\r\n```java\r\n        final String s =\r\n            \"asdf\"\r\n                + \"qwer\"\r\n                + \"qwer\";\r\n```\r\nwhich is like the worst outcome.\r\n\r\n",
        "createdAt" : "2019-03-07T22:07:27Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "d7d8caf3-5115-4589-b424-94ca2fdf72f6",
        "parentId" : "3d2be8c6-f1d5-4ac1-957c-6c3e0355e22c",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I don't know the IDE setting -- this case is rare enough that I \"fix\" fit manually if it happens.",
        "createdAt" : "2019-03-07T22:23:14Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "7dbc23d6-7014-4a19-9e7d-8b93e75566d1",
        "parentId" : "3d2be8c6-f1d5-4ac1-957c-6c3e0355e22c",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Let me know if you plan to address or ignore this -- I am fine either way.",
        "createdAt" : "2019-03-07T22:30:27Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fd09300b-2275-4135-8417-1c6367d39e06",
        "parentId" : "3d2be8c6-f1d5-4ac1-957c-6c3e0355e22c",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, I agree with you in principle, but I think I'll just leave it as-is, until I can figure out a way to get the IDE to do it for me.",
        "createdAt" : "2019-03-07T23:01:41Z",
        "updatedAt" : "2019-03-07T23:01:41Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9136d41ec244506da0739e2c2213fdaab4629c7",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +221,225 @@            log.error(\n                \"Timeout exception caught when sending record to topic {}. \" +\n                    \"This might happen if the producer cannot send data to the Kafka cluster and thus, \" +\n                    \"its internal buffer fills up. \" +\n                    \"This can also happen if the broker is slow to respond, if the network connection to \" +"
  },
  {
    "id" : "e895fd0f-7b0f-4e14-87ad-352480e442a7",
    "prId" : 7223,
    "prUrl" : "https://github.com/apache/kafka/pull/7223#pullrequestreview-278123699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "218b8eae-1025-47ee-b2ab-32356243571b",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Would it be simpler to hand out a deep copy of the map directly?",
        "createdAt" : "2019-08-20T01:15:15Z",
        "updatedAt" : "2019-08-26T15:29:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "68c8986f-4b92-43c0-8dff-6fa94e686de2",
        "parentId" : "218b8eae-1025-47ee-b2ab-32356243571b",
        "authorId" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "body" : "That would be totally fine for the current usage pattern, where we have a single query that ends up copying the map anyway. However, if we ever ended up with other queries then those queries would pay the cost of the copy whether they need it or not. It would be a bit surprising that a copy is happening without looking at the implementation. My bias would be towards defensive coding without surprise performance impact.",
        "createdAt" : "2019-08-20T02:20:18Z",
        "updatedAt" : "2019-08-26T15:29:28Z",
        "lastEditedBy" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "tags" : [
        ]
      },
      {
        "id" : "4269fd24-5f24-4e17-b859-74434c07ecc7",
        "parentId" : "218b8eae-1025-47ee-b2ab-32356243571b",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Fair enough.",
        "createdAt" : "2019-08-21T23:25:02Z",
        "updatedAt" : "2019-08-26T15:29:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e0fd82b3fcf3eb7d338aabc674aead40c28e0f6",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +281,285 @@    @Override\n    public Map<TopicPartition, Long> offsets() {\n        return Collections.unmodifiableMap(offsets);\n    }\n"
  }
]