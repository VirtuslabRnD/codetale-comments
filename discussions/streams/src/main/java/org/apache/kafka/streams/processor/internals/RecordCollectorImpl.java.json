[
  {
    "id" : "468bc459-f1a9-4e3d-83fa-ce52a8509376",
    "prId" : 5279,
    "prUrl" : "https://github.com/apache/kafka/pull/5279#pullrequestreview-131379868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Isn't `toString()` called implicitly?",
        "createdAt" : "2018-06-22T19:32:22Z",
        "updatedAt" : "2018-06-22T19:32:22Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fd3fab72-470a-468f-9f22-6f48be3a30b3",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We've observed from the actual logs that it's not actually.. and the reason is this: \r\n\r\nhttps://stackoverflow.com/questions/6371638/slf4j-how-to-log-formatted-message-object-array-exception",
        "createdAt" : "2018-06-23T00:24:42Z",
        "updatedAt" : "2018-06-23T00:24:42Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0f211f1d-3177-4c82-8da1-139d4efa4b2f",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Note this is indeed fixed in trunk but not in older versions.",
        "createdAt" : "2018-06-23T00:25:07Z",
        "updatedAt" : "2018-06-23T00:25:07Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "4c4d3e3f-75b6-425a-af34-7875279c07b3",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Thanks for clarification, @guozhangwang !",
        "createdAt" : "2018-06-23T01:14:58Z",
        "updatedAt" : "2018-06-23T01:14:58Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "867f3c36-d4ad-4e23-965c-1b99385a67b2",
        "parentId" : "a0c51f5f-bb31-480c-8ace-84bf1ace37e0",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Is only `1.0` affected? What about `1.1` or `0.11.0` and older?",
        "createdAt" : "2018-06-23T01:15:59Z",
        "updatedAt" : "2018-06-23T01:16:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e27ce2a851508e7093ab0f7ebf2b107c3f7b1306",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +111,115 @@                                log.error(\"Error sending record (key {} value {} timestamp {}) to topic {} due to {}; \" +\n                                                \"No more records will be sent and no more offsets will be recorded for this task.\",\n                                        key, value, timestamp, topic, exception.toString());\n                                if (exception instanceof ProducerFencedException) {\n                                    sendException = new ProducerFencedException(String.format(\"%sAbort sending since producer got fenced with a previous record (key %s value %s timestamp %d) to topic %s, error message: %s\","
  },
  {
    "id" : "734d6d33-50b8-46bf-86f7-5a0b6a1170e0",
    "prId" : 5613,
    "prUrl" : "https://github.com/apache/kafka/pull/5613#pullrequestreview-152962987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3059fbc-a045-482b-8d03-472184863ea7",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "this import was missing.",
        "createdAt" : "2018-09-05T13:47:58Z",
        "updatedAt" : "2018-09-06T14:24:33Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "43548a53-6f14-443d-90eb-34c4071ebaac",
        "parentId" : "c3059fbc-a045-482b-8d03-472184863ea7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why it did not fail before?",
        "createdAt" : "2018-09-05T17:44:36Z",
        "updatedAt" : "2018-09-06T14:24:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "5557845b-f6bc-465f-8904-0ba570fcc535",
        "parentId" : "c3059fbc-a045-482b-8d03-472184863ea7",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "don't know, maybe something was missed in porting back? compared against upstream/0.11.0 and the import is not there. The lines (128-137) requiring the import were introduced in #5520 so maybe in cherry-picking, it was missed",
        "createdAt" : "2018-09-06T14:37:46Z",
        "updatedAt" : "2018-09-06T14:50:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "34402b501d00d4f2af8b421a63004c2af3c3bd34",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +24,28 @@import org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.kafka.common.TopicPartition;"
  }
]