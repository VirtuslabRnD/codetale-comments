[
  {
    "id" : "2e9b01c3-ad71-4ac3-b790-6b8b4663d0ee",
    "prId" : 4343,
    "prUrl" : "https://github.com/apache/kafka/pull/4343#pullrequestreview-86074571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccc1c4a9-0aa4-4de8-86df-53f0dc933cca",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why this change? AFAIK, `unsubscribe()` will never throw.",
        "createdAt" : "2017-12-27T21:37:59Z",
        "updatedAt" : "2018-01-02T02:05:30Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fff882b8-f475-4bf0-be3d-436f9ad404c3",
        "parentId" : "ccc1c4a9-0aa4-4de8-86df-53f0dc933cca",
        "authorId" : "5b2b5fa0-0fb4-4e52-a4f9-9d4318b5ae5d",
        "body" : "It throws in the test since the consumer was never opened.",
        "createdAt" : "2018-01-02T01:55:21Z",
        "updatedAt" : "2018-01-02T02:05:30Z",
        "lastEditedBy" : "5b2b5fa0-0fb4-4e52-a4f9-9d4318b5ae5d",
        "tags" : [
        ]
      }
    ],
    "commit" : "5902a645c8eb9fc9f25f3c7076a0acc7d1e5def3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +277,281 @@        try {\n            restoreConsumer.unsubscribe();\n        } catch (final RuntimeException fatalException) {\n            firstException.compareAndSet(null, fatalException);\n        }"
  },
  {
    "id" : "e5a27415-476f-4ef2-aa0d-f2014179cab1",
    "prId" : 4636,
    "prUrl" : "https://github.com/apache/kafka/pull/4636#pullrequestreview-113914060",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4633a886-67b0-4ee6-be3e-f9b80cc3b951",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I think this can still be package-private.",
        "createdAt" : "2018-04-19T21:56:03Z",
        "updatedAt" : "2018-05-31T03:24:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "2db233ae-5d13-457f-bf5f-f81f77664c27",
        "parentId" : "4633a886-67b0-4ee6-be3e-f9b80cc3b951",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Unfortunately not. We need access in `FutureStreamsPartitionAssignor` that is in a different package.",
        "createdAt" : "2018-04-20T09:44:55Z",
        "updatedAt" : "2018-05-31T03:24:33Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a87bd5254155a9d60ba479371305ddaae99282d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +43,47 @@import static java.util.Collections.singleton;\n\npublic class TaskManager {\n    // initialize the task list\n    // activeTasks needs to be concurrent as it can be accessed"
  },
  {
    "id" : "46c2ea2a-f736-41ed-ba76-88b31a01e1b4",
    "prId" : 4909,
    "prUrl" : "https://github.com/apache/kafka/pull/4909#pullrequestreview-117689380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7310105a-9031-4324-b6b3-140c53ead153",
        "parentId" : null,
        "authorId" : "2c9c4dbb-be9d-424e-8d8a-9f3d67f8372b",
        "body" : "ditto",
        "createdAt" : "2018-05-04T17:22:26Z",
        "updatedAt" : "2018-05-04T21:46:46Z",
        "lastEditedBy" : "2c9c4dbb-be9d-424e-8d8a-9f3d67f8372b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9abf1fea8a9e8c80b3f736aecb7a5445a9a4e5f",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +459,463 @@    public String toString(final String indent) {\n        final StringBuilder builder = new StringBuilder();\n        builder.append(\"TaskManager\\n\");\n        builder.append(indent).append(\"\\tMetadataState:\\n\");\n        builder.append(streamsMetadataState.toString(indent + \"\\t\\t\"));"
  },
  {
    "id" : "62af1781-8e8e-47c5-90b3-135b909b1ca1",
    "prId" : 6113,
    "prUrl" : "https://github.com/apache/kafka/pull/6113#pullrequestreview-206979987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "709aa2b6-546e-4e1a-951f-9505086aef34",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why do we close those tasks instead of just suspending them? Could we close them only if they are not re-assigned?",
        "createdAt" : "2019-01-14T17:41:42Z",
        "updatedAt" : "2019-02-22T01:03:34Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "f0311874-95ec-4cb9-9e9d-6f7acd70e88d",
        "parentId" : "709aa2b6-546e-4e1a-951f-9505086aef34",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The life time of a task is:\r\n\r\ncreated -> [initializeStateStores] -> restoring (writes to the initialized state stores) -> [initializeTopology] -> running -> [closeTopology] -> suspended -> [closeStateManager] -> dead\r\n\r\nI.e. the restoring tasks do not have topology initialized at all, whereas `suspend` call is just trying to closeTopology.",
        "createdAt" : "2019-02-22T00:56:53Z",
        "updatedAt" : "2019-02-22T01:03:34Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "b353f98c-2a22-4188-9de8-62d53ae28015",
        "parentId" : "709aa2b6-546e-4e1a-951f-9505086aef34",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ok, but why can't we keep restoring task open than, and hope they get reassigned so we can continue restoring them?",
        "createdAt" : "2019-02-22T01:40:43Z",
        "updatedAt" : "2019-02-22T01:40:43Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "df40113c-6ddf-4334-8abd-1d371c0d3eb6",
        "parentId" : "709aa2b6-546e-4e1a-951f-9505086aef34",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yes we can, the issue is that today we clear all the `store-restorers` hence there's an issue.\r\n\r\nWe can, of course do some optimizations like `do not close restoring tasks, and also do not clear their corresponding restorers as well`, but this is out of the scope of this PR and I want to address it separately.\r\n\r\ncc @vvcephei ",
        "createdAt" : "2019-02-22T18:46:40Z",
        "updatedAt" : "2019-02-22T18:47:53Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "b26be70b-2b94-4472-9847-b54e6fa3e9f8",
        "parentId" : "709aa2b6-546e-4e1a-951f-9505086aef34",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ack",
        "createdAt" : "2019-02-22T18:48:47Z",
        "updatedAt" : "2019-02-22T18:48:48Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "447a9bb3-1718-4b0a-b62a-24cd6ad26bb4",
        "parentId" : "709aa2b6-546e-4e1a-951f-9505086aef34",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "@guozhangwang Can you create a Jira to track this cleanup?",
        "createdAt" : "2019-02-22T18:55:39Z",
        "updatedAt" : "2019-02-22T18:55:39Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "9607b351-b286-4a62-9e68-227482116219",
        "parentId" : "709aa2b6-546e-4e1a-951f-9505086aef34",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yup: https://issues.apache.org/jira/browse/KAFKA-7985",
        "createdAt" : "2019-02-22T19:07:08Z",
        "updatedAt" : "2019-02-22T19:07:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "912a45642a92f4298d203d5c4eda6a1678208c05",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +244,248 @@        // for those restoring and still assigned tasks, they will be re-created\n        // in addStreamTasks.\n        firstException.compareAndSet(null, active.closeAllRestoringTasks());\n        changelogReader.reset();\n"
  },
  {
    "id" : "a41809f3-78d6-451c-b62b-ad84c1d42d4b",
    "prId" : 7321,
    "prUrl" : "https://github.com/apache/kafka/pull/7321#pullrequestreview-291486026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "294c4f97-377a-4c8f-afaf-e8d7c4baa1fb",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "In our current code we check `if (assignment.containsAll(partitions)) {` and if the passed in partitions does not include all the partitions of the task, we treat it as `log.warn(\"Task {} owned partitions {} are not contained in the assignment {}\", taskId, partitions, assignment);` and create it as new (but I think the existing code did not close the old tasks from suspended, which is a bug..).\r\n\r\nPondering on it now, I'm not sure what would be the best solution to this, since such things would only happen if 1) users changed the partition-grouper, which is very rare and we are deprecating that API, and 2) users changed their code hence changed the topology, in which case `taskA_B` no longer mean the same thing at all. In either case, we are facing a much more severe issue than resuming suspended tasks.\r\n\r\nMaybe for now it's better to still log a WARN and not resume the task?",
        "createdAt" : "2019-09-20T17:31:33Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f3a8ff6c-fd84-48e5-b920-fcd06b3751c9",
        "parentId" : "294c4f97-377a-4c8f-afaf-e8d7c4baa1fb",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Yeah the implications of changing the task -> partition mapping seem pretty hairy (hence wanting to deprecate PartitionGrouper ASAP -- thanks @mjsax 😉)\r\nI was a bit worried about throwing an exception here, but if you follow the code path it really shouldn't be possible for a partition not to be found. The partition -> task and task -> partition maps are created at the same time from the assigned partitions list. There's no way for a task to be assigned partitions that aren't in the assignment.\r\nIf the mapping does change, we catch this in `maybeResumeSuspenedTask` and close/recreate the task",
        "createdAt" : "2019-09-22T01:47:32Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3935aa6be7cc55911bb4e24332557df24d01f18",
    "line" : 405,
    "diffHunk" : "@@ -1,1 +574,578 @@    private Set<TaskId> partitionsToTaskSet(final Collection<TopicPartition> partitions) {\n        final Set<TaskId> taskIds = new HashSet<>();\n        for (final TopicPartition tp : partitions) {\n            final TaskId id = partitionsToTaskId.get(tp);\n            if (id != null) {"
  },
  {
    "id" : "ea867a60-2ac6-442a-814f-b46db53adda8",
    "prId" : 7321,
    "prUrl" : "https://github.com/apache/kafka/pull/7321#pullrequestreview-292743295",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Related to my other comments: if we assume that the same thread never have active and standby of the same task at the same time then we can simplify this a bit: first we can check inside `onAssignment` to make sure it is indeed the case, and if not treat it as a fatal error. Then we can be ensured that the partitions do not have any overlaps and we can just \"blindly\" remove partitions as we remove standbys since if they are not assigned yet, the removal would be safely a no-op. The only usage of this would be line 124 below.",
        "createdAt" : "2019-09-20T18:00:19Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "fc58d8f8-f8d2-42b1-b4a4-50160f010630",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Not sure I follow, why does it help us to check this in `onAssignment`? We should be confident there is no overlap (unless there is a bug, might be good to verify somewhere I suppose) -- `restoreConsumerAssignedStandbys` is meant as a flag whether it's assigned standby OR active, ie this would be false if it is assigned any active.",
        "createdAt" : "2019-09-22T01:51:15Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "ebc89809-dd22-4189-834b-5028cb0f3a8b",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Technically it should always be safe to remove any revoked standby or actives since its a no-op if not assigned them anyway. This is what the PR was doing originally, which was \"accidentally correct\" since I wasn't aware it was only assigned one or the other.",
        "createdAt" : "2019-09-22T01:52:30Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "780ea210-ba71-444b-b0e2-db6862261d59",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "My only motivation is to simplify the usage of the flag a bit :) I feel that some conditions on this flag is not necessary.",
        "createdAt" : "2019-09-23T17:13:20Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "42d3614f-2d3d-4440-a576-8305c64d6150",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I was thinking of it more as a slight optimization to avoid making a copy of the assignment and re-assigning, but maybe it's better to play it safe & decrease complexity by just always removing the partitions",
        "createdAt" : "2019-09-24T00:15:06Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "484a791c-a76f-4047-a335-61736d9e82a2",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Moved the removing changelogs from restore consumer into a separate method. I left in the check for whether its assigned/removing standbys or active but (I think) I simplified it a bit -- happy to take it out if you feel strongly though (note we can't get rid of it entirely as we need to check eg whether to unsubscribe all if its reading standbys and there are new restoring tasks",
        "createdAt" : "2019-09-24T00:23:54Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "3277192c-48a1-4884-a3bf-c0989f7815d4",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "@ableegoldman this part of the code seems not changing to me?",
        "createdAt" : "2019-09-24T21:07:28Z",
        "updatedAt" : "2019-09-24T21:07:29Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "e2eb74e8-c23d-4cbd-aeb2-c98747239221",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Sorry, what seems not changed?\r\n`restoreConsumerAssigneStandbys` is now used in only two places, first to determine whether it should unsubscribe all in case there are new restoring tasks, and second in `removeChangelogsFromRestoreConsumer` where we skip removing partitions if they weren't what it was assigned to (to avoid overhead of copying assignment)",
        "createdAt" : "2019-09-24T21:16:44Z",
        "updatedAt" : "2019-09-24T21:16:44Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "f531d2f3-b68c-49ed-8ab0-5db15558bd9e",
        "parentId" : "74c0ad2f-e194-49b2-a16f-8b073baa07a9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Ah I think I was looking at an old commit :P LGTM now.",
        "createdAt" : "2019-09-24T22:30:54Z",
        "updatedAt" : "2019-09-24T22:30:54Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3935aa6be7cc55911bb4e24332557df24d01f18",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +63,67 @@\n    // the restore consumer is only ever assigned changelogs from restoring tasks or standbys (but not both)\n    private boolean restoreConsumerAssignedStandbys = false;\n\n    // following information is updated during rebalance phase by the partition assignor"
  },
  {
    "id" : "9f195015-b486-4665-8ec6-5ff4bc5fa650",
    "prId" : 7608,
    "prUrl" : "https://github.com/apache/kafka/pull/7608#pullrequestreview-308822028",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f67735e4-52f8-49fe-9ec6-9af348f87d55",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Tried to tighten up the safety check here by verifying that all the things we expect to be fully cleared after `onPartitionsLost` are indeed empty of state/old objects",
        "createdAt" : "2019-10-29T02:32:46Z",
        "updatedAt" : "2019-10-29T23:41:43Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "b76d8dda-40de-4c57-ad26-428615fa19ed",
        "parentId" : "f67735e4-52f8-49fe-9ec6-9af348f87d55",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Should we also check other maps in `TaskManager`, like `assignedStandbyTasks`or `revokedActiveTasks`. Again just my feeling about the current setup, I don't find it intuitive to deal with multiple mapping structs and they look so similar to me :) It's very easy to forget what they refer to really soon.",
        "createdAt" : "2019-10-29T19:12:02Z",
        "updatedAt" : "2019-10-29T23:41:43Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "2a657dfc-b9bc-42fe-9bcb-15edd169ee53",
        "parentId" : "f67735e4-52f8-49fe-9ec6-9af348f87d55",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Well, we actually don't clear the standbys on `onPartitionsLost` as they weren't part of the group subscription to begin with. `revokedActiveTasks` and `addedActiveTasks` just tell us how the assignment changed since the previous one, and don't matter at all to us here (they aren't required to be empty, since they're not updated except after a rebalance)",
        "createdAt" : "2019-10-29T21:12:52Z",
        "updatedAt" : "2019-10-29T23:41:43Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "01a2aedaf962ef937687f97786c637ed6821efd5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +270,274 @@        if (exception != null) {\n            throw exception;\n        } else if (!(active.isEmpty() && assignedActiveTasks.isEmpty() && changelogReader.isEmpty())) {\n            throw new IllegalStateException(\"TaskManager found leftover active task state after closing all zombies\");\n        }"
  },
  {
    "id" : "2c95d166-dde1-48ec-b9f5-c431bc8993a4",
    "prId" : 7631,
    "prUrl" : "https://github.com/apache/kafka/pull/7631#pullrequestreview-310732592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1f259c4-c545-4251-b405-4737e2e37add",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Not sure why this was trace as the same message is debug in the parallel active method",
        "createdAt" : "2019-11-01T20:36:57Z",
        "updatedAt" : "2019-11-01T23:07:24Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "220ca4c7-22e4-4bf7-b3ce-7acb505b3993",
        "parentId" : "c1f259c4-c545-4251-b405-4737e2e37add",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Good catch!",
        "createdAt" : "2019-11-01T23:11:41Z",
        "updatedAt" : "2019-11-01T23:11:42Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be1994fa4dd51c67bdb8e964d40699bedf770f30",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +164,168 @@\n    private void addNewStandbyTasks(final Map<TaskId, Set<TopicPartition>> newStandbyTasks) {\n        log.debug(\"New standby tasks to be created: {}\", newStandbyTasks);\n\n        for (final StandbyTask task : standbyTaskCreator.createTasks(consumer, newStandbyTasks)) {"
  },
  {
    "id" : "b17672ef-8843-43f7-9fa0-f37ebf53ad61",
    "prId" : 7681,
    "prUrl" : "https://github.com/apache/kafka/pull/7681#pullrequestreview-316029454",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5eb16849-f21b-459a-98c4-c6147b7788d7",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Adding a DEBUG log. This is also in support of the new system tests, but I think it's again nice to have this information available. I actually added this log while debugging the issue, before I wrote the test, to manually verify the bug.",
        "createdAt" : "2019-11-13T06:52:34Z",
        "updatedAt" : "2019-11-13T21:20:04Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "741306599487c10a3d5f753920f1f1c2e0e0ec73",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +441,445 @@        }\n\n        log.debug(\"Assigning and seeking restoreConsumer to {}\", checkpointedOffsets);\n        restoreConsumerAssignedStandbys = true;\n        restoreConsumer.assign(checkpointedOffsets.keySet());"
  },
  {
    "id" : "84ffd9d6-29d2-48dc-a7eb-dc69238310f2",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-354871741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e13e57e-3752-4230-952f-b244f1d8f46e",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We have to call changelog.remove() before task.closeDirty since now we clear the `stores` map in closeDirty, and after that task.changelogPartitions() would return nothing.",
        "createdAt" : "2020-02-07T00:55:47Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +128,132 @@\n            // this call is idempotent so even if the task is only CREATED we can still call it\n            changelogReader.remove(task.changelogPartitions());\n\n            // mark corrupted partitions to not be checkpointed, and then close the task as dirty"
  },
  {
    "id" : "e9bece07-4f47-48c9-a8bb-03821059137e",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-354871741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77d47d73-fed6-4a2c-9ecf-c61dce36691c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Consolidated a couple of the same pattern into this private function.",
        "createdAt" : "2020-02-07T00:56:39Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +175,179 @@                standbyTasksToCreate.remove(task.id());\n            } else /* we previously owned this task, and we don't have it anymore, or it has changed active/standby state */ {\n                cleanupTask(task);\n\n                try {"
  },
  {
    "id" : "68f05ff0-c215-40bd-811d-27b2920042f1",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-354871741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d26b917-c9aa-4e08-9293-176ea8cf3fb4",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The order here cannot be changed so I left this comment.",
        "createdAt" : "2020-02-07T00:58:18Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +360,364 @@\n    private void cleanupTask(final Task task) {\n        // 1. remove the changelog partitions from changelog reader;\n        // 2. remove the input partitions from the materialized map;\n        // 3. remove the task metrics from the metrics registry"
  },
  {
    "id" : "1579703f-d064-4618-8172-f2d366d65ec9",
    "prId" : 8121,
    "prUrl" : "https://github.com/apache/kafka/pull/8121#pullrequestreview-369950615",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ba0dc11-0ced-4bd0-b3ff-e08e43b08cd8",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I'm just a _tiny_ bit uncomfortable with re-using that sentinel, because the correctness of our logic depends on the active sentinel being less than the standby sentinel, so it _must be_ less than zero. Do we have a reason to believe that `Task.LATEST_OFFSET` would never change to a number that would spoil us here, such as zero?",
        "createdAt" : "2020-03-05T20:54:11Z",
        "updatedAt" : "2020-03-05T23:38:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "3712bd5f-a2ca-4951-be52-d7165fb24fb0",
        "parentId" : "8ba0dc11-0ced-4bd0-b3ff-e08e43b08cd8",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I actually changed this based on working on the next PR, as `Task#changelogOffsets` uses this sentinel for exactly the same thing, ie an indicator that the task is running (and active). This is only used in computing the lag info for KIP-535, which has a similar desire to differentiate between a running task that is completely caught up and any other. So, I can't imagine this being changed -- but I can add a comment to the constant explaining it should always be negative (not sure why it's \"-2\" specifically, as opposed to \"-1\", do you?)",
        "createdAt" : "2020-03-05T22:08:07Z",
        "updatedAt" : "2020-03-05T23:38:27Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c1849c0c1506c3db22a16d0954faf76135b89e2",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +364,368 @@        for (final TaskId id : tasksOnLocalStorage()) {\n            if (isRunning(id)) {\n                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n            } else {\n                taskOffsetSums.put(id, 0L);"
  },
  {
    "id" : "ff9548f9-2517-44c0-9372-dc58ae49df24",
    "prId" : 8140,
    "prUrl" : "https://github.com/apache/kafka/pull/8140#pullrequestreview-361605374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "828aa10f-dd13-4e2d-a77f-6b54b08ca067",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Also not necessary, but I feel it better suggests/reminds that it is taking some action here and not just checking a condition",
        "createdAt" : "2020-02-20T03:01:17Z",
        "updatedAt" : "2020-02-20T20:31:48Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "3164fce49de499e640dad11ed3d56ba47ad14bb7",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +209,213 @@     * @throws StreamsException if the store's change log does not contain the partition\n     */\n    boolean tryToCompleteRestoration() {\n        boolean allRunning = true;\n"
  },
  {
    "id" : "2c1b915f-f4f8-4ae9-bfb6-78586bab93cc",
    "prId" : 8140,
    "prUrl" : "https://github.com/apache/kafka/pull/8140#pullrequestreview-362141507",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ce85afc-5d28-4724-8ef8-092a48dad4b7",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yup! I also found this bug and fixed in another PR, but great to see it confirmed in yours as well :)",
        "createdAt" : "2020-02-20T18:47:43Z",
        "updatedAt" : "2020-02-20T20:31:48Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3164fce49de499e640dad11ed3d56ba47ad14bb7",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +152,156 @@                try {\n                    changelogReader.remove(task.changelogPartitions());\n                    task.closeClean();\n                } catch (final RuntimeException e) {\n                    log.error(String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id()), e);"
  },
  {
    "id" : "a9fa2502-8fd9-43a4-9b9d-087c4d4c0763",
    "prId" : 8187,
    "prUrl" : "https://github.com/apache/kafka/pull/8187#pullrequestreview-366115978",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b63d227-d9cf-4dfe-a97e-e9ee8f9df2e7",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Simplify the logic here as we no longer throws before task suspend",
        "createdAt" : "2020-02-27T22:48:20Z",
        "updatedAt" : "2020-02-28T04:50:31Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "32906d27-efc4-481c-9c57-e49dbb14bbf3",
        "parentId" : "6b63d227-d9cf-4dfe-a97e-e9ee8f9df2e7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Not sure I follow: we can still throw from inside `suspend` right?",
        "createdAt" : "2020-02-28T01:01:23Z",
        "updatedAt" : "2020-02-28T04:50:31Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "cc890619-2fe5-42a0-a4c1-41466f636956",
        "parentId" : "6b63d227-d9cf-4dfe-a97e-e9ee8f9df2e7",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "What I mean is that we do not throw if remaining partitions are not empty anymore. We could directly suspend a task here.",
        "createdAt" : "2020-02-28T01:35:18Z",
        "updatedAt" : "2020-02-28T04:50:31Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "a9e6f239-8a7f-430f-89db-0054a44f1520",
        "parentId" : "6b63d227-d9cf-4dfe-a97e-e9ee8f9df2e7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I see.",
        "createdAt" : "2020-02-28T01:45:47Z",
        "updatedAt" : "2020-02-28T04:50:31Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "abbb2e38df0ff7c250e3b77588c7cc62b22bb4de",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +310,314 @@        for (final Task task : tasks.values()) {\n            if (remainingPartitions.containsAll(task.inputPartitions())) {\n                task.suspend();\n            }\n            remainingPartitions.removeAll(task.inputPartitions());"
  },
  {
    "id" : "0611a8e0-80cc-4088-99dc-0a8dc13e978c",
    "prId" : 8187,
    "prUrl" : "https://github.com/apache/kafka/pull/8187#pullrequestreview-366116529",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aabb7885-d31a-44a8-ad73-d96467947090",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think we should use `instanceof KafkaException` as well here:  if it is any inheritance of KafkaException, we should not wrap it.",
        "createdAt" : "2020-02-28T01:02:27Z",
        "updatedAt" : "2020-02-28T04:50:31Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "ec54f246-1042-40b0-9aee-b1bb244cdc5f",
        "parentId" : "aabb7885-d31a-44a8-ad73-d96467947090",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sounds good",
        "createdAt" : "2020-02-28T01:47:42Z",
        "updatedAt" : "2020-02-28T04:50:31Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "abbb2e38df0ff7c250e3b77588c7cc62b22bb4de",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +200,204 @@        if (!taskCloseExceptions.isEmpty()) {\n            for (final Map.Entry<TaskId, RuntimeException> entry : taskCloseExceptions.entrySet()) {\n                if (!(entry.getValue() instanceof TaskMigratedException)) {\n                    if (entry.getValue() instanceof KafkaException) {\n                        log.error(\"Hit Kafka exception while closing for first task {}\", entry.getKey());"
  },
  {
    "id" : "589e2c48-072c-4c51-9491-3864566bc0e0",
    "prId" : 8213,
    "prUrl" : "https://github.com/apache/kafka/pull/8213#pullrequestreview-369125744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5554efd3-3357-4c4c-a7af-3514964a65ef",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: These two functions are not for testing only.",
        "createdAt" : "2020-03-03T23:16:05Z",
        "updatedAt" : "2020-03-05T18:59:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "47125c9c-767e-4350-b080-65fbfa046789",
        "parentId" : "5554efd3-3357-4c4c-a7af-3514964a65ef",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks for the feedback; I didn't understand this particular comment, though.",
        "createdAt" : "2020-03-04T17:38:53Z",
        "updatedAt" : "2020-03-05T18:59:47Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "017b1747-bd5e-48e2-a4f3-beb040b84add",
        "parentId" : "5554efd3-3357-4c4c-a7af-3514964a65ef",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The comment line above these two method declaration says `the following functions are for test only`, but these two functions are not.",
        "createdAt" : "2020-03-04T18:23:24Z",
        "updatedAt" : "2020-03-05T18:59:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8fb300ed-8cce-4aa2-9078-36d978db51d7",
        "parentId" : "5554efd3-3357-4c4c-a7af-3514964a65ef",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ah, I just found what you were talking about:\r\n```\r\n    // below are for testing only\r\n```\r\n\r\nI didn't notice that up there.",
        "createdAt" : "2020-03-04T21:11:41Z",
        "updatedAt" : "2020-03-05T18:59:47Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "2eb596b0721bcf1908d19f811d96e5121ca4e41d",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +654,658 @@    }\n\n    Map<MetricName, Metric> producerMetrics() {\n        return activeTaskCreator.producerMetrics();\n    }"
  },
  {
    "id" : "a53fd6aa-245b-4bdc-b8bd-75e08ddcaf3e",
    "prId" : 8213,
    "prUrl" : "https://github.com/apache/kafka/pull/8213#pullrequestreview-369014249",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bcc8732a-e1e2-4cb1-a716-b7c5ce6f684e",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "In `shutdown(final boolean clean)` we should also release task producers as well right?",
        "createdAt" : "2020-03-03T23:21:56Z",
        "updatedAt" : "2020-03-05T18:59:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "b7c5b55a-e3f2-49e7-bb34-2978876f412c",
        "parentId" : "bcc8732a-e1e2-4cb1-a716-b7c5ce6f684e",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Yep. Good catch.",
        "createdAt" : "2020-03-04T18:22:59Z",
        "updatedAt" : "2020-03-05T18:59:47Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "2eb596b0721bcf1908d19f811d96e5121ca4e41d",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +83,87 @@                final String logPrefix,\n                final StreamsMetricsImpl streamsMetrics,\n                final ActiveTaskCreator activeTaskCreator,\n                final StandbyTaskCreator standbyTaskCreator,\n                final InternalTopologyBuilder builder,"
  },
  {
    "id" : "d5d12cbf-c696-45ac-ba88-cd0ed97bc472",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-372454953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d543f16-ab25-4c73-b2de-3a7758ce46bb",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "What's the reasoning her for only wrapping the consumer offset commit case here, not for EOS case?",
        "createdAt" : "2020-03-05T01:20:06Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "3711f037-3e77-4c51-a81e-fc51106ef318",
        "parentId" : "8d543f16-ab25-4c73-b2de-3a7758ce46bb",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "You mean exception handling? For the producer all exception handling is done within `StreamsProducer` (note that `threadProducer` above is a `StreamsProducer`, not a `KafkaProducer`)",
        "createdAt" : "2020-03-06T17:54:52Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "e38f04c4-4d91-4c39-ad84-810d24621c2a",
        "parentId" : "8d543f16-ab25-4c73-b2de-3a7758ce46bb",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "👍 ",
        "createdAt" : "2020-03-11T20:18:43Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +757,761 @@                activeTaskCreator.streamsProducerForTask(taskToCommit.getKey()).commitTransaction(taskToCommit.getValue());\n            }\n        } else {\n            try {\n                final Map<TopicPartition, OffsetAndMetadata> allOffsets = offsetsPerTask.values().stream()"
  },
  {
    "id" : "d88a77fa-188e-4153-83c7-ea32039de5d1",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375614304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0880abe6-d489-465f-b46c-b3e994d2afcc",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Having a `prepareCloseDirty` makes the calling of `closeDirty` a bit cumbersome as we always need to call `prepareCloseDirty` first. To simplify or just do a reminder, I have two suggestions:\r\n\r\n1. Internally create a task state called PREPARE_CLOSE or just a boolean like `closeDirtyPrepared` as the state check, so that closeDirty will throw illegal state if the flag is false\r\n2. Following #1, instead of throw, if we don't see the prepareClose is being called, the `closeDirty` will invoke `prepareCloseDirty` first internally.",
        "createdAt" : "2020-03-11T20:13:52Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "46a5c297-e612-4537-b38e-e94bb1f6bd03",
        "parentId" : "0880abe6-d489-465f-b46c-b3e994d2afcc",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I actually had a similar though, but was not sure if it's worth it. Would like to hear from @guozhangwang @vvcephei what they think?\r\n\r\nIf we do this, we might want to do it for \"commit\" and \"suspend\", too. For suspend() adding a state SUSPEND_PREPARED is not helpful as suspend() does different things depending on the previous state. (For commit and close an additional state would work). For consistency reasons, an internal flag might be better though.\r\n\r\nNot sure ate if calling \"prepare\" automatically would actually be correct for all cases?",
        "createdAt" : "2020-03-12T00:28:08Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "abd0e795-c4bb-4c8a-a4a4-b758f35228b3",
        "parentId" : "0880abe6-d489-465f-b46c-b3e994d2afcc",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "We can address this in a follow up PR.",
        "createdAt" : "2020-03-16T05:05:18Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "b2487326-5644-4549-8764-392415defcef",
        "parentId" : "0880abe6-d489-465f-b46c-b3e994d2afcc",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Yea, a TODO is also ok.",
        "createdAt" : "2020-03-16T17:31:51Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "7983e394-ec5a-4aa7-b3d7-787c57642185",
        "parentId" : "0880abe6-d489-465f-b46c-b3e994d2afcc",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`For the next PR`: I think we can save prepareClose (or more accurately, merge prepareClose and close together again) if we make a state diagram change that only suspended state can transit to closed state, i.e. at task-manager level whenever we want to close a task we call its `suspend` function first, which would, depending on its state, be a no-op, or flushing, or closing topology etc, and then after that the task is always in SUSPENDED state, and then we call \"commit\" if necessary, and then we call close (a minor thing is that today when the state is in SUSPENDED we would omit committing inside task, and we need to lift this restriction; and also the transition actions to transit to SUSPENDED need to rely on the clean flag, hence we need `suspend(clean-flag)`).\r\n\r\nAND we can further merge prepareSuspend and suspend as well by just making the checkpointing logic as part of post-commit instead of post-suspend, since as I mentioned above you only have three cases:\r\n\r\n1) do not need to checkpoint: if you are in CREATED.\r\n2) checkpoint written and consumed offsets: if you are in RUNNING, in which you need to commit offsets as well.\r\n3) checkpoint only store offsets: if you are in RESTORING, and in which case you do not need to commit offsets.\r\n\r\nIn fact, if we are not in the RUNNING state yet, the `consumedOffsets` as well as `recordCollector#offsets()` are always going to be empty, so it is always safe to call `stateMgr.checkpoint(checkpointableOffsets())` and not condition on the state and call `stateMgr.checkpoint(emptySet())`.\r\n\r\nAnd if we now allow committing in SUSPENDED state as part of closing (i.e. suspend -> commit -> close), similar rules apply: if we are suspending from a RESTORING state, then in `postCommit` while we ``stateMgr.checkpoint(checkpointableOffsets())` the `checkpointableOffsets` would always be empty; if we are suspending from a RUNNING state it would contain some offsets.",
        "createdAt" : "2020-03-17T00:37:48Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +160,164 @@            task.markChangelogAsCorrupted(corruptedPartitions);\n\n            task.prepareCloseDirty();\n            task.closeDirty();\n            task.revive();"
  },
  {
    "id" : "b53416fe-3ee9-4b6d-bbf7-36392d4bb1e7",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375707445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b853b75d-c5ee-4414-9e51-8301b267edcc",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "@guozhangwang For my own education, why we do `and` here instead of just checking `commitRequested`?",
        "createdAt" : "2020-03-11T20:17:03Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "0fb2207d-8d24-4e11-a9aa-e9ab634ad309",
        "parentId" : "b853b75d-c5ee-4414-9e51-8301b267edcc",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "To avoid the overhead to commit offset that are already committed, ie, the previous commit committed offset 5 and now we would commit offset 5 again.",
        "createdAt" : "2020-03-12T00:40:13Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "dd4624a0-9b69-4bf2-8e4e-bf444b2e1911",
        "parentId" : "b853b75d-c5ee-4414-9e51-8301b267edcc",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "\"as above\" :)",
        "createdAt" : "2020-03-17T00:51:31Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "2ff12adb-98b9-49f2-be8c-d9e1fb8ff7be",
        "parentId" : "b853b75d-c5ee-4414-9e51-8301b267edcc",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Within `maybeCommitActiveTasksPerUserRequested` we know that we are in state `RUNNING` and thus, no need to check what `committableOffsetsAndMetadata()` returns but we can \"blindly\" commit.",
        "createdAt" : "2020-03-17T02:58:33Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 288,
    "diffHunk" : "@@ -1,1 +731,735 @@            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n            for (final Task task : activeTaskIterable()) {\n                if (task.commitRequested() && task.commitNeeded()) {\n                    task.prepareCommit();\n                    final Map<TopicPartition, OffsetAndMetadata> offsetAndMetadata = task.committableOffsetsAndMetadata();"
  },
  {
    "id" : "a0288cc5-a391-43d9-ae9b-3a355bd8a27e",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-373212661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60c49399-fe4a-4db7-baf7-a1101b9adab3",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.",
        "createdAt" : "2020-03-11T21:47:16Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "1f75d55b-e147-48fa-be9f-ce848f471747",
        "parentId" : "60c49399-fe4a-4db7-baf7-a1101b9adab3",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "We need to call `prepareCloseClean` (as done in L196 above) _before_ we call `commitOffsetsOrTransaction` (L215 above).",
        "createdAt" : "2020-03-12T00:35:23Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +232,236 @@            final Task task = taskAndCheckpoint.getKey();\n            try {\n                task.closeClean(checkpointPerTask.get(task));\n            } catch (final RuntimeException e) {\n                final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());"
  },
  {
    "id" : "0d3847c7-1539-47a4-a92b-959357f65550",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-373212743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25d32c4c-66f2-45cc-bf4c-241f8b308238",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Similarly for `closeDirty` and `prepareCloseDirty`",
        "createdAt" : "2020-03-11T21:47:37Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "53c4b51b-568a-47b1-a543-72dcd83529b5",
        "parentId" : "25d32c4c-66f2-45cc-bf4c-241f8b308238",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Some comment as above.",
        "createdAt" : "2020-03-12T00:35:38Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +239,243 @@                // We've already recorded the exception (which is the point of clean).\n                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n                task.closeDirty();\n            } finally {\n                cleanUpTaskProducer(task, taskCloseExceptions);"
  },
  {
    "id" : "26549df4-0800-4d7c-808b-4138660cf8c6",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-374926783",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9026423-733e-45cc-9800-7f4d3fb15aca",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "We don't have unit test coverage for this exception case",
        "createdAt" : "2020-03-11T22:17:34Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "2544d7d9-2be1-4244-b6c3-2ff36adbeb50",
        "parentId" : "c9026423-733e-45cc-9800-7f4d3fb15aca",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Added test `shouldThrowWhenHandlingClosingTasksOnProducerCloseError`",
        "createdAt" : "2020-03-16T05:56:23Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +296,300 @@            try {\n                activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n            } catch (final RuntimeException e) {\n                final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n                log.error(uncleanMessage, e);"
  },
  {
    "id" : "de02ada2-5069-4270-b95e-467c25a1ce04",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-373212767",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "683c2a7e-d7dd-4f4b-ac51-85fdd9b4080d",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "We lack unit test coverage for this case",
        "createdAt" : "2020-03-11T22:18:14Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "7c978ff0-8ffc-44b9-9c3d-ab7fca7c5286",
        "parentId" : "683c2a7e-d7dd-4f4b-ac51-85fdd9b4080d",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I know...",
        "createdAt" : "2020-03-12T00:35:44Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +233,237 @@            try {\n                task.closeClean(checkpointPerTask.get(task));\n            } catch (final RuntimeException e) {\n                final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n                log.error(uncleanMessage, e);"
  },
  {
    "id" : "a6a7d123-499f-41a4-b363-4251254aea36",
    "prId" : 8246,
    "prUrl" : "https://github.com/apache/kafka/pull/8246#pullrequestreview-372534576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "222fac4f-fcdf-43ab-855e-ee4088be7cb6",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Q: This question is unrelated to your change. Why is `firstException` an atomic variable? It is a local variable and `shutdown()` is only called from `StreamThread` which should be single-threaded. \\cc @guozhangwang ",
        "createdAt" : "2020-03-10T11:36:55Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "b16ff3a0-faef-431a-96a5-7a27f27e7976",
        "parentId" : "222fac4f-fcdf-43ab-855e-ee4088be7cb6",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I think it's just more convenient than the conditional block to check if it's null.",
        "createdAt" : "2020-03-10T21:45:20Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "4ca7043f-1b45-42c5-93d8-46bc7958a850",
        "parentId" : "222fac4f-fcdf-43ab-855e-ee4088be7cb6",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Fair enough if the performance is similar.",
        "createdAt" : "2020-03-11T07:54:08Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "a757918bbbf7c4b27aa29720e540a5603f890b1e",
    "line" : 184,
    "diffHunk" : "@@ -1,1 +545,549 @@            releaseLockedUnassignedTaskDirectories();\n        } catch (final RuntimeException e) {\n            firstException.compareAndSet(null, e);\n        }\n"
  },
  {
    "id" : "fbe1e683-ef7b-4cdd-90e6-bfcf7a1407d3",
    "prId" : 8246,
    "prUrl" : "https://github.com/apache/kafka/pull/8246#pullrequestreview-372708595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bbcae5a-4a87-4c91-b9b8-1134be1627bc",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Nice! I always forget about making fields unmodifiable when they become visible to the outside.",
        "createdAt" : "2020-03-11T13:26:22Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "a757918bbbf7c4b27aa29720e540a5603f890b1e",
    "line" : 217,
    "diffHunk" : "@@ -1,1 +747,751 @@\n    Set<TaskId> lockedTaskDirectories() {\n        return Collections.unmodifiableSet(lockedTaskDirectories);\n    }\n}"
  },
  {
    "id" : "364fec86-fc1b-47dc-bb04-fcc1b8a20f11",
    "prId" : 8246,
    "prUrl" : "https://github.com/apache/kafka/pull/8246#pullrequestreview-374501318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a47328e2-6daa-4ec0-9746-45e615423fd2",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "you could avoid computing the addition twice by checking after this line if `offsetSum < 0`",
        "createdAt" : "2020-03-12T19:00:29Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "faec697d-c4d4-4688-aa61-ae92504ceb1b",
        "parentId" : "a47328e2-6daa-4ec0-9746-45e615423fd2",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "That's what I did originally, Bruno suggested changing it to use `else` -- but, maybe I misunderstood his actual proposal...I'll set it back",
        "createdAt" : "2020-03-12T21:14:35Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "46e9aa8c-0af5-4f41-b513-c16965ff97bc",
        "parentId" : "a47328e2-6daa-4ec0-9746-45e615423fd2",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "My original proposal was\r\n```\r\n            if (offset < 0L) {\r\n                if (offset == -1L) {\r\n                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\r\n                } else {\r\n                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\r\n                }\r\n            } else {\r\n                offsetSum += offset;\r\n\r\n                if (offsetSum < 0) {\r\n                    log.warn(\"Sum of changelog offsets for task {} overflowed, pinning to Long.MAX_VALUE\", id);\r\n                    return Long.MAX_VALUE;\r\n                }\r\n            }\r\n```\r\nI find this easier to read.",
        "createdAt" : "2020-03-13T14:39:43Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "144d3c53-5527-4ab8-808c-0ba5c6c75a11",
        "parentId" : "a47328e2-6daa-4ec0-9746-45e615423fd2",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I see, I think I find it easier to read without the else as it makes it clear that we are just adding the offset, except in these two potential edge cases (overflow and negative). But we still need to come to a consensus about how to handle the negative case anyway",
        "createdAt" : "2020-03-13T17:52:21Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "a757918bbbf7c4b27aa29720e540a5603f890b1e",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +470,474 @@            final long offset = changelogEntry.getValue();\n\n            offsetSum += offset;\n            if (offsetSum < 0) {\n                log.warn(\"Sum of changelog offsets for task {} overflowed, pinning to Long.MAX_VALUE\", id);"
  },
  {
    "id" : "3557d012-91df-4aad-a904-f65d934e7c90",
    "prId" : 8327,
    "prUrl" : "https://github.com/apache/kafka/pull/8327#pullrequestreview-379069920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6b866c4-b00c-4703-8f35-fb725cb1b18f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think we can consolidate the three blocks 1) prepareClose / committableOffsetsAndMetadata and 2) commitOffsetsOrTransaction, and 3) closeClean which are all wrapping runtime exceptions and updating exception / dirty tasks map into a single one.\r\n\r\nThis can be done in another PR.",
        "createdAt" : "2020-03-22T23:44:09Z",
        "updatedAt" : "2020-03-24T03:46:30Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3de186888785fdd16bc40ad29cd7f94de997b336",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +238,242 @@            }\n\n            try {\n                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n"
  },
  {
    "id" : "e6e200b6-e07f-4951-9afd-ac5fd7f720e8",
    "prId" : 8327,
    "prUrl" : "https://github.com/apache/kafka/pull/8327#pullrequestreview-379943085",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c3ab746-8262-4080-8996-a49f6da598af",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "A `CommitFaileException` should result in a `TaskMigratedException` -- should we only catch this one? Catching a generic `RuntimeException` does seem to be a little coarse grained?\r\n\r\nAlso, I am wondering why this case is not handled by the caller, ie, the exception should finally pop out of `Consumer#poll()` and the thread would handle this case and close all tasks?\r\n\r\nAlso, why do we only handle it at this place explicitly? We call `commitOffsetsOrTransaction` on multiple places and never handle it, but just let upper layer take care?",
        "createdAt" : "2020-03-23T19:24:52Z",
        "updatedAt" : "2020-03-24T03:46:30Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "f1263697-5545-4119-805c-08800f1003d0",
        "parentId" : "4c3ab746-8262-4080-8996-a49f6da598af",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "The reason is because here we are removing the tasks before the commit block, so effectively this would be the last time we are going to see these tasks before we do a lostAll. I checked other `commitOffsetsOrTransaction` cases which are ok because there would be error handling in place or it is not inside a closing place. And if the current thread crashes for some non-recoverable exception during `commit`, these half-closed tasks are already polluting the rocksDBMetricsRecordingTrigger which later will bring down other stream threads as well. \r\n",
        "createdAt" : "2020-03-23T21:12:19Z",
        "updatedAt" : "2020-03-24T03:46:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "671c8f53-5ec9-49b1-9845-a5d391b76992",
        "parentId" : "4c3ab746-8262-4080-8996-a49f6da598af",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ack. Thanks for clarifying.",
        "createdAt" : "2020-03-24T00:18:15Z",
        "updatedAt" : "2020-03-24T03:46:30Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "1089d908-b531-4dc2-9db3-28d38376412f",
        "parentId" : "4c3ab746-8262-4080-8996-a49f6da598af",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "One more follow up question: With incremental rebalancing and eos-beta, we might commit RUNNING tasks (that were never suspended etc) and task that got assigned to another thread/instance within the same transaction. Hence, if committing fails, we would also need to rollback those RUNNING tasks (that we track via `additionalTasksForCommitting`) -- is this rolling back ensured by upper levels or do we need to do something special here, too?",
        "createdAt" : "2020-03-24T00:25:25Z",
        "updatedAt" : "2020-03-24T03:46:30Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "2d351e7a-7f71-43b3-b167-e9cf53a7397d",
        "parentId" : "4c3ab746-8262-4080-8996-a49f6da598af",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "My understanding is that the tasks in `additionalTasksForCommitting` are not wiped out during the task iterations, so if we through all the way up to the thread level, we should still have one last chance to call `handleLostAll` to clean them up.",
        "createdAt" : "2020-03-24T01:25:24Z",
        "updatedAt" : "2020-03-24T03:46:30Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "b2928737-4d3f-458f-8031-12ed1eedba28",
        "parentId" : "4c3ab746-8262-4080-8996-a49f6da598af",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I agree that we would not loose the reference to those tasks -- just wondering if the cleanup would be done correctly? (I believe yes, but worth to verify)",
        "createdAt" : "2020-03-24T01:33:35Z",
        "updatedAt" : "2020-03-24T03:46:30Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3de186888785fdd16bc40ad29cd7f94de997b336",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +244,248 @@                    task.postCommit();\n                }\n            } catch (final RuntimeException e) {\n                log.error(\"Failed to commit tasks that are \" +\n                    \"prepared to close clean, will close them as dirty instead\", e);"
  },
  {
    "id" : "17e0a711-cf19-4007-ae56-c32637878e42",
    "prId" : 8331,
    "prUrl" : "https://github.com/apache/kafka/pull/8331#pullrequestreview-380549304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e4aa24a-0d14-4516-baa4-afd50ff7d74c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "See my other comment above: with eos-beta I think this should not be triggered by ProducerFencedException ever?",
        "createdAt" : "2020-03-24T18:02:20Z",
        "updatedAt" : "2020-03-30T21:41:00Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c01468233fc24b2deb017a7bdf80dc2c644d92fc",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +479,483 @@\n        if (processingMode == EXACTLY_ONCE_BETA) {\n            activeTaskCreator.reInitializeThreadProducer();\n        }\n    }"
  },
  {
    "id" : "a7a91316-98eb-434b-9488-27b942560ba4",
    "prId" : 8433,
    "prUrl" : "https://github.com/apache/kafka/pull/8433#pullrequestreview-388597167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05313768-53f6-41c6-999c-d831cbf9f98a",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It seems like this would have initially appeared unnecessary because we already removed it on L230 when we first put it into the dirtyTasks map. But what appears to have happened is that we actually got an exception during commit, and added some more tasks in L252-L253, which were not removed from the task map.\r\n\r\nWhat is the overall algorithm here? Offhand, it seems like we should only remove the task after we know it is closed, which means we should delete the `iterator.remove();` on L230 and keep the line you've added here, along with adding a similar line between L264-265 (after closeClean). I'm also wondering if we should really do closeDirty on L271, or just add it to dirtyTasks. If we keep it there, then we also need to remove it from the task map at that location.",
        "createdAt" : "2020-04-06T20:30:39Z",
        "updatedAt" : "2020-04-06T22:24:39Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "e0b97361-25dd-4622-98ed-cecead279d65",
        "parentId" : "05313768-53f6-41c6-999c-d831cbf9f98a",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Thanks for the question @vvcephei , the overall algorithm is like we are adding more tasks into the dirty task list to cleanup on L252-253, as the whole pre-commit + commit section fails. I could see that doing a separate removal here is misleading, so I will try to move this logic up to the catch block where we actually augment the dirtyTasks map.",
        "createdAt" : "2020-04-06T20:36:10Z",
        "updatedAt" : "2020-04-06T22:24:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5c7843c39c57f235009c1098e088f854ace13cf",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +277,281 @@            task.closeDirty();\n            cleanUpTaskProducer(task, taskCloseExceptions);\n            tasks.remove(task.id());\n        }\n"
  },
  {
    "id" : "58b0df41-b4b7-4b1c-bc95-aeb6f45e72db",
    "prId" : 8440,
    "prUrl" : "https://github.com/apache/kafka/pull/8440#pullrequestreview-391001414",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f81f476a-38ca-41ec-9c27-94b5661cb705",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is an actually bug fix: `consumedOffsetsAndMetadataPerTask` could be empty, if only standby tasks (but no active tasks) are assigned to a thread.",
        "createdAt" : "2020-04-07T20:55:26Z",
        "updatedAt" : "2020-04-14T23:37:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "95c626ed-fb67-4c85-8578-b14ae8bc4909",
        "parentId" : "f81f476a-38ca-41ec-9c27-94b5661cb705",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could you elaborate more on why committing an empty map will fail?",
        "createdAt" : "2020-04-07T22:13:28Z",
        "updatedAt" : "2020-04-14T23:37:49Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "5b049ed8-ac1d-4970-a577-cbd337f45931",
        "parentId" : "f81f476a-38ca-41ec-9c27-94b5661cb705",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "If we only have `StandbyTasks` assigned, the `RecordCollector` would not be initialized and thus the KafkaProducer would not initialize transactions and hence the offset commit would fail as we cannot begin a new transaction.",
        "createdAt" : "2020-04-09T17:43:43Z",
        "updatedAt" : "2020-04-14T23:37:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "15e0b98ef384c15022ee76e6ca88b325567e31cb",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +760,764 @@            if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n            }\n\n            for (final Task task : tasks) {"
  },
  {
    "id" : "474079de-b1fa-4f32-8784-54d841830a4e",
    "prId" : 8463,
    "prUrl" : "https://github.com/apache/kafka/pull/8463#pullrequestreview-391887808",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c41081ae-774b-490c-809a-4ce94a11bc0c",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "This was the main bug, we try to batch commit active tasks above and deal with failure by adding to this `dirtyTasks` set. But we didn't actually do all of the required cleanup for a removed task in this loop here, specifically we did not ever call `cleanupTask` on tasks that were added to `dirtyTasks` from the `additionalTasksForCommitting` set on line 250",
        "createdAt" : "2020-04-10T23:29:57Z",
        "updatedAt" : "2020-04-10T23:32:52Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "427fc27f-1997-4973-803d-421d70e48f3a",
        "parentId" : "c41081ae-774b-490c-809a-4ce94a11bc0c",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I think we also didn't always call `prepareCloseDirty` before calling `closeDirty`. That was not the source of this particular bug, but I tried to consolidate all the cleanup tasks a bit to make sure they end up being called at all times, in the correct order",
        "createdAt" : "2020-04-10T23:31:24Z",
        "updatedAt" : "2020-04-10T23:32:52Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9f1d0444-8c66-4d81-934d-b2aced7d911b",
        "parentId" : "c41081ae-774b-490c-809a-4ce94a11bc0c",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "LGTM.",
        "createdAt" : "2020-04-12T19:26:05Z",
        "updatedAt" : "2020-04-12T19:31:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8b15773b7d1eb96e8804a90e29cb27d390f6b3",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +274,278 @@\n        for (final Task task : dirtyTasks) {\n            closeTaskDirty(task);\n            cleanUpTaskProducer(task, taskCloseExceptions);\n            tasks.remove(task.id());"
  },
  {
    "id" : "219ae9db-dd35-4e15-ab2b-f23a2f04c287",
    "prId" : 8463,
    "prUrl" : "https://github.com/apache/kafka/pull/8463#pullrequestreview-392470039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20d144db-df9a-4575-b45c-162b97af35c1",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Could we move `cleanUpTaskProducer` into `completeTaskCloseClean` and `closeTaskDirty` respectively?",
        "createdAt" : "2020-04-12T19:27:50Z",
        "updatedAt" : "2020-04-12T19:31:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "2c78c814-19cc-4737-b80e-4c482dcf4f37",
        "parentId" : "20d144db-df9a-4575-b45c-162b97af35c1",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I think yes, ultimately we can (and should), but the task producer cleanup is handled differently throughout this class and this would be a larger refactoring. I know there's some `TaskManager` cleanup planned or in progress already so  I wanted to hold off on anything more than the trivial refactoring done here",
        "createdAt" : "2020-04-13T21:41:20Z",
        "updatedAt" : "2020-04-13T21:41:20Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "cf366e24-7784-4532-b50b-6b0e92069101",
        "parentId" : "20d144db-df9a-4575-b45c-162b97af35c1",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Cool, sounds good.",
        "createdAt" : "2020-04-13T22:00:36Z",
        "updatedAt" : "2020-04-13T22:00:37Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8b15773b7d1eb96e8804a90e29cb27d390f6b3",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +261,265 @@            try {\n                completeTaskCloseClean(task, checkpoint);\n                cleanUpTaskProducer(task, taskCloseExceptions);\n                tasks.remove(task.id());\n            } catch (final RuntimeException e) {"
  },
  {
    "id" : "157ceed5-ae96-4f85-b1f3-1c8d65ce24b7",
    "prId" : 8463,
    "prUrl" : "https://github.com/apache/kafka/pull/8463#pullrequestreview-392461113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7cc95937-71a5-494e-a3d1-a0145f35ca67",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Should we also close producers for the lost tasks as well?",
        "createdAt" : "2020-04-12T19:29:54Z",
        "updatedAt" : "2020-04-12T19:31:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "6aed7528-634e-4766-a3b0-11b0fd6eac31",
        "parentId" : "7cc95937-71a5-494e-a3d1-a0145f35ca67",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "We do, right below this, but rather than delegate to `#cleanupTaskProducer`  we just call `activeTaskCreator.closeAndRemoveTaskProducerIfNeeded` directly (which is what `#cleanupTaskProducer` ultimately does). I think we should consolidate these calls eventually, but not necessarily in this hotfix",
        "createdAt" : "2020-04-13T21:43:30Z",
        "updatedAt" : "2020-04-13T21:43:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8b15773b7d1eb96e8804a90e29cb27d390f6b3",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +464,468 @@            // standby tasks while we rejoin.\n            if (task.isActive()) {\n                closeTaskDirty(task);\n                iterator.remove();\n                try {"
  },
  {
    "id" : "83c73b4c-96af-4f80-bde4-1c433e5c0d6f",
    "prId" : 8463,
    "prUrl" : "https://github.com/apache/kafka/pull/8463#pullrequestreview-392461746",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad5471b5-068c-4f95-87fd-a76b5a925f0c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Ditto, should we close producers as well?",
        "createdAt" : "2020-04-12T19:30:32Z",
        "updatedAt" : "2020-04-12T19:31:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "dbb8d6e7-0fdd-422a-89af-14bab1ae94bd",
        "parentId" : "ad5471b5-068c-4f95-87fd-a76b5a925f0c",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Same here, we do close it eventually a few loops after this one",
        "createdAt" : "2020-04-13T21:44:37Z",
        "updatedAt" : "2020-04-13T21:44:37Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8b15773b7d1eb96e8804a90e29cb27d390f6b3",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +631,635 @@                } catch (final TaskMigratedException e) {\n                    // just ignore the exception as it doesn't matter during shutdown\n                    closeTaskDirty(task);\n                } catch (final RuntimeException e) {\n                    firstException.compareAndSet(null, e);"
  },
  {
    "id" : "ac23fd2c-b014-4b40-bbdc-e359c36f63be",
    "prId" : 8632,
    "prUrl" : "https://github.com/apache/kafka/pull/8632#pullrequestreview-408474487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "709f029f-f5e8-4fae-a397-5851717d9a49",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "FWIW, I'd rather just inline it and avoid maintaining APIs that are just for tests in our production code.",
        "createdAt" : "2020-05-08T19:28:15Z",
        "updatedAt" : "2020-05-08T19:29:18Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "e7217260-06aa-4ff5-bf2d-1dba1ceea336",
        "parentId" : "709f029f-f5e8-4fae-a397-5851717d9a49",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I see your point and thought about it, but I feel a little bit reluctant to remove it for the reason that it is indeed a handy call to make for testing purpose :)",
        "createdAt" : "2020-05-08T19:33:26Z",
        "updatedAt" : "2020-05-08T19:33:26Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3a532c57d3e538ea2b108237a8742960eccf674",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +737,741 @@        return commit(tasks.values());\n    }\n\n    /**\n     * @throws TaskMigratedException if committing offsets failed (non-EOS)"
  }
]