[
  {
    "id" : "a8c0dc49-a822-4bcb-af72-0cab466224da",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-83592281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "At this point, we hit a race condition for the restore case of a regular changelog topic: `restorer.hasCompleted(pos, endOffset)` might return `true` even if some record got appended to the changelog iff (by chance), the last `poll()` batch of records ends at \"endOffsets - 1\".\r\n\r\nWe should add an additional check here, and get the `endOffset` of the partition again, and check if `restorer.hasCompleted(pos, newEndOffset) == true` -- if it's false, we should also throw `TaskMigratedException`.",
        "createdAt" : "2017-12-12T21:28:39Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "6eaa4d50-01d2-4509-ae6e-b877598920fd",
        "parentId" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Maybe I don't recall all the details of the offline conversation, or I'm missing something else, but by fetching the `endOffset` again and doing another `restorer.hasCompleted(pos, newEndOffset)` call won't this cause issues for `KTable` with a source topic?\r\n\r\nEDIT: unless we indicate this for this is a source log topic vs regular changelog topic, In which case I think that should go into a separate Jira/PR so as not to hold this one up.",
        "createdAt" : "2017-12-14T17:07:08Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "0570f955-9f90-4b9e-a5eb-b9b2cc58526e",
        "parentId" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think @mjsax 's comment was that we can add another `endOffset` call and check if the returned value is still the same, this will help reduce the likelihood of race condition but not eliminate it: however, we need to distinguish the case for source KTable such that in the case when `offset limit ` is specified, we do not need this additional check.",
        "createdAt" : "2017-12-14T17:45:01Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d533dd2d-84f3-4690-96cc-5752cd79e14f",
        "parentId" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "ack",
        "createdAt" : "2017-12-14T17:59:58Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +252,256 @@        restorer.setRestoredOffset(pos);\n        if (restorer.hasCompleted(pos, endOffset)) {\n            if (pos > endOffset) {\n                throw new TaskMigratedException(task, topicPartition, endOffset, pos);\n            }"
  },
  {
    "id" : "8de51519-7b8a-4a00-a6f5-0f5f43acdc0f",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-83328117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1bd9850-2ba1-48b4-9f95-34561858edff",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good catch！",
        "createdAt" : "2017-12-13T21:39:05Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +252,256 @@        restorer.setRestoredOffset(pos);\n        if (restorer.hasCompleted(pos, endOffset)) {\n            if (pos > endOffset) {\n                throw new TaskMigratedException(task, topicPartition, endOffset, pos);\n            }"
  },
  {
    "id" : "d6a4df80-73bf-4659-8097-94d77cd7b8fb",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-83668920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: remove empty line\r\n\r\nAs discussed with @guozhangwang offline, I think passing in `nextPosition` is not correct. We should pass in the offset of the latest restored offset, but `nextPosition` is the offset of the first not restored offset.\r\n\r\nQuestion is, if a `nextPosition - 1` would be desirable or not because this might be a commit marker position and not an actual record position. ",
        "createdAt" : "2017-12-14T18:34:57Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "55fc85da-a7eb-429f-8223-8a94166d8c3a",
        "parentId" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Correct, this will be handled in a follow-up PR.  I synced off-line with @guozhangwang last night about this.",
        "createdAt" : "2017-12-14T19:11:01Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "c44556ea-18cc-4ee9-9f58-051afc38b248",
        "parentId" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ack. Can you create a JIRA for it?",
        "createdAt" : "2017-12-14T21:40:31Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "7135b4ba-3efc-4fa9-9f1a-741ea70c2102",
        "parentId" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Done - https://issues.apache.org/jira/browse/KAFKA-6367",
        "createdAt" : "2017-12-14T22:41:17Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +297,301 @@            restorer.restore(restoreRecords);\n            restorer.restoreBatchCompleted(nextPosition, records.size());\n\n        }\n"
  },
  {
    "id" : "08297b25-d609-468d-92c3-f35070a71e5c",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-84911333",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2751300b-8d99-4ab5-9457-62ef68e7e516",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I've thought about this some more, and given the previous comments (https://github.com/apache/kafka/pull/4300#discussion_r156816065) that `processNext` was correct but susceptible to a race condition where we don't restore all records, IMHO we just need to add an additional check we did not restore all records in the list.   If you insist I can put in the `Math.min(restorer.OffestLimit(),restoreConsumer...)`",
        "createdAt" : "2017-12-19T18:32:19Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "31f7954c-cfc3-42bf-9df1-462180b3600f",
        "parentId" : "2751300b-8d99-4ab5-9457-62ef68e7e516",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I think this logic is correct now.`Math.min(...)` should do the exact same computation if I got it write. Both approaches have advantages: yours is more descriptive and thus explains itself better. The other one is more concise code allowing us to remove some local vars (`numberRecrods`, `numberRestored`), but harder to understand whats going if if you are not familiar with the code. Not sure which one is better.\r\n\r\n@guozhangwang @dguy WDYT? ",
        "createdAt" : "2017-12-20T22:01:00Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +290,294 @@        // otherwise if we did not fully restore to that point we need to set nextPosition\n        // to the position of the restoreConsumer and we'll cause a TaskMigratedException exception\n        if (nextPosition == -1 || (restorer.offsetLimit() == Long.MAX_VALUE && numberRecords != numberRestored)) {\n            nextPosition = restoreConsumer.position(restorer.partition());\n        }"
  },
  {
    "id" : "f2842725-492d-4ae9-8f65-63502596ef60",
    "prId" : 5013,
    "prUrl" : "https://github.com/apache/kafka/pull/5013#pullrequestreview-121996537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37e40195-134d-4f2b-abba-6dad93a28dc2",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "we can also remove the entry in `updatedEndOffsets` if this is true.",
        "createdAt" : "2018-05-22T01:35:00Z",
        "updatedAt" : "2018-06-04T21:37:31Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e742d310817ba306c0a217666b983755418549e1",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +88,92 @@                final long pos = processNext(records.records(partition), restorer, updatedEndOffsets.get(partition));\n                restorer.setRestoredOffset(pos);\n                if (restorer.hasCompleted(pos, updatedEndOffsets.get(partition))) {\n                    restorer.restoreDone();\n                    updatedEndOffsets.remove(partition);"
  },
  {
    "id" : "f66aeb3a-5139-4446-bd54-629bb3011079",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9351f4a1-2da1-4265-b9d0-138c5ed181c8",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is just centralizing the structs a bit: I only store the map from topic partitions to restorers in one place now, namely  the `stateRestorers`, all other structs are simplified to set of topic partitions only, and they can retrieve the corresponding restorer from the first map always.",
        "createdAt" : "2018-07-27T21:36:36Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +49,53 @@    private final Map<String, List<PartitionInfo>> partitionInfo = new HashMap<>();\n    private final Map<TopicPartition, StateRestorer> stateRestorers = new HashMap<>();\n    private final Set<TopicPartition> needsRestoring = new HashSet<>();\n    private final Set<TopicPartition> needsInitializing = new HashSet<>();\n    private final Set<TopicPartition> completedRestorers = new HashSet<>();"
  },
  {
    "id" : "fb99106e-46b1-4fb4-bdd6-6ed99391daf3",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8fb8aed6-63fc-4899-b92c-8a8caacac461",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I've added this set to replace the logic of `completed = total (stateRestorers) - needsRestoring`, since it is no longer true: if a partition is not in needsRestoring, it may because of the re-initialization.",
        "createdAt" : "2018-07-27T21:37:31Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +51,55 @@    private final Set<TopicPartition> needsRestoring = new HashSet<>();\n    private final Set<TopicPartition> needsInitializing = new HashSet<>();\n    private final Set<TopicPartition> completedRestorers = new HashSet<>();\n    private final Duration pollTime;\n"
  },
  {
    "id" : "920eeaf8-7197-4d55-b9f8-8be3de09411f",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "000f3353-2743-4b2c-bea6-369c20ac7677",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Okay this is a bit hack but is the key of the fix: I've decided to keep the restorer in the map and note replacing it with the new restorer, since from the ProcessorStateManager the checkpoint offset cannot be inferred. This is just to walk around the code hierarchy of Restorer and the ProcessorStateManager.",
        "createdAt" : "2018-07-27T21:39:15Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +66,70 @@    @Override\n    public void register(final StateRestorer restorer) {\n        if (!stateRestorers.containsKey(restorer.partition())) {\n            restorer.setUserRestoreListener(userStateRestoreListener);\n            stateRestorers.put(restorer.partition(), restorer);"
  },
  {
    "id" : "f2610047-7842-491f-b010-a43a4c2da9d9",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5709da88-f673-4b63-8911-60f45ef0c5ec",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is the other fix: whenever we initialize, inside `reinitializeStateStoresForPartitions` the store.init will be called which will call storechangelogger.register() again to put it into the `needsInitializing` set.",
        "createdAt" : "2018-07-27T21:40:12Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +106,110 @@                log.info(\"Reinitializing StreamTask {} for changelog {}\", task, partition);\n\n                needsInitializing.remove(partition);\n                needsRestoring.remove(partition);\n"
  },
  {
    "id" : "a557089c-1098-4147-9658-096bf4762b70",
    "prId" : 5767,
    "prUrl" : "https://github.com/apache/kafka/pull/5767#pullrequestreview-176440736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88efebdc-c094-474d-a4b9-39a045cfc973",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I have some questions just for my education.\r\n\r\nI think I get it what the issue is.  The `ProcessorStateManager#register` creates a new `StateRestorer` each time, and it gets passed in here, so without reusing the `existingRestorer` restoring never completes. Is that correct?  My question is `StateRestorer` should contain the same state even if a new instance, so how does using the existing restorer solve the issue? From what I can see the state should be the same between the new and existing restorer.  Since the fix seems to be the same as #5915, I'll ask my question here.",
        "createdAt" : "2018-11-15T15:44:35Z",
        "updatedAt" : "2018-11-15T15:49:18Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "a4c72857-acb7-4f9a-b7de-51f13af69ee9",
        "parentId" : "88efebdc-c094-474d-a4b9-39a045cfc973",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : ">  Is that correct?\r\n\r\nYes.\r\n\r\nFor EOS, on startup, we read the checkpoint file, store the information in the restorer, and delete the checkpoint file (we only write one on clean shutdown). The magic happens in L173/180 `if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) {` and L204 `restorer.setCheckpointOffset(restoreConsumer.position(restoringPartition));`. \r\n\r\nIf there is no checkpoint store in the restorer, we seekToBeginning and mark as `needsPositionUpdate`. Later, we wipe out the store and put the current beginningOffset as checkpoint to the restorer. Thus, when we re-register, we need to preserve this checkpoint information. The newly created `StateRestored` would get initialized by looking if there is checkpoint file on disk and would be initialized with \"no checkpoint\".\r\n\r\nIt's a little hacky... Maybe we can revisit this design at some point and make it cleaner.\r\n\r\nMakes sense?",
        "createdAt" : "2018-11-17T02:40:48Z",
        "updatedAt" : "2018-11-17T02:42:15Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "87c9e33a-0e1d-4f22-92e6-c20c6263b225",
        "parentId" : "88efebdc-c094-474d-a4b9-39a045cfc973",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Yes, thanks for the clarification.",
        "createdAt" : "2018-11-19T18:41:03Z",
        "updatedAt" : "2018-11-19T18:41:03Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "5256317b9b3d01ba50c6287c6b88216dcc588498",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +68,72 @@            needsInitializing.put(restorer.partition(), restorer);\n        } else {\n            needsInitializing.put(restorer.partition(), existingRestorer);\n        }\n    }"
  },
  {
    "id" : "d6f51cc7-3203-41e7-99e2-b65f4d70b94b",
    "prId" : 6113,
    "prUrl" : "https://github.com/apache/kafka/pull/6113#pullrequestreview-200949375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1355236f-93ea-4ca5-abae-aa44242bd7fe",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is @linyli001 's fix.",
        "createdAt" : "2019-01-10T23:55:13Z",
        "updatedAt" : "2019-02-22T01:03:34Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f2bce32b-02da-41db-8329-8355feaf74ad",
        "parentId" : "1355236f-93ea-4ca5-abae-aa44242bd7fe",
        "authorId" : "7ec9f39d-9147-4743-a257-c07990c83519",
        "body" : "added some hack code to listeners and transformer init func before next version release..",
        "createdAt" : "2019-02-07T07:36:25Z",
        "updatedAt" : "2019-02-22T01:03:34Z",
        "lastEditedBy" : "7ec9f39d-9147-4743-a257-c07990c83519",
        "tags" : [
        ]
      }
    ],
    "commit" : "912a45642a92f4298d203d5c4eda6a1678208c05",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +278,282 @@        endOffsets.clear();\n        needsInitializing.clear();\n        completedRestorers.clear();\n    }\n"
  },
  {
    "id" : "d30d7f20-f46a-4384-b564-e61493ccf433",
    "prId" : 7961,
    "prUrl" : "https://github.com/apache/kafka/pull/7961#pullrequestreview-344868994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a3b3787-3e56-43fb-973f-3184d61e1136",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "I noticed some maps are changed to ConcurrentHashMap.\r\nMay I ask what was the selection criterion for the change ?\r\n\r\nthanks",
        "createdAt" : "2020-01-16T23:56:05Z",
        "updatedAt" : "2020-01-16T23:56:06Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "852b4390-0915-4302-821f-5e8d83b9a652",
        "parentId" : "9a3b3787-3e56-43fb-973f-3184d61e1136",
        "authorId" : "478a572c-b267-486a-b845-4847ccf71f62",
        "body" : "This is for safe iteration from the thread calling the lag fetch API",
        "createdAt" : "2020-01-17T22:11:12Z",
        "updatedAt" : "2020-01-17T22:11:13Z",
        "lastEditedBy" : "478a572c-b267-486a-b845-4847ccf71f62",
        "tags" : [
        ]
      }
    ],
    "commit" : "026b9dd5f36a78839a5e628cd51d2e98f91313e9",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +48,52 @@    private final Map<TopicPartition, Long> restoreToOffsets = new HashMap<>();\n    private final Map<String, List<PartitionInfo>> partitionInfo = new HashMap<>();\n    private final Map<TopicPartition, StateRestorer> stateRestorers = new ConcurrentHashMap<>();\n    private final Set<TopicPartition> needsRestoring = new HashSet<>();\n    private final Set<TopicPartition> needsInitializing = new HashSet<>();"
  },
  {
    "id" : "78042856-98dd-436f-9da6-b129f29bbaf9",
    "prId" : 8047,
    "prUrl" : "https://github.com/apache/kafka/pull/8047#pullrequestreview-354028896",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de17dcb8-4ea0-459f-bcb0-b6f77dccbd9e",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "If user set this value to infinity we should still have a non-inf value to take care of the manual commit.",
        "createdAt" : "2020-02-05T20:49:06Z",
        "updatedAt" : "2020-02-06T23:27:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "26d868a440a6d83372195a67c8a4e68ef5b4ece8",
    "line" : 110,
    "diffHunk" : "@@ -1,1 +218,222 @@        this.pollTime = Duration.ofMillis(config.getLong(StreamsConfig.POLL_MS_CONFIG));\n        this.updateOffsetIntervalMs = config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG) == Long.MAX_VALUE ?\n            DEFAULT_OFFSET_UPDATE_MS : config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG);\n        this.lastUpdateOffsetTime = 0L;\n"
  },
  {
    "id" : "6f8c8468-465c-46c0-a25e-19e8411d416a",
    "prId" : 8047,
    "prUrl" : "https://github.com/apache/kafka/pull/8047#pullrequestreview-354840570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f39fa06b-f88e-4135-8185-d57b74ab23aa",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "If there's nothing to be updated or timed out, we do not update the timer.",
        "createdAt" : "2020-02-05T20:51:16Z",
        "updatedAt" : "2020-02-06T23:27:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "692e6ef7-95df-4515-9083-3fd5be1ab2e2",
        "parentId" : "f39fa06b-f88e-4135-8185-d57b74ab23aa",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This would be less mysterious if this method were inlined into `updateLimitOffsets`. Right now, it's not terribly clear why it's ok to set the \"last update offset time\" in a method that doesn't update the offsets.",
        "createdAt" : "2020-02-06T20:17:00Z",
        "updatedAt" : "2020-02-06T23:27:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "0f2ef2a1-e9e0-461e-a497-a1d871659b13",
        "parentId" : "f39fa06b-f88e-4135-8185-d57b74ab23aa",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This function is triggered by another caller besides `updateLimitOffsets`, plus it is a bit close to the NPathComplexity threshold.. ",
        "createdAt" : "2020-02-06T23:17:28Z",
        "updatedAt" : "2020-02-06T23:27:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "26d868a440a6d83372195a67c8a4e68ef5b4ece8",
    "line" : 209,
    "diffHunk" : "@@ -1,1 +537,541 @@        }\n\n        lastUpdateOffsetTime = time.milliseconds();\n\n        return committedOffsets;"
  },
  {
    "id" : "8e7824e6-6f7f-476f-ba23-29ff50ea8463",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-355299590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9768635e-d26a-4e72-854c-a19f565fb03b",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Just wondering, why is it ok to get `InvalidOffsetException` during `restore`/`poll` but not here? When might this get thrown from `#position`? ditto for in `prepareChangelogs` down below",
        "createdAt" : "2020-02-07T01:30:57Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "6b997215-04fe-4620-b558-143a26257381",
        "parentId" : "9768635e-d26a-4e72-854c-a19f565fb03b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`consumer.poll` throwing `InvalidOffsetException` should be handled as TaskCorrupted; `consumer.position` throwing `InvalidOffsetException` should not happen under normal scenarios, when it happens it indicates a bug and hence we do not need to special handle it.",
        "createdAt" : "2020-02-07T16:59:57Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +259,263 @@                return false;\n            } catch (final KafkaException e) {\n                // this also includes InvalidOffsetException, which should not happen under normal\n                // execution, hence it is also okay to wrap it as fatal StreamsException\n                throw new StreamsException(\"Restore consumer get unexpected error trying to get the position \" +"
  },
  {
    "id" : "d8d34768-c5c5-4f2a-b650-71e01808798c",
    "prId" : 8105,
    "prUrl" : "https://github.com/apache/kafka/pull/8105#pullrequestreview-357953488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73e36ed5-b817-402f-b5bd-bf25a3379345",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Just some side cleanup :)",
        "createdAt" : "2020-02-13T03:12:14Z",
        "updatedAt" : "2020-02-21T20:06:35Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "6e92eda7-2957-4588-8c2f-d43fb588315a",
        "parentId" : "73e36ed5-b817-402f-b5bd-bf25a3379345",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "This is nice!",
        "createdAt" : "2020-02-13T04:53:27Z",
        "updatedAt" : "2020-02-21T20:06:35Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffe90a4254278614feeb110dbd3bde4687e82676",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +176,180 @@    }\n\n    private final static long DEFAULT_OFFSET_UPDATE_MS = Duration.ofMinutes(5L).toMillis();\n\n    private ChangelogReaderState state;"
  },
  {
    "id" : "0fc998f2-15b9-477a-bbc9-0530209244e7",
    "prId" : 8235,
    "prUrl" : "https://github.com/apache/kafka/pull/8235#pullrequestreview-371545424",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96a412c8-ca8d-45ea-9edb-23e1e8676ac0",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.",
        "createdAt" : "2020-03-09T19:33:54Z",
        "updatedAt" : "2020-03-09T21:35:06Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "b6d3e85a-be55-4c4c-9968-3852a4e918ac",
        "parentId" : "96a412c8-ca8d-45ea-9edb-23e1e8676ac0",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "If `changelogsWithLimitOffsets` is empty then the for loop would be a no-op and the `updateLimitOffsetsForStandbyChangelogs` would not be called.",
        "createdAt" : "2020-03-09T21:37:31Z",
        "updatedAt" : "2020-03-09T21:37:38Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "00390129ed6c84be7b74018e60f4e6f764efeab9",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +465,469 @@                .filter(entry -> entry.getValue().stateManager.taskType() == Task.TaskType.STANDBY &&\n                    entry.getValue().stateManager.changelogAsSource(entry.getKey()))\n                .map(Map.Entry::getKey).collect(Collectors.toSet());\n\n            for (final TopicPartition partition : changelogsWithLimitOffsets) {"
  },
  {
    "id" : "9f04031b-df52-46f7-8700-f6a987d8b08c",
    "prId" : 8248,
    "prUrl" : "https://github.com/apache/kafka/pull/8248#pullrequestreview-420331881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Alright, the situation here is that we need to make sure we toggle bulk loading off for any active restoring tasks that convert to standby. Rather than try and force a direct call to toggleForBulkLoading on the store itself I figured we should just call onRestoreEnd. Technically, restoration is ending. It just happens to be due to type transition, rather than restore completion.\r\n\r\nI figured this might be relevant for users of custom stores, which might do something similar to bulk loading that they wish to turn off for standbys. But since this is only relevant to the underlying store, and doesn't mean we have actually finished restoring a task, we should only call the specific store's listener -- and not the user registered global listener.\r\nWDYT?",
        "createdAt" : "2020-05-27T04:02:26Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "3bf24ab5-f746-4156-bba2-6088d81598fc",
        "parentId" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I think this makes sense. The restore listener / bulk loading interaction is a bit wacky, but it seems reasonable to just work around it for now.\r\n\r\nJust to play devil's advocate briefly, though, is it not true for _all_ listeners that the restore has ended, for exactly the reason you cited above?",
        "createdAt" : "2020-05-27T17:40:36Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "693e5ced-bc14-421b-aad3-f9aa0a542c21",
        "parentId" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I assume you're referring to the user registered global listener? I was trying to imagine what users actually might be using this for, and figured a major reason was to alert when a store is ready for IQ. Obviously, the store is not in fact ready for IQ in this case. I assume the worst that could happen is they'll just get an exception saying the store is not in fact ready if they do try to query it, but it still seems likely to cause confusion. \r\n\r\nIf you're also wondering about the seemingly arbitrary distinction made between the store-specific listener and the global one, it seems like the intent of the store-specific listener is to take action on a particular store as it transitions between restoring and not. The store-specific listener has a reference to the actual store, and can for example toggle it for bulk loading.\r\n\r\nBut IIUC the global listener does not have a reference to any actual stores and thus it's purpose seems more for alerting on the completion of restoration rather than taking some action on the store as restoration begins/ends. \r\n\r\nRestoration completion =/= restoration ending, but unfortunately we just have the one callback",
        "createdAt" : "2020-05-27T22:37:18Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "eb64f735-77f0-4698-a254-1030c9ffaa6c",
        "parentId" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Yeah, that last sentence is what I was thinking would make sense to just call all the listeners, not just the inner ones. If you implemented the listener so that you could log or collect metrics to watch the restore process, it seems like it would be strange just to see the restoration disappear, and then a new restoration start (for the post-recycled task), without the prior one ever having \"ended\". It just seems natural to see an end for every start, even if the \"end\" doesn't mean \"completed\". But I can see how that could also be confusing.\r\n\r\nI'm not totally sure what the best call here is, so maybe we should just defer to your judgement, since you're the closest to this code right now.",
        "createdAt" : "2020-05-27T23:49:14Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "c605b6f8-db6b-4bf3-8369-495d50d3229e",
        "parentId" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "> a new restoration start (for the post-recycled task), without the prior one ever having \"ended\".\r\n\r\nThis could happen if you have a restoring task that transitions to standby and then back to restoring. But invoking `onRestoreStart` twice in a row in that case is the current behavior, so your listener should presumably already be handling the situation. My impression is that users understand the global restore listener's `onRestoreEnd` to mean that restoration has completed, and invoking it before this would be an unexpected behavior change. \r\n\r\nI think in an ideal world we would decouple these two callbacks to make the distinction more apparent.",
        "createdAt" : "2020-05-28T00:15:04Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "3f7ea98c-645d-496e-bcc1-b747c4655c91",
        "parentId" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Ah actually @guozhangwang filed a ticket recently for exactly that: https://issues.apache.org/jira/browse/KAFKA-10005",
        "createdAt" : "2020-05-28T00:15:51Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "2881bcc1-8505-4f09-983a-313d07a0c0b7",
        "parentId" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, I'm satisfied with this conclusion.",
        "createdAt" : "2020-05-28T14:29:32Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "11a639ba-b02d-41c9-94e9-7063afe8be07",
        "parentId" : "4ea9f949-bf5f-4b74-975d-faa166f0af23",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yup, I propose to decouple these two and only allow restore callback at the per-store level and restore listener at the global level. We will always open the store with compaction disabled etc when we are transiting to restoring, and after we've done the restoration (for active) we will do a one-off compaction, and then reopen the store with configs overridden.",
        "createdAt" : "2020-05-28T17:22:13Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "30ac7b3ccd47063497c17ac148d90f9b29683e82",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +840,844 @@            final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n            if (changelogMetadata != null) {\n                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {\n                    firstException.compareAndSet(null, invokeOnRestoreEnd(partition, changelogMetadata));\n                }"
  }
]