[
  {
    "id" : "a8c0dc49-a822-4bcb-af72-0cab466224da",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-83592281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "At this point, we hit a race condition for the restore case of a regular changelog topic: `restorer.hasCompleted(pos, endOffset)` might return `true` even if some record got appended to the changelog iff (by chance), the last `poll()` batch of records ends at \"endOffsets - 1\".\r\n\r\nWe should add an additional check here, and get the `endOffset` of the partition again, and check if `restorer.hasCompleted(pos, newEndOffset) == true` -- if it's false, we should also throw `TaskMigratedException`.",
        "createdAt" : "2017-12-12T21:28:39Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "6eaa4d50-01d2-4509-ae6e-b877598920fd",
        "parentId" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Maybe I don't recall all the details of the offline conversation, or I'm missing something else, but by fetching the `endOffset` again and doing another `restorer.hasCompleted(pos, newEndOffset)` call won't this cause issues for `KTable` with a source topic?\r\n\r\nEDIT: unless we indicate this for this is a source log topic vs regular changelog topic, In which case I think that should go into a separate Jira/PR so as not to hold this one up.",
        "createdAt" : "2017-12-14T17:07:08Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "0570f955-9f90-4b9e-a5eb-b9b2cc58526e",
        "parentId" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think @mjsax 's comment was that we can add another `endOffset` call and check if the returned value is still the same, this will help reduce the likelihood of race condition but not eliminate it: however, we need to distinguish the case for source KTable such that in the case when `offset limit ` is specified, we do not need this additional check.",
        "createdAt" : "2017-12-14T17:45:01Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d533dd2d-84f3-4690-96cc-5752cd79e14f",
        "parentId" : "7a9126e8-33c3-4fae-a017-460923f162d8",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "ack",
        "createdAt" : "2017-12-14T17:59:58Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +252,256 @@        restorer.setRestoredOffset(pos);\n        if (restorer.hasCompleted(pos, endOffset)) {\n            if (pos > endOffset) {\n                throw new TaskMigratedException(task, topicPartition, endOffset, pos);\n            }"
  },
  {
    "id" : "8de51519-7b8a-4a00-a6f5-0f5f43acdc0f",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-83328117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1bd9850-2ba1-48b4-9f95-34561858edff",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good catchï¼",
        "createdAt" : "2017-12-13T21:39:05Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +252,256 @@        restorer.setRestoredOffset(pos);\n        if (restorer.hasCompleted(pos, endOffset)) {\n            if (pos > endOffset) {\n                throw new TaskMigratedException(task, topicPartition, endOffset, pos);\n            }"
  },
  {
    "id" : "d6a4df80-73bf-4659-8097-94d77cd7b8fb",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-83668920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: remove empty line\r\n\r\nAs discussed with @guozhangwang offline, I think passing in `nextPosition` is not correct. We should pass in the offset of the latest restored offset, but `nextPosition` is the offset of the first not restored offset.\r\n\r\nQuestion is, if a `nextPosition - 1` would be desirable or not because this might be a commit marker position and not an actual record position. ",
        "createdAt" : "2017-12-14T18:34:57Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "55fc85da-a7eb-429f-8223-8a94166d8c3a",
        "parentId" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Correct, this will be handled in a follow-up PR.  I synced off-line with @guozhangwang last night about this.",
        "createdAt" : "2017-12-14T19:11:01Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "c44556ea-18cc-4ee9-9f58-051afc38b248",
        "parentId" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ack. Can you create a JIRA for it?",
        "createdAt" : "2017-12-14T21:40:31Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "7135b4ba-3efc-4fa9-9f1a-741ea70c2102",
        "parentId" : "bbe18135-c5e2-4778-b99f-032d1ec8f593",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Done - https://issues.apache.org/jira/browse/KAFKA-6367",
        "createdAt" : "2017-12-14T22:41:17Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +297,301 @@            restorer.restore(restoreRecords);\n            restorer.restoreBatchCompleted(nextPosition, records.size());\n\n        }\n"
  },
  {
    "id" : "08297b25-d609-468d-92c3-f35070a71e5c",
    "prId" : 4300,
    "prUrl" : "https://github.com/apache/kafka/pull/4300#pullrequestreview-84911333",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2751300b-8d99-4ab5-9457-62ef68e7e516",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I've thought about this some more, and given the previous comments (https://github.com/apache/kafka/pull/4300#discussion_r156816065) that `processNext` was correct but susceptible to a race condition where we don't restore all records, IMHO we just need to add an additional check we did not restore all records in the list.   If you insist I can put in the `Math.min(restorer.OffestLimit(),restoreConsumer...)`",
        "createdAt" : "2017-12-19T18:32:19Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "31f7954c-cfc3-42bf-9df1-462180b3600f",
        "parentId" : "2751300b-8d99-4ab5-9457-62ef68e7e516",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I think this logic is correct now.`Math.min(...)` should do the exact same computation if I got it write. Both approaches have advantages: yours is more descriptive and thus explains itself better. The other one is more concise code allowing us to remove some local vars (`numberRecrods`, `numberRestored`), but harder to understand whats going if if you are not familiar with the code. Not sure which one is better.\r\n\r\n@guozhangwang @dguy WDYT? ",
        "createdAt" : "2017-12-20T22:01:00Z",
        "updatedAt" : "2017-12-27T17:53:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "85444fb99e659aeee7638f0183f608115e98a212",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +290,294 @@        // otherwise if we did not fully restore to that point we need to set nextPosition\n        // to the position of the restoreConsumer and we'll cause a TaskMigratedException exception\n        if (nextPosition == -1 || (restorer.offsetLimit() == Long.MAX_VALUE && numberRecords != numberRestored)) {\n            nextPosition = restoreConsumer.position(restorer.partition());\n        }"
  },
  {
    "id" : "f2842725-492d-4ae9-8f65-63502596ef60",
    "prId" : 5013,
    "prUrl" : "https://github.com/apache/kafka/pull/5013#pullrequestreview-121996537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37e40195-134d-4f2b-abba-6dad93a28dc2",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "we can also remove the entry in `updatedEndOffsets` if this is true.",
        "createdAt" : "2018-05-22T01:35:00Z",
        "updatedAt" : "2018-06-04T21:37:31Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e742d310817ba306c0a217666b983755418549e1",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +88,92 @@                final long pos = processNext(records.records(partition), restorer, updatedEndOffsets.get(partition));\n                restorer.setRestoredOffset(pos);\n                if (restorer.hasCompleted(pos, updatedEndOffsets.get(partition))) {\n                    restorer.restoreDone();\n                    updatedEndOffsets.remove(partition);"
  },
  {
    "id" : "f66aeb3a-5139-4446-bd54-629bb3011079",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9351f4a1-2da1-4265-b9d0-138c5ed181c8",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is just centralizing the structs a bit: I only store the map from topic partitions to restorers in one place now, namely  the `stateRestorers`, all other structs are simplified to set of topic partitions only, and they can retrieve the corresponding restorer from the first map always.",
        "createdAt" : "2018-07-27T21:36:36Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +49,53 @@    private final Map<String, List<PartitionInfo>> partitionInfo = new HashMap<>();\n    private final Map<TopicPartition, StateRestorer> stateRestorers = new HashMap<>();\n    private final Set<TopicPartition> needsRestoring = new HashSet<>();\n    private final Set<TopicPartition> needsInitializing = new HashSet<>();\n    private final Set<TopicPartition> completedRestorers = new HashSet<>();"
  },
  {
    "id" : "fb99106e-46b1-4fb4-bdd6-6ed99391daf3",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8fb8aed6-63fc-4899-b92c-8a8caacac461",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I've added this set to replace the logic of `completed = total (stateRestorers) - needsRestoring`, since it is no longer true: if a partition is not in needsRestoring, it may because of the re-initialization.",
        "createdAt" : "2018-07-27T21:37:31Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +51,55 @@    private final Set<TopicPartition> needsRestoring = new HashSet<>();\n    private final Set<TopicPartition> needsInitializing = new HashSet<>();\n    private final Set<TopicPartition> completedRestorers = new HashSet<>();\n    private final Duration pollTime;\n"
  },
  {
    "id" : "920eeaf8-7197-4d55-b9f8-8be3de09411f",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "000f3353-2743-4b2c-bea6-369c20ac7677",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Okay this is a bit hack but is the key of the fix: I've decided to keep the restorer in the map and note replacing it with the new restorer, since from the ProcessorStateManager the checkpoint offset cannot be inferred. This is just to walk around the code hierarchy of Restorer and the ProcessorStateManager.",
        "createdAt" : "2018-07-27T21:39:15Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +66,70 @@    @Override\n    public void register(final StateRestorer restorer) {\n        if (!stateRestorers.containsKey(restorer.partition())) {\n            restorer.setUserRestoreListener(userStateRestoreListener);\n            stateRestorers.put(restorer.partition(), restorer);"
  },
  {
    "id" : "f2610047-7842-491f-b010-a43a4c2da9d9",
    "prId" : 5430,
    "prUrl" : "https://github.com/apache/kafka/pull/5430#pullrequestreview-141276279",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5709da88-f673-4b63-8911-60f45ef0c5ec",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is the other fix: whenever we initialize, inside `reinitializeStateStoresForPartitions` the store.init will be called which will call storechangelogger.register() again to put it into the `needsInitializing` set.",
        "createdAt" : "2018-07-27T21:40:12Z",
        "updatedAt" : "2018-07-28T01:08:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda58c38bbc5f2936654e768d67eea4742a831f",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +106,110 @@                log.info(\"Reinitializing StreamTask {} for changelog {}\", task, partition);\n\n                needsInitializing.remove(partition);\n                needsRestoring.remove(partition);\n"
  },
  {
    "id" : "a557089c-1098-4147-9658-096bf4762b70",
    "prId" : 5767,
    "prUrl" : "https://github.com/apache/kafka/pull/5767#pullrequestreview-176440736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88efebdc-c094-474d-a4b9-39a045cfc973",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I have some questions just for my education.\r\n\r\nI think I get it what the issue is.  The `ProcessorStateManager#register` creates a new `StateRestorer` each time, and it gets passed in here, so without reusing the `existingRestorer` restoring never completes. Is that correct?  My question is `StateRestorer` should contain the same state even if a new instance, so how does using the existing restorer solve the issue? From what I can see the state should be the same between the new and existing restorer.  Since the fix seems to be the same as #5915, I'll ask my question here.",
        "createdAt" : "2018-11-15T15:44:35Z",
        "updatedAt" : "2018-11-15T15:49:18Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "a4c72857-acb7-4f9a-b7de-51f13af69ee9",
        "parentId" : "88efebdc-c094-474d-a4b9-39a045cfc973",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : ">  Is that correct?\r\n\r\nYes.\r\n\r\nFor EOS, on startup, we read the checkpoint file, store the information in the restorer, and delete the checkpoint file (we only write one on clean shutdown). The magic happens in L173/180 `if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) {` and L204 `restorer.setCheckpointOffset(restoreConsumer.position(restoringPartition));`. \r\n\r\nIf there is no checkpoint store in the restorer, we seekToBeginning and mark as `needsPositionUpdate`. Later, we wipe out the store and put the current beginningOffset as checkpoint to the restorer. Thus, when we re-register, we need to preserve this checkpoint information. The newly created `StateRestored` would get initialized by looking if there is checkpoint file on disk and would be initialized with \"no checkpoint\".\r\n\r\nIt's a little hacky... Maybe we can revisit this design at some point and make it cleaner.\r\n\r\nMakes sense?",
        "createdAt" : "2018-11-17T02:40:48Z",
        "updatedAt" : "2018-11-17T02:42:15Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "87c9e33a-0e1d-4f22-92e6-c20c6263b225",
        "parentId" : "88efebdc-c094-474d-a4b9-39a045cfc973",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Yes, thanks for the clarification.",
        "createdAt" : "2018-11-19T18:41:03Z",
        "updatedAt" : "2018-11-19T18:41:03Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "5256317b9b3d01ba50c6287c6b88216dcc588498",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +68,72 @@            needsInitializing.put(restorer.partition(), restorer);\n        } else {\n            needsInitializing.put(restorer.partition(), existingRestorer);\n        }\n    }"
  },
  {
    "id" : "d6f51cc7-3203-41e7-99e2-b65f4d70b94b",
    "prId" : 6113,
    "prUrl" : "https://github.com/apache/kafka/pull/6113#pullrequestreview-200949375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1355236f-93ea-4ca5-abae-aa44242bd7fe",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is @linyli001 's fix.",
        "createdAt" : "2019-01-10T23:55:13Z",
        "updatedAt" : "2019-02-22T01:03:34Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f2bce32b-02da-41db-8329-8355feaf74ad",
        "parentId" : "1355236f-93ea-4ca5-abae-aa44242bd7fe",
        "authorId" : "7ec9f39d-9147-4743-a257-c07990c83519",
        "body" : "added some hack code to listeners and transformer init func before next version release..",
        "createdAt" : "2019-02-07T07:36:25Z",
        "updatedAt" : "2019-02-22T01:03:34Z",
        "lastEditedBy" : "7ec9f39d-9147-4743-a257-c07990c83519",
        "tags" : [
        ]
      }
    ],
    "commit" : "912a45642a92f4298d203d5c4eda6a1678208c05",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +278,282 @@        endOffsets.clear();\n        needsInitializing.clear();\n        completedRestorers.clear();\n    }\n"
  },
  {
    "id" : "d30d7f20-f46a-4384-b564-e61493ccf433",
    "prId" : 7961,
    "prUrl" : "https://github.com/apache/kafka/pull/7961#pullrequestreview-344868994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a3b3787-3e56-43fb-973f-3184d61e1136",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "I noticed some maps are changed to ConcurrentHashMap.\r\nMay I ask what was the selection criterion for the change ?\r\n\r\nthanks",
        "createdAt" : "2020-01-16T23:56:05Z",
        "updatedAt" : "2020-01-16T23:56:06Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "852b4390-0915-4302-821f-5e8d83b9a652",
        "parentId" : "9a3b3787-3e56-43fb-973f-3184d61e1136",
        "authorId" : "478a572c-b267-486a-b845-4847ccf71f62",
        "body" : "This is for safe iteration from the thread calling the lag fetch API",
        "createdAt" : "2020-01-17T22:11:12Z",
        "updatedAt" : "2020-01-17T22:11:13Z",
        "lastEditedBy" : "478a572c-b267-486a-b845-4847ccf71f62",
        "tags" : [
        ]
      }
    ],
    "commit" : "026b9dd5f36a78839a5e628cd51d2e98f91313e9",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +48,52 @@    private final Map<TopicPartition, Long> restoreToOffsets = new HashMap<>();\n    private final Map<String, List<PartitionInfo>> partitionInfo = new HashMap<>();\n    private final Map<TopicPartition, StateRestorer> stateRestorers = new ConcurrentHashMap<>();\n    private final Set<TopicPartition> needsRestoring = new HashSet<>();\n    private final Set<TopicPartition> needsInitializing = new HashSet<>();"
  }
]