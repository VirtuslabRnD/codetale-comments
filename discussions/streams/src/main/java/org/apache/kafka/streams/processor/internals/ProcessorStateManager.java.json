[
  {
    "id" : "0a5f4289-ee03-4364-9333-688c5133a401",
    "prId" : 5187,
    "prUrl" : "https://github.com/apache/kafka/pull/5187#pullrequestreview-127669574",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34c5f689-325a-4d35-b693-4876dad5ca1d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is a Java8 rewrite only.",
        "createdAt" : "2018-06-11T17:42:28Z",
        "updatedAt" : "2018-06-13T21:53:27Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "99b3999656cc1fbebe18159dd2394c1b3494c610",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +169,173 @@            final TopicPartition storePartition = new TopicPartition(topicName, partition);\n\n            partitionsAndOffsets.put(storePartition, checkpointableOffsets.getOrDefault(storePartition, -1L));\n        }\n        return partitionsAndOffsets;"
  },
  {
    "id" : "b631dad6-b050-43dd-b6f1-dff7a132e767",
    "prId" : 5641,
    "prUrl" : "https://github.com/apache/kafka/pull/5641#pullrequestreview-154894631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bfa5066-86c0-458d-bf4e-5a01e74bd5ea",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "do you think it's worth verifying that the map actually is one-to-one?",
        "createdAt" : "2018-09-12T21:00:07Z",
        "updatedAt" : "2018-09-13T00:43:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "aec93bed-cd0d-46a7-8b78-937fd939c9c6",
        "parentId" : "5bfa5066-86c0-458d-bf4e-5a01e74bd5ea",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Should be fine as is IMHO",
        "createdAt" : "2018-09-13T00:42:33Z",
        "updatedAt" : "2018-09-13T00:43:56Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "dac3e52d-e64b-4346-bf5e-d2a4aefcd2b1",
        "parentId" : "5bfa5066-86c0-458d-bf4e-5a01e74bd5ea",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "If you feel strong about it, we can fix in `trunk` -- for `0.11` I will just leave as-is",
        "createdAt" : "2018-09-13T00:43:00Z",
        "updatedAt" : "2018-09-13T00:43:56Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ffefd7b389b4cab42e084cef8028640e90496f4",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +235,239 @@        final Map<String, String> reversedMap = new HashMap<>();\n        for (final Map.Entry<String, String> entry : origin.entrySet()) {\n            reversedMap.put(entry.getValue(), entry.getKey());\n        }\n        return reversedMap;"
  },
  {
    "id" : "186b1058-a6f1-4d75-8a21-572ef0c286ef",
    "prId" : 5657,
    "prUrl" : "https://github.com/apache/kafka/pull/5657#pullrequestreview-156618325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e6bb8b5-d4e7-4071-9cc8-65dc0f3b9ab5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Has this same state store potentially had `init` called previously? If so, do implementers know that init may be called multiple times?",
        "createdAt" : "2018-09-18T22:04:45Z",
        "updatedAt" : "2018-09-18T22:05:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "709e818c-c52f-4876-ad1a-6b568a35b685",
        "parentId" : "2e6bb8b5-d4e7-4071-9cc8-65dc0f3b9ab5",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Actually, I think it's ok calling `init` again as the `stateStore` is closed above on line 288.",
        "createdAt" : "2018-09-18T23:47:09Z",
        "updatedAt" : "2018-09-18T23:47:42Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "7f6557cf-3255-4782-b037-14d826deac15",
        "parentId" : "2e6bb8b5-d4e7-4071-9cc8-65dc0f3b9ab5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "As Bill pointed out -- we close the store above and thus, we can `init()` it again -- it was always like this, as during rebalance, we might close and re-init a store, too.",
        "createdAt" : "2018-09-19T00:24:58Z",
        "updatedAt" : "2018-09-19T00:24:58Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "317e534d50cb26c93dda69261d51295817f9c6a9",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +225,229 @@                }\n\n                stateStore.init(processorContext, stateStore);\n            }\n        }"
  },
  {
    "id" : "77598d6e-03ae-428c-9466-49f4dfa9bd5a",
    "prId" : 6204,
    "prUrl" : "https://github.com/apache/kafka/pull/6204#pullrequestreview-197403293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hmm.. I thought the restorer / state manager would now be agnostic to the convert at all, since the inner store impl is responsible for extending the interface as well as calling its `TimestampedBytesStore` function internally. Why do we still need an internal `RecordConverter` class?",
        "createdAt" : "2019-01-28T04:30:34Z",
        "updatedAt" : "2019-01-29T01:06:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "62413f6b-e8d8-4983-80a2-253c2da3f0a3",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "The new static method that is used by the stores, will translate on-disk data from old to new format.\r\n\r\nWe still need a `RecordConverter` that translates data from the changelog topic to the new format, because on restore, we put plain `<byte[],byte[]>` key-value pairs into the store, and the store expects those record to be in the new format.\r\n\r\nThis make we realize, the the implementation of `RecordConverter` is actually not correct in this PR -- it should not use the new static method (that inserts a `-1` as timestamp), but it need to put the actual record timestamp... Will update the PR accordingly.",
        "createdAt" : "2019-01-29T00:58:32Z",
        "updatedAt" : "2019-01-29T01:06:09Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "54104621-3864-409e-beb3-b08eb7bddc33",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "But can't we make this logic inside the `prepareBatch` call inside the internal store impl (of course we would require customized users to do so as well) so that the callers do not need to be aware of that?",
        "createdAt" : "2019-01-29T01:39:51Z",
        "updatedAt" : "2019-01-29T01:39:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8850d3cd-98b3-4644-9856-16963a81d58b",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "No, because `prepareBatch` (or actually `restoreAllInternal`) takes `KeyValue<byte[],byte[]` but not a `ConsumerRecord`. Thus there is not timestamp information we can add to the value.",
        "createdAt" : "2019-01-29T01:52:20Z",
        "updatedAt" : "2019-01-29T01:52:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "cd2904ef-6beb-4575-a5cc-7347323e17d4",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : ":+1: The `convertToTimestampedFormat` function is purely for converting the already-stored data into the new binary format. It can't actually add any timestamp information, because we don't have it when we retrieve the old-format data. So the function is like `(value) -> [-1,value]`.\r\n\r\nIn contrast, the state restorer needs to be able to insert the timestamp, so the function is like `(value, timestamp) -> [timestamp, value]`.\r\n\r\nActually, looking at this code again, I see Matthias was right, the `RecordConverter` erroneously calls through to the `convertToTimestampedFormat` function. Oops! Good catch!",
        "createdAt" : "2019-01-29T06:14:30Z",
        "updatedAt" : "2019-01-29T06:14:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "f402982f-2d01-4f93-b31d-a5ce1738f6cc",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ah, I just refreshed, and looked at the new code. It LGTM. Thanks!",
        "createdAt" : "2019-01-29T06:19:56Z",
        "updatedAt" : "2019-01-29T06:19:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "3058c615-35ae-4363-8c4b-ab77d0d90ff3",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "@mjsax Thanks for the explanation! LGTM.",
        "createdAt" : "2019-01-29T07:11:32Z",
        "updatedAt" : "2019-01-29T07:11:32Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2f34bb3907bc00582ab7d70f64e475427090fa5",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +138,142 @@            store instanceof WrappedStateStore ? ((WrappedStateStore) store).inner() : store;\n        final RecordConverter recordConverter =\n            stateStore instanceof TimestampedBytesStore ? RecordConverter.converter() : record -> record;\n\n        if (isStandby) {"
  },
  {
    "id" : "96840ba3-368c-44de-a3eb-a22bf5959ad5",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80684b54-8ab6-4ffb-951d-e89af0bc1182",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "pretty sure this was a typo",
        "createdAt" : "2019-07-03T17:09:08Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +68,72 @@\n    // TODO: this map does not work with customized grouper where multiple partitions\n    // of the same topic can be assigned to the same task.\n    private final Map<String, TopicPartition> partitionForTopic;\n"
  },
  {
    "id" : "b26c753f-4c65-4f6f-aff8-1af0f513d815",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4e13a22-4c99-4831-8eee-1905572e5d77",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "adding some clarity",
        "createdAt" : "2019-07-03T17:09:44Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +114,118 @@\n        if (eosEnabled) {\n            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n            // delete the checkpoint file after loading its stored offsets.\n            checkpointFile.delete();"
  },
  {
    "id" : "b2cc6ba4-6c3e-4450-b1dd-47939bb588fd",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d06282b-382e-4592-8235-135b290a72c0",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It's equivalent, but this makes more sense.",
        "createdAt" : "2019-07-03T17:10:30Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +141,145 @@\n        if (CHECKPOINT_FILE_NAME.equals(storeName)) {\n            throw new IllegalArgumentException(String.format(\"%sIllegal store name: %s\", logPrefix, storeName));\n        }\n"
  },
  {
    "id" : "68a9b66b-822b-46a0-903f-807df46282c1",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f969be5f-411f-4e16-ad01-1bf30d120567",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "this block has nothing to do with adding the store to `registeredStores`, so I moved that operation to after the block for clarity.",
        "createdAt" : "2019-07-03T17:11:20Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +150,154 @@        // check that the underlying change log topic exist or not\n        final String topic = storeToChangelogTopic.get(storeName);\n        if (topic != null) {\n            final TopicPartition storePartition = new TopicPartition(topic, getPartition(topic));\n"
  },
  {
    "id" : "3f85c17f-edff-444b-882e-2ae408c3fc81",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "beccca2d-9e5e-4ca3-a996-071f6af6ed90",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "not necessary to check whether there is a checkpoint file, since it's now encapsulated in `clearCheckpoints`.",
        "createdAt" : "2019-07-03T17:12:18Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 220,
    "diffHunk" : "@@ -1,1 +325,329 @@        }\n\n        if (!clean && eosEnabled) {\n            // delete the checkpoint file if this is an unclean close\n            try {"
  },
  {
    "id" : "0175eddd-c76b-4161-b1d6-2d987b5c2351",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24593b7a-b074-4f2e-bbdf-351999cbdf59",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "The complicated for loop in the old version has the side-effect of checking this condition, so I'm keeping here, but making it explicit.",
        "createdAt" : "2019-07-03T17:13:08Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +341,345 @@    @Override\n    public void checkpoint(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n        ensureStoresRegistered();\n\n        // write the checkpoint file before closing"
  },
  {
    "id" : "ebace2f6-6e5c-46ac-b05d-8c2c8644e9e3",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "671371f8-5445-4af2-bbe0-1c007e7c1788",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Also here, no longer sharing mutable state between super and sub classes.",
        "createdAt" : "2019-07-03T20:59:43Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +47,51 @@\n\npublic class ProcessorStateManager implements StateManager {\n    private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n"
  },
  {
    "id" : "cf7a864d-217b-4fb7-aaf2-16dd0034d5da",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2eaddb4-bc41-46bc-84ac-fdcf1a59bbc5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Adding this collection breaks a circular dependency in this class:\r\n* the checkpoints we load from disk are potentially not valid for the current topology\r\n* we have to load the checkpoints immediately because we have to delete the checkpoint file before processing in the case of EOS\r\n* we also need to have read the checkpoint file _before_ registering stores, since it might be needed to create a restorer\r\n* we can't know if a checkpoint from the file is valid until _after_ registering stores \r\n\r\nIn other words, if the prior code wanted to validate the loaded checkpoints, it would have to register the stores before loading checkpoints, but it also needs to load the checkpoints before registering the stores.\r\n\r\nWe're breaking the cycle here by keeping the loaded checkpoints separate. Now we read the checkpoint file into `initialLoadedCheckpoints`, which is used to register the stores, and then we are able to make sure that we only ever write valid checkpoints into the `checkpointFileCache`, which is used to update the checkpoint file later on.",
        "createdAt" : "2019-07-03T22:04:29Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +75,79 @@    private OffsetCheckpoint checkpointFile;\n    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n\n    /**"
  },
  {
    "id" : "cc0b1306-89f4-488d-9887-850224521397",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6195434-0b07-4c95-8197-c3700c7ebef6",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This is where we're using the _loaded_ checkpoint for store registration. Note the missing condition which is now handled... if the store is not persistent, it should _not_ use the loaded checkpoint.",
        "createdAt" : "2019-07-03T22:05:52Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 110,
    "diffHunk" : "@@ -1,1 +161,165 @@                recordConverters.put(topic, recordConverter);\n            } else {\n                final Long restoreCheckpoint = store.persistent() ? initialLoadedCheckpoints.get(storePartition) : null;\n                if (restoreCheckpoint != null) {\n                    checkpointFileCache.put(storePartition, restoreCheckpoint);"
  },
  {
    "id" : "1369e674-a9ad-4ba8-8318-bad3de8858f3",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6fb24cc-fe3f-41a8-b6a0-6f5838a59e97",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "encapsulating this operation so that outside classes don't have to directly mutate our `checkpointFile` field.",
        "createdAt" : "2019-07-03T22:06:39Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +200,204 @@    }\n\n    void clearCheckpoints() throws IOException {\n        if (checkpointFile != null) {\n            checkpointFile.delete();"
  },
  {
    "id" : "79420b0d-5cc7-4058-b0a0-c8d7c60a4f56",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bce62d1-8d73-4a30-8596-24d2eb961d66",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "We didn't previously clear the cache on the blocks that this method replaces, but after reading the code, I'm pretty sure this is the right thing to do.",
        "createdAt" : "2019-07-03T22:07:33Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +205,209 @@            checkpointFile = null;\n\n            checkpointFileCache.clear();\n        }\n    }"
  },
  {
    "id" : "f0b1a216-0db1-4b28-a38c-f031edd1c05f",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-258333947",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1721e9dc-4eb6-4cae-af75-a6df7eac861e",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "I would assign `storeToChangelog.getKey()` to a variable called `storeName` to make the code more readable.",
        "createdAt" : "2019-07-05T10:24:52Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 326,
    "diffHunk" : "@@ -1,1 +427,431 @@\n        final Set<TopicPartition> result = new HashSet<>(storeToChangelogTopic.size());\n        for (final Map.Entry<String, String> storeToChangelog : storeToChangelogTopic.entrySet()) {\n            final String storeName = storeToChangelog.getKey();\n            if (registeredStores.containsKey(storeName)"
  },
  {
    "id" : "dfda4471-2aeb-4741-a58f-e9f231ce8498",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-259804929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "530d25a2-f8f0-4690-8db0-4d28c30c3f1f",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "In this method, you call `validCheckpointableTopics` three times, once directly and twice within `validCheckpointableOffsets`. Wouldn't it make sense to call it once in the beginning and passing the result into `validCheckpointableOffsets`?",
        "createdAt" : "2019-07-05T10:35:57Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "99c95209-36d3-4da1-920c-05b018dc94ed",
        "parentId" : "530d25a2-f8f0-4690-8db0-4d28c30c3f1f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "yep. done.",
        "createdAt" : "2019-07-09T23:17:35Z",
        "updatedAt" : "2019-07-09T23:17:36Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "b16e9031-f68a-40f1-bbd6-d6bc452d8b41",
        "parentId" : "530d25a2-f8f0-4690-8db0-4d28c30c3f1f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "thanks for catching this.",
        "createdAt" : "2019-07-09T23:17:47Z",
        "updatedAt" : "2019-07-09T23:17:47Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 285,
    "diffHunk" : "@@ -1,1 +360,364 @@    }\n\n    private void updateCheckpointFileCache(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n        final Set<TopicPartition> validCheckpointableTopics = validCheckpointableTopics();\n        final Map<TopicPartition, Long> restoredOffsets = validCheckpointableOffsets("
  },
  {
    "id" : "fb7933cc-8d96-4fae-92fe-d989e8e60602",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-259802857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06b46d6e-692f-446b-92c8-94d6c42feb37",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Wouldn't it be more meaningful to rename this class to `TaskStateManager`? ",
        "createdAt" : "2019-07-05T12:52:28Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "3b6a6d9d-7082-4990-ab40-e303d690757d",
        "parentId" : "06b46d6e-692f-446b-92c8-94d6c42feb37",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Maybe, I'm not sure of the historical reason to name it this way.",
        "createdAt" : "2019-07-09T23:09:38Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +47,51 @@\n\npublic class ProcessorStateManager implements StateManager {\n    private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n"
  },
  {
    "id" : "879fa8a1-32dc-4b4f-ab7a-a912e413cb9c",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-260341432",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "921f9ebe-90d6-4c4d-bf4d-f2988e668135",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "If the store is not peristent or the read checkpoint file does not contain the partition, this will throw a NPE, right? If yes, you should add unit tests for these cases.",
        "createdAt" : "2019-07-05T14:45:55Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "b0d0a17b-c98b-4dbd-a358-232bb37099e2",
        "parentId" : "921f9ebe-90d6-4c4d-bf4d-f2988e668135",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "No, I guess you were thinking the `Long` would become unboxed at this point? It's actually a `Long` parameter, and the `StateRestorer` constructor checks for null... Not the cleanest code, I guess, but it looks like it's been this way since 2017.",
        "createdAt" : "2019-07-09T23:15:45Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "fb3afb05-9b1d-4129-abb5-3bd52dd60037",
        "parentId" : "921f9ebe-90d6-4c4d-bf4d-f2988e668135",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.",
        "createdAt" : "2019-07-10T20:35:11Z",
        "updatedAt" : "2019-07-10T20:35:11Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +170,174 @@                    storePartition,\n                    new CompositeRestoreListener(stateRestoreCallback),\n                    restoreCheckpoint,\n                    offsetLimit(storePartition),\n                    store.persistent(),"
  },
  {
    "id" : "61429fa6-5209-4dde-b05a-81fee303cc9d",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-263363652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2cf7a8d-01a9-4b06-baeb-d1a78b928b6f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I liked this refactoring a lot, thanks @vvcephei !",
        "createdAt" : "2019-07-17T23:18:30Z",
        "updatedAt" : "2019-07-17T23:40:55Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +188,192 @@    public void reinitializeStateStoresForPartitions(final Collection<TopicPartition> partitions,\n                                                     final InternalProcessorContext processorContext) {\n        StateManagerUtil.reinitializeStateStoresForPartitions(log,\n                                                              eosEnabled,\n                                                              baseDir,"
  },
  {
    "id" : "4ba08fd2-6129-4a59-988d-03ee85eaba4d",
    "prId" : 7304,
    "prUrl" : "https://github.com/apache/kafka/pull/7304#pullrequestreview-288712381",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "329b5396-f7a3-4a0e-9e65-d916758a80f8",
        "parentId" : null,
        "authorId" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "body" : "Thanks!",
        "createdAt" : "2019-09-16T15:38:26Z",
        "updatedAt" : "2019-09-20T20:04:08Z",
        "lastEditedBy" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bc8bf0f0703bf557bff6c8ae5c9a347599c8d4f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +253,257 @@    }\n\n    private long offsetLimit(final TopicPartition partition) {\n        final Long limit = offsetLimits.get(partition);\n        return limit != null ? limit : Long.MAX_VALUE;"
  },
  {
    "id" : "db803cf1-926b-4cdb-9b47-1c04854eabe6",
    "prId" : 7681,
    "prUrl" : "https://github.com/apache/kafka/pull/7681#pullrequestreview-316029454",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6e62f00-d139-413e-a5cf-97f2ba0504ce",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Removed a redundant log line and upgraded this log to DEBUG.\r\n\r\nSystem tests run at DEBUG level, so this change makes this log visible to the system tests. Ideally, I could have just turned this class's logger to TRACE, but it happens to be a real pain with the way the system tests are set up.\r\n\r\nI think this is reasonable, though, since this is not a high-volume log message, and it's pretty handy to know this information while debugging state restoration and initialization.",
        "createdAt" : "2019-11-13T06:49:52Z",
        "updatedAt" : "2019-11-13T21:20:04Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "741306599487c10a3d5f753920f1f1c2e0e0ec73",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +349,353 @@        updateCheckpointFileCache(checkpointableOffsetsFromProcessing);\n\n        log.debug(\"Writing checkpoint: {}\", checkpointFileCache);\n        try {\n            checkpointFile.write(checkpointFileCache);"
  },
  {
    "id" : "566527f8-9089-4927-aca5-53233ab1948b",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353327753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0855a3b1-193e-43b3-aaf4-73a155115248",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "unresolved TODO here. it's unclear to me why we would only consider this task corrupted in EOS mode. It seems like the lack of a checkpoint _file_ just means that we loaded a cached task in an undefined state, and we should discard it and restore. If we have the file, but some of a changelog is missing from it, then maybe it just never got written, before shutdown, or more likely, the topology has changed and the task is corrupted, whether or not we are in EOS.",
        "createdAt" : "2020-02-04T21:32:26Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "0edbcd27-5069-4bc2-8c09-c75256a7b268",
        "parentId" : "0855a3b1-193e-43b3-aaf4-73a155115248",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Today we write the checkpoint file before we commit the offsets, so under non-EOS we can just restore from scratch without wiping the local store image since restoring is just overwriting the local store and we can just restore to the end of the changelog (this may result in duplicates but is fine under non-EOS); but with non-EOS we have to first wipe out before restoring from scratch.",
        "createdAt" : "2020-02-04T21:51:49Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 253,
    "diffHunk" : "@@ -1,1 +207,211 @@                            store.stateStore.name(), store.offset, store.changelogPartition);\n                    } else {\n                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted\n\n                        log.info(\"State store {} did not find checkpoint offset, hence would \" +"
  },
  {
    "id" : "be975924-db34-4584-a635-871ef987f5fb",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353328951",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4aac5bea-d92e-42e1-a41a-a53ca78bc10a",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "seems like this also indicates the task is corrupted",
        "createdAt" : "2020-02-04T21:32:44Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "d8d640dc-ef3a-4449-ba01-5824e4480cc1",
        "parentId" : "4aac5bea-d92e-42e1-a41a-a53ca78bc10a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Arguably yes, I'm just following the old logic intentionally here -- I think originally we want to be more compatible with topology changes (if some topology optimizations decides that some stores are not needed) but on second thought I think this is not safe either. We can consider making this change in another PR to make it stricter.",
        "createdAt" : "2020-02-04T21:53:50Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 263,
    "diffHunk" : "@@ -1,1 +217,221 @@\n            if (!loadedCheckpoints.isEmpty()) {\n                log.warn(\"Some loaded checkpoint offsets cannot find their corresponding state stores: {}\", loadedCheckpoints);\n            }\n"
  },
  {
    "id" : "65229ae2-c1ed-49ce-b2d6-8d50403460e4",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353330547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cae7aa2-ca5a-4431-bc00-9f069f86e631",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "in retrospect, I like the idea of doing this regardless of EOS. Why should we deliberately produce wrong results in ALO mode? We can certainly optimize to be able to use semi-trustworthy data, but let's treat that separately from EOS vs ALO",
        "createdAt" : "2020-02-04T21:33:09Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "b5b9aab6-4c3c-44a4-ab8f-413572284d3e",
        "parentId" : "5cae7aa2-ca5a-4431-bc00-9f069f86e631",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The idea is that for ALO if we failed to write the first checkpoint after restarting we can still fallback to the original checkpoint even though the store may have been updated and hence there would be duplicates. But since the window gap (just one commit interval) is so small I think it does not worth making the code more complicated with EO v.s. ALO.",
        "createdAt" : "2020-02-04T21:56:35Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 266,
    "diffHunk" : "@@ -1,1 +220,224 @@            }\n\n            checkpointFile.delete();\n        } catch (final IOException | RuntimeException e) {\n            // both IOException or runtime exception like number parsing can throw"
  },
  {
    "id" : "5c97f608-cfe2-4155-a6bb-50ba547bdd1b",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353316379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74d020d6-b9d5-495a-b9fb-fd76ad89f0e3",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "these kinds of comments paradoxically lead to unmaintainability. Either a method is part of the public contract or not. If it is, then this comment would become out of date, if not, then the changelog reader shouldn't be using it. The recommendation is simply to delete the comment (and similar ones). This even applies to \"for testing\" comments. I have found several methods in use in this code base that were commented \"visible for testing\". Even for tests, either move both the class and the test into an isolated package and use package-private, refactor the test, or remove the comment. Also, if the changelog reader really needs four \"holes\" poked into this class, then we should reconsider the relationship between the state manager and the changelog reader.",
        "createdAt" : "2020-02-04T21:34:55Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 406,
    "diffHunk" : "@@ -1,1 +303,307 @@    }\n\n    // used by the changelog reader only\n    boolean changelogAsSource(final TopicPartition partition) {\n        return sourcePartitions.contains(partition);"
  },
  {
    "id" : "8092790a-2f03-4b70-8166-bfd8e703c2c2",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353331803",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9619fa9-5b89-4112-92f3-6a7b05eee8a4",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "maybe it doesn't matter, but this method makes the only usage an n^2 algorithm. If we instead inverted the `stores` collection and used it for lookups in `register`, it would be o(n)",
        "createdAt" : "2020-02-04T21:37:40Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "ef8656fa-4d6f-464c-975c-dfca5eb06f6f",
        "parentId" : "a9619fa9-5b89-4112-92f3-6a7b05eee8a4",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I've thought about it: inverting the `stores` collection makes other calls that depends on the store name more complicated, while keeping two collections indexed by storeName / changelog partition is not much worthy since within a task there are usually no more than 10 stores so this n^2 algorithm should not be a big deal.",
        "createdAt" : "2020-02-04T21:58:52Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 722,
    "diffHunk" : "@@ -1,1 +456,460 @@    }\n\n    private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n        final List<StateStoreMetadata> found = stores.values().stream()\n            .filter(metadata -> changelogPartition.equals(metadata.changelogPartition))"
  },
  {
    "id" : "8ae774b0-8d73-494e-bd09-1434fc2e4a1b",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-355298844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7fcf1f5-bcc3-4c5e-8861-4d6701a5bb10",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We need to clear the stores map now since we may re-initialize the state stores upon reviving a task.",
        "createdAt" : "2020-02-07T00:49:09Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "ff1b145e-79c6-4241-83e9-6298974563d2",
        "parentId" : "f7fcf1f5-bcc3-4c5e-8861-4d6701a5bb10",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Do we also need to clear `storeToChangelogTopic`, etc?",
        "createdAt" : "2020-02-07T01:27:42Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9f00311a-35e9-4350-a0c1-a94c47450f76",
        "parentId" : "f7fcf1f5-bcc3-4c5e-8861-4d6701a5bb10",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`storeToChangelogTopic` and `sourcePartitions` are passed in at construction time and final, so we cannot clear them (since they would only be initialized once).",
        "createdAt" : "2020-02-07T16:58:45Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +436,440 @@            }\n\n            stores.clear();\n        }\n"
  },
  {
    "id" : "f6fea996-21bb-4f53-a293-1af4d3d98a91",
    "prId" : 8180,
    "prUrl" : "https://github.com/apache/kafka/pull/8180#pullrequestreview-365335349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b6f5525-d798-49f9-957d-11c2de008bd6",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Here I just re-arranged the parameter orders and below I use the log-prefix from the log-context rather than creating a new string, no critical changes here.",
        "createdAt" : "2020-02-27T00:12:36Z",
        "updatedAt" : "2020-03-06T23:37:23Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1bea170bbbf209ad20e5493ae832fd247ebfc3d",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +165,169 @@                                 final TaskType taskType,\n                                 final boolean eosEnabled,\n                                 final LogContext logContext,\n                                 final StateDirectory stateDirectory,\n                                 final ChangelogRegister changelogReader,"
  },
  {
    "id" : "8a2f106f-7eda-45aa-9c3b-57e1b686b2f7",
    "prId" : 8248,
    "prUrl" : "https://github.com/apache/kafka/pull/8248#pullrequestreview-417409203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "beea755d-b290-4f3e-b4d6-2c35d9b150d7",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: I think now it's better to move the debug line 464 before `unregisterAllStoresWithChangelogReader`.",
        "createdAt" : "2020-05-25T01:05:51Z",
        "updatedAt" : "2020-05-28T23:49:30Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "30ac7b3ccd47063497c17ac148d90f9b29683e82",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +461,465 @@\n        changelogReader.unregister(getAllChangelogTopicPartitions(), false);\n\n        RuntimeException firstException = null;\n        // attempting to close the stores, just in case they"
  },
  {
    "id" : "72c2e17c-e6a9-4436-962e-4ac02c362a41",
    "prId" : 8676,
    "prUrl" : "https://github.com/apache/kafka/pull/8676#pullrequestreview-425715982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2a9cb10-d5b9-45ac-8bcd-2729edee73a2",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is the warn log I added, lmk wdyt @vvcephei ",
        "createdAt" : "2020-06-06T06:06:05Z",
        "updatedAt" : "2020-06-06T06:06:41Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7dccc0f15bd970fc0650181e5341dace13ff327a",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +301,305 @@        }\n\n        if (stateRestoreCallback instanceof StateRestoreListener) {\n            log.warn(\"The registered state restore callback is also implementing the state restore listener interface, \" +\n                    \"which is not expected and would be ignored\");"
  },
  {
    "id" : "ebc80004-0bf0-4999-a3cc-7c4b11669c20",
    "prId" : 8872,
    "prUrl" : "https://github.com/apache/kafka/pull/8872#pullrequestreview-430869021",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00d65cda-8792-46ec-9ff8-d5f733222f6b",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Another alternative for the log message would be\r\n\r\n```\r\nstandby-task [0_2] Transitioning state manager for STANDBY task 0_2 to ACTIVE \r\n```\r\n\r\nThat is, the log prefix would change from `task` to `standby-task`.",
        "createdAt" : "2020-06-15T12:28:15Z",
        "updatedAt" : "2020-06-15T12:29:25Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "3594eb22-4b94-460f-bca2-219dc25b1bba",
        "parentId" : "00d65cda-8792-46ec-9ff8-d5f733222f6b",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I agree it would be useful to prefix the logs with `standby` or `active`, but I'd prefer to do that everywhere in a separate PR.\r\n\r\nCan we just move the log message to before we reassign `taskType = newType`? Or do you think we might forget/not notice that the ordering is relevant and accidentally move it back at some future time?",
        "createdAt" : "2020-06-15T16:27:46Z",
        "updatedAt" : "2020-06-15T16:27:46Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "1ba20fc4-4c7e-4090-a309-e0a1bf2e71a0",
        "parentId" : "00d65cda-8792-46ec-9ff8-d5f733222f6b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think using oldType/newType is fine here since then we do not need to keep in mind of the ordering of each call?",
        "createdAt" : "2020-06-15T17:55:49Z",
        "updatedAt" : "2020-06-15T17:55:50Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfbadff2e1fb81ff65f89b327cb668f1e22f4090",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +515,519 @@        logPrefix = logContext.logPrefix();\n\n        log.debug(\"Transitioning state manager for {} task {} to {}\", oldType, taskId, newType);\n    }\n"
  },
  {
    "id" : "30e142ac-0b01-4607-b818-7914340dc7d3",
    "prId" : 8962,
    "prUrl" : "https://github.com/apache/kafka/pull/8962#pullrequestreview-441310945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0721a47-0df9-41cd-a96b-6bb4f8a7334f",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "This is a little awkward, and it might be cleaner to just replace the use of `null` with this `OFFSET_UNKNOWN` sentinel throughout the ProcessorStateManager/StoreChangelogReader -- but, I wanted to keep the changes as short and simple as possible for now",
        "createdAt" : "2020-07-02T02:35:51Z",
        "updatedAt" : "2020-07-06T19:48:31Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca3274ffc9a1973ed7a4977889e257cc8fc8b67e",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +584,588 @@    // Pass in a sentinel value to checkpoint when the changelog offset is not yet initialized/known\n    private long checkpointableOffsetFromChangelogOffset(final Long offset) {\n        return offset != null ? offset : OFFSET_UNKNOWN;\n    }\n"
  },
  {
    "id" : "b58b2e3b-5a81-4291-b236-1d5c6a04f830",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-462115704",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Admit this is kinda hacky, but I'd have to do this for cached store and suppression buffer. Moving forward I think the first can be removed when we decouple caching with emitting, but for suppression buffer maybe we can have a more general way to fix it, for example maybe we could just have changelogger to always buffer itself so that suppression buffers do not need to buffer itself to changelogger. cc @vvcephei ",
        "createdAt" : "2020-07-07T02:28:02Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d7e5e157-0bf2-422e-9ae1-1286648fc747",
        "parentId" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "WDYT about adding a generic `FlushingRequiredStore` marker interface that all caching state stores and the suppression buffer would both implement. It seems weird to handle them separately.\r\n\r\nWe could even make this public and allow user custom state stores to implement this, but that might be opening a can of worms we will greatly regret 😉 ",
        "createdAt" : "2020-07-24T18:12:18Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9f1ccfd0-51aa-47cc-bae7-d402180b596e",
        "parentId" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "My plan is actually to remove the `flushCache` once we decoupled caching with emitting (see the TODO comment on the caller).",
        "createdAt" : "2020-07-27T00:16:49Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "b2b63ce0-ece6-4565-a98d-4f8a77d071cb",
        "parentId" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "So you're saying we'd still need to flush the suppression buffer but not the cache once we decouple caching from emitting? Or that we can remove this `flushCache` method altogether once that is done? Or that it will still do some flushing, but will not resemble the current `flushCache` method at all",
        "createdAt" : "2020-07-27T23:36:31Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "56af302b-52f0-460d-8d0f-74c0ecd0bf0b",
        "parentId" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I'm thinking we can remove the whole `flushCache` method.",
        "createdAt" : "2020-07-28T01:46:48Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "9e14fb46-0484-428a-9072-d368ca756644",
        "parentId" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Is there a ticket for that?",
        "createdAt" : "2020-07-28T17:23:15Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "f402386d-b3e0-4584-9718-032617da5a67",
        "parentId" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I agree. IMHO, we should get rid of KTable/store cache all together and only have a \"changelog-writer-cache\" that regular stores and suppress() can use to reduce the write load on the changelog topic. For downstream rate control, users should use suppress() (and we might want to try to unify suppress() and the upstream store somehow eventually to avoid the current redundancy)",
        "createdAt" : "2020-08-05T23:06:22Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "5daef038-4059-4fea-9a66-0926aa6f6035",
        "parentId" : "ec2e7480-882e-4721-8f36-6331d97b6c63",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I'm going to create the ticket of that very soon.",
        "createdAt" : "2020-08-06T00:50:11Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +476,480 @@                try {\n                    // buffer should be flushed to send all records to changelog\n                    if (store instanceof TimeOrderedKeyValueBuffer) {\n                        store.flush();\n                    } else if (store instanceof CachedStateStore) {"
  },
  {
    "id" : "ef56caeb-0b72-4d04-a6bf-8a2e7f65d153",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-454632520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40ac2bf8-4f52-49ea-a984-0b318e7c1669",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "nit: can you use braces here?",
        "createdAt" : "2020-07-21T17:52:35Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "4c70a4ae-6026-497e-b395-803a50920b81",
        "parentId" : "40ac2bf8-4f52-49ea-a984-0b318e7c1669",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Ack",
        "createdAt" : "2020-07-24T04:48:29Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +488,492 @@                            firstException = exception;\n                        } else {\n                            firstException = new ProcessorStateException(\n                                format(\"%sFailed to flush cache of store %s\", logPrefix, store.name()),\n                                exception"
  },
  {
    "id" : "ab046ccb-4df7-4ffd-a1de-a31e773284fa",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-460366767",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d5a8f2e-0292-4a78-a836-354886c39c1f",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Seems like there's the missing possibility that it's not TimeOrdered or Cached. Should we log a different message than \"Flushed cache or buffer\" in that case, to indicate we _didn't_ flush it?",
        "createdAt" : "2020-07-31T22:08:16Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "ceb8c866-ad3c-40c0-a6b8-b4ba523339cb",
        "parentId" : "5d5a8f2e-0292-4a78-a836-354886c39c1f",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "For stores that's not time-ordered or cached, we should not flush them indeed. In fact moving forward I think we would not flush cache store anyways since they will be removed. I.e. generally speaking we should not `flush cache` always. In that sense the log4j entry looks reasonable to me?",
        "createdAt" : "2020-08-03T21:56:21Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +480,484 @@                    } else if (store instanceof CachedStateStore) {\n                        ((CachedStateStore) store).flushCache();\n                    }\n                    log.trace(\"Flushed cache or buffer {}\", store.name());\n                } catch (final RuntimeException exception) {"
  },
  {
    "id" : "94b30eee-6142-4855-9e1c-11d704b50474",
    "prId" : 8996,
    "prUrl" : "https://github.com/apache/kafka/pull/8996#pullrequestreview-445268833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22eb14a9-c2c3-41d7-a3a1-d4df6d99f5e8",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Don't we need to call `store.setOffset(null)` or this case?",
        "createdAt" : "2020-07-09T00:39:06Z",
        "updatedAt" : "2020-07-09T17:32:39Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "e6674897-a8f6-4d1e-ae9a-00eb1909d769",
        "parentId" : "22eb14a9-c2c3-41d7-a3a1-d4df6d99f5e8",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Just wondering about https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L769-L802 (can't comment below).\r\n\r\nFor this case, if should hold that `store.offset() == changelogOffsetFromCheckpointedOffset(loadedCheckpoints.remove(store.changelogPartition))` ?\r\n\r\nOr maybe `>=`?\r\n\r\nShould we add a sanity check? (Not related to this PR itself actually. -- Just wondering.)",
        "createdAt" : "2020-07-09T00:42:50Z",
        "updatedAt" : "2020-07-09T17:32:39Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "4e50a8d1-f7d6-4499-aa71-8e916f61e5da",
        "parentId" : "22eb14a9-c2c3-41d7-a3a1-d4df6d99f5e8",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "> Don't we need to call store.setOffset(null) or this case?\r\n\r\nWell, either it's a recycled task in which case no, we don't want to wipe out the existing offset, or it's a new task in which case it's initialized to `null` anyway.\r\n\r\nI'm also not sure what you mean in the second comment. Did you maybe paste the link to the wrong code? (just guessing since you linked to that same code earlier in John's PR)",
        "createdAt" : "2020-07-09T02:04:32Z",
        "updatedAt" : "2020-07-09T17:32:39Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "bfa9044a-0909-48b3-ad01-d54aced96bca",
        "parentId" : "22eb14a9-c2c3-41d7-a3a1-d4df6d99f5e8",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ups. Wrong link. This one: https://github.com/apache/kafka/pull/8996/files#diff-cc98a6c20f2a8483e1849aea6921c34dL251-L255",
        "createdAt" : "2020-07-09T02:22:06Z",
        "updatedAt" : "2020-07-09T17:32:39Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "b0d3130e-44ed-43df-9696-2e527cee3866",
        "parentId" : "22eb14a9-c2c3-41d7-a3a1-d4df6d99f5e8",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "> Well, either it's a recycled task in which case no, \r\n\r\nSo a active->standby or standby->active conversion. I was wondering about corrupted tasks? So we don't call `initializeStoreOffsetsFromCheckpoint` when reviving a corrupted task?",
        "createdAt" : "2020-07-09T02:23:54Z",
        "updatedAt" : "2020-07-09T17:32:39Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "1f53ae13-9b01-46e7-ac52-f3e204805104",
        "parentId" : "22eb14a9-c2c3-41d7-a3a1-d4df6d99f5e8",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "No, because we close the state manager completely and clear all this data before reviving a corrupted task.\r\n\r\nI've had to re-convince myself of this several times already. Maybe we can also add a check that none of them are marked corrupted while initializing offsets",
        "createdAt" : "2020-07-09T03:38:51Z",
        "updatedAt" : "2020-07-09T17:32:39Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f6db6d6dea1a2164ee59895e46ccc268e6a477d",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +232,236 @@                } else if (!store.stateStore.persistent()) {\n                    log.info(\"Initializing to the starting offset for changelog {} of in-memory state store {}\",\n                             store.changelogPartition, store.stateStore.name());\n                } else if (store.offset() == null) {\n                    if (loadedCheckpoints.containsKey(store.changelogPartition)) {"
  },
  {
    "id" : "c206da0b-33ed-49e5-ac72-e16e4c56b367",
    "prId" : 8996,
    "prUrl" : "https://github.com/apache/kafka/pull/8996#pullrequestreview-445965304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a3ca298-c67d-4b25-964f-5ef3c0672828",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I think we should also remove the changelogPartition from loadedCheckpoints, if it exists. Otherwise, we'll spuriously warn in L267.",
        "createdAt" : "2020-07-09T19:07:04Z",
        "updatedAt" : "2020-07-09T19:14:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "8246b677-b890-4500-93b5-b4132daaa600",
        "parentId" : "7a3ca298-c67d-4b25-964f-5ef3c0672828",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I'm not sure I'd call it a spurious warning -- if we don't expect to have checkpointed in-memory stores, and we happen to have an offset for one in the checkpoint file, it seems reasonable to log a warning",
        "createdAt" : "2020-07-09T20:53:16Z",
        "updatedAt" : "2020-07-09T20:53:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "716b3fa2-8dee-46fd-9a67-c8ba31dacf2c",
        "parentId" : "7a3ca298-c67d-4b25-964f-5ef3c0672828",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Fair enough.",
        "createdAt" : "2020-07-09T21:00:01Z",
        "updatedAt" : "2020-07-09T21:00:02Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f6db6d6dea1a2164ee59895e46ccc268e6a477d",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +230,234 @@                if (store.changelogPartition == null) {\n                    log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n                } else if (!store.stateStore.persistent()) {\n                    log.info(\"Initializing to the starting offset for changelog {} of in-memory state store {}\",\n                             store.changelogPartition, store.stateStore.name());"
  },
  {
    "id" : "1fba1dc0-755b-4696-af94-a051c406af46",
    "prId" : 10342,
    "prUrl" : "https://github.com/apache/kafka/pull/10342#pullrequestreview-614858244",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e0ab13a-d5a2-4073-8773-7eed0a020f4f",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Note: the only logical changes are in `StateDirectory` and `KafkaStreams`, the rest of the files were just touched by renaming this method to include the `getOrCreate` prefix. Sorry for the expanded surface area, I felt the renaming was merited since otherwise this critical functionality is easy to miss. \r\n\r\nThere are also some non-renaming changes in StateManagerUtil and TaskManager that just involve removing the try-catch logic for the no longer throwable IOException. For tests, you should focus on `StateDirectoryTest` and `StateManagerUtilTest`",
        "createdAt" : "2021-03-17T23:10:06Z",
        "updatedAt" : "2021-03-30T19:31:35Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "de4dbd1e14ae4345c75c26c6825df6232f310dee",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +187,191 @@        this.sourcePartitions = sourcePartitions;\n\n        this.baseDir = stateDirectory.getOrCreateDirectoryForTask(taskId);\n        this.checkpointFile = new OffsetCheckpoint(stateDirectory.checkpointFileFor(taskId));\n"
  },
  {
    "id" : "20f9412e-07d1-418b-b4ae-d39d9a22f8b3",
    "prId" : 10646,
    "prUrl" : "https://github.com/apache/kafka/pull/10646#pullrequestreview-660945518",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b27dd86-20d1-4d1c-859a-16f89572d3e1",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "See my comment above.",
        "createdAt" : "2021-05-17T13:15:15Z",
        "updatedAt" : "2021-05-17T13:30:55Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "85d28192174125f7c8c641eeb22d4015711473a7",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +335,339 @@        // register the store first, so that if later an exception is thrown then eventually while we call `close`\n        // on the state manager this state store would be closed as well\n        stores.put(storeName, storeMetadata);\n\n        maybeRegisterStoreWithChangelogReader(storeName);"
  }
]