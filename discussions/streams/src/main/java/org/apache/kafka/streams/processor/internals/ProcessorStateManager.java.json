[
  {
    "id" : "0a5f4289-ee03-4364-9333-688c5133a401",
    "prId" : 5187,
    "prUrl" : "https://github.com/apache/kafka/pull/5187#pullrequestreview-127669574",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34c5f689-325a-4d35-b693-4876dad5ca1d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is a Java8 rewrite only.",
        "createdAt" : "2018-06-11T17:42:28Z",
        "updatedAt" : "2018-06-13T21:53:27Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "99b3999656cc1fbebe18159dd2394c1b3494c610",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +169,173 @@            final TopicPartition storePartition = new TopicPartition(topicName, partition);\n\n            partitionsAndOffsets.put(storePartition, checkpointableOffsets.getOrDefault(storePartition, -1L));\n        }\n        return partitionsAndOffsets;"
  },
  {
    "id" : "b631dad6-b050-43dd-b6f1-dff7a132e767",
    "prId" : 5641,
    "prUrl" : "https://github.com/apache/kafka/pull/5641#pullrequestreview-154894631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bfa5066-86c0-458d-bf4e-5a01e74bd5ea",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "do you think it's worth verifying that the map actually is one-to-one?",
        "createdAt" : "2018-09-12T21:00:07Z",
        "updatedAt" : "2018-09-13T00:43:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "aec93bed-cd0d-46a7-8b78-937fd939c9c6",
        "parentId" : "5bfa5066-86c0-458d-bf4e-5a01e74bd5ea",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Should be fine as is IMHO",
        "createdAt" : "2018-09-13T00:42:33Z",
        "updatedAt" : "2018-09-13T00:43:56Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "dac3e52d-e64b-4346-bf5e-d2a4aefcd2b1",
        "parentId" : "5bfa5066-86c0-458d-bf4e-5a01e74bd5ea",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "If you feel strong about it, we can fix in `trunk` -- for `0.11` I will just leave as-is",
        "createdAt" : "2018-09-13T00:43:00Z",
        "updatedAt" : "2018-09-13T00:43:56Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ffefd7b389b4cab42e084cef8028640e90496f4",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +235,239 @@        final Map<String, String> reversedMap = new HashMap<>();\n        for (final Map.Entry<String, String> entry : origin.entrySet()) {\n            reversedMap.put(entry.getValue(), entry.getKey());\n        }\n        return reversedMap;"
  },
  {
    "id" : "186b1058-a6f1-4d75-8a21-572ef0c286ef",
    "prId" : 5657,
    "prUrl" : "https://github.com/apache/kafka/pull/5657#pullrequestreview-156618325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e6bb8b5-d4e7-4071-9cc8-65dc0f3b9ab5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Has this same state store potentially had `init` called previously? If so, do implementers know that init may be called multiple times?",
        "createdAt" : "2018-09-18T22:04:45Z",
        "updatedAt" : "2018-09-18T22:05:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "709e818c-c52f-4876-ad1a-6b568a35b685",
        "parentId" : "2e6bb8b5-d4e7-4071-9cc8-65dc0f3b9ab5",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Actually, I think it's ok calling `init` again as the `stateStore` is closed above on line 288.",
        "createdAt" : "2018-09-18T23:47:09Z",
        "updatedAt" : "2018-09-18T23:47:42Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "7f6557cf-3255-4782-b037-14d826deac15",
        "parentId" : "2e6bb8b5-d4e7-4071-9cc8-65dc0f3b9ab5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "As Bill pointed out -- we close the store above and thus, we can `init()` it again -- it was always like this, as during rebalance, we might close and re-init a store, too.",
        "createdAt" : "2018-09-19T00:24:58Z",
        "updatedAt" : "2018-09-19T00:24:58Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "317e534d50cb26c93dda69261d51295817f9c6a9",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +225,229 @@                }\n\n                stateStore.init(processorContext, stateStore);\n            }\n        }"
  },
  {
    "id" : "77598d6e-03ae-428c-9466-49f4dfa9bd5a",
    "prId" : 6204,
    "prUrl" : "https://github.com/apache/kafka/pull/6204#pullrequestreview-197403293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hmm.. I thought the restorer / state manager would now be agnostic to the convert at all, since the inner store impl is responsible for extending the interface as well as calling its `TimestampedBytesStore` function internally. Why do we still need an internal `RecordConverter` class?",
        "createdAt" : "2019-01-28T04:30:34Z",
        "updatedAt" : "2019-01-29T01:06:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "62413f6b-e8d8-4983-80a2-253c2da3f0a3",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "The new static method that is used by the stores, will translate on-disk data from old to new format.\r\n\r\nWe still need a `RecordConverter` that translates data from the changelog topic to the new format, because on restore, we put plain `<byte[],byte[]>` key-value pairs into the store, and the store expects those record to be in the new format.\r\n\r\nThis make we realize, the the implementation of `RecordConverter` is actually not correct in this PR -- it should not use the new static method (that inserts a `-1` as timestamp), but it need to put the actual record timestamp... Will update the PR accordingly.",
        "createdAt" : "2019-01-29T00:58:32Z",
        "updatedAt" : "2019-01-29T01:06:09Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "54104621-3864-409e-beb3-b08eb7bddc33",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "But can't we make this logic inside the `prepareBatch` call inside the internal store impl (of course we would require customized users to do so as well) so that the callers do not need to be aware of that?",
        "createdAt" : "2019-01-29T01:39:51Z",
        "updatedAt" : "2019-01-29T01:39:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8850d3cd-98b3-4644-9856-16963a81d58b",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "No, because `prepareBatch` (or actually `restoreAllInternal`) takes `KeyValue<byte[],byte[]` but not a `ConsumerRecord`. Thus there is not timestamp information we can add to the value.",
        "createdAt" : "2019-01-29T01:52:20Z",
        "updatedAt" : "2019-01-29T01:52:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "cd2904ef-6beb-4575-a5cc-7347323e17d4",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : ":+1: The `convertToTimestampedFormat` function is purely for converting the already-stored data into the new binary format. It can't actually add any timestamp information, because we don't have it when we retrieve the old-format data. So the function is like `(value) -> [-1,value]`.\r\n\r\nIn contrast, the state restorer needs to be able to insert the timestamp, so the function is like `(value, timestamp) -> [timestamp, value]`.\r\n\r\nActually, looking at this code again, I see Matthias was right, the `RecordConverter` erroneously calls through to the `convertToTimestampedFormat` function. Oops! Good catch!",
        "createdAt" : "2019-01-29T06:14:30Z",
        "updatedAt" : "2019-01-29T06:14:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "f402982f-2d01-4f93-b31d-a5ce1738f6cc",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ah, I just refreshed, and looked at the new code. It LGTM. Thanks!",
        "createdAt" : "2019-01-29T06:19:56Z",
        "updatedAt" : "2019-01-29T06:19:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "3058c615-35ae-4363-8c4b-ab77d0d90ff3",
        "parentId" : "ce2cd52f-a637-4ffb-9fbb-91add18b62f7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "@mjsax Thanks for the explanation! LGTM.",
        "createdAt" : "2019-01-29T07:11:32Z",
        "updatedAt" : "2019-01-29T07:11:32Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2f34bb3907bc00582ab7d70f64e475427090fa5",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +138,142 @@            store instanceof WrappedStateStore ? ((WrappedStateStore) store).inner() : store;\n        final RecordConverter recordConverter =\n            stateStore instanceof TimestampedBytesStore ? RecordConverter.converter() : record -> record;\n\n        if (isStandby) {"
  },
  {
    "id" : "96840ba3-368c-44de-a3eb-a22bf5959ad5",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80684b54-8ab6-4ffb-951d-e89af0bc1182",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "pretty sure this was a typo",
        "createdAt" : "2019-07-03T17:09:08Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +68,72 @@\n    // TODO: this map does not work with customized grouper where multiple partitions\n    // of the same topic can be assigned to the same task.\n    private final Map<String, TopicPartition> partitionForTopic;\n"
  },
  {
    "id" : "b26c753f-4c65-4f6f-aff8-1af0f513d815",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4e13a22-4c99-4831-8eee-1905572e5d77",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "adding some clarity",
        "createdAt" : "2019-07-03T17:09:44Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +114,118 @@\n        if (eosEnabled) {\n            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n            // delete the checkpoint file after loading its stored offsets.\n            checkpointFile.delete();"
  },
  {
    "id" : "b2cc6ba4-6c3e-4450-b1dd-47939bb588fd",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d06282b-382e-4592-8235-135b290a72c0",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It's equivalent, but this makes more sense.",
        "createdAt" : "2019-07-03T17:10:30Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +141,145 @@\n        if (CHECKPOINT_FILE_NAME.equals(storeName)) {\n            throw new IllegalArgumentException(String.format(\"%sIllegal store name: %s\", logPrefix, storeName));\n        }\n"
  },
  {
    "id" : "68a9b66b-822b-46a0-903f-807df46282c1",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f969be5f-411f-4e16-ad01-1bf30d120567",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "this block has nothing to do with adding the store to `registeredStores`, so I moved that operation to after the block for clarity.",
        "createdAt" : "2019-07-03T17:11:20Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +150,154 @@        // check that the underlying change log topic exist or not\n        final String topic = storeToChangelogTopic.get(storeName);\n        if (topic != null) {\n            final TopicPartition storePartition = new TopicPartition(topic, getPartition(topic));\n"
  },
  {
    "id" : "3f85c17f-edff-444b-882e-2ae408c3fc81",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "beccca2d-9e5e-4ca3-a996-071f6af6ed90",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "not necessary to check whether there is a checkpoint file, since it's now encapsulated in `clearCheckpoints`.",
        "createdAt" : "2019-07-03T17:12:18Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 220,
    "diffHunk" : "@@ -1,1 +325,329 @@        }\n\n        if (!clean && eosEnabled) {\n            // delete the checkpoint file if this is an unclean close\n            try {"
  },
  {
    "id" : "0175eddd-c76b-4161-b1d6-2d987b5c2351",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24593b7a-b074-4f2e-bbdf-351999cbdf59",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "The complicated for loop in the old version has the side-effect of checking this condition, so I'm keeping here, but making it explicit.",
        "createdAt" : "2019-07-03T17:13:08Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +341,345 @@    @Override\n    public void checkpoint(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n        ensureStoresRegistered();\n\n        // write the checkpoint file before closing"
  },
  {
    "id" : "ebace2f6-6e5c-46ac-b05d-8c2c8644e9e3",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "671371f8-5445-4af2-bbe0-1c007e7c1788",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Also here, no longer sharing mutable state between super and sub classes.",
        "createdAt" : "2019-07-03T20:59:43Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +47,51 @@\n\npublic class ProcessorStateManager implements StateManager {\n    private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n"
  },
  {
    "id" : "cf7a864d-217b-4fb7-aaf2-16dd0034d5da",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2eaddb4-bc41-46bc-84ac-fdcf1a59bbc5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Adding this collection breaks a circular dependency in this class:\r\n* the checkpoints we load from disk are potentially not valid for the current topology\r\n* we have to load the checkpoints immediately because we have to delete the checkpoint file before processing in the case of EOS\r\n* we also need to have read the checkpoint file _before_ registering stores, since it might be needed to create a restorer\r\n* we can't know if a checkpoint from the file is valid until _after_ registering stores \r\n\r\nIn other words, if the prior code wanted to validate the loaded checkpoints, it would have to register the stores before loading checkpoints, but it also needs to load the checkpoints before registering the stores.\r\n\r\nWe're breaking the cycle here by keeping the loaded checkpoints separate. Now we read the checkpoint file into `initialLoadedCheckpoints`, which is used to register the stores, and then we are able to make sure that we only ever write valid checkpoints into the `checkpointFileCache`, which is used to update the checkpoint file later on.",
        "createdAt" : "2019-07-03T22:04:29Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +75,79 @@    private OffsetCheckpoint checkpointFile;\n    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n\n    /**"
  },
  {
    "id" : "cc0b1306-89f4-488d-9887-850224521397",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6195434-0b07-4c95-8197-c3700c7ebef6",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This is where we're using the _loaded_ checkpoint for store registration. Note the missing condition which is now handled... if the store is not persistent, it should _not_ use the loaded checkpoint.",
        "createdAt" : "2019-07-03T22:05:52Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 110,
    "diffHunk" : "@@ -1,1 +161,165 @@                recordConverters.put(topic, recordConverter);\n            } else {\n                final Long restoreCheckpoint = store.persistent() ? initialLoadedCheckpoints.get(storePartition) : null;\n                if (restoreCheckpoint != null) {\n                    checkpointFileCache.put(storePartition, restoreCheckpoint);"
  },
  {
    "id" : "1369e674-a9ad-4ba8-8318-bad3de8858f3",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6fb24cc-fe3f-41a8-b6a0-6f5838a59e97",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "encapsulating this operation so that outside classes don't have to directly mutate our `checkpointFile` field.",
        "createdAt" : "2019-07-03T22:06:39Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +200,204 @@    }\n\n    void clearCheckpoints() throws IOException {\n        if (checkpointFile != null) {\n            checkpointFile.delete();"
  },
  {
    "id" : "79420b0d-5cc7-4058-b0a0-c8d7c60a4f56",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257732570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bce62d1-8d73-4a30-8596-24d2eb961d66",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "We didn't previously clear the cache on the blocks that this method replaces, but after reading the code, I'm pretty sure this is the right thing to do.",
        "createdAt" : "2019-07-03T22:07:33Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +205,209 @@            checkpointFile = null;\n\n            checkpointFileCache.clear();\n        }\n    }"
  },
  {
    "id" : "f0b1a216-0db1-4b28-a38c-f031edd1c05f",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-258333947",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1721e9dc-4eb6-4cae-af75-a6df7eac861e",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "I would assign `storeToChangelog.getKey()` to a variable called `storeName` to make the code more readable.",
        "createdAt" : "2019-07-05T10:24:52Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 326,
    "diffHunk" : "@@ -1,1 +427,431 @@\n        final Set<TopicPartition> result = new HashSet<>(storeToChangelogTopic.size());\n        for (final Map.Entry<String, String> storeToChangelog : storeToChangelogTopic.entrySet()) {\n            final String storeName = storeToChangelog.getKey();\n            if (registeredStores.containsKey(storeName)"
  },
  {
    "id" : "dfda4471-2aeb-4741-a58f-e9f231ce8498",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-259804929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "530d25a2-f8f0-4690-8db0-4d28c30c3f1f",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "In this method, you call `validCheckpointableTopics` three times, once directly and twice within `validCheckpointableOffsets`. Wouldn't it make sense to call it once in the beginning and passing the result into `validCheckpointableOffsets`?",
        "createdAt" : "2019-07-05T10:35:57Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "99c95209-36d3-4da1-920c-05b018dc94ed",
        "parentId" : "530d25a2-f8f0-4690-8db0-4d28c30c3f1f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "yep. done.",
        "createdAt" : "2019-07-09T23:17:35Z",
        "updatedAt" : "2019-07-09T23:17:36Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "b16e9031-f68a-40f1-bbd6-d6bc452d8b41",
        "parentId" : "530d25a2-f8f0-4690-8db0-4d28c30c3f1f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "thanks for catching this.",
        "createdAt" : "2019-07-09T23:17:47Z",
        "updatedAt" : "2019-07-09T23:17:47Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 285,
    "diffHunk" : "@@ -1,1 +360,364 @@    }\n\n    private void updateCheckpointFileCache(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n        final Set<TopicPartition> validCheckpointableTopics = validCheckpointableTopics();\n        final Map<TopicPartition, Long> restoredOffsets = validCheckpointableOffsets("
  },
  {
    "id" : "fb7933cc-8d96-4fae-92fe-d989e8e60602",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-259802857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06b46d6e-692f-446b-92c8-94d6c42feb37",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Wouldn't it be more meaningful to rename this class to `TaskStateManager`? ",
        "createdAt" : "2019-07-05T12:52:28Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "3b6a6d9d-7082-4990-ab40-e303d690757d",
        "parentId" : "06b46d6e-692f-446b-92c8-94d6c42feb37",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Maybe, I'm not sure of the historical reason to name it this way.",
        "createdAt" : "2019-07-09T23:09:38Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +47,51 @@\n\npublic class ProcessorStateManager implements StateManager {\n    private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n"
  },
  {
    "id" : "879fa8a1-32dc-4b4f-ab7a-a912e413cb9c",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-260341432",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "921f9ebe-90d6-4c4d-bf4d-f2988e668135",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "If the store is not peristent or the read checkpoint file does not contain the partition, this will throw a NPE, right? If yes, you should add unit tests for these cases.",
        "createdAt" : "2019-07-05T14:45:55Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "b0d0a17b-c98b-4dbd-a358-232bb37099e2",
        "parentId" : "921f9ebe-90d6-4c4d-bf4d-f2988e668135",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "No, I guess you were thinking the `Long` would become unboxed at this point? It's actually a `Long` parameter, and the `StateRestorer` constructor checks for null... Not the cleanest code, I guess, but it looks like it's been this way since 2017.",
        "createdAt" : "2019-07-09T23:15:45Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "fb3afb05-9b1d-4129-abb5-3bd52dd60037",
        "parentId" : "921f9ebe-90d6-4c4d-bf4d-f2988e668135",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.",
        "createdAt" : "2019-07-10T20:35:11Z",
        "updatedAt" : "2019-07-10T20:35:11Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +170,174 @@                    storePartition,\n                    new CompositeRestoreListener(stateRestoreCallback),\n                    restoreCheckpoint,\n                    offsetLimit(storePartition),\n                    store.persistent(),"
  },
  {
    "id" : "61429fa6-5209-4dde-b05a-81fee303cc9d",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-263363652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2cf7a8d-01a9-4b06-baeb-d1a78b928b6f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I liked this refactoring a lot, thanks @vvcephei !",
        "createdAt" : "2019-07-17T23:18:30Z",
        "updatedAt" : "2019-07-17T23:40:55Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +188,192 @@    public void reinitializeStateStoresForPartitions(final Collection<TopicPartition> partitions,\n                                                     final InternalProcessorContext processorContext) {\n        StateManagerUtil.reinitializeStateStoresForPartitions(log,\n                                                              eosEnabled,\n                                                              baseDir,"
  },
  {
    "id" : "4ba08fd2-6129-4a59-988d-03ee85eaba4d",
    "prId" : 7304,
    "prUrl" : "https://github.com/apache/kafka/pull/7304#pullrequestreview-288712381",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "329b5396-f7a3-4a0e-9e65-d916758a80f8",
        "parentId" : null,
        "authorId" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "body" : "Thanks!",
        "createdAt" : "2019-09-16T15:38:26Z",
        "updatedAt" : "2019-09-20T20:04:08Z",
        "lastEditedBy" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bc8bf0f0703bf557bff6c8ae5c9a347599c8d4f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +253,257 @@    }\n\n    private long offsetLimit(final TopicPartition partition) {\n        final Long limit = offsetLimits.get(partition);\n        return limit != null ? limit : Long.MAX_VALUE;"
  },
  {
    "id" : "db803cf1-926b-4cdb-9b47-1c04854eabe6",
    "prId" : 7681,
    "prUrl" : "https://github.com/apache/kafka/pull/7681#pullrequestreview-316029454",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6e62f00-d139-413e-a5cf-97f2ba0504ce",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Removed a redundant log line and upgraded this log to DEBUG.\r\n\r\nSystem tests run at DEBUG level, so this change makes this log visible to the system tests. Ideally, I could have just turned this class's logger to TRACE, but it happens to be a real pain with the way the system tests are set up.\r\n\r\nI think this is reasonable, though, since this is not a high-volume log message, and it's pretty handy to know this information while debugging state restoration and initialization.",
        "createdAt" : "2019-11-13T06:49:52Z",
        "updatedAt" : "2019-11-13T21:20:04Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "741306599487c10a3d5f753920f1f1c2e0e0ec73",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +349,353 @@        updateCheckpointFileCache(checkpointableOffsetsFromProcessing);\n\n        log.debug(\"Writing checkpoint: {}\", checkpointFileCache);\n        try {\n            checkpointFile.write(checkpointFileCache);"
  },
  {
    "id" : "566527f8-9089-4927-aca5-53233ab1948b",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353327753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0855a3b1-193e-43b3-aaf4-73a155115248",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "unresolved TODO here. it's unclear to me why we would only consider this task corrupted in EOS mode. It seems like the lack of a checkpoint _file_ just means that we loaded a cached task in an undefined state, and we should discard it and restore. If we have the file, but some of a changelog is missing from it, then maybe it just never got written, before shutdown, or more likely, the topology has changed and the task is corrupted, whether or not we are in EOS.",
        "createdAt" : "2020-02-04T21:32:26Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "0edbcd27-5069-4bc2-8c09-c75256a7b268",
        "parentId" : "0855a3b1-193e-43b3-aaf4-73a155115248",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Today we write the checkpoint file before we commit the offsets, so under non-EOS we can just restore from scratch without wiping the local store image since restoring is just overwriting the local store and we can just restore to the end of the changelog (this may result in duplicates but is fine under non-EOS); but with non-EOS we have to first wipe out before restoring from scratch.",
        "createdAt" : "2020-02-04T21:51:49Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 253,
    "diffHunk" : "@@ -1,1 +207,211 @@                            store.stateStore.name(), store.offset, store.changelogPartition);\n                    } else {\n                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted\n\n                        log.info(\"State store {} did not find checkpoint offset, hence would \" +"
  },
  {
    "id" : "be975924-db34-4584-a635-871ef987f5fb",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353328951",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4aac5bea-d92e-42e1-a41a-a53ca78bc10a",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "seems like this also indicates the task is corrupted",
        "createdAt" : "2020-02-04T21:32:44Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "d8d640dc-ef3a-4449-ba01-5824e4480cc1",
        "parentId" : "4aac5bea-d92e-42e1-a41a-a53ca78bc10a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Arguably yes, I'm just following the old logic intentionally here -- I think originally we want to be more compatible with topology changes (if some topology optimizations decides that some stores are not needed) but on second thought I think this is not safe either. We can consider making this change in another PR to make it stricter.",
        "createdAt" : "2020-02-04T21:53:50Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 263,
    "diffHunk" : "@@ -1,1 +217,221 @@\n            if (!loadedCheckpoints.isEmpty()) {\n                log.warn(\"Some loaded checkpoint offsets cannot find their corresponding state stores: {}\", loadedCheckpoints);\n            }\n"
  },
  {
    "id" : "65229ae2-c1ed-49ce-b2d6-8d50403460e4",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353330547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cae7aa2-ca5a-4431-bc00-9f069f86e631",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "in retrospect, I like the idea of doing this regardless of EOS. Why should we deliberately produce wrong results in ALO mode? We can certainly optimize to be able to use semi-trustworthy data, but let's treat that separately from EOS vs ALO",
        "createdAt" : "2020-02-04T21:33:09Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "b5b9aab6-4c3c-44a4-ab8f-413572284d3e",
        "parentId" : "5cae7aa2-ca5a-4431-bc00-9f069f86e631",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The idea is that for ALO if we failed to write the first checkpoint after restarting we can still fallback to the original checkpoint even though the store may have been updated and hence there would be duplicates. But since the window gap (just one commit interval) is so small I think it does not worth making the code more complicated with EO v.s. ALO.",
        "createdAt" : "2020-02-04T21:56:35Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 266,
    "diffHunk" : "@@ -1,1 +220,224 @@            }\n\n            checkpointFile.delete();\n        } catch (final IOException | RuntimeException e) {\n            // both IOException or runtime exception like number parsing can throw"
  },
  {
    "id" : "5c97f608-cfe2-4155-a6bb-50ba547bdd1b",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353316379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74d020d6-b9d5-495a-b9fb-fd76ad89f0e3",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "these kinds of comments paradoxically lead to unmaintainability. Either a method is part of the public contract or not. If it is, then this comment would become out of date, if not, then the changelog reader shouldn't be using it. The recommendation is simply to delete the comment (and similar ones). This even applies to \"for testing\" comments. I have found several methods in use in this code base that were commented \"visible for testing\". Even for tests, either move both the class and the test into an isolated package and use package-private, refactor the test, or remove the comment. Also, if the changelog reader really needs four \"holes\" poked into this class, then we should reconsider the relationship between the state manager and the changelog reader.",
        "createdAt" : "2020-02-04T21:34:55Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 406,
    "diffHunk" : "@@ -1,1 +303,307 @@    }\n\n    // used by the changelog reader only\n    boolean changelogAsSource(final TopicPartition partition) {\n        return sourcePartitions.contains(partition);"
  },
  {
    "id" : "8092790a-2f03-4b70-8166-bfd8e703c2c2",
    "prId" : 7997,
    "prUrl" : "https://github.com/apache/kafka/pull/7997#pullrequestreview-353331803",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9619fa9-5b89-4112-92f3-6a7b05eee8a4",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "maybe it doesn't matter, but this method makes the only usage an n^2 algorithm. If we instead inverted the `stores` collection and used it for lookups in `register`, it would be o(n)",
        "createdAt" : "2020-02-04T21:37:40Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "ef8656fa-4d6f-464c-975c-dfca5eb06f6f",
        "parentId" : "a9619fa9-5b89-4112-92f3-6a7b05eee8a4",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I've thought about it: inverting the `stores` collection makes other calls that depends on the store name more complicated, while keeping two collections indexed by storeName / changelog partition is not much worthy since within a task there are usually no more than 10 stores so this n^2 algorithm should not be a big deal.",
        "createdAt" : "2020-02-04T21:58:52Z",
        "updatedAt" : "2020-02-05T05:05:33Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "196e7c0210b659ec81335d5624ae0cf5127fdf21",
    "line" : 722,
    "diffHunk" : "@@ -1,1 +456,460 @@    }\n\n    private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n        final List<StateStoreMetadata> found = stores.values().stream()\n            .filter(metadata -> changelogPartition.equals(metadata.changelogPartition))"
  },
  {
    "id" : "8ae774b0-8d73-494e-bd09-1434fc2e4a1b",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-355298844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7fcf1f5-bcc3-4c5e-8861-4d6701a5bb10",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We need to clear the stores map now since we may re-initialize the state stores upon reviving a task.",
        "createdAt" : "2020-02-07T00:49:09Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "ff1b145e-79c6-4241-83e9-6298974563d2",
        "parentId" : "f7fcf1f5-bcc3-4c5e-8861-4d6701a5bb10",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Do we also need to clear `storeToChangelogTopic`, etc?",
        "createdAt" : "2020-02-07T01:27:42Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9f00311a-35e9-4350-a0c1-a94c47450f76",
        "parentId" : "f7fcf1f5-bcc3-4c5e-8861-4d6701a5bb10",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`storeToChangelogTopic` and `sourcePartitions` are passed in at construction time and final, so we cannot clear them (since they would only be initialized once).",
        "createdAt" : "2020-02-07T16:58:45Z",
        "updatedAt" : "2020-02-20T23:04:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +436,440 @@            }\n\n            stores.clear();\n        }\n"
  }
]