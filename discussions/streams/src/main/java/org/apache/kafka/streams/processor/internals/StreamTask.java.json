[
  {
    "id" : "929c634f-bcc8-4f18-923a-fc30eade8968",
    "prId" : 4826,
    "prUrl" : "https://github.com/apache/kafka/pull/4826#pullrequestreview-109814681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4c581c0-1e9a-4f7d-b8e4-fd1bf6471d77",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This comment is not for this line but for line 361 above (it is not introduced in this PR but we can tighten the screws a bit more along side):\r\n\r\nwe should switch the line `transactionInFlight = true;` to go after `producer.beginTransaction();`\r\n",
        "createdAt" : "2018-04-05T17:56:28Z",
        "updatedAt" : "2018-04-05T22:09:17Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fcd3e0ec75d3e5133c97a33246e248268acebdb6",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +483,487 @@                if (!clean) {\n                    try {\n                        if (!isZombie && transactionInFlight) {\n                            producer.abortTransaction();\n                        }"
  },
  {
    "id" : "213d344a-0b57-463b-bbac-dedbd26bb49d",
    "prId" : 5398,
    "prUrl" : "https://github.com/apache/kafka/pull/5398#pullrequestreview-145330266",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Can you open PRs for older branches to back-port this fix?",
        "createdAt" : "2018-07-20T17:34:44Z",
        "updatedAt" : "2018-08-02T22:06:29Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "e2bacee0-cf66-4473-8b62-85ac2501d8a2",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "yup.",
        "createdAt" : "2018-07-20T18:24:20Z",
        "updatedAt" : "2018-08-02T22:06:29Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "91920164-d9f4-469e-a6c1-58f6502e8f77",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "ah geez; good catch.",
        "createdAt" : "2018-07-31T21:50:26Z",
        "updatedAt" : "2018-08-02T22:06:29Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "348b918c-3cb8-47f2-855a-41fb7df75117",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "FWIW, it wouldn't break anything. This is only the name of the sensor, which just needs to be unique, and it's just as unique in this order, unless some other task is named \"commit\" (which wouldn't happen).",
        "createdAt" : "2018-08-02T18:18:29Z",
        "updatedAt" : "2018-08-02T22:06:29Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "6a7b6ba5-3442-4705-a36a-cfa625e8d634",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "To be clear, we should fix it, just not sure if we need to bother backporting.",
        "createdAt" : "2018-08-02T18:19:16Z",
        "updatedAt" : "2018-08-02T22:06:29Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "06b118a7-ad80-4c5f-9f3e-143fe0547e94",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Cannot follow the argument? It seems to be an easy fix and we should back port all fixes by default and only not back port if there is a good reason for not doing it.\r\n\r\n@guozhangwang Is there already a PR or ticket so this is not dropped on the floor?",
        "createdAt" : "2018-08-02T21:26:33Z",
        "updatedAt" : "2018-08-02T22:06:29Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "4b7e79ea-b67d-4881-82b4-9095a2c5a49a",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "@vvcephei I think it did break sth.. the sensorName is used as part of `fullSensorName = key + \".\" + sensorName;`. With `sensorName == taskName`, we will not add them since `Sensor s = getSensor(name);` already exists, i.e. we will lose some metrics, right?",
        "createdAt" : "2018-08-02T21:28:10Z",
        "updatedAt" : "2018-08-02T22:06:29Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "fd2361c9-8286-4f6f-b776-f5728e85d7eb",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "@mjsax fair enough!\r\n\r\n@guozhangwang I think you're right. The full sensor name is still unique, but it would mess up the ability to unload sensors (we basically wouldn't be able to unload any task level sensors). My bad.",
        "createdAt" : "2018-08-03T22:31:31Z",
        "updatedAt" : "2018-08-03T22:31:31Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "625a59ba-9308-4ff2-95f7-7679114c92eb",
        "parentId" : "0a0a615c-9fea-40f4-9fb1-b5f41f647ea5",
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "Maybe create (internal) SensorName type (alias for String) so that there is no chance of mixup in the future.",
        "createdAt" : "2018-08-10T17:25:08Z",
        "updatedAt" : "2018-08-10T17:25:09Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c57cec79fa53032b55dd7a5f374c9ee62c25098",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +113,117 @@            // add the operation metrics with additional tags\n            final Map<String, String> tagMap = metrics.tagMap(\"task-id\", taskName);\n            taskCommitTimeSensor = metrics.taskLevelSensor(taskName, \"commit\", Sensor.RecordingLevel.DEBUG, parent);\n            taskCommitTimeSensor.add(\n                new MetricName(\"commit-latency-avg\", group, \"The average latency of commit operation.\", tagMap),"
  },
  {
    "id" : "f7fee4d1-0d0d-4a9a-bc3b-e714f6ccede1",
    "prId" : 5428,
    "prUrl" : "https://github.com/apache/kafka/pull/5428#pullrequestreview-148681869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1a991cc-5f3a-4cbc-913c-be5b9b5e5e13",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "In trunk, the ordering of calls for the `producer` during a commit was broken up, but now they are all grouped together.  It seems ok to do this and is cleaner to follow, I just wanted to double check the change of ordering doesn't matter. \r\n\r\nMaybe we should run system tests to confirm?",
        "createdAt" : "2018-08-07T18:54:08Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "58bc5045-4e53-4bfb-96ed-4d30088603aa",
        "parentId" : "b1a991cc-5f3a-4cbc-913c-be5b9b5e5e13",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good point, I will run the system test accordingly.",
        "createdAt" : "2018-08-08T20:37:54Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0c8bf06a-41c9-4ee5-97bb-0c9c13c485f0",
        "parentId" : "b1a991cc-5f3a-4cbc-913c-be5b9b5e5e13",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "It was broken apart because we checked if there is anything to commit in the first place (ie, do the check on one place)-- if we did not process any data, we don't need to commit.\r\n\r\nThis check now happens outside of `StreamTask` as pointed out by Guozhang https://github.com/apache/kafka/pull/5428/files#r212395430 Thus, regrouping makes sense. Code is cleaner this way.",
        "createdAt" : "2018-08-22T22:13:22Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "71b2b16d0b0f37a05af3df60ae9b5ff88649a7a4",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +455,459 @@\n        try {\n            if (eosEnabled) {\n                producer.sendOffsetsToTransaction(consumedOffsetsAndMetadata, applicationId);\n                producer.commitTransaction();"
  },
  {
    "id" : "3a78173f-9786-438b-853e-0bf000e71d92",
    "prId" : 5428,
    "prUrl" : "https://github.com/apache/kafka/pull/5428#pullrequestreview-149953014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "is this just because punctuations might result in context.forwards?",
        "createdAt" : "2018-08-10T16:09:53Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "f10a947a-6cef-48e9-a5ee-4df87f5def99",
        "parentId" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Actually it is because users can call `context.commit()` in either ` punctuate()` or `process()` calls.",
        "createdAt" : "2018-08-10T23:24:16Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "adebb24d-126e-4227-911a-8b920ba318af",
        "parentId" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Hmmm... if there is nothing to commit, it might also be fine to ignore the user commit request? It's a tricky question what to do for this case. Just follow what the user demands, or be smart? From a correctness point of view, it should not make a difference, would it?\r\n\r\nAlso, we set flag `commitRequested` for this case -- thus, it might be better to put this logic somewhere else? Eg: `AbstractTask` or overwrite in `StreamTask`:\r\n```\r\npublic boolean commitNeeded() {\r\n    return commitNeeded || commitRequested;\r\n}\r\n```\r\n\r\n (An alternative, that I like less would be to add a check if `commitRequested==true`)?",
        "createdAt" : "2018-08-22T22:21:57Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "68279723-cbca-49e6-a0d6-eaf9650bd113",
        "parentId" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Not sure I follow your comment here.. let me elaborate a bit on my logic:\r\n\r\nWe have two commits in places: commitAll (periodic) and maybeCommit (for user requested):\r\n\r\nThe latter checks\r\n\r\n```\r\nif (task.commitRequested() && task.commitNeeded()) \r\n                    task.commit();\r\n```\r\n\r\nWhile the former only checks:\r\n\r\n```\r\nif (task.commitNeeded()) \r\n                    task.commit();\r\n```\r\n\r\nI.e. the logic for the latter is that \"only if user have requested, and it is indeed needed to commit\": for example, if we have actually committed from the commit interval, and then user requested it as well, the second will be omitted.\r\n\r\nI intentionally separated \"commitRequest\" (this is only set by user) and \"commitNeeded\" (this is determined by the library) because this way looks cleaner to me.",
        "createdAt" : "2018-08-23T17:47:42Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "7bac8d5e-b9fc-400f-9680-518fe4a244ec",
        "parentId" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "So setting `commitNeeded` is a conservative approach, because we don't know what the user did within punctuation call? Might be better to set `commitNeeded` if user calls `context.forward` or `state.put()` -- not sure how hard this would be -- would also be out-of-scope for this PR. If we think it might be worth it, we should create a JIRA for this optimization.",
        "createdAt" : "2018-08-23T18:03:45Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "7ad87686-7aed-4d3f-bc87-570cd0e3b0b3",
        "parentId" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Thought on my last comment?",
        "createdAt" : "2018-08-25T00:14:15Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "026e5009-286a-4e49-94a4-3116e7a813b1",
        "parentId" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We still need to commit even if no records are processed: consider a topology which only contains a single source node, then no data processed at all, but we still want to commit so that we would not re-process them right?",
        "createdAt" : "2018-08-27T22:59:59Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "12d3cecb-4c2c-49a3-989f-d5f29b765aa7",
        "parentId" : "6085021a-6973-4ed8-99f0-f8743e5c4b03",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Fair enough. Thanks for pointing it out.",
        "createdAt" : "2018-08-28T03:11:36Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "71b2b16d0b0f37a05af3df60ae9b5ff88649a7a4",
    "line" : 239,
    "diffHunk" : "@@ -1,1 +794,798 @@\n            if (punctuated) {\n                commitNeeded = true;\n            }\n"
  },
  {
    "id" : "f197a9dd-1706-4641-bede-d92e8d377cb2",
    "prId" : 5428,
    "prUrl" : "https://github.com/apache/kafka/pull/5428#pullrequestreview-149019658",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "603e1f0c-423b-4c03-9709-a2ff6b308346",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why do we not check `if(commitNeeded)` any longer?",
        "createdAt" : "2018-08-22T22:09:07Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "73f6657b-a2b8-4158-a3e5-43cbf0addbb5",
        "parentId" : "603e1f0c-423b-4c03-9709-a2ff6b308346",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We check this in the AssignedTasks now: if no commit is needed, we skip the whole committing function, including commit offsets, flushing stores, etc.",
        "createdAt" : "2018-08-23T17:37:20Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "71b2b16d0b0f37a05af3df60ae9b5ff88649a7a4",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +446,450 @@        }\n\n        final Map<TopicPartition, OffsetAndMetadata> consumedOffsetsAndMetadata = new HashMap<>(consumedOffsets.size());\n        for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n            final TopicPartition partition = entry.getKey();"
  },
  {
    "id" : "795f0b14-a972-492a-836c-cd31921723a4",
    "prId" : 5428,
    "prUrl" : "https://github.com/apache/kafka/pull/5428#pullrequestreview-153876402",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c3fb631-27f5-4af1-bd37-117ca6c0a451",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: should it be `>` instead of `>=` ?",
        "createdAt" : "2018-09-06T18:38:24Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "44d1fd3f-d6c8-404a-a560-a6f32c9fb36c",
        "parentId" : "5c3fb631-27f5-4af1-bd37-117ca6c0a451",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "There was an early comment on the test code that suggests `>=`. Personally I think it does not make a big difference at all.",
        "createdAt" : "2018-09-07T17:32:21Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "721169e2-bf17-443c-86cb-1eaaa9890cd6",
        "parentId" : "5c3fb631-27f5-4af1-bd37-117ca6c0a451",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I am to blame for this suggestion. I agree it doesn't make a big difference.\r\nThe reasoning was that if it's the \"maximum idle time\", then you shouldn't idle longer than it, otherwise, it's not really a maximum.",
        "createdAt" : "2018-09-07T19:38:19Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "c5c3bdaf-3f6a-4983-94d0-9a65a92a21d7",
        "parentId" : "5c3fb631-27f5-4af1-bd37-117ca6c0a451",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I agree that it does not matter too much :) (that why it's a nit)\r\n\r\nHowever, I think that the maximum is inclusive, and only if we exceed it, we should force processing. From my understanding, \"maximum idle time\" is actually a lower bound (-> don't force processing until this time passed) because we cannot guarantee anyway to not exceed this threshold. I see your point why the name might be counter intuitive (even if I think the name is correct). If you interpret the name strictly, we would be allowed (or actually we would be required) to force processing before the time passed. This interpretation would make the parameter useless (ie, user tells us to idle max 5 minutes and we obey by forcing processing after 1 minute).\r\n\r\nTo me, the right interpretation is, \"wait until this time passed and force processing asap if the time is exceeded\". Chaning the name to `min.idle.time.ms` would be more precise, but I think it would be more confusing to users. ",
        "createdAt" : "2018-09-08T21:50:01Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "6004d97c-77aa-419e-afe1-9076c0787dfd",
        "parentId" : "5c3fb631-27f5-4af1-bd37-117ca6c0a451",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Okay guys, I'm going to make a final call here to end the discussion: I'm staying with `max.idle..` since I feel it is easier to understand for users, and be aware that this is not strictly respected in practice unless it is set to `0`. Also I'm staying with `>=` since again, it is easier to understand though not strictly sound mathematically.",
        "createdAt" : "2018-09-10T17:09:01Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "59407b08-636d-476d-a615-3e54b7a3064a",
        "parentId" : "5c3fb631-27f5-4af1-bd37-117ca6c0a451",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : ":) Fair enough.",
        "createdAt" : "2018-09-10T17:17:35Z",
        "updatedAt" : "2018-09-11T21:32:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "71b2b16d0b0f37a05af3df60ae9b5ff88649a7a4",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +313,317 @@            }\n\n            if (now - idleStartTime >= maxTaskIdleMs) {\n                taskMetrics.taskEnforcedProcessSensor.record();\n                return true;"
  },
  {
    "id" : "e8d7ebaa-9c5f-4605-9012-57321f53dca2",
    "prId" : 6115,
    "prUrl" : "https://github.com/apache/kafka/pull/6115#pullrequestreview-194301968",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8f4260f-0f04-4e95-a167-a37cddb688b6",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I seems, we should remove writing the checkpoint file in `closeStateManager`, too? Note, that `suspend()` will be called anyway (during rebalance, and during shutdown)\r\n\r\nAlso, why do we write the checkpoint if EOS is enabled only? To me, it seems we can write the checkpoint file if EOS is disabled, too?\r\n\r\n\\cc @guozhangwang ",
        "createdAt" : "2019-01-14T17:50:16Z",
        "updatedAt" : "2019-02-23T05:48:22Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "bedc648a-5e0a-4ec3-ad4e-ab90ea5fba00",
        "parentId" : "e8f4260f-0f04-4e95-a167-a37cddb688b6",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "@mjsax  Because in the #commit() function we shall write checkpoint file if EOS is not turned on. So we are avoiding writing checkpoint file twice in consecutive.",
        "createdAt" : "2019-01-14T18:57:21Z",
        "updatedAt" : "2019-02-23T05:48:22Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "ec262be5-55ca-4cff-878b-f7ea6f03fe12",
        "parentId" : "e8f4260f-0f04-4e95-a167-a37cddb688b6",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ack. Might be worth to add this to the comment.",
        "createdAt" : "2019-01-16T17:56:12Z",
        "updatedAt" : "2019-02-23T05:48:22Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "eac86d98-83a4-44c3-bc79-ef1afe0b24e5",
        "parentId" : "e8f4260f-0f04-4e95-a167-a37cddb688b6",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sure, will do!",
        "createdAt" : "2019-01-18T01:57:56Z",
        "updatedAt" : "2019-02-23T05:48:22Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "692b0906-5f4b-4f73-a23a-62eb31f5135b",
        "parentId" : "e8f4260f-0f04-4e95-a167-a37cddb688b6",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "What about:\r\n\r\n> I seems, we should remove writing the checkpoint file in closeStateManager, too? Note, that suspend() will be called anyway (during rebalance, and during shutdown)",
        "createdAt" : "2019-01-18T18:55:31Z",
        "updatedAt" : "2019-02-23T05:48:22Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "442b3e70-1fd7-492e-bf65-822ff6aac5f7",
        "parentId" : "e8f4260f-0f04-4e95-a167-a37cddb688b6",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I see, will address this!",
        "createdAt" : "2019-01-18T22:08:10Z",
        "updatedAt" : "2019-02-23T05:48:22Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "aefb0007849d918b9d0eec09c5f4ffcf92ad3aeb",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +580,584 @@                if (eosEnabled) {\n\n                    stateMgr.checkpoint(activeTaskCheckpointableOffsets());\n\n                    try {"
  },
  {
    "id" : "5fde03e7-0fe0-4968-b841-e4f1a839d523",
    "prId" : 6310,
    "prUrl" : "https://github.com/apache/kafka/pull/6310#pullrequestreview-206983232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbcf824c-060b-4b21-b623-6c85ad3dd2b8",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Maybe we could inline this, but my personal preference is to have a separate method call for readability.",
        "createdAt" : "2019-02-22T19:14:42Z",
        "updatedAt" : "2019-02-23T16:30:03Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "48ef2b74706f9ceea98186b22ed2a7d8da3918f7",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +394,398 @@    }\n\n    private String getStacktraceString(final KafkaException e) {\n        String stacktrace = null;\n        try (final StringWriter stringWriter = new StringWriter();"
  },
  {
    "id" : "73a26cac-9497-4fa0-9574-81425bb05486",
    "prId" : 6372,
    "prUrl" : "https://github.com/apache/kafka/pull/6372#pullrequestreview-211569989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "501f87ae-9ac0-49fe-b337-4b112bd63b8d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: as above",
        "createdAt" : "2019-03-07T02:08:35Z",
        "updatedAt" : "2019-03-07T23:00:49Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9136d41ec244506da0739e2c2213fdaab4629c7",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +881,885 @@            log.error(\n                \"Timeout exception caught when initializing transactions for task {}. \" +\n                    \"This might happen if the broker is slow to respond, if the network connection to \" +\n                    \"the broker was interrupted, or if similar circumstances arise. \" +\n                    \"You can increase producer parameter `max.block.ms` to increase this timeout.\","
  },
  {
    "id" : "d5e96513-e635-4c6f-995a-8e76e06d8f2c",
    "prId" : 6636,
    "prUrl" : "https://github.com/apache/kafka/pull/6636#pullrequestreview-239206933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6eed79a-e7bc-40ff-8630-25edf8d3cfa7",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "We actually did not call `close()` for this case on purpose. IIRC, the producer contract is to not call _any_ method (not even `close()`) after a `ProducerFencedException` was thrown by the producer, as indicated by the `isZombie` flag.\r\n\r\n\\cc @hachikuji to confirm.",
        "createdAt" : "2019-04-25T18:22:29Z",
        "updatedAt" : "2019-04-25T18:30:44Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "ea871e01-9428-4d87-b8c0-39f120925f9e",
        "parentId" : "a6eed79a-e7bc-40ff-8630-25edf8d3cfa7",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : ">it seems that the old code ignores the `isZombie` flag and calls close() if eosEnable=true\r\n\r\nDo you mean the new code? The previous code only closed the `recordCollector` (which in turn closes the `producer`) when `isZombie=false`.  This was leading to streams leaking producers that never got closed after a rebalance from a fenced producer.",
        "createdAt" : "2019-04-25T18:36:28Z",
        "updatedAt" : "2019-04-25T18:36:29Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "db503ca4-6573-42b2-a4da-36c8b639848d",
        "parentId" : "a6eed79a-e7bc-40ff-8630-25edf8d3cfa7",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I was confused initially -- sorry for the confusion -- deleted my other comment already...",
        "createdAt" : "2019-04-25T18:40:02Z",
        "updatedAt" : "2019-04-25T18:40:03Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fb96183e-ab39-43bf-99af-7c1cef4578a2",
        "parentId" : "a6eed79a-e7bc-40ff-8630-25edf8d3cfa7",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "@mjsax @bbejeck Just to clarify, I think we should always close the producer even if it was fenced. Resources like network connections only get cleaned up in `KafkaProducer.close()`.",
        "createdAt" : "2019-05-18T17:16:25Z",
        "updatedAt" : "2019-05-18T17:16:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "711e6c4e-ecc1-4bf8-9c49-e51b5bc0ba2e",
        "parentId" : "a6eed79a-e7bc-40ff-8630-25edf8d3cfa7",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Thanks for confirming @hachikuji!",
        "createdAt" : "2019-05-18T17:46:43Z",
        "updatedAt" : "2019-05-18T17:46:44Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e0e7bc13104cf0308cec2351ec766a1d6a44826",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +638,642 @@        }\n\n        if (eosEnabled) {\n            try {\n                recordCollector.close();"
  },
  {
    "id" : "debe6e2d-ec7d-4fbf-a9a0-efcd706e06ca",
    "prId" : 6694,
    "prUrl" : "https://github.com/apache/kafka/pull/6694#pullrequestreview-272919477",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "629b26ec-0899-40a2-bd66-9fca3cb654c2",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found",
        "createdAt" : "2019-08-08T18:00:42Z",
        "updatedAt" : "2019-09-16T23:28:41Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "d5a65407-fa10-4df4-aef1-93afce1350e3",
        "parentId" : "629b26ec-0899-40a2-bd66-9fca3cb654c2",
        "authorId" : "32d43f1c-0232-4c2f-9548-9c3dfd6f181a",
        "body" : "Done that.",
        "createdAt" : "2019-08-09T02:23:56Z",
        "updatedAt" : "2019-09-16T23:28:41Z",
        "lastEditedBy" : "32d43f1c-0232-4c2f-9548-9c3dfd6f181a",
        "tags" : [
        ]
      }
    ],
    "commit" : "b42a883a14bc3a48a28624e3964be81e4f75d8f5",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +747,751 @@        } else {\n            log.debug(\"No committed timestamp was found in metadata for partition {}\", partition);\n        }\n    }\n"
  },
  {
    "id" : "4ea2f63c-483e-4d0d-bc54-732127c68b8d",
    "prId" : 6694,
    "prUrl" : "https://github.com/apache/kafka/pull/6694#pullrequestreview-272737998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d658414c-5915-4015-a1dc-efb935cdd548",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: add comment `// visible for testing` (same for decodeTimestamp() below)\r\n\r\nAlso add test methods to `StreamTaskTest` to test both methods.",
        "createdAt" : "2019-08-08T18:04:15Z",
        "updatedAt" : "2019-09-16T23:28:41Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b42a883a14bc3a48a28624e3964be81e4f75d8f5",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +932,936 @@\n    // visible for testing\n    String encodeTimestamp(final long partitionTime) {\n        final ByteBuffer buffer = ByteBuffer.allocate(9);\n        buffer.put(LATEST_MAGIC_BYTE);"
  },
  {
    "id" : "e7b080f9-acf3-4e70-8bdd-30fd79db6283",
    "prId" : 6694,
    "prUrl" : "https://github.com/apache/kafka/pull/6694#pullrequestreview-275542672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57a3dfad-f25c-46ff-b6cc-c730215c91f7",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "I would put methods to write and read record metadata in their own classes. Those classes would be kind of SerDes for metadata. Such SerDes would make the code better testable and separates the concerns of a task and reading and writing metadata which are completely independent. It does not need to be done in this PR. I just wanted to mention it. ",
        "createdAt" : "2019-08-15T09:09:53Z",
        "updatedAt" : "2019-09-16T23:28:41Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "a333e4f8-33b2-486c-a7a9-0331b6789712",
        "parentId" : "57a3dfad-f25c-46ff-b6cc-c730215c91f7",
        "authorId" : "32d43f1c-0232-4c2f-9548-9c3dfd6f181a",
        "body" : "Yeah, that would probably be a good idea in the future.",
        "createdAt" : "2019-08-15T16:46:54Z",
        "updatedAt" : "2019-09-16T23:28:41Z",
        "lastEditedBy" : "32d43f1c-0232-4c2f-9548-9c3dfd6f181a",
        "tags" : [
        ]
      }
    ],
    "commit" : "b42a883a14bc3a48a28624e3964be81e4f75d8f5",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +932,936 @@\n    // visible for testing\n    String encodeTimestamp(final long partitionTime) {\n        final ByteBuffer buffer = ByteBuffer.allocate(9);\n        buffer.put(LATEST_MAGIC_BYTE);"
  },
  {
    "id" : "31adfa11-b21a-40e4-b99c-2be48b2caf16",
    "prId" : 7030,
    "prUrl" : "https://github.com/apache/kafka/pull/7030#pullrequestreview-257624022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ce75710-f9cf-48a1-8881-0e9e6905f5be",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "checking for null is now encapsulated.",
        "createdAt" : "2019-07-03T17:20:06Z",
        "updatedAt" : "2019-07-09T23:16:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b33b85c4fa71ed08802599697404720a66d88ef",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +288,292 @@\n            try {\n                stateMgr.clearCheckpoints();\n            } catch (final IOException e) {\n                throw new ProcessorStateException(format(\"%sError while deleting the checkpoint file\", logPrefix), e);"
  },
  {
    "id" : "0b4aae60-85bc-4b8f-b522-c19d141d9bbb",
    "prId" : 7238,
    "prUrl" : "https://github.com/apache/kafka/pull/7238#pullrequestreview-285031723",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50837f83-e6cf-47a0-a66a-70b2e648e474",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "With EOS, Consumer.committed maybe taking long time as the previous txn is being completed, we need to make sure that it would not throw any unexpected exceptions and if so we should not fail the streams instance. cc @abbccdda @mjsax .",
        "createdAt" : "2019-09-05T23:27:43Z",
        "updatedAt" : "2019-09-11T16:35:57Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d6aa407c-8846-4a80-bade-c3c0c3c3391c",
        "parentId" : "50837f83-e6cf-47a0-a66a-70b2e648e474",
        "authorId" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "body" : "This code should be functionally equivalent to the previous code. Previously we initialized the offset limits in `registerStateStores`, which is called by `initializeStateStores` here. The biggest difference is that we no longer do this immediately for standby tasks - we defer to the first time we need a new offset limit to apply a record.",
        "createdAt" : "2019-09-06T15:24:36Z",
        "updatedAt" : "2019-09-11T16:35:57Z",
        "lastEditedBy" : "12543f19-3885-429e-8f77-e0f748c56d1f",
        "tags" : [
        ]
      },
      {
        "id" : "0c9875cc-93d5-420c-b523-b3eb83d9fd22",
        "parentId" : "50837f83-e6cf-47a0-a66a-70b2e648e474",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "The only exception that is non-fatal should be `TimeoutException`? ",
        "createdAt" : "2019-09-06T18:04:56Z",
        "updatedAt" : "2019-09-11T16:35:57Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "9748a733-a37a-416d-a4a3-e7d457469f32",
        "parentId" : "50837f83-e6cf-47a0-a66a-70b2e648e474",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yeah this is not a comment for this PR but more or less an FYI for folks who are working on KIP-447 :) I also left some thoughts on the voting thread regarding this purpose.",
        "createdAt" : "2019-09-06T18:32:57Z",
        "updatedAt" : "2019-09-11T16:35:57Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b14fbdce1f31b17070923325715f68c0238d43d",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +245,249 @@        // partitions of topics that are both sources and changelogs and set the consumer committed\n        // offset via stateMgr as there is not a more direct route.\n        final Set<String> changelogTopicNames = new HashSet<>(topology.storeToChangelogTopic().values());\n        partitions.stream()\n            .filter(tp -> changelogTopicNames.contains(tp.topic()))"
  },
  {
    "id" : "69a8a1ec-7fe9-4268-88cf-79da7d067eb4",
    "prId" : 7304,
    "prUrl" : "https://github.com/apache/kafka/pull/7304#pullrequestreview-337924758",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7f7f270-cf53-41cd-b8a7-9de4d6e17f13",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "If there is no `metadata` would we want to use the latest timestamp seen so far for the `StreamTask` and use that to set `PartitionGroup#setPartitionTime`?",
        "createdAt" : "2019-09-20T20:30:04Z",
        "updatedAt" : "2019-09-20T20:32:32Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "985dd874-4379-4779-a071-998695d45ed4",
        "parentId" : "e7f7f270-cf53-41cd-b8a7-9de4d6e17f13",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I followed the logic from the other PR that @RichardYuSTUG did, would leave to Richard to explain why we did this :)",
        "createdAt" : "2019-09-20T20:35:16Z",
        "updatedAt" : "2019-09-20T20:35:17Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "c93a3c67-a7d2-4038-a92e-c68186a4b798",
        "parentId" : "e7f7f270-cf53-41cd-b8a7-9de4d6e17f13",
        "authorId" : "32d43f1c-0232-4c2f-9548-9c3dfd6f181a",
        "body" : "@bbejeck @guozhangwang  Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.",
        "createdAt" : "2020-01-03T01:20:06Z",
        "updatedAt" : "2020-01-03T01:20:07Z",
        "lastEditedBy" : "32d43f1c-0232-4c2f-9548-9c3dfd6f181a",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bc8bf0f0703bf557bff6c8ae5c9a347599c8d4f",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +751,755 @@                    + \" to {} in stream task {}\", partition, committedTimestamp, this);\n            } else {\n                log.debug(\"No committed timestamp was found in metadata for partition {}\", partition);\n            }\n        }"
  },
  {
    "id" : "78677307-e8c8-4209-afc8-cde25b0f0527",
    "prId" : 7463,
    "prUrl" : "https://github.com/apache/kafka/pull/7463#pullrequestreview-298553524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3162310-dc9d-4634-b382-cb3087a80cbb",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is a mix of \"old\" `initializeStateStores` (the part to filter for source topic partitions that are use as changelogs) and \"old\" `AbstractTaks#committedOffsetForPartitions`)",
        "createdAt" : "2019-10-08T06:28:04Z",
        "updatedAt" : "2019-10-10T01:02:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c47dfc79b0b4a8238e7bdc8707ee9ca32ff324ea",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +270,274 @@    }\n\n    private void initializeCommittedOffsets(final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata) {\n        // Currently there is no easy way to tell the ProcessorStateManager to only restore up to\n        // a specific offset. In most cases this is fine. However, in optimized topologies we can"
  },
  {
    "id" : "62522ca8-b327-438f-832e-17edb212ed70",
    "prId" : 7463,
    "prUrl" : "https://github.com/apache/kafka/pull/7463#pullrequestreview-298553743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7356799b-ac5e-4a93-ae4b-04d6b92d724f",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is not private. Same implementation as before, however, we get `offsetsAndMetadata` passed in, and don't use the consumer to get them from the brokers.",
        "createdAt" : "2019-10-08T06:28:48Z",
        "updatedAt" : "2019-10-10T01:02:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c47dfc79b0b4a8238e7bdc8707ee9ca32ff324ea",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +292,296 @@    }\n\n    private void initializeTaskTime(final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata) {\n        for (final Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsetsAndMetadata.entrySet()) {\n            final TopicPartition partition = entry.getKey();"
  },
  {
    "id" : "e41ea101-ef90-4c41-a3e9-a26dd40b930f",
    "prId" : 7463,
    "prUrl" : "https://github.com/apache/kafka/pull/7463#pullrequestreview-298553997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b47c94b-ef25-425a-9ad3-bdf83c1829bb",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Just simplified -- moved the offset initialization into the constructor.",
        "createdAt" : "2019-10-08T06:29:34Z",
        "updatedAt" : "2019-10-10T01:02:23Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c47dfc79b0b4a8238e7bdc8707ee9ca32ff324ea",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +316,320 @@\n    @Override\n    public boolean initializeStateStores() {\n        log.debug(\"Initializing state stores\");\n        registerStateStores();"
  },
  {
    "id" : "aa80b403-ca1e-4d6b-96c0-266e341b9374",
    "prId" : 7463,
    "prUrl" : "https://github.com/apache/kafka/pull/7463#pullrequestreview-298684931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12bcd06c-f0c8-41fe-9e42-1dc9e6852d4f",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Apparently it is not completely save to call instance methods from the constructor since the object and its member variables are  not completely initialised yet. Static methods would be fine, though.\r\n\r\nI do not think that `initializeCommittedOffsets()` is currently problematic since it only accesses member variables from `AbstractTask` which should be already initialised. However, `initializeTaskTime()` accesses `partitionGroup` which is created in the constructor. Since we cannot know what reorderings a compiler does, it would be better to specify `initializeTaskTime()` as static and pass in `partitionGroup`. To make `initializeCommittedOffsets()` future-proof, it would also be good to also transform it to a static method.",
        "createdAt" : "2019-10-08T12:23:37Z",
        "updatedAt" : "2019-10-10T01:02:23Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "c47dfc79b0b4a8238e7bdc8707ee9ca32ff324ea",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +260,264 @@            final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = consumer.committed(partitions);\n            initializeCommittedOffsets(offsetsAndMetadata);\n            initializeTaskTime(offsetsAndMetadata);\n        } catch (final AuthorizationException e) {\n            throw new ProcessorStateException(String.format(\"task [%s] AuthorizationException when initializing offsets for %s\", id, partitions), e);"
  },
  {
    "id" : "8047a7cb-80c3-42f3-9e5a-f412fc6d762a",
    "prId" : 7463,
    "prUrl" : "https://github.com/apache/kafka/pull/7463#pullrequestreview-300479851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74d58e42-6940-4592-b0e0-618133c794f5",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is also new -- we implicitly re-initialize the task on `resume()` -- was done \"outside\" before",
        "createdAt" : "2019-10-10T01:06:45Z",
        "updatedAt" : "2019-10-10T01:06:45Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "eb7a87fc-0888-4d03-86d4-cecbbcf36b9c",
        "parentId" : "74d58e42-6940-4592-b0e0-618133c794f5",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Do we really need to re-initialize committed offsets and task time during Resume?",
        "createdAt" : "2019-10-10T22:03:15Z",
        "updatedAt" : "2019-10-10T22:03:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0f020881-d857-4f99-aa3c-3f01832cf8db",
        "parentId" : "74d58e42-6940-4592-b0e0-618133c794f5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes, because in `StreamTask#suspend()` we call `closeTopology()` that calls `partitionGroup.clear()` that resets all times to `UNKNOWN`.",
        "createdAt" : "2019-10-11T06:00:02Z",
        "updatedAt" : "2019-10-11T06:00:03Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c47dfc79b0b4a8238e7bdc8707ee9ca32ff324ea",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +373,377 @@            }\n        }\n        initializeMetadata();\n    }\n"
  },
  {
    "id" : "ccba96cc-dbe6-4f8b-a587-61588fd0b393",
    "prId" : 7566,
    "prUrl" : "https://github.com/apache/kafka/pull/7566#pullrequestreview-306864390",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c9f3fcf-6a62-4765-b97a-17afb248f040",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Nice cleanup, it makes looking at `StreamTask` bit easier having `TaskMetrics` in a separate class.",
        "createdAt" : "2019-10-24T21:06:11Z",
        "updatedAt" : "2019-10-25T09:01:15Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "415bca6f0f8d8acd47bfb9bdbc19e08fa50eb673",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +141,145 @@        punctuateLatencySensor = TaskMetrics.punctuateSensor(threadId, taskId, streamsMetrics);\n        recordLatenessSensor = TaskMetrics.recordLatenessSensor(threadId, taskId, streamsMetrics);\n        TaskMetrics.droppedRecordsSensor(threadId, taskId, streamsMetrics);\n\n        final ProductionExceptionHandler productionExceptionHandler = config.defaultProductionExceptionHandler();"
  },
  {
    "id" : "8cba26fd-6675-43b2-8c0e-f6095c7d649a",
    "prId" : 7566,
    "prUrl" : "https://github.com/apache/kafka/pull/7566#pullrequestreview-307314605",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f42bc93b-4210-4d19-87d0-b27a3b29c0b1",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Are we ever going to need to use the return value here? Instead of using a `Supplier` maybe we could use a `Runnable` in the signature and we could get rid of the `return null` statements.",
        "createdAt" : "2019-10-24T21:11:01Z",
        "updatedAt" : "2019-10-25T09:01:15Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "97271194-a9c2-4e87-a132-7d1c446c010d",
        "parentId" : "f42bc93b-4210-4d19-87d0-b27a3b29c0b1",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "+1. I think the appropriate functional interface is ‘Java.util.function.Consumer’",
        "createdAt" : "2019-10-25T16:22:56Z",
        "updatedAt" : "2019-10-25T16:22:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "415bca6f0f8d8acd47bfb9bdbc19e08fa50eb673",
    "line" : 212,
    "diffHunk" : "@@ -1,1 +439,443 @@        try {\n            StreamsMetricsImpl.maybeMeasureLatency(\n                () -> {\n                    node.punctuate(timestamp, punctuator);\n                },"
  },
  {
    "id" : "fcca0254-a8a5-4c0c-96f7-273498d83005",
    "prId" : 7589,
    "prUrl" : "https://github.com/apache/kafka/pull/7589#pullrequestreview-306262564",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4056b20a-57a8-4df7-a38d-5612cf5d4ddf",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "just a small cleanup on the side",
        "createdAt" : "2019-10-24T00:27:01Z",
        "updatedAt" : "2019-10-25T00:11:45Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e32bf0cafcfb8d5e7765f081fe54450112465e1",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +682,686 @@            super.flushState();\n\n            if (eosEnabled) {\n                maybeAbortTransactionAndCloseRecordCollector(isZombie);\n            }"
  },
  {
    "id" : "685ffb7b-8eb9-4c15-adae-fee289ca6b10",
    "prId" : 7748,
    "prUrl" : "https://github.com/apache/kafka/pull/7748#pullrequestreview-326537264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I don't think that an `UnknownProducerIdException` implies that a task was migrated.",
        "createdAt" : "2019-11-30T02:52:58Z",
        "updatedAt" : "2019-12-03T04:02:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "8d7f7e04-bc32-45a9-9591-732cea73a7ed",
        "parentId" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Agreed, but `TaskMigratedException` is the mechanism to initiate a rebalance, which is currently the only way that we can re-initialize the producers.\r\n\r\nI think we can stand to re-evaluate how we're handling all these different producer and consumer exceptions (with and without eos). Right now, there are a _lot_ of catch blocks for specific exceptions sprinkled throughout the codebase, and we universally either crash the thread or initiate a rebalance to attempt recovery. It would certainly benefit from developing a holistic approach, but right now, I'm just trying to get Streams threads to quit dying within a few hours of running in EOS mode, and I'm trying to restrain myself from broad refactoring so we can hope to merge this in for 2.4.0.\r\n\r\n(About that last statement, I've been on holiday this week, so I haven't had time to document the comprehensive analysis in the ticket, but I do plan to make a case that it is a regression. When I ran 2.3 under similar conditions, we still had threads dying, but they were only dying from KIP-360-related-causes. In 2.4, Streams threads are dying from multiple new causes, related to the changes that we made in 2.4... Anyway, more details coming early next week, then we can discuss it)",
        "createdAt" : "2019-11-30T03:15:56Z",
        "updatedAt" : "2019-12-03T04:02:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "84e7ee4d-960f-4f3a-a054-43668b229308",
        "parentId" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Give the fix for the root cause of `UnknownProducerIdException` (via KIP-360), I am wondering if we should actually treat it as fatal? I also don't see why it would be regression if KS dies if this exception is thrown though? (I understand that we did introduce some regression bugs, but those seem unrelated to `UnknownProducerIdException`?)\r\n\r\nTo be fair, 2.4.0 does not contain a full implementation of KIP-360 and thus, this fix might be a workaround... But frankly, I am not very happy about it, even if I understand the desire to just stabilize it somehow without a refactoring that won't make it into 2.4.0 anyway.",
        "createdAt" : "2019-11-30T05:18:08Z",
        "updatedAt" : "2019-12-03T04:02:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "f0048ce6-6579-45bc-bef3-7004689f2708",
        "parentId" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, then it seems worth hashing it out. Can you elaborate what is there to be unhappy about? I figured it would be reasonable to treat it as \"recoverable by creating a new Producer\" instead of \"fatal and you should immediately terminate the thread\".\r\n\r\nWhat I was thinking was that is seems like the broker is telling us it doesn't know who we are (anymore). Because of the context, we know that it _used_ to know who we were, so it must have forgotten somehow, and it doesn't really matter how because we can always shrug it off and re-create our producer to try again.\r\n\r\nIIUC, KIP-360 would reduce the occurrence of the exception by keeping your producer id cached even after its ttl has expired, but it by no means guarantees that it'll remember you regardless of how long you're silent.\r\n\r\nFrom that perspective it seems reasonable to catch this exception and conclude, \"Oops, it looks like we were silent too long and our Producer has effectively expired. We should make a new one and try again.\" This isn't the same thing as getting fenced, but the resolution is the same (make a new Producer and try again).\r\n\r\nThen again, this perspective may be based on a faulty understanding of the system. Is this approach unsafe, and we should actually terminate the application instead?",
        "createdAt" : "2019-11-30T05:39:02Z",
        "updatedAt" : "2019-12-03T04:02:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "f742f5ba-424f-44db-9a1f-9ae174904bbe",
        "parentId" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "For this type or error, before KIP-360, the producer should self-recover and never throw an exception. (Cf \"motivation\" section of the KIP). If this error occurs, something really bad happened for the transaction and we are in an unknown state -- not sure if rebalancing and retrying is the right approach for this case \\cc @guozhangwang @hachikuji @bob-barrett -- also, maybe it's just sufficient to close the task locally and recreate it -- I don't see a need to trigger a rebalance (in case it is ok to recorve from within KS).\r\n\r\n>  This isn't the same thing as getting fenced, but the resolution is the same (make a new Producer and try again).\r\n\r\nI disagree. If we got fenced, a new producer with the same transactional.id was created and we know that we don't own the task any longer. For a `UnknownProducerIdException` we still own the task.\r\n\r\n> Is this approach unsafe, and we should actually terminate the application instead?\r\n\r\nI don't thinks it's unsafe, but it's not the \"right\" fix IMHO, and this bug is also not a regression.",
        "createdAt" : "2019-11-30T20:57:37Z",
        "updatedAt" : "2019-12-03T04:02:12Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "c78b9c6a-1984-40d1-b00b-46c1aea66793",
        "parentId" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks for the clarification, @mjsax .\r\n\r\nI agree that it would be better to just close the task and re-open it, but I don't feel comfortable introducing a whole new task lifecycle at this point for the 2.4.0 release. My rationale was that triggering a rebalance would put Kafka Streams through a well-known and well-tested code path that will result in the task getting closed and then opened again.\r\n\r\nI do think it makes sense to go ahead and handle this case along with the two regressions. In my testing, this exception was just as fatal for Streams as the other two. Specifically, unless I include all three fixes, my soak test saw StreamThreads start dying within a few hours.\r\n\r\nIt seems like maybe a good approach right now would be to take the simple and sub-optimal path of just rebalancing when we encounter this exception for the 2.4.0 release, and file a Jira to optimize it in the way you suggest.\r\n\r\nIt would be good, by the way, to find out the answer to your question of whether it's safe to continue or not. Note that the current behavior, both in 2.4 and 2.3, without this patch is that a thread will get this exception and then shut itself down. This leads to a rebalance (since a member has left the group), after which the task is assigned to another thread, which continues processing it. Thus, we are already responding to this condition by rebalancing. It's just that we permanently lose a thread in the process. With this patch, we still rebalance, but we get to keep the thread running. If it's really unsafe to continue processing, though, we should stop the entire Streams application and force the operator to diagnose the problem with the brokers and start the app back up in a safe state.",
        "createdAt" : "2019-12-02T05:18:15Z",
        "updatedAt" : "2019-12-03T04:02:12Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "6bfeffc1-32e9-4f6e-9719-12601474ed70",
        "parentId" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I shared some thoughts on the comment above: before KIP-447, when we re-trigger the rebalance if the task is indeed migrated out the corresponding producer would be closed, otherwise that producer will be retained. But I think after KIP-447 we need to revisit this again.\r\n\r\nAt the moment I feel okay to treat `UnknownProducerId` equally as `ProducerFenced`.",
        "createdAt" : "2019-12-03T22:10:12Z",
        "updatedAt" : "2019-12-03T23:29:13Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "17f7e1b5-4135-48a2-80fd-eb9ecac6688f",
        "parentId" : "79f72754-1c4e-4ae2-a906-a2da97d618af",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks. That will be a good opportunity to clean up what is honestly a pretty ham-handed approach here to gracefully deal with this condition without changing too much code.",
        "createdAt" : "2019-12-04T00:28:38Z",
        "updatedAt" : "2019-12-04T00:28:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "96a2d969df7fe000bbda04c9aab3b0d50eaa655c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +340,344 @@                this.producer.beginTransaction();\n            } catch (final ProducerFencedException | UnknownProducerIdException e) {\n                throw new TaskMigratedException(this, e);\n            }\n            transactionInFlight = true;"
  },
  {
    "id" : "9ae63bef-8e89-40f0-b99d-f8a5a72a42bb",
    "prId" : 7748,
    "prUrl" : "https://github.com/apache/kafka/pull/7748#pullrequestreview-326539096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32dc6cc3-f0f0-4ba1-8f12-4f6059da6a68",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "For here and `producer.beginTransaction` above: I think `beginTxn / sendOffsetsToTxn / commitTxn` would not throw UnknownProducerIdException but it does not harm to be more careful for 2.4 release -- so if you want to keep it as is I'm fine with it too.",
        "createdAt" : "2019-12-03T22:47:17Z",
        "updatedAt" : "2019-12-03T23:29:13Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "4b48f025-3cb2-4df0-a695-2586640c67ce",
        "parentId" : "32dc6cc3-f0f0-4ba1-8f12-4f6059da6a68",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, I wasn't sure, so I just put it everywhere that we already check for `ProducerFencedException`, on the rationale that, if we can get fenced, then we must have a transactional id, and if we have an id, then it could be \"unknown\". Clearly, this is more intuitive than analytical, so I find your feedback plausible.",
        "createdAt" : "2019-12-04T00:34:29Z",
        "updatedAt" : "2019-12-04T00:34:29Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "96a2d969df7fe000bbda04c9aab3b0d50eaa655c",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +560,564 @@                consumer.commitSync(consumedOffsetsAndMetadata);\n            }\n        } catch (final CommitFailedException | ProducerFencedException | UnknownProducerIdException error) {\n            throw new TaskMigratedException(this, error);\n        }"
  },
  {
    "id" : "b905a606-0515-4330-b13a-2c8aee97e19f",
    "prId" : 7833,
    "prUrl" : "https://github.com/apache/kafka/pull/7833#pullrequestreview-332279479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a933758b-883b-4394-9721-622244cd8b32",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Minor cleanup.",
        "createdAt" : "2019-12-15T18:22:54Z",
        "updatedAt" : "2019-12-16T22:43:13Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d9c7dac7c2dd1088bd1dcf75c130708cb72e80f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +178,182 @@\n        // initialize the topology with its own context\n        processorContext = new ProcessorContextImpl(id, this, config, this.recordCollector, stateMgr, streamsMetrics, cache);\n\n        final TimestampExtractor defaultTimestampExtractor = config.defaultTimestampExtractor();"
  },
  {
    "id" : "60f73d3b-2aa7-4d74-b124-767af85a0512",
    "prId" : 8040,
    "prUrl" : "https://github.com/apache/kafka/pull/8040#pullrequestreview-355526952",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f8c1b2c-4b8d-412f-9fda-757789973d5c",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Previously, we clear the `partitionGroup` within `closeTopology()` that we call above -- however, because of the consumer position tracking, we need to delay it after the commit.",
        "createdAt" : "2020-02-08T03:30:01Z",
        "updatedAt" : "2020-02-11T03:10:50Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "944fe8ec3720a43d895669f340184d025c880708",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +637,641 @@                commit(false, partitionTimes);\n            } finally {\n                partitionGroup.clear();\n\n                if (eosEnabled) {"
  },
  {
    "id" : "a2a3efde-e32c-4125-9438-c7bf0ae063e2",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-362291724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b05483b-fcfd-4cd0-92ce-ee4bbed1c45f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Here is the attempted fix of https://issues.apache.org/jira/browse/KAFKA-9572: if we are closing / suspending a restoring task, we should only update the checkpoint file but should NOT commit offsets, since the committed offsets indicate the  \"restore end\" and should not be updated, cc @cadonna who filed the JIRA.",
        "createdAt" : "2020-02-20T22:50:45Z",
        "updatedAt" : "2020-02-20T23:13:53Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +237,241 @@            // do nothing\n            log.trace(\"Skip suspending since state is {}\", state());\n        } else if (state() == State.RUNNING) {\n            closeTopology(true);\n"
  },
  {
    "id" : "28f31bce-02fd-4214-82c7-2593ad7ea449",
    "prId" : 8058,
    "prUrl" : "https://github.com/apache/kafka/pull/8058#pullrequestreview-362291724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4deffb7-124f-48cb-9577-9dca360c20af",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is part of the fix as well: only flushing / checkpointing, but not committing.",
        "createdAt" : "2020-02-20T22:51:30Z",
        "updatedAt" : "2020-02-20T23:13:53Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03f4778cba8697bad6e5c5d7ce40df6e59214c02",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +429,433 @@\n                transitionTo(State.CLOSING);\n            } else if (state() == State.RESTORING) {\n                executeAndMaybeSwallow(clean, () -> {\n                    stateMgr.flush();"
  },
  {
    "id" : "35181657-46cf-46d7-9535-fecf40e3b5ec",
    "prId" : 8060,
    "prUrl" : "https://github.com/apache/kafka/pull/8060#pullrequestreview-357139833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00ede6cc-b562-4d1a-9f6e-9f961ac9fbda",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why do we throw `TimeoutException` directly but not wrap it?",
        "createdAt" : "2020-02-10T01:56:41Z",
        "updatedAt" : "2020-02-14T22:46:42Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "83bc4163-b257-480c-83ac-ada779399842",
        "parentId" : "00ede6cc-b562-4d1a-9f6e-9f961ac9fbda",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Because on the caller `TaskManager` we would swallow TimeoutException anyways.",
        "createdAt" : "2020-02-12T01:43:31Z",
        "updatedAt" : "2020-02-14T22:46:42Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "19332579cacd46146a8309938215845c16448a8d",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +625,629 @@                ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);\n\n            throw e;\n        } catch (final KafkaException e) {\n            throw new StreamsException(format(\"task [%s] Failed to initialize offsets for %s\", id, partitions), e);"
  },
  {
    "id" : "45f73f54-7057-4004-b415-89293c55a7fd",
    "prId" : 8065,
    "prUrl" : "https://github.com/apache/kafka/pull/8065#pullrequestreview-356417505",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65268339-8208-4f50-9677-5df83c08dd17",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Exposed this again for use in TopologyTestDriver",
        "createdAt" : "2020-02-11T04:55:54Z",
        "updatedAt" : "2020-02-11T22:50:50Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fc4da0e7f0a29845782c4f3a860289bbdfb356a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +445,449 @@     * source topic partitions, or if it is enforced to be processable\n     */\n    public boolean isProcessable(final long wallClockTime) {\n        if (partitionGroup.allPartitionsBuffered()) {\n            idleStartTime = RecordQueue.UNKNOWN;"
  },
  {
    "id" : "8133cef1-87de-4f00-abf0-b584c4479097",
    "prId" : 8116,
    "prUrl" : "https://github.com/apache/kafka/pull/8116#pullrequestreview-359211897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "736a3cb8-b53e-4f2c-959c-261e0950cf89",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Judging from the stacktrace generated in the prior logging change, this is where we're throwing a TaskMigratedException, even though this block is an \"unclean close\". Just trying out ignoring the exception to see what happens.",
        "createdAt" : "2020-02-13T23:20:32Z",
        "updatedAt" : "2020-02-20T20:36:25Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "6035425a-f6b5-4ca3-890e-15dcfd407678",
        "parentId" : "736a3cb8-b53e-4f2c-959c-261e0950cf89",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just to clarify we should only swallow under `dirty` suspension (in trunk it is wrapped in `close`) right? For clean close / suspend if it throws we would capture it and wrap as TaskMigrated still and then handle them by closing the task as dirty (in that second try we would swallow).",
        "createdAt" : "2020-02-14T16:29:33Z",
        "updatedAt" : "2020-02-20T20:36:25Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f9013948-8d41-4afc-80a0-868f450f0ac0",
        "parentId" : "736a3cb8-b53e-4f2c-959c-261e0950cf89",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "That's right",
        "createdAt" : "2020-02-14T20:51:36Z",
        "updatedAt" : "2020-02-20T20:36:25Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ba79bf6de34142dff929bae3d5c9e50b96800c1",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +660,664 @@            } catch (final ProcessorStateException e) {\n                // ignore any exceptions while flushing (all stores would have had a chance to flush anyway)\n            }\n\n            if (eosEnabled) {"
  },
  {
    "id" : "b70ad110-b0c8-4ad6-b430-cc9a492d0984",
    "prId" : 8140,
    "prUrl" : "https://github.com/apache/kafka/pull/8140#pullrequestreview-361604555",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ef59624-38c5-42c9-b112-4a644a516252",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Technically it may be running or restoring here",
        "createdAt" : "2020-02-20T02:58:03Z",
        "updatedAt" : "2020-02-20T20:31:48Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "3164fce49de499e640dad11ed3d56ba47ad14bb7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +255,259 @@\n                transitionTo(State.SUSPENDED);\n                log.info(\"Suspended active\");\n            } else {\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);"
  },
  {
    "id" : "e7ebfb0d-bc62-40db-88de-fea5b6c4414c",
    "prId" : 8173,
    "prUrl" : "https://github.com/apache/kafka/pull/8173#pullrequestreview-365270536",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e15310e5-3ce4-4799-8df4-a61b100aaba9",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Seems like maybe we could just get rid of the `suspended` state entirely, but I guess we plan to do that anyway if/when we remove support for EAGER",
        "createdAt" : "2020-02-26T21:30:59Z",
        "updatedAt" : "2020-02-26T21:31:00Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "b7497b2c-88ce-4146-a903-8e4af5e369c1",
        "parentId" : "e15310e5-3ce4-4799-8df4-a61b100aaba9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yes -- from restoring to suspended it is only to update the local `checkpoint` file, from suspended to restoring there's actually nothing needed. In that sense restoring and suspended states are very similar already, BUT still not exactly the same.\r\n\r\nI do hope we get rid of it some time after we remove the EAGER protocol.",
        "createdAt" : "2020-02-26T21:52:42Z",
        "updatedAt" : "2020-02-26T21:52:43Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef60891ca88d46a9f2bf6d26dfa16626420b4df9",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +284,288 @@            case SUSPENDED:\n                // just transit the state without any logical changes: suspended and restoring states\n                // are not actually any different for inner modules\n                transitionTo(State.RESTORING);\n                log.info(\"Resumed to restoring state\");"
  },
  {
    "id" : "d87c70b2-76bd-44fe-bfb8-bb7152bcc9c2",
    "prId" : 8180,
    "prUrl" : "https://github.com/apache/kafka/pull/8180#pullrequestreview-370505629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9329ff1-1c90-4815-a5a2-27a553e38269",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is an improvement I want to add along with the PR: since we delete the checkpoint file after completed loading, and before we initialize to RESTORING if there's an exception we could lose that checkpoint. So here in Restoring / Created state upon closing I also added the checkpoint logic here.",
        "createdAt" : "2020-03-06T17:39:23Z",
        "updatedAt" : "2020-03-06T23:37:23Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1bea170bbbf209ad20e5493ae832fd247ebfc3d",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +415,419 @@        if (state() == State.CREATED) {\n            // the task is created and not initialized, just re-write the checkpoint file\n            executeAndMaybeSwallow(clean, () -> {\n                stateMgr.checkpoint(Collections.emptyMap());\n            }, \"state manager checkpoint\");"
  },
  {
    "id" : "ca866840-07a0-443b-9523-70fdbfe3d739",
    "prId" : 8180,
    "prUrl" : "https://github.com/apache/kafka/pull/8180#pullrequestreview-370505629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57ea11a0-8baa-4b04-887c-89e08aaf4bc9",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We could have added the stores, but then before transiting to RESTORING an exception happens; hence here I always call closeStateManager which would just be an no-op if the lock is not grabbed / stores not added.",
        "createdAt" : "2020-03-06T17:40:50Z",
        "updatedAt" : "2020-03-06T23:37:23Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1bea170bbbf209ad20e5493ae832fd247ebfc3d",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +445,449 @@        }\n\n        if (state() == State.CLOSING) {\n            // if EOS is enabled, we wipe out the whole state store for unclean close\n            // since they are invalid to use anymore"
  },
  {
    "id" : "1fd08a18-8e3b-4ea9-8c76-560b7f8a5566",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-370512287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07cdd6b2-9915-4e1b-a15a-8bac43966776",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Why do we start to suppress warnings?",
        "createdAt" : "2020-03-05T00:33:39Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "c9c9ef5c-a259-4710-bee7-1b1aa7af708a",
        "parentId" : "07cdd6b2-9915-4e1b-a15a-8bac43966776",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "We should have done this from the beginning on... (it's just a \"side fix\")",
        "createdAt" : "2020-03-06T17:50:27Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 375,
    "diffHunk" : "@@ -1,1 +562,566 @@     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    public boolean process(final long wallClockTime) {\n        if (!isProcessable(wallClockTime)) {"
  },
  {
    "id" : "81f2983a-3ec4-4cb2-be41-68a33b0d465b",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375614304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b39992d-fb9e-4358-843f-18d2f6f16184",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could we add a `@return` for this method? Also we should comment about the different indications when we return an empty map vs null.",
        "createdAt" : "2020-03-11T21:01:37Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "ae4e6568-ea3c-4382-b1e8-6df785123d38",
        "parentId" : "8b39992d-fb9e-4358-843f-18d2f6f16184",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why do we need to document this in the method JavaDoc? It's an internal method? Internal comment outdate quickly if code is changed and comments are not updated accordingly (what happens 99% of the time). Hence, I would prefer to limit comments if possible. In doubt, we should document at `Task` level anyway.",
        "createdAt" : "2020-03-12T00:13:04Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "fff75631-b190-4e07-ae69-64835ab2892b",
        "parentId" : "8b39992d-fb9e-4358-843f-18d2f6f16184",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "@guozhangwang I am actually wondering about point (5) -- why do we need to checkpoint the state manager if we wipe out the store later anyway for the unclean EOS case?",
        "createdAt" : "2020-03-12T00:17:53Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "0f9e00fb-e27b-4373-b76b-ef4d63947dca",
        "parentId" : "8b39992d-fb9e-4358-843f-18d2f6f16184",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yes we are unnecessarily checkpointing here --- the reason is that EOS flag was original striped out of task and only processor-state-manager knows about it; now since we get this EOS flag back to task (sigh.. :) we can add this additional check.",
        "createdAt" : "2020-03-16T23:52:01Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 250,
    "diffHunk" : "@@ -1,1 +437,441 @@     * @throws TaskMigratedException if the task producer got fenced (EOS)\n     */\n    private Map<TopicPartition, Long> prepareClose(final boolean clean) {\n        final Map<TopicPartition, Long> checkpoint;\n"
  },
  {
    "id" : "94b178af-b8e2-44bc-9b48-15cf894325c0",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-372454953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a776c78b-fd64-4508-8e9e-f243a07e8e96",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Remove `if`",
        "createdAt" : "2020-03-11T21:04:39Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +458,462 @@            checkpoint = Collections.emptyMap();\n        } else if (state() == State.SUSPENDED) {\n            // if `SUSPENDED` do not need to checkpoint, since when suspending we've already committed the state\n            checkpoint = null; // `null` indicates to not write a checkpoint\n        } else {"
  },
  {
    "id" : "242d280a-7e07-4e68-99ce-dc05cc740522",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375614304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d52ce389-a257-4b8d-93f4-3e0db0113d71",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Also the above step #4 is no longer correct, the commit is done on TaskManager now.",
        "createdAt" : "2020-03-11T21:57:16Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "05aa84db-dd89-4b00-8864-006b6f0a13c9",
        "parentId" : "d52ce389-a257-4b8d-93f4-3e0db0113d71",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "You see -- that is may point from above... The code should be written in a way that explains itself... Updating comments always slips...",
        "createdAt" : "2020-03-12T00:13:57Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "1049e289-f7a1-4c5e-89c2-6e6126d8882e",
        "parentId" : "d52ce389-a257-4b8d-93f4-3e0db0113d71",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I couldn't fully follow this idea, just playing devil advocates here, if we think meta code comments actually hinder the readability of internal class, why not just remove all the internal function meta comments, as they would get outdated anyway? For me the return type comment is still valuable for understandability. If the comment gets outdated, we should just update it. cc @guozhangwang if the idea here makes sense, or we could get a consensus on what needs to be done in internal class comments, and what's not.",
        "createdAt" : "2020-03-16T17:06:41Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "f93693f3-99d5-4411-a1ee-933278b8c837",
        "parentId" : "d52ce389-a257-4b8d-93f4-3e0db0113d71",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I would suggest not restricting ourselves to some specific rules about comments :) Personally I tried to avoid the `one line comment explaining one line code` type of comments inside a function since it should be obvious, rather I'd add some comments for a block or several blocks if I fear it maybe hard to read by itself. I think you guys should just make your best judgement here.\r\n\r\nAnd for internal functions, I agree that we do not necessarily need to write java-docs, and this one, for example, I wrote the java-doc as part of the tech debt cleanup just to remind what operations MUST be considered here inside closing / suspending etc so that later on when we change the function itself by other contributors, they would use it as a reference to check if they mistakenly missed some steps or re-ordered some steps. However if we are going to split this function into multiple, instead of just re-structuring the function as a whole, then although I have my preference I'd leave to you guys if you want to add the javadoc for both pre/post of you feel now it is too obvious to bother :) ",
        "createdAt" : "2020-03-16T23:57:18Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 250,
    "diffHunk" : "@@ -1,1 +437,441 @@     * @throws TaskMigratedException if the task producer got fenced (EOS)\n     */\n    private Map<TopicPartition, Long> prepareClose(final boolean clean) {\n        final Map<TopicPartition, Long> checkpoint;\n"
  },
  {
    "id" : "97c8e889-43e4-490f-ad35-0550b0aa55fc",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-374913535",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90c6b577-a344-4c44-8e1a-7c4fb46317eb",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Looks like we lack test coverage for TimeoutException and KafkaException cases",
        "createdAt" : "2020-03-11T22:13:39Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "4c935653-557b-466c-9bf1-baa0faf7173f",
        "parentId" : "90c6b577-a344-4c44-8e1a-7c4fb46317eb",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yeah, this PR does not yet add all required test...",
        "createdAt" : "2020-03-12T00:10:57Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "2c434d70-9979-42f7-88d5-8ff8ab837701",
        "parentId" : "90c6b577-a344-4c44-8e1a-7c4fb46317eb",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Covered via `shouldCommitNextOffsetFromQueueIfAvailable` and `shouldCommitConsumerPositionIfRecordQueueIsEmpty`",
        "createdAt" : "2020-03-16T05:02:44Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +384,388 @@        }\n\n        return consumedOffsetsAndMetadata;\n    }\n"
  },
  {
    "id" : "ab5e0859-cae0-4a10-89e6-d476111bf1e1",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375631922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48b86590-d8a2-4c39-92ae-8e6e44718b1c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is not introduced in this PR, but: while thinking about it, I realized for RESTORING state we do not need to rely on eosDisabled to checkpoint, in fact we can always checkpoint during RESTORING here.",
        "createdAt" : "2020-03-13T03:50:05Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "16f2671f-65ea-4641-b560-4d89af3a5a34",
        "parentId" : "48b86590-d8a2-4c39-92ae-8e6e44718b1c",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Ack",
        "createdAt" : "2020-03-16T05:00:28Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "69e476b1-ffff-46dc-9312-a7398086022e",
        "parentId" : "48b86590-d8a2-4c39-92ae-8e6e44718b1c",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Do we have unit test to check the checkpoint status after `postCommit()`?",
        "createdAt" : "2020-03-16T16:10:32Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "a96fbd77-5ec4-4fd0-87ff-1f2ae72d6350",
        "parentId" : "48b86590-d8a2-4c39-92ae-8e6e44718b1c",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes, `shouldRespectCommitNeeded()` check this already.",
        "createdAt" : "2020-03-16T22:44:47Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +325,329 @@            case RUNNING:\n                commitNeeded = false;\n                commitRequested = false;\n\n                if (eosDisabled) {"
  },
  {
    "id" : "6318076a-990a-4f5f-8d1a-d9af1ed5aa9d",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375631536",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "282b3b85-5206-4b29-91d0-dd1f68710aad",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "we could just do one log in front: `log.info(\"Prepare suspending {}\", state());`",
        "createdAt" : "2020-03-16T16:06:09Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "3c065f67-338b-4825-9bfc-c47071be08b7",
        "parentId" : "282b3b85-5206-4b29-91d0-dd1f68710aad",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Well, we log \"skip\" for state CREATED and we throw for invalid states. Note sure how to do this?",
        "createdAt" : "2020-03-16T22:43:50Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +239,243 @@            recordCollector.flush();\n\n            log.info(\"Prepare suspending running\");\n        } else if (state() == State.RESTORING) {\n            stateMgr.flush();"
  },
  {
    "id" : "2bf013a7-a541-4d8a-b8ee-3417ef9b31d1",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375614304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "857b56e2-4fdd-4e36-9544-f5fff073b8a4",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "In either eos-alpha or eos-beta or non-eos, we can just loop over all the \"committable partitions\" and call `mainConsumer#position` once, so this function can be extracted out of the task as a per-task call.\r\n\r\nMore specifically, in the prepareXX calls, we know based on the state of the task and clean flag whether or not we should commit the source topic offsets for this task, so we can let the prepareXX function to return `Map<TopicPartition, Long> partitionTimes` encoding the extracted timestamps for each partition instead of void --- when we decided not to commit we return an empty map. And then inside TaskManager we just use the `mainConsumer` to call position once and then pass that to the `commitOffsetsOrTransaction` call.",
        "createdAt" : "2020-03-16T22:23:14Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 169,
    "diffHunk" : "@@ -1,1 +351,355 @@\n    @Override\n    public Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n        if (state() == State.CLOSED) {\n            throw new IllegalStateException(\"Task \" + id + \" is closed.\");"
  },
  {
    "id" : "c8e06442-87cd-4d34-9513-8b23c6fbb82c",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-375614304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8b6a231-4231-4d95-bf7b-37f6e92949ac",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`For the next PR`: as I mentioned in the last commit I feel `prepareSuspend` and `prepareClose` can be consolidated with `prepareCommit` but in the next PR these logic would be changed again for eos-beta so maybe we cannot do that any more, so I'm fine with keeping as-is and we can revisit to see if we can really do this refactoring or not in the next PR when we did the eos-beta.",
        "createdAt" : "2020-03-16T23:47:56Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +229,233 @@     */\n    @Override\n    public void prepareSuspend() {\n        if (state() == State.CREATED || state() == State.SUSPENDED) {\n            // do nothing"
  },
  {
    "id" : "d8785680-6813-4e7e-abc9-35191e645311",
    "prId" : 8218,
    "prUrl" : "https://github.com/apache/kafka/pull/8218#pullrequestreview-376297222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "337006b3-6f87-4386-9db3-fa886d73b87d",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`For the next PR`: I see the reason I return the checkpoint is that we are now extracting the committing out of the task and I need to remember if we need to checkpoint and if yes which offsets after we've flushed and before we checkpoint, but since the state of the task would not change before / after the commit during close. \r\n\r\nMore specifically we only have three cases: 1) to not write checkpoint, 2) write checkpoints for written offsets (changelogs) only, 3) write checkpoint for written and consumed offsets. And no matter which case it is during the `preClose`, it would always be the same in the `post`, so why do we need to return it to task-manager, book-keep there, and then after commit to pass it back to tasks?",
        "createdAt" : "2020-03-17T00:07:51Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "cd7ed42a-6241-4811-9a79-30d5b67120c6",
        "parentId" : "337006b3-6f87-4386-9db3-fa886d73b87d",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "By `next PR`, you mean the one after we finish the EOS-beta commit feature right?",
        "createdAt" : "2020-03-17T16:05:29Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "e03c1b40-7ab7-4d5f-a66b-88582ff4e7b4",
        "parentId" : "337006b3-6f87-4386-9db3-fa886d73b87d",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I mean the next PR when we add the EOS-beta feature --- please see the first comment I have with this tag.",
        "createdAt" : "2020-03-17T18:25:59Z",
        "updatedAt" : "2020-03-19T08:49:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b03a487a518756f7c9acd1f3177b12b407867d5",
    "line" : 250,
    "diffHunk" : "@@ -1,1 +437,441 @@     * @throws TaskMigratedException if the task producer got fenced (EOS)\n     */\n    private Map<TopicPartition, Long> prepareClose(final boolean clean) {\n        final Map<TopicPartition, Long> checkpoint;\n"
  },
  {
    "id" : "d32d417d-e015-4896-b900-7c6dff8f5ab1",
    "prId" : 8221,
    "prUrl" : "https://github.com/apache/kafka/pull/8221#pullrequestreview-374782223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aafd7c60-4d69-4540-9f4e-92a40fd9b10f",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "This is nice!",
        "createdAt" : "2020-03-13T21:34:10Z",
        "updatedAt" : "2020-05-26T16:32:00Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "4b9cdd59-f770-44a7-9c8d-5c65c0a372aa",
        "parentId" : "aafd7c60-4d69-4540-9f4e-92a40fd9b10f",
        "authorId" : "889d8126-0663-4a3e-8712-81869bada9e0",
        "body" : "Thank you",
        "createdAt" : "2020-03-15T09:06:42Z",
        "updatedAt" : "2020-05-26T16:32:00Z",
        "lastEditedBy" : "889d8126-0663-4a3e-8712-81869bada9e0",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc85a4694d663ad13d75681d28398d4b61a4f482",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +1023,1027 @@    }\n\n    private class RecordQueueCreator {\n        private final LogContext logContext;\n        private final TimestampExtractor defaultTimestampExtractor;"
  },
  {
    "id" : "ae41bbad-8d4b-47ae-818b-936dfb983600",
    "prId" : 8221,
    "prUrl" : "https://github.com/apache/kafka/pull/8221#pullrequestreview-409930931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d086138-f11f-4a8a-98b7-7a3663c4b17c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "If the state was RUNNING then we would effectively call `initializeTopology` twice. Is that intentional?",
        "createdAt" : "2020-05-10T04:33:26Z",
        "updatedAt" : "2020-05-26T16:32:00Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8d6d1d97-9fcb-420c-9449-ede7d5bce0e8",
        "parentId" : "9d086138-f11f-4a8a-98b7-7a3663c4b17c",
        "authorId" : "889d8126-0663-4a3e-8712-81869bada9e0",
        "body" : "if I correctly remember if state==`RUNNING` `initializeTopology` is called only once in `update` method. Only if state == RESTORING it will be called later. It's no problem to call this method twice but it's aka optimization.",
        "createdAt" : "2020-05-12T11:03:17Z",
        "updatedAt" : "2020-05-26T16:32:00Z",
        "lastEditedBy" : "889d8126-0663-4a3e-8712-81869bada9e0",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc85a4694d663ad13d75681d28398d4b61a4f482",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +428,432 @@        super.update(topicPartitions, processorTopology);\n        partitionGroup.updatePartitions(topicPartitions, recordQueueCreator::createQueue);\n        if (state() != State.RESTORING) { // if task is RESTORING then topology will be initialized in completeRestoration\n            initializeTopology();\n        }"
  },
  {
    "id" : "2131e18c-0ebe-4966-b720-58ee2ed82cac",
    "prId" : 8246,
    "prUrl" : "https://github.com/apache/kafka/pull/8246#pullrequestreview-372252785",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec986846-708c-4790-a2b4-8a2597be5e8b",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Not taking a hard stance on this spelling, just aiming for consistency across the code base",
        "createdAt" : "2020-03-10T19:21:08Z",
        "updatedAt" : "2020-03-13T22:09:33Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "a757918bbbf7c4b27aa29720e540a5603f890b1e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +672,676 @@\n    @Override\n    public Map<TopicPartition, Long> purgeableOffsets() {\n        final Map<TopicPartition, Long> purgeableConsumedOffsets = new HashMap<>();\n        for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {"
  },
  {
    "id" : "713d6f2e-815c-4603-a8af-b41a6f348dee",
    "prId" : 8307,
    "prUrl" : "https://github.com/apache/kafka/pull/8307#pullrequestreview-378782369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "718ab1ad-ca99-4445-a78d-c518aa2ee8e6",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This part will have some conflicts with @mjsax 's PR, just a note.",
        "createdAt" : "2020-03-20T18:57:54Z",
        "updatedAt" : "2020-03-20T19:02:03Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "a6995ea1-5e32-4f33-ad29-687e02b88b2e",
        "parentId" : "718ab1ad-ca99-4445-a78d-c518aa2ee8e6",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Yea, one of us probably needs to rebase",
        "createdAt" : "2020-03-20T20:10:45Z",
        "updatedAt" : "2020-03-20T20:10:45Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f4cfd7503c0959ed2453e88ee9f1c98d280ca71",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +121,125 @@        this.time = time;\n        this.recordCollector = recordCollector;\n        eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n\n        final String threadId = Thread.currentThread().getName();"
  },
  {
    "id" : "f4e15dec-d784-4a87-a22d-b20ec1615d6c",
    "prId" : 8371,
    "prUrl" : "https://github.com/apache/kafka/pull/8371#pullrequestreview-387632893",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81d69d30-5d84-47da-9fdb-879d925193eb",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "req: Please verify the creation of those metrics in `StreamTaskTest#shouldConstructMetricsWithBuiltInMetricsVersionLatest()`.",
        "createdAt" : "2020-04-03T10:04:26Z",
        "updatedAt" : "2020-04-06T18:04:42Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "614b547c-923c-4094-9516-90007b317cf1",
        "parentId" : "81d69d30-5d84-47da-9fdb-879d925193eb",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Ack.",
        "createdAt" : "2020-04-03T23:29:12Z",
        "updatedAt" : "2020-04-06T18:04:42Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "325f0a4e5b38f0cdd495629577cba64c9857b98c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +139,143 @@        processLatencySensor = TaskMetrics.processLatencySensor(threadId, taskId, streamsMetrics);\n        punctuateLatencySensor = TaskMetrics.punctuateSensor(threadId, taskId, streamsMetrics);\n        bufferedRecordsSensor = TaskMetrics.activeBufferedRecordsSensor(threadId, taskId, streamsMetrics);\n\n        streamTimePunctuationQueue = new PunctuationQueue();"
  },
  {
    "id" : "05ca4bd5-ec46-4889-8c5b-117eb9db83e1",
    "prId" : 8463,
    "prUrl" : "https://github.com/apache/kafka/pull/8463#pullrequestreview-391887808",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb801063-2119-4474-8842-b483e5909d00",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "These were absolutely spamming the logs, and don't seem like general-interest \"info\", so I demoted them to debug. But if they were set to info for a good reason I'm happy to revert",
        "createdAt" : "2020-04-10T23:26:42Z",
        "updatedAt" : "2020-04-10T23:32:52Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "5c28c386-0a7b-4688-9ec9-10b395861049",
        "parentId" : "cb801063-2119-4474-8842-b483e5909d00",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think at TaskManager we already log at INFO level for time-based and user-based committing, so I'm fine with this.",
        "createdAt" : "2020-04-12T19:21:09Z",
        "updatedAt" : "2020-04-12T19:31:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8b15773b7d1eb96e8804a90e29cb27d390f6b3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +318,322 @@                recordCollector.flush();\n\n                log.debug(\"Prepared task for committing\");\n\n                break;"
  },
  {
    "id" : "c28e126c-398f-4aad-aaea-36ce5d5636be",
    "prId" : 8697,
    "prUrl" : "https://github.com/apache/kafka/pull/8697#pullrequestreview-418844438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07febf5f-88a0-44c0-a1ec-8114cff75c7f",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "If I understand this right, we are recording sink latencies after processing, but source latencies before processing. This nicely avoids the problem with recording non-sink latencies after processing, but is it accurate?",
        "createdAt" : "2020-05-26T17:21:21Z",
        "updatedAt" : "2020-05-26T23:33:26Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "a1f2f304-7518-421c-9b6e-30acfaf09484",
        "parentId" : "07febf5f-88a0-44c0-a1ec-8114cff75c7f",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "We discussed this offline, but in case anyone else was wondering:\r\n\r\nYes. We can't record the latency _after_ processing for source nodes due to our recursive DFS approach to processing, as the source node's `#process` actually doesn't complete until the record has been processed by every other node in the subtopology. And anyways, the intent of the source node metric is to gauge the e2e latency when the record arrives at the subtopology, which is what we are recording here.",
        "createdAt" : "2020-05-26T21:20:27Z",
        "updatedAt" : "2020-05-26T23:33:26Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9089566f-0393-4777-8985-b74b851e4af9",
        "parentId" : "07febf5f-88a0-44c0-a1ec-8114cff75c7f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks, @ableegoldman , I think this is a fine tradeoff. Also helping is the fact that we know all \"source nodes\" are actually instances of SourceNode, which specifically do nothing except forward every record, so whether we measure these nodes before or \"after\" their processing logic should make no practical difference at all.",
        "createdAt" : "2020-05-26T21:24:10Z",
        "updatedAt" : "2020-05-26T23:33:26Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "cedfee86-9988-46d9-8f7c-d8f7e2d43744",
        "parentId" : "07febf5f-88a0-44c0-a1ec-8114cff75c7f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "> o whether we measure these nodes before or \"after\" their processing logic should make no practical difference at all.\r\n\r\nI think it make a big difference, and only recording _before_ processing is what we want (according to what the KIP says). Otherwise, the latency includes the processing time for one or more processors (in the worst case even all processors).",
        "createdAt" : "2020-05-26T23:13:07Z",
        "updatedAt" : "2020-05-26T23:33:26Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "5295a4d3-5243-4690-b302-7f05b29c8acf",
        "parentId" : "07febf5f-88a0-44c0-a1ec-8114cff75c7f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Sorry for my ambiguity. Please let me clarify my terms. Currently if you wait until the end of the \"process\" method, you wind up including the call to forward, which recursively calls process on all descendents of the source node. This is _not_ what I was talking about. I meant only the time spent _just_ in processing the SourceNode, excluding the time in \"forward\". What shall we call this? Maybe \"actual\", or \"proper\", or \"internal\" processing time?\r\n\r\nSo, my comment was that, given that we know the implementation of SourceNode, we know that it's \"actual\", \"proper\", \"internal\" processing time is going to be very small, probably far less than a single millisecond. So it doesn't make any practical difference whether we measure before the call for just the special case of source nodes, or magically solve the problem of measuring the e2e latency after internal processing, but not including the calls to \"forward\".\r\n\r\nThis is why I think it's fine to measure SourceNodes _before_ the call to process, even though the KIP technically specifies that processors' end-to-end latencies should include processing latency. We're making a simplifying assumption that for source nodes specifically, the processing latency would be `<< 1`, so we can ignore it.",
        "createdAt" : "2020-05-27T04:13:06Z",
        "updatedAt" : "2020-05-27T04:13:07Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "8223ea29b07d9754b28ef4b06e765c66f208863c",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +605,609 @@\n            updateProcessorContext(record, currNode, wallClockTime);\n            maybeRecordE2ELatency(record.timestamp, wallClockTime, currNode.name());\n            maybeMeasureLatency(() -> currNode.process(record.key(), record.value()), time, processLatencySensor);\n"
  },
  {
    "id" : "e344e8dd-e8ad-4371-9c3e-0fc201dff4bc",
    "prId" : 8776,
    "prUrl" : "https://github.com/apache/kafka/pull/8776#pullrequestreview-424932591",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "527b6dd4-530c-48b7-aa56-38aed469a3f9",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Align code style to use `switch` if all states are used",
        "createdAt" : "2020-06-02T06:56:41Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "7abf528e-d661-401c-a946-646500cc6b63",
        "parentId" : "527b6dd4-530c-48b7-aa56-38aed469a3f9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Is this intentional to use switch and if/else in different functions?",
        "createdAt" : "2020-06-02T20:32:17Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "2266812e-94fe-4b33-9d8e-f1d58a0704df",
        "parentId" : "527b6dd4-530c-48b7-aa56-38aed469a3f9",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I think in general it's better to use `switch` and I never changed from `switch -> if/else` -- but did not update all methods either (if we only check for a single state, it seems overkill to use `switch`?",
        "createdAt" : "2020-06-02T22:44:44Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "4acd76ef-06b3-49f2-9b99-fdacb8d1fc38",
        "parentId" : "527b6dd4-530c-48b7-aa56-38aed469a3f9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yup, makes sense, just curious :)",
        "createdAt" : "2020-06-05T01:03:41Z",
        "updatedAt" : "2020-06-05T01:03:42Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f98207900282ef4d7a27980a4cc018216098236c",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +260,264 @@    @Override\n    public void prepareSuspend() {\n        switch (state()) {\n            case CREATED:\n            case SUSPENDED:"
  },
  {
    "id" : "6d9bd498-6c47-48fc-ad17-0bc828724e9e",
    "prId" : 8776,
    "prUrl" : "https://github.com/apache/kafka/pull/8776#pullrequestreview-422390802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ba001ec-0215-4b6e-8b5a-9d7054eca5c2",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Align code style to use `switch` if all states are used",
        "createdAt" : "2020-06-02T06:56:59Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f98207900282ef4d7a27980a4cc018216098236c",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +294,298 @@    @Override\n    public void suspend() {\n        switch (state()) {\n            case CREATED:\n            case SUSPENDED:"
  },
  {
    "id" : "f9dd837e-f8ab-4813-ae40-4292db7b68f9",
    "prId" : 8776,
    "prUrl" : "https://github.com/apache/kafka/pull/8776#pullrequestreview-422390924",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a3d6f84-b54f-43bd-91ea-f7d779299970",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Minor: improve error message",
        "createdAt" : "2020-06-02T06:57:11Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f98207900282ef4d7a27980a4cc018216098236c",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +288,292 @@\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n        }\n    }"
  },
  {
    "id" : "6915f740-d59a-4faf-b6f0-0ed4fec30c59",
    "prId" : 8776,
    "prUrl" : "https://github.com/apache/kafka/pull/8776#pullrequestreview-422390989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2bf14fc-af83-4c52-b4f0-0f0d1dc7dd03",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Minor: improve error message",
        "createdAt" : "2020-06-02T06:57:17Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f98207900282ef4d7a27980a4cc018216098236c",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +328,332 @@\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n        }\n    }"
  },
  {
    "id" : "f7002355-0ec0-4e11-a88e-e2f8f369a721",
    "prId" : 8776,
    "prUrl" : "https://github.com/apache/kafka/pull/8776#pullrequestreview-422882162",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2680b7d-9720-49ff-9057-469e9499ddb3",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Can we use if/ else if here for consistency?",
        "createdAt" : "2020-06-02T17:03:52Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f98207900282ef4d7a27980a4cc018216098236c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +222,226 @@            case RUNNING:\n                return;\n\n            case RESTORING:\n                initializeMetadata();"
  },
  {
    "id" : "2327b3f3-ee81-455c-92b0-8d12bf9b5201",
    "prId" : 8776,
    "prUrl" : "https://github.com/apache/kafka/pull/8776#pullrequestreview-423108505",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99fbe406-42ed-44c1-8ff6-b4c766df0479",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could we merge the case `CLOSED` and `CREATED`? Also could you elaborate why we do empty checkpoint map instead of null?",
        "createdAt" : "2020-06-02T19:04:20Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "9204285d-8e03-4a17-a27f-bb17b87c998f",
        "parentId" : "99fbe406-42ed-44c1-8ff6-b4c766df0479",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Not sure if we can merge CLOSE and CREATED -- but I plan to do follow up PRs to change state handling further. Hence, I would like to keep it out-of-scope for this PR.\r\n\r\n`emptyMap()` is not an empty checkpoint: the map we return is some additional data we write into the checkpoint. `null` on the other hand means to _not_ write any checkpoint but in a clean-close case we want to write a checkpoint.",
        "createdAt" : "2020-06-02T22:49:04Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f98207900282ef4d7a27980a4cc018216098236c",
    "line" : 248,
    "diffHunk" : "@@ -1,1 +578,582 @@\n            case SUSPENDED:\n            case CLOSED:\n                // not need to checkpoint, since when suspending we've already committed the state\n                checkpoint = null; // `null` indicates to not write a checkpoint"
  },
  {
    "id" : "8947edd3-b839-482e-82b4-7900d18e2fe7",
    "prId" : 8776,
    "prUrl" : "https://github.com/apache/kafka/pull/8776#pullrequestreview-423107442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b90adc23-11ba-429d-980f-c5b63b4dd21f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Out of the scope of this PR: why we need to return the checkpoint map to bookkeep at the `task-manager`? It seems we just re-distribute it in the `close` call.\r\n\r\nI think we do not need to expose the checkpoint map in task-manager eventually?",
        "createdAt" : "2020-06-02T20:37:33Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "7ad16c3b-f654-4ac3-8907-08987b7914fe",
        "parentId" : "b90adc23-11ba-429d-980f-c5b63b4dd21f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Not 100% sure what the end state will be atm, but yes, we should be able to simplify this further in follow up PRs.",
        "createdAt" : "2020-06-02T22:46:33Z",
        "updatedAt" : "2020-06-04T22:38:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f98207900282ef4d7a27980a4cc018216098236c",
    "line" : 215,
    "diffHunk" : "@@ -1,1 +550,554 @@        final Map<TopicPartition, Long> checkpoint;\n\n        switch (state()) {\n            case CREATED:\n                // the task is created and not initialized, just re-write the checkpoint file"
  },
  {
    "id" : "1b2335da-1664-43b6-82ee-7369c4bad072",
    "prId" : 8803,
    "prUrl" : "https://github.com/apache/kafka/pull/8803#pullrequestreview-425657619",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4dc6a4d-0ccf-4d24-a477-0bc691553fa2",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could we add a unit test in `StreamTaskTest`?",
        "createdAt" : "2020-06-05T19:42:05Z",
        "updatedAt" : "2020-06-05T22:36:42Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "f23be8f6-9bdb-4f4e-8d20-2eca1b431faf",
        "parentId" : "d4dc6a4d-0ccf-4d24-a477-0bc691553fa2",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Ack, done",
        "createdAt" : "2020-06-05T22:13:39Z",
        "updatedAt" : "2020-06-05T22:36:42Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "61a200ccbd54035438ae8628a6fadb43d3b174a3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +497,501 @@\n    @Override\n    public void update(final Set<TopicPartition> topicPartitions, final Map<String, List<String>> nodeToSourceTopics) {\n        super.update(topicPartitions, nodeToSourceTopics);\n        partitionGroup.updatePartitions(topicPartitions, recordQueueCreator::createQueue);"
  },
  {
    "id" : "ad0c4212-14ef-4b43-801d-4bb164bea2e6",
    "prId" : 8803,
    "prUrl" : "https://github.com/apache/kafka/pull/8803#pullrequestreview-425658856",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ba3161d-d897-4d0f-a388-3125ce91d8b6",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Thanks for the cleanup!",
        "createdAt" : "2020-06-05T22:17:10Z",
        "updatedAt" : "2020-06-05T22:36:42Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "61a200ccbd54035438ae8628a6fadb43d3b174a3",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +520,524 @@                throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n        }\n"
  },
  {
    "id" : "eb06a61e-ac89-422d-88d2-ad0a084f603c",
    "prId" : 8820,
    "prUrl" : "https://github.com/apache/kafka/pull/8820#pullrequestreview-425699917",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65f8cdc4-0791-4796-a5bc-43d43b3e6dc8",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I think we could just reset any checkpoint at the beginning, so that we only do checkpointing if this call thinks so. @mjsax ",
        "createdAt" : "2020-06-06T00:49:47Z",
        "updatedAt" : "2020-06-06T15:46:57Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "21e85aa2-b695-4a9a-92dd-59373f352a7f",
        "parentId" : "65f8cdc4-0791-4796-a5bc-43d43b3e6dc8",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "That should work.",
        "createdAt" : "2020-06-06T01:10:05Z",
        "updatedAt" : "2020-06-06T15:46:57Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc9c9dacb0254f78b628d845f1c3e2d8e91fb2e0",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +548,552 @@     */\n    private void prepareClose(final boolean clean) {\n        // Reset any previously scheduled checkpoint.\n        checkpoint = null;\n"
  },
  {
    "id" : "a65c8cc9-5b6f-4146-8a6f-831e5728473f",
    "prId" : 8833,
    "prUrl" : "https://github.com/apache/kafka/pull/8833#pullrequestreview-426453007",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c0d841a-dc0c-4e74-8683-c07d9fbc52bb",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Instead of \"blindly\" writing a checkpoint in `postCommit()`, we only do it if a checkpoint get's scheduled.",
        "createdAt" : "2020-06-08T17:19:07Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "329e187a9187c4ea532ce88e0202402cb6f78b86",
    "line" : 151,
    "diffHunk" : "@@ -1,1 +343,347 @@            case RESTORING:\n            case SUSPENDED:\n                maybeScheduleCheckpoint();\n                stateMgr.flush();\n                recordCollector.flush();"
  },
  {
    "id" : "9f508ed7-dd2e-487c-8900-f4207e837ac5",
    "prId" : 8833,
    "prUrl" : "https://github.com/apache/kafka/pull/8833#pullrequestreview-428416478",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "572d4c43-eac9-4efd-b03c-ba5e468a9494",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Prefer to throw different illegal state exception here than making comments",
        "createdAt" : "2020-06-08T22:10:46Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "0cf3723e-7995-47b8-b13e-ca13e141a661",
        "parentId" : "572d4c43-eac9-4efd-b03c-ba5e468a9494",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "What does it improve?",
        "createdAt" : "2020-06-09T01:23:17Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "81eeef20-7632-42c1-b98b-fe47f0bde9ce",
        "parentId" : "572d4c43-eac9-4efd-b03c-ba5e468a9494",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Maybe not necessary after a second thought. However, one more question: why not making `closeAndRecycleState` idempotent as well?",
        "createdAt" : "2020-06-10T16:16:56Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "c540e208-6ba8-4d71-848c-f007e5e6321b",
        "parentId" : "572d4c43-eac9-4efd-b03c-ba5e468a9494",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I see your point, but for this case, I would prefer to introduce a new state -- atm, `closeAndRecycleState` transits to `CLOSED` state what is the same as when we actually close a task -- however, the `stateMgr` would be closed for a proper `CLOSED` state, while for recycling the `stateMgr` is not closed -- so in general, the `CLOSED` state is not a \"safe\" state to provide idempotence. Thoughts?",
        "createdAt" : "2020-06-10T19:39:22Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "56f1ba1c-5a7b-4b6a-b5fe-506d53be3665",
        "parentId" : "572d4c43-eac9-4efd-b03c-ba5e468a9494",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "That makes sense",
        "createdAt" : "2020-06-10T20:48:18Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "329e187a9187c4ea532ce88e0202402cb6f78b86",
    "line" : 358,
    "diffHunk" : "@@ -1,1 +487,491 @@                break;\n\n            case RESTORING: // we should have transitioned to `SUSPENDED` already\n            case RUNNING: // we should have transitioned to `SUSPENDED` already\n            case CLOSED:"
  },
  {
    "id" : "5f84f65a-3b61-4515-b00f-9cbb710289a0",
    "prId" : 8833,
    "prUrl" : "https://github.com/apache/kafka/pull/8833#pullrequestreview-427591550",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d38a7c56-cfe3-4ea5-9fec-3775977b8b83",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could we merge RESTORING and SUSPENDED?",
        "createdAt" : "2020-06-09T20:09:15Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "1c2fc576-5a4c-4200-9611-239c071d4839",
        "parentId" : "d38a7c56-cfe3-4ea5-9fec-3775977b8b83",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "+1, IDEA also suggests it :)",
        "createdAt" : "2020-06-09T22:57:20Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "329e187a9187c4ea532ce88e0202402cb6f78b86",
    "line" : 396,
    "diffHunk" : "@@ -1,1 +505,509 @@    private void maybeScheduleCheckpoint() {\n        switch (state()) {\n            case RESTORING:\n            case SUSPENDED:\n                this.checkpoint = checkpointableOffsets();"
  },
  {
    "id" : "7e2db7ac-ff43-4597-8add-bee0435c8046",
    "prId" : 8833,
    "prUrl" : "https://github.com/apache/kafka/pull/8833#pullrequestreview-428366910",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ace3f65-59d0-406d-b065-7a6564c5fa17",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Maybe we can skip calling this if we are in `RESTORING`; I have another comment below.\r\n\r\nAlso could we add javadoc on top explaining what exception can be thrown?",
        "createdAt" : "2020-06-09T22:56:14Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "61aa0f72-4483-4610-b5f0-4707b58e52d4",
        "parentId" : "3ace3f65-59d0-406d-b065-7a6564c5fa17",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Is this addressed?",
        "createdAt" : "2020-06-10T02:46:16Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "27f543f7-d210-44c1-890c-a4840a9087d0",
        "parentId" : "3ace3f65-59d0-406d-b065-7a6564c5fa17",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes. `RESTORING` above is it's own \"case\" branch now (before `RUNNING` and `RESTORING` was shared the code).",
        "createdAt" : "2020-06-10T19:33:15Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "329e187a9187c4ea532ce88e0202402cb6f78b86",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +265,269 @@                try {\n                    // use try-catch to ensure state transition to SUSPENDED even if user code throws in `Processor#close()`\n                    closeTopology();\n                } finally {\n                    transitionTo(State.SUSPENDED);"
  },
  {
    "id" : "067e2fa8-2426-4eeb-b176-0f04ce5b6cca",
    "prId" : 8833,
    "prUrl" : "https://github.com/apache/kafka/pull/8833#pullrequestreview-428260540",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e30140e-6ab0-4cc5-be07-e50a8336af10",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Maybe we can make an optimization by remembering the `committableOffsets`, and then if the value (both offset and time) does not change we do not need to give it out to consumer to commit.",
        "createdAt" : "2020-06-09T22:59:43Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "bff5f1d3-8b21-457f-b279-d4687faa101f",
        "parentId" : "6e30140e-6ab0-4cc5-be07-e50a8336af10",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Well, what is the probability that it did not change? Feel free to file a ticket if you think it's worth it, but I would like to not piggy-back other things into the PR>",
        "createdAt" : "2020-06-10T00:38:06Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "c0acb721-8be9-4e02-812c-146582985252",
        "parentId" : "6e30140e-6ab0-4cc5-be07-e50a8336af10",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "+1, this seems not really necessary atm.",
        "createdAt" : "2020-06-10T16:10:18Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "18836ad4-d7cf-4739-9817-91ef1eebade0",
        "parentId" : "6e30140e-6ab0-4cc5-be07-e50a8336af10",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Sounds fair, we can do that in another PR.",
        "createdAt" : "2020-06-10T17:05:45Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "329e187a9187c4ea532ce88e0202402cb6f78b86",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +376,380 @@                final Map<TopicPartition, Long> partitionTimes = extractPartitionTimes();\n\n                committableOffsets = new HashMap<>(consumedOffsets.size());\n                for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n                    final TopicPartition partition = entry.getKey();"
  },
  {
    "id" : "67aa39d1-62fc-46c1-a25b-53f06d120d6b",
    "prId" : 8833,
    "prUrl" : "https://github.com/apache/kafka/pull/8833#pullrequestreview-429295554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why we need this check?\r\n\r\nAlso nit: how about `maybeWriteCheckpoint` to align with `maybeScheduleCheckpoint`.",
        "createdAt" : "2020-06-09T23:02:25Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "1a51c6cf-4a9f-406a-adf8-8d53ddcc1d5f",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why do we need any check? To avoid bugs :)\r\n\r\nIf like the different names, because \"maybe\" indicated that the method makes a decision, while \"ifNeeded\" implies that the methods executed a decision that was already made? At least my personal interpretation?",
        "createdAt" : "2020-06-10T00:40:18Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "e0c709b7-6ae9-43fc-8d46-bc18fb83ee73",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "1) okay, fair enough :)\r\n\r\n2) actually I think we use `maybeXXX` and `YYYIfNeeded` across the repo for both semantics :) my very paranoid nit intention is to just make the private function names more aligned. Your call.",
        "createdAt" : "2020-06-10T17:11:07Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "e4a48898-7e98-4199-9cc7-2b9fdb775d7e",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "If we want to align them, I would recommend to go a single PR to align all of them at once :)",
        "createdAt" : "2020-06-10T19:49:21Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "aaef83e9-6f68-4c06-88e8-9c77321e5b5c",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "@mjsax  By `should only be written if no commit is needed` do you mean `...if a commit was just completed`? \r\nDoesn't this break `closeAndRecycleState` (I thought iI saw in another comment that we don't write checkpoints during recycle anymore?)",
        "createdAt" : "2020-06-11T17:33:47Z",
        "updatedAt" : "2020-06-11T17:35:03Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "65da2e0a-78ed-4b05-8d7b-8f32e9ff6e47",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Maybe I'm thinking of standby tasks (ie we only skip checkpointing for recycled standbys). For active tasks, we should probably commit them before recycling right? Or is it ok to skip committing altogether 🤔 ",
        "createdAt" : "2020-06-11T17:40:49Z",
        "updatedAt" : "2020-06-11T17:40:49Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "6a49f42c-5bbe-4145-9b93-803cb25c7dd5",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "@guozhangwang  Just saw this in the `SmokeTestDriverIntegrationTest#shouldWorkWithRebalance` -- not sure if it merits a separate ticket or can just be fixed together with https://issues.apache.org/jira/browse/KAFKA-10150 ?\r\n```\r\nCaused by: java.lang.IllegalStateException: A checkpoint should only be written if no commit is needed.\r\n\tat org.apache.kafka.streams.processor.internals.StreamTask.writeCheckpointIfNeed(StreamTask.java:534)\r\n\tat org.apache.kafka.streams.processor.internals.StreamTask.closeAndRecycleState(StreamTask.java:482)\r\n\tat org.apache.kafka.streams.processor.internals.StandbyTaskCreator.createStandbyTaskFromActive(StandbyTaskCreator.java:115)\r\n\tat org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:288)\r\n```",
        "createdAt" : "2020-06-11T18:59:41Z",
        "updatedAt" : "2020-06-11T18:59:50Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "56c638e1-f6e4-441b-890c-2d37253db99d",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Nice catch. Let's just fix it along with 10150?",
        "createdAt" : "2020-06-11T21:16:08Z",
        "updatedAt" : "2020-06-11T21:16:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "a6bbd2b2-ac5a-4e9f-9958-634cc0fb2065",
        "parentId" : "3b077a3c-974b-4529-b8f9-389df100b29f",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Too late, I already created a ticket for it 🙂 But after starting to work on it, I agree, they should be addressed in one PR",
        "createdAt" : "2020-06-11T21:18:14Z",
        "updatedAt" : "2020-06-11T21:18:14Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "329e187a9187c4ea532ce88e0202402cb6f78b86",
    "line" : 441,
    "diffHunk" : "@@ -1,1 +528,532 @@\n    private void writeCheckpointIfNeed() {\n        if (commitNeeded) {\n            throw new IllegalStateException(\"A checkpoint should only be written if no commit is needed.\");\n        }"
  },
  {
    "id" : "bdd9aa7a-203e-409e-b7c5-4c4657473292",
    "prId" : 8833,
    "prUrl" : "https://github.com/apache/kafka/pull/8833#pullrequestreview-427640403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01f3353e-1338-4da1-b3ed-20fee1c94ea5",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think we actually do not need to commit (including write-checkpoint) when closeAndRecycle actually, and only need to suspend the task before recycle it. But this is out of the scope and we can discuss about this in another PR (cc @ableegoldman ).",
        "createdAt" : "2020-06-09T23:15:15Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "170f45e9-b97b-46a5-9385-0fc2b22dd710",
        "parentId" : "01f3353e-1338-4da1-b3ed-20fee1c94ea5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Assuming rebalancing does not happen often (in a stable deployment) it might be re-mature optimization?",
        "createdAt" : "2020-06-10T00:52:31Z",
        "updatedAt" : "2020-06-10T22:33:10Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "329e187a9187c4ea532ce88e0202402cb6f78b86",
    "line" : 345,
    "diffHunk" : "@@ -1,1 +476,480 @@    public void closeAndRecycleState() {\n        suspend();\n        prepareCommit();\n        writeCheckpointIfNeed();\n"
  },
  {
    "id" : "100abc69-99be-4b53-9b5c-e76de9c67d5a",
    "prId" : 8882,
    "prUrl" : "https://github.com/apache/kafka/pull/8882#pullrequestreview-431909259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "645d723b-9203-4bd3-b379-78ab2470b228",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "We previously relied on the task manager to remove these sensors before calling close, but forgot to do it before recycling. In retrospect, it's better to do it within the same class that creates the sensors to begin with.",
        "createdAt" : "2020-06-16T20:18:43Z",
        "updatedAt" : "2020-06-17T03:42:08Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "632481d5-6318-49fc-b55e-167c2b67104f",
        "parentId" : "645d723b-9203-4bd3-b379-78ab2470b228",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Agreed, we should clean up anything we created in the same class",
        "createdAt" : "2020-06-16T21:43:29Z",
        "updatedAt" : "2020-06-17T03:42:08Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae1a3908bcaad82e30ad35a0e4462d7a4b7978b5",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +464,468 @@    @Override\n    public void closeClean() {\n        streamsMetrics.removeAllTaskLevelSensors(Thread.currentThread().getName(), id.toString());\n        close(true);\n        log.info(\"Closed clean\");"
  },
  {
    "id" : "317777d6-d1b1-45ba-b743-39926939af81",
    "prId" : 8882,
    "prUrl" : "https://github.com/apache/kafka/pull/8882#pullrequestreview-431857684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d730715e-eabc-4001-8e20-cc7c132767b2",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Fixes the sensor leak by simply registering these as task-level sensors. Note the node name is still provided to scope the sensors themselves.",
        "createdAt" : "2020-06-16T20:19:35Z",
        "updatedAt" : "2020-06-17T03:42:08Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae1a3908bcaad82e30ad35a0e4462d7a4b7978b5",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +153,157 @@            e2eLatencySensors.put(\n                terminalNodeName,\n                TaskMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, RecordingLevel.INFO, streamsMetrics)\n            );\n        }"
  },
  {
    "id" : "3e9d0ee7-d261-4729-842b-bb5476be6b7d",
    "prId" : 8900,
    "prUrl" : "https://github.com/apache/kafka/pull/8900#pullrequestreview-435332563",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "459bf89d-2008-4202-af56-4d3cdb5fa52b",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "It seems this is the only place we call `closeDirty`, thus, I am wondering if it might be better to use a boolean flag ie, `RecordCollector#close(boolean)` and just call `() -> recordCollector(clean)` here?",
        "createdAt" : "2020-06-19T22:40:21Z",
        "updatedAt" : "2020-06-23T22:09:07Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "d7a095fd-6d9f-4ac6-a9a8-0dc74a08f69a",
        "parentId" : "459bf89d-2008-4202-af56-4d3cdb5fa52b",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I'm just following the pattern we use elsewhere with `closeDirty`/`closeClean`. Personally I think it does make the code a bit more readable so you don't have to then go and look up what the boolean argument to `RecordCollector#close` actually is",
        "createdAt" : "2020-06-22T22:02:57Z",
        "updatedAt" : "2020-06-23T22:09:07Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "264b3ded-5c9a-4a13-b1ab-c428c2c94c87",
        "parentId" : "459bf89d-2008-4202-af56-4d3cdb5fa52b",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Sure. Was just an idea.",
        "createdAt" : "2020-06-22T23:05:45Z",
        "updatedAt" : "2020-06-23T22:09:07Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "20321fd1325d402405a4b5899c22c83a51732b1c",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +547,551 @@                TaskManager.executeAndMaybeSwallow(\n                    clean,\n                    clean ? recordCollector::closeClean : recordCollector::closeDirty,\n                    \"record collector close\",\n                    log"
  },
  {
    "id" : "581a58dd-c6be-4c62-a51b-f915472d44c4",
    "prId" : 8924,
    "prUrl" : "https://github.com/apache/kafka/pull/8924#pullrequestreview-437057227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7c0049c-ccdb-450b-97b3-a3518cb11463",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "This diff turned out a bit awkward, basically I just factored this check out into a separate method that we should call at the beginning of both flavors of clean close",
        "createdAt" : "2020-06-24T22:46:45Z",
        "updatedAt" : "2020-06-24T22:55:26Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc71861dfcb093d5d430f8e7a3fa8786c1d12fb1",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +531,535 @@     * You must commit a task and checkpoint the state manager before closing as this will release the state dir lock\n     */\n    private void close(final boolean clean) {\n        switch (state()) {\n            case SUSPENDED:"
  },
  {
    "id" : "cb9e7577-30f1-4307-996e-a70d9c377765",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-454643019",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "221a01cf-a115-445c-8ce8-b9d324e4483e",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I decided to extract out the update of the changelog offsets from actually writing the offsets since even if we do not want to write the file, we still need to update the offsets.\r\n\r\nThe reason I did not yet remove the parameter from `checkpoint` is that global-task is still using it. I plan to remove it when consolidating the global task.",
        "createdAt" : "2020-07-01T03:25:32Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "4a51fbb5-7b1c-4422-8d88-acfb5685332d",
        "parentId" : "221a01cf-a115-445c-8ce8-b9d324e4483e",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Was there a reason to not just add `#updateChangelogOffsets` to the `StateManager` interface and remove the checkpointable offsets argument from `#checkpoint`?",
        "createdAt" : "2020-07-21T22:28:11Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "7d832446-ea70-4599-9fa8-9fa430f84e62",
        "parentId" : "221a01cf-a115-445c-8ce8-b9d324e4483e",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Let me try and see if that works; will get back to you.",
        "createdAt" : "2020-07-24T05:35:32Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 204,
    "diffHunk" : "@@ -1,1 +532,536 @@     */\n    @Override\n    protected void maybeWriteCheckpoint(final boolean enforceCheckpoint) {\n        // commitNeeded indicates we may have processed some records since last commit\n        // and hence we need to refresh checkpointable offsets regardless whether we should checkpoint or not"
  },
  {
    "id" : "daaf20fd-bc67-4b66-8fe5-5d7d3ab81156",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-462915780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53da1320-dd93-46c5-8209-2ffedac11b83",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "It seems like almost every method might throw `StreamsException` and/or `TaskMigratedExcetpion` -- is it really worth to have those comments all over the place?",
        "createdAt" : "2020-08-05T23:51:27Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "3db89733-7910-4cb6-a1ea-35a69c326009",
        "parentId" : "53da1320-dd93-46c5-8209-2ffedac11b83",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think it is a good habit keeping that since those functions are called by various callers and if the caller do not catch them, the the ancestor callers should -- I've personally found such debugging very helpful.",
        "createdAt" : "2020-08-06T01:00:42Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "a9ec4d35-fcb5-4be6-acc4-6523174e0934",
        "parentId" : "53da1320-dd93-46c5-8209-2ffedac11b83",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Works for me -- I am just \"worried\" that those comments might get out-of-sync with the code at some point leading to bug, as we rely on the comments when changing the caller code...",
        "createdAt" : "2020-08-06T22:23:22Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +348,352 @@    /**\n     * @throws StreamsException fatal error that should cause the thread to die\n     * @throws TaskMigratedException recoverable error that would cause the task to be removed\n     * @return offsets that should be committed for this task\n     */"
  },
  {
    "id" : "00df514a-3d65-4718-bc14-6abbe940575c",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-462135292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9fcf491-92e3-45f0-9c1f-cb18cfcd3855",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "nit: Why this? We know that we are in state `created` ?",
        "createdAt" : "2020-08-05T23:53:57Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "f4ca7499-5ef6-493c-ab28-621b2837b241",
        "parentId" : "c9fcf491-92e3-45f0-9c1f-cb18cfcd3855",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I know, but I just want to make the text consistent (in other places it would use `state()` which is capitalized..) really nit thing.",
        "createdAt" : "2020-08-06T01:55:38Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +438,442 @@                // We should never write a checkpoint for a CREATED task as we may overwrite an existing checkpoint\n                // with empty uninitialized offsets\n                log.debug(\"Skipped writing checkpoint for {} task\", state());\n\n                break;"
  },
  {
    "id" : "23fd2e4d-aa73-44ac-aa6b-9293ede12373",
    "prId" : 8964,
    "prUrl" : "https://github.com/apache/kafka/pull/8964#pullrequestreview-462138998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "163b9381-9778-4f07-b997-d3d418383096",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Nit: Can we keep `SUSPENDED` after `RUNNING` case? We use the same order in all methods and always follow the \"natural\" state transition order, that is CREATE, RESTORING, RUNNING, SUSPENDED, CLOSED.",
        "createdAt" : "2020-08-05T23:56:56Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "d48aac51-c38b-4239-b44b-1aa65372e6b6",
        "parentId" : "163b9381-9778-4f07-b997-d3d418383096",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Seems IDEA liked it the other way (it auto merges these two cases). ",
        "createdAt" : "2020-08-06T02:08:21Z",
        "updatedAt" : "2020-08-11T21:25:16Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4f2272986357de9c48032d1fd0d9b1482d5a974",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +443,447 @@\n            case RESTORING:\n            case SUSPENDED:\n                maybeWriteCheckpoint(enforceCheckpoint);\n                log.debug(\"Finalized commit for {} task with enforce checkpoint {}\", state(), enforceCheckpoint);"
  },
  {
    "id" : "515d8d67-b7b1-460a-b173-54ce0e6fcafc",
    "prId" : 9361,
    "prUrl" : "https://github.com/apache/kafka/pull/9361#pullrequestreview-500554881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0375747d-dc72-4350-9d5e-cadbe8ccfc65",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This is pulling out the timestamp and headers that we just set a few lines earlier.",
        "createdAt" : "2020-10-01T20:58:45Z",
        "updatedAt" : "2020-10-02T15:50:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8de75f6f16cf8bdcccbcb1bc1fc0a11dd40c1d1",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +689,693 @@                record.value(),\n                processorContext.timestamp(),\n                processorContext.headers()\n            );\n            maybeMeasureLatency(() -> currNode.process(toProcess), time, processLatencySensor);"
  },
  {
    "id" : "913b5e59-90b1-4178-8c91-543d91b574b3",
    "prId" : 9361,
    "prUrl" : "https://github.com/apache/kafka/pull/9361#pullrequestreview-500554881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ebec86d9-0d02-43db-8853-54052483c6d5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Instead of setting a dummy context, we're now just setting the context to `null` aka \"undefined\".",
        "createdAt" : "2020-10-01T20:59:21Z",
        "updatedAt" : "2020-10-02T15:50:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8de75f6f16cf8bdcccbcb1bc1fc0a11dd40c1d1",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +761,765 @@        }\n\n        updateProcessorContext(node, time.milliseconds(), null);\n\n        if (log.isTraceEnabled()) {"
  },
  {
    "id" : "4177491b-49d7-4c08-b0df-66e166707b2e",
    "prId" : 9361,
    "prUrl" : "https://github.com/apache/kafka/pull/9361#pullrequestreview-501204733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "527db3cd-32ee-4479-a1dc-50a56a630aeb",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think this is a proof of code simplicity that recordContext is unnecessarily passing around here :)",
        "createdAt" : "2020-10-01T21:51:17Z",
        "updatedAt" : "2020-10-02T15:50:27Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "154fb14f-e119-4012-b7a0-274164de8e19",
        "parentId" : "527db3cd-32ee-4479-a1dc-50a56a630aeb",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Yeah, I agree. I think this is much simpler for both the implementation and the API. Sorry I didn't see it at first ;) ",
        "createdAt" : "2020-10-02T14:52:07Z",
        "updatedAt" : "2020-10-02T15:50:27Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8de75f6f16cf8bdcccbcb1bc1fc0a11dd40c1d1",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +691,695 @@                processorContext.headers()\n            );\n            maybeMeasureLatency(() -> currNode.process(toProcess), time, processLatencySensor);\n\n            log.trace(\"Completed processing one record [{}]\", record);"
  },
  {
    "id" : "8c501c5f-3b13-4752-8c27-d9f546d38920",
    "prId" : 9570,
    "prUrl" : "https://github.com/apache/kafka/pull/9570#pullrequestreview-532605103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87e6de0a-8730-491d-b24a-295685a5bfc2",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It might still be nice to see the stacktrace here (even if it also gets logged elsewhere). If you want to do it, don't forget you have to change to using `String.format` for the variable substitution.\r\n\r\nI don't feel strongly in this case, so I'll defer to you whether you want to do this or not.",
        "createdAt" : "2020-11-16T20:52:15Z",
        "updatedAt" : "2020-11-18T20:00:55Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "c119f50f-e06f-40ba-a474-4158e71af5b3",
        "parentId" : "87e6de0a-8730-491d-b24a-295685a5bfc2",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I guess I leave it as-is.",
        "createdAt" : "2020-11-17T17:16:15Z",
        "updatedAt" : "2020-11-18T20:00:55Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b919fa30c48dcf77d1cff44f4a82fc6fb1494269",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +823,827 @@                    \"\\nConsider overwriting consumer config {} to a larger value to avoid timeout errors\",\n                time.toString(),\n                ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);\n\n            // re-throw to trigger `task.timeout.ms`"
  },
  {
    "id" : "701d53b8-58b2-42d6-b09c-0c006bb7f406",
    "prId" : 9840,
    "prUrl" : "https://github.com/apache/kafka/pull/9840#pullrequestreview-573600588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec70e605-20e3-4fb9-9ee3-78041e586794",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The only reason that we need to add this function at `Task` seems to be `tasks.activeTasksForInputPartition(partition)` at `TaskManager`. and there's a TODO to convert its return to `StreamTask` anyways. So let's just move this function to `StreamTask` only and in `TaskManager` force convert the `task` to `StreamTask`. And then we can remove it from `StandbyTask`.",
        "createdAt" : "2021-01-21T01:57:17Z",
        "updatedAt" : "2021-01-28T00:19:52Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "2f876e2d-dbf3-441a-aef3-6cd7e9ce9c41",
        "parentId" : "ec70e605-20e3-4fb9-9ee3-78041e586794",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Wouldn't it be good to avoid taking on unrelated refactoring TODOs in this PR? It seems better to me to leave it to whoever decides to pick up that TODO and instead just keep this PR focused on the feature.",
        "createdAt" : "2021-01-21T18:06:30Z",
        "updatedAt" : "2021-01-28T00:19:52Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc1afd02eae2120b4bc557cec69b9044625c5b33",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +903,907 @@\n    @Override\n    public void addFetchedMetadata(final TopicPartition partition, final ConsumerRecords.Metadata metadata) {\n        partitionGroup.addFetchedMetadata(partition, metadata);\n    }"
  },
  {
    "id" : "9fecf287-842f-4ffb-8e54-fa6d8975739a",
    "prId" : 9997,
    "prUrl" : "https://github.com/apache/kafka/pull/9997#pullrequestreview-578942594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "306b4e3d-4040-4ec3-a1a4-19f4741e86f4",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "We pull this variable out form the method to use it as a \"cache\" -- if we fail on send(), we can process the record a second time.",
        "createdAt" : "2021-01-29T05:01:09Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3fcf016539392f428d56a7b0ef7f35ff3f47d48",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +101,105 @@    private final RecordQueueCreator recordQueueCreator;\n\n    private StampedRecord record;\n    private boolean commitNeeded = false;\n    private boolean commitRequested = false;"
  },
  {
    "id" : "2f44ff2c-34a8-481c-89c5-132d55701aec",
    "prId" : 9997,
    "prUrl" : "https://github.com/apache/kafka/pull/9997#pullrequestreview-578942830",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "909566e4-ee64-4b5a-9d73-a302cfdf9099",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "If we have a cached record, it implies we failed to process it before -- thus we don't pull a new record from the buffer but retry to process the cached record.",
        "createdAt" : "2021-01-29T05:01:59Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3fcf016539392f428d56a7b0ef7f35ff3f47d48",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +667,671 @@    @SuppressWarnings(\"unchecked\")\n    public boolean process(final long wallClockTime) {\n        if (record == null) {\n            if (!isProcessable(wallClockTime)) {\n                return false;"
  },
  {
    "id" : "2580baea-bdb4-4229-8f12-cf57975b1a79",
    "prId" : 9997,
    "prUrl" : "https://github.com/apache/kafka/pull/9997#pullrequestreview-580927529",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eebf884-2a9d-4d0f-aa6c-65b5723de9c5",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "For this case, we don't need the cached record, as we need to reset the task anyway to cleanup potentially \"corrupted\" state store.",
        "createdAt" : "2021-01-29T05:02:38Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "a7a38545-f50d-492f-afa2-f2a69b96d4b0",
        "parentId" : "6eebf884-2a9d-4d0f-aa6c-65b5723de9c5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Note that we don't trigger `task.timeout.ms` for this case atm. Because we need to restore state what might talk some time, it seems questionable if we should tigger `task.timeout.ms` for this case of not.\r\n\r\nCf. TaskManager#process() that catches `TimeoutException` and trigger the timeout, but does not catch `TaskCorruptedException`",
        "createdAt" : "2021-01-29T05:24:14Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "b990eeac-09eb-4673-bfbc-fd9cce3ca74f",
        "parentId" : "6eebf884-2a9d-4d0f-aa6c-65b5723de9c5",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I agree, we shouldn't trigger `task.timeout.ms` until it's back to RUNNING right?",
        "createdAt" : "2021-01-30T01:18:37Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "bf1a92e1-87d0-450a-82ec-4ef3ed7163bb",
        "parentId" : "6eebf884-2a9d-4d0f-aa6c-65b5723de9c5",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Not sure if starting the timeout at all makes sense for this case?\r\n\r\nAlso, starting it when we transit to RUNNING seems to be a non-trivial change.",
        "createdAt" : "2021-02-02T02:07:40Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3fcf016539392f428d56a7b0ef7f35ff3f47d48",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +728,732 @@            } else {\n                record = null;\n                throw new TaskCorruptedException(Collections.singletonMap(id, changelogPartitions()));\n            }\n        } catch (final StreamsException exception) {"
  },
  {
    "id" : "7ffea406-0eef-468d-b30e-22841bbccb67",
    "prId" : 9997,
    "prUrl" : "https://github.com/apache/kafka/pull/9997#pullrequestreview-582655532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d67a9ada-cde3-41dd-856d-94548f13638f",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It seems like KIP-572 should retry only when it's safe to do so. Under ALOS, it's not safe to retry, so we should just crash here, right?",
        "createdAt" : "2021-02-01T19:18:13Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "e763b97b-7451-49fe-8469-9aa95d77c771",
        "parentId" : "d67a9ada-cde3-41dd-856d-94548f13638f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "If I'm reading the code right, this would also mean we don't need to mess with caching the StampedRecord, which would be nice.",
        "createdAt" : "2021-02-01T19:19:59Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "ee512ea5-c44b-47e1-86fb-2b72505a1633",
        "parentId" : "d67a9ada-cde3-41dd-856d-94548f13638f",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "@vvcephei what do you mean by \"crash\" here? Throw TaskCorrutpedException?",
        "createdAt" : "2021-02-01T21:39:36Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9e5bb723-e9ae-4aa7-b88f-f3924c3c7773",
        "parentId" : "d67a9ada-cde3-41dd-856d-94548f13638f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why would it not be safe to retry the commit for at-least-once? -- The indention of the PR was to retry for this case. -- It may lead to duplication, but that seems fine with at-least-once.",
        "createdAt" : "2021-02-02T02:12:45Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "c3222e8a-2981-480c-9fb1-2e9d8786515c",
        "parentId" : "d67a9ada-cde3-41dd-856d-94548f13638f",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks, all, sorry for the delay. I meant \"crash\" as in, crash the thread. This is what happens today, right? We throw the TimeoutException, convert it to a StreamsException, and throw it up to the top level, killing the thread?\r\n\r\nWhat I am thinking is that it's silly to just crash a thread when it has gotten a timeout in `poll()` or in checking `partitions()` because we can harmlessly retry those operations. Along those lines, retrying `commit` should be safe as well.\r\n\r\nBut it looks like this PR is not retrying just `commit`, but is retrying the entire processing of a record. Is that a wrong conclusion?\r\n\r\nRetrying \"process a record\" is definitely not \"safe\" or \"harmless\" it will absolutely result in producing incorrect results, therefore we should not do it. ",
        "createdAt" : "2021-02-03T16:51:21Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "309207c3-4d1d-488c-8f11-439db39b6067",
        "parentId" : "d67a9ada-cde3-41dd-856d-94548f13638f",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "> But it looks like this PR is not retrying just commit, but is retrying the entire processing of a record. Is that a wrong conclusion?\r\n>\r\n> Retrying \"process a record\" is definitely not \"safe\" or \"harmless\" it will absolutely result in producing incorrect results, therefore we should not do it.\r\n\r\nWell, overall I agree, but if we crash the thread with at-least-once, a we rebalance, the other thread will reprocess based on the last committed offset, so the net result is, that a rebalance might reprocess even _more_ records -- if we handle the timeout, we only try to reprocess a single record.",
        "createdAt" : "2021-02-03T18:08:45Z",
        "updatedAt" : "2021-02-04T00:27:40Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3fcf016539392f428d56a7b0ef7f35ff3f47d48",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +725,729 @@        } catch (final TimeoutException timeoutException) {\n            if (!eosEnabled) {\n                throw timeoutException;\n            } else {\n                record = null;"
  },
  {
    "id" : "3cecf7b9-4ba7-4db6-aae1-5eff64d9c6ce",
    "prId" : 10000,
    "prUrl" : "https://github.com/apache/kafka/pull/10000#pullrequestreview-584788677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec884f24-90aa-489e-8c5a-4bc75e353054",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "cool, thanks, this seems much cleaner to me",
        "createdAt" : "2021-02-06T00:10:28Z",
        "updatedAt" : "2021-02-06T00:10:29Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce7eeb403e31a70a20d4a0581068fa6b1c2c6a94",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +247,251 @@\n            case RESTORING:\n                resetOffsetsIfNeededAndInitializeMetadata(offsetResetter);\n                initializeTopology();\n                processorContext.initialize();"
  },
  {
    "id" : "a3f3a1ea-bf97-4c8e-801e-aec16fa32477",
    "prId" : 10170,
    "prUrl" : "https://github.com/apache/kafka/pull/10170#pullrequestreview-596023679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0ac8102-5ffc-4e51-af84-63db1150d6d9",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why not pass `null` ? ",
        "createdAt" : "2021-02-22T19:52:04Z",
        "updatedAt" : "2021-02-24T04:39:03Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "51e95e40-5256-4c92-9935-9cf428df7570",
        "parentId" : "c0ac8102-5ffc-4e51-af84-63db1150d6d9",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I'm following the existing behavior here: if the record context is `null`, we also return a `return new RecordHeaders();`.",
        "createdAt" : "2021-02-23T06:23:51Z",
        "updatedAt" : "2021-02-24T04:39:03Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "485df54fb2822ae5d1a97fbce9d92f2277bacff2",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +798,802 @@            -1,\n            null,\n            new RecordHeaders()\n        );\n        updateProcessorContext(node, time.milliseconds(), recordContext);"
  },
  {
    "id" : "3d3e62d2-47db-483c-bc86-1edfedf957c2",
    "prId" : 10407,
    "prUrl" : "https://github.com/apache/kafka/pull/10407#pullrequestreview-622796047",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8ac4180-d8dd-41ee-ba85-d20b03cf2ac4",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Can we do a follow up PR that extends the `StreamTaskTest` unit test to verify that we reset the flags?",
        "createdAt" : "2021-03-28T23:18:35Z",
        "updatedAt" : "2021-03-29T20:53:44Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "9aa2ab85-22c1-4a5b-886f-c2eeee570ade",
        "parentId" : "b8ac4180-d8dd-41ee-ba85-d20b03cf2ac4",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "done (in this PR)",
        "createdAt" : "2021-03-28T23:24:24Z",
        "updatedAt" : "2021-03-29T20:53:44Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa4cde53e6e3f60b90fafa2ec54fba011e6aba6c",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +498,502 @@    }\n\n    private void clearCommitStatuses() {\n        commitNeeded = false;\n        commitRequested = false;"
  }
]