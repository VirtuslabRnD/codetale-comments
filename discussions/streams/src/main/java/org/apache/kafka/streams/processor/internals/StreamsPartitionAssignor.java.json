[
  {
    "id" : "fed71ff1-361e-48e3-949c-9dbd63ec4ee2",
    "prId" : 4630,
    "prUrl" : "https://github.com/apache/kafka/pull/4630#pullrequestreview-100636363",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0d9e539-9e7b-47a2-8018-218b196084a8",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Could we add a unit test with two subscriptions, v1 and v2, and then check that the returned assignment map contains v1?",
        "createdAt" : "2018-03-02T00:13:32Z",
        "updatedAt" : "2018-03-02T23:28:08Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "994187626421459d0895add72fa09e38a021aa17",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +286,290 @@     */\n    @Override\n    public Map<String, Assignment> assign(final Cluster metadata,\n                                          final Map<String, Subscription> subscriptions) {\n        // construct the client metadata from the decoded subscription info"
  },
  {
    "id" : "a4a07b2e-02c5-4191-90a8-c50590d81683",
    "prId" : 4880,
    "prUrl" : "https://github.com/apache/kafka/pull/4880#pullrequestreview-112959764",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e1fd052-75c5-4e7b-8722-cb6f0f4fada3",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "do you think it's safe to say `minUserMetadata >= 2` here so we don't have to add a new clause with every version? Or do you think the logic that follows may not apply to later metadata versions?",
        "createdAt" : "2018-04-17T15:55:37Z",
        "updatedAt" : "2018-04-17T20:51:18Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "dbecde0c-d4bf-422d-b1fb-8c648f6a0461",
        "parentId" : "6e1fd052-75c5-4e7b-8722-cb6f0f4fada3",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Maybe --- for know I think it's fine, but we should consider to do a \"closed range\" if we add version 4. I don't like open-ended. Not sure what others think.",
        "createdAt" : "2018-04-17T16:57:42Z",
        "updatedAt" : "2018-04-17T20:51:18Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "38272fe1-0806-4a8d-a4cf-43aff62d19a1",
        "parentId" : "6e1fd052-75c5-4e7b-8722-cb6f0f4fada3",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I agree, closed range seems less risky.",
        "createdAt" : "2018-04-17T19:31:41Z",
        "updatedAt" : "2018-04-17T20:51:18Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e762d6e1f24dc67a738950aa75264801b98a27f",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +527,531 @@        // construct the global partition assignment per host map\n        final Map<HostInfo, Set<TopicPartition>> partitionsByHostState = new HashMap<>();\n        if (minUserMetadataVersion == 2 || minUserMetadataVersion == 3) {\n            for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {\n                final HostInfo hostInfo = entry.getValue().hostInfo;"
  },
  {
    "id" : "f51c7b04-2ffa-4a64-b218-8882d9033522",
    "prId" : 7249,
    "prUrl" : "https://github.com/apache/kafka/pull/7249#pullrequestreview-279262691",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03d03b52-f607-4305-a18e-cc17ae3ed435",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "avoiding unnecessary negation.",
        "createdAt" : "2019-08-24T00:05:01Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cc2c66abcb7619589ed3366cb162c78dd7bbef8",
    "line" : 361,
    "diffHunk" : "@@ -1,1 +304,308 @@\n        final boolean versionProbing;\n        if (futureMetadataVersion == UNKNOWN) {\n            versionProbing = false;\n        } else {"
  },
  {
    "id" : "786a077d-5eb5-47bc-a1f9-be07edb306c7",
    "prId" : 7249,
    "prUrl" : "https://github.com/apache/kafka/pull/7249#pullrequestreview-283315679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd83f0ca-0811-4030-ab20-b2b40e9b0889",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "avoiding an unboxing warning.",
        "createdAt" : "2019-08-24T00:05:36Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "d4ecb562-273f-4c02-b3bd-e50b0990204d",
        "parentId" : "bd83f0ca-0811-4030-ab20-b2b40e9b0889",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The topic may not be in the metadata due to some edge conditions, we cannot throw the exception in this case.",
        "createdAt" : "2019-09-03T20:54:59Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "22f531be-69b4-4d11-9ca7-1d8469ce1375",
        "parentId" : "bd83f0ca-0811-4030-ab20-b2b40e9b0889",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This exception would always have happened anyway (it would have thrown an NPE on unboxing). What alternative handling would you recommend?",
        "createdAt" : "2019-09-03T21:01:22Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "07e47d10-1aa2-4671-9628-f720a63960cb",
        "parentId" : "bd83f0ca-0811-4030-ab20-b2b40e9b0889",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hmm, in that case we can do this as-is then: I need to think through what's possible under the context of KIP-429 anyways so we can have a thorough discussion in a later PR.",
        "createdAt" : "2019-09-03T23:46:09Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cc2c66abcb7619589ed3366cb162c78dd7bbef8",
    "line" : 442,
    "diffHunk" : "@@ -1,1 +380,384 @@                                    } else {\n                                        final Integer count = metadata.partitionCountForTopic(sourceTopicName);\n                                        if (count == null) {\n                                            throw new IllegalStateException(\n                                                \"No partition count found for source topic \""
  },
  {
    "id" : "6e388b46-5f37-405b-a6a9-79b785f26725",
    "prId" : 7249,
    "prUrl" : "https://github.com/apache/kafka/pull/7249#pullrequestreview-284458826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "590b2a8e-ce60-4ea1-9b16-b4b6a8f2ac1b",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Migrated the non-trivial config parsing logic to a new class. The new class does all its work in the constructor, so we'll still throw any exceptions at the same point.",
        "createdAt" : "2019-09-05T18:38:16Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cc2c66abcb7619589ed3366cb162c78dd7bbef8",
    "line" : 278,
    "diffHunk" : "@@ -1,1 +183,187 @@    @Override\n    public void configure(final Map<String, ?> configs) {\n        final AssignorConfiguration assignorConfiguration = new AssignorConfiguration(configs);\n\n        logPrefix = assignorConfiguration.logPrefix();"
  },
  {
    "id" : "78ecee7a-b3ec-4fe2-9cd0-44f8286fb64c",
    "prId" : 7249,
    "prUrl" : "https://github.com/apache/kafka/pull/7249#pullrequestreview-284458826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4d6db796-ad46-4f8b-8812-f8a9e588b013",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "just shortening some long lines.",
        "createdAt" : "2019-09-05T18:41:32Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cc2c66abcb7619589ed3366cb162c78dd7bbef8",
    "line" : 371,
    "diffHunk" : "@@ -1,1 +311,315 @@                             + \" Sending empty assignment back (with supported version {}).\",\n                         futureMetadataVersion,\n                         LATEST_SUPPORTED_VERSION);\n                versionProbing = true;\n            } else {"
  },
  {
    "id" : "de1ca337-199b-4a89-8d4d-b7675dbe4389",
    "prId" : 7249,
    "prUrl" : "https://github.com/apache/kafka/pull/7249#pullrequestreview-284458826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9798a8f-b47e-4e95-91ad-897781866c25",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "switched to object reference so that we can use \"null\" as \"unset\" and avoid an ambiguous usage of an \"unknown == -1\" constant.",
        "createdAt" : "2019-09-05T18:43:12Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cc2c66abcb7619589ed3366cb162c78dd7bbef8",
    "line" : 419,
    "diffHunk" : "@@ -1,1 +361,365 @@                for (final String topicName : topicsInfo.repartitionSourceTopics.keySet()) {\n                    final Optional<Integer> maybeNumPartitions = repartitionTopicMetadata.get(topicName).numberOfPartitions();\n                    Integer numPartitions = null;\n\n                    if (!maybeNumPartitions.isPresent()) {"
  },
  {
    "id" : "d00270d2-4483-460a-b15b-27c60d270562",
    "prId" : 7249,
    "prUrl" : "https://github.com/apache/kafka/pull/7249#pullrequestreview-284458826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50fc5114-02c2-4813-b351-2f7b5f97507c",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "defaulting to `-1` if unknown here because it works with the loop below, and the scope is extremely local (i.e., you don't need any larger context than the next 10 lines of code to understand what's happening).",
        "createdAt" : "2019-09-05T18:58:11Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cc2c66abcb7619589ed3366cb162c78dd7bbef8",
    "line" : 471,
    "diffHunk" : "@@ -1,1 +423,427 @@        for (final Map.Entry<String, InternalTopicConfig> entry : repartitionTopicMetadata.entrySet()) {\n            final String topic = entry.getKey();\n            final int numPartitions = entry.getValue().numberOfPartitions().orElse(-1);\n\n            for (int partition = 0; partition < numPartitions; partition++) {"
  },
  {
    "id" : "96857218-38b5-401d-8c0c-50a3377b96f8",
    "prId" : 7249,
    "prUrl" : "https://github.com/apache/kafka/pull/7249#pullrequestreview-284458826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9d3977a-1f80-41c4-a4c0-0a6f047f0749",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I'd ultimately like to move more of the v1-4 assignment logic out of this class to make it more natural to add in the KIP-441 logic, but I think this PR is big enough as-is.",
        "createdAt" : "2019-09-05T19:01:23Z",
        "updatedAt" : "2019-09-06T21:04:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cc2c66abcb7619589ed3366cb162c78dd7bbef8",
    "line" : 558,
    "diffHunk" : "@@ -1,1 +572,576 @@    }\n\n    private static Map<String, Assignment> computeNewAssignment(final Map<UUID, ClientMetadata> clientsMetadata,\n                                                                final Map<TaskId, Set<TopicPartition>> partitionsForTask,\n                                                                final Map<HostInfo, Set<TopicPartition>> partitionsByHostState,"
  },
  {
    "id" : "227b495e-83b8-48b9-bf93-ff43cc8bd562",
    "prId" : 7321,
    "prUrl" : "https://github.com/apache/kafka/pull/7321#pullrequestreview-292074718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe35b4c6-bcb2-49b1-a3ad-aef99e0969b7",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Please add a unit test for this. Could not find one. ",
        "createdAt" : "2019-09-12T10:27:04Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "9024d6bf-47fd-4837-bd22-78e5d42bb3e2",
        "parentId" : "fe35b4c6-bcb2-49b1-a3ad-aef99e0969b7",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "The mechanism for choosing the right rebalance protocol is turned off in this PR, but I will add a test for it when it is activated in the second PR",
        "createdAt" : "2019-09-23T21:51:33Z",
        "updatedAt" : "2019-09-24T05:45:47Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3935aa6be7cc55911bb4e24332557df24d01f18",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +207,211 @@\n    @Override\n    public List<RebalanceProtocol> supportedProtocols() {\n        final List<RebalanceProtocol> supportedProtocols = new ArrayList<>();\n        supportedProtocols.add(RebalanceProtocol.EAGER);"
  },
  {
    "id" : "3f549e77-047f-4b06-9029-8dcf90efee81",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-298252020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a707749b-4388-4430-86ea-ef809f687409",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just to clarify:\r\n\r\n1. If the member is on old version (then of course it's in EAGER), the `subscription.ownedPartitions` would always be empty, so this condition is always skipped.\r\n\r\n2. If the member is on newer version but still on EAGER, then `ownedPartitions` are still empty, same as 1) above.\r\n\r\n3. If the member is on newer version but on COOPERATIVE, then `ownedPartitions` are not empty, and we are effectively adding them back to the `prev-owned tasks`.\r\n\r\nSo it seems to me that the only reason we are doing all this is to avoid double encoding it in both user-data and in the consumer protocol itself for owned partitions, is that right? If yes I'd suggest the above javadoc be refactored a bit, since now it reads like in either EAGER or COOPERATIVE this logic could be triggered, but I think only in case 3) is this logic necessary.",
        "createdAt" : "2019-10-01T16:35:16Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "a81a0c71-c818-4e1d-aa31-bd5408a983c4",
        "parentId" : "a707749b-4388-4430-86ea-ef809f687409",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Yes, `ownedPartitions` should only be non-empty when following cooperative protocol. Note though that in the Streams upgrade path, some clients may be on eager while some are on cooperative, so we must check all `ownedPartitions` even if the leader is on eager.\r\nThe reason for doing this is because the ownedPartitions should be the ultimate \"source of truth\" for a consumer, for a few reasons: the task <-> partitions mapping could change, or a StreamThread could have failed to initialize a task and thus not included it in the encoded `activeTasks`, but the consumer still owns the task and we can't give it away until it's been revoked",
        "createdAt" : "2019-10-03T00:08:43Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "445fc012-af5e-41fd-a368-5db8eb98e639",
        "parentId" : "a707749b-4388-4430-86ea-ef809f687409",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "In that case, should we check in `state.addPreviousActiveTasks` that the task ids were not added in other client metadata? I might be paranoid here but what if client A claims (cooperative) its ownership of partition1 which maps to task1, while client B (eager) encodes task1 as its prev owned tasks with empty owned partitions?",
        "createdAt" : "2019-10-06T22:57:06Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "aab4a248-b6ad-4e87-baab-f276c62f0734",
        "parentId" : "a707749b-4388-4430-86ea-ef809f687409",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "During assignment we go by the `ownedPartitions` as the ultimate source of truth, w.r.t which consumer to try and give tasks back to and which tasks should be removed from the assignment because they were previously owned. If a consumer on eager claims task1 while a cooperative consumer claims partition1, we will ultimately try and give task1/partition1 back to the cooperative consumer. And if it has to be assigned to another consumer, even the eager consumer that claimed task1, we will still remove it from the assignment until the cooperative consumer can revoked it.\r\n\r\nWe also check that the task <-> partitions mapping does not change such that (for example) task1 maps to partition1 and partition2 which are claimed by different consumers.",
        "createdAt" : "2019-10-06T23:31:17Z",
        "updatedAt" : "2019-10-06T23:54:03Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9df1e53d-5577-4aca-b700-4ab335b1d2ea",
        "parentId" : "a707749b-4388-4430-86ea-ef809f687409",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Does that make sense? In the end we always go with the \"safest\" assignment and don't assign partitions claimed in someone's ownedPartitions to anyone else. If the scenario you described occurs. Which I believe is possible, if for example the eager consumer misses a rebalance it won't have cleared it's prevActiveTasks. However we will still try and give it back to the cooperative consumer, and if not to them then to no one (during this rebalance).",
        "createdAt" : "2019-10-06T23:34:44Z",
        "updatedAt" : "2019-10-06T23:54:03Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "fa99435a-a2a1-4b56-a99d-9252edb2c9c3",
        "parentId" : "a707749b-4388-4430-86ea-ef809f687409",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "SG, thanks for the explanation!",
        "createdAt" : "2019-10-07T16:22:51Z",
        "updatedAt" : "2019-10-07T16:22:52Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 375,
    "diffHunk" : "@@ -1,1 +591,595 @@            // Either the active tasks (eager) OR the owned partitions (cooperative) were encoded in the subscription\n            // according to the rebalancing protocol, so convert any partitions in a client to tasks where necessary\n            if (!state.ownedPartitions().isEmpty()) {\n                final Set<TaskId> previousActiveTasks = new HashSet<>();\n                for (final Map.Entry<TopicPartition, String> partitionEntry : state.ownedPartitions().entrySet()) {"
  },
  {
    "id" : "90386060-62b7-4ee5-b3e2-96ccb63d71e6",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-295923615",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5160be4-8012-4df5-8d3c-5e910cf90d6c",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "During version probing rebalances we used to just give all clients back their old assignments, except for \"future\" consumers who would get an empty assignment since we can't interpret their subscriptions. But we can now determine their previous active tasks by the `ownedPartitions` so we can now put them in their assignment. \r\n\r\nBut actually we might as well just do our best to generate a \"real\" (not version probing) assignment during this rebalance, so that on the second rebalance we will not have to revoke any partitions and trigger a third rebalance. So I consolidated the assignment generation into a separate method that both `computeNewAssignment` and `versionProbingAssignment` can use",
        "createdAt" : "2019-10-01T22:11:12Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "9c6e745b-3058-43be-8889-ee3dbf0d8b17",
        "parentId" : "c5160be4-8012-4df5-8d3c-5e910cf90d6c",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "There might be kind of an ugly merge/rebasing ahead once this [version probing bugfix PR](https://github.com/apache/kafka/pull/7423) is fixed, but I think the overall behavior will remain the same",
        "createdAt" : "2019-10-01T22:12:17Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 574,
    "diffHunk" : "@@ -1,1 +744,748 @@    }\n\n    private void addClientAssignments(final Map<String, Assignment> assignment,\n                                      final ClientMetadata clientMetadata,\n                                      final Map<TaskId, Set<TopicPartition>> partitionsForTask,"
  },
  {
    "id" : "ab58816e-333f-4432-84c8-c4bfe621080f",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-296592325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "baedf267-2f6d-48a3-a88f-ddb46fa416d6",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: within a task -> you meant `client` I think?",
        "createdAt" : "2019-10-02T00:38:23Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "50a4f0fb-c6f9-44ed-8b70-16333ac82312",
        "parentId" : "baedf267-2f6d-48a3-a88f-ddb46fa416d6",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Yep. Good catch",
        "createdAt" : "2019-10-03T00:24:48Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 459,
    "diffHunk" : "@@ -1,1 +679,683 @@            Map<String, List<TaskId>> activeTaskAssignments;\n\n            // Try to avoid triggering another rebalance by giving active tasks back to their previous owners within a\n            // client, without violating load balance. If we already know another rebalance will be required, or the\n            // client had no owned partitions, try to balance the workload as evenly as possible by interleaving the"
  },
  {
    "id" : "391406f8-3bbb-4a46-9680-c3bef8d8c77c",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-296595357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0eda8cc1-5b0d-4b59-a527-8c8db61e5791",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is not introduced by this PR, but: is line 934 necessary? We checked `sortedTasks.isEmpty()` at the head of the loop and there's no other operations on this list anywhere else.",
        "createdAt" : "2019-10-02T16:00:49Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "dc952036-db9c-4abf-857b-95cae9411824",
        "parentId" : "0eda8cc1-5b0d-4b59-a527-8c8db61e5791",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "See my response to the comment on the similar code in `giveTasksBackToConsumers` above -- basically it's because of the nested loop",
        "createdAt" : "2019-10-03T00:39:23Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 851,
    "diffHunk" : "@@ -1,1 +997,1001 @@        while (!sortedTasks.isEmpty()) {\n            for (final Map.Entry<String, List<TaskId>> consumerTaskIds : taskIdsForConsumerAssignment.entrySet()) {\n                final List<TaskId> taskIdList = consumerTaskIds.getValue();\n                final TaskId taskId = sortedTasks.poll();\n"
  },
  {
    "id" : "46c0af89-7723-46ab-a155-18d0d29f0ebf",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-296589167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30f73ebc-68fa-4113-a09e-6a390aefa04c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is a meta comment (and we can address it later): right now the logic of 1) assigning tasks at the client-level, and 2) within a client, assign tasks to threads, are both quite complicated and may worth be extracted into its own classes -- for 1) we have `TaskAssignor` already, for 2) we can move `computeNewAssignment` / `versionProbingAssignment` / `giveBack` / `interleave` / `previousConsumerOfTaskPartitions` / `addClientAssignment` together.\r\n\r\nAlso cc @vvcephei who have a PR for refactoring / extracting StreamsPartitionAssignor as well.",
        "createdAt" : "2019-10-02T16:18:51Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "36a0c018-bbcb-4ae5-b014-b9997a5dcadb",
        "parentId" : "30f73ebc-68fa-4113-a09e-6a390aefa04c",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Agreed, I decided to hold off of any large scale refactoring in this PR so it wouldn't be a pain to review but I think a cleanup is in order soon",
        "createdAt" : "2019-10-03T00:10:28Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 424,
    "diffHunk" : "@@ -1,1 +663,667 @@    }\n\n    private Map<String, Assignment> computeNewAssignment(final Map<UUID, ClientMetadata> clientsMetadata,\n                                                         final Map<TaskId, Set<TopicPartition>> partitionsForTask,\n                                                         final Map<HostInfo, Set<TopicPartition>> partitionsByHostState,"
  },
  {
    "id" : "70fa2595-4712-4d28-90ed-61235bbbd80a",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-296594990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e72210ae-ddcd-45a0-97e6-a3fea2efbb50",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why this can happen? We checked `!newTasks.isEmpty()` before already right?",
        "createdAt" : "2019-10-02T18:27:32Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "73b5f24a-3e01-4cf0-aa72-74d504e37c9a",
        "parentId" : "e72210ae-ddcd-45a0-97e6-a3fea2efbb50",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "We have a loop inside the main `while (!newTasks.isEmpty())` loop, where we are actively removing things from `newTasks`. So, we might have polled the last task in the `while (consumerIt.hasNext())` loop before we get back to the outer loop and check `newTasks.isEmpty`\r\nThis code is actually pretty much the same code as in the `interleaveTasksByGroupId` method. I'll see if it can be moved out into a shared method.\r\n",
        "createdAt" : "2019-10-03T00:37:31Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 778,
    "diffHunk" : "@@ -1,1 +928,932 @@                final List<TaskId> consumerAssignment = assignments.get(consumer);\n                final TaskId task = newTasks.poll();\n                if (task == null) {\n                    break;\n                }"
  },
  {
    "id" : "9d1648b1-24d5-4c88-8a73-d12dce8be12e",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-296601374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4098ecff-17bd-4113-bd4c-a625101458dc",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "This looks like an equivalent logic as \r\n```\r\n if (futureMetadataVersion == UNKNOWN) {\r\n...\r\n} else if (minReceivedMetadataVersion >= EARLIEST_PROBEABLE_VERSION) {\r\n...\r\n} else\r\n```\r\nCould we refactor?",
        "createdAt" : "2019-10-03T01:10:56Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 245,
    "diffHunk" : "@@ -1,1 +375,379 @@                minSupportedMetadataVersion);\n\n        } else {\n            throw new IllegalStateException(\n                \"Received a future (version probing) subscription (version: \" + futureMetadataVersion"
  },
  {
    "id" : "0a1e3224-561a-4e89-b406-c326ff977605",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-297152172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba910cd4-cc6b-4639-80b2-da8461a1bd6c",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "For my own education purpose, why we need to use TreeMap as underlying return struct?",
        "createdAt" : "2019-10-03T17:12:07Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "4630c888-d3d2-4c56-9415-3b0686e0ad50",
        "parentId" : "ba910cd4-cc6b-4639-80b2-da8461a1bd6c",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "The motivation is to avoid the random order of consumers in HashMap, so that we hopefully end up with a similar task -> consumer assignment in subsequent rebalances.",
        "createdAt" : "2019-10-03T21:07:20Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 839,
    "diffHunk" : "@@ -1,1 +986,990 @@        // Initialize the assignment map and task list for each consumer. We use a TreeMap here for a consistent\n        // ordering of the consumers in the hope they will end up with the same set of tasks in subsequent assignments\n        final Map<String, List<TaskId>> taskIdsForConsumerAssignment = new TreeMap<>();\n        for (final String consumer : consumers) {\n            taskIdsForConsumerAssignment.put(consumer, new ArrayList<>());"
  },
  {
    "id" : "7b8a91bd-86b0-4749-befa-582a06314d21",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-297154676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "417f0dfe-32fd-416e-a98d-29265f8996b9",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "For my own education purpose, what's the benefit we get from initializing a linked list vs a general array list?",
        "createdAt" : "2019-10-03T17:12:37Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "06723de5-6467-4160-aed5-2d83f8e85446",
        "parentId" : "417f0dfe-32fd-416e-a98d-29265f8996b9",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I didn't choose this this so I can only guess, but probably so we can easily/efficiently poll and remove the last element until empty..? ",
        "createdAt" : "2019-10-03T21:12:18Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 831,
    "diffHunk" : "@@ -1,1 +981,985 @@                                                                      final Set<String> consumers) {\n        // First we make a sorted list of the tasks, grouping them by groupId\n        final LinkedList<TaskId> sortedTasks = new LinkedList<>(taskIds);\n        Collections.sort(sortedTasks);\n"
  },
  {
    "id" : "fef7e1f0-b80d-493e-8081-a23d7416747e",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-297686588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0e2ba4c-cdee-4f8e-a722-e76d63bf242a",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "How about state.activeTaskCount() / consumers.size() + 1?",
        "createdAt" : "2019-10-03T17:41:12Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "16183f61-c928-44e5-806c-065721e6ba4c",
        "parentId" : "d0e2ba4c-cdee-4f8e-a722-e76d63bf242a",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Doesn't work when numConsumers evenly divides numTasks",
        "createdAt" : "2019-10-03T21:20:43Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "5bbc44c1-64fa-4063-adff-c83adbebc71f",
        "parentId" : "d0e2ba4c-cdee-4f8e-a722-e76d63bf242a",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sounds good",
        "createdAt" : "2019-10-04T19:54:22Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 714,
    "diffHunk" : "@@ -1,1 +864,868 @@        final Set<String> unfilledConsumers = new HashSet<>(consumers);\n\n        final int maxTasksPerClient = (int) Math.ceil(((double) state.activeTaskCount()) / consumers.size());\n\n        // initialize task list for consumers"
  },
  {
    "id" : "6ef323b7-2026-483b-a06b-7f6c26988ff5",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-297162417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26badbe1-cfe3-4114-baef-4bbc9d424a2a",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I don't quite follow the logic here. If this partition is owned by the current owner, then it should appear in `allOwnedPartitions` which means we expect a count of 2 for the final result of previousConsumers.\r\nWhile outside we check for this set to be > 1 and early terminate, is that true?",
        "createdAt" : "2019-10-03T17:47:44Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "da8c0347-7879-4e7b-8a10-d7fcdd12934a",
        "parentId" : "26badbe1-cfe3-4114-baef-4bbc9d424a2a",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "If this partition is owned by a consumer in this client, then `currentPartitionConsumer != null` and `currentPartitionConsumer` gets added to the `previousConsumers` set. We don't even check `allOwnedPartitions` in that case since it's in an \"else if\"",
        "createdAt" : "2019-10-03T21:28:03Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 812,
    "diffHunk" : "@@ -1,1 +962,966 @@            if (currentPartitionConsumer != null) {\n                previousConsumers.add(currentPartitionConsumer);\n            } else if (allOwnedPartitions.contains(tp)) {\n                previousConsumers.add(foreignConsumer);\n            }"
  },
  {
    "id" : "9d8d19e1-320b-4253-8cc8-7c7ee4e71dec",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-297716439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "204f84c5-afac-466a-af58-00c1d96a75d2",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I think the logic here is not very intuitive as we are doing a double loop here. What we could do is to loop through all the unfilled consumers and fetch unfulfilled amount of new tasks to fed them. This could also reduce the number of loop cycles we have to go through.",
        "createdAt" : "2019-10-03T18:01:18Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "89d9ddaf-e75c-4df1-9d9d-d07d6a77cce7",
        "parentId" : "204f84c5-afac-466a-af58-00c1d96a75d2",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Well, the goal here is to interleave the tasks of the same groupId (ie subtopology) across different consumers as much as possible, since some subtopologies might be quite heavy while others are quite light. (This is what we do in `interleaveTasksByGroupId` also)\r\nIt is a little unintuitive I agree, so let me know if you have any suggestions for clearer code and/or comments. But if it's an consolation,\r\na) we probably aren't looping through that many remaining consumers and/or tasks\r\nb) we may be doing a nested loop but within the inner loop we are removing things from the outer loop. So, in the end we are actually only looping over `newTasks` and hitting each task in it once",
        "createdAt" : "2019-10-03T21:32:50Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "8203670f-5c4c-417b-8db8-1621f8a22218",
        "parentId" : "204f84c5-afac-466a-af58-00c1d96a75d2",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I see your point, could you reflect your goal as meta comments for `interleaveTasksByGroupId`? It's a bit hard to interpret the goal just by reading this code.",
        "createdAt" : "2019-10-04T20:15:10Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "550bc2c0-f70d-461c-b97c-2957fa3bdc31",
        "parentId" : "204f84c5-afac-466a-af58-00c1d96a75d2",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Sure, I'll try to shore up the explanation for `interleaveTasksByGroupId` and then refer to that here ",
        "createdAt" : "2019-10-04T21:02:04Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 774,
    "diffHunk" : "@@ -1,1 +924,928 @@\n            // Loop through the unfilled consumers and distribute tasks until newTasks is empty\n            while (consumerIt.hasNext()) {\n                final String consumer = consumerIt.next();\n                final List<TaskId> consumerAssignment = assignments.get(consumer);"
  },
  {
    "id" : "abc49f0f-2717-42ec-9212-4654cc6d61d0",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-297866906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b840280-41fa-43f8-ad44-8286c7974ef8",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Is it possible that a task maps to multiple partitions, while only some of them have old owners, would that cause us to encode a partial task here?",
        "createdAt" : "2019-10-06T23:01:56Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "70bbc4ae-f8f1-4494-b79a-0fa2ce4eaeee",
        "parentId" : "5b840280-41fa-43f8-ad44-8286c7974ef8",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Ah, I see what you mean. We should collect the assignedPartitions for a single task and only add them to the `assignedPartitions` list at the end, if all can be safely assigned",
        "createdAt" : "2019-10-06T23:41:19Z",
        "updatedAt" : "2019-10-06T23:54:03Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 660,
    "diffHunk" : "@@ -1,1 +809,813 @@                // If the partition is new to this consumer but is still owned by another, remove from the assignment\n                // until it has been revoked and can safely be reassigned according the COOPERATIVE protocol\n                if (newPartitionForConsumer && allOwnedPartitions.contains(partition)) {\n                    log.debug(\"Removing task {} from assignment until it is safely revoked\", taskId);\n                    clientState.removeFromAssignment(taskId);"
  },
  {
    "id" : "ceec1c12-0619-426d-8e92-8f81305db436",
    "prId" : 7386,
    "prUrl" : "https://github.com/apache/kafka/pull/7386#pullrequestreview-297864337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e4b7562-d1bc-4ff1-9c4f-e94b4dfd532a",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: add a debug entry as well?",
        "createdAt" : "2019-10-06T23:02:43Z",
        "updatedAt" : "2019-10-06T23:53:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f047cd9bea88a4ffc88ce2b4f82946d52202c539",
    "line" : 751,
    "diffHunk" : "@@ -1,1 +901,905 @@                        \"previous tasks than it has capacity for during this assignment, falling back to interleaved \" +\n                        \"assignment since a realance is inevitable.\");\n                    return Collections.emptyMap();\n                }\n"
  },
  {
    "id" : "995d5564-b9ae-4c0e-be5d-660c16aeb08c",
    "prId" : 7419,
    "prUrl" : "https://github.com/apache/kafka/pull/7419#pullrequestreview-295300832",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5307005b-79f5-4605-b6fd-c863910acaea",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Here's the change if the source topic is a repartition topic, drop into a new block to check if the repartition topic has a partition count available.  If not we don't throw an `Exception`, as we will only throw when a non-internal topic reports no partition count available.",
        "createdAt" : "2019-09-30T16:23:48Z",
        "updatedAt" : "2019-09-30T16:23:53Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "9e4269fe-4c9a-446c-92aa-4cb9b84e54ce",
        "parentId" : "5307005b-79f5-4605-b6fd-c863910acaea",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "@bbejeck @vvcephei I checked the code that right now we use linked-hashmap for the node-groups / topic-groups construction, whose order is preserved. I think that means that assuming the topology is a DAG with no cycles, one pass from sub-topology 1 is arguably sufficient. However, once case that we did not handle today which is also why we are still doing a while-loop here is, e.g. (numbers are sub-topology indices):\r\n\r\n```\r\n1 -> 2,\r\n1 -> 3,\r\n3 -> 2\r\n```\r\n\r\nAnd if we loop over the order of 1,2,3, then when we are processing 2 since 3's not set yet we do no have the num.partitions for the repartition topic between 3 -> 2.\r\n\r\nLooking at `InternalTopologyBuilder#makeNodeGroups`, I think it is possible that we ensure it ordered as \r\n\r\n```\r\n1 -> 3,\r\n1 -> 2,\r\n3 -> 3\r\n```\r\n\r\nso that we can make one pass without the while loop, and can also assume that the parent sub-topologies sink/repartition topic num.partitions are set when processing this, WDYT?",
        "createdAt" : "2019-09-30T19:15:18Z",
        "updatedAt" : "2019-09-30T19:17:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "972b14d0-9a6e-4668-a209-50933bf3089c",
        "parentId" : "5307005b-79f5-4605-b6fd-c863910acaea",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.",
        "createdAt" : "2019-09-30T19:16:57Z",
        "updatedAt" : "2019-09-30T19:17:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "7566c3d7-557c-4cd3-a183-7974a08408e8",
        "parentId" : "5307005b-79f5-4605-b6fd-c863910acaea",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks for the idea @guozhangwang . If I understand right, it sounds like you're suggesting to propagate the partition count from (external) sources all the way through the topology, in topological order. If the partition count is purely determined by the external source topics, then it should indeed work to do this in topological order in one pass.\r\n\r\nWhat I'm wondering now is whether there's any situation where some of the repartition topics might already exist with a specific number of partitions. An easy strawman is, \"what if the operator has pre-created some of the internal topics?\", which may or may not be allowed. Another is \"what if the topology has changed slightly to add a new repartition topic early in the topology?\" Maybe there are some other similar scenarios. I'm not sure if any of these are real possibilities, or if they'd affect the outcome, or if we want to disallow them anyway to make our lives easier.\r\n\r\nWDYT?",
        "createdAt" : "2019-09-30T19:40:35Z",
        "updatedAt" : "2019-09-30T19:40:35Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "1e98e156-251b-49b4-bb28-cfea1f3197c5",
        "parentId" : "5307005b-79f5-4605-b6fd-c863910acaea",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine.\r\n\r\nIf user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.",
        "createdAt" : "2019-09-30T23:18:47Z",
        "updatedAt" : "2019-09-30T23:18:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f77e2eb4d1a075cd9ac0890966765c65859a3447",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +389,393 @@                                    // map().join().join(map())\n                                    if (repartitionTopicMetadata.containsKey(sourceTopicName)) {\n                                        if (repartitionTopicMetadata.get(sourceTopicName).numberOfPartitions().isPresent()) {\n                                            numPartitionsCandidate = repartitionTopicMetadata.get(sourceTopicName).numberOfPartitions().get();\n                                        }"
  },
  {
    "id" : "cd4d21e8-62c5-4bd9-9b40-f7a192f581a4",
    "prId" : 7419,
    "prUrl" : "https://github.com/apache/kafka/pull/7419#pullrequestreview-295185237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae283881-74c7-44e9-bc49-7da9773abb23",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic:\r\n\r\n```\r\ntopic.setNumberOfPartitions(numPartitions.get());\r\n```\r\n\r\nis not necessary since the `numPartitions` is read from the topic.",
        "createdAt" : "2019-09-30T19:00:16Z",
        "updatedAt" : "2019-09-30T19:17:21Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f77e2eb4d1a075cd9ac0890966765c65859a3447",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +391,395 @@                                        if (repartitionTopicMetadata.get(sourceTopicName).numberOfPartitions().isPresent()) {\n                                            numPartitionsCandidate = repartitionTopicMetadata.get(sourceTopicName).numberOfPartitions().get();\n                                        }\n                                    } else {\n                                        final Integer count = metadata.partitionCountForTopic(sourceTopicName);"
  }
]