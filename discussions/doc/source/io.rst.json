[
  {
    "id" : "3acc8620-49bc-4227-a376-9200429a1185",
    "prId" : 4857,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23213b52-b674-482c-9e7a-b019cc198960",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "I think they are all optional right? and aren't you making xlsxwriter the default now?\n",
        "createdAt" : "2013-09-16T21:39:39Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "a2b7da3a-694a-4b74-a9b3-a567f85cffe4",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "body" : "I guess we need to edit the default settings to check if xlsxwriter or openpyxl is installed.  Not sure if we could neaten this up with some importlib magic or something...\n\ni.e.: \n\n``` python\nxlsx_set = False\ntry:\n    import xlsxwriter\n    if not xlsx_set:\n        set_option('io.excel.xlsx.writer', 'xlsxwriter')\n        xlsx_set = True\nexcept ImportError:\n    pass\ntry:\n    import openpyxl\n    if not xlsx_set:\n        set_option('io.excel.xlsx.writer', 'openpyxl')\n        xlsx_set = True\nexcept ImportError:\n    pass\n    # etc\n```\n\nAnd we can decide on order later.  I think only openpyxl supports `.xlsm` files. It also may be the case that xlwt supports xlsx files.  If so, it would be trivial to add it here.\n",
        "createdAt" : "2013-09-16T21:50:15Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "tags" : [
        ]
      },
      {
        "id" : "b5c507bf-bce0-4960-a30a-26f1fb22df7b",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "body" : "They are all optional but openpyxl is the default for xlsx and xlwt is the default for xls insofar as they are the default classes bound to the file extensions.\n\nAnd it wasn't my intention to make xlsxwriter the default. It is probably best to see if people use it or prefer it as a default for a release or two.\n",
        "createdAt" : "2013-09-16T21:59:33Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "tags" : [
        ]
      },
      {
        "id" : "f57e952e-f2a5-49ff-b56d-93c3ba7a003b",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "body" : "Just to reiterate, I don't think it is worth changing the current behaviour of the (optional) defaults. At least not in 0.13. If it proves to be popular and robust we can consider that for 0.14.\n",
        "createdAt" : "2013-09-17T10:43:41Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "tags" : [
        ]
      },
      {
        "id" : "c8f42109-a4cc-4203-85e5-2f42f2a6c5ce",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "body" : "@jmcnamara keep in mind that you have to explicitly choose to install xlsxwriter to have this work - so it's not that big of a deal. `xlsxwriter` isn't in the major prepackaged distributions (enthought, anaconda, python(x,y), winpython, etc), so there's a low probability for people to be surprised.\n",
        "createdAt" : "2013-09-17T11:26:12Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "tags" : [
        ]
      },
      {
        "id" : "7c1837dd-558b-4fc9-a028-2a2383fe47ce",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "body" : "I think this is fine for now.\n",
        "createdAt" : "2013-09-20T21:54:42Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0c290f4879a542b027b4c36a922f5ae2216ed5a",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +1694,1698 @@2. the filename extension (via the default specified in config options)\n\nBy default ``pandas`` only supports\n`openpyxl <http://packages.python.org/openpyxl/>`__ as a writer for ``.xlsx``\nand ``.xlsm`` files and `xlwt <http://www.python-excel.org/>`__ as a writer for"
  },
  {
    "id" : "c5abb56a-7e91-49b7-8b5f-45c2029eee16",
    "prId" : 6021,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "904e3d8d-d10b-4501-9696-725f1543c142",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "foo.csv has been removed before (under 'Specifying date columns'), so you will have to move that remove below this.\n",
        "createdAt" : "2014-01-24T07:40:47Z",
        "updatedAt" : "2014-01-24T07:40:47Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "879f270c120a2c0f63de449ab6fd1bcff2628175",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +531,535 @@\n   # Try to infer the format for the index column\n   df = pd.read_csv('foo.csv', index_col=0, parse_dates=True,\n                    infer_datetime_format=True)\n"
  },
  {
    "id" : "a072aef3-aef5-4ef4-8c65-d6bcd3206dda",
    "prId" : 6021,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06069c07-4d29-43a8-aa40-6888eb34905a",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "Can you make a list of this? (just `-` before each), or otherwise a code-block, as you like (but just one enter will be disregarded by Sphinx)\n",
        "createdAt" : "2014-01-24T07:41:44Z",
        "updatedAt" : "2014-01-24T07:41:44Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "879f270c120a2c0f63de449ab6fd1bcff2628175",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +522,526 @@\"12/30/2011 00:00:00\"\n\"30/Dec/2011 00:00:00\"\n\"30/December/2011 00:00:00\"\n\n`infer_datetime_format` is sensitive to `dayfirst`.  With `dayfirst=True`, it"
  },
  {
    "id" : "139d32a0-0aea-4402-a673-4ae69c5db915",
    "prId" : 6021,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04bd4206-0f64-4def-a3d3-034834f5a276",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "In the previous sections, parse_dates is always written with double backtick quotation (```parse_dates```). This will render as code, while single backtick as italic.\n",
        "createdAt" : "2014-01-24T07:43:51Z",
        "updatedAt" : "2014-01-24T07:43:51Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "879f270c120a2c0f63de449ab6fd1bcff2628175",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +504,508 @@Inferring Datetime Format\n~~~~~~~~~~~~~~~~~~~~~~~~~\nIf you have `parse_dates` enabled for some or all of your columns, and your\ndatetime strings are all formatted the same way, you may get a large speed\nup by setting `infer_datetime_format=True`.  If set, pandas will attempt"
  },
  {
    "id" : "0d81a24e-e850-466c-bdbd-5af1b13c45fd",
    "prId" : 6576,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "can you remove the leading spaces here\n",
        "createdAt" : "2014-03-11T11:46:05Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "64787771-b7e8-48fa-bf38-25aa1367ebf0",
        "parentId" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "authorId" : "05966bcf-6059-4f72-8d7c-d93433335047",
        "body" : "I changed this to bullet point. Looks like the other bullet point sections use leading spaces.\n",
        "createdAt" : "2014-03-11T14:22:08Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "05966bcf-6059-4f72-8d7c-d93433335047",
        "tags" : [
        ]
      },
      {
        "id" : "b456a8e4-9297-40f0-a629-c0b95285d0bd",
        "parentId" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "It's OK like this\n",
        "createdAt" : "2014-03-11T14:25:34Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      },
      {
        "id" : "a0e38688-1f19-48a1-b3e0-416241464d0a",
        "parentId" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "yep..that's fine...\n",
        "createdAt" : "2014-03-11T15:20:55Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "5be379f013f275b58c9ee6c04669ab7919ff728f",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1848,1852 @@\n- Pass a string to refer to the name of a particular sheet in the workbook.\n- Pass an integer to refer to the index of a sheet. Indices follow Python \n  convention, beginning at 0.\n- The default value is ``sheet_name=0``. This reads the first sheet."
  },
  {
    "id" : "79d1b9ee-72dc-47c7-a060-ad7c947159a1",
    "prId" : 6889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aef087a5-ffdc-4ed5-8deb-8151913b9b70",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "can you update the skip_footer/dtype in io.parsers.rst (at the top where the arguments are laid out) for the same way you did in the doc-strings?\n",
        "createdAt" : "2014-04-23T20:11:06Z",
        "updatedAt" : "2014-04-23T21:41:01Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "f45b7143991df96c85a1b9860a7674c3b3f2a91e",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +277,281 @@    df = pd.read_csv(StringIO(data), dtype={'b': object, 'c': np.float64})\n    df.dtypes\n\n.. note::\n    The ``dtype`` option is currently only supported by the C engine."
  },
  {
    "id" : "567c0d81-2862-4c7c-9ad4-3ab35fba2724",
    "prId" : 6937,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28ebc67c-9578-4db5-b118-97cc3e9d0743",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "pandas.io.read_gbq -> pandas.read_gbq\n",
        "createdAt" : "2014-06-10T07:22:25Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd5b6e97108b02f70e5c101eaf04401b8d5315b6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +3381,3385 @@\nAs an example, suppose you want to load all data from an existing BigQuery \ntable : `test_dataset.test_table` into a DataFrame using the :func:`~pandas.io.read_gbq` \nfunction.\n"
  },
  {
    "id" : "b84d7899-0c61-4a6a-b546-485ac6307775",
    "prId" : 6937,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45c24584-2a18-4404-8d58-cd60f617f078",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "and is it then appended? (not fully clear to me, previously you had fail/replace/append, now only one default action?)\n",
        "createdAt" : "2014-06-10T07:31:47Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      },
      {
        "id" : "4c97d70b-0dc9-4f78-a935-bc1247971aaf",
        "parentId" : "45c24584-2a18-4404-8d58-cd60f617f078",
        "authorId" : "661a0526-88d1-45b3-bba2-ca16313d3f2c",
        "body" : "The other actions were a benefit of relying on bq.py in the past. While possible to do strictly with the API, it's a lot of code for very little benefit. The data is strictly appended which was, at least in our experience, the most common use case.\n",
        "createdAt" : "2014-06-11T05:01:38Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "661a0526-88d1-45b3-bba2-ca16313d3f2c",
        "tags" : [
        ]
      },
      {
        "id" : "51451d17-14d4-4164-8449-457985963f25",
        "parentId" : "45c24584-2a18-4404-8d58-cd60f617f078",
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "Ah, I missed the rather obvious \"you can _append_ data using to_gbq()\" part.\n\nSo OK, no problem here. But maybe add it more explicitely in the docstring of `to_gbq` as well?\n",
        "createdAt" : "2014-06-11T20:22:39Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd5b6e97108b02f70e5c101eaf04401b8d5315b6",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +3411,3415 @@Google streaming API which requires that your destination table exists in\nBigQuery. Given the BigQuery table already exists, your DataFrame should\nmatch the destination table in column order, structure, and data types. \nDataFrame indexes are not supported. By default, rows are streamed to \nBigQuery in chunks of 10,000 rows, but you can pass other chuck values "
  },
  {
    "id" : "805e7153-c03e-4694-883e-11d0d2b8068d",
    "prId" : 9601,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e28786be-df6f-46ff-b66d-8b714c1def68",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "There needs to be a blank line between the `.. ipython:: python` (directive command) and the actual code content\n",
        "createdAt" : "2015-03-06T12:52:43Z",
        "updatedAt" : "2015-03-06T13:00:00Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "08297e62945626c25413257d5a38c366d047aea6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +3839,3843 @@.. ipython:: python\n\n  reader = pd.read_stata('stata.dta', chunksize=3)\n  for df in reader:\n      print(df.shape)"
  },
  {
    "id" : "1a82335d-5669-4dfc-abd4-e2c3cb6e5b5c",
    "prId" : 9711,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbf5afff-c023-4c0a-8833-299449d041d7",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "you need `.. ipython: :python` around each of these blocks to have it exectue, or probably better `.. code-block` to have it look like code but not execute\n",
        "createdAt" : "2015-06-15T11:16:12Z",
        "updatedAt" : "2015-08-14T08:03:58Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "4694a428fd63e971159fffea4559e589088881e9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +4147,4151 @@.. code-block:: python\n\n    df = pd.read_sas('sas_xport.xpt')\n\nObtain an iterator and read an XPORT file 100,000 lines at a time:"
  },
  {
    "id" : "3d731c11-490b-4f0e-aba0-8b7bd5b3a865",
    "prId" : 10069,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "This is only true for when using compression, IIRC.\n",
        "createdAt" : "2015-05-06T15:10:16Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "d44bee17-7691-4674-8c39-b67f4c94f584",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "body" : "the \"stores need to be rewritten\" part?  I rewrote the test table uncompressed in the old version and querying in the new version does not return all rows.  I think it needs to be done regardless.\n",
        "createdAt" : "2015-05-06T15:21:30Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "tags" : [
        ]
      },
      {
        "id" : "063c8047-06e1-449b-911b-3944a6376bf4",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "oh, ok. I just wanted to make it as specific as possible. This sounds like its ALWAYS a bug. IIRC I think you actually have to specify `start` or `stop` when you query. (e.g. your query needs to be something like: `s.select('df',where='.....',start=10)`, right? (iow it has to be a chunked query)\n",
        "createdAt" : "2015-05-06T15:28:20Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "e3bfa90a-9100-4944-8e73-a0853686fa96",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "body" : "It doesn't need to be chunked.  My sample was just s.select('df', where=Term(...)).  I tried to make it non-specific by \"may appear.\"\n",
        "createdAt" : "2015-05-06T15:31:18Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "tags" : [
        ]
      },
      {
        "id" : "8380f097-9a0c-4c23-b524-b697f27b9cf3",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "hmm, ok, that's fine then.\n",
        "createdAt" : "2015-05-06T15:38:44Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "d2dd5add-11e0-412a-8a18-f3ee121fe601",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "body" : "Since this is such a nuanced bug, I think it would be worth making 3.2 a requirement once that is officially out.\n",
        "createdAt" : "2015-05-06T15:46:50Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "tags" : [
        ]
      },
      {
        "id" : "da452891-dda3-44a4-8254-7f6068331dd3",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "sure, let's make an issue for 0.17.0 for that, maybe now just update install.rst to say highly recommened to use 3.2\n",
        "createdAt" : "2015-05-06T15:53:15Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "61fae01fb757a30d494a523b3dc71eb7ffe1487f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2368,2372 @@   \n   There is a ``PyTables`` indexing bug which may appear when querying stores using an index.  If you see a subset of results being returned, upgrade to ``PyTables`` >= 3.2.  Stores created previously will need to be rewritten using the updated version.\n\n.. ipython:: python\n   :suppress:"
  },
  {
    "id" : "dc2a3fe2-894c-4662-a98e-1a5f6ac4db67",
    "prId" : 10097,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5e03b89-e8eb-40e9-99ca-f4ac8960d0fc",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "say you can pass the `dropna=True` to get the previous behavior\n\nI see you did below, so nvm.\n",
        "createdAt" : "2015-07-30T18:08:17Z",
        "updatedAt" : "2015-07-31T18:00:57Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "2377b5c1aafa33fb5a3fb3966e0bb16b8bcd2c6a",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2414,2418 @@\n   As of version 0.17.0, ``HDFStore`` will not drop rows that have all missing values by default. Previously, if all values (except the index) were missing, ``HDFStore`` would not write those rows to disk. \n\n.. ipython:: python\n   :suppress:"
  },
  {
    "id" : "9293964a-01bc-4754-aae9-6560f2222be1",
    "prId" : 10376,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d57dcb10-314f-472d-8197-94134e086e27",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "can you add a reference here (so we can link to it)\n",
        "createdAt" : "2015-06-20T07:47:42Z",
        "updatedAt" : "2015-06-20T16:11:04Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "92203096ceba0cfea72bb28d434f34f58e68bb0a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2184,2188 @@\n   df.to_excel('path_to_file.xlsx', sheet_name='Sheet1')\n\nWriting Excel Files to Memory\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
  },
  {
    "id" : "0df60c2d-7df0-44e6-83dc-39fd1d8ad335",
    "prId" : 10857,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da8514ce-6ccd-47d2-b834-e15cbec48b6b",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "or you have to make this a static `code-block` like the others, or you have to remove the output (but I would go for the code-block, since all the others are that as well)\n",
        "createdAt" : "2015-09-10T07:53:02Z",
        "updatedAt" : "2015-09-13T16:30:46Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      },
      {
        "id" : "da6e6249-1d3d-4765-9df8-6ba9d2ffd642",
        "parentId" : "da8514ce-6ccd-47d2-b834-e15cbec48b6b",
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "or maybe better is to use `.. ipython::` instead of `.. ipython:: python`, then the code is not executed, but the formatting is more similar (`.. code-block:: python` does look a bit different)\n",
        "createdAt" : "2015-09-10T07:54:54Z",
        "updatedAt" : "2015-09-13T16:30:46Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "2622cb313072f3a02c0c8752a0496a518eb057b1",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +4020,4024 @@Assume we want to write a DataFrame ``df`` into a BigQuery table using :func:`~pandas.DataFrame.to_gbq`.\n\n.. ipython:: python\n\n   df = pd.DataFrame({'my_string': list('abc'),"
  },
  {
    "id" : "59e7c4b2-1905-4dbb-b71d-c285dffaf0ec",
    "prId" : 10857,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46dc4d7f-b8ef-4ca2-a883-129328843a31",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "in case of using a code-block, a `In [3]` should be added here\n",
        "createdAt" : "2015-09-10T07:53:37Z",
        "updatedAt" : "2015-09-13T16:30:46Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "2622cb313072f3a02c0c8752a0496a518eb057b1",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +4022,4026 @@.. ipython:: python\n\n   df = pd.DataFrame({'my_string': list('abc'),\n                      'my_int64': list(range(1, 4)),\n                      'my_float64': np.arange(4.0, 7.0),"
  },
  {
    "id" : "1bb0da69-5a03-4dc7-8479-e7d6c559a959",
    "prId" : 10967,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "427a9695-7267-454f-819e-228594e3be28",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "ok for now, but maybe make this have sub-sections to make this a bit easier to navigate \n",
        "createdAt" : "2015-09-09T12:02:54Z",
        "updatedAt" : "2015-09-09T12:02:54Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "98405f043e619683288f3e28cb1d5140a4252d12",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1990,1994 @@'''''''''''''''''''\n\n.. versionadded:: 0.17\n\n``read_excel`` can read a ``MultiIndex`` index, by passing a list of columns to ``index_col``"
  },
  {
    "id" : "c9b971df-7bd1-4577-976c-1276d259582c",
    "prId" : 13575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1e99214-a74d-4d2b-9cb4-695918a7cc70",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "can you add a version added tag for these\n",
        "createdAt" : "2016-07-20T21:41:55Z",
        "updatedAt" : "2016-07-21T12:50:14Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cb8243f2dd31cc2155627f29cfc89bbf6d4b84b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1961,1965 @@\nSpecify values that should be converted to NaN\n\n.. code-block:: python\n"
  },
  {
    "id" : "8af719d3-717a-4d8c-b6a5-1302358d1b4f",
    "prId" : 14295,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/14295#pullrequestreview-8710731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a4b986-2c25-4b9e-bc46-adbed6933a55",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "I think you need a blank line here (to avoid warnings)\n",
        "createdAt" : "2016-11-15T22:23:07Z",
        "updatedAt" : "2016-11-25T20:36:17Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "3abb0bd6e46e78557c1fd480ac173881dc5d530b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +479,483 @@  .. versionadded:: 0.20.0 support for the Python parser.\n\n     The ``dtype`` option is supported by the 'python' engine\n\n.. note::"
  },
  {
    "id" : "5ccdd163-d86f-4891-9847-70d845195dc2",
    "prId" : 14904,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/14904#pullrequestreview-20359152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65b1d04d-d7eb-4ae5-b384-cb1e5eeea887",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "you *may* want a ref tag here ",
        "createdAt" : "2017-02-06T20:21:26Z",
        "updatedAt" : "2017-03-04T11:46:35Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fac34ce6c646407111c09a942cbd195b6cbf590",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2034,2038 @@  df.to_json(orient='records', lines=True)\n\n\n.. _io.table_schema:\n"
  },
  {
    "id" : "9c7a0ed3-23ec-4d40-b894-aeff1e2e724c",
    "prId" : 14904,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/14904#pullrequestreview-20359152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a595709-463b-48c1-b256-a47cb4179fad",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "``orient='table'``",
        "createdAt" : "2017-02-06T20:21:43Z",
        "updatedAt" : "2017-03-04T11:46:35Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fac34ce6c646407111c09a942cbd195b6cbf590",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2044,2048 @@`Table Schema`_ is a spec for describing tabular datasets as a JSON\nobject. The JSON includes information on the field names, types, and\nother attributes. You can use the orient ``table`` to build\na JSON string with two fields, ``schema`` and ``data``.\n"
  },
  {
    "id" : "6d718a60-1f8c-4242-8ff6-10719dff9c30",
    "prId" : 15637,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/15637#pullrequestreview-26154396",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a420427d-102b-4f34-a8ea-496f6cebae52",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "same line in v0.20.0.txt as well :>",
        "createdAt" : "2017-03-09T21:55:10Z",
        "updatedAt" : "2017-03-09T22:00:28Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ab711662d76d963014f3dcd9071e6891fcc7491",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +3097,3101 @@.. ipython:: python\n   :suppress:\n\n   import os\n   os.remove(\"data.pkl.compress\")"
  },
  {
    "id" : "866a3fb1-2913-4e42-b8e6-a20eb8807918",
    "prId" : 15838,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/15838#pullrequestreview-52611848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cb20fef-3e87-4852-b185-98ed681819bf",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "This raises an error, since categorical is not supported by pyarrow?",
        "createdAt" : "2017-07-27T07:38:45Z",
        "updatedAt" : "2017-08-01T22:28:31Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      },
      {
        "id" : "c5d1db2a-373f-43a9-a803-d7a0a7135a54",
        "parentId" : "0cb20fef-3e87-4852-b185-98ed681819bf",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "https://issues.apache.org/jira/browse/ARROW-1285\r\n(and going to remove the cat; we test this but docs didn't get updated)",
        "createdAt" : "2017-07-27T10:08:03Z",
        "updatedAt" : "2017-08-01T22:28:31Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "f553a5fbce80b98c4a55bd6469859e8f8922b162",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +4601,4605 @@.. ipython:: python\n\n   df.to_parquet('example_pa.parquet', engine='pyarrow')\n   df.to_parquet('example_fp.parquet', engine='fastparquet')\n"
  },
  {
    "id" : "66b8fbf4-7aa1-4134-b6ab-2dc1b34b1056",
    "prId" : 15838,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/15838#pullrequestreview-52576290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5902b042-57eb-497b-8f2b-3ccc8c888eea",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "Looking at the tests, there seem to be some differences at what data types they support? If that is correct, we should maybe mention it here ?",
        "createdAt" : "2017-07-27T07:39:50Z",
        "updatedAt" : "2017-08-01T22:28:31Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "f553a5fbce80b98c4a55bd6469859e8f8922b162",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +4580,4584 @@.. note::\n\n   These engines are very similar and should read/write nearly identical parquet format files.\n   These libraries differ by having different underlying dependencies (``fastparquet`` by using ``numba``, while ``pyarrow`` uses a c-library).\n"
  },
  {
    "id" : "24b07696-4f97-4042-ad85-1ef5e2d72995",
    "prId" : 15838,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/15838#pullrequestreview-52576290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92c43eb8-815a-4404-914b-313945a85f92",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "This also fails because fastparquet is not installed in the doc build. \r\nBut maybe rather than adding it to the doc build, maybe we can make this a code block? (just showing how to specify the engine) To not further burden the doc build with more dependencies (pyarrow is already included for feather)",
        "createdAt" : "2017-07-27T07:42:34Z",
        "updatedAt" : "2017-08-01T22:28:31Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "f553a5fbce80b98c4a55bd6469859e8f8922b162",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +4602,4606 @@\n   df.to_parquet('example_pa.parquet', engine='pyarrow')\n   df.to_parquet('example_fp.parquet', engine='fastparquet')\n\nRead from a parquet file."
  },
  {
    "id" : "c4993c3f-44f3-4b2a-aa09-1f8fa9e0c4ff",
    "prId" : 16355,
    "prUrl" : "https://github.com/pandas-dev/pandas/pull/16355#pullrequestreview-42269969",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82a11307-2436-478a-adfe-a295df96bd73",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "I think let's change the defaults to ``complevel=None``, ``complib=None``.\r\n\r\nIf then complevel is not None and > 0 you can set complib to zlib (if it not defined)\r\nif complib is not None and complevel is not 0 then you set the filter.",
        "createdAt" : "2017-06-06T10:38:39Z",
        "updatedAt" : "2017-06-13T11:33:21Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "af7ecbfa37525d646e04a2d7557feddd97c5a0b3",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +4070,4074 @@control compression: ``complevel`` and ``complib``.\n\n``complevel`` specifies if and how hard data is to be compressed.\n              ``complevel=0`` and ``complevel=None`` disables\n              compression and ``0<complevel<10`` enables compression."
  }
]