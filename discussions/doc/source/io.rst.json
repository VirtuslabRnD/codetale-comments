[
  {
    "id" : "3acc8620-49bc-4227-a376-9200429a1185",
    "prId" : 4857,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23213b52-b674-482c-9e7a-b019cc198960",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "I think they are all optional right? and aren't you making xlsxwriter the default now?\n",
        "createdAt" : "2013-09-16T21:39:39Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "a2b7da3a-694a-4b74-a9b3-a567f85cffe4",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "body" : "I guess we need to edit the default settings to check if xlsxwriter or openpyxl is installed.  Not sure if we could neaten this up with some importlib magic or something...\n\ni.e.: \n\n``` python\nxlsx_set = False\ntry:\n    import xlsxwriter\n    if not xlsx_set:\n        set_option('io.excel.xlsx.writer', 'xlsxwriter')\n        xlsx_set = True\nexcept ImportError:\n    pass\ntry:\n    import openpyxl\n    if not xlsx_set:\n        set_option('io.excel.xlsx.writer', 'openpyxl')\n        xlsx_set = True\nexcept ImportError:\n    pass\n    # etc\n```\n\nAnd we can decide on order later.  I think only openpyxl supports `.xlsm` files. It also may be the case that xlwt supports xlsx files.  If so, it would be trivial to add it here.\n",
        "createdAt" : "2013-09-16T21:50:15Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "tags" : [
        ]
      },
      {
        "id" : "b5c507bf-bce0-4960-a30a-26f1fb22df7b",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "body" : "They are all optional but openpyxl is the default for xlsx and xlwt is the default for xls insofar as they are the default classes bound to the file extensions.\n\nAnd it wasn't my intention to make xlsxwriter the default. It is probably best to see if people use it or prefer it as a default for a release or two.\n",
        "createdAt" : "2013-09-16T21:59:33Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "tags" : [
        ]
      },
      {
        "id" : "f57e952e-f2a5-49ff-b56d-93c3ba7a003b",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "body" : "Just to reiterate, I don't think it is worth changing the current behaviour of the (optional) defaults. At least not in 0.13. If it proves to be popular and robust we can consider that for 0.14.\n",
        "createdAt" : "2013-09-17T10:43:41Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "56bacb42-5a21-41c7-bce7-c4ce57a99717",
        "tags" : [
        ]
      },
      {
        "id" : "c8f42109-a4cc-4203-85e5-2f42f2a6c5ce",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "body" : "@jmcnamara keep in mind that you have to explicitly choose to install xlsxwriter to have this work - so it's not that big of a deal. `xlsxwriter` isn't in the major prepackaged distributions (enthought, anaconda, python(x,y), winpython, etc), so there's a low probability for people to be surprised.\n",
        "createdAt" : "2013-09-17T11:26:12Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "tags" : [
        ]
      },
      {
        "id" : "7c1837dd-558b-4fc9-a028-2a2383fe47ce",
        "parentId" : "23213b52-b674-482c-9e7a-b019cc198960",
        "authorId" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "body" : "I think this is fine for now.\n",
        "createdAt" : "2013-09-20T21:54:42Z",
        "updatedAt" : "2013-09-22T22:43:13Z",
        "lastEditedBy" : "6f890fbc-4bdf-4397-95eb-a8225d4af04f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0c290f4879a542b027b4c36a922f5ae2216ed5a",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +1694,1698 @@2. the filename extension (via the default specified in config options)\n\nBy default ``pandas`` only supports\n`openpyxl <http://packages.python.org/openpyxl/>`__ as a writer for ``.xlsx``\nand ``.xlsm`` files and `xlwt <http://www.python-excel.org/>`__ as a writer for"
  },
  {
    "id" : "c5abb56a-7e91-49b7-8b5f-45c2029eee16",
    "prId" : 6021,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "904e3d8d-d10b-4501-9696-725f1543c142",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "foo.csv has been removed before (under 'Specifying date columns'), so you will have to move that remove below this.\n",
        "createdAt" : "2014-01-24T07:40:47Z",
        "updatedAt" : "2014-01-24T07:40:47Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "879f270c120a2c0f63de449ab6fd1bcff2628175",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +531,535 @@\n   # Try to infer the format for the index column\n   df = pd.read_csv('foo.csv', index_col=0, parse_dates=True,\n                    infer_datetime_format=True)\n"
  },
  {
    "id" : "a072aef3-aef5-4ef4-8c65-d6bcd3206dda",
    "prId" : 6021,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06069c07-4d29-43a8-aa40-6888eb34905a",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "Can you make a list of this? (just `-` before each), or otherwise a code-block, as you like (but just one enter will be disregarded by Sphinx)\n",
        "createdAt" : "2014-01-24T07:41:44Z",
        "updatedAt" : "2014-01-24T07:41:44Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "879f270c120a2c0f63de449ab6fd1bcff2628175",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +522,526 @@\"12/30/2011 00:00:00\"\n\"30/Dec/2011 00:00:00\"\n\"30/December/2011 00:00:00\"\n\n`infer_datetime_format` is sensitive to `dayfirst`.  With `dayfirst=True`, it"
  },
  {
    "id" : "139d32a0-0aea-4402-a673-4ae69c5db915",
    "prId" : 6021,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04bd4206-0f64-4def-a3d3-034834f5a276",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "In the previous sections, parse_dates is always written with double backtick quotation (```parse_dates```). This will render as code, while single backtick as italic.\n",
        "createdAt" : "2014-01-24T07:43:51Z",
        "updatedAt" : "2014-01-24T07:43:51Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "879f270c120a2c0f63de449ab6fd1bcff2628175",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +504,508 @@Inferring Datetime Format\n~~~~~~~~~~~~~~~~~~~~~~~~~\nIf you have `parse_dates` enabled for some or all of your columns, and your\ndatetime strings are all formatted the same way, you may get a large speed\nup by setting `infer_datetime_format=True`.  If set, pandas will attempt"
  },
  {
    "id" : "0d81a24e-e850-466c-bdbd-5af1b13c45fd",
    "prId" : 6576,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "can you remove the leading spaces here\n",
        "createdAt" : "2014-03-11T11:46:05Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "64787771-b7e8-48fa-bf38-25aa1367ebf0",
        "parentId" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "authorId" : "05966bcf-6059-4f72-8d7c-d93433335047",
        "body" : "I changed this to bullet point. Looks like the other bullet point sections use leading spaces.\n",
        "createdAt" : "2014-03-11T14:22:08Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "05966bcf-6059-4f72-8d7c-d93433335047",
        "tags" : [
        ]
      },
      {
        "id" : "b456a8e4-9297-40f0-a629-c0b95285d0bd",
        "parentId" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "It's OK like this\n",
        "createdAt" : "2014-03-11T14:25:34Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      },
      {
        "id" : "a0e38688-1f19-48a1-b3e0-416241464d0a",
        "parentId" : "eb9b7f8e-f3f9-409d-b22f-c639f370b152",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "yep..that's fine...\n",
        "createdAt" : "2014-03-11T15:20:55Z",
        "updatedAt" : "2014-03-12T04:49:46Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "5be379f013f275b58c9ee6c04669ab7919ff728f",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1848,1852 @@\n- Pass a string to refer to the name of a particular sheet in the workbook.\n- Pass an integer to refer to the index of a sheet. Indices follow Python \n  convention, beginning at 0.\n- The default value is ``sheet_name=0``. This reads the first sheet."
  },
  {
    "id" : "79d1b9ee-72dc-47c7-a060-ad7c947159a1",
    "prId" : 6889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aef087a5-ffdc-4ed5-8deb-8151913b9b70",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "can you update the skip_footer/dtype in io.parsers.rst (at the top where the arguments are laid out) for the same way you did in the doc-strings?\n",
        "createdAt" : "2014-04-23T20:11:06Z",
        "updatedAt" : "2014-04-23T21:41:01Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "f45b7143991df96c85a1b9860a7674c3b3f2a91e",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +277,281 @@    df = pd.read_csv(StringIO(data), dtype={'b': object, 'c': np.float64})\n    df.dtypes\n\n.. note::\n    The ``dtype`` option is currently only supported by the C engine."
  },
  {
    "id" : "567c0d81-2862-4c7c-9ad4-3ab35fba2724",
    "prId" : 6937,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28ebc67c-9578-4db5-b118-97cc3e9d0743",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "pandas.io.read_gbq -> pandas.read_gbq\n",
        "createdAt" : "2014-06-10T07:22:25Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd5b6e97108b02f70e5c101eaf04401b8d5315b6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +3381,3385 @@\nAs an example, suppose you want to load all data from an existing BigQuery \ntable : `test_dataset.test_table` into a DataFrame using the :func:`~pandas.io.read_gbq` \nfunction.\n"
  },
  {
    "id" : "b84d7899-0c61-4a6a-b546-485ac6307775",
    "prId" : 6937,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45c24584-2a18-4404-8d58-cd60f617f078",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "and is it then appended? (not fully clear to me, previously you had fail/replace/append, now only one default action?)\n",
        "createdAt" : "2014-06-10T07:31:47Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      },
      {
        "id" : "4c97d70b-0dc9-4f78-a935-bc1247971aaf",
        "parentId" : "45c24584-2a18-4404-8d58-cd60f617f078",
        "authorId" : "661a0526-88d1-45b3-bba2-ca16313d3f2c",
        "body" : "The other actions were a benefit of relying on bq.py in the past. While possible to do strictly with the API, it's a lot of code for very little benefit. The data is strictly appended which was, at least in our experience, the most common use case.\n",
        "createdAt" : "2014-06-11T05:01:38Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "661a0526-88d1-45b3-bba2-ca16313d3f2c",
        "tags" : [
        ]
      },
      {
        "id" : "51451d17-14d4-4164-8449-457985963f25",
        "parentId" : "45c24584-2a18-4404-8d58-cd60f617f078",
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "Ah, I missed the rather obvious \"you can _append_ data using to_gbq()\" part.\n\nSo OK, no problem here. But maybe add it more explicitely in the docstring of `to_gbq` as well?\n",
        "createdAt" : "2014-06-11T20:22:39Z",
        "updatedAt" : "2014-06-29T21:22:19Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd5b6e97108b02f70e5c101eaf04401b8d5315b6",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +3411,3415 @@Google streaming API which requires that your destination table exists in\nBigQuery. Given the BigQuery table already exists, your DataFrame should\nmatch the destination table in column order, structure, and data types. \nDataFrame indexes are not supported. By default, rows are streamed to \nBigQuery in chunks of 10,000 rows, but you can pass other chuck values "
  },
  {
    "id" : "805e7153-c03e-4694-883e-11d0d2b8068d",
    "prId" : 9601,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e28786be-df6f-46ff-b66d-8b714c1def68",
        "parentId" : null,
        "authorId" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "body" : "There needs to be a blank line between the `.. ipython:: python` (directive command) and the actual code content\n",
        "createdAt" : "2015-03-06T12:52:43Z",
        "updatedAt" : "2015-03-06T13:00:00Z",
        "lastEditedBy" : "cc7022b2-2831-4c63-a4da-d18b0d342508",
        "tags" : [
        ]
      }
    ],
    "commit" : "08297e62945626c25413257d5a38c366d047aea6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +3839,3843 @@.. ipython:: python\n\n  reader = pd.read_stata('stata.dta', chunksize=3)\n  for df in reader:\n      print(df.shape)"
  },
  {
    "id" : "1a82335d-5669-4dfc-abd4-e2c3cb6e5b5c",
    "prId" : 9711,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbf5afff-c023-4c0a-8833-299449d041d7",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "you need `.. ipython: :python` around each of these blocks to have it exectue, or probably better `.. code-block` to have it look like code but not execute\n",
        "createdAt" : "2015-06-15T11:16:12Z",
        "updatedAt" : "2015-08-14T08:03:58Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "4694a428fd63e971159fffea4559e589088881e9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +4147,4151 @@.. code-block:: python\n\n    df = pd.read_sas('sas_xport.xpt')\n\nObtain an iterator and read an XPORT file 100,000 lines at a time:"
  },
  {
    "id" : "3d731c11-490b-4f0e-aba0-8b7bd5b3a865",
    "prId" : 10069,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "This is only true for when using compression, IIRC.\n",
        "createdAt" : "2015-05-06T15:10:16Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "d44bee17-7691-4674-8c39-b67f4c94f584",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "body" : "the \"stores need to be rewritten\" part?  I rewrote the test table uncompressed in the old version and querying in the new version does not return all rows.  I think it needs to be done regardless.\n",
        "createdAt" : "2015-05-06T15:21:30Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "tags" : [
        ]
      },
      {
        "id" : "063c8047-06e1-449b-911b-3944a6376bf4",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "oh, ok. I just wanted to make it as specific as possible. This sounds like its ALWAYS a bug. IIRC I think you actually have to specify `start` or `stop` when you query. (e.g. your query needs to be something like: `s.select('df',where='.....',start=10)`, right? (iow it has to be a chunked query)\n",
        "createdAt" : "2015-05-06T15:28:20Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "e3bfa90a-9100-4944-8e73-a0853686fa96",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "body" : "It doesn't need to be chunked.  My sample was just s.select('df', where=Term(...)).  I tried to make it non-specific by \"may appear.\"\n",
        "createdAt" : "2015-05-06T15:31:18Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "tags" : [
        ]
      },
      {
        "id" : "8380f097-9a0c-4c23-b524-b697f27b9cf3",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "hmm, ok, that's fine then.\n",
        "createdAt" : "2015-05-06T15:38:44Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      },
      {
        "id" : "d2dd5add-11e0-412a-8a18-f3ee121fe601",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "body" : "Since this is such a nuanced bug, I think it would be worth making 3.2 a requirement once that is officially out.\n",
        "createdAt" : "2015-05-06T15:46:50Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "e4cf42f5-ae7f-4a31-b7fb-d8c96a5a33ff",
        "tags" : [
        ]
      },
      {
        "id" : "da452891-dda3-44a4-8254-7f6068331dd3",
        "parentId" : "5d316876-11ed-40e9-a665-38ea4733f5b4",
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "sure, let's make an issue for 0.17.0 for that, maybe now just update install.rst to say highly recommened to use 3.2\n",
        "createdAt" : "2015-05-06T15:53:15Z",
        "updatedAt" : "2015-05-06T17:26:25Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "61fae01fb757a30d494a523b3dc71eb7ffe1487f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2368,2372 @@   \n   There is a ``PyTables`` indexing bug which may appear when querying stores using an index.  If you see a subset of results being returned, upgrade to ``PyTables`` >= 3.2.  Stores created previously will need to be rewritten using the updated version.\n\n.. ipython:: python\n   :suppress:"
  },
  {
    "id" : "dc2a3fe2-894c-4662-a98e-1a5f6ac4db67",
    "prId" : 10097,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5e03b89-e8eb-40e9-99ca-f4ac8960d0fc",
        "parentId" : null,
        "authorId" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "body" : "say you can pass the `dropna=True` to get the previous behavior\n\nI see you did below, so nvm.\n",
        "createdAt" : "2015-07-30T18:08:17Z",
        "updatedAt" : "2015-07-31T18:00:57Z",
        "lastEditedBy" : "7086d5c0-382b-4c41-b70d-144a07643f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "2377b5c1aafa33fb5a3fb3966e0bb16b8bcd2c6a",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2414,2418 @@\n   As of version 0.17.0, ``HDFStore`` will not drop rows that have all missing values by default. Previously, if all values (except the index) were missing, ``HDFStore`` would not write those rows to disk. \n\n.. ipython:: python\n   :suppress:"
  }
]