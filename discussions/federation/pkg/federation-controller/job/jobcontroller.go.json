[
  {
    "id" : "c2ffad84-f075-4045-aa1b-1ac38bcf40bf",
    "prId" : 36197,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36197#pullrequestreview-7536137",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35aa66c4-e271-486d-b590-d52827cad543",
        "parentId" : null,
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "All new controllers should also have deletion helper. Ref https://github.com/kubernetes/kubernetes/pull/36296. Its fine if you want to do that in next PR but wanted to make sure you know\n",
        "createdAt" : "2016-11-07T23:46:46Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "ef603002-ca9c-44d9-8315-a917e766e7c5",
        "parentId" : "35aa66c4-e271-486d-b590-d52827cad543",
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "yeah, I saw your PRs for federated delete operatin, will update it soon.\n",
        "createdAt" : "2016-11-08T00:32:08Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      }
    ],
    "commit" : "746444e43a8a1965f6907ee29f65804e36187505",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +89,93 @@\t// For events\n\teventRecorder record.EventRecorder\n\n\tdefaultPlanner *planner.Planner\n\tdeletionHelper *deletionhelper.DeletionHelper"
  },
  {
    "id" : "84f28902-0c61-4779-9bf0-d33fd79a837a",
    "prId" : 36197,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36197#pullrequestreview-7538121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f28e7fc-99ce-45b8-b350-a562fdc74ff1",
        "parentId" : null,
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Shouldnt it retry after some time?\n",
        "createdAt" : "2016-11-07T23:52:38Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "06667339-4c7d-42f7-86f6-4fc8a3e4cb18",
        "parentId" : "0f28e7fc-99ce-45b8-b350-a562fdc74ff1",
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "the cache actually never returns an error.\n",
        "createdAt" : "2016-11-08T00:49:38Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      }
    ],
    "commit" : "746444e43a8a1965f6907ee29f65804e36187505",
    "line" : 257,
    "diffHunk" : "@@ -1,1 +255,259 @@\tif err != nil {\n\t\tglog.Errorf(\"Couldn't get federated job %v: %v\", key, err)\n\t\treturn\n\t}\n\tif exists { // ignore jobs exists only in local k8s"
  },
  {
    "id" : "b39e7fee-1bb7-4afc-82e3-a9ff29d5f560",
    "prId" : 36197,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36197#pullrequestreview-19538358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa4c4f04-90b9-448d-8e59-ef8044c0a81a",
        "parentId" : null,
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "this can also be moved to a common location usable for all controllers.",
        "createdAt" : "2017-02-01T12:27:50Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      }
    ],
    "commit" : "746444e43a8a1965f6907ee29f65804e36187505",
    "line" : 283,
    "diffHunk" : "@@ -1,1 +281,285 @@}\n\ntype reconciliationStatus string\n\nconst ("
  },
  {
    "id" : "dfab22fe-68fc-4799-8625-e1fbff9db22e",
    "prId" : 36197,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36197#pullrequestreview-31457124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f52d3d7d-6264-4bc3-86f0-c224c5dbec90",
        "parentId" : null,
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "are you sure this works passing the currentReplicaCount and estimatedCapacity as nil.\r\nI see in planner code that this is used in a way it can crash, for example (when preferences are specified) :\r\n```\r\n\tif p.preferences.Rebalance == false {\r\n\t\tfor _, preference := range preferences {\r\n\t\t\tplanned := plan[preference.clusterName]\r\n\t\t\tcount, hasSome := currentReplicaCount[preference.clusterName]\r\n```",
        "createdAt" : "2017-02-01T12:47:43Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "7ba9bf1d-ee4d-4217-9392-dff38c4a3879",
        "parentId" : "f52d3d7d-6264-4bc3-86f0-c224c5dbec90",
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "It doesn't crash, and the hasSome will be false.",
        "createdAt" : "2017-02-10T05:23:49Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      },
      {
        "id" : "cb5f88f6-06eb-4b43-b87f-b2d4f8b5ac20",
        "parentId" : "f52d3d7d-6264-4bc3-86f0-c224c5dbec90",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "ok",
        "createdAt" : "2017-02-15T14:43:44Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "788eef43-2ba6-4b8d-ae14-1cae2aae4219",
        "parentId" : "f52d3d7d-6264-4bc3-86f0-c224c5dbec90",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Actually I have my own qualms about using the planner code originally meant for replicasets, but if it works as intended, then it might be good enough.\r\nIt seems to be quite statically used though. \r\nOn the other hand, when used in replicasets, the schedule (or reschedule) gets the status of pods for example, get pods which are in unschedulable conditions and updates the cluster capacity accordingly so in the next pass that particular cluster will get lesser replicas.\r\n\r\nNow lets consider a similar scenario for jobs.\r\noriginally a job started with parallelism = 3 and completions = 6 in a federation of 3 clusters\r\nso as per this code first pass, each cluster will get a job with p = 1 and c = 2\r\nlets say in cluster 3 for some reason pods don't get into run status for some reason.\r\nShould the local job controller take care of rescheduling pods for such jobs? \r\nIf the job remains idle and none of active, succeeded or failed count is incremented, what happens?\r\nOther clusters might return 2 and 2 successful completions each.\r\nOne simple answer might be that the job remains in the unfinished state, but I think federation ideally can take care of this, by letting other clusters finish the job.\r\nPlease correct me if I am missing something.",
        "createdAt" : "2017-02-15T15:30:49Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "8b88ddd9-5144-452b-b06b-46ecdb1ecd7c",
        "parentId" : "f52d3d7d-6264-4bc3-86f0-c224c5dbec90",
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "I agree it's a nice to have feature. however the problem is, if you intent to move jobs to other clusters, how to determine that the failing cluster will not succeed jobs later as it keeps trying.",
        "createdAt" : "2017-02-16T00:31:19Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      },
      {
        "id" : "3cf0bb10-802e-4a36-8ccd-15e6a11a4751",
        "parentId" : "f52d3d7d-6264-4bc3-86f0-c224c5dbec90",
        "authorId" : null,
        "body" : "As per my comments in the design doc, I think that the solution here is to adjust downward the spec.completions of the job in the cluster where pods are failing to schedule, and transfer that number of completions to the job in another cluster where pods are successfully able to schedule. That way the total number of completions across all clusters cannot exceed the desired number.\r\n\r\nI'm not sure what happens if one increases the number of completions for a job that has already completed it's allocated number of completions (and hence already shut down it's pods). One might hope that it will respawn pods to perform the additional required completions.  That would be first prize.  If this is not the case, it might be necessary to spawn a new Job for the additional completions required.  This raises the question of potential name clashes with the original completed but not yet deleted job, but hopefully we can figure that out (e.g. by deleting the completed job) if that becomes necessary.",
        "createdAt" : "2017-04-06T23:55:58Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "746444e43a8a1965f6907ee29f65804e36187505",
    "line" : 342,
    "diffHunk" : "@@ -1,1 +340,344 @@\t\tclusterNames = append(clusterNames, cluster.Name)\n\t}\n\tparallelismResult, _ := plnr.Plan(parallelism, clusterNames, nil, nil, fjob.Namespace+\"/\"+fjob.Name)\n\n\tif frsPref != nil {"
  },
  {
    "id" : "bb8b914e-15e2-421a-bfed-337a90e825b9",
    "prId" : 36197,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36197#pullrequestreview-22140428",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa9d1af5-35f4-40d6-adae-03e3020c13f1",
        "parentId" : null,
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Whats the specific reason of doing this?",
        "createdAt" : "2017-02-01T18:17:44Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "904bfd16-fa56-4520-af2a-60bf7ec449b6",
        "parentId" : "aa9d1af5-35f4-40d6-adae-03e3020c13f1",
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "the first plan schedules the job parallelism which is max running pods at any given time which honor the plan preferences.\r\nthe second one schedules the job completion which the count of completed jobs. It could be a lot larger than the former makes it unable to honor the min/max constrains either.",
        "createdAt" : "2017-02-10T05:38:00Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      },
      {
        "id" : "f25d9681-bfd0-445a-8e88-f7737f53da57",
        "parentId" : "aa9d1af5-35f4-40d6-adae-03e3020c13f1",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "As far as I understand from this I guess, the current impl cannot honor the min/max constraints coming in preferences from user. In that case, why do we need to take those preferences from the user in the first place. Wouldn't it be bad user experience?\r\nI mean we can have a subset of preferences allowed when specified for jobs, right?",
        "createdAt" : "2017-02-15T14:40:29Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "58da72e4-0d20-40e3-96f8-d1a7b5a27de0",
        "parentId" : "aa9d1af5-35f4-40d6-adae-03e3020c13f1",
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "this is for scheduling the completions which means how many jobs needed to finish in that cluster.\r\nthe job parallelism is how many job pods could be running at the same time which is honors user preferences like `replicas` in `replicaset`.",
        "createdAt" : "2017-02-15T23:39:22Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      }
    ],
    "commit" : "746444e43a8a1965f6907ee29f65804e36187505",
    "line" : 346,
    "diffHunk" : "@@ -1,1 +344,348 @@\tif frsPref != nil {\n\t\tfor _, clusterPref := range frsPref.Clusters {\n\t\t\tclusterPref.MinReplicas = 0\n\t\t\tclusterPref.MaxReplicas = nil\n\t\t}"
  },
  {
    "id" : "7ecbc926-9509-40cb-a218-48b909649dc7",
    "prId" : 36197,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36197#pullrequestreview-19617728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1185af8b-c013-4e1c-a203-27f6c9f4e077",
        "parentId" : null,
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "same as comment on line number 331",
        "createdAt" : "2017-02-01T18:18:22Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      }
    ],
    "commit" : "746444e43a8a1965f6907ee29f65804e36187505",
    "line" : 357,
    "diffHunk" : "@@ -1,1 +355,359 @@\tcompletionsResult := make(map[string]int64)\n\tif fjob.Spec.Completions != nil {\n\t\tcompletionsResult, _ = plnr.Plan(int64(*fjob.Spec.Completions), clusterNames, nil, nil, fjob.Namespace+\"/\"+fjob.Name)\n\t}\n"
  },
  {
    "id" : "e9f6a1bf-0548-4945-aaea-35e01e7fe219",
    "prId" : 36197,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36197#pullrequestreview-22014335",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd32b661-bf20-41fc-87e6-58d7ce4c0e55",
        "parentId" : null,
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "this is just a naming nit, but I guess it just reads bad.. :-), you can choose to leave it as is though.",
        "createdAt" : "2017-02-15T14:42:37Z",
        "updatedAt" : "2017-08-07T19:17:10Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      }
    ],
    "commit" : "746444e43a8a1965f6907ee29f65804e36187505",
    "line" : 363,
    "diffHunk" : "@@ -1,1 +361,365 @@\tfor _, clusterName := range clusterNames {\n\t\tparalle := int32(parallelismResult[clusterName])\n\t\tcomplet := int32(completionsResult[clusterName])\n\t\tresult := scheduleResult{\n\t\t\tParallelism: &paralle,"
  }
]