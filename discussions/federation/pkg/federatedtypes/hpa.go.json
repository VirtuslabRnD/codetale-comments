[
  {
    "id" : "9ef9ff2e-f04a-4718-9c8d-46f4e7767dfe",
    "prId" : 49950,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49950#pullrequestreview-56390913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64d4e2c3-59ca-4f7f-8b15-c8bea648d260",
        "parentId" : null,
        "authorId" : null,
        "body" : "I think that you will need tests for Deployments also.",
        "createdAt" : "2017-08-11T19:11:53Z",
        "updatedAt" : "2017-08-24T10:07:12Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "c0f15f8e-8e8d-46fa-a13e-a323f92bb56e",
        "parentId" : "64d4e2c3-59ca-4f7f-8b15-c8bea648d260",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "This is actually used only for crud test written with the sync controller (which tests crud of the hpa object alone). Meanwhile, I will add tests for both types in e2e.",
        "createdAt" : "2017-08-15T16:21:18Z",
        "updatedAt" : "2017-08-24T10:07:12Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      }
    ],
    "commit" : "da2db332f1552a7ddcba31d5f8c9811ccf2b87d1",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +172,176 @@\t\tSpec: autoscalingv1.HorizontalPodAutoscalerSpec{\n\t\t\tScaleTargetRef: autoscalingv1.CrossVersionObjectReference{\n\t\t\t\tKind: \"ReplicaSet\",\n\t\t\t\tName: \"myrs\",\n\t\t\t},"
  },
  {
    "id" : "9fb68bac-6d23-4f5c-8466-a9c563bac270",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-54210232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e3365f1-8b02-479e-8781-50b2aa9db759",
        "parentId" : null,
        "authorId" : null,
        "body" : "What if the cast fails?",
        "createdAt" : "2017-06-01T03:23:21Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "6c6414be-a92f-41c2-ae9e-841c13940f51",
        "parentId" : "1e3365f1-8b02-479e-8781-50b2aa9db759",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Catching this failure here and at all other places which use runtime.object and then cast to a specific type might be considered as being extremely safe programming local to the function. But if this happens then there probably is much bigger a problem which needs to be looked into and the tests I guess can catch that, for sure. Also, this is same as done for all other controllers using sync controller.",
        "createdAt" : "2017-07-19T11:55:46Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "d4e860df-41d7-4f8f-bc23-f23ed0704c1d",
        "parentId" : "1e3365f1-8b02-479e-8781-50b2aa9db759",
        "authorId" : null,
        "body" : "Fair enough, but failure will cause the whole controller manager to crash.  Please file an issue to have it fixed holistically across sync controllers.",
        "createdAt" : "2017-07-25T23:04:32Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "2beee0ea-2290-49f3-8984-5bf269ea6ed4",
        "parentId" : "1e3365f1-8b02-479e-8781-50b2aa9db759",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "I think @marun might be better at explaining the point of view, this is this way (per his original code); or conclude that this needs correction, if any. Request @marun to have a look.",
        "createdAt" : "2017-07-26T16:39:28Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "cc718462-e4bc-4894-88c6-d2a2cd6a02b9",
        "parentId" : "1e3365f1-8b02-479e-8781-50b2aa9db759",
        "authorId" : null,
        "body" : "ok",
        "createdAt" : "2017-08-03T20:15:40Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +94,98 @@\nfunc (a *HpaAdapter) FedCreate(obj pkgruntime.Object) (pkgruntime.Object, error) {\n\thpa := obj.(*autoscalingv1.HorizontalPodAutoscaler)\n\treturn a.client.AutoscalingV1().HorizontalPodAutoscalers(hpa.Namespace).Create(hpa)\n}"
  },
  {
    "id" : "d1b51df1-b2ad-43e2-b9f4-bbdad598c30d",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-52221702",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afae5c2e-92db-4033-a7be-31c30bc32f57",
        "parentId" : null,
        "authorId" : null,
        "body" : "As above.",
        "createdAt" : "2017-06-01T03:23:56Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "9c070f02-981f-4f33-b6d7-d1d48dce0526",
        "parentId" : "afae5c2e-92db-4033-a7be-31c30bc32f57",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Ditto!",
        "createdAt" : "2017-07-19T11:55:58Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "148dde91-8a57-4708-8c55-7b6493ce6fc8",
        "parentId" : "afae5c2e-92db-4033-a7be-31c30bc32f57",
        "authorId" : null,
        "body" : "ok",
        "createdAt" : "2017-07-25T23:04:52Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +111,115 @@\nfunc (a *HpaAdapter) FedUpdate(obj pkgruntime.Object) (pkgruntime.Object, error) {\n\thpa := obj.(*autoscalingv1.HorizontalPodAutoscaler)\n\treturn a.client.AutoscalingV1().HorizontalPodAutoscalers(hpa.Namespace).Update(hpa)\n}"
  },
  {
    "id" : "d8bd46ce-f1f7-471e-bf5b-a776f6974c87",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-52221722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a582121-8f37-4955-9960-b3bf87d59619",
        "parentId" : null,
        "authorId" : null,
        "body" : "As above.",
        "createdAt" : "2017-06-01T03:24:08Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "56ed2a76-ca59-4ae4-a261-4d56bd616520",
        "parentId" : "5a582121-8f37-4955-9960-b3bf87d59619",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Ditto!",
        "createdAt" : "2017-07-19T11:56:20Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "f7da29c5-8ce1-48d9-8f01-471d5b930174",
        "parentId" : "5a582121-8f37-4955-9960-b3bf87d59619",
        "authorId" : null,
        "body" : "ok",
        "createdAt" : "2017-07-25T23:04:59Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +120,124 @@\nfunc (a *HpaAdapter) ClusterCreate(client kubeclientset.Interface, obj pkgruntime.Object) (pkgruntime.Object, error) {\n\thpa := obj.(*autoscalingv1.HorizontalPodAutoscaler)\n\treturn client.AutoscalingV1().HorizontalPodAutoscalers(hpa.Namespace).Create(hpa)\n}"
  },
  {
    "id" : "57058bfb-2d06-4d4a-80bb-5018bbcde787",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-52221739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae4299fc-ab93-44b2-8a4e-e534e49b7d6a",
        "parentId" : null,
        "authorId" : null,
        "body" : "As above.",
        "createdAt" : "2017-06-01T03:24:28Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "b92441af-9485-48e5-88ef-0ce3e70be0f3",
        "parentId" : "ae4299fc-ab93-44b2-8a4e-e534e49b7d6a",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Ditto!",
        "createdAt" : "2017-07-19T11:56:22Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "67581e52-cb6a-421a-9d61-39b3027cead2",
        "parentId" : "ae4299fc-ab93-44b2-8a4e-e534e49b7d6a",
        "authorId" : null,
        "body" : "ok",
        "createdAt" : "2017-07-25T23:05:08Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 139,
    "diffHunk" : "@@ -1,1 +137,141 @@\nfunc (a *HpaAdapter) ClusterUpdate(client kubeclientset.Interface, obj pkgruntime.Object) (pkgruntime.Object, error) {\n\thpa := obj.(*autoscalingv1.HorizontalPodAutoscaler)\n\treturn client.AutoscalingV1().HorizontalPodAutoscalers(hpa.Namespace).Update(hpa)\n}"
  },
  {
    "id" : "c6985ae3-9c8d-4485-ae6c-da45331fe219",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-52229016",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78d294da-f085-454e-8763-d007ec60288d",
        "parentId" : null,
        "authorId" : null,
        "body" : "nit: This could perhaps be eliminated by using max(1, ...) above?  Up to you whether or not you prefer this.",
        "createdAt" : "2017-06-01T19:26:50Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "46b63dd9-2001-427d-9b04-42273defff82",
        "parentId" : "78d294da-f085-454e-8763-d007ec60288d",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Skipping for now!",
        "createdAt" : "2017-07-19T13:24:17Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "59e1aa26-7bd2-45a5-b9fa-02468ee97fb3",
        "parentId" : "78d294da-f085-454e-8763-d007ec60288d",
        "authorId" : null,
        "body" : "ok",
        "createdAt" : "2017-07-25T23:54:55Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 306,
    "diffHunk" : "@@ -1,1 +304,308 @@\t\tmax: requestedReplicas.max / int32(len(currentObjs)),\n\t}\n\tif rdc.min < 1 {\n\t\trdc.min = 1\n\t}"
  },
  {
    "id" : "d83644e1-afab-4d85-bb0e-4fab2f5cd59d",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-52225755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e9a9bc7-43b5-4233-a6fb-4d266c88e3ad",
        "parentId" : null,
        "authorId" : null,
        "body" : "Are cluster selectors taken into account higher up in the stack? You don't want to create HPA's in clusters that are not selected by the cluster selector.",
        "createdAt" : "2017-06-01T22:59:59Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "972a07ae-f901-472f-931d-d79172ce633c",
        "parentId" : "1e9a9bc7-43b5-4233-a6fb-4d266c88e3ad",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Yes.",
        "createdAt" : "2017-07-19T15:21:41Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "36cc09b3-e333-4f6f-9519-e8a29f671931",
        "parentId" : "1e9a9bc7-43b5-4233-a6fb-4d266c88e3ad",
        "authorId" : null,
        "body" : "Thanks.",
        "createdAt" : "2017-07-25T23:32:14Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 638,
    "diffHunk" : "@@ -1,1 +636,640 @@\t// If we have new clusters where we can  give our replicas,\n\t// then give away all our replicas to the new clusters first.\n\tif lists.noHpa.Len() > 0 {\n\t\tfor toDistributeMax > 0 {\n\t\t\tfor _, cluster := range lists.noHpa.UnsortedList() {"
  },
  {
    "id" : "8e797572-e41b-4cd9-b66d-da25281b5ff7",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-54520459",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "751fd2f2-a023-4c6b-9b0e-34708d1fd278",
        "parentId" : null,
        "authorId" : null,
        "body" : "Again here you dump all remaining replicas on one cluster.  I don't understand why.  Can you explain?  It seems that you should distribute the remaining replicas across all clusters, rather than fill one cluster, and leave the others empty?",
        "createdAt" : "2017-06-07T16:07:51Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "2adc6311-dda1-4f88-94c0-35397f079728",
        "parentId" : "751fd2f2-a023-4c6b-9b0e-34708d1fd278",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "As above.",
        "createdAt" : "2017-07-19T15:26:25Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "00d35439-2555-422d-a52b-2953db370a9e",
        "parentId" : "751fd2f2-a023-4c6b-9b0e-34708d1fd278",
        "authorId" : null,
        "body" : "OK, I'll go through the code again and make sure I understand it.",
        "createdAt" : "2017-07-25T23:33:04Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "5880d880-9c51-454d-8004-54fc1bb34c32",
        "parentId" : "751fd2f2-a023-4c6b-9b0e-34708d1fd278",
        "authorId" : null,
        "body" : "Comment as above.  I don't think it's right.",
        "createdAt" : "2017-08-03T19:33:57Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "c2be6602-b927-4d61-bcec-9721a07912b3",
        "parentId" : "751fd2f2-a023-4c6b-9b0e-34708d1fd278",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Extending https://github.com/kubernetes/kubernetes/pull/45993#discussion_r131527414.\r\n\r\nmin value of rdc for max is set to 2. (in a similar case of 99/100 or 5/6 which results in 0). So for an example of 5 replicas on 6 clusters, the distribution that will happen would be 2:2:1 max replicas into 3 clusters.\r\n```\r\n\t// TODO: Is there a better way?\r\n\t// We need to cap the lowest limit of Max to 2, because in a\r\n\t// situation like both min and max become 1 (same) for all clusters,\r\n\t// no rebalancing would happen.\r\n\tif rdc.max < 2 {\r\n\t\trdc.max = 2\r\n\t}\r\n```\r\nI intentionally kept this value to 2, for reasons in the comment. As of now, I dont have a better way.. :).",
        "createdAt" : "2017-08-05T18:46:26Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 659,
    "diffHunk" : "@@ -1,1 +688,692 @@\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif toDistributeMax < rdc.max {\n\t\t\t\t\treplicas.max += toDistributeMax\n\t\t\t\t\ttoDistributeMax = 0"
  },
  {
    "id" : "c93d1686-5351-4cd8-94a1-20eb84d9ac71",
    "prId" : 45993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45993#pullrequestreview-54520517",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "264cd321-7ccf-4c89-922e-4976c92a68d4",
        "parentId" : null,
        "authorId" : null,
        "body" : "This seems to assume that all clusters are weighted equally, which is not generally the case.  Surely you need to apply cluster weightings here?",
        "createdAt" : "2017-07-26T00:38:34Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "e8af4b83-05d0-49a0-84c5-8fc9bc28712b",
        "parentId" : "264cd321-7ccf-4c89-922e-4976c92a68d4",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "When I was at including cluster weights for the hpa (using annotations on fed object). I could not really contemplate, how/what a user might want the weights to be put use to. \r\nI mean autoscaler is already specifying the target cpu utilization, and thats the criteria we choose to do an eventual distribution of hpa replicas across clusters, to an effect that the replicas remain or move where they are needed most. I could think of specific min/max which could be hard limits put by user per cluster (which I haven't implemented yet and intend to do as a follow up). I could as mentioned not figure out real usage of weights put on hpas (as to what to apply weights to, cpu utilisation?). I see here that you are suggesting that the replica distribution number could be weighted (which I see as contradicting use as it can eventually conflict with the conditions we use to move replicas around, which for the autoscaler targets cpu utilization). I think more intricate discussion is needed on this and would request that we let it be as a follow up (I see hpa clearly usable without preferences also). Have put an item in the follow up issue https://github.com/kubernetes/kubernetes/issues/49644.",
        "createdAt" : "2017-07-26T16:35:18Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      },
      {
        "id" : "34f9d7fe-a864-4e19-ae64-6bba41c93c62",
        "parentId" : "264cd321-7ccf-4c89-922e-4976c92a68d4",
        "authorId" : null,
        "body" : "Fair enough.  I was thinking of what the desired initial placement of replicas might be before the rebalancing starts to happen.  But I agree, that requires some more thought and discussion, and can be done as a followup.\r\n\r\nIn my experience, a user might know approximately what the relative weightings of clusters are (e.g. us : eu : asia = 3 : 2 : 1) and generally that's how they want their min and max replicas distributed in the normal case.  It's only in relatively unusual  situations like failures or unexpected traffic shifts that they want to redistribute max replicas (e.g. SuperBowl happens in the U.S, while Europe is asleep, so they want some unneeded \"max\" capacity in Europe moved to the U.S. to prevent the U.S from overloading, rather than have it hit it's max per it's originally specified weight).\r\n\r\nHappy to defer to followup discussion/PR.",
        "createdAt" : "2017-08-03T19:45:55Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "4345cace-16eb-4499-9c1f-600597575dce",
        "parentId" : "264cd321-7ccf-4c89-922e-4976c92a68d4",
        "authorId" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "body" : "Thanks for the suggestion. Will put up the details that you suggested and alternatives, that I can think of against the issue, when I get there. ",
        "createdAt" : "2017-08-05T18:50:02Z",
        "updatedAt" : "2017-08-05T19:09:37Z",
        "lastEditedBy" : "cc7376b4-34cd-480b-ad10-0b4b879dde52",
        "tags" : [
        ]
      }
    ],
    "commit" : "0bea0ca1d94745096548ce643dad80ed9c5b9504",
    "line" : 302,
    "diffHunk" : "@@ -1,1 +300,304 @@\t}\n\t// replica distribution count, per cluster\n\trdc := replicaNums{\n\t\tmin: requestedReplicas.min / int32(len(currentObjs)),\n\t\tmax: requestedReplicas.max / int32(len(currentObjs)),"
  }
]