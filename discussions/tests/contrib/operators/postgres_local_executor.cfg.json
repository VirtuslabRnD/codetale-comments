[
  {
    "id" : "1d5dba3d-bef3-4411-b03b-ec207af62862",
    "prId" : 4493,
    "prUrl" : "https://github.com/apache/airflow/pull/4493#pullrequestreview-191935600",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fab63152-2cba-494d-871a-75c6714aee63",
        "parentId" : null,
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "Why do we need LocalExecutor?",
        "createdAt" : "2019-01-12T02:23:56Z",
        "updatedAt" : "2019-01-12T02:55:17Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "4ff4389a-6781-409c-a4eb-0a7768321c42",
        "parentId" : "fab63152-2cba-494d-871a-75c6714aee63",
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "There are some system tests that actually require Local Executor - It's actually explained in the comment above :) . We have one test (test_gcp_bigtable_operator_system.py) which is based on example_gcp_bigtable_operators.py DAG. In this test we start an operator (create bigtable database) and sensor (wait for database replication) in parallel. It makes sense to start them in parallel. only when both are complete we attempt to delete the database. This way we make sure that the sensor is actually waiting for something and not simply firing off immediately after create. And that it does some polling even before the database is created and in the process of being created and works fine for that statuses as well. \r\n\r\nNote that this is merely a configuration file that you can set in AIRFLOW_CONFIG - it is disabled by default in local environment and IDE system tests (there we use sqlite + sequential executor by default). Only when you set AIRFLOW_CONFIG variable to this file before starting the tests, local executor and postgres are used. But the tests in the container environment (and in Cloud Build) are executed using Postgres + LocalExecutor by default. This way we also check that our examples are not working simply because they run sequentially. We caught couple of errors (races/parallel execution problems) which we would not have found if we run them with Sequential Executor.",
        "createdAt" : "2019-01-12T02:44:26Z",
        "updatedAt" : "2019-01-12T02:55:17Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      },
      {
        "id" : "01425275-a06f-4e99-b8b8-6d9f9253b000",
        "parentId" : "fab63152-2cba-494d-871a-75c6714aee63",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "Thanks. Probably I was just too sleepy after a long working friday :) that I missed the comment",
        "createdAt" : "2019-01-12T11:35:11Z",
        "updatedAt" : "2019-01-12T11:35:11Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ac496aa7e77b4de0d9f9de7dbc90d4568f60b4c",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +26,30 @@[core]\nexecutor = LocalExecutor\nsql_alchemy_conn = postgresql:///airflow/airflow.db"
  }
]