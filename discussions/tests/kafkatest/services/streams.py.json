[
  {
    "id" : "0d910e8b-a076-4a12-8af4-57c47c1084dd",
    "prId" : 4513,
    "prUrl" : "https://github.com/apache/kafka/pull/4513#pullrequestreview-94758026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "326e2fcd-9997-4285-a47d-881b4482789e",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Where is this function used?",
        "createdAt" : "2018-02-07T00:45:26Z",
        "updatedAt" : "2018-02-07T15:50:05Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d8d3a6c3-ae7a-4ab7-bea3-428f2585d98f",
        "parentId" : "326e2fcd-9997-4285-a47d-881b4482789e",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "when calling `processor.start()`.  It overrides the `streams.py#start_cmd` which in turn overrides the [Service.start_cmd](http://ducktape-docs.readthedocs.io/en/latest/_modules/ducktape/services/service.html#Service.start) \r\n\r\nThis is needed due to one test starting with Kafka down.  The default implementation verifies Kafka is up, but we need to skip that for the `test_streams_runs_with_broker_down_initially`",
        "createdAt" : "2018-02-07T15:48:08Z",
        "updatedAt" : "2018-02-07T15:50:05Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "0390346035e44f40d413e78cd08f1781c8e2bd46",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +217,221 @@                                                                 configs)\n\n    def start_cmd(self, node):\n        args = self.args.copy()\n        args['kafka'] = self.kafka.bootstrap_servers(validate=False)"
  },
  {
    "id" : "6aa5088a-3114-4bea-89a1-b950e62a21f0",
    "prId" : 4636,
    "prUrl" : "https://github.com/apache/kafka/pull/4636#pullrequestreview-123526358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cfdfb5e-582b-412c-89b2-159dfab97047",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "will the system test results still get errors when these files aren't found?",
        "createdAt" : "2018-05-25T21:30:29Z",
        "updatedAt" : "2018-05-31T03:24:33Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "a5eafb9b-7502-468c-8142-831dfc7f503c",
        "parentId" : "9cfdfb5e-582b-412c-89b2-159dfab97047",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes, we need to specify all used files here -- otherwise they won't be collected after the test finished.",
        "createdAt" : "2018-05-25T22:16:58Z",
        "updatedAt" : "2018-05-31T03:24:33Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a87bd5254155a9d60ba479371305ddaae99282d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +54,58 @@            \"collect_default\": True},\n        \"streams_log.1\": {\n            \"path\": LOG_FILE + \".1\",\n            \"collect_default\": True},\n        \"streams_stdout.1\": {"
  },
  {
    "id" : "ba34f1b7-62f8-4177-b905-5cf79bc86f71",
    "prId" : 4689,
    "prUrl" : "https://github.com/apache/kafka/pull/4689#pullrequestreview-103171384",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be7775ee-f390-4d5b-a19f-e825819da993",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "So, this wasn't valid python3... It's also the only `super` invocation in kafkatest that's formatted this way. I'm not sure if it's valid python2.\r\n\r\nBut it only affects cleanup for the Eos test, and it just generates a warning instead of failing the test. So I can see how this could go unnoticed if the tests are usually run from a clean state instead of cleaning up after themselves.",
        "createdAt" : "2018-03-12T18:41:24Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "31719e4426c9591c2fb2f640070c1c0709f2a737",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +165,169 @@    def clean_node(self, node):\n        if self.clean_node_enabled:\n            super(StreamsEosTestBaseService, self).clean_node(node)\n\n"
  },
  {
    "id" : "257c8dbb-e083-4cbb-9dec-bbe04f6150de",
    "prId" : 4689,
    "prUrl" : "https://github.com/apache/kafka/pull/4689#pullrequestreview-103655784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6301adca-1f36-4c97-bc4e-9796dd524611",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I got a lot of value out of having this in the output. What do you think about leaving this in?",
        "createdAt" : "2018-03-13T18:49:01Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "8e340265-3fad-45cd-81da-b34042ce5432",
        "parentId" : "6301adca-1f36-4c97-bc4e-9796dd524611",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "+1 from me",
        "createdAt" : "2018-03-13T20:32:40Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "79118d6b-d071-435a-89f7-9e6cf155e360",
        "parentId" : "6301adca-1f36-4c97-bc4e-9796dd524611",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "+1.",
        "createdAt" : "2018-03-14T00:03:45Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "31719e4426c9591c2fb2f640070c1c0709f2a737",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +124,128 @@              \" %(kafka)s %(state_dir)s %(user_test_args)s %(user_test_args1)s %(user_test_args2)s\" \\\n              \" %(user_test_args3)s & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s\" % args\n        self.logger.info(\"Executing: \" + cmd)\n\n        return cmd"
  },
  {
    "id" : "753aff8a-0adb-46a1-b609-95568c856f14",
    "prId" : 4746,
    "prUrl" : "https://github.com/apache/kafka/pull/4746#pullrequestreview-105945754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87030422-561d-48fe-a46a-cc040cc5cd3c",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Could you elaborate a bit why we need to disable_auto_terminate here?",
        "createdAt" : "2018-03-21T21:59:30Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d4d62ee7-885b-4cb5-9007-4ae1f0f14304",
        "parentId" : "87030422-561d-48fe-a46a-cc040cc5cd3c",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "The SmokeTestDriver generates 5000 records and terminates -- this is not enough data for the upgrade test. I modified the SmokeTestDriver to allow generating and \"infinite\" amount of data, ie, it does not auto-terminate itself after 5000 records but runs until it's stopped externally.",
        "createdAt" : "2018-03-21T23:35:16Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe469ed34fb4f0d1094a542e2b2dd7d27cedd2e2",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +258,262 @@        args['pidfile'] = self.PID_FILE\n        args['log4j'] = self.LOG4J_CONFIG_FILE\n        args['disable_auto_terminate'] = self.DISABLE_AUTO_TERMINATE\n        args['kafka_run_class'] = self.path.script(\"kafka-run-class.sh\", node)\n"
  },
  {
    "id" : "9c5b767e-db74-44b1-8448-ed2fa0e56add",
    "prId" : 6382,
    "prUrl" : "https://github.com/apache/kafka/pull/6382#pullrequestreview-212484491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d8e4644-cb87-42f3-9341-09f194b057bb",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Let me know if there's a better way to do this...",
        "createdAt" : "2019-03-06T17:05:37Z",
        "updatedAt" : "2019-03-08T19:03:13Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "40167662-58fd-4128-81d1-761056b78f1c",
        "parentId" : "8d8e4644-cb87-42f3-9341-09f194b057bb",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I thought there were already existing`EOS` tests that use the `StreamsSmokeTest`?",
        "createdAt" : "2019-03-06T18:47:04Z",
        "updatedAt" : "2019-03-08T19:03:13Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "159099f7-f151-4965-9663-dc595acfc32c",
        "parentId" : "8d8e4644-cb87-42f3-9341-09f194b057bb",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "To my other comment above: what's the difference of running `StreamsSmokeTestBaseService#process-eos` v.s. `StreamsEosTestBaseService#process`? The former uses StreamsSmokeTest while the latter use StreamsEosTest client. Can we just consolidate them?",
        "createdAt" : "2019-03-07T01:24:09Z",
        "updatedAt" : "2019-03-08T19:03:13Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "08376545-cb33-4f84-9663-be19ba3dd954",
        "parentId" : "8d8e4644-cb87-42f3-9341-09f194b057bb",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Shameful admission: I thought about this, but just didn't take the time to check.\r\n\r\nThe bounce test was obviously duplicated, but the eos test is more complex. I knew I'd have to do a case-by-case check to make sure I didn't remove coverage. I'll do my homework now...",
        "createdAt" : "2019-03-07T18:24:54Z",
        "updatedAt" : "2019-03-08T19:03:13Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "0b3b0378-46ae-4a2f-8c01-6cb84a6b39af",
        "parentId" : "8d8e4644-cb87-42f3-9341-09f194b057bb",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, so the smoke test and the eos test are similar, but not identical.\r\n\r\nThe smoke test application has more features in it, though. So, we have more feature coverage under eos when we test with the smoke test.\r\n\r\nThe eos test evaluates two topologies one with no repartition, and one with a repartition. The smoke test topology contains repartitions, so it only tests _with_ repartition. I think that it should be sufficient to test only _with_ repartition.\r\n\r\nThe eos test verification specifically checks that \"all transactions finished\" (`org.apache.kafka.streams.tests.EosTestDriver#verifyAllTransactionFinished`). I'm not clear on exactly what we're looking for here. It looks like we create a transactional producer and send a record to each partition and then expect to get all those records back, without seeing any other records. But I'm not sure why we're doing this. If we want to drop the eos test, then we might need to add this to the smoke test verification.\r\n\r\nWDYT?",
        "createdAt" : "2019-03-07T19:38:38Z",
        "updatedAt" : "2019-03-08T19:03:13Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "6d9327d4-8db8-4bdf-b32a-6228c1a1d81a",
        "parentId" : "8d8e4644-cb87-42f3-9341-09f194b057bb",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`verifyAllTransactionFinished` aimed to avoid a situation that some dangling txn is open forever, without committing or aborting. Because the consumer needs to guarantee offset ordering when returning data, with read-committed they will also be blocked on those open txn's data forever (this usually indicates a broker-side issue, not streams though, but still).\r\n\r\nI think we should still retain this check if we want to merge in the EosTest to SmokeTest, but also if you do not want to drag on it we could keep it as a separate ticket and merge this as-is. Your call :)",
        "createdAt" : "2019-03-08T19:16:49Z",
        "updatedAt" : "2019-03-08T19:16:50Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0f8e7997-0a4c-4f04-a976-c9deb7753998",
        "parentId" : "8d8e4644-cb87-42f3-9341-09f194b057bb",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, thanks for the explanation. Let's merge this, and I'll create a ticket to follow up by including this check and removing the eos test.\r\n\r\nThat'll help review burden as well, I think.",
        "createdAt" : "2019-03-08T21:20:16Z",
        "updatedAt" : "2019-03-08T21:20:16Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7420fd04c43eb0f9ac183756a964f27ab8d1d218",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +356,360 @@class StreamsSmokeTestEOSJobRunnerService(StreamsSmokeTestBaseService):\n    def __init__(self, test_context, kafka):\n        super(StreamsSmokeTestEOSJobRunnerService, self).__init__(test_context, kafka, \"process-eos\")\n\n"
  },
  {
    "id" : "65a078ed-0c95-4667-a896-2e5713efb7ec",
    "prId" : 7441,
    "prUrl" : "https://github.com/apache/kafka/pull/7441#pullrequestreview-306281581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "faa030a8-b1c4-4e4a-874e-8a265a0328d3",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Could you clarify the motivation of modifying the system test here? By default StreamsSmokeTest client is configured with 3 threads.",
        "createdAt" : "2019-10-21T21:35:28Z",
        "updatedAt" : "2019-10-24T17:00:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "6b74d0e7-6885-4c78-a9e8-d1949e4ca776",
        "parentId" : "faa030a8-b1c4-4e4a-874e-8a265a0328d3",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Stated in the caller code",
        "createdAt" : "2019-10-24T01:57:28Z",
        "updatedAt" : "2019-10-24T17:00:49Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1bdf5cf22677bebab206c070889d709efcc7aa1e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +304,308 @@    \"\"\"Base class for Streams Smoke Test services providing some common settings and functionality\"\"\"\n\n    def __init__(self, test_context, kafka, command, num_threads = 3):\n        super(StreamsSmokeTestBaseService, self).__init__(test_context,\n                                                          kafka,"
  },
  {
    "id" : "da64fc98-dc03-4bbe-8a34-092675ab3afc",
    "prId" : 7529,
    "prUrl" : "https://github.com/apache/kafka/pull/7529#pullrequestreview-302929434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d46503e5-3f0d-48be-a999-26a70a05bcd0",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "nice 😄 ",
        "createdAt" : "2019-10-16T23:36:26Z",
        "updatedAt" : "2019-10-17T02:39:12Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce2e86b1297c78b52afff364eb8d667dd90b9f95",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +630,634 @@                del properties['upgrade.from']\n            except KeyError:\n                self.logger.info(\"Key 'upgrade.from' not there, better safe than sorry\")\n\n        if self.upgrade_phase is not None:"
  },
  {
    "id" : "ee0f6584-b877-4395-88b3-503095f3761b",
    "prId" : 7529,
    "prUrl" : "https://github.com/apache/kafka/pull/7529#pullrequestreview-302987154",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cba1b248-10ac-4bfc-b5cd-52633e8deb19",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I'm a bit confused here: we did not use the passed in kafka string to set `BOOTSTRAP_SERVERS_CONFIG` in any of the versioned streams instance, is that intentional? ",
        "createdAt" : "2019-10-17T01:01:26Z",
        "updatedAt" : "2019-10-17T02:39:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "71670430-3d23-4986-b29d-905cb0ea7d7e",
        "parentId" : "cba1b248-10ac-4bfc-b5cd-52633e8deb19",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Also why in 2.4 we do not need `kafka` any more?",
        "createdAt" : "2019-10-17T01:04:42Z",
        "updatedAt" : "2019-10-17T02:39:12Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "a9f68e7c-b809-4ed9-b93d-fd2fe31caa98",
        "parentId" : "cba1b248-10ac-4bfc-b5cd-52633e8deb19",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I was following the pattern from the existing upgrade test.  Again maybe I can refactor this test in a follow-on PR?",
        "createdAt" : "2019-10-17T01:45:23Z",
        "updatedAt" : "2019-10-17T02:39:12Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "ea41408c-673c-4194-a3f5-e48ffabfd48b",
        "parentId" : "cba1b248-10ac-4bfc-b5cd-52633e8deb19",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Personally I'd vote for focusing on correctness now and saving any larger scale cleanup/refactoring for a follow-up PR",
        "createdAt" : "2019-10-17T03:35:19Z",
        "updatedAt" : "2019-10-17T03:35:20Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce2e86b1297c78b52afff364eb8d667dd90b9f95",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +596,600 @@                                          str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1),\n                                          str(LATEST_2_0), str(LATEST_2_1), str(LATEST_2_2), str(LATEST_2_3)]:\n            args['kafka'] = self.kafka.bootstrap_servers()\n        else:\n            args['kafka'] = \"\""
  },
  {
    "id" : "a51a803f-3c69-4560-b11b-d093f47568e4",
    "prId" : 8367,
    "prUrl" : "https://github.com/apache/kafka/pull/8367#pullrequestreview-382404529",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bcd1cfd0-aec6-4a51-bdc4-2894cfd68f70",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Switch from \"boolean\" flag to \"string\"",
        "createdAt" : "2020-03-26T20:12:35Z",
        "updatedAt" : "2020-03-27T19:25:36Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "217e4290-ece2-4c96-8a70-37a7f38faa49",
        "parentId" : "bcd1cfd0-aec6-4a51-bdc4-2894cfd68f70",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Good call.",
        "createdAt" : "2020-03-26T20:47:06Z",
        "updatedAt" : "2020-03-27T19:25:36Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef41d1a1c748b3930b4bf6175a015896eaefe962",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +397,401 @@\nclass StreamsBrokerCompatibilityService(StreamsTestBaseService):\n    def __init__(self, test_context, kafka, processingMode):\n        super(StreamsBrokerCompatibilityService, self).__init__(test_context,\n                                                                kafka,"
  },
  {
    "id" : "f266b266-e76c-4f9d-93fd-e29442548eab",
    "prId" : 8541,
    "prUrl" : "https://github.com/apache/kafka/pull/8541#pullrequestreview-400306958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b45a6c7-da0f-4aec-87fd-60a021329426",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I've added this as a general mechanism in a couple of places to pass specific configs into Streams, so we don't have to make new constructors for every different parameterization.",
        "createdAt" : "2020-04-24T22:35:09Z",
        "updatedAt" : "2020-04-28T03:20:48Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +481,485 @@\n    def set_config(self, key, value):\n        self.extra_properties[key] = value\n\n    def set_version(self, kafka_streams_version):"
  },
  {
    "id" : "459d8029-e4ca-4139-9924-99efd34096de",
    "prId" : 8541,
    "prUrl" : "https://github.com/apache/kafka/pull/8541#pullrequestreview-400306958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2563d79f-f5fa-495a-bd1e-413cd36f45fa",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "These will become follow-on tasks to fix each test. Thankfully, there aren't many.",
        "createdAt" : "2020-04-24T22:36:01Z",
        "updatedAt" : "2020-04-28T03:20:48Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +570,574 @@        properties['input.topic'] = self.INPUT_TOPIC\n        # TODO KIP-441: consider rewriting the test for HighAvailabilityTaskAssignor\n        properties['internal.task.assignor.class'] = \"org.apache.kafka.streams.processor.internals.assignment.StickyTaskAssignor\"\n\n        cfg = KafkaConfig(**properties)"
  },
  {
    "id" : "e85d1eee-d73f-44c3-8078-f540c43aaf18",
    "prId" : 8716,
    "prUrl" : "https://github.com/apache/kafka/pull/8716#pullrequestreview-417052308",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1734ab8f-8755-4024-9e03-c8a8b1a63780",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "It was handy to be able to see the used config file while debugging.",
        "createdAt" : "2020-05-22T17:11:19Z",
        "updatedAt" : "2020-05-27T14:44:04Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "94194f2f6cc87a202266fbabfe426f8bc4fb09aa",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +47,51 @@        \"streams_config\": {\n            \"path\": CONFIG_FILE,\n            \"collect_default\": True},\n        \"streams_log\": {\n            \"path\": LOG_FILE,"
  },
  {
    "id" : "7d61f0fb-a1e1-4cf3-b87a-b8e77e8c692d",
    "prId" : 8716,
    "prUrl" : "https://github.com/apache/kafka/pull/8716#pullrequestreview-417052308",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42c3f6c3-fd2e-4cbc-aa4f-7f99e114de19",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Added this configuration to fix the flaky `StreamsOptimizedTest.test_upgrade_optimized_topology`",
        "createdAt" : "2020-05-22T17:11:58Z",
        "updatedAt" : "2020-05-27T14:44:04Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "94194f2f6cc87a202266fbabfe426f8bc4fb09aa",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +471,475 @@        # Long.MAX_VALUE lets us do the assignment without a warmup\n        properties['acceptable.recovery.lag'] = \"9223372036854775807\"\n\n        cfg = KafkaConfig(**properties)\n        return cfg.render()"
  }
]