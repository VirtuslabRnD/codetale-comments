[
  {
    "id" : "9d6dea8f-9eda-4a45-9bf1-7cdc8548f6f7",
    "prId" : 5912,
    "prUrl" : "https://github.com/apache/kafka/pull/5912#pullrequestreview-174911958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43508698-4739-433f-aebd-c8381ec1768c",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "`monitor.wait_for` uses `grep` without the `E` switch so I need to escape the `|` in the pattern",
        "createdAt" : "2018-11-14T15:15:32Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e36e541b8fce3b7fa934ef694c80dcbc9255684",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +33,37 @@    reduce_topic = 'reduceTopic'\n    join_topic = 'joinTopic'\n    operation_pattern = 'AGGREGATED\\|REDUCED\\|JOINED'\n\n    def __init__(self, test_context):"
  },
  {
    "id" : "0934d153-a665-48a0-8b76-ad1a68f33c20",
    "prId" : 5912,
    "prUrl" : "https://github.com/apache/kafka/pull/5912#pullrequestreview-174911958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88834f25-249a-4e7e-935e-48173c4eb898",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Testing for the existence of each term by itself in the `STDOUT` file",
        "createdAt" : "2018-11-14T15:25:44Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e36e541b8fce3b7fa934ef694c80dcbc9255684",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +114,118 @@            if not self.all_source_subtopology_tasks(processor):\n                if verify_individual_operations:\n                    for operation in self.operation_pattern.split('\\|'):\n                        self.do_verify(processor, operation)\n                else:"
  },
  {
    "id" : "2f17929a-b158-49c8-8eff-46af2760bafb",
    "prId" : 5912,
    "prUrl" : "https://github.com/apache/kafka/pull/5912#pullrequestreview-178414692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6f18f1f-8f6d-4825-b8c8-86712a768055",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "In a tiny percentage of test runs one streams instance ends up with all input source tasks, i.e. (`0_0`, `0_1`, `0_2`, `0_3`), so none of the expected operations are processed on that node.  So we check the task assignment and if it's all input source tasks we skip checking this node.\r\n\r\nI added this check after noticing some test flakiness.",
        "createdAt" : "2018-11-14T15:29:48Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "ece000f5-8997-40e0-b12c-7fad6ac3b0fb",
        "parentId" : "c6f18f1f-8f6d-4825-b8c8-86712a768055",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hmm.. I'm wondering why it is still possible, as we should have achieved balance across sub-topologies right? Or are there any potential edge cases you are aware of while working on that PR @bbejeck ?",
        "createdAt" : "2018-11-16T05:23:57Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "366bd478-cad9-4239-8633-5aecc1cb24f2",
        "parentId" : "c6f18f1f-8f6d-4825-b8c8-86712a768055",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I think as long as we favor stickiness over load-balancing this is always a possibility.  One thing to note I only observed one instance getting all tasks from one sub-topology _**after**_ the first phase of the test meaning stickiness is a factor, and it seemed to be a tiny percentage.  I put the check in to eliminate test flakiness.\r\n\r\nI have some additional thoughts on putting back the check we had in making sure that adding a task from the same sub-topology only happens when all clients are over-capacity.  But I'd like to do that in a separate PR and write an independent system test for that.\r\n\r\nWDYT?",
        "createdAt" : "2018-11-16T15:27:53Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "fabe5126-8dc4-42e5-a8c3-5eb5d004abad",
        "parentId" : "c6f18f1f-8f6d-4825-b8c8-86712a768055",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Interesting! I didn't realize we attempt to balance subtopologies over the instances... Is this important for some reason?",
        "createdAt" : "2018-11-26T17:40:39Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e36e541b8fce3b7fa934ef694c80dcbc9255684",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +112,116 @@    def verify_processing(self, processors, verify_individual_operations):\n        for processor in processors:\n            if not self.all_source_subtopology_tasks(processor):\n                if verify_individual_operations:\n                    for operation in self.operation_pattern.split('\\|'):"
  },
  {
    "id" : "43a53201-3ded-4c86-9868-9b619265449e",
    "prId" : 5912,
    "prUrl" : "https://github.com/apache/kafka/pull/5912#pullrequestreview-174911958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a2d3e3c-a81d-4742-8905-daff14669781",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Checking the task assignment for this processor node.",
        "createdAt" : "2018-11-14T15:32:29Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e36e541b8fce3b7fa934ef694c80dcbc9255684",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +128,132 @@                               err_msg=\"Never saw processing of %s \" % pattern + str(processor.node.account))\n\n    def all_source_subtopology_tasks(self, processor):\n        retries = 0\n        while retries < 5:"
  },
  {
    "id" : "37b4eca3-90fc-4b5a-a035-7f7c89204a8e",
    "prId" : 5912,
    "prUrl" : "https://github.com/apache/kafka/pull/5912#pullrequestreview-175989876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb1288d1-f045-4a06-912f-45df96ae0479",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "changed this as `current active tasks` always shows up in the log files, where `Committed active tasks` may or may not be in the log file.",
        "createdAt" : "2018-11-16T21:35:38Z",
        "updatedAt" : "2018-11-27T15:37:30Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e36e541b8fce3b7fa934ef694c80dcbc9255684",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +131,135 @@        retries = 0\n        while retries < 5:\n            found = list(processor.node.account.ssh_capture(\"sed -n 's/.*current active tasks: \\[\\(\\(0_[0-9], \\)\\{3\\}0_[0-9]\\)\\].*/\\1/p' %s\" % processor.LOG_FILE, allow_fail=True))\n            self.logger.info(\"Returned %s from assigned task check\" % found)\n            if len(found) > 0:"
  },
  {
    "id" : "4f0d7a95-ca7b-471b-972d-c67ca9d82138",
    "prId" : 6105,
    "prUrl" : "https://github.com/apache/kafka/pull/6105#pullrequestreview-190452480",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c93e9a8d-3c78-47cc-af53-e4fc9ca27de3",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Need to increase timeout for startup and rebalance to running, noticing some failures due longer times.  I'll investigate this further, but for now increasing the timeout to stop the bleeding.",
        "createdAt" : "2019-01-08T21:33:54Z",
        "updatedAt" : "2019-01-08T21:33:59Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bf426a1e8cc1aa99dff9c3723a8ae2afa715bfa",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +97,101 @@            processor.start()\n            monitor.wait_until('REBALANCING -> RUNNING with REPARTITION TOPIC COUNT=%s' % repartition_topic_count,\n                               timeout_sec=120,\n                               err_msg=\"Never saw 'REBALANCING -> RUNNING with REPARTITION TOPIC COUNT=%s' message \"\n                                       % repartition_topic_count + str(processor.node.account))"
  }
]