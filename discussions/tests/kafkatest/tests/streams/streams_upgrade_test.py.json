[
  {
    "id" : "fece5076-6416-4450-94c8-b00e3a367cb1",
    "prId" : 4689,
    "prUrl" : "https://github.com/apache/kafka/pull/4689#pullrequestreview-103658728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14f6126b-4147-407e-a012-0ff8b075b02b",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I think you can do the zookeeper start in the `setUp` method",
        "createdAt" : "2018-03-13T21:01:08Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "0d56ce4b-66a8-47eb-be34-ab7d0b832749",
        "parentId" : "14f6126b-4147-407e-a012-0ff8b075b02b",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "hm. actually, there isn't a `setUp` method... Are you suggesting I make one?\r\n\r\nFWIW, I think it's actually fine like this. The test is linearly readable this way.",
        "createdAt" : "2018-03-13T23:30:14Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "7e3a761f-6a73-43f4-a536-39ca3a426b99",
        "parentId" : "14f6126b-4147-407e-a012-0ff8b075b02b",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "There's a `setUp` method on the [Test](https://github.com/confluentinc/ducktape/blob/master/ducktape/tests/test.py#L98) class that we can override.  It's fine like it is, but most of the other tests do ZK start-up in the `setUp` method, so I was aiming for consistency.",
        "createdAt" : "2018-03-14T00:05:52Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "8c6c8a52-26d5-4ba8-8964-6b7d97af77d2",
        "parentId" : "14f6126b-4147-407e-a012-0ff8b075b02b",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "aha, I gotcha. sure, I can do that.",
        "createdAt" : "2018-03-14T00:11:18Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "80f4e799-4b0a-47c0-ab00-37a9379cb2fc",
        "parentId" : "14f6126b-4147-407e-a012-0ff8b075b02b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "+1",
        "createdAt" : "2018-03-14T00:17:09Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f32e23ba-fa74-4559-9004-3209d16a12a4",
        "parentId" : "14f6126b-4147-407e-a012-0ff8b075b02b",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ok, I'm glad that I know what you were talking about now. I actually think in this case, it's a little better as is, since we want to start the broker with different versions during the test.\r\n\r\nWe could still do it, but then we'd have to stop the broker and upgrade it and restart it during the test, or only start zk in startUp and start the broker during the tests... I think in this case, it's a more complicated control flow in the tests in exchange for diminishing returns.\r\n\r\nI can totally get it for tests that just need zk and kafka running and don't care about the version, but in this case, I think I like it better flat.",
        "createdAt" : "2018-03-14T00:21:32Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "31719e4426c9591c2fb2f640070c1c0709f2a737",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +93,97 @@            # Setup phase\n            self.zk = ZookeeperService(self.test_context, num_nodes=1)\n            self.zk.start()\n\n            # number of nodes needs to be >= 3 for the smoke test"
  },
  {
    "id" : "2f44116d-5745-4ee4-a947-5403db499124",
    "prId" : 4689,
    "prUrl" : "https://github.com/apache/kafka/pull/4689#pullrequestreview-103611152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8a530fb-7e8d-41b4-9798-d5b1601a75aa",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "same as above",
        "createdAt" : "2018-03-13T21:01:41Z",
        "updatedAt" : "2018-03-15T15:32:40Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "31719e4426c9591c2fb2f640070c1c0709f2a737",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +135,139 @@            # Setup phase\n            self.zk = ZookeeperService(self.test_context, num_nodes=1)\n            self.zk.start()\n\n            # number of nodes needs to be >= 3 for the smoke test"
  },
  {
    "id" : "da168fbc-0671-427e-ba3d-e19dc4840bd9",
    "prId" : 4746,
    "prUrl" : "https://github.com/apache/kafka/pull/4746#pullrequestreview-105944367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf448dd5-38b2-414c-b322-7cb3c98d6180",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is a meta comment: in newer versions we should consider parameterize it instead of writing a new function for each from / to version pair.",
        "createdAt" : "2018-03-21T21:50:26Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "c6685a36-ee88-4c01-a8f0-06865472756f",
        "parentId" : "cf448dd5-38b2-414c-b322-7cb3c98d6180",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Agreed. I'll upgrade the code accordingly when porting the PR to other branches.",
        "createdAt" : "2018-03-21T23:27:57Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe469ed34fb4f0d1094a542e2b2dd7d27cedd2e2",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +77,81 @@        self.driver.stop()\n\n    def start_all_nodes_with_0100(self):\n        # start first with 0.10.0\n        self.prepare_for_0100(self.processor1)"
  },
  {
    "id" : "15e98bbe-e6fb-48ac-bf0c-560439e8fb33",
    "prId" : 4746,
    "prUrl" : "https://github.com/apache/kafka/pull/4746#pullrequestreview-106648066",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1a0444f-323e-4607-84b0-2ec60d55b949",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I might have missed it... did we verify that everything is using the new metadata version after the second bounce?",
        "createdAt" : "2018-03-23T15:56:14Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "129a1412-8172-4546-b415-83faedb8e704",
        "parentId" : "a1a0444f-323e-4607-84b0-2ec60d55b949",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Hmmm... Not really... Also not sure how to check this? Ideas? We could put additional DEBUG logs that print the version of the received `AssignmentInfo`. \\cc @bbejeck @guozhangwang  WDYT?\r\n\r\nWe only make sure that the instances did not crash and process data after second rolling bounce.",
        "createdAt" : "2018-03-23T17:09:21Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "79e8c31d-b7e7-43c1-9d95-6ae9eb82ea2d",
        "parentId" : "a1a0444f-323e-4607-84b0-2ec60d55b949",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think this check should better be covered in unit test or integration test, not system test.",
        "createdAt" : "2018-03-23T18:27:15Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "76c01d8c-b0f0-454c-ad53-243d097a3536",
        "parentId" : "a1a0444f-323e-4607-84b0-2ec60d55b949",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "It's covered in `StreamPartitionAssignorTest` -- seems we are good than.",
        "createdAt" : "2018-03-23T19:54:54Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "07921b5f-37a8-4cb8-98f2-103f7838828f",
        "parentId" : "a1a0444f-323e-4607-84b0-2ec60d55b949",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : ":+1: ",
        "createdAt" : "2018-03-23T21:03:26Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe469ed34fb4f0d1094a542e2b2dd7d27cedd2e2",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +75,79 @@                                   err_msg=\"Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on\" + str(node.account))\n\n        self.driver.stop()\n\n    def start_all_nodes_with_0100(self):"
  },
  {
    "id" : "29e246e2-69f9-49cc-95b5-8f4ebf4cca66",
    "prId" : 4746,
    "prUrl" : "https://github.com/apache/kafka/pull/4746#pullrequestreview-106572993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ab117da-c3e9-45fb-ae81-b794db7a579e",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "\"wait on rebalance\" was not part of the instructions in the doc. Is this a necessary step?\r\n\r\nI suppose it is the distinction between an offline upgrade and a rolling one...\r\n\r\nSupposing that is the intent, maybe a better phrasing would be \"stop processor and ensure the others continue making progress\".",
        "createdAt" : "2018-03-23T16:07:41Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "830fac61-26f8-4a78-ad9d-96ff4ea9de0d",
        "parentId" : "8ab117da-c3e9-45fb-ae81-b794db7a579e",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "For users, it's not required. It's a system test thing, that allows us to \"track\" the progress -- if we bounce instances without waiting, we introduce a race condition in the test because we don't know how many rebalances might actually be triggered: it could be a single rebalance or two, depending how quickly the instance comes back online.",
        "createdAt" : "2018-03-23T17:15:00Z",
        "updatedAt" : "2018-03-27T01:03:28Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe469ed34fb4f0d1094a542e2b2dd7d27cedd2e2",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +148,152 @@        second_other_node = second_other_processor.node\n\n        # stop processor and wait for rebalance of others\n        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:\n            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:"
  },
  {
    "id" : "2f0d03c7-d0b4-4f6d-9d9f-edd67e330b97",
    "prId" : 4758,
    "prUrl" : "https://github.com/apache/kafka/pull/4758#pullrequestreview-106649262",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7568ea78-465d-4de2-801f-8da515ba3299",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "For this upgrade, we do not need two rolling bounces with `upgrade.from` config I think?",
        "createdAt" : "2018-03-23T06:00:37Z",
        "updatedAt" : "2018-03-26T00:23:29Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "ca9ffbcc-8cef-4159-ac5c-a83a6cc90ece",
        "parentId" : "7568ea78-465d-4de2-801f-8da515ba3299",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes. The test does only do one rolling bounce. Not sure what you mean? Is the comment confusing? It does actually no say anything about the number of rolling bounces.",
        "createdAt" : "2018-03-23T07:05:36Z",
        "updatedAt" : "2018-03-26T00:23:29Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "58ab84a9-c6de-4db5-8315-5864d6c50e22",
        "parentId" : "7568ea78-465d-4de2-801f-8da515ba3299",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "My bad, I'd better leave this comment on line 57 below. I meant that we do not need to specify `0.10.0` for the upgrade.from parameter, but instead we should be able to just write\r\n\r\n```\r\nself.do_rolling_bounce(p, \"\", new_version, counter)\r\n```\r\n\r\nIs that right?",
        "createdAt" : "2018-03-23T16:26:34Z",
        "updatedAt" : "2018-03-26T00:23:29Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "fa9c0401-b461-4afd-a079-9250a6856cb6",
        "parentId" : "7568ea78-465d-4de2-801f-8da515ba3299",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Good catch!",
        "createdAt" : "2018-03-23T21:08:11Z",
        "updatedAt" : "2018-03-26T00:23:29Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cb064a5d0891c989ee8a725d115d21dbe82aed1",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +41,45 @@    def test_simple_upgrade(self):\n        \"\"\"\n        Starts 3 KafkaStreams instances with version 0.10.1, and upgrades one-by-one to 0.10.2\n        \"\"\"\n"
  },
  {
    "id" : "f4cacb32-903c-41f7-8202-cc887b0783fb",
    "prId" : 4768,
    "prUrl" : "https://github.com/apache/kafka/pull/4768#pullrequestreview-108683306",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27d9df89-bdf0-4d69-8a3a-a049e1022383",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I did a double-take here... I think I get it now, but to check my understanding...\r\n\r\nThe plan is to wait until those patch releases, and then do something like :\r\n```\r\n@matrix(new_version=simple_upgrade_versions_metadata_version_2)\r\n```\r\n\r\nYou're effectively slicing 0.10.0 out from the upgrade/downgrade test matrix because:\r\n* upgrading from 0.10.0 requires a different algorithm (double bounce)\r\n* downgrading to 0.10.0 is not supported\r\n\r\nRight?",
        "createdAt" : "2018-04-02T14:40:43Z",
        "updatedAt" : "2018-04-02T14:42:09Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "b1ef78c6-cac9-4a60-85b1-3607aa4a5185",
        "parentId" : "27d9df89-bdf0-4d69-8a3a-a049e1022383",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes. Upgrading from 0.10.0 requires two rolling bounces including the new config `upgrade.from` and thus we have this extra test for it -- this new config is only available in un-release code (for older non-dev branches) and thus, we cannot run this test for those versions atm.\r\n\r\nWe chould add a downgrade test though -- but it would be a new test method, too, as it also requires two rolling bounces but with different order of command compare to this test. It would be something like:\r\n - prepare all instances to bounce and set config `upgrade.from=\"0.10.0\"`\r\n - do first round of rolling bounce -- in this rolling bounce you stay on current version and don't downgrade yet, but only prepare all instances for downgrading (via setting the config)\r\n - do a second round of rolling bounces downgrading to 0.10.0\r\n\r\nI think it's not worth to add a downgrade test.",
        "createdAt" : "2018-04-02T17:11:04Z",
        "updatedAt" : "2018-04-02T17:11:04Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "d3442cbd-86a3-40e0-9d48-361888ec6b32",
        "parentId" : "27d9df89-bdf0-4d69-8a3a-a049e1022383",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Agreed. We have no evidence that downgrade is necessary. We don't need to document it or test it unless or until we actually choose to support it.",
        "createdAt" : "2018-04-02T17:45:56Z",
        "updatedAt" : "2018-04-02T17:45:56Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d3e925e7bdce704a3cedfb5f130ec114cfc441",
    "line" : 175,
    "diffHunk" : "@@ -1,1 +169,173 @@    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released\n    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released\n    @parametrize(new_version=str(DEV_VERSION))\n    def test_metadata_upgrade(self, new_version):\n        \"\"\""
  },
  {
    "id" : "1bee2baf-1ce8-4712-9c70-9ccb7351595e",
    "prId" : 5754,
    "prUrl" : "https://github.com/apache/kafka/pull/5754#pullrequestreview-162525435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "498b779a-22b1-4ded-a418-0d4bb0c72687",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "good catch",
        "createdAt" : "2018-10-08T15:28:50Z",
        "updatedAt" : "2018-10-11T01:07:47Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "bac04aacb48cf42715b95b8f64e150b607e3dd4d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +123,127 @@\n        node = self.driver.node\n        node.account.ssh(\"grep -E 'ALL-RECORDS-DELIVERED|PROCESSED-MORE-THAN-GENERATED' %s\" % self.driver.STDOUT_FILE, allow_fail=False)\n        self.processor1.node.account.ssh_capture(\"grep SMOKE-TEST-CLIENT-CLOSED %s\" % self.processor1.STDOUT_FILE, allow_fail=False)\n"
  },
  {
    "id" : "3635de29-3a25-4ea5-8651-00cd73b29d8c",
    "prId" : 5754,
    "prUrl" : "https://github.com/apache/kafka/pull/5754#pullrequestreview-162538049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "302b3c61-b228-4da3-8c3f-70f88041c348",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "same here have a `retrieve_generation_num` method for extracting the generation number from captured output",
        "createdAt" : "2018-10-08T16:00:14Z",
        "updatedAt" : "2018-10-11T01:07:47Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "bac04aacb48cf42715b95b8f64e150b607e3dd4d",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +539,543 @@                            first_other_processor_generation = self.extract_highest_generation(first_other_processor_found)\n                            second_other_processor_generation = self.extract_highest_generation(second_other_processor_found)\n\n                            if processor_generation == first_other_processor_generation and processor_generation == second_other_processor_generation:\n                                current_generation = processor_generation"
  },
  {
    "id" : "dbd546d9-4462-4e81-8915-f97d2fdb2666",
    "prId" : 6046,
    "prUrl" : "https://github.com/apache/kafka/pull/6046#pullrequestreview-189659467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a6400d8-0300-4890-bedc-8b79470901d1",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Unrelated to this PR. Nit: update comment in L41/43 above: `Metadata version was bumped in 0.10.1.0` \r\n\r\nIt was also dumped in 2.0 release (that also added version probing)",
        "createdAt" : "2018-12-18T10:44:56Z",
        "updatedAt" : "2019-01-07T02:54:38Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "59ffee6e-5160-47a7-ac24-44f77faf9b07",
        "parentId" : "8a6400d8-0300-4890-bedc-8b79470901d1",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "I don't follow, I should add `metadata version dumped in 2.0 release` then?",
        "createdAt" : "2018-12-21T20:20:19Z",
        "updatedAt" : "2019-01-07T02:54:38Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "03456ee6-a933-4395-8541-cc72f8adfc51",
        "parentId" : "8a6400d8-0300-4890-bedc-8b79470901d1",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Something like this. Just for completeness (or maybe remove the comment, because we have corresponding variable anyway?)",
        "createdAt" : "2018-12-21T20:44:13Z",
        "updatedAt" : "2019-01-07T02:54:38Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "e449c00c-2faf-4ef1-9cb2-c564d1e2f707",
        "parentId" : "8a6400d8-0300-4890-bedc-8b79470901d1",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "ack",
        "createdAt" : "2018-12-21T21:09:47Z",
        "updatedAt" : "2019-01-07T02:54:38Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "2ee1c42b-242c-4805-be0f-b3043d434de3",
        "parentId" : "8a6400d8-0300-4890-bedc-8b79470901d1",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "`dropped` -> `bumped` ?",
        "createdAt" : "2019-01-07T01:55:32Z",
        "updatedAt" : "2019-01-07T02:54:38Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0583e1e6-9eee-4e3f-b56b-3e0e5751cd87",
        "parentId" : "8a6400d8-0300-4890-bedc-8b79470901d1",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "ack \r\nEDIT:\r\nactually from @mjsax comment above (https://github.com/apache/kafka/pull/6046/files#r242487208) I thought we removed the metadata version and went with version probing instead?\r\n\r\nOtherwise, I can make the change from `dropped` to `bumped`",
        "createdAt" : "2019-01-07T02:05:30Z",
        "updatedAt" : "2019-01-07T02:54:38Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfa263f48d71bb36037debb539528549592343cf",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +51,55 @@            'data' : { 'partitions': 5 },\n        }\n        self.leader = None\n        self.leader_counter = {}\n"
  },
  {
    "id" : "9de9daa7-6a2e-4a8f-9b74-80cbcbe7006b",
    "prId" : 6046,
    "prUrl" : "https://github.com/apache/kafka/pull/6046#pullrequestreview-186005106",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "210ce994-5455-4a1e-a159-5e0599db6d6f",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "unrelated nit: fix indentions above",
        "createdAt" : "2018-12-18T10:47:43Z",
        "updatedAt" : "2019-01-07T02:54:38Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfa263f48d71bb36037debb539528549592343cf",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +102,106 @@        # Setup phase\n        self.zk = ZookeeperService(self.test_context, num_nodes=1)\n        self.zk.start()\n\n        # number of nodes needs to be >= 3 for the smoke test"
  },
  {
    "id" : "384d1977-fc61-4bf7-a7c9-4df0ec8265ab",
    "prId" : 6271,
    "prUrl" : "https://github.com/apache/kafka/pull/6271#pullrequestreview-204501903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "264e82bc-1f22-4375-8dc7-b5335769174b",
        "parentId" : null,
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : "Thinking about this I could add the returned topics from a generator to a set and make an `expected == actual` comparison but I also think this check is sufficient and I don't have to sort the sets before comparison. Also by creating a set of topics found on the broker, I'd have to remove any internal topics as well, further complicating the logic (admittedly not by much).",
        "createdAt" : "2019-02-15T00:27:13Z",
        "updatedAt" : "2019-02-15T00:27:18Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      },
      {
        "id" : "1dc6fdee-647c-450a-8991-c73dc03a6e01",
        "parentId" : "264e82bc-1f22-4375-8dc7-b5335769174b",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either.\r\n\r\nI guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.",
        "createdAt" : "2019-02-16T02:56:24Z",
        "updatedAt" : "2019-02-16T02:56:38Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a6e3ac80720b579b2e4cb086d373416e81d9bf0",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +644,648 @@            topic_list_generator = self.kafka.list_topics(\"placeholder\", node)\n            for topic in topic_list_generator:\n                if topic in expected_topic_set:\n                    match_count += 1\n"
  },
  {
    "id" : "36d0e537-9e1f-4d83-8463-9b8c35921504",
    "prId" : 6531,
    "prUrl" : "https://github.com/apache/kafka/pull/6531#pullrequestreview-221892981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7be104c0-4ca2-4f8c-99f7-b0e7655a2fa6",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "These are to cover cases where not exactly 100 records are processed.",
        "createdAt" : "2019-04-02T20:45:25Z",
        "updatedAt" : "2019-04-02T20:45:25Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "519967ca8faee6e769eb449ff9e124721ba97167",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +383,387 @@                                       timeout_sec=60,\n                                       err_msg=\"Could not detect Kafka Streams version \" + version + \" \" + str(node1.account))\n                monitor.wait_until(self.processed_msg,\n                                   timeout_sec=60,\n                                   err_msg=\"Never saw output '%s' on \" % self.processed_msg + str(node1.account))"
  },
  {
    "id" : "e1613419-2ee4-49b2-9764-2e8299486ac4",
    "prId" : 6763,
    "prUrl" : "https://github.com/apache/kafka/pull/6763#pullrequestreview-239255275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f64d3511-4290-4bc7-8d80-9e46d5f91500",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is the actual fix. Everything else is just cleanup to address Intellij warnings.",
        "createdAt" : "2019-05-19T21:06:57Z",
        "updatedAt" : "2019-05-19T21:06:57Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "81d273b61ffe452ce6eb7cea768fe343ad5b4a4d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +349,353 @@            for p in self.processors:\n                found = list(p.node.account.ssh_capture(\"grep \\\"Finished assignment for group\\\" %s\" % p.LOG_FILE, allow_fail=True))\n                if len(found) >= self.leader_counter[p] + 1:\n                    if self.leader is not None:\n                        raise Exception(\"Could not uniquely identify leader\")"
  },
  {
    "id" : "1d943e0a-21fa-4389-abec-849ee05fec29",
    "prId" : 6764,
    "prUrl" : "https://github.com/apache/kafka/pull/6764#pullrequestreview-240273745",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5c88f5e-edb8-4451-8bb6-7a1889a9f2bf",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "This is one fix. If rebalance happens quickly, we might see this message for the second rebalance early. Cf. #6763 ",
        "createdAt" : "2019-05-19T21:08:05Z",
        "updatedAt" : "2019-05-23T22:53:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "3461a502-9e40-4a3e-a55a-b05976d9f8db",
        "parentId" : "d5c88f5e-edb8-4451-8bb6-7a1889a9f2bf",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "After https://github.com/apache/kafka/pull/6779 is merged, I can rebase this PR -- we still want this fix I guess :)",
        "createdAt" : "2019-05-21T20:11:57Z",
        "updatedAt" : "2019-05-23T22:53:46Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0080b24694f56500138c3511791631fc32dafd6e",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +350,354 @@            for p in self.processors:\n                found = list(p.node.account.ssh_capture(\"grep \\\"Finished assignment for group\\\" %s\" % p.LOG_FILE, allow_fail=True))\n                if len(found) >= self.leader_counter[p] + 1:\n                    if self.leader is not None:\n                        raise Exception(\"Could not uniquely identify leader\")"
  },
  {
    "id" : "f9f415b4-6ff1-4aa9-90f4-7f6aadc8b7a8",
    "prId" : 6835,
    "prUrl" : "https://github.com/apache/kafka/pull/6835#pullrequestreview-243338390",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30b2fd3f-d8bd-49b9-ada1-fa302231b97a",
        "parentId" : null,
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "Shouldn't `str(DEV_VERSION)` in the error message also be changed?",
        "createdAt" : "2019-05-29T13:44:52Z",
        "updatedAt" : "2019-05-29T15:21:40Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "56c2f510-69dc-407f-8469-dfa874438ce9",
        "parentId" : "30b2fd3f-d8bd-49b9-ada1-fa302231b97a",
        "authorId" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "body" : " I wanted to leave the `str(DEV_VERSION)` as is so it's more clear in the logs if an error occurs",
        "createdAt" : "2019-05-29T15:25:00Z",
        "updatedAt" : "2019-05-29T15:25:01Z",
        "lastEditedBy" : "4c968502-bb3d-46ee-8719-0c0bdbc6242f",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b2a0aeacae04270d784958466e6f9284422d48",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +543,547 @@                    log_monitor.wait_until(\"Kafka version.*\" + self.base_version_number + \".*SNAPSHOT\",\n                                           timeout_sec=60,\n                                           err_msg=\"Could not detect Kafka Streams version \" + str(DEV_VERSION) + \" in \" + str(node.account))\n                    log_monitor.offset = 5\n                    log_monitor.wait_until(\"partition\\.assignment\\.strategy = \\[org\\.apache\\.kafka\\.streams\\.tests\\.StreamsUpgradeTest$FutureStreamsPartitionAssignor\\]\","
  },
  {
    "id" : "158428b1-6389-4646-aaf7-32ffe6216286",
    "prId" : 7423,
    "prUrl" : "https://github.com/apache/kafka/pull/7423#pullrequestreview-295322674",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0666a72c-4f5f-4e0c-bc7c-05060f6b6ff6",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "This error message didn't make sense, it should be referring to upgrading the metadata to the newer version",
        "createdAt" : "2019-10-01T00:51:27Z",
        "updatedAt" : "2019-10-02T02:12:15Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "55080af3c9ce8155147fe4f561ed059f71f6c71d",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +574,578 @@                        second_other_monitor.wait_until(\"Sent a version 5 subscription and group.s latest commonly supported version is 6 (successful version probing and end of rolling upgrade). Upgrading subscription metadata version to 6 for next rebalance.\",\n                                                        timeout_sec=60,\n                                                        err_msg=\"Never saw output 'Upgrade metadata to version 6' on\" + str(second_other_node.account))\n\n                    log_monitor.wait_until(\"Version probing detected. Triggering new rebalance.\","
  },
  {
    "id" : "62c47a0a-43fb-4829-af94-a097c123e25d",
    "prId" : 7841,
    "prUrl" : "https://github.com/apache/kafka/pull/7841#pullrequestreview-334898582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a40f2e29-d5a1-403f-be14-283744ab5707",
        "parentId" : null,
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "Hm. We actually changed the metadata version in 2.4, we should have moved `DEV_VERSION` to a new metadata group at that time. I'm not that familiar with the metadata upgrade test, maybe someone else can chime in on what else (if anything) needs to be updated so that we're actually testing the metadata upgrade here?\r\n...I do notice the referenced `start_cmd` function no longer seems to take any versions, so we should update the comment above if anyone knows what's going on here @mjsax @guozhangwang @bbejeck @vvcephei ",
        "createdAt" : "2019-12-17T22:00:19Z",
        "updatedAt" : "2019-12-18T12:41:32Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "469a4886-197c-44fe-8ec3-f108fa9f5cb9",
        "parentId" : "a40f2e29-d5a1-403f-be14-283744ab5707",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "As the variable name says, it's all version with version 3 or higher -- in version 3 we introduced version probing and thus to upgrade, no \"upgrade_from\" config must be specified (from a metadata version point of view), ie, the metadata version itself can be bumped without an issue.\r\n\r\nHowever, given `2.4` release introduces eager/incremental upgrade, I am not sure if we can simply upgrade from`2.3` to `2.4` without setting the config -- but for a different reason.\r\n\r\nMaybe we need to change how we structure the test?",
        "createdAt" : "2019-12-17T23:43:02Z",
        "updatedAt" : "2019-12-18T12:41:32Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "944c0121-22e7-4462-9be1-288fb36b5b3a",
        "parentId" : "a40f2e29-d5a1-403f-be14-283744ab5707",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I see...I wonder if we should rethink the general approach to upgrade system tests. We have all these different upgrade paths:\r\na) pre-VP: double rolling bounce, need to pin metadata to lower version\r\nb) version probing: single rolling bounce, but need to wait for group to stabilize\r\nc) cooperative upgrade: double rolling bounce (first of which will technically be VP)\r\n\r\n Obviously it only makes sense to follow/test one path for a given (to, from) pair. Instead of one system test per upgrade path that tests all version combinations, we should have one test that goes through all (to, from) versions and chooses the correct upgrade path for that pair, then just calls it on that pair. WDYT?\r\n\r\nNote that the current VP system test actually tests forward compatibility, and not any existing upgrade path. That's valuable and we should keep that as a separate test, but it does mean we have a gap in our current system tests w.r.t an actual VP upgrade between existing versions",
        "createdAt" : "2019-12-18T00:55:13Z",
        "updatedAt" : "2019-12-18T12:41:32Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "68867e91-9ce9-44a8-8477-834ef1f4c283",
        "parentId" : "a40f2e29-d5a1-403f-be14-283744ab5707",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "> Instead of one system test per upgrade path that tests all version combinations, we should have one test that goes through all (to, from) versions and chooses the correct upgrade path for that pair, then just calls it on that pair.\r\n\r\nNot sure if this would simplify the code? Having \"groups of versions\" and have a different test for each \"upgrade path\" (from all versions of one group to all version of the other group) seems to make it clear which \"upgrade path\" we are testing?\r\n\r\n> Note that the current VP system test actually tests forward compatibility, and not any existing upgrade path.\r\n\r\nWell, that seems to be fine, as the upgrade path is just a single rolling bounce. But see where you are coming from.\r\n\r\n> but it does mean we have a gap in our current system tests w.r.t an actual VP upgrade between existing versions\r\n\r\nWhy that? If the versions are compatible, there is nothing to be tested but compatibility? Or maybe I miss your point?\r\n\r\nLet see what @cadonna thinks. In any case, I don't care too much how we structure the test as long as we don't miss a case :)",
        "createdAt" : "2019-12-18T01:26:00Z",
        "updatedAt" : "2019-12-18T12:41:32Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "eb821c5c-9ecb-4968-a16c-7ea8f8710315",
        "parentId" : "a40f2e29-d5a1-403f-be14-283744ab5707",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "> Instead of one system test per upgrade path that tests all version combinations, we should have one test that goes through all (to, from) versions and chooses the correct upgrade path for that pair, then just calls it on that pair.\r\n\r\nI agree with @mjsax here.\r\n\r\n> Note that the current VP system test actually tests forward compatibility, and not any existing upgrade path. That's valuable and we should keep that as a separate test, but it does mean we have a gap in our current system tests w.r.t an actual VP upgrade between existing versions\r\n\r\nI am not sure wether I understand all details of `test_version_probing_upgrade`. AFAIU, the test just verifies the version numbers in the subscription and assignment info as well as whether the group stabilizes. That is not a full-fledged real-world upgrade test, but the test rather verifies a fundamental aspect of upgrades with an increased version of metadata. Testing the behavior of the clients according to the changed metadata is done in other system tests like `streams_static_membership_test` and `streams_cooperative_rebalance_upgrade_test`. Please correct me if I am wrong or missed something. I am wondering why we do not test downgrades. Do we not support them?\r\n\r\nAnyways, I would not restructure anything in this PR, because this PR should actually only add the 2.4 version to system tests. Since it should also document the update of the system tests for the next release, I would like to keep it as simple as possible.  \r\n",
        "createdAt" : "2019-12-18T12:31:05Z",
        "updatedAt" : "2019-12-18T21:11:54Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "ecc84099-d9b6-45ac-a8a0-8f5c59749df3",
        "parentId" : "a40f2e29-d5a1-403f-be14-283744ab5707",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "I agree that we should keep this simple, and _if_ we want to refactor something do a follow up PR. Also agree, that the purpose of version probing test is, to ensure that we can dump the metadata version without crashing, and it's not a full upgrade test.\r\n\r\n> I am wondering why we do not test downgrades. Do we not support them?\r\n\r\nDowngrades should work (for some upgrades; not all upgrades are backward compatible). We could add them if we think it's worth the effort. So far, I am not aware of any questions or reported issue and thus I am not sure if it's worth to spent time?",
        "createdAt" : "2019-12-19T16:53:56Z",
        "updatedAt" : "2019-12-19T16:53:56Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "294edce4-e6f1-4a93-8872-56a671d34e19",
        "parentId" : "a40f2e29-d5a1-403f-be14-283744ab5707",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I definitely wasn't saying we should block this PR on the issue, refactoring the upgrade tests is way way out of scope here. \r\n> I don't care too much how we structure the test as long as we don't miss a case :)\r\n\r\nI agree completely, but we _have_ missed things: rolling upgrades across a metadata version bump were broken for several versions (this is the \"gap\" I mentioned -- we test forward compatibility, not backwards, eg we didn't test the 2.0 -> 2.1 upgrade path in 2.1, we just tested 2.1 -> ?).\r\n\r\nWe _need_ to test a realistic upgrade scenario; if the existing tests serve other purposes, then maybe we need a new test rather than a refactoring. That said, \"test whether we can bump the metadata version\" sounds like the job of a unit/integration test, not a system test: the system tests should test an actual upgrade.\r\n\r\nI can open a ticket to discuss this further, but I feel pretty strongly that we should reexamine our upgrade-related system tests (I also have strong feelings about what that would look like, but I'm happy to debate that on the ticket). This PR LGTM though, thanks Bruno!",
        "createdAt" : "2019-12-19T20:01:44Z",
        "updatedAt" : "2019-12-19T20:02:57Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      }
    ],
    "commit" : "df644b1eb2111528e8f27fb5ac5f32815fcb5f22",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +37,41 @@# can be replaced with metadata_2_versions\nbackward_compatible_metadata_2_versions = [str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1)]\nmetadata_3_or_higher_versions = [str(LATEST_2_0), str(LATEST_2_1), str(LATEST_2_2), str(LATEST_2_3), str(LATEST_2_4), str(DEV_VERSION)]\n\n\"\"\""
  },
  {
    "id" : "0e5aa893-5dc3-4ff9-8bb6-2857a797af4f",
    "prId" : 8613,
    "prUrl" : "https://github.com/apache/kafka/pull/8613#pullrequestreview-406033528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7503fad-5012-4372-9764-19a9b0d686aa",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "I know that this check was here in some fashion before, but I'm drawing a blank on why we need to verify this log line. It seems like _just_ checking the version number logs and nothing else would be the key to a long and happy life.",
        "createdAt" : "2020-05-04T20:05:59Z",
        "updatedAt" : "2020-05-04T20:11:31Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "b6d1cb10-e781-4346-9e4c-1a6f706c2390",
        "parentId" : "d7503fad-5012-4372-9764-19a9b0d686aa",
        "authorId" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "body" : "I think the idea is to verify that the actual version probing rebalance takes place, ie that the partition assignor actually handles the version probing once it's detected. And that it signals to the stream thread which also handles it correctly in turn. But idk -- I've probably broken and fixed the version probing test 2 or 3 times now due to this one line in particular.\r\n\r\nSo, I'd be happy to see it go. I probably have too much bad history to make an unbiased call here though ðŸ˜„ ",
        "createdAt" : "2020-05-05T02:11:04Z",
        "updatedAt" : "2020-05-05T02:14:07Z",
        "lastEditedBy" : "d97f50bf-60f9-45b3-81a0-a24a5f42f740",
        "tags" : [
        ]
      },
      {
        "id" : "0cf0ea75-4c05-48c7-96f1-e631c5d7ae73",
        "parentId" : "d7503fad-5012-4372-9764-19a9b0d686aa",
        "authorId" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "body" : "We can leave it because it verifies whether the assignment was triggered in the assignor, which is better than nothing. However, it does not give us any guarantee that the rebalance took actually place. \r\n\r\nI guess what we really would need is a way to check if a group stabilized and if the assignment is valid. We try to do that by verifying that the generations of the processors are synced. However, I ran into cases where all processors had the same generation, but one processor did not have any tasks assigned. So we would actually need to check if they have the highest generation in sync across the processors AND if all processors have at least one task assigned (AND if all tasks were assigned).   ",
        "createdAt" : "2020-05-05T11:56:36Z",
        "updatedAt" : "2020-05-05T13:27:28Z",
        "lastEditedBy" : "b7cbfdaf-f3e2-4130-8254-501ace9562ac",
        "tags" : [
        ]
      },
      {
        "id" : "90ee2ba2-5f19-4543-88b9-3406b7ef3571",
        "parentId" : "d7503fad-5012-4372-9764-19a9b0d686aa",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Thanks, all. This doesn't seem like the best way to verify what we're trying to verify, but it also seems about the same as before. I'm happy to leave this here for now.\r\n\r\nIf/when the test breaks again, I'd prefer for us to put in a more reliable and direct mechanism.",
        "createdAt" : "2020-05-05T18:07:31Z",
        "updatedAt" : "2020-05-05T18:07:50Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      }
    ],
    "commit" : "b80ef741379b3044715738f16648bc0b35370fed",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +529,533 @@                                               timeout_sec=60,\n                                               err_msg=\"Could not detect 'successful version probing' at upgrading node \" + str(node.account))\n                        log_monitor.wait_until(\"Detected that the assignor requested a rebalance. Rejoining the consumer group to trigger a new rebalance.\",\n                                               timeout_sec=60,\n                                               err_msg=\"Could not detect 'Triggering new rebalance' at upgrading node \" + str(node.account))"
  },
  {
    "id" : "11da2a61-9a09-40fa-a541-c4160fc2b40c",
    "prId" : 8971,
    "prUrl" : "https://github.com/apache/kafka/pull/8971#pullrequestreview-443418710",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d54ec543-2784-45e1-814a-7cf2b74cebb5",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I was originally thinking that `streams_application_upgrade_test.py` and `streams_upgrade_test.py` can be re-consolidated so that the latter would be for broker-clients compatibility (including rolling upgrade the brokers), and would for now be ignored. While the former would be used for upgrading / downgrading clients for different paths (one rolling bounce including version probing, two rolling bounces, full shutdown and restart). In that case, this test and the test_version_probing_upgrade below could be moved into the other file. WDYT?",
        "createdAt" : "2020-07-03T19:11:25Z",
        "updatedAt" : "2020-07-03T19:13:13Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "49d5b96c-035b-45b3-8520-f13130e81e5c",
        "parentId" : "d54ec543-2784-45e1-814a-7cf2b74cebb5",
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Ah, I see.\r\n\r\nSince you mention the client-broker test, I was going to pose this question to you: The test is currently ignored, as has been ignored for as long as I can remember. It seems like we should fix it or delete it. I'm guessing your preference would be to fix it, right?\r\n\r\nTo the point about the client-client metadata upgrade test, I left it and the version probing test here because they are specifically testing that the relevant metadata versions get upgraded, whereas the test I moved was instead just about making sure that the app keeps running after an upgrade.\r\n\r\nIt does seem like we could improve the metadata and broker upgrade tests by making them run the full soak app instead, but I'm wondering if we could do it separately. This PR in particular seems like a good addition to add asap, since it should dramatically reduce the number of tests we have to run during system tests.",
        "createdAt" : "2020-07-06T17:15:57Z",
        "updatedAt" : "2020-07-06T17:15:57Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "9b828a0e-783c-4b13-a1a6-de2c64e7d492",
        "parentId" : "d54ec543-2784-45e1-814a-7cf2b74cebb5",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yes I agree. That makes sense. My thoughts are just aligned with our previous discussions about merging various tests, especially even considering merging the StreamsSmokeTest with StreamsUpgradeTest but that is out of the scope of this one.",
        "createdAt" : "2020-07-06T21:27:32Z",
        "updatedAt" : "2020-07-06T21:28:41Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc267912abe5fe304e9d5e00e1964dc31fdeb244",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +187,191 @@\n    @matrix(from_version=metadata_1_versions, to_version=[str(DEV_VERSION)])\n    @matrix(from_version=metadata_2_versions, to_version=[str(DEV_VERSION)])\n    def test_metadata_upgrade(self, from_version, to_version):\n        \"\"\""
  },
  {
    "id" : "28b610f9-b71b-4b77-9d85-fa96cc0c1fdb",
    "prId" : 10602,
    "prUrl" : "https://github.com/apache/kafka/pull/10602#pullrequestreview-701827568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9d6349c-930d-48fd-a77c-bc52bc57b74b",
        "parentId" : null,
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "I noticed we are updating the versions for one test (test_upgrade_downgrade_brokers), but not for the other one (test_metadata_upgrade)\r\n\r\nThe metadata_1_versions and metadata_2_versions are the versions used in this test. Should we keep updating those as well? I'm not super familiar with the streams tests here, only noticed this when running the system test files changed in this PR.\r\n\r\nThe tests that were not ignored passed btw :)\r\n\r\n",
        "createdAt" : "2021-05-04T21:20:20Z",
        "updatedAt" : "2021-05-04T21:20:20Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      },
      {
        "id" : "4885325f-7ea8-49e1-89a6-aea0e650422f",
        "parentId" : "e9d6349c-930d-48fd-a77c-bc52bc57b74b",
        "authorId" : "85578594-6b0b-4724-b709-a8c84f206391",
        "body" : "Added v2.0 and above to the metadata_upgrade test. As per [KIP-268](https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade), one rolling restart is enough for apps upgrading from 2.x to the latest version. I kept the test behavior unchanged, we can fix this later if required in a follow-up PR. Yeah, the upgrade_downgrade_brokers test was already disabled, so adding versions to this test won't help. \r\n\r\nSorry for late reply.",
        "createdAt" : "2021-07-08T09:13:47Z",
        "updatedAt" : "2021-07-08T09:13:47Z",
        "lastEditedBy" : "85578594-6b0b-4724-b709-a8c84f206391",
        "tags" : [
        ]
      }
    ],
    "commit" : "4872befc96b91815de44aa805652d6e3fac4c1f3",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +31,35 @@broker_upgrade_versions = [str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1), \\\n                           str(LATEST_2_0), str(LATEST_2_1), str(LATEST_2_2), str(LATEST_2_3), \\\n                           str(LATEST_2_4), str(LATEST_2_5), str(LATEST_2_6), str(LATEST_2_7), str(LATEST_2_8), str(DEV_BRANCH)]\n\nmetadata_1_versions = [str(LATEST_0_10_0)]"
  }
]