[
  {
    "id" : "4b264ba4-05c2-456c-b6f0-bff1f6f99559",
    "prId" : 11053,
    "prUrl" : "https://github.com/apache/kafka/pull/11053#pullrequestreview-707878265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b5526bf-7ab2-4659-abb3-dc6401c4da54",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "It is not clear to me that this is enough to verify that the controller loaded a snapshot and caught to the leader.",
        "createdAt" : "2021-07-16T19:43:12Z",
        "updatedAt" : "2021-07-16T19:43:25Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "6a91cec2-3f79-4f53-93d3-577e8d3f7042",
        "parentId" : "1b5526bf-7ab2-4659-abb3-dc6401c4da54",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "I agree that this is not a good check. I was trying to look for verification that other tests do, but it seems that maybe topic creation and production/consumption is the best verification. I will do that here as well.",
        "createdAt" : "2021-07-16T20:02:33Z",
        "updatedAt" : "2021-07-23T18:33:43Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      },
      {
        "id" : "43523f6a-dee9-4ef1-a165-8d061d8e3809",
        "parentId" : "1b5526bf-7ab2-4659-abb3-dc6401c4da54",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I agree with @jsancio . What about if we bring down all the controller nodes and then bring them back? They'd have to load snapshot in that case, right, or else the quorum would not work?",
        "createdAt" : "2021-07-21T17:53:38Z",
        "updatedAt" : "2021-07-21T17:53:38Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "9bf07553-f0e0-4e1c-8e4c-9cb22864e592",
        "parentId" : "1b5526bf-7ab2-4659-abb3-dc6401c4da54",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "Bringing down any one controller, cleaning up it's kafka dir and then rebooting is already ensuring that the snapshot is being loaded on recovery. I think Jose's comment was on the previous version of the the test, where we were not doing a produce/consume check. In this version of the PR we are creating a topic and verifying we can produce to it and consume from it. The rationale is that to be able to create topic and produce to it, the quorum needs to agree on the metadata up until this point.\r\n\r\nI take your point that maybe when testing the controller, it would be a better check to bring all controllers down, restart them and then do a produce/consume check as that would ensure that a quorum of nodes was able to load from their snapshots and continue operation. Let me make that change. Will update the PR.",
        "createdAt" : "2021-07-21T18:09:49Z",
        "updatedAt" : "2021-07-23T18:33:44Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      },
      {
        "id" : "374c1d08-7980-48d9-a629-90892460ea0e",
        "parentId" : "1b5526bf-7ab2-4659-abb3-dc6401c4da54",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "Updated to bounce all controllers in the latest revision.",
        "createdAt" : "2021-07-21T22:17:52Z",
        "updatedAt" : "2021-07-23T18:33:44Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      },
      {
        "id" : "b2050767-44c7-4b45-a0f9-60ce5c9615ed",
        "parentId" : "1b5526bf-7ab2-4659-abb3-dc6401c4da54",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "> What about if we bring down all the controller nodes and then bring them back?\r\n\r\nYes. Here are the steps in a bit more details:\r\n1. Check that all of the controllers deleted their log segment starting at base offset 0.\r\n2. For each controller one at a time:\r\n  a. shutdown the controller\r\n  b. delete all of the local data\r\n  c. start the controller\r\n  d. wait until it joins the quorum. I am not sure we have the tools to do this today. Maybe for now we need to sleep and file a Jira to build this tooling.\r\n3. Create a new topic, produce to it, consume from it. This shows that the quorum is available and the brokers are fetching from it.",
        "createdAt" : "2021-07-23T17:26:58Z",
        "updatedAt" : "2021-07-23T17:27:14Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "0c153419-2b95-4f9a-a05d-a9acaeffc93f",
        "parentId" : "1b5526bf-7ab2-4659-abb3-dc6401c4da54",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "Agree with you @jsancio . The latest commit for the PR should already be doing this.",
        "createdAt" : "2021-07-23T17:58:35Z",
        "updatedAt" : "2021-07-23T18:33:44Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b86531bbdf97de3579f62917b808f990496e6f0",
    "line" : 251,
    "diffHunk" : "@@ -1,1 +249,253 @@\n        # Produce to a newly created topic and make sure it works.\n        self.validate_success()"
  },
  {
    "id" : "cb4381bb-7c82-4ed3-8976-5d1e1c41b6e9",
    "prId" : 11053,
    "prUrl" : "https://github.com/apache/kafka/pull/11053#pullrequestreview-707878265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36c812d1-545d-474e-8b2b-a77d93511666",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Should this be a separate test function? Putting it here seems like it might muddy the waters, especially if the nodes you hit are combined controller/brokers.",
        "createdAt" : "2021-07-21T17:51:07Z",
        "updatedAt" : "2021-07-21T17:51:07Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "0e78fe93-1e04-4e32-b8d8-a1dca18c42df",
        "parentId" : "36c812d1-545d-474e-8b2b-a77d93511666",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "I am not sure I got your point. Will sync offline about this.",
        "createdAt" : "2021-07-21T18:10:21Z",
        "updatedAt" : "2021-07-23T18:33:44Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      },
      {
        "id" : "63b70664-9f15-4d01-bc7b-e40aa9bbb244",
        "parentId" : "36c812d1-545d-474e-8b2b-a77d93511666",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "Synced offline. Breaking away into separate test functions would greatly increase the test runtime. Siding towards keeping them in one function. The reasoning for multiple scenarios is now added to the test description",
        "createdAt" : "2021-07-21T22:17:27Z",
        "updatedAt" : "2021-07-23T18:33:44Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b86531bbdf97de3579f62917b808f990496e6f0",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +169,173 @@        \"\"\"\n\n        # Scenario -- Re-init broker after cleaning up all persistent state\n        node = random.choice(self.kafka.nodes)\n        self.logger.debug(\"Scenario: kill-clean-start on broker node %s\", self.kafka.who_am_i(node))"
  },
  {
    "id" : "28096484-a605-4b6c-81af-00ca80ba858a",
    "prId" : 11053,
    "prUrl" : "https://github.com/apache/kafka/pull/11053#pullrequestreview-707878265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "578c3bd2-078c-4850-8b80-f10fbccad011",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "For this to be true don't we need to call `validate_success` after every scenario?",
        "createdAt" : "2021-07-23T17:55:57Z",
        "updatedAt" : "2021-07-23T17:58:26Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "d1358e26-a91e-47fd-b28c-64612e924e04",
        "parentId" : "578c3bd2-078c-4850-8b80-f10fbccad011",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "What you are suggesting is definitely a stronger check. The reason I did not do that was :\r\n  1. The way the code is laid out, doing a produce consume cycle needs two additional nodes each time. Doing it multiple times would have needed (num_scenarios*2) extra nodes to run the test.\r\n  2. Being able to start up the controller/broker without error is a check by itself (we caught bugs just while starting up/creating topics), and for completeness the final validate_success validates all prior actions.\r\n\r\nI can address this concern the following ways:\r\n  1. I could modify code to have the same Producer/Consumer node do validation for multiple topics, but that might take time.\r\n  2. I could add more nodes to the test to account for the extra producers/consumers.\r\n  3. Check in the code as is for now and improve it (alongside other test cases in another iteration)\r\n\r\nWhat would you prefer?",
        "createdAt" : "2021-07-23T18:18:57Z",
        "updatedAt" : "2021-07-23T18:33:44Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b86531bbdf97de3579f62917b808f990496e6f0",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +226,230 @@        left in the test to make debugging a failure of the test easier\n        e.g. if the first scenario passes and the second fails, it hints towards\n        a problem with the application of delta records while catching up\n        \"\"\"\n"
  },
  {
    "id" : "24739abc-7616-4989-96e2-a2cd2789f4ca",
    "prId" : 11053,
    "prUrl" : "https://github.com/apache/kafka/pull/11053#pullrequestreview-707878265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46653971-26d5-4833-a50c-80d9a9df0d52",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "For this to be true don't we need to call validate_success after every scenario?",
        "createdAt" : "2021-07-23T17:56:32Z",
        "updatedAt" : "2021-07-23T17:58:26Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "71d411f1-67f1-45cc-af5f-36386770b18b",
        "parentId" : "46653971-26d5-4833-a50c-80d9a9df0d52",
        "authorId" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "body" : "Same as below\r\n",
        "createdAt" : "2021-07-23T18:19:35Z",
        "updatedAt" : "2021-07-23T18:33:44Z",
        "lastEditedBy" : "a4dbaf80-7a7c-4314-af6e-b4cf858a6ce2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b86531bbdf97de3579f62917b808f990496e6f0",
    "line" : 168,
    "diffHunk" : "@@ -1,1 +166,170 @@        left in the test to make debugging a failure of the test easier\n        e.g. if the first scenario passes and the second fails, it hints towards\n        a problem with the application of delta records while catching up\n        \"\"\"\n"
  }
]