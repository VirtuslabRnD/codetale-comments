[
  {
    "id" : "5858e29a-dac2-46b7-ba2a-cdb313f01fc9",
    "prId" : 4173,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/4173#pullrequestreview-621662583",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1c73f50-9ebc-4e18-9b50-811ffce3a187",
        "parentId" : null,
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "Nice, could we also add a test that setting different values of `silent` across different per-framework autologging integrations doesn't interfere with each other? Maybe actually in the unit tests in `tests/autologging/test_autologging_behaviors_unit.py`? e.g. to cover the (probably rare) case where the user calls something like `mlflow.sklearn.autolog(silent=True)` and `mlflow.spark.autolog(silent=False)`?",
        "createdAt" : "2021-03-25T16:24:48Z",
        "updatedAt" : "2021-03-26T00:10:56Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      },
      {
        "id" : "d5afa66d-bf68-4874-8f38-d1cdf81184bf",
        "parentId" : "d1c73f50-9ebc-4e18-9b50-811ffce3a187",
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Absolutely! Added a `test_silent_mode_operates_independently_across_integrations` test case to `test_autologging_behaviors_unit.py`.",
        "createdAt" : "2021-03-26T00:11:35Z",
        "updatedAt" : "2021-03-26T00:12:04Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "becef61f28e9166eca3411a8e0acdef801113c21",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +230,234 @@    stream.truncate(0)\n\n    mlflow.sklearn.autolog(silent=False, log_input_examples=True)\n\n    with ThreadPoolExecutor(max_workers=50) as executor:"
  }
]