[
  {
    "id" : "48c7a3dd-2e37-45e7-a051-27b4f4a4bf26",
    "prId" : 3815,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3815#pullrequestreview-551060663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c98e788-819f-4e7e-ac64-d9912bd10b7c",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "@harupy These changes are necessary in order to pass the fast.ai autologging tests now that `try_mlflow_log` throws in test mode. The fastai auotlogging call to `log_model` has been silently failing in our test environment because it attempts to pickle a lambda function (line 185) defined within the scope of another function; the version of pickle used by fast.ai doesn't support this.\r\n\r\nAs a workaround, instead of passing function-style callbacks to the constructor of the fastai model (which requires that we use some sort of lambda or nested function in order to pass the model instance & patience value to the callback), we pass a callback instance to the `fit()` call instead.",
        "createdAt" : "2020-12-14T03:01:22Z",
        "updatedAt" : "2020-12-14T08:57:44Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      },
      {
        "id" : "31674f52-328d-4669-aab1-41d257503cf0",
        "parentId" : "1c98e788-819f-4e7e-ac64-d9912bd10b7c",
        "authorId" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "body" : "Nice fix!",
        "createdAt" : "2020-12-14T05:51:18Z",
        "updatedAt" : "2020-12-14T08:57:44Z",
        "lastEditedBy" : "0e487e6e-a7e7-4430-a4d6-ca9e76a34cba",
        "tags" : [
        ]
      }
    ],
    "commit" : "13ee8000d57e4735c70179a4ddc89f7ffd5e8603",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +189,193 @@        model.fit_one_cycle(NUM_EPOCHS, callbacks=callbacks)\n    else:\n        model.fit(NUM_EPOCHS, callbacks=callbacks)\n\n    client = mlflow.tracking.MlflowClient()"
  },
  {
    "id" : "11248797-f8e3-4b75-93a7-7864edd7cf28",
    "prId" : 3723,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/3723#pullrequestreview-542486244",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f81e241f-2290-4038-af40-9521b010abcf",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Can we add a small check to each of these test cases across the various modules?:\r\n\r\n- Pick *one* concrete metric name (e.g., `train_loss`) and assert that the metric name is in `run.data.metrics`.\r\n\r\nThis assertion will guard against the case that our mocking logic somehow resulted in metrics being swallowed or inadvertently mutated. Looks like we do the this with Keras and TF Keras for  `restored_epoch` and `stopped_epoch`, which is awesome; let's add it to the other ones!",
        "createdAt" : "2020-12-02T04:24:31Z",
        "updatedAt" : "2020-12-03T08:37:47Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffc8b8e2ec950e7532a387cc7f49dc05851bc6c3",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +292,296 @@\n    patched_metrics_data = dict(patched_metrics_data)\n    original_metrics = run.data.metrics\n    for metric_name in original_metrics:\n        assert metric_name in patched_metrics_data"
  }
]