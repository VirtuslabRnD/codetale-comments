[
  {
    "id" : "5717f091-3e8c-43e8-a9fa-61a91412c429",
    "prId" : 28,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/28#pullrequestreview-129437345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76872eac-74b9-4777-9d75-95eba0ca9a59",
        "parentId" : null,
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "Based on conversation with @andrewmchen & @tomasatdatabricks we should refactor this to to pytest format, though IMO we can wait till after we release TF functionality.\r\n\r\nSome notes on how this could be done:\r\n* Use the [tmpdir fixture](https://docs.pytest.org/en/latest/tmpdir.html) to generate a temporary test dir.\r\n* See https://github.com/databricks/mlflow/blob/master/tests/tracking/test_tracking.py for an example of pytest-style tests (except I didn't / should have used the tmpdir fixture when writing them :P). \r\n* Instance variables used in the test method (`self._dnn`, `self._dnn_predict`, `self._X`, `self._feature_names`) can be passed as fixtures. We could have a utility method that returns a tuple of these values & use [pytest.mark.parametrize](https://docs.pytest.org/en/latest/parametrize.html#pytest-mark-parametrize-parametrizing-test-functions) to feed values into the test function. Speaking of which, we should be able to implement the test with just `estimator`, `X`, and `y` fixture values (fit the estimator on X and y, compare predictions on y before & after saving/loading the model).",
        "createdAt" : "2018-06-18T07:14:44Z",
        "updatedAt" : "2018-06-18T23:20:48Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      }
    ],
    "commit" : "741c9ad2e7feab784e9ca124b1ed633151c74ab4",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +15,19 @@\n\nclass TestModelExport(unittest.TestCase):\n    def setUp(self):\n        iris = datasets.load_iris()"
  }
]