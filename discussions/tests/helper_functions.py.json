[
  {
    "id" : "4632f43c-ad97-4c99-aae3-c88fac3e9777",
    "prId" : 1257,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/1257#pullrequestreview-238041424",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a01223a-98c8-4dd1-a02a-e95a599e1d3c",
        "parentId" : null,
        "authorId" : "6e958077-8d60-494d-8079-e77af3a2951f",
        "body" : "btw, we may consider just making it `mlflow serve` and `mlflow predict` instead of being within models",
        "createdAt" : "2019-05-15T00:19:48Z",
        "updatedAt" : "2019-05-17T21:35:56Z",
        "lastEditedBy" : "6e958077-8d60-494d-8079-e77af3a2951f",
        "tags" : [
        ]
      },
      {
        "id" : "a0f2a4a9-ff96-465a-ab5b-af54c922425d",
        "parentId" : "8a01223a-98c8-4dd1-a02a-e95a599e1d3c",
        "authorId" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "body" : "Oh I thought we have decided on having this inside models? Either way, it should be a simple change on the cli side.",
        "createdAt" : "2019-05-15T20:03:35Z",
        "updatedAt" : "2019-05-17T21:35:56Z",
        "lastEditedBy" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "tags" : [
        ]
      }
    ],
    "commit" : "91f1f3368065ace10cc8d07e5fffb77dc4d75302",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +66,70 @@    env = dict(os.environ)\n    env.update(LC_ALL=\"en_US.UTF-8\", LANG=\"en_US.UTF-8\")\n    scoring_cmd = ['mlflow', 'models', 'serve', '-m', model_uri, \"-p\", \"0\"]\n    if extra_args is not None:\n        scoring_cmd += extra_args"
  },
  {
    "id" : "e592f9cb-c8ef-4311-8160-cd2e42583dde",
    "prId" : 710,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/710#pullrequestreview-174228401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a911957f-ab79-4a14-9f72-dcf0ae4453e8",
        "parentId" : null,
        "authorId" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "body" : "500s seems a little large, was there a reason to bump this number (looks like the previous behavior was to wait up to 250 secs)? It's not a huge deal - I mainly ask out of curiosity & since Travis will wait up to 10 min for a `pytest` invocation to finish before declaring it \"timed out\" & killing it, and having one of these tests hang/fail for ~8.5 min could prevent us from seeing whether other tests pass.",
        "createdAt" : "2018-11-13T03:35:00Z",
        "updatedAt" : "2018-11-13T23:20:28Z",
        "lastEditedBy" : "bd3067fd-855b-4bd1-898d-ca29199fd092",
        "tags" : [
        ]
      },
      {
        "id" : "6fd421d8-f4bf-446b-9796-044049184c78",
        "parentId" : "a911957f-ab79-4a14-9f72-dcf0ae4453e8",
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Good question! I bumped up the number because I was finding that the conda environment installation process for PyTorch takes a significant amount of time: it needs to install the `torchvision` libraries which depend on cudNN. The installation process usually did not finish within 250 seconds. However, 360 seconds should be just fine. I've updated the timeout to `360`. Let me know if this works for you!",
        "createdAt" : "2018-11-13T07:54:08Z",
        "updatedAt" : "2018-11-13T23:20:28Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "96e7de56474e0a2380a3b0a5c10d2b9fe621d03d",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +48,52 @@\ndef pyfunc_serve_and_score_model(\n        model_path, data, content_type, activity_polling_timeout_seconds=500):\n    \"\"\"\n    :param model_path: Path to the model to be served."
  },
  {
    "id" : "73e65c8a-cb86-4626-ad9a-b9c925b15f03",
    "prId" : 440,
    "prUrl" : "https://github.com/mlflow/mlflow/pull/440#pullrequestreview-153106557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "314e640f-0a87-45ab-a31d-61a92e4a944a",
        "parentId" : null,
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "`score_proc` already handles the process of waiting for the scoring process to launch and become active. Unless we're waiting for the process to write to `stdout`, this doesn't provide any additional safety. Additionally, matching on a specific string whose contents are controlled by the Flask library seems prone to failure.\r\n\r\nDid we observe a case where the scoring process was not ready before we attempted to query it? If so, it seems like we can add a simple sleep between `Popen` and `score_proc` to solve this problem.\r\n\r\nI would recommend removing this and calling `Popen` with the default `stdout` and `stderr` buffers. ",
        "createdAt" : "2018-09-06T18:51:00Z",
        "updatedAt" : "2018-09-06T20:29:07Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      },
      {
        "id" : "22c7c5de-4128-41d1-a28c-5949c7b82a71",
        "parentId" : "314e640f-0a87-45ab-a31d-61a92e4a944a",
        "authorId" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "body" : "The regular expression match is there to read the port number, I start it at port 0 which lets os pick whatever port is available and I than read it from the output.\r\n\r\nI agree that  the message can change, but that is a) unlikely, b) simple to fix. \r\n\r\nI could change the code to run on a set port similar to sagemaker scoring but I prefer this way so that users can run this test on their laptop and not interfere with other stuff that might be running.",
        "createdAt" : "2018-09-06T20:05:53Z",
        "updatedAt" : "2018-09-06T20:29:07Z",
        "lastEditedBy" : "d79ebb0f-0833-4329-8a37-7ae540003587",
        "tags" : [
        ]
      },
      {
        "id" : "51778e70-8fe5-4d04-a184-1d876292b1c4",
        "parentId" : "314e640f-0a87-45ab-a31d-61a92e4a944a",
        "authorId" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "body" : "Discussed offline. This seems fine.",
        "createdAt" : "2018-09-06T20:27:35Z",
        "updatedAt" : "2018-09-06T20:29:07Z",
        "lastEditedBy" : "3f60ced2-d2f0-4cc5-9898-5aefe16e0be8",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a4564119f0f2d20d8895ef4790f2d974a5f5583",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +50,54 @@    for x in iter(proc.stdout.readline, \"\"):\n        print(x)\n        m = re.match(pattern=\".*Running on http://127.0.0.1:(\\\\d+).*\", string=x)\n        if m:\n            return pd.read_json(_score_proc(proc, int(m.group(1)), data, data_type=\"csv\").content,"
  }
]