[
  {
    "id" : "45d0af58-222e-46d8-9daa-8ea6b6a1599d",
    "prId" : 100762,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100762#pullrequestreview-629965139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01eb8279-ec04-4f07-abbc-b894c4fa8b4c",
        "parentId" : null,
        "authorId" : "5af3a49e-2ce9-4046-8a13-ee66b8cbca2e",
        "body" : "What if a given pod isn't scheduled yet?\r\n\r\nShouldn't we handle the case when pod.Spec.NodeName is empty, explicitly?",
        "createdAt" : "2021-04-07T07:00:20Z",
        "updatedAt" : "2021-04-07T07:00:20Z",
        "lastEditedBy" : "5af3a49e-2ce9-4046-8a13-ee66b8cbca2e",
        "tags" : [
        ]
      },
      {
        "id" : "46cb0e57-a491-45ac-8f86-69a594679b67",
        "parentId" : "01eb8279-ec04-4f07-abbc-b894c4fa8b4c",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "We are already effectively ignoring them when iterating the nodes.",
        "createdAt" : "2021-04-07T12:50:43Z",
        "updatedAt" : "2021-04-07T12:50:44Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "200ef16f1d51c9abea8c818fe958cea5fe511665",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +590,594 @@\t}\n\tfor _, pod := range allPods.Items {\n\t\tnodeName := pod.Spec.NodeName\n\t\tnodeNameToPodList[nodeName] = append(nodeNameToPodList[nodeName], &pod)\n\t}"
  },
  {
    "id" : "946c597c-ff03-418a-a35e-b1eb6c9ceab7",
    "prId" : 99020,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99020#pullrequestreview-589296325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b960517d-ffe6-4142-9b27-6cfa073f068e",
        "parentId" : null,
        "authorId" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "body" : "IMO we should prefer keys you can do a simple prefix match on, but admittedly moving this to the front was not important. \"-taint-key\" seems redundant. This should now be 40 + 23 characters.\r\n\r\nI don't think having a valid UUID is important, and we should have enough bits, I think.",
        "createdAt" : "2021-02-12T09:33:28Z",
        "updatedAt" : "2021-02-12T09:33:28Z",
        "lastEditedBy" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad325377b5b370d3f7a77181e382c9c662d4c563",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +638,642 @@func getRandomTaint() v1.Taint {\n\treturn v1.Taint{\n\t\tKey:    fmt.Sprintf(\"kubernetes.io/e2e-scheduling-priorities-%s\", string(uuid.NewUUID()[:23])),\n\t\tValue:  fmt.Sprintf(\"testing-taint-value-%s\", string(uuid.NewUUID())),\n\t\tEffect: v1.TaintEffectPreferNoSchedule,"
  },
  {
    "id" : "b7b0abbc-451d-4b00-b0d8-8bc0c9c420d7",
    "prId" : 98073,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98073#pullrequestreview-587708622",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e77170f-7ca8-4858-8b2d-2f95d7b08eda",
        "parentId" : null,
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "This additional check seems based on cri-o limitation.\r\nIs it fine to apply the same limitation for environments which runtimes are different from cri-o?\r\n\r\n/cc @oomichi ",
        "createdAt" : "2021-01-15T20:06:28Z",
        "updatedAt" : "2021-02-18T14:30:48Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      },
      {
        "id" : "ef9aed7d-146a-47c6-ab72-d07e654d93ef",
        "parentId" : "6e77170f-7ca8-4858-8b2d-2f95d7b08eda",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "This is a good question, and I would appreciate if any runtimes folks could chime in. I imagine most runtimes will have some sort of similar minimums, but even finding the cri-o minimum was difficult for me. I'm not sure who to tag for that",
        "createdAt" : "2021-01-15T20:10:59Z",
        "updatedAt" : "2021-02-18T14:30:48Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "e2838084-bcdc-422d-81d4-242d963db9da",
        "parentId" : "6e77170f-7ca8-4858-8b2d-2f95d7b08eda",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "@mrunalp , would you be able to help us figure out how `minMemoryLimit` should be handled for runtimes other than cri-o? I see you originally introduced it to cri-o (in https://github.com/cri-o/cri-o/pull/1723), so hoping you could shed some light on this issue",
        "createdAt" : "2021-01-17T02:30:31Z",
        "updatedAt" : "2021-02-18T14:30:48Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "49cc758b-111b-4d37-8423-02b58f7cc18b",
        "parentId" : "6e77170f-7ca8-4858-8b2d-2f95d7b08eda",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "@oomichi I am still trying to reach someone more familiar with different runtimes, but from my own assessment I don't think there is any risk to other runtimes for adding this change. At most, the test will be using more resources than necessary on runtimes with smaller minimums (but it is a serial test, so that isn't a problem)\r\n\r\nHowever, without this change the test is flakey on CRI-O, which needs to be supported by k8s. So I think that is justification for needing this",
        "createdAt" : "2021-02-10T14:42:32Z",
        "updatedAt" : "2021-02-18T14:30:48Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "d4f99e64-1084-489a-8c96-4eb7b60970d0",
        "parentId" : "6e77170f-7ca8-4858-8b2d-2f95d7b08eda",
        "authorId" : "cd4c1563-e5f0-4cd8-91b7-6e3ae815e95d",
        "body" : "from my (brief) investigation, containerd does not add a similar check.\r\n\r\nCRI-O added the check because we found 12MB was a good minimum to allow all containers in the pod to be created. runc itself takes 3-4MB, so in a sufficiently large pod (two containers, plus infra container), container creates were being killed.\r\n\r\nI agree with @damemi, at worse it uses more resources than necessary (on the order of maybe 8 MB per pod)",
        "createdAt" : "2021-02-10T15:06:45Z",
        "updatedAt" : "2021-02-18T14:30:48Z",
        "lastEditedBy" : "cd4c1563-e5f0-4cd8-91b7-6e3ae815e95d",
        "tags" : [
        ]
      }
    ],
    "commit" : "32b0333c158a9c7419746ed61cc587cb51453596",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +170,174 @@\t\t// skip if the most utilized node has less than the cri-o minMemLimit available\n\t\t// otherwise we will not be able to run the test pod once all nodes are balanced\n\t\tif nodesAreTooUtilized(cs, nodeList) {\n\t\t\tginkgo.Skip(\"nodes are too utilized to schedule test pods\")\n\t\t}"
  },
  {
    "id" : "ca357b3b-6a3d-4da9-8a70-af9be2556727",
    "prId" : 97819,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97819#pullrequestreview-589295471",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3cace5e-5088-4ece-83d4-b42b729a1a81",
        "parentId" : null,
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "Is there a max taint key length we should be watching out for?  Labels are capped at 63; the hardcoded string part here is 52 chars and `string(uuid.NewUUID())` appears to be 37 chars",
        "createdAt" : "2021-01-15T22:42:24Z",
        "updatedAt" : "2021-01-29T15:17:36Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      },
      {
        "id" : "c4899ea1-d36b-424b-a74c-a0da5a003c2a",
        "parentId" : "a3cace5e-5088-4ece-83d4-b42b729a1a81",
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint\r\n\r\n>The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.\r\n\r\nok, this is well within the limit",
        "createdAt" : "2021-01-15T22:47:15Z",
        "updatedAt" : "2021-01-29T15:17:36Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      },
      {
        "id" : "b6ecceb0-6dc3-4ab5-8847-79dae8263bd6",
        "parentId" : "a3cace5e-5088-4ece-83d4-b42b729a1a81",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "@spiffxp did that limit recently increase? Trying to port this to openshift I am seeing an error that this key is too long: `name part must be no more than 63 characters`",
        "createdAt" : "2021-02-09T16:53:13Z",
        "updatedAt" : "2021-02-09T16:53:13Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "56d4b83a-3bf6-4c13-ab5f-bc83c41ef446",
        "parentId" : "a3cace5e-5088-4ece-83d4-b42b729a1a81",
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "@damemi this is failing in the upstream CI too, due to this key limitation\r\nhttps://testgrid.k8s.io/sig-network-gce#gci-gce-serial&width=5\r\n```\r\n                       Type: \"FieldValueInvalid\",\r\n                        Message: \"Invalid value: \\\"kubernetes.io/scheduling-priorities-e2e-taint-key-c19ab4b9-07b1-4a86-9a0f-52c1a6028392\\\": name part must be no more than 63 characters\",\r\n                        Field: \"metadata.taints[0].key\",\r\n                    },\r\n                ],\r\n```",
        "createdAt" : "2021-02-12T09:08:24Z",
        "updatedAt" : "2021-02-12T09:08:52Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "eb9cfa11-fe5f-47d9-969c-1f02f9298c76",
        "parentId" : "a3cace5e-5088-4ece-83d4-b42b729a1a81",
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "ping @spiffxp ^^^",
        "createdAt" : "2021-02-12T09:09:34Z",
        "updatedAt" : "2021-02-12T09:09:34Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "0fedaf6f-1260-40db-893c-055466c8e7fd",
        "parentId" : "a3cace5e-5088-4ece-83d4-b42b729a1a81",
        "authorId" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "body" : "filed https://github.com/kubernetes/kubernetes/pull/99020\r\n\r\nwe may also need to update the docs.",
        "createdAt" : "2021-02-12T09:32:20Z",
        "updatedAt" : "2021-02-12T09:32:20Z",
        "lastEditedBy" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc1eab1ca2f0d7b65f24dc64472001d786c1a950",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +606,610 @@func getRandomTaint() v1.Taint {\n\treturn v1.Taint{\n\t\tKey:    fmt.Sprintf(\"kubernetes.io/scheduling-priorities-e2e-taint-key-%s\", string(uuid.NewUUID())),\n\t\tValue:  fmt.Sprintf(\"testing-taint-value-%s\", string(uuid.NewUUID())),\n\t\tEffect: v1.TaintEffectPreferNoSchedule,"
  },
  {
    "id" : "e348c8d1-f03a-487e-83ad-aa6a315acc41",
    "prId" : 88105,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88105#pullrequestreview-359260890",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31be100d-47fd-42cd-be2e-674b23d22124",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I know we need this so that the \"node resource\" plugin doesn't influence the score, but this is going to cause flakes because a pod created by a previous test may get deleted after this function is called, and so breaking the \"balanced\" assumption. I don't have a better suggestion how to fix this though. \r\n\r\nGiving PodTopologySpread a higher priority should make this less important.\r\n\r\n",
        "createdAt" : "2020-02-14T20:02:03Z",
        "updatedAt" : "2020-02-16T12:42:40Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "95f6c74e-aa36-4c95-ac94-83fde2c2d15a",
        "parentId" : "31be100d-47fd-42cd-be2e-674b23d22124",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Exactly like what I mentioned in https://github.com/kubernetes/kubernetes/issues/88174#issuecomment-586470458. And this is why I hesitated to add this score test in e2e - you can't adjust the default weight (which is 1) for a given e2e env.",
        "createdAt" : "2020-02-14T22:45:32Z",
        "updatedAt" : "2020-02-15T00:48:49Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "c93dffdfc44b3a1259129a7d925e588fa9fe40f1",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +368,372 @@\n\t\t\t// Make the nodes have balanced cpu,mem usage.\n\t\t\terr := createBalancedPodForNodes(f, cs, ns, nodes, podRequestedResource, 0.5)\n\t\t\tframework.ExpectNoError(err)\n"
  },
  {
    "id" : "9968e881-bd27-4554-a887-a7118e3d9763",
    "prId" : 84868,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84868#pullrequestreview-313575067",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ac031e6-004f-465d-ada2-322a0ebf933a",
        "parentId" : null,
        "authorId" : "1d2372e4-a834-465f-a6f1-aa973013751c",
        "body" : "Although it is not the main subject, some other files define the above line as `apierrors`.\r\nFor example, test/e2e/storage/subpath.go\r\nIt might be better to unify the definitions for readability.",
        "createdAt" : "2019-11-07T01:58:46Z",
        "updatedAt" : "2019-11-07T01:58:47Z",
        "lastEditedBy" : "1d2372e4-a834-465f-a6f1-aa973013751c",
        "tags" : [
        ]
      },
      {
        "id" : "36a4584c-c694-456b-b69f-51770d0c4bdf",
        "parentId" : "0ac031e6-004f-465d-ada2-322a0ebf933a",
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "That is a nice point.\r\nActually we are managing this kind of thing with `./hack/.import_aliases` file and the above apierrs is not included at this time.\r\nSo it is nice to add it to the list.\r\n\r\nI investigated current situation of `\"k8s.io/apimachinery/pkg/api/errors\"`.\r\nUnder `/test`, `apierrs` is more than `apierrors` as the following.\r\nSo it is fine to keep the above code today.\r\n```\r\n$ grep \"k8s.io/apimachinery/pkg/api/errors\" * -R | grep apierrs | wc\r\n     30      90    2251\r\n$ grep \"k8s.io/apimachinery/pkg/api/errors\" * -R | grep apierrors | wc\r\n     14      42    1153\r\n```\r\n",
        "createdAt" : "2019-11-07T19:17:44Z",
        "updatedAt" : "2019-11-07T19:18:06Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb9d1cb5ccabf2c7f06464bf77fbb07bead27222",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +29,33 @@\n\tv1 \"k8s.io/api/core/v1\"\n\tapierrs \"k8s.io/apimachinery/pkg/api/errors\"\n\t\"k8s.io/apimachinery/pkg/api/resource\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\""
  },
  {
    "id" : "cf94e84e-362c-4e34-906f-f115d07ff3cc",
    "prId" : 70329,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70329#pullrequestreview-169095226",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40b8708f-d2dc-4430-a403-fbbcd6240340",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Is that to say, `Patch()` doesn't work for API fields in status? Or, it has bug on updating both memory and cpu fields? (b/c I checked the history, it seems that our code was always using `Patch()`, but updating memory only)",
        "createdAt" : "2018-10-28T05:52:28Z",
        "updatedAt" : "2018-10-28T06:12:30Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "12f471b5-58c7-4b3e-9e41-f6dbc4460472",
        "parentId" : "40b8708f-d2dc-4430-a403-fbbcd6240340",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "> Is that to say, Patch() doesn't work for API fields in status? \r\nYeah, that's right. Patch wasn't working for updating cpu or memory. \r\n\r\nIn order to test it, I added taints to node and applied patch, this worked while cpu and memory were not getting updated. So, I had to use UpdateStatus.",
        "createdAt" : "2018-10-28T14:41:45Z",
        "updatedAt" : "2018-10-28T14:41:45Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "fedfddcb217fcb8beded757d7b62af8078eae06f",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +469,473 @@\tnode.Status.Allocatable[v1.ResourceMemory] = *resource.NewQuantity(memory, resource.BinarySI)\n\tnode.Status.Allocatable[v1.ResourceCPU] = *resource.NewMilliQuantity(cpu, resource.DecimalSI)\n\t_, err = c.CoreV1().Nodes().UpdateStatus(node)\n\treturn err\n}"
  },
  {
    "id" : "e20a244c-1d18-4f94-9ba9-a67bdd47c747",
    "prId" : 70329,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70329#pullrequestreview-169113487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e51ebc96-709f-421f-bea7-50f8685b024a",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Any reason to set this particular value? Isn't `10000` (greater than incoming limit `5000m`) good enough?",
        "createdAt" : "2018-10-28T06:03:44Z",
        "updatedAt" : "2018-10-28T06:03:44Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "edbeadbd-d5dd-4160-80e9-9a1427c3bac4",
        "parentId" : "e51ebc96-709f-421f-bea7-50f8685b024a",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "This value is for memory not cpu",
        "createdAt" : "2018-10-28T14:42:02Z",
        "updatedAt" : "2018-10-28T14:42:02Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "5283c400-23c2-439a-95d4-8e67c29da27d",
        "parentId" : "e51ebc96-709f-421f-bea7-50f8685b024a",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "sry, I misread... Let me rephrase: so the original mem val `10000` (greater than incoming limit `3000Mi`) should be put as a form `10737418240`, to represent 10Gi, right?",
        "createdAt" : "2018-10-28T22:18:10Z",
        "updatedAt" : "2018-10-28T22:18:10Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "536f9e49-7981-42b1-b7e6-31d5b1172b23",
        "parentId" : "e51ebc96-709f-421f-bea7-50f8685b024a",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Yes, that is right and that code path was not getting exercised as patch had not effect.",
        "createdAt" : "2018-10-28T22:43:00Z",
        "updatedAt" : "2018-10-28T22:43:00Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "fedfddcb217fcb8beded757d7b62af8078eae06f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +280,284 @@\t\tnodeOriginalMemoryVal := nodeOriginalMemory.Value()\n\t\tnodeOriginalCPUVal := nodeOriginalCPU.MilliValue()\n\t\terr := updateNodeAllocatable(cs, nodeName, int64(10737418240), int64(12000))\n\t\tExpect(err).NotTo(HaveOccurred())\n\t\tdefer func() {"
  },
  {
    "id" : "25380c9a-7b9c-4cad-9bfc-b914e1e57a1c",
    "prId" : 44309,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/44309#pullrequestreview-32945040",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "377ba8f2-670c-414d-a8ef-a7ef7ae723ef",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "s/\"scheduler-priority-selector-spreading\"/config.Name/",
        "createdAt" : "2017-04-16T14:16:21Z",
        "updatedAt" : "2017-04-26T10:45:57Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "55acc00626fd7db6ff6eca328d45136dcb07143c",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +130,134 @@\t\tdefer func() {\n\t\t\t// Resize the replication controller to zero to get rid of pods.\n\t\t\tif err := framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, \"scheduler-priority-selector-spreading\"); err != nil {\n\t\t\t\tframework.Logf(\"Failed to cleanup replication controller %v: %v.\", \"scheduler-priority-selector-spreading\", err)\n\t\t\t}"
  },
  {
    "id" : "daf79748-ede9-4dcc-ae0a-2b055f23c0dc",
    "prId" : 42890,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42890#pullrequestreview-29063323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60ea5ecd-5302-4058-a611-8e7852700651",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "I think one case for `ReplicationController`, `ReplicaSet`, `StatefulSet` and `Service` is enough, as the scheduling logic is same; the only different is how to get selector (we can cover it in unit test.)\r\n\r\nLess cases will not impact the coverage; but reduce e2e time.",
        "createdAt" : "2017-03-23T02:09:53Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "bcc9d905-54cb-40ac-8829-dbe0e69ae71e",
        "parentId" : "60ea5ecd-5302-4058-a611-8e7852700651",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "@k82cn I just want to make sure all of them are covered",
        "createdAt" : "2017-03-23T06:10:50Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "d1b1cb09-f764-4570-862e-c5c5143765d5",
        "parentId" : "60ea5ecd-5302-4058-a611-8e7852700651",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "prefer to just keep one case which is enough to cover the \"selector\" priority's logic; for how to get selector, I'd like to dependent on unit test :).",
        "createdAt" : "2017-03-23T06:42:31Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "ad1757b3-7630-4c29-b4de-167f18e04d26",
        "parentId" : "60ea5ecd-5302-4058-a611-8e7852700651",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "@k82cn Yeah, sounds reasonable, I can remove it.",
        "createdAt" : "2017-03-26T06:30:22Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b3a6c80cad8be7ff04ec5cde6dbd6c80e4941d6",
    "line" : 110,
    "diffHunk" : "@@ -1,1 +108,112 @@\t})\n\n\tIt(\"Pods created by ReplicationController should spread to different node\", func() {\n\t\tBy(\"Create a pod for each node to make the nodes have almost same cpu/mem usage ratio\")\n"
  },
  {
    "id" : "5c724d5c-bb5b-48a9-a896-72f09e107936",
    "prId" : 42890,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42890#pullrequestreview-29102058",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49065cd0-ef6e-481f-bc39-5870020b741f",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Why two selector here (hostname + k=v)?",
        "createdAt" : "2017-03-23T02:16:52Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "42e80dc8-c467-4d31-9592-2dbd13a0a919",
        "parentId" : "49065cd0-ef6e-481f-bc39-5870020b741f",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "@k82cn No, just add two item to match the affinity, but we can using one selector if you don't like.",
        "createdAt" : "2017-03-26T06:52:46Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "6fa9caef-6e97-4818-9d2b-93ed9cd06374",
        "parentId" : "49065cd0-ef6e-481f-bc39-5870020b741f",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "I'm OK with that.",
        "createdAt" : "2017-03-27T05:36:56Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b3a6c80cad8be7ff04ec5cde6dbd6c80e4941d6",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +161,165 @@\t\t\t\t\tPreferredDuringSchedulingIgnoredDuringExecution: []v1.PreferredSchedulingTerm{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tPreference: v1.NodeSelectorTerm{\n\t\t\t\t\t\t\t\tMatchExpressions: []v1.NodeSelectorRequirement{\n\t\t\t\t\t\t\t\t\t{"
  },
  {
    "id" : "25365e7d-bc74-431e-869a-0f4bed44b762",
    "prId" : 42890,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42890#pullrequestreview-29881152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5da656e5-5d9e-43aa-9365-dd38ba3e5266",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Why add label to node? It's testing pod affinity.",
        "createdAt" : "2017-03-23T02:20:07Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "8ba80902-9bbc-4a59-b964-51c1b38aceb2",
        "parentId" : "5da656e5-5d9e-43aa-9365-dd38ba3e5266",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "@k82cn For the topologyKey in the podAffinity iterm",
        "createdAt" : "2017-03-23T06:13:13Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "82b13b0f-f449-474e-af51-68a6ca14f0dc",
        "parentId" : "5da656e5-5d9e-43aa-9365-dd38ba3e5266",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Yes, prefer to add two nodes in this topology; current case is \"similar\" to node affinity.",
        "createdAt" : "2017-03-23T06:40:53Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "befabf53-afe3-483c-ab11-8ae457b3cd96",
        "parentId" : "5da656e5-5d9e-43aa-9365-dd38ba3e5266",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "@k82cn nodeAffinity don't have this key, this is just use to filter out the pods that on the node with that label",
        "createdAt" : "2017-03-26T07:04:44Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "9b9c2418-0c1f-4410-8507-a0d0110339a6",
        "parentId" : "5da656e5-5d9e-43aa-9365-dd38ba3e5266",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "I'm OK with that; not a block comments :).",
        "createdAt" : "2017-03-30T01:11:13Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b3a6c80cad8be7ff04ec5cde6dbd6c80e4941d6",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +196,200 @@\t\tk := fmt.Sprintf(\"kubernetes.io/e2e-%s\", \"node-topologyKey\")\n\t\tv := \"topologyvalue\"\n\t\tframework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)\n\t\tframework.ExpectNodeHasLabel(cs, nodeName, k, v)\n\t\tdefer framework.RemoveLabelOffNode(cs, nodeName, k)"
  },
  {
    "id" : "462b802a-3993-4c9d-9340-f2a585f35ec9",
    "prId" : 42890,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42890#pullrequestreview-28553762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04403ed0-2829-4dc5-9fba-cce6b87c61d3",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "ditto",
        "createdAt" : "2017-03-23T02:20:38Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b3a6c80cad8be7ff04ec5cde6dbd6c80e4941d6",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +259,263 @@\t\tv := \"topologyvalue\"\n\t\tframework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)\n\t\tframework.ExpectNodeHasLabel(cs, nodeName, k, v)\n\t\tdefer framework.RemoveLabelOffNode(cs, nodeName, k)\n"
  },
  {
    "id" : "105d42c2-c147-4a4f-9f9c-3abdaf1f767f",
    "prId" : 42890,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42890#pullrequestreview-29882712",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "363244ba-2b0f-431c-af44-69a354a84294",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Why we need this func? Suppose all nodes has similar allocatable.",
        "createdAt" : "2017-03-23T02:51:38Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "64e53a86-3262-4d1d-8db4-5151b65fc522",
        "parentId" : "363244ba-2b0f-431c-af44-69a354a84294",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "Similar does not mean same, as before we run the test, there really have a lot system pods make the resource not balanced.",
        "createdAt" : "2017-03-26T07:06:05Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "d93dd374-dd14-4b73-855f-b7e597d2c9cf",
        "parentId" : "363244ba-2b0f-431c-af44-69a354a84294",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Can we remove pods after each cases?",
        "createdAt" : "2017-03-27T05:37:46Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "7a179613-64bb-4af4-9aeb-5279b1f3ddf9",
        "parentId" : "363244ba-2b0f-431c-af44-69a354a84294",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "@k82cn The namespaces will be deleted so the pods will be deleted also.",
        "createdAt" : "2017-03-27T06:09:34Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "7e9cd8a0-0c38-4527-8b4e-3825977938d7",
        "parentId" : "363244ba-2b0f-431c-af44-69a354a84294",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "I means deleting Pods after each test cases (If the pods was deleted, why we need this?). \r\n\r\nThe ratio seems different from cases, e.g. 0.5, 0.6, 0.9, how did you define it? and how other contributor define it for other e2e cases?",
        "createdAt" : "2017-03-30T01:10:43Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "4154fb7f-3dbf-4ad0-97b3-d6ed09301c48",
        "parentId" : "363244ba-2b0f-431c-af44-69a354a84294",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "@k82cn Every test case will have a new namespace before start and deleted after finished. the balanced pods are created for each cases, and will be deleted after each case. For the ratio, just set the value you want to is ok.",
        "createdAt" : "2017-03-30T01:28:36Z",
        "updatedAt" : "2017-03-30T02:54:42Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b3a6c80cad8be7ff04ec5cde6dbd6c80e4941d6",
    "line" : 423,
    "diffHunk" : "@@ -1,1 +421,425 @@\n// createBalancedPodForNodes creates a pod per node that asks for enough resources to make all nodes have the same mem/cpu usage ratio.\nfunc createBalancedPodForNodes(f *framework.Framework, cs clientset.Interface, ns string, nodes []v1.Node, requestedResource *v1.ResourceRequirements, ratio float64) {\n\t// find the max, if the node has the max,use the one, if not,use the ratio parameter\n\tvar maxCpuFraction, maxMemFraction float64 = ratio, ratio"
  }
]