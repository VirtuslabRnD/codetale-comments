[
  {
    "id" : "bc8c7553-141f-4c63-956e-b780ef824316",
    "prId" : 100128,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100128#pullrequestreview-633948740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f474a54b-15ed-43e5-a509-5b643daea39b",
        "parentId" : null,
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "this is still 2, if it's supposed to request 4/5 it should be 4 right? Not trying to be snarky, I am actually confused by why this is changed to 4/5",
        "createdAt" : "2021-03-12T21:25:11Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "3196afdc-e765-4555-9fa4-16cb6e07990e",
        "parentId" : "f474a54b-15ed-43e5-a509-5b643daea39b",
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "Two pods (`for j := 0; j < 2; j++`), each allocating 2/5, in total 4/5",
        "createdAt" : "2021-03-14T10:24:49Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      },
      {
        "id" : "134ef5f6-1f19-49e1-baf9-b7ec5da83bcf",
        "parentId" : "f474a54b-15ed-43e5-a509-5b643daea39b",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "Ah, I didn't notice this was changed to a loop.",
        "createdAt" : "2021-03-15T15:34:46Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "fd5a3968-1bfe-4c19-92ed-323f3d7918cc",
        "parentId" : "f474a54b-15ed-43e5-a509-5b643daea39b",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Can you explain why this change is necessary?",
        "createdAt" : "2021-04-09T13:00:45Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "1c32d5c1-6976-4b10-a039-183594eefab1",
        "parentId" : "f474a54b-15ed-43e5-a509-5b643daea39b",
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "In the commit message:\r\n```\r\nTo run the tests in a single node cluster, create two pods consuming 2/5 of the extended resource instead of one consuming 2/3.\r\nThe low priority pod will be consuming 2/5 of the extended resource instead so in case there's only a single node,\r\na high priority pod consuming 2/5 of the extended resource can be still scheduled. Thus, making sure only the low priority pod\r\ngets preempted once the preemptor pod consuming 2/5 of the extended resource gets scheduled while keeping the high priority pod untouched.\r\n```",
        "createdAt" : "2021-04-11T12:41:58Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      },
      {
        "id" : "64c0ee74-6fa6-4e2f-ab77-b81557616602",
        "parentId" : "f474a54b-15ed-43e5-a509-5b643daea39b",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Make this comment in the PR description instead of commit (for more visibility)",
        "createdAt" : "2021-04-12T13:22:48Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "b248865c-28f0-4cba-acd1-cb5a710d3ec7",
        "parentId" : "f474a54b-15ed-43e5-a509-5b643daea39b",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "Or copy it to the PR description too. More descriptive commit messages are never bad :)",
        "createdAt" : "2021-04-12T20:42:52Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2fc250a4d3ac704a8647d78fadf0565c371fcd",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +137,141 @@\t\t\t\t// Request 2 of the available resources for the victim pods\n\t\t\t\tpodRes = v1.ResourceList{}\n\t\t\t\tpodRes[testExtendedResource] = resource.MustParse(\"2\")\n\n\t\t\t\t// make the first pod low priority and the rest medium priority."
  },
  {
    "id" : "e4c4cc9c-9f05-48c8-8a62-20138b723d88",
    "prId" : 100128,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100128#pullrequestreview-611163006",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13515e6d-a511-4d5d-ac7b-cf18f8e0942b",
        "parentId" : null,
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "same comment",
        "createdAt" : "2021-03-12T21:25:25Z",
        "updatedAt" : "2021-04-13T07:47:51Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2fc250a4d3ac704a8647d78fadf0565c371fcd",
    "line" : 104,
    "diffHunk" : "@@ -1,1 +226,230 @@\t\t\t\t// Request 2 of the available resources for the victim pods\n\t\t\t\tpodRes = v1.ResourceList{}\n\t\t\t\tpodRes[testExtendedResource] = resource.MustParse(\"2\")\n\n\t\t\t\t// make the first pod low priority and the rest medium priority."
  },
  {
    "id" : "9938b1ed-4eaf-484e-95f0-0c72e7011ab0",
    "prId" : 95340,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95340#pullrequestreview-519108595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96865146-2e78-4fb8-a9f3-a3e426eea61e",
        "parentId" : null,
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "even \"get\" and \"list\" have been implicitly included in this testcase, I am wondering whether we need to explicitly test it here?",
        "createdAt" : "2020-10-11T08:51:11Z",
        "updatedAt" : "2020-10-13T19:06:59Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "52b09094-4edb-4a63-a367-e9dd90ab4665",
        "parentId" : "96865146-2e78-4fb8-a9f3-a3e426eea61e",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I think \"get\" is explicitly covered? (in the for loop in step 3).\r\n\r\n\"list\" does need to be explicitly tested, I will get it covered.",
        "createdAt" : "2020-10-12T17:50:43Z",
        "updatedAt" : "2020-10-13T19:06:59Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "bd6db3d6-cbf6-423b-9fc6-57d9a288a63e",
        "parentId" : "96865146-2e78-4fb8-a9f3-a3e426eea61e",
        "authorId" : "ecbf1877-287d-4d66-96d7-e2559a42fcc8",
        "body" : "deleteSchedulingV1CollectionPriorityClass isn't currently showing as hit/tested at this time for some reason:\r\n\r\nhttps://apisnoop.cncf.io/conformance-progress/endpoints/1.20.0?filter=untested\r\n\r\n![image](https://user-images.githubusercontent.com/43581769/97385960-b5449b00-1937-11eb-9cf1-c7484ca5900f.png)\r\n",
        "createdAt" : "2020-10-28T03:08:30Z",
        "updatedAt" : "2020-10-28T03:08:30Z",
        "lastEditedBy" : "ecbf1877-287d-4d66-96d7-e2559a42fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "9ff8ba17-a170-4d96-85ce-06d1ff611e5f",
        "parentId" : "96865146-2e78-4fb8-a9f3-a3e426eea61e",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "That's as expected. As PriorityClass is a cluster-level resource, if we go delete the collection, it would delete all PriorityClass - which includes the system-critical ones, which is not accepted.\r\n\r\nI also mentioned it in the PR description (check out the \"Special notes for your reviewer:\" section).",
        "createdAt" : "2020-10-28T06:07:56Z",
        "updatedAt" : "2020-10-28T06:07:56Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "eaa31649-28c7-44cc-b433-8f34d6b051df",
        "parentId" : "96865146-2e78-4fb8-a9f3-a3e426eea61e",
        "authorId" : "97f5510f-d2e4-4cef-b8eb-c4184feaff72",
        "body" : "I would recommend using labels selectors to delete collections, make sure that the resources managed in this test are statically labeled, so at the end, they can be deleted using a given label. e.g:\r\n\r\n```go\r\n_ = f.ClientSet.CoreV1().ReplicationControllers(testRcNamespace).DeleteCollection(context.TODO(), metav1.DeleteOptions{}, metav1.ListOptions{LabelSelector: \"test-rc-static=true\"})\r\n```\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/90880/files#diff-2a2edebd256ce20f5b0964acb9360c53d8922c6ed84ab85d60f11649503f5b70R381",
        "createdAt" : "2020-10-28T20:17:26Z",
        "updatedAt" : "2020-10-28T20:17:26Z",
        "lastEditedBy" : "97f5510f-d2e4-4cef-b8eb-c4184feaff72",
        "tags" : [
        ]
      },
      {
        "id" : "a3c7f42f-9916-4c26-ab4d-70c54e5f79db",
        "parentId" : "96865146-2e78-4fb8-a9f3-a3e426eea61e",
        "authorId" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "body" : "> I would recommend using labels selectors to delete collections, make sure that the resources managed in this test are statically labeled, so at the end, they can be deleted using a given label. e.g:\r\n> \r\n> ```go\r\n> _ = f.ClientSet.CoreV1().ReplicationControllers(testRcNamespace).DeleteCollection(context.TODO(), metav1.DeleteOptions{}, metav1.ListOptions{LabelSelector: \"test-rc-static=true\"})\r\n> ```\r\n> \r\n> https://github.com/kubernetes/kubernetes/pull/90880/files#diff-2a2edebd256ce20f5b0964acb9360c53d8922c6ed84ab85d60f11649503f5b70R381\r\n\r\n@Huang-Wei please reach out if you need more support to get the `deleteSchedulingV1CollectionPriorityClass` endpoint tested. \r\nThank you for the great work.\r\n\r\n",
        "createdAt" : "2020-10-28T20:33:18Z",
        "updatedAt" : "2020-10-28T20:33:19Z",
        "lastEditedBy" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "tags" : [
        ]
      },
      {
        "id" : "3671423c-cd47-46fe-998d-83af3fb79c8c",
        "parentId" : "96865146-2e78-4fb8-a9f3-a3e426eea61e",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Thanks @BobyMCbobs @Riaankl I will submit another PR exercising `DeleteCollection` with ListOptions.",
        "createdAt" : "2020-10-28T21:45:09Z",
        "updatedAt" : "2020-10-28T21:45:10Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8cfbc8ac1ae7fde30932b7fbaee16158d04aba1",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +712,716 @@\n\t\tginkgo.It(\"verify PriorityClass endpoints can be operated with different HTTP methods\", func() {\n\t\t\t// 1. Patch/Update on immutable fields will fail.\n\t\t\tpcCopy := pcs[0].DeepCopy()\n\t\t\tpcCopy.Value = pcCopy.Value * 10"
  },
  {
    "id" : "f0b45808-1cb3-48bf-95e2-bf4a1dfd916a",
    "prId" : 95340,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95340#pullrequestreview-507739342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "95116787-805c-4804-8fe7-5426c1d348de",
        "parentId" : null,
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "just a question, it that doable to run collection deletion so that the test would be covered, and add those system level priorities back later?",
        "createdAt" : "2020-10-13T10:31:33Z",
        "updatedAt" : "2020-10-13T19:06:59Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "c1e16541-bb0f-4c7f-9b65-c3cc4b1407f2",
        "parentId" : "95116787-805c-4804-8fe7-5426c1d348de",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "that's way too much IMO... You won't really want to delete the system level priority classes, no matter adding them back or not :)",
        "createdAt" : "2020-10-13T18:56:44Z",
        "updatedAt" : "2020-10-13T19:06:59Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8cfbc8ac1ae7fde30932b7fbaee16158d04aba1",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +704,708 @@\t\t\t}\n\n\t\t\t// Cannot run collection deletion which would delete all system level priority classes.\n\t\t\tfor _, pc := range pcs {\n\t\t\t\terr := cs.SchedulingV1().PriorityClasses().Delete(context.TODO(), pc.Name, *metav1.NewDeleteOptions(0))"
  },
  {
    "id" : "46f7842c-9f9b-4b9d-8c6d-a196331bb642",
    "prId" : 91955,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91955#pullrequestreview-427512379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecfc4ae0-f960-4fee-a332-8065dfb87fab",
        "parentId" : null,
        "authorId" : "36d7f8b5-3497-43b0-96eb-612a05dccdf8",
        "body" : "I think this is an improvement, but I think the error described in the issue may be misleading as I am also seeing this: `Jun  9 12:22:18.370: failed pod observation expectations: rs1 had more than 2 pods created: 3`",
        "createdAt" : "2020-06-09T19:45:47Z",
        "updatedAt" : "2020-06-09T23:09:10Z",
        "lastEditedBy" : "36d7f8b5-3497-43b0-96eb-612a05dccdf8",
        "tags" : [
        ]
      },
      {
        "id" : "6a52f855-ba35-4a1c-9e08-ea0cd9a954d9",
        "parentId" : "ecfc4ae0-f960-4fee-a332-8065dfb87fab",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Are you referring to another test failure (other than `basic preemption works`)? That may be related (but I need to confirm) - the node is not updated successfully with proper \"fake resource\", hence the rs doesn't get expected replicas scheduled. But anyway, let focus on the `basic preemption works` failure in this PR.",
        "createdAt" : "2020-06-09T20:07:47Z",
        "updatedAt" : "2020-06-09T23:09:10Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "46e74749-66cc-43d4-ac80-4725f2cfe5e3",
        "parentId" : "ecfc4ae0-f960-4fee-a332-8065dfb87fab",
        "authorId" : "36d7f8b5-3497-43b0-96eb-612a05dccdf8",
        "body" : "Ah yes, this new failure appears to be different. I will open a separate issue :+1: ",
        "createdAt" : "2020-06-09T20:10:09Z",
        "updatedAt" : "2020-06-09T23:09:10Z",
        "lastEditedBy" : "36d7f8b5-3497-43b0-96eb-612a05dccdf8",
        "tags" : [
        ]
      },
      {
        "id" : "338c891e-d6a2-4c7c-8877-d5c6680b3e11",
        "parentId" : "ecfc4ae0-f960-4fee-a332-8065dfb87fab",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Please, thanks - I'm aware of that one: \"PreemptionExecutionPath runs ReplicaSets \". Hope this one fixes that one as well.",
        "createdAt" : "2020-06-09T20:23:21Z",
        "updatedAt" : "2020-06-09T23:09:10Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "43da0fff291a5c4b47215df63dd734786d3f105f",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +737,741 @@}\n\nfunc patchNode(client clientset.Interface, old *v1.Node, new *v1.Node) error {\n\toldData, err := json.Marshal(old)\n\tif err != nil {"
  },
  {
    "id" : "d203d45e-871e-42b4-9bae-ebdd3780062e",
    "prId" : 91955,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91955#pullrequestreview-428236706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2373b109-ccfe-40fd-a612-e6baa70b4064",
        "parentId" : null,
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "I'm fine with this change, it should have been this way to begin with.  This probably landed before the verify job started enforcing the no-skips requirement for conformance",
        "createdAt" : "2020-06-10T02:57:12Z",
        "updatedAt" : "2020-06-10T02:58:16Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      },
      {
        "id" : "a80ec32a-6dcb-40c9-a316-ea4e9ec2be90",
        "parentId" : "2373b109-ccfe-40fd-a612-e6baa70b4064",
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "Yeah, that is a nice point.\r\nAll conformance tests should be operated without considering any conditions.\r\nSo it seems fine to replace Skipf with Failf to know the conformance test is operated always.\r\n\r\nBTW I found a small thing here.\r\nWe need to put a space after `but` to distinguish it from later `all` word.",
        "createdAt" : "2020-06-10T05:51:19Z",
        "updatedAt" : "2020-06-10T05:51:32Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      },
      {
        "id" : "3542d614-255d-46cb-8132-a6f3d390db88",
        "parentId" : "2373b109-ccfe-40fd-a612-e6baa70b4064",
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "/lgtm\r\n\r\nanyways :-) ",
        "createdAt" : "2020-06-10T05:52:53Z",
        "updatedAt" : "2020-06-10T05:52:54Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      },
      {
        "id" : "6eb113bc-2c2e-4715-b9be-5862fa2c04e7",
        "parentId" : "2373b109-ccfe-40fd-a612-e6baa70b4064",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> BTW I found a small thing here.\r\nWe need to put a space after but to distinguish it from later all word.\r\n\r\nThanks for pointing out. Let's get it merged first to start observing the flake.",
        "createdAt" : "2020-06-10T16:35:21Z",
        "updatedAt" : "2020-06-10T16:35:22Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "43da0fff291a5c4b47215df63dd734786d3f105f",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +248,252 @@\t\t}\n\t\tif len(pods) < 2 {\n\t\t\tframework.Failf(\"We need at least two pods to be created but\" +\n\t\t\t\t\"all nodes are already heavily utilized, so preemption tests cannot be run\")\n\t\t}"
  },
  {
    "id" : "e9286ab0-e7a6-48aa-996f-af80cebbdb33",
    "prId" : 90740,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90740#pullrequestreview-408218762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93f281b8-1b2d-4c15-bfa6-f17efee617de",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Do we need `Limits`?",
        "createdAt" : "2020-05-08T00:32:30Z",
        "updatedAt" : "2020-05-08T19:53:50Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "38ecc107-1e2b-494d-a930-d6e493e7ff68",
        "parentId" : "93f281b8-1b2d-4c15-bfa6-f17efee617de",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "Yeah, without them I got an error that non-over-committable resources need to have a limit set",
        "createdAt" : "2020-05-08T12:57:48Z",
        "updatedAt" : "2020-05-08T19:53:50Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      }
    ],
    "commit" : "49464d9ec35ac710329cc43b5321cf25f96c8e7d",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +171,175 @@\t\t\tResources: &v1.ResourceRequirements{\n\t\t\t\tRequests: podRes,\n\t\t\t\tLimits:   podRes,\n\t\t\t},\n\t\t})"
  },
  {
    "id" : "f993ab60-2139-48f7-8775-8bf64f31145c",
    "prId" : 88681,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88681#pullrequestreview-367476799",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f68ae9b9-be77-4c30-8047-9d5b56a914be",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Add a comment to explain why we poll instead of doing a one-time check. Also, isn't there any function like this in the framework?",
        "createdAt" : "2020-03-02T18:17:18Z",
        "updatedAt" : "2020-03-03T00:25:22Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "41492be3-34a1-4af3-9fb7-c9fb9f7bc323",
        "parentId" : "f68ae9b9-be77-4c30-8047-9d5b56a914be",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Seems there isn't any in the framework :(",
        "createdAt" : "2020-03-02T18:22:34Z",
        "updatedAt" : "2020-03-03T00:25:22Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "1c2339e6-f41c-4f21-b727-fef192e115de",
        "parentId" : "f68ae9b9-be77-4c30-8047-9d5b56a914be",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "if we don't need another OWNER approval, I would suggest to just add `WaitForNumberOfRunningPods` or something like that. It would make the test more readable.",
        "createdAt" : "2020-03-02T18:28:24Z",
        "updatedAt" : "2020-03-03T00:25:22Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "d7f10da1-c785-4c43-9f3a-60d87ee1f924",
        "parentId" : "f68ae9b9-be77-4c30-8047-9d5b56a914be",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "maybe in a followup, I want it to be merged ASAP so as not to impact the release timeline.",
        "createdAt" : "2020-03-02T18:36:12Z",
        "updatedAt" : "2020-03-03T00:25:22Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "aea4aab8-b552-4d1d-a8c1-abc8688969ed",
        "parentId" : "f68ae9b9-be77-4c30-8047-9d5b56a914be",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "sounds good. But just add the comment here. Maybe:\r\n\r\n```Wait until the number of pods stabilizes. Note that `medium` pod can get scheduled once the second low priority pod is marked as terminating.```",
        "createdAt" : "2020-03-02T18:44:21Z",
        "updatedAt" : "2020-03-03T00:25:22Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "8ac8f94b-0c42-4b65-b821-0370b7e31792",
        "parentId" : "f68ae9b9-be77-4c30-8047-9d5b56a914be",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Thanks. Done.",
        "createdAt" : "2020-03-02T19:45:19Z",
        "updatedAt" : "2020-03-03T00:25:22Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "9902021ccd0464130e2910b491825ae27009776c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +403,407 @@\t\t\t// second low priority pod is marked as terminating.\n\t\t\t// TODO: exact the wait.PollImmediate block to framework.WaitForNumberOfRunningPods.\n\t\t\terr := wait.PollImmediate(framework.Poll, framework.PollShortTimeout, func() (bool, error) {\n\t\t\t\tpodList, err := cs.CoreV1().Pods(ns).List(context.TODO(), metav1.ListOptions{})\n\t\t\t\t// ignore intermittent network error"
  },
  {
    "id" : "da17c267-215b-4a44-99b0-97a3df07e937",
    "prId" : 88556,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88556#pullrequestreview-365278944",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8363391-cf67-472a-877c-200f0c4db764",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "How come this was passing before?",
        "createdAt" : "2020-02-26T15:37:11Z",
        "updatedAt" : "2020-02-27T03:06:14Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "1aab09c2-5e11-4374-a42d-2af79252520b",
        "parentId" : "d8363391-cf67-472a-877c-200f0c4db764",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "No idea. I'm curious whether it really ran before - which looks like an obvious bug.",
        "createdAt" : "2020-02-26T17:32:50Z",
        "updatedAt" : "2020-02-27T03:06:14Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "9ed28d0d-358b-47e8-b6a3-e40343bcc08d",
        "parentId" : "d8363391-cf67-472a-877c-200f0c4db764",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "This is an obvious fix, but I'm not so sure about the other one. The scheduler should work if some pods are directly assigned. Unless this is related to #88389 (the fact that we have different event handlers).",
        "createdAt" : "2020-02-26T21:34:13Z",
        "updatedAt" : "2020-02-27T03:06:14Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "1d65aeeb-7b07-4e1f-80a9-2ca2ba9ba3ee",
        "parentId" : "d8363391-cf67-472a-877c-200f0c4db764",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Yup, I don't have clear clues why the other fix _can_ resolve the problem in my env. Let me dig more.",
        "createdAt" : "2020-02-26T22:06:34Z",
        "updatedAt" : "2020-02-27T03:06:14Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef786c9fa2d9d6d9fcc0cad635ff00b4bcd4108d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +215,219 @@\t\t\t\tpriorityName = lowPriorityClassName\n\t\t\t}\n\t\t\tpods = append(pods, createPausePod(f, pausePodConfig{\n\t\t\t\tName:              fmt.Sprintf(\"pod%d-%v\", i, priorityName),\n\t\t\t\tPriorityClassName: priorityName,"
  },
  {
    "id" : "a11673d4-89ec-4ca1-b906-0c446624189f",
    "prId" : 88105,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88105#pullrequestreview-359259154",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b975ff41-b0d3-420d-bad8-f8c94913e9a3",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "is this reliable? will kubelet erase this since no such resource actually exist? ",
        "createdAt" : "2020-02-14T19:35:20Z",
        "updatedAt" : "2020-02-15T00:48:49Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "3b3ab29e-c31b-4fa0-bc3b-5a065ad10f58",
        "parentId" : "b975ff41-b0d3-420d-bad8-f8c94913e9a3",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> will kubelet erase this since no such resource actually exist?\r\n\r\nFor extended resources, no. We used to exercise this pattern for another test here:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/25651408aeadf38c3df7ea8c760e7519fd37d625/test/e2e/scheduling/preemption.go#L280",
        "createdAt" : "2020-02-14T22:39:58Z",
        "updatedAt" : "2020-02-15T00:50:58Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "c93dffdfc44b3a1259129a7d925e588fa9fe40f1",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +294,298 @@\t\t\t\tnodeCopy.ResourceVersion = \"0\"\n\t\t\t\tginkgo.By(fmt.Sprintf(\"Apply 10 fake resource to node %v.\", node.Name))\n\t\t\t\tnodeCopy.Status.Capacity[fakeRes] = resource.MustParse(\"10\")\n\t\t\t\tnode, err = cs.CoreV1().Nodes().UpdateStatus(context.TODO(), nodeCopy, metav1.UpdateOptions{})\n\t\t\t\tframework.ExpectNoError(err)"
  },
  {
    "id" : "aae360f7-2b9b-4678-bcd7-11c3d80af12f",
    "prId" : 86400,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86400#pullrequestreview-334676658",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2aaf844-5cab-4ad0-b422-058477284228",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "This will return as soon as NominatedNodeName is set on the pod. that happens *before* preempted pods are deleted, which means that this will race with observing the additional rescheduled rs pods in podNamesSeen.\r\n\r\nShouldn't waitForPreemptingWithTimeout wait for `spec.nodeName` to be set instead?",
        "createdAt" : "2019-12-19T05:25:02Z",
        "updatedAt" : "2020-01-13T17:40:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "0ff05bf7-72bb-4dbd-a5f3-f91daa2afcd4",
        "parentId" : "e2aaf844-5cab-4ad0-b422-058477284228",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I can't think of a good way except for adding a sleeping internal inbetween. As when the podNamesSeen equals to a number for the first time, we can't assert that it's a success - the number may increase which means an unnecessary preemption which is what this case wants to cover.\r\n\r\n> Shouldn't waitForPreemptingWithTimeout wait for spec.nodeName to be set instead?\r\n\r\nUpdated.",
        "createdAt" : "2019-12-19T14:41:55Z",
        "updatedAt" : "2020-01-13T17:40:02Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "4083c7d49c813846c5071f150ddb1a2a6052a197",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +410,414 @@\t\t}\n\t\tpreemptorPod := createPod(f, preemptorPodConf)\n\t\twaitForPreemptingWithTimeout(f, preemptorPod, framework.PodGetTimeout)\n\n\t\tframework.Logf(\"pods created so far: %v\", podNamesSeen)"
  },
  {
    "id" : "b0760fe0-9148-47e7-adbc-56f387c08a82",
    "prId" : 86400,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86400#pullrequestreview-340106511",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2ed1fbb-4251-4a03-a9db-3f2aecefe81d",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "should this be:\r\n```\r\nif got > expected {\r\n  framework.Failf(\"pods of ReplicaSet%d have been over-preempted: expect %v pod names, but got %d\", i+1, expected, got)\r\n} else if got < expected {\r\n  framework.Failf(\"pods of ReplicaSet%d have been under-preempted: expect %v pod names, but got %d\", i+1, expected, got)\r\n}\r\n```",
        "createdAt" : "2020-01-08T16:44:04Z",
        "updatedAt" : "2020-01-13T17:40:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "c48afc6a-948c-49d7-8bc7-b0e0cfc2316a",
        "parentId" : "a2ed1fbb-4251-4a03-a9db-3f2aecefe81d",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "This is better.",
        "createdAt" : "2020-01-08T19:53:46Z",
        "updatedAt" : "2020-01-13T17:40:02Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "4083c7d49c813846c5071f150ddb1a2a6052a197",
    "line" : 199,
    "diffHunk" : "@@ -1,1 +445,449 @@\t\t\t} else if got > expectedRSPods[i] {\n\t\t\t\tframework.Failf(\"pods of ReplicaSet%d have been over-preempted: expect %v pod names, but got %d\", i+1, expectedRSPods[i], got)\n\t\t\t}\n\t\t}\n\t})"
  },
  {
    "id" : "e7ec4fe4-c616-4bde-b2b8-a0853df862a5",
    "prId" : 86400,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86400#pullrequestreview-340723348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "418a8203-0c6d-4470-82bd-86dc0660ed8b",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "hmm... if we want to make sure this actually triggered an eviction, I guess we should wait for both nominatedNodeName and nodeName:\r\n\r\n```\r\nif len(pod.Spec.NominatedNodeName) == 0 {\r\n  // log no preemption triggered yet\r\n  return false, nil\r\n}\r\nif len(pod.Spec.NodeName) == 0 {\r\n  // log preemption not complete yet\r\n  return false, nil\r\n}\r\n```",
        "createdAt" : "2020-01-09T17:53:01Z",
        "updatedAt" : "2020-01-13T17:40:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "2145bd88-f082-4086-8a77-083aaa624d81",
        "parentId" : "418a8203-0c6d-4470-82bd-86dc0660ed8b",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "But the chances are \"pod.Spec.NominatedNodeName\" might be set and unset within our checking interval - only \"pod.Spec.NodeName\" is the final stable state.",
        "createdAt" : "2020-01-09T18:23:16Z",
        "updatedAt" : "2020-01-13T17:40:02Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "06d64eee-0b53-4922-b89d-94250b674ade",
        "parentId" : "418a8203-0c6d-4470-82bd-86dc0660ed8b",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "ah, I didn't realize nominated node name got unset. ok.",
        "createdAt" : "2020-01-09T18:25:14Z",
        "updatedAt" : "2020-01-13T17:40:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "4083c7d49c813846c5071f150ddb1a2a6052a197",
    "line" : 229,
    "diffHunk" : "@@ -1,1 +510,514 @@\t\t\treturn false, err\n\t\t}\n\t\tif len(pod.Spec.NodeName) > 0 {\n\t\t\treturn true, nil\n\t\t}"
  },
  {
    "id" : "f1edf724-6591-4c77-b3dd-f0aa03c2ba2b",
    "prId" : 82350,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82350#pullrequestreview-349671590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6556e0e3-7796-404e-afc3-f33e115e63f6",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I think this is fine, someone running this test on heavily utilized clusters will at least get a clear reason.  We *may* in the future want to wait a bit and retry, but we can always measure the impact of this (and because it's serial it should be rare that it flakes and either passes or fails).",
        "createdAt" : "2020-01-28T20:15:29Z",
        "updatedAt" : "2020-01-29T16:54:35Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "99b301d7e1aaf2b2632b17df769ddcc862a24ee6",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +144,148 @@\t\t}\n\t\tif len(pods) < 2 {\n\t\t\tframework.Failf(\"We need at least two pods to be created but\" +\n\t\t\t\t\"all nodes are already heavily utilized, so preemption tests cannot be run\")\n\t\t}"
  },
  {
    "id" : "7a746176-10d1-4a31-b9c2-fc85049b0570",
    "prId" : 82350,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82350#pullrequestreview-350285099",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96af6b7a-d5af-43bf-9173-05ac6cea5d77",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "This doesn't take into account init containers, so there's a small chance that this fails because a larger init container than regular container is running.  I think it's ok for now, but we should fix this in a separate issue.  Resources used for a pod is `max(max(init_container usage), sum(container usage))`",
        "createdAt" : "2020-01-28T20:23:42Z",
        "updatedAt" : "2020-01-29T16:54:35Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "8b48b081-b29e-4f45-b981-5ae453e33eb3",
        "parentId" : "96af6b7a-d5af-43bf-9173-05ac6cea5d77",
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "Why non-zero requests? Why not just ignore resource requests which are not set?",
        "createdAt" : "2020-01-29T16:14:05Z",
        "updatedAt" : "2020-01-29T16:54:35Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      },
      {
        "id" : "f3ba8143-8dda-4c53-a4f6-37ca9ac3b09a",
        "parentId" : "96af6b7a-d5af-43bf-9173-05ac6cea5d77",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "Sorry, what is the difference between them? It seems like this function is essentially just trying to get the requests, not caring about zero (or unset) because they don't add into the usage. Is that right?\r\n\r\nEdit: I see what you mean, `nonZeroRequests` assumes the default request if none is set, which is only applicable during scheduling. However, not considering anything for containers with no request set doesn't help us consider the actual usage on the node. So in this case, I think assuming the default will at least get us a closer approximation than not",
        "createdAt" : "2020-01-29T16:56:30Z",
        "updatedAt" : "2020-01-29T17:32:44Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "41196cb9-e86b-49b2-8d76-0c87b994aeef",
        "parentId" : "96af6b7a-d5af-43bf-9173-05ac6cea5d77",
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "`GetNonzeroRequests` which is called by `getNonZeroRequests` and implemented in https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/util/non_zero.go adds +100m or +200MB in case cpu/memory resource requests on a container is not set. So `getNonZeroRequests` can give you higher usage on a node than it is.",
        "createdAt" : "2020-01-29T17:21:10Z",
        "updatedAt" : "2020-01-29T17:21:56Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      }
    ],
    "commit" : "99b301d7e1aaf2b2632b17df769ddcc862a24ee6",
    "line" : 200,
    "diffHunk" : "@@ -1,1 +571,575 @@\t\t\tcontinue\n\t\t}\n\t\tresult := getNonZeroRequests(&pod)\n\t\ttotalRequestedCPUResource += result.MilliCPU\n\t\ttotalRequestedMemResource += result.Memory"
  },
  {
    "id" : "36b42b35-05f1-4bcc-b9d9-7c7656611ece",
    "prId" : 80545,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80545#pullrequestreview-323006851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6cb4e26a-4b80-4755-90a1-6c22402422a0",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Looking at the code for this test, the calculation to request 40% of allocatable memory is not really a safe check.  It should be subtracting the currently scheduled CPU requests on that node from allocatable, then taking 40%.",
        "createdAt" : "2019-11-26T06:19:02Z",
        "updatedAt" : "2020-05-21T06:05:31Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "de4d9f63-3a75-48e3-990c-6a07a7aed2b0",
        "parentId" : "6cb4e26a-4b80-4755-90a1-6c22402422a0",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "> It should be subtracting the currently scheduled CPU requests on that node from allocatable, then taking 40%.\r\n\r\n@smarterclayton in origin, we [do currently do that](https://github.com/openshift/origin/blob/master/vendor/k8s.io/kubernetes/test/e2e/scheduling/preemption.go#L111-L117), but it still flakes (if you look at my [origin PR above](https://github.com/openshift/origin/pull/23758), I temporarily attempt to revert those changes). I also have had a PR open for a while now in kubernetes to update this as well: https://github.com/kubernetes/kubernetes/pull/82350",
        "createdAt" : "2019-11-26T14:10:25Z",
        "updatedAt" : "2020-05-21T06:05:31Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      }
    ],
    "commit" : "4857893a0c99737f4693b150dc0f84e8c5d602d5",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +111,115 @@\t\tschedule the high priority pod.\n\t*/\n\tframework.ConformanceIt(\"validates basic preemption works\", func() {\n\t\tvar podRes v1.ResourceList\n"
  },
  {
    "id" : "d049681a-f05f-4ffc-b1af-f967c7f706aa",
    "prId" : 80545,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80545#pullrequestreview-391227895",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1edbe88a-9e16-49ff-b332-34460aa96d9e",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "nit: Would you mind correcting the typo in L125: `let not's` -> `let's not`.\r\n\r\n",
        "createdAt" : "2020-04-08T04:34:44Z",
        "updatedAt" : "2020-05-21T06:05:31Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "d9716799-e167-4022-9f03-44ecfde412b2",
        "parentId" : "1edbe88a-9e16-49ff-b332-34460aa96d9e",
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "done",
        "createdAt" : "2020-04-10T01:45:14Z",
        "updatedAt" : "2020-05-21T06:05:31Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      }
    ],
    "commit" : "4857893a0c99737f4693b150dc0f84e8c5d602d5",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +104,108 @@\t})\n\n\t/*\n\t\tRelease : v1.19\n\t\tTestname: Scheduler, Basic Preemption"
  },
  {
    "id" : "2977c39f-6eea-4d63-94cf-6c79d67d7998",
    "prId" : 71281,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71281#pullrequestreview-179601506",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ac15de3-df5c-45da-8233-e5fc4eeb3619",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Does this resource stays on the node object or it goes away with the next node update?",
        "createdAt" : "2018-11-29T00:43:34Z",
        "updatedAt" : "2018-11-29T18:33:13Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "f3db137b-fceb-4cd8-a5e8-1080b8ab0557",
        "parentId" : "5ac15de3-df5c-45da-8233-e5fc4eeb3619",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "It stays. Kubelet seems to have no interests on this kind of extended resource.",
        "createdAt" : "2018-11-29T02:01:23Z",
        "updatedAt" : "2018-11-29T18:33:13Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9f588f05227da8fb4a7de6353f34cc490f74d7f",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +419,423 @@\t\t// force it to update\n\t\tnodeCopy.ResourceVersion = \"0\"\n\t\tnodeCopy.Status.Capacity[fakecpu] = resource.MustParse(\"800\")\n\t\tnode, err = cs.CoreV1().Nodes().UpdateStatus(nodeCopy)\n\t\tframework.ExpectNoError(err)"
  },
  {
    "id" : "90cb9a66-d64a-401c-816a-cc38d41f92fb",
    "prId" : 71281,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71281#pullrequestreview-180031542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04a3daf0-fef4-442c-a888-de140fe56441",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "We may want to move everything below this line to util.go.",
        "createdAt" : "2018-11-29T01:57:58Z",
        "updatedAt" : "2018-11-29T18:33:13Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "c633f317-3424-47dd-9db0-4e7199f43cd0",
        "parentId" : "04a3daf0-fef4-442c-a888-de140fe56441",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "which `util.go` do you mean specifically?",
        "createdAt" : "2018-11-29T18:00:00Z",
        "updatedAt" : "2018-11-29T18:33:13Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "6caed0f9-7c57-416c-9d2d-7ef8569e0312",
        "parentId" : "04a3daf0-fef4-442c-a888-de140fe56441",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Never mind. We have a util.go in our integration tests, but not in e2e tests.",
        "createdAt" : "2018-11-29T21:49:31Z",
        "updatedAt" : "2018-11-29T21:57:01Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9f588f05227da8fb4a7de6353f34cc490f74d7f",
    "line" : 264,
    "diffHunk" : "@@ -1,1 +561,565 @@})\n\ntype pauseRSConfig struct {\n\tReplicas  int32\n\tPodConfig pausePodConfig"
  },
  {
    "id" : "fef54a95-5afe-442b-9e42-b3ea300c9218",
    "prId" : 68621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/68621#pullrequestreview-159290126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "154b8504-c60e-4747-b41b-52e0863c6cb6",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "The previous defer kind of made sense, b/c there were error conditions that could be hit before the cleanup.  This one makes less sense to me.  Is there a specific condition you are looking to prevent? ",
        "createdAt" : "2018-09-17T14:26:36Z",
        "updatedAt" : "2018-09-17T14:26:46Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "65f2d945-bce9-4dd3-af9c-2047d460cfa1",
        "parentId" : "154b8504-c60e-4747-b41b-52e0863c6cb6",
        "authorId" : "a35534f1-fb58-44cc-9ab8-ef5faa669c34",
        "body" : "Since there is an `Expect` statement before the clean up I considered there was a possibility for the test to finish without calling deleting the pod.",
        "createdAt" : "2018-09-18T11:39:06Z",
        "updatedAt" : "2018-09-18T11:39:07Z",
        "lastEditedBy" : "a35534f1-fb58-44cc-9ab8-ef5faa669c34",
        "tags" : [
        ]
      },
      {
        "id" : "12c339a3-f321-4988-bede-971ed17bf9f2",
        "parentId" : "154b8504-c60e-4747-b41b-52e0863c6cb6",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "This is inside a loop. This is not equivalent to the original code. These pods would no longer be deleted inside the loop. Their deletion would be deferred to the time that the function returns.",
        "createdAt" : "2018-09-26T21:13:43Z",
        "updatedAt" : "2018-09-26T21:13:49Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "e4e88929-44b3-4ab4-bb11-00d3070213c6",
        "parentId" : "154b8504-c60e-4747-b41b-52e0863c6cb6",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "> These pods would no longer be deleted inside the loop. \r\n\r\nThere'll be just one additional BE pod here, so I think that should be fine :)",
        "createdAt" : "2018-09-27T05:42:36Z",
        "updatedAt" : "2018-09-27T05:42:36Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "52f87963a472e1ba22223d137ee0ace4efdd6b57",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +344,348 @@\t\t\t\tPriorityClassName: spc,\n\t\t\t})\n\t\t\tdefer func() {\n\t\t\t\t// Clean-up the pod.\n\t\t\t\terr := f.ClientSet.CoreV1().Pods(pod.Namespace).Delete(pod.Name, metav1.NewDeleteOptions(0))"
  },
  {
    "id" : "5340bef8-9ddf-413a-821b-453d7a484dbf",
    "prId" : 58835,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58835#pullrequestreview-93183275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63a04b0a-1baf-44be-be71-78f581c2a9a2",
        "parentId" : null,
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "s/preemption/preempted",
        "createdAt" : "2018-02-01T07:10:30Z",
        "updatedAt" : "2018-02-22T01:53:41Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7da5a2e4dd6a2752d2a2d45189656d5536d9258e",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +130,134 @@\t// enough resources is found, scheduler preempts a lower priority pod to schedule\n\t// this critical pod.\n\tIt(\"validates lower priority pod preemption by critical pod\", func() {\n\t\tvar podRes v1.ResourceList\n\t\t// Create one pod per node that uses a lot of the node's resources."
  },
  {
    "id" : "72f9dde7-3aca-408f-bd48-226b6afbd5a2",
    "prId" : 58835,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58835#pullrequestreview-93183623",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "166ebdaa-0c81-43ab-92c5-56e2d00717d7",
        "parentId" : null,
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "we have an func `createBalancedPodForNodes` in priority.go ,not sure whether it fits you",
        "createdAt" : "2018-02-01T07:12:52Z",
        "updatedAt" : "2018-02-22T01:53:41Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7da5a2e4dd6a2752d2a2d45189656d5536d9258e",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +134,138 @@\t\t// Create one pod per node that uses a lot of the node's resources.\n\t\tBy(\"Create pods that use 60% of node resources.\")\n\t\tpods := make([]*v1.Pod, len(nodeList.Items))\n\t\tfor i, node := range nodeList.Items {\n\t\t\tcpuAllocatable, found := node.Status.Allocatable[\"cpu\"]"
  },
  {
    "id" : "de2b0bc1-e65e-478f-bc52-bd3f5e13a7a5",
    "prId" : 51781,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51781#pullrequestreview-62441096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Isn't this a pretty expensive test if you have a large cluster, do we want it defaulted on or behind [Feature] ? ",
        "createdAt" : "2017-09-12T18:55:57Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "84f4cd5a-6b61-4d11-94b7-f9032984ba16",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "It is behind [Feature] right now. How can we prevent it from being used in scale tests?",
        "createdAt" : "2017-09-12T20:21:02Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "77b2ebc6-618c-4c8d-bf38-57da89b48a95",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Ahh I scrolled back and see it now... \r\n\r\nIs there any reason you need to hit * nodes vs a subset?  It's still really expensive. ",
        "createdAt" : "2017-09-12T20:50:58Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "c8c25883-8a90-42b4-8357-45e5cc7f95bc",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Yes, there is a reason. In order to trigger preemption, cluster should be under resource pressure.",
        "createdAt" : "2017-09-12T21:33:01Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "e6c3f01c-dcf6-4392-8f78-5f6640fad4ee",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Can't you use node selectors to force it?  ",
        "createdAt" : "2017-09-12T22:07:08Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "87035224-4e10-4c3e-be3b-b9fd76b280d2",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Yes, I could, but the current scenario is better, as it also exercises node scoring and ensures that we preempt the right pod no matter where in the cluster it is scheduled and also makes sure that other pods are not impacted. Node selector would remove these benefits.\r\nWhat I heard is that the CI clusters typically have 4-5 nodes, except those that run scalability tests. If this is true, then the test shouldn't be very time consuming.",
        "createdAt" : "2017-09-12T22:37:34Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "6f6b6946-ff42-4b0a-9ca5-ffba7f57ba12",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "What I'm really concerned about is the e2e tests are used in the field to validate clusters.  There are times when they want to validate that features are working properly.  If I was company (X) trying to validate a 200 node cluster you'd be loading their cluster and this is a very expensive test.  The usage of the e2e's goes well beyond upstream CI. ",
        "createdAt" : "2017-09-12T23:27:43Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "9e9c6478-6afe-42b9-90d1-bcd7bf93e581",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Perhaps using a [Disruptive] tag on this test. \r\n\r\nOr some other tag. \r\n",
        "createdAt" : "2017-09-12T23:33:36Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "c81c2964-3142-4a98-80c8-49334ceb4299",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I see your point. I just changed the test to create a maximum of 4 pods.",
        "createdAt" : "2017-09-13T00:00:31Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "90735c8c-733d-4d41-8f84-9dbf68bf24fe",
        "parentId" : "7c3936fe-df39-422f-bdd7-ac6236c34b2a",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "thx for the change. ",
        "createdAt" : "2017-09-13T13:41:45Z",
        "updatedAt" : "2017-09-13T19:12:23Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "f11b0a65d10a86965aacda0d02b41514af775d27",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +132,136 @@\t// It also verifies that existing low priority pods are not preempted as their\n\t// preemption wouldn't help.\n\tIt(\"validates pod anti-affinity works in preemption\", func() {\n\t\tvar podRes v1.ResourceList\n\t\t// Create a few pods that uses a small amount of resources."
  },
  {
    "id" : "c875bd45-f971-449d-b023-bd7f2e2f7b31",
    "prId" : 50949,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50949#pullrequestreview-59777004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a630e7d2-1571-4263-ba26-271e54aa0619",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I guess because serial test does not enable alpha feature gate for preemption, CI will not actually run this test?\r\n",
        "createdAt" : "2017-08-27T00:19:07Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "fa66d3b5-6c44-42d3-a1a2-db3ae76e8643",
        "parentId" : "a630e7d2-1571-4263-ba26-271e54aa0619",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I think you are right. I will look into this.",
        "createdAt" : "2017-08-29T00:40:34Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "13f5c67c-50c8-4512-a783-58152f75372a",
        "parentId" : "a630e7d2-1571-4263-ba26-271e54aa0619",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Yeah you should probably ask on the [kubernetes-sig-testing](https://groups.google.com/forum/#!forum/kubernetes-sig-testing) mailing list. They might tell you to create a separate Jenkins job just for the e2e tests for this feature while it is in alpha, and then merge it back into regular serial once it goes to beta and is no longer flag-gated.",
        "createdAt" : "2017-08-29T07:41:49Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "55c48a57-9929-4c18-9a56-08ce1f60b0f6",
        "parentId" : "a630e7d2-1571-4263-ba26-271e54aa0619",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I sent the email. Will make the changes once I learn how to do it.",
        "createdAt" : "2017-08-29T18:26:17Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "25db9002-5638-434b-9bfb-2894d7ed77d1",
        "parentId" : "a630e7d2-1571-4263-ba26-271e54aa0619",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "There already is a gce and gke alpha suite.  Just tag the tests with \"[Feature:Preemption]\" and modify the [config](https://github.com/kubernetes/test-infra/blob/master/jobs/config.json) for the alpha suites to add the tag.",
        "createdAt" : "2017-08-31T06:43:21Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "afaec2c0-9780-48e0-aeeb-af16b7519071",
        "parentId" : "a630e7d2-1571-4263-ba26-271e54aa0619",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Yes, that's what I just did: https://github.com/kubernetes/test-infra/pull/4285",
        "createdAt" : "2017-08-31T07:36:21Z",
        "updatedAt" : "2017-09-07T22:32:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0b718373befb2168befc0c4579fd9a02155d5bc",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +-1,3 @@/*\nCopyright 2017 The Kubernetes Authors.\n"
  }
]