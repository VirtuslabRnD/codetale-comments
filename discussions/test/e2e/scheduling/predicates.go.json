[
  {
    "id" : "fc0503bf-fc38-4827-a814-063de166f484",
    "prId" : 94684,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94684#pullrequestreview-573472334",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ce15068-8e42-4dd4-b2c5-f047a332b55d",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "why is this change necessary?",
        "createdAt" : "2021-01-21T15:05:42Z",
        "updatedAt" : "2021-01-21T16:08:47Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "aff998b1-2fc8-4ed1-b3b1-a634f6c31082",
        "parentId" : "9ce15068-8e42-4dd4-b2c5-f047a332b55d",
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "`testutils.StartPods` modifies Labels field. `Labels: conf.Labels` is a shallow copy and `conf.Labels` is a [shallow copy](https://github.com/kubernetes/kubernetes/blob/64c099d67069f85f2b9b78d7d12fd1a92fcc3d75/test/e2e/scheduling/priorities.go#L464) of `balancePodLabel`. Every call to `testutils.StartPods` then modifies `balancePodLabel` and disallows `balancePodLabel` to be used for listing all pods created for memory balancing.",
        "createdAt" : "2021-01-21T15:43:24Z",
        "updatedAt" : "2021-01-21T16:08:47Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      },
      {
        "id" : "e4c4da54-bf94-4bd6-bbab-d01f50a18bc9",
        "parentId" : "9ce15068-8e42-4dd4-b2c5-f047a332b55d",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "sg",
        "createdAt" : "2021-01-21T15:52:19Z",
        "updatedAt" : "2021-01-21T16:08:47Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "23189922271c080dabb4b9ec36831d972e52534d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +888,892 @@\t\t\tName:            conf.Name,\n\t\t\tNamespace:       conf.Namespace,\n\t\t\tLabels:          map[string]string{},\n\t\t\tAnnotations:     map[string]string{},\n\t\t\tOwnerReferences: conf.OwnerReferences,"
  },
  {
    "id" : "82bc1ac5-13fc-46c3-a8df-5c81388546b5",
    "prId" : 92450,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92450#pullrequestreview-436751277",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03faebc3-60a0-4b58-b630-37c3dc68ecc2",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "How about updating the parameter `masterNodes` of WaitForStableCluster() and getScheduledAndUnscheduledPods to `workerNodes`?",
        "createdAt" : "2020-06-24T06:15:57Z",
        "updatedAt" : "2020-06-24T15:22:06Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "8c98fb69-04b4-4470-a33f-0af14c0a9d85",
        "parentId" : "03faebc3-60a0-4b58-b630-37c3dc68ecc2",
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "Nice point, I forgot to update it (^^;)",
        "createdAt" : "2020-06-24T15:17:32Z",
        "updatedAt" : "2020-06-24T15:22:06Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "5edf15ea974851c12da97991e09125d5ffe327c5",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +133,137 @@\t\t\t}\n\t\t}\n\t\tWaitForStableCluster(cs, workerNodes)\n\n\t\tpods, err := cs.CoreV1().Pods(metav1.NamespaceAll).List(context.TODO(), metav1.ListOptions{})"
  },
  {
    "id" : "b90b5e96-2ff0-49fa-a498-56ec42312a22",
    "prId" : 88556,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88556#pullrequestreview-365355572",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b152840-f09d-4971-b6c4-d83542bebcc2",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "why do we want this to go through the scheduler?",
        "createdAt" : "2020-02-26T15:39:16Z",
        "updatedAt" : "2020-02-27T03:06:14Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "1dc1aff5-25d3-454a-a2dc-0dfe5a0bd1ec",
        "parentId" : "5b152840-f09d-4971-b6c4-d83542bebcc2",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "To not block the main release, this acts more as a workaround. So I'm extracting this section into a separate commit and leave a TODO so as to buy us some time to find out the real root cause in the future.",
        "createdAt" : "2020-02-27T01:12:31Z",
        "updatedAt" : "2020-02-27T03:06:14Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef786c9fa2d9d6d9fcc0cad635ff00b4bcd4108d",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +843,847 @@\t// TODO: setting the Pod's nodeAffinity instead of setting .spec.nodeName works around the\n\t// Preemption e2e flake (#88441), but we should investigate deeper to get to the bottom of it.\n\tif len(conf.NodeName) != 0 {\n\t\te2epod.SetNodeAffinity(&pod.Spec, conf.NodeName)\n\t}"
  },
  {
    "id" : "7bb49741-45e9-46bb-9599-895188e92929",
    "prId" : 87215,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87215#pullrequestreview-362414903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fa3763a-ace2-4e4d-8ffa-ba9c62619c79",
        "parentId" : null,
        "authorId" : "d3b9ec12-6001-425a-940b-b74c57282ba6",
        "body" : "```\r\n/b/f/w/test/e2e/scheduling/predicates.go:231:16: undefined: node\r\ncompilepkg: error running subcommand: exit status 2\r\n```\r\n\r\nref: https://prow.k8s.io/view/gcs/kubernetes-jenkins/pr-logs/pull/87215/pull-kubernetes-bazel-build/1230641889611354112#1:build-log.txt%3A55",
        "createdAt" : "2020-02-21T06:03:24Z",
        "updatedAt" : "2020-02-21T16:38:04Z",
        "lastEditedBy" : "d3b9ec12-6001-425a-940b-b74c57282ba6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d896db61ccdb573f5dfd90712d664fd213b125f",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +229,233 @@\n\t\t\t// update Node API object with a fake resource\n\t\t\tnodeCopy := node.DeepCopy()\n\t\t\tnodeCopy.ResourceVersion = \"0\"\n"
  },
  {
    "id" : "37f08770-53c1-4587-bda7-a4f557e6872d",
    "prId" : 83652,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83652#pullrequestreview-326405015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86485f73-1c8e-4e20-a98c-a7026351fc3a",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "The wording here is a bit unclear to me.\r\n\r\nThe basic idea here is: Find an available node which can afford the testing pod, and taint that node with a specific key with effect NoSchedule, then schedule a Pod with (1) particular nodeLabel to ensure it can only be placed onto node, and also (2) particular tolerations spec to ensure it matches the pre-applied taint. Finally check if the Pod is running on the node.\r\n\r\n~~And also, L529~L533 may be a copy/paste mistake, we need to reword them as well.~~\r\n\r\nCould you reword it?",
        "createdAt" : "2019-10-21T19:04:46Z",
        "updatedAt" : "2019-12-06T00:54:17Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "83a8bb2a-3310-449c-acd4-39a5447f9bcb",
        "parentId" : "86485f73-1c8e-4e20-a98c-a7026351fc3a",
        "authorId" : "3f579bf7-089d-41f0-abf9-3bf5462ec695",
        "body" : "ack, reworded the description. Sorry what is the issue with L529-533? It seems to be doing the same checks as the tests above it?",
        "createdAt" : "2019-11-19T01:04:58Z",
        "updatedAt" : "2019-12-06T00:54:17Z",
        "lastEditedBy" : "3f579bf7-089d-41f0-abf9-3bf5462ec695",
        "tags" : [
        ]
      },
      {
        "id" : "060ea8c3-b8f1-41bf-86cf-150023e78325",
        "parentId" : "86485f73-1c8e-4e20-a98c-a7026351fc3a",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Nvm, L529~L533 is fine. Just leave them as is.",
        "createdAt" : "2019-12-03T18:54:23Z",
        "updatedAt" : "2019-12-06T00:54:17Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "cdf50f52-aaa7-4803-89ab-81737bc5f574",
        "parentId" : "86485f73-1c8e-4e20-a98c-a7026351fc3a",
        "authorId" : "3f579bf7-089d-41f0-abf9-3bf5462ec695",
        "body" : "Just did a rebase, would you mind reviewing?",
        "createdAt" : "2019-12-03T19:49:01Z",
        "updatedAt" : "2019-12-06T00:54:17Z",
        "lastEditedBy" : "3f579bf7-089d-41f0-abf9-3bf5462ec695",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d3d364d2baf86419c3f144f8712546335d76897",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +460,464 @@\t\t        corresponding nodeLabel and toleration spec such that it should only be able to run on the selected node.\n\t\t        Ensure that the pod is scheduled and running on the node.\n\t*/\n\tframework.ConformanceIt(\"validates that taints-tolerations is respected if matching [Disruptive]\", func() {\n\t\tnodeName := getNodeThatCanRunPodWithoutToleration(f)"
  },
  {
    "id" : "2ebf2d5a-a214-4a07-96f2-611a8ac2af8f",
    "prId" : 83652,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83652#pullrequestreview-334216907",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a3c3371-8809-4dad-83b9-3cb765a2d0db",
        "parentId" : null,
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "This change should have been split out into a separate PR prior to promoting to Conformance, so we could verify whether the test still passed",
        "createdAt" : "2019-12-18T14:59:07Z",
        "updatedAt" : "2019-12-18T14:59:07Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      },
      {
        "id" : "0ec0b9ff-1e20-4c58-b5c2-6c2b1ce04c03",
        "parentId" : "2a3c3371-8809-4dad-83b9-3cb765a2d0db",
        "authorId" : "3f579bf7-089d-41f0-abf9-3bf5462ec695",
        "body" : "ack, sorry about that",
        "createdAt" : "2019-12-18T19:40:58Z",
        "updatedAt" : "2019-12-18T19:40:59Z",
        "lastEditedBy" : "3f579bf7-089d-41f0-abf9-3bf5462ec695",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d3d364d2baf86419c3f144f8712546335d76897",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +489,493 @@\t\t})\n\n\t\tframework.ExpectNoError(e2epod.WaitForPodNameRunningInNamespace(cs, ns, tolerationPodName))\n\t\tdeployedPod, err := cs.CoreV1().Pods(ns).Get(tolerationPodName, metav1.GetOptions{})\n\t\tframework.ExpectNoError(err)"
  },
  {
    "id" : "a6fd019e-1fa7-49fd-9614-64ea737acc32",
    "prId" : 80398,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80398#pullrequestreview-265587863",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b44157c-0814-4586-8847-edecc560ce28",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "This is a bit weird IMO, we're determining IP family by checking the cluster IP but we are validating host IP with it. Can we confirm that the host IP is always matching whatever the primary cluster IP is? ",
        "createdAt" : "2019-07-23T15:06:35Z",
        "updatedAt" : "2019-07-25T14:18:45Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "a830308e-5b54-40d8-911a-0195a4dcbb4f",
        "parentId" : "3b44157c-0814-4586-8847-edecc560ce28",
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "the problem with these tests are they are using hardcoded IPv4 addresses.\r\nThe `TranslateIPv4ToIPv6(hostIP)` function returns the hostIP as IPv6, only if the cluster family is IPv6, otherwise returns hostIP without any change (as an IPv4 address)",
        "createdAt" : "2019-07-23T15:48:13Z",
        "updatedAt" : "2019-07-25T14:18:45Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "a9658eab-33d2-473b-a1e2-2f4e91cde3c4",
        "parentId" : "3b44157c-0814-4586-8847-edecc560ce28",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "But this would break if an e2e test ends up passing in a v6 address for hostIP here right? We should account for that here or at the very least document that. ",
        "createdAt" : "2019-07-23T17:51:14Z",
        "updatedAt" : "2019-07-25T14:18:45Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "3a631ba3-3f0b-4210-841a-66e566602b3f",
        "parentId" : "3b44157c-0814-4586-8847-edecc560ce28",
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "ahh, ok got it, you are right, I was only focused on these tests since `createHostPortonNode` is not exported but I think I can modify the TranslateIPv4ToIPv6 to verify that the IP passed as argument is IPv4, otherwise do nothing",
        "createdAt" : "2019-07-23T17:59:51Z",
        "updatedAt" : "2019-07-25T14:18:45Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "421c9018-f017-4013-b601-5486338e0f14",
        "parentId" : "3b44157c-0814-4586-8847-edecc560ce28",
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "done",
        "createdAt" : "2019-07-23T18:13:42Z",
        "updatedAt" : "2019-07-25T14:18:45Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd15f3f9f76855d61be1396888fae0cc6499df14",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +826,830 @@// create pod which using hostport on the specified node according to the nodeSelector\nfunc createHostPortPodOnNode(f *framework.Framework, podName, ns, hostIP string, port int32, protocol v1.Protocol, nodeSelector map[string]string, expectScheduled bool) {\n\thostIP = framework.TranslateIPv4ToIPv6(hostIP)\n\tcreatePausePod(f, pausePodConfig{\n\t\tName: podName,"
  },
  {
    "id" : "51240fa5-4109-4cc0-9bd5-edf38e978525",
    "prId" : 63829,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63829#pullrequestreview-120045973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1251069b-e30b-419c-938f-aafdd0c64e00",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "nit: You  could use `framework.SkipUnlessServerVersionGTE` to  skip for tests with version less than existing v1.10.2 as the first line.",
        "createdAt" : "2018-05-14T23:21:32Z",
        "updatedAt" : "2018-05-14T23:21:32Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "59a62f59-803d-4f65-8cbd-f053b42dd11d",
        "parentId" : "1251069b-e30b-419c-938f-aafdd0c64e00",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "I think I misunderstand that helper. It skips old server versions, presumably because the feature under test has not yet been implemented. In this case we want to skip newer server versions, so I either need to add a new helper or explicitly compare versions. (Since there is only one use, I didn't think a helper would be very helpful.)",
        "createdAt" : "2018-05-14T23:24:26Z",
        "updatedAt" : "2018-05-14T23:24:26Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "bb46193c-bb26-43c3-930c-2790e3da9e9a",
        "parentId" : "1251069b-e30b-419c-938f-aafdd0c64e00",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Apologies, I might have misunderstood as well. Why can't we check the reverse - like skip this test if version is greater than 1.9 for example?",
        "createdAt" : "2018-05-14T23:30:39Z",
        "updatedAt" : "2018-05-14T23:30:39Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "fe736ea8-193e-4f07-bd19-0f07936a16dd",
        "parentId" : "1251069b-e30b-419c-938f-aafdd0c64e00",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "Thanks for the feedback. Since I'm working on the release-1.9 branch, I don't think there's much value in adding new utils. I would not want to cherry-pick it forward, so I would end up writing the util in master, picking it backward, then fixing this test.",
        "createdAt" : "2018-05-14T23:38:33Z",
        "updatedAt" : "2018-05-14T23:38:33Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      }
    ],
    "commit" : "d62c1bb56aa914d3680761fedcd6fd7e930e386e",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +369,373 @@\tIt(\"validates that a pod with an invalid NodeAffinity is rejected\", func() {\n\t\tBy(\"Trying to launch a pod with an invalid Affinity data.\")\n\t\tvalidationChangedVersion := utilversion.MustParseSemantic(\"v1.10.2\")\n\t\tvalidationChanged, err := framework.ServerVersionGTE(validationChangedVersion, f.ClientSet.Discovery())\n\t\tif err != nil {"
  },
  {
    "id" : "09268c10-8387-4297-a8a4-850c8fd1359a",
    "prId" : 53169,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53169#pullrequestreview-66050462",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36f4ce9e-ea70-4ef3-8733-deafd37ca253",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "How about the other status, e.g. network-unavailable?",
        "createdAt" : "2017-09-29T01:31:14Z",
        "updatedAt" : "2017-09-29T02:48:09Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "dbc7ee30-1291-48b9-8676-eff9afbe7058",
        "parentId" : "36f4ce9e-ea70-4ef3-8733-deafd37ca253",
        "authorId" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "body" : "IIRC, if a node is in network-unavailable, `condition.Status` will be changed into `v1.ConditionUnknonw`. The only corner case is that nodes are not immediately marked Unknown when they get unknown.",
        "createdAt" : "2017-09-29T02:40:06Z",
        "updatedAt" : "2017-09-29T02:48:09Z",
        "lastEditedBy" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "tags" : [
        ]
      },
      {
        "id" : "c1f83109-96c9-42fd-a917-a185738eae8c",
        "parentId" : "36f4ce9e-ea70-4ef3-8733-deafd37ca253",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Generally, it should be OK for the test to fail in case nodes have issues like network error, etc. We don't expect nodes to be unhealthy in this test.",
        "createdAt" : "2017-09-29T02:52:07Z",
        "updatedAt" : "2017-09-29T02:52:07Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "8a5d0dcb-4f48-4eac-a03d-6c812c0c1d27",
        "parentId" : "36f4ce9e-ea70-4ef3-8733-deafd37ca253",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "SGTM :).",
        "createdAt" : "2017-09-29T03:01:18Z",
        "updatedAt" : "2017-09-29T03:01:18Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4c8eefd41a7753c4abf3334faaf1e48ed3770e6",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +245,249 @@\t\t\tnodeReady := false\n\t\t\tfor _, condition := range node.Status.Conditions {\n\t\t\t\tif condition.Type == v1.NodeReady && condition.Status == v1.ConditionTrue {\n\t\t\t\t\tnodeReady = true\n\t\t\t\t\tbreak"
  },
  {
    "id" : "719277c9-d0c7-4afd-9f88-9c322faaaf29",
    "prId" : 53169,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53169#pullrequestreview-66066996",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9299136-c6a6-44a8-bb7d-03aaa52f5327",
        "parentId" : null,
        "authorId" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "body" : "Nit: you may use the label `\"kubernetes.io/hostname\"=<node.Name>` for latter NodeAffinity, to avoid unnecessary node label modifications",
        "createdAt" : "2017-09-29T03:24:51Z",
        "updatedAt" : "2017-09-29T04:29:30Z",
        "lastEditedBy" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "tags" : [
        ]
      },
      {
        "id" : "84caddb2-cc89-4579-8ebe-82455c5e0a22",
        "parentId" : "e9299136-c6a6-44a8-bb7d-03aaa52f5327",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Good point. Done.",
        "createdAt" : "2017-09-29T06:23:04Z",
        "updatedAt" : "2017-09-29T06:23:04Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "6f6b9c00-6636-4213-9628-a5ab102b12ab",
        "parentId" : "e9299136-c6a6-44a8-bb7d-03aaa52f5327",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "oh well. Too late. It is already merged.",
        "createdAt" : "2017-09-29T06:24:02Z",
        "updatedAt" : "2017-09-29T06:24:02Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4c8eefd41a7753c4abf3334faaf1e48ed3770e6",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +254,258 @@\t\t\t}\n\t\t\t// Apply node label to each node\n\t\t\tframework.AddOrUpdateLabelOnNode(cs, node.Name, \"node\", node.Name)\n\t\t\tframework.ExpectNodeHasLabel(cs, node.Name, \"node\", node.Name)\n\t\t\t// Find allocatable amount of CPU."
  },
  {
    "id" : "06818f80-75d2-41f3-9d98-2b81ec479a0d",
    "prId" : 53169,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53169#pullrequestreview-66052454",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a085124c-b7d8-40a2-be3a-2d8c3bd52871",
        "parentId" : null,
        "authorId" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "body" : "Nit: same as above, can be \"kubernetes.io/hostname\" here.",
        "createdAt" : "2017-09-29T04:29:03Z",
        "updatedAt" : "2017-09-29T04:29:30Z",
        "lastEditedBy" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4c8eefd41a7753c4abf3334faaf1e48ed3770e6",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +303,307 @@\t\t\t\t\t\t\t\t\tMatchExpressions: []v1.NodeSelectorRequirement{\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tKey:      \"node\",\n\t\t\t\t\t\t\t\t\t\t\tOperator: v1.NodeSelectorOpIn,\n\t\t\t\t\t\t\t\t\t\t\tValues:   []string{nodeName},"
  },
  {
    "id" : "8253730b-6d20-44a1-b426-91e3eb6bd9f1",
    "prId" : 50005,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50005#pullrequestreview-61071685",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1cba6dc-83b1-46fa-ab0e-89f35ec90d0f",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "fwiw we need to start notifying @kubernetes/sig-testing-feature-requests on [Conformance] additions.   It doesn't look like this test is version gated, so it will fail when you run the 1.8 tests on a 1.7 cluster.  \r\n\r\n",
        "createdAt" : "2017-09-06T22:44:31Z",
        "updatedAt" : "2017-09-06T22:44:31Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "18f086c6f62092586f51113ee7fca64c2b6a8818",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +152,156 @@\t// It assumes that cluster add-on pods stay stable and cannot be run in parallel with any other test that touches Nodes or Pods.\n\t// It is so because we need to have precise control on what's running in the cluster.\n\tIt(\"validates local ephemeral storage resource limits of pods that are allowed to run [Conformance]\", func() {\n\t\tnodeMaxAllocatable := int64(0)\n"
  }
]