[
  {
    "id" : "e39b007b-237e-405e-8578-1650fc67bfee",
    "prId" : 100412,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100412#pullrequestreview-694442551",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7190530-4c1b-4e4f-8b57-29e52b5f9cbc",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "What's the reason for this change?",
        "createdAt" : "2021-04-29T21:04:56Z",
        "updatedAt" : "2021-04-29T21:50:51Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "1006f0b4-368d-4fe3-9c72-9030b40148bd",
        "parentId" : "e7190530-4c1b-4e4f-8b57-29e52b5f9cbc",
        "authorId" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "body" : "This seems not relevant, whether we create NP or not for this service, it should fail. Will change it back.",
        "createdAt" : "2021-04-29T21:33:03Z",
        "updatedAt" : "2021-04-29T21:50:51Z",
        "lastEditedBy" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "tags" : [
        ]
      },
      {
        "id" : "7692dcd3-fe33-48e4-81d2-0e0bbf5a120a",
        "parentId" : "e7190530-4c1b-4e4f-8b57-29e52b5f9cbc",
        "authorId" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "body" : "Revisited the case and sorted it out. Actually this is a bug in the test. The creation request was rejected because the `allocateLoadBalancerNodePorts` was turned off by default (thus each LB type service will allocate a node port and exceed quota and failed). Now as the feature is enabled by default, the creation will success if we don't allocate a node port to the service. We need to set it to true here so that the node port quota will exceed. Btw I've added more tests for this `allocateLoadBalancerNodePorts` feature in a separate conformance test so it's not interfering with the conformance tests written since 1.16.",
        "createdAt" : "2021-06-25T23:25:19Z",
        "updatedAt" : "2021-06-25T23:25:19Z",
        "lastEditedBy" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "tags" : [
        ]
      },
      {
        "id" : "af8e4ad5-d423-4ee9-b5ae-7647721e7e0e",
        "parentId" : "e7190530-4c1b-4e4f-8b57-29e52b5f9cbc",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "We should have a case where we pass true and it fails and another where we pass false and it does not fail.",
        "createdAt" : "2021-06-28T22:26:26Z",
        "updatedAt" : "2021-06-28T22:28:50Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "c96c809539f399acee92f9073418b61f5dd5df0f",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +111,115 @@\n\t\tginkgo.By(\"Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota\")\n\t\tloadbalancer := newTestServiceForQuota(\"test-service-lb\", v1.ServiceTypeLoadBalancer, true)\n\t\t_, err = f.ClientSet.CoreV1().Services(f.Namespace.Name).Create(context.TODO(), loadbalancer, metav1.CreateOptions{})\n\t\tframework.ExpectError(err)"
  },
  {
    "id" : "7cc10460-df39-424b-a0ab-5977b9386595",
    "prId" : 100412,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100412#pullrequestreview-696705245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "All three places that call this pass `false` for `allocateLoadBalancerNodePorts`.\r\n\r\nCan you go thru that and make sure it tests quota for type=NodePort *and* NodePorts allocated in type=LoadBalancer when this is true *and not* NodePorts in type=LoadBalancer when this is false?",
        "createdAt" : "2021-06-25T16:40:59Z",
        "updatedAt" : "2021-06-25T16:45:25Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "dde6f25e-9e43-445e-a5c4-5977bb062fb0",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "body" : "Not sure if this is what you're expecting, added the happy path for LB type service creation both with and w/o node port, and retain the negative case for resource quota exceed test.",
        "createdAt" : "2021-06-25T18:25:01Z",
        "updatedAt" : "2021-06-25T18:25:01Z",
        "lastEditedBy" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "tags" : [
        ]
      },
      {
        "id" : "52b46fda-3b81-4fb3-b021-31e8e5dba6d8",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "body" : "Ahh, had a few tries and I just realized that, the `allocateLoadBalancerNodePorts` feature is still Beta and is not enabled in the GA feature only conformance tests (namely `pull-kubernetes-conformance-kind-ga-only-parallel`), so it was failing consistently. That said, in the current resource quota conformance tests, I think we need to assume that the `allocateLoadBalancerNodePorts` feature is not enabled and for each LB type service created, a node port will be allocated. I reverted the new conformance test, and we can enable the `allocateLoadBalancerNodePorts` specific test once the feature is GAed.",
        "createdAt" : "2021-06-26T00:34:38Z",
        "updatedAt" : "2021-06-26T00:34:39Z",
        "lastEditedBy" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "tags" : [
        ]
      },
      {
        "id" : "09111157-af94-469d-a029-64a886239224",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "It worries me that we have more work to do when we GA - it seems likely to get forgotten.  Can we add the code now, but explicitly check the gate for `PreRelease = featuregate.GA`.  That way it starts testing as soon as we mark it GA and when we remove the gate, we will also have to remove that check.",
        "createdAt" : "2021-06-28T22:28:44Z",
        "updatedAt" : "2021-06-28T22:28:50Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "7f7c96be-2f75-4647-866a-e29bdf560fa6",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "body" : "Hi @thockin, I think `defaultKubernetesFeatureGates` is not exposed\r\nhttps://github.com/kubernetes/kubernetes/blob/38f012320fa138260dca09e135ed386026a061bc/pkg/features/kube_features.go#L726\r\nso it's hard to get maturity of a specific feature. By checking feature gate https://github.com/kubernetes/kubernetes/blob/442a69c3bdf6fe8e525b05887e57d89db1e2f3a5/staging/src/k8s.io/component-base/featuregate/feature_gate.go#L86-L96\r\nwe can check if a feature is enabled, but we can't get its maturity. So for now the test will be skipped if only GAed features are enabled and will be covered if all beta features are enabled. We have both testbed in our pipeline so both scenarios are covered.",
        "createdAt" : "2021-06-29T00:28:38Z",
        "updatedAt" : "2021-06-29T00:28:38Z",
        "lastEditedBy" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "tags" : [
        ]
      },
      {
        "id" : "ae23f68b-313b-4741-b22c-393ea3795921",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "body" : "conformance tests may not skip under any circumstances.\r\nnon-GA features need to be tested in not-a-conformance-test.",
        "createdAt" : "2021-06-29T04:39:46Z",
        "updatedAt" : "2021-06-29T04:39:47Z",
        "lastEditedBy" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "tags" : [
        ]
      },
      {
        "id" : "409b5b89-0f32-4436-ac7c-d9ecde5ed4ac",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "body" : "Followed suggestions from @bentheelder and @liggitt, for now keep the test as an e2e test. Later once we promote the feature to GA, we can promote the test to a conformance test.",
        "createdAt" : "2021-06-29T04:51:18Z",
        "updatedAt" : "2021-06-29T04:51:19Z",
        "lastEditedBy" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "tags" : [
        ]
      },
      {
        "id" : "615da9f0-3d15-4829-bd2f-f3ad18592c57",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "body" : "even if the feature were GA: we usually start by adding a test if missing and then wait for evidence that the test is stable + reliable on some existing CI before promoting it to a test everyone must pass when certifying their clusters ðŸ˜… ",
        "createdAt" : "2021-06-29T19:22:45Z",
        "updatedAt" : "2021-06-29T19:22:45Z",
        "lastEditedBy" : "46e1ba13-482b-4bcd-8fb1-be821bac3b04",
        "tags" : [
        ]
      },
      {
        "id" : "9fb97f8f-b611-48c7-aea7-e55bc4ad6f35",
        "parentId" : "ae54c1ac-4030-47ee-bfc7-631ab2536dee",
        "authorId" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "body" : "Followed suggestion from @aojea, moved the test from e2e to an integration test.",
        "createdAt" : "2021-06-30T23:56:57Z",
        "updatedAt" : "2021-06-30T23:57:02Z",
        "lastEditedBy" : "89c2ffa8-b294-43fc-85c3-8f33b486c9b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "c96c809539f399acee92f9073418b61f5dd5df0f",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1736,1740 @@\n// newTestServiceForQuota returns a simple service\nfunc newTestServiceForQuota(name string, serviceType v1.ServiceType, allocateLoadBalancerNodePorts bool) *v1.Service {\n\tvar allocateNPs *bool\n\t// Only set allocateLoadBalancerNodePorts when service type is LB"
  },
  {
    "id" : "e18047c3-663e-4c56-aa5a-eabf98ba2b15",
    "prId" : 100056,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100056#pullrequestreview-609337142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1dc823b0-7b05-4cdc-a1b5-a5f4df406826",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this only fixes a single e2e test... newTestResourceQuota is used in many tests\r\n\r\nshould we:\r\n1. loosen newTestResourceQuota to 10 configmaps like it has for secrets\r\n2. update newTestResourceQuota to accomodate all existing configmaps and secrets in the test namespace (requires passing in the namespace and a client to newTestResourceQuota and doing this poll inside that method)",
        "createdAt" : "2021-03-10T16:25:41Z",
        "updatedAt" : "2021-03-11T03:24:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "80f44470-dd55-4426-a4ae-43a21e112e3e",
        "parentId" : "1dc823b0-7b05-4cdc-a1b5-a5f4df406826",
        "authorId" : "5e225159-999d-430a-8b58-d5220dc1429d",
        "body" : "I checked that only this test case creates configmap.\r\n\r\nAs you mentioned, it may be related to the CA configmap created by default in 1.20. Changing it to 10 configmaps is acceptable.\r\n\r\nI wondered that if we poll inside that method, it will have some side effects like performance issues or flakes? (Let me refactor it if this is considered to be a better solution.)\r\n\r\nSo I prefer to update it to 10. ",
        "createdAt" : "2021-03-11T01:58:16Z",
        "updatedAt" : "2021-03-11T03:24:29Z",
        "lastEditedBy" : "5e225159-999d-430a-8b58-d5220dc1429d",
        "tags" : [
        ]
      },
      {
        "id" : "5c069f93-18c8-4350-85ef-34f6c809992b",
        "parentId" : "1dc823b0-7b05-4cdc-a1b5-a5f4df406826",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "loosening to 10 makes sense to me",
        "createdAt" : "2021-03-11T02:03:07Z",
        "updatedAt" : "2021-03-11T03:24:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "8a15f511-fa8e-41ee-87ea-a72731e1af63",
        "parentId" : "1dc823b0-7b05-4cdc-a1b5-a5f4df406826",
        "authorId" : "5e225159-999d-430a-8b58-d5220dc1429d",
        "body" : "Updated. Thanks.",
        "createdAt" : "2021-03-11T02:09:20Z",
        "updatedAt" : "2021-03-11T03:24:29Z",
        "lastEditedBy" : "5e225159-999d-430a-8b58-d5220dc1429d",
        "tags" : [
        ]
      }
    ],
    "commit" : "2105d7ef6e2de987a876c71b1d11687592ef087b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +336,340 @@\t\tquotaName := \"test-quota\"\n\t\tresourceQuota := newTestResourceQuota(quotaName)\n\t\tresourceQuota.Spec.Hard[v1.ResourceConfigMaps] = resource.MustParse(hardConfigMaps)\n\t\t_, err = createResourceQuota(f.ClientSet, f.Namespace.Name, resourceQuota)\n\t\tframework.ExpectNoError(err)"
  },
  {
    "id" : "640ff0d4-1939-4c77-a878-be59d41427ac",
    "prId" : 100056,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100056#pullrequestreview-609362715",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09165b92-e916-492c-84eb-823fe27f19bc",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "drop the \"we expect there to be two configmaps\" comment block below. that is not a generally correct assumption when there exist clusters that inject configmaps by default (as the check for the \"default\" number of configmaps in the namespace at the beginning of the test shows)",
        "createdAt" : "2021-03-11T03:16:23Z",
        "updatedAt" : "2021-03-11T03:24:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "2105d7ef6e2de987a876c71b1d11687592ef087b",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +339,343 @@\t\t_, err = createResourceQuota(f.ClientSet, f.Namespace.Name, resourceQuota)\n\t\tframework.ExpectNoError(err)\n\n\t\tginkgo.By(\"Ensuring resource quota status is calculated\")\n\t\tusedResources := v1.ResourceList{}"
  },
  {
    "id" : "c8478b59-07b9-4d12-8b4b-f1566789a45a",
    "prId" : 97451,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97451#pullrequestreview-561982443",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60a54b13-e64a-4384-a90b-83d0d01e4f92",
        "parentId" : null,
        "authorId" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "body" : "please add the test case for the scenario described in the PR description:\r\n\r\n\"when nodeport quota usage exceeds the quota limit, cluster ip service should still be allowed if cluster ip service quota still has some left.\"",
        "createdAt" : "2021-01-05T17:32:40Z",
        "updatedAt" : "2021-01-06T23:16:11Z",
        "lastEditedBy" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "tags" : [
        ]
      }
    ],
    "commit" : "15867d9e8a1faf007f6df563c26a9b5e8744b2a1",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +115,119 @@\t\t_, err = f.ClientSet.CoreV1().Services(f.Namespace.Name).Create(context.TODO(), loadbalancer, metav1.CreateOptions{})\n\t\tframework.ExpectError(err)\n\n\t\tginkgo.By(\"Ensuring resource quota status captures service creation\")\n\t\tusedResources = v1.ResourceList{}"
  },
  {
    "id" : "77689321-e6e2-4712-901c-9c47999926e4",
    "prId" : 78331,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78331#pullrequestreview-247972076",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "469a27c1-88f5-4939-8b93-f4c67b427af0",
        "parentId" : null,
        "authorId" : "0aee6f4f-d40c-4f7b-9383-7ec73a655652",
        "body" : "**Note for reviewer:** I suspect, recent `gofmt` or `goimport` adding this named import alias.",
        "createdAt" : "2019-06-11T06:28:53Z",
        "updatedAt" : "2019-07-25T14:32:08Z",
        "lastEditedBy" : "0aee6f4f-d40c-4f7b-9383-7ec73a655652",
        "tags" : [
        ]
      }
    ],
    "commit" : "a92197074512544eb405319b069ea47779ed8032",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +23,27 @@\n\tappsv1 \"k8s.io/api/apps/v1\"\n\tv1 \"k8s.io/api/core/v1\"\n\tschedulingv1 \"k8s.io/api/scheduling/v1\"\n\t\"k8s.io/apimachinery/pkg/api/errors\""
  },
  {
    "id" : "feb3a8cb-4b9f-4b6b-8e26-d551b7d452aa",
    "prId" : 78331,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78331#pullrequestreview-266672519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7d1140d-5067-4311-a5e7-4001e490ec46",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Is this test no longer flaky?  For a very long time it was.  @kubernetes/sig-api-machinery-api-reviews am I recalling correctly?  Did jordan's fixes finally squash this?",
        "createdAt" : "2019-06-18T19:09:37Z",
        "updatedAt" : "2019-07-25T14:32:08Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "48b8c3d6-9946-4d40-849c-96b6f23aefc3",
        "parentId" : "a7d1140d-5067-4311-a5e7-4001e490ec46",
        "authorId" : "0aee6f4f-d40c-4f7b-9383-7ec73a655652",
        "body" : "Seems not flaking.\r\nhttps://k8s-testgrid.appspot.com/sig-release-master-blocking#gce-cos-master-default&include-filter-by-regex=should%20create%20a%20ResourceQuota%20and%20capture%20the%20life%20of%20a%20secret&graph-metrics=test-duration-minutes",
        "createdAt" : "2019-07-19T16:06:34Z",
        "updatedAt" : "2019-07-25T14:32:08Z",
        "lastEditedBy" : "0aee6f4f-d40c-4f7b-9383-7ec73a655652",
        "tags" : [
        ]
      },
      {
        "id" : "215ddacc-1e13-4ac4-bed9-4aceb84914fc",
        "parentId" : "a7d1140d-5067-4311-a5e7-4001e490ec46",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "It would be useful to comment in the code below why the test need to wait for the number of secrets to stabilize",
        "createdAt" : "2019-07-19T19:02:19Z",
        "updatedAt" : "2019-07-25T14:32:08Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "c994aa9b-39ae-493e-aec9-796f4efe61ae",
        "parentId" : "a7d1140d-5067-4311-a5e7-4001e490ec46",
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "It would be nice but I wouldn't block promoting on the lack of such a comment.  The test has been like this for a while.\r\n\r\nref @smarterclayton https://github.com/kubernetes/kubernetes/pull/40340\r\n\r\n>On contended servers the service account controller can slow down,\r\nleading to the count changing during a run. Wait up to 5s for the count\r\nto stabilize, assuming that updates come at a consistent rate, and are\r\nnot held indefinitely.",
        "createdAt" : "2019-07-23T00:11:15Z",
        "updatedAt" : "2019-07-25T14:32:08Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      },
      {
        "id" : "5b6e7a5e-3cf6-428a-8c90-8959527149b3",
        "parentId" : "a7d1140d-5067-4311-a5e7-4001e490ec46",
        "authorId" : "0aee6f4f-d40c-4f7b-9383-7ec73a655652",
        "body" : "Added comment as well to bring visibility.( helpful for new contributor )",
        "createdAt" : "2019-07-25T14:39:28Z",
        "updatedAt" : "2019-07-25T14:39:28Z",
        "lastEditedBy" : "0aee6f4f-d40c-4f7b-9383-7ec73a655652",
        "tags" : [
        ]
      }
    ],
    "commit" : "a92197074512544eb405319b069ea47779ed8032",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +128,132 @@\t\tCreate a Secret. Its creation MUST be successful and resource usage count against the Secret object and resourceQuota object MUST be captured in ResourceQuotaStatus of the ResourceQuota.\n\t\tDelete the Secret. Deletion MUST succeed and resource usage count against the Secret object MUST be released from ResourceQuotaStatus of the ResourceQuota.\n\t*/\n\tframework.ConformanceIt(\"should create a ResourceQuota and capture the life of a secret.\", func() {\n\t\tginkgo.By(\"Discovering how many secrets are in namespace by default\")"
  },
  {
    "id" : "7cb98830-52fc-48b8-8e3d-136982f54c7c",
    "prId" : 78331,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78331#pullrequestreview-271639161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d37173ea-2c02-413e-8c2c-5ee23166ce11",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Does this test really need to be so complicated, or did it just copy the secret test?",
        "createdAt" : "2019-07-19T19:01:21Z",
        "updatedAt" : "2019-07-25T14:32:08Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "201ab8f9-4ef8-4a22-9296-d1c42c2e2972",
        "parentId" : "d37173ea-2c02-413e-8c2c-5ee23166ce11",
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "@mikedanese asked a similar question when this test was first introduced https://github.com/kubernetes/kubernetes/pull/68812/files#r230931734\r\n\r\nPer comments below this is making an allowance for `ca.crt` to show up",
        "createdAt" : "2019-07-23T00:33:14Z",
        "updatedAt" : "2019-07-25T14:32:08Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      },
      {
        "id" : "3bbe74d0-8c9c-47c2-953f-0db6dd2e03a0",
        "parentId" : "d37173ea-2c02-413e-8c2c-5ee23166ce11",
        "authorId" : "580b4924-2623-4aee-8bbf-3e1735a9dc88",
        "body" : "Why do we need to do that? Is this just related to how we have configured the e2e cluster? If there is this race condition in pod startup, it would seem that users can hit this too, we shouldn't just work around it.",
        "createdAt" : "2019-08-06T22:01:50Z",
        "updatedAt" : "2019-08-06T22:01:51Z",
        "lastEditedBy" : "580b4924-2623-4aee-8bbf-3e1735a9dc88",
        "tags" : [
        ]
      }
    ],
    "commit" : "a92197074512544eb405319b069ea47779ed8032",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +295,299 @@\t\tDelete the ConfigMap. Deletion MUST succeed and resource usage count against the ConfigMap object MUST be released from ResourceQuotaStatus of the ResourceQuota.\n\t*/\n\tframework.ConformanceIt(\"should create a ResourceQuota and capture the life of a configMap.\", func() {\n\t\tfound, unchanged := 0, 0\n\t\t// On contended servers the service account controller can slow down, leading to the count changing during a run."
  },
  {
    "id" : "89f89783-5789-4ab9-a361-01ca0669340e",
    "prId" : 74570,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74570#pullrequestreview-207955528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26dd5a6c-9995-4233-8bda-458e95ae382b",
        "parentId" : null,
        "authorId" : "1c129c56-3102-4f2b-9811-3cdbb8ceb2d6",
        "body" : "@liggitt  Hi, just a query that can we ensure the deleted resourceQuota like we are using for secrets, configmap etc. ? For example \t\tBy(\"Ensuring resource quota status released usage\")...",
        "createdAt" : "2019-02-26T08:11:52Z",
        "updatedAt" : "2019-02-27T02:15:04Z",
        "lastEditedBy" : "1c129c56-3102-4f2b-9811-3cdbb8ceb2d6",
        "tags" : [
        ]
      },
      {
        "id" : "4484d152-4bbe-4907-a776-5bbd9dc968b8",
        "parentId" : "26dd5a6c-9995-4233-8bda-458e95ae382b",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I'm not sure I understand what this is asking",
        "createdAt" : "2019-02-26T13:35:06Z",
        "updatedAt" : "2019-02-27T02:15:04Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "5752bc35fbe877ee56fa7d51b10c9e36bedb6c3f",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +702,706 @@\t\tExpect(resourceQuotaResult.Spec.Hard[v1.ResourceMemory]).To(Equal(resource.MustParse(\"1Gi\")))\n\n\t\tBy(\"Deleting a ResourceQuota\")\n\t\terr = deleteResourceQuota(client, ns, quotaName)\n\t\tExpect(err).NotTo(HaveOccurred())"
  },
  {
    "id" : "df13f6c9-4e6a-497e-b1be-8b1fcb5c9cd1",
    "prId" : 74570,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74570#pullrequestreview-208307520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "844a2b8f-d7bd-414e-884f-ac642b292156",
        "parentId" : null,
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "I guess we can verify the ResourceQuota is deleted by adding the following:\r\n```\r\nBy(\"Verifying the deleted ResourceQuota\")\r\n_, err := client.CoreV1().ResourceQuotas(ns).Get(quotaName, metav1.GetOptions{})\r\nExpect(errors.IsNotFound(err)).To(Equal(true))\r\n```\r\n\r\n",
        "createdAt" : "2019-02-26T22:42:05Z",
        "updatedAt" : "2019-02-27T02:15:04Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      },
      {
        "id" : "806f1b82-7934-4a27-9f0c-5897e0b194ad",
        "parentId" : "844a2b8f-d7bd-414e-884f-ac642b292156",
        "authorId" : "dcab10bd-e585-4065-b62c-ab73b7a208ac",
        "body" : "Thanks for the review.\r\nI added it.",
        "createdAt" : "2019-02-27T02:22:49Z",
        "updatedAt" : "2019-02-27T02:22:49Z",
        "lastEditedBy" : "dcab10bd-e585-4065-b62c-ab73b7a208ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "5752bc35fbe877ee56fa7d51b10c9e36bedb6c3f",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +704,708 @@\t\tBy(\"Deleting a ResourceQuota\")\n\t\terr = deleteResourceQuota(client, ns, quotaName)\n\t\tExpect(err).NotTo(HaveOccurred())\n\n\t\tBy(\"Verifying the deleted ResourceQuota\")"
  },
  {
    "id" : "a075d78c-fc5f-41ca-933b-0b821c9f125a",
    "prId" : 72384,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72384#pullrequestreview-214245466",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "464c62b6-ad74-48c3-bcba-68c07787f1b3",
        "parentId" : null,
        "authorId" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "body" : "could you add a test case that 'not allow creating more CRs that exceeds the remaining quota'",
        "createdAt" : "2019-03-12T21:57:02Z",
        "updatedAt" : "2019-03-27T18:08:41Z",
        "lastEditedBy" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "tags" : [
        ]
      },
      {
        "id" : "ccb10983-3d9a-42e2-aea6-8063af703d1f",
        "parentId" : "464c62b6-ad74-48c3-bcba-68c07787f1b3",
        "authorId" : "948c8bee-49af-46ed-93a8-309275b51d61",
        "body" : "Yeah, sure, this is also added.",
        "createdAt" : "2019-03-13T22:35:02Z",
        "updatedAt" : "2019-03-27T18:08:41Z",
        "lastEditedBy" : "948c8bee-49af-46ed-93a8-309275b51d61",
        "tags" : [
        ]
      }
    ],
    "commit" : "f58c2ae62d1988d82386cc542899b71cc164e141",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +549,553 @@\t\terr = waitForResourceQuota(f.ClientSet, f.Namespace.Name, quotaName, usedResources)\n\t\tExpect(err).NotTo(HaveOccurred())\n\n\t\tBy(\"Creating a second custom resource\")\n\t\t_, err = instantiateCustomResource(&unstructured.Unstructured{"
  },
  {
    "id" : "39a54c5c-fa3e-4a79-b4ab-8b096570a4fb",
    "prId" : 72384,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72384#pullrequestreview-219593976",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93af79c7-0ec5-4fee-81c3-c85b7c64a2cb",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this need to set something that varies each time... setting the same value will no-op in etcd and result in no watch events being delivered",
        "createdAt" : "2019-03-27T16:41:56Z",
        "updatedAt" : "2019-03-27T18:08:41Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "17e20041-71a2-4b11-8d23-1ebb72f59fa6",
        "parentId" : "93af79c7-0ec5-4fee-81c3-c85b7c64a2cb",
        "authorId" : "948c8bee-49af-46ed-93a8-309275b51d61",
        "body" : "Doesn't it add `1` every time?",
        "createdAt" : "2019-03-27T16:49:24Z",
        "updatedAt" : "2019-03-27T18:08:41Z",
        "lastEditedBy" : "948c8bee-49af-46ed-93a8-309275b51d61",
        "tags" : [
        ]
      },
      {
        "id" : "f8e4fde1-fb5e-4a92-8aff-f68832687c9e",
        "parentId" : "93af79c7-0ec5-4fee-81c3-c85b7c64a2cb",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "ah, yes. I missed the `Add`... I thought it was setting to \"1\" every time",
        "createdAt" : "2019-03-27T16:51:50Z",
        "updatedAt" : "2019-03-27T18:08:41Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f58c2ae62d1988d82386cc542899b71cc164e141",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +1626,1630 @@\n\t\tcurrent := resourceQuota.Spec.Hard[resourceName]\n\t\tcurrent.Add(resource.MustParse(\"1\"))\n\t\tresourceQuota.Spec.Hard[resourceName] = current\n\t\t_, err = c.CoreV1().ResourceQuotas(ns).Update(resourceQuota)"
  },
  {
    "id" : "a50722ba-86c0-43cd-9ca5-94b9d7c78e78",
    "prId" : 70968,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70968#pullrequestreview-175974391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cfbcef3-720b-4335-98df-a6ae4bd159bd",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "is this needed?",
        "createdAt" : "2018-11-16T20:41:48Z",
        "updatedAt" : "2018-12-27T18:47:16Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "0560cc59-2bc7-4af5-bafa-3c377f8e93e8",
        "parentId" : "4cfbcef3-720b-4335-98df-a6ae4bd159bd",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "When I movedd both limit_range.go and resource_quote.go, the line is not needed is b/c resource_quota.go used `podName` which is defined in limit_range.go, and now they are separated in two folders, then it's needed.",
        "createdAt" : "2018-11-16T20:46:50Z",
        "updatedAt" : "2018-12-27T18:47:16Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "5492e2f8c3e7e05952ac415555df7a245271c8c2",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +41,45 @@\t// how long to wait for a resource quota update to occur\n\tresourceQuotaTimeout = 30 * time.Second\n\tpodName              = \"pfpod\"\n)\n"
  }
]