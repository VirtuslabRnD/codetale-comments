[
  {
    "id" : "bd9f34f5-4a28-4ecf-9664-f90428405636",
    "prId" : 72407,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72407#pullrequestreview-192191587",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "do we have anything else testing non-default metrics paths if we remove this?",
        "createdAt" : "2018-12-29T06:28:20Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "a40f6ca6-87eb-4b55-ba61-a23d8d0c325b",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "body" : "I think only this e2e test.",
        "createdAt" : "2019-01-02T20:49:55Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "tags" : [
        ]
      },
      {
        "id" : "dd175857-8422-4a83-8ba1-5b2627c549f2",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "that seems useful to preserve. would it be clearer if this image used an obviously non-standard endpoint like \"/custom-metrics-endpoint\"?",
        "createdAt" : "2019-01-02T20:53:38Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "34084063-c13b-4457-be9e-b5dc5934c9ab",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "bda88b53-a264-45f9-b87d-70b1494d83b0",
        "body" : "@liggitt I don't get the point of preserve this if we don't have anything else using this. Preserving this will force us to create a new non-standard endpoint to pass the test and I don't see the advantage of testing two endpoints exposing the same info (one of them a random non-standard endpoint) instead of just one endpoint with that info.\r\nWasn't this endpoint's (/metrics) purpose to \"mock\" standard Prometheus metrics? Remember what the [doc](https://github.com/kubernetes/kubernetes/tree/master/test/images/resource-consumer) says: \"Custom metrics in Prometheus format are exposed on \"/metrics\" endpoint.\"\r\n\r\nMaybe I'm missing something, I'm pretty new contributing here! :)",
        "createdAt" : "2019-01-08T09:54:27Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "bda88b53-a264-45f9-b87d-70b1494d83b0",
        "tags" : [
        ]
      },
      {
        "id" : "4f2cc35a-4f6f-4b07-b798-a5ed267edaff",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "if I understand the test correctly, part of what this is testing is that the `prometheus.io/path` is respected for scraping metrics from non-standard paths. if that is a feature that we support (and it appears we do), we must have test coverage for it to have confidence it continues working.\r\n\r\n/cc @brancz ",
        "createdAt" : "2019-01-08T14:10:34Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "d2f80e5e-68cb-42d3-89bd-e6d5af0c4e83",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "This is highly specific to how Prometheus is configured, as in Prometheus itself has no knowledge of the `prometheus.io/path` annotation. I personally don't think this is something we should be testing at all.",
        "createdAt" : "2019-01-08T14:51:30Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "814065ae-60c3-4a3a-aca5-2a9b290a3192",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "maybe I'm misunderstanding this, then... what does the `prometheus.io/path` annotation do, why is this test using the non-standard path, and how is it working currently?",
        "createdAt" : "2019-01-08T14:54:43Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "e2e07510-7e26-45dd-b50e-fdaedd20c2e2",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "The `prometheus.io/path` annotation is simply (in this specific configuration that is being used) a Prometheus relabeling rule that configures the path, my point is that the annotation itself is free form, it could just as well be `mycompany.io/prometheus-metrics-path`. This is just a matter of how Prometheus is configured. The point I'm trying to make is that I'm not sure testing Prometheus' configuration/discovery system here is something we should be doing.",
        "createdAt" : "2019-01-08T17:18:46Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "2813a015-9499-4975-9de2-442757135657",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "bda88b53-a264-45f9-b87d-70b1494d83b0",
        "body" : "I thought that here we were testing if the resource consumer was exposing the fake Prometheus-format metrics correctly, not the Prometheus configuration or anything else.\r\n",
        "createdAt" : "2019-01-08T17:42:47Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "bda88b53-a264-45f9-b87d-70b1494d83b0",
        "tags" : [
        ]
      },
      {
        "id" : "5f9c2af1-2269-4861-9d0f-a59009879f33",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "if that's what we want to test then as is, is ok or just skip the annotation entirely as `/metrics` is the default",
        "createdAt" : "2019-01-08T19:57:45Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "94d83fbf-6c8c-4e72-b0a3-c5c8a380b1d5",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "Usage of \"/Metrics\" endpoint in resource-consumer precedes implementation of Prometheus tests and adding this addon. Prometheus tests just adapted to what was already available to skip breaking dependencies and releasing new images.  \r\n\r\nResource consumer is not owned by sig-instrumentation, by it's original usage for HPA tests I it was created by sig-autoscaling.",
        "createdAt" : "2019-01-08T20:42:33Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      },
      {
        "id" : "b8155331-fe40-4c0a-9bc4-8579b9a64f53",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "if the test owner doesn't think the distinction is important, I won't object... just wanted to raise the question to make sure they were aware of the previously covered case being dropped, in case this was the only coverage of it",
        "createdAt" : "2019-01-08T20:51:56Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "e08cc33e-46f8-4252-85b9-6e3fc57a94f5",
        "parentId" : "314b28f0-9327-4285-98d9-684e1afbfc9b",
        "authorId" : "bda88b53-a264-45f9-b87d-70b1494d83b0",
        "body" : "@liggitt Who is the test owner? can we ping him? What is blocking this PR?",
        "createdAt" : "2019-01-14T14:59:02Z",
        "updatedAt" : "2019-02-05T09:25:07Z",
        "lastEditedBy" : "bda88b53-a264-45f9-b87d-70b1494d83b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "688e1d105393cbf188004868b7861fd7780ff79c",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +121,125 @@\tserviceAnnotations := map[string]string{\n\t\t\"prometheus.io/scrape\": \"true\",\n\t\t\"prometheus.io/path\":   \"/metrics\",\n\t\t\"prometheus.io/port\":   \"8080\",\n\t}"
  },
  {
    "id" : "67f91a97-d6d3-4060-9612-29113aa54969",
    "prId" : 62195,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/62195#pullrequestreview-111557814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aaba0338-fb7a-4a65-a157-a9e36ec47069",
        "parentId" : null,
        "authorId" : "acf367dc-26b7-4dc5-b0fa-2397add126d0",
        "body" : "What targets are required? I don't ask you do change code here, just want to understand it better. E.g. is a system pod an active target? Why do we expect all targets to be healthy, will it be reported unhealthy if some system component is failing? Should it cause Prometheus tests to fail?",
        "createdAt" : "2018-04-11T12:01:33Z",
        "updatedAt" : "2018-04-13T09:12:21Z",
        "lastEditedBy" : "acf367dc-26b7-4dc5-b0fa-2397add126d0",
        "tags" : [
        ]
      },
      {
        "id" : "3eb67921-6b05-4134-82d1-8cfb2e467162",
        "parentId" : "aaba0338-fb7a-4a65-a157-a9e36ec47069",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "Target is url endpoint that is scraped by prometheus. It can be anything that is found by prometheus service discovery or hardcoded (nodes, pods, services, ingress, apiserver). Prometheus applies rules on those targets that can modify them, add labels or discard them. Active targets are those that were not discarded. For kubernetes we will drop all targets that do not have proper annotations. \r\n\r\nNot healthy targets can mean that prometheus didn't manage to run request (after reconfiguration there is delay or prometheus wasn't fast enough to make request) or request failed (misconfiguration, timeout, unauthorized).\r\n\r\nIn that test I want mainly verify configuration, that all targets are reachable. ",
        "createdAt" : "2018-04-11T12:28:55Z",
        "updatedAt" : "2018-04-13T09:12:21Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      },
      {
        "id" : "36d1f716-f7df-4721-b6e8-5f332de82ad5",
        "parentId" : "aaba0338-fb7a-4a65-a157-a9e36ec47069",
        "authorId" : "acf367dc-26b7-4dc5-b0fa-2397add126d0",
        "body" : "Got it, this is very informative, consider adding this explanation (or part of it) in the code comments as well.\r\n\r\nI'm also thinking if it's possible to hit a window when only one or few targets are discovered and are succeeding, and we didn't get to test some failing targets. Do you think it makes sense to wait for some more representative set of active targets to test? (Not necessarily in this PR)",
        "createdAt" : "2018-04-11T12:44:59Z",
        "updatedAt" : "2018-04-13T09:12:21Z",
        "lastEditedBy" : "acf367dc-26b7-4dc5-b0fa-2397add126d0",
        "tags" : [
        ]
      },
      {
        "id" : "0601000b-10e5-4320-a896-ee084dac574a",
        "parentId" : "aaba0338-fb7a-4a65-a157-a9e36ec47069",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "Some targets are available from configuration (static prometheus self monitoring) and other are depended on service discovery that requires call to kubernetes apiserver. I would guess that there is window between reloading configuration and apiserver response (especially for large clusters).\r\n\r\nWe don't want to pin to specific job, but we may want to ensure that targets were discovered and will not change.",
        "createdAt" : "2018-04-12T08:54:17Z",
        "updatedAt" : "2018-04-13T09:12:21Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      },
      {
        "id" : "68ce558a-741f-49e3-8569-7c55f72d4366",
        "parentId" : "aaba0338-fb7a-4a65-a157-a9e36ec47069",
        "authorId" : "acf367dc-26b7-4dc5-b0fa-2397add126d0",
        "body" : "Sounds good, something like checking that there is the same number of targets between two iterations before proceeding?",
        "createdAt" : "2018-04-12T09:17:49Z",
        "updatedAt" : "2018-04-13T09:12:21Z",
        "lastEditedBy" : "acf367dc-26b7-4dc5-b0fa-2397add126d0",
        "tags" : [
        ]
      },
      {
        "id" : "719ecd5e-c4f7-47f2-81c4-4ed5dc7fcee0",
        "parentId" : "aaba0338-fb7a-4a65-a157-a9e36ec47069",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "I was thinking about something right that with small percent tolerations to handle large clusters.",
        "createdAt" : "2018-04-12T10:09:45Z",
        "updatedAt" : "2018-04-13T09:12:21Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9544222e919f1aec3381620aca3025d10668a071",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +154,158 @@\t}\n\tif len(discovery.ActiveTargets) == 0 {\n\t\treturn fmt.Errorf(\"Prometheus is not scraping any targets, at least one target is required\")\n\t}\n\tfor _, target := range discovery.ActiveTargets {"
  }
]