[
  {
    "id" : "aba28d09-3685-45e4-99b8-5221937a9f32",
    "prId" : 81043,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81043#pullrequestreview-276754371",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c90fb8c-d14d-4330-9357-928e4914776c",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "What happens with the master tainted nodes? ",
        "createdAt" : "2019-08-14T21:06:58Z",
        "updatedAt" : "2019-09-05T19:37:27Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "a2a0b828-fd19-44a4-ada7-b83489be92db",
        "parentId" : "6c90fb8c-d14d-4330-9357-928e4914776c",
        "authorId" : "73cba353-2317-4305-8981-830b29b34769",
        "body" : "I'm not sure I understand the question.\r\n\r\nIf you have a node that is tainted as `node-role.kubernetes.io/master`, that is also the default for the whitelisted taint regexp. This would mean that we would use the path above (not `else` path this comment is on).\r\n\r\nThe end result would be that we would just check that it is ready and doesn't have other NoSchedule taints on it besides that one. Since it doesnt, this method would return `true` and the node would not be considered a blocker for the tests starting up.\r\n\r\nThis will become more clear when I extract the multi-node logic into a more testable situation. That way we can see at a high level when the cluster would start up vs keep polling.",
        "createdAt" : "2019-08-19T19:02:34Z",
        "updatedAt" : "2019-09-05T19:37:27Z",
        "lastEditedBy" : "73cba353-2317-4305-8981-830b29b34769",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c53481d5cf2a779981c83f88e42fd08ed42160f",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +274,278 @@\t\t}\n\t} else {\n\t\tif !IsNodeSchedulable(node) || !IsNodeUntainted(node) {\n\t\t\treturn false\n\t\t}"
  },
  {
    "id" : "58d36bff-381e-4386-b824-2c7f25e986d9",
    "prId" : 79879,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79879#pullrequestreview-262742166",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3dd1e28d-af21-4f93-bf4a-a524aee166e6",
        "parentId" : null,
        "authorId" : "9df64fa3-0a5c-4488-b8c7-8fca4ef36432",
        "body" : "After removing `OrDie` suffix, there were two naming `waitListSchedulableNodes` methods. So I changed this one to `checkWaitListSchedulableNodes`",
        "createdAt" : "2019-07-16T23:40:44Z",
        "updatedAt" : "2019-07-18T06:19:09Z",
        "lastEditedBy" : "9df64fa3-0a5c-4488-b8c7-8fca4ef36432",
        "tags" : [
        ]
      }
    ],
    "commit" : "e24a9628210cf47b18874db4695638282fb63c1a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +201,205 @@// checkWaitListSchedulableNodes is a wrapper around listing nodes supporting retries.\nfunc checkWaitListSchedulableNodes(c clientset.Interface) (*v1.NodeList, error) {\n\tnodes, err := waitListSchedulableNodes(c)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error: %s. Non-retryable failure or timed out while listing nodes for e2e cluster\", err)"
  }
]