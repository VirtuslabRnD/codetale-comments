[
  {
    "id" : "f76052f0-5bca-4c9f-bbbc-26b08171ed52",
    "prId" : 34645,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34645#pullrequestreview-4251005",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6e124bb-180b-4e45-b589-947d30d009aa",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "will increasing the quota also satisfy the rc and remove failed condition?\n",
        "createdAt" : "2016-10-14T11:21:24Z",
        "updatedAt" : "2016-11-02T12:08:25Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "a41c2591-f016-4925-a4cd-2f4dbe896f51",
        "parentId" : "e6e124bb-180b-4e45-b589-947d30d009aa",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "It would if updating the quota would trigger all resources in the namespace to be reconciled. Right now, we need to wait for the controller interval to pass in order for the rc to be reconciled and updated. \n",
        "createdAt" : "2016-10-14T11:26:58Z",
        "updatedAt" : "2016-11-02T12:08:25Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e865692d441d2d2cbeeb6cf354f643eda01de8d4",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +206,210 @@\tExpect(err).NotTo(HaveOccurred())\n\n\tBy(fmt.Sprintf(\"Scaling down rc %q to satisfy pod quota\", name))\n\trc, err = framework.UpdateReplicationControllerWithRetries(c, namespace, name, func(update *api.ReplicationController) {\n\t\tupdate.Spec.Replicas = 2"
  },
  {
    "id" : "7e7f4ef7-3fe1-4546-b68f-3b4728eb392b",
    "prId" : 6712,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a4d6671-4503-46f0-8d24-59b31e94ba5c",
        "parentId" : null,
        "authorId" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "body" : "unimportant nit, but I noticed that you lowercased L131 but not this line. :)\n",
        "createdAt" : "2015-04-15T00:22:56Z",
        "updatedAt" : "2015-04-15T00:43:07Z",
        "lastEditedBy" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "tags" : [
        ]
      },
      {
        "id" : "5abce6b8-4199-4106-899e-82b86a55516a",
        "parentId" : "6a4d6671-4503-46f0-8d24-59b31e94ba5c",
        "authorId" : null,
        "body" : "Done.  Thanks for spotting that.\n",
        "createdAt" : "2015-04-15T00:43:37Z",
        "updatedAt" : "2015-04-15T00:43:37Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "4e7998379ffd746b2f5803496ec0cb714ef953bf",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +139,143 @@\n\t// Verify that something is listening.\n\tBy(\"Trying to dial each unique pod\")\n\tretryTimeout := 2 * time.Minute\n\tretryInterval := 5 * time.Second"
  },
  {
    "id" : "bf4078b6-2957-4dfb-b090-5fcf5dec5d44",
    "prId" : 4269,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1c5fa21-ccc9-40e0-b3c4-8296fd543742",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "If this becomes a common pattern (or already is, I haven't read through too much of the other test code yet), we should make a utility function to handle timeouts consistently. Something like Until(listTimeout, func() { })\n",
        "createdAt" : "2015-02-10T03:37:45Z",
        "updatedAt" : "2015-02-10T19:24:29Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "6e0d957f-bfcd-4d29-90b6-620dc0b42797",
        "parentId" : "c1c5fa21-ccc9-40e0-b3c4-8296fd543742",
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "Yeah, I noticed this as I was porting it. We now have three timeout mechanisms that I know of in the system There's this one, which is actually nice because it's \"true\". There's this one: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/test/e2e/util.go#L43 (which uses the \"sleep and add up the sleeps\" method). And then when I was porting Brendan's endpoints test, I gutted his goroutine based individual timeouts and actually forced the test into a full test timeout: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/test/e2e/service.go#L248 (which is the combination of that timeout and the \"done Done\" on the function signature).\n\nI can't decide if we want fine-grained timeouts for individual actions or to just go through and profile whole tests, honestly. We probably want some combination of both.\n",
        "createdAt" : "2015-02-10T15:58:13Z",
        "updatedAt" : "2015-02-10T19:24:29Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "3538e45c-a61b-4554-9b74-416730bd9eba",
        "parentId" : "c1c5fa21-ccc9-40e0-b3c4-8296fd543742",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I created #4292 to discuss outside of this PR. \n",
        "createdAt" : "2015-02-10T19:02:11Z",
        "updatedAt" : "2015-02-10T19:24:29Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7efc60580e25f0c35e2e6ca47b98bf08ea65000e",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +115,119 @@\tExpect(err).NotTo(HaveOccurred())\n\tt := time.Now()\n\tfor {\n\t\tBy(fmt.Sprintf(\"Controller %s: Found %d pods out of %d\", name, len(pods.Items), replicas))\n\t\tif len(pods.Items) == replicas {"
  }
]