[
  {
    "id" : "87175b26-a741-4b03-ab3e-fdb73acd4a86",
    "prId" : 39615,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39615#pullrequestreview-15816253",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64c02db4-d31a-4c0a-8964-0793cb9b85f3",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "At some point we're going to need to refactor this, it's not really a sustainable method. ",
        "createdAt" : "2017-01-09T22:31:12Z",
        "updatedAt" : "2017-01-09T22:31:12Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "e8fc6945-97f6-4086-a024-932946a85a89",
        "parentId" : "64c02db4-d31a-4c0a-8964-0793cb9b85f3",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yup. I agree - this is the reason why I originally wanted to move this to perf-test repo and create a separate binary that can read dedicated perf-test config or something. But I don't have time to do that...",
        "createdAt" : "2017-01-09T22:59:04Z",
        "updatedAt" : "2017-01-09T22:59:04Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e79abe542447f16e468f6538472f07286e57e52",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +396,400 @@\t\t{podsPerNode: 95, runLatencyTest: true, kind: api.Kind(\"ReplicationController\")},\n\t\t{podsPerNode: 100, runLatencyTest: false, kind: api.Kind(\"ReplicationController\")},\n\t\t// Tests for other resource types:\n\t\t{podsPerNode: 30, runLatencyTest: true, kind: extensions.Kind(\"Deployment\")},\n\t\t{podsPerNode: 30, runLatencyTest: true, kind: batch.Kind(\"Job\")},"
  },
  {
    "id" : "2879d1eb-0487-428c-a9c1-9142426896a8",
    "prId" : 38461,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38461#pullrequestreview-12649875",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acec7735-11b0-4a71-84a9-02cee830007b",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Please remove TODO above.",
        "createdAt" : "2016-12-13T10:44:38Z",
        "updatedAt" : "2016-12-13T12:21:38Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9e78f1cd5eefd064e4943811871b0839a5534c6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +259,263 @@\t\tnamespace := dtc.Configs[i].GetNamespace()\n\t\tkind := dtc.Configs[i].GetKind()\n\t\tif framework.TestContext.GarbageCollectorEnabled && kindSupportsGarbageCollector(kind) {\n\t\t\tBy(fmt.Sprintf(\"Cleaning up only the %v, garbage collector will clean up the pods\", kind))\n\t\t\terr := framework.DeleteResourceAndWaitForGC(dtc.ClientSet, kind, namespace, name)"
  },
  {
    "id" : "b59bcf96-f74f-47d4-a294-6cb070980bc2",
    "prId" : 38292,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38292#pullrequestreview-12186723",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b447bee7-a8c4-4064-ac49-bfe9871a0ce4",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This is already merged - can you please rebase?",
        "createdAt" : "2016-12-09T07:15:44Z",
        "updatedAt" : "2016-12-09T13:31:54Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "ca741fff-9aba-4de8-bcde-c787d6411720",
        "parentId" : "b447bee7-a8c4-4064-ac49-bfe9871a0ce4",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.",
        "createdAt" : "2016-12-09T08:41:45Z",
        "updatedAt" : "2016-12-09T13:31:54Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfe2a2b03c21c2a4c57d77dbddb5425fb81922e4",
    "line" : null,
    "diffHunk" : "@@ -1,1 +66,70 @@\t// What kind of resource we want to create\n\tkind          schema.GroupKind\n\tSecretConfigs []*testutils.SecretConfig\n\tDaemonConfigs []*testutils.DaemonConfig\n}"
  },
  {
    "id" : "e6ebb89a-fd92-403b-b4d5-4cfc3edc3a79",
    "prId" : 37837,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/37837#pullrequestreview-11597757",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c307c1db-a29c-4df6-84cd-7203f28532d3",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I think we should configure it differently. In a real world, i don't think that all secrets in the system will be different. \r\nI think that what we should do is to:\r\n- create a number of secrets (this number should be the parameters)\r\n- mount (up to?) <secretsPerPod> random secrets to each pod\r\n\r\nHaving each pod a different set of secrets is a bit artificial.",
        "createdAt" : "2016-12-06T14:01:05Z",
        "updatedAt" : "2016-12-08T10:15:08Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "8570d3c6-2e15-4c23-8ae0-0fd9f4bae6e1",
        "parentId" : "c307c1db-a29c-4df6-84cd-7203f28532d3",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Discussed f2f - we mount the same 'secretsPerPod' number of secrets for all Pods in a single RC/RS/Dep.",
        "createdAt" : "2016-12-06T14:31:01Z",
        "updatedAt" : "2016-12-08T10:15:08Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "be3889810dd2a7a5ce8cf8315a74f7ac46169a41",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +428,432 @@\t\t\t\tsecretNames := []string{}\n\t\t\t\tfor j := 0; j < itArg.secretsPerPod; j++ {\n\t\t\t\t\tsecretName := fmt.Sprintf(\"density-secret-%v-%v\", i, j)\n\t\t\t\t\tsecretConfigs = append(secretConfigs, &testutils.SecretConfig{\n\t\t\t\t\t\tContent:   map[string]string{\"foo\": \"bar\"},"
  },
  {
    "id" : "7d7a3259-260c-440c-ab78-6b7d06c833e0",
    "prId" : 31094,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "889729f8-dad7-43b3-b4ea-deba3eb16ba3",
        "parentId" : null,
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Any issue number?\n",
        "createdAt" : "2016-08-22T08:00:21Z",
        "updatedAt" : "2016-08-22T08:03:55Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "96c16c23-ddbc-471c-bf1a-24464b7c7f9e",
        "parentId" : "889729f8-dad7-43b3-b4ea-deba3eb16ba3",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "done\n",
        "createdAt" : "2016-08-22T08:04:12Z",
        "updatedAt" : "2016-08-22T08:04:12Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "943860b3c74e7237e7de3a2adc3aef6d9acb06b9",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +104,108 @@\t} else {\n\t\tif numNodes <= 100 {\n\t\t\t// TODO: Investigate higher apiserver consumption and\n\t\t\t// potentially revert to 1.5cpu and 1.3GB - see #30871\n\t\t\tapiserverCPU = 1.8"
  },
  {
    "id" : "945d6a26-4430-4846-905b-f1c1940fdd7f",
    "prId" : 25968,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20891956-d6fc-4f45-95e8-69b4a929da97",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "This won't work, as you need to take into account Pods already running in the cluster (e.g. KubeProxy on each node). Take a look at SchedulerPredicates test.\n",
        "createdAt" : "2016-05-20T15:52:22Z",
        "updatedAt" : "2016-08-04T15:46:41Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "5ef0e44d-fd11-4cb5-91a6-dd62cc11ed54",
        "parentId" : "20891956-d6fc-4f45-95e8-69b4a929da97",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Done\n",
        "createdAt" : "2016-05-23T16:19:19Z",
        "updatedAt" : "2016-08-04T15:46:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb72b501352544c943652616a1955ff26364b839",
    "line" : null,
    "diffHunk" : "@@ -1,1 +668,672 @@\tIt(\"[Feature:ManualPerformance] should allow running maximum capacity pods on nodes\", func() {\n\t\ttotalPods = 0\n\t\tfor _, n := range nodes.Items {\n\t\t\ttotalPods += int(n.Status.Capacity.Pods().Value())\n\t\t}"
  },
  {
    "id" : "81ca8f8e-195c-46ee-a4b6-b67745c8fdcc",
    "prId" : 25968,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92aabe53-b3d7-41bf-bf44-b5f28963681f",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Add a comment what the return value is.\n",
        "createdAt" : "2016-08-04T10:24:17Z",
        "updatedAt" : "2016-08-04T15:46:41Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "c1816a04-fe13-478c-9161-2f9da624cd0c",
        "parentId" : "92aabe53-b3d7-41bf-bf44-b5f28963681f",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Done\n",
        "createdAt" : "2016-08-04T14:37:53Z",
        "updatedAt" : "2016-08-04T15:46:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb72b501352544c943652616a1955ff26364b839",
    "line" : null,
    "diffHunk" : "@@ -1,1 +180,184 @@// runDensityTest will perform a density test and return the time it took for\n// all pods to start\nfunc runDensityTest(dtc DensityTestConfig) time.Duration {\n\tdefer GinkgoRecover()\n\t// Create a listener for events."
  },
  {
    "id" : "bc9827db-e15e-4c0c-970b-2bdad42cee73",
    "prId" : 25968,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "973afe61-94f6-4c08-ae65-62f5dd3d62d7",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "You probably want to have a flag or env var that enables/disables writing output files (when we had this kind of behavior previously it ended in a number of disk full problems).\n",
        "createdAt" : "2016-08-04T10:26:44Z",
        "updatedAt" : "2016-08-04T15:46:41Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "515ffe3c-443d-4e05-ab15-fed524f17c8c",
        "parentId" : "973afe61-94f6-4c08-ae65-62f5dd3d62d7",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "The disk usage should end up being similar to other density tests and they haven been small text files that haven't caused a problem (yet).  This data file is pretty important for our performance evaluations, so for the time being I think I would like to keep it like the other density tests.  If this file causes disk space issues something is seriously wrong (like the test timeout doesn't work).\n",
        "createdAt" : "2016-08-04T15:16:27Z",
        "updatedAt" : "2016-08-04T15:46:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb72b501352544c943652616a1955ff26364b839",
    "line" : 396,
    "diffHunk" : "@@ -1,1 +673,677 @@\t\ttotalPods -= framework.WaitForStableCluster(c, masters)\n\n\t\tfileHndl, err := os.Create(fmt.Sprintf(framework.TestContext.OutputDir+\"/%s/pod_states.csv\", uuid))\n\t\tframework.ExpectNoError(err)\n\t\tdefer fileHndl.Close()"
  },
  {
    "id" : "864e2428-35a4-452b-9d0a-d26ee98f16f6",
    "prId" : 24434,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b05d6b20-2e81-45b0-b6fe-f56a5758e6f4",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "And then, we probably want Silent: false\n",
        "createdAt" : "2016-04-20T10:19:54Z",
        "updatedAt" : "2016-04-21T15:20:14Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "ef4d5548-1914-41e2-bd2b-0eb7d981b0c3",
        "parentId" : "b05d6b20-2e81-45b0-b6fe-f56a5758e6f4",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I'd rather keep the new logging on - it doesn't change output.\n",
        "createdAt" : "2016-04-21T14:21:22Z",
        "updatedAt" : "2016-04-21T15:20:14Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "d344c2e32b443237892200eba72da8e00711cf67",
    "line" : null,
    "diffHunk" : "@@ -1,1 +243,247 @@\t\t\t\t\tMemRequest:           nodeMemCapacity / 100,\n\t\t\t\t\tMaxContainerFailures: &MaxContainerFailures,\n\t\t\t\t\tSilent:               true,\n\t\t\t\t}\n\t\t\t}"
  },
  {
    "id" : "5e20ed0f-8c5e-4472-b853-be974c832e1b",
    "prId" : 22513,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acade396-86c7-43bb-8272-45b2439687d6",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Hmm.....the qps limit of the apiserver client is more of a bottleneck now than docker during batch creation.\n",
        "createdAt" : "2016-03-04T18:45:22Z",
        "updatedAt" : "2016-03-04T18:45:22Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "26e14580-cd84-4ece-9dd3-d77fb6ed7b00",
        "parentId" : "acade396-86c7-43bb-8272-45b2439687d6",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "How many QPS does Kubelet client allow?\n",
        "createdAt" : "2016-03-04T19:12:13Z",
        "updatedAt" : "2016-03-04T19:12:13Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "4e5ee1e9-ef13-4b96-b749-228b8fea17ba",
        "parentId" : "acade396-86c7-43bb-8272-45b2439687d6",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "5.0.\n\nSome numbers:\nwith 5 qps, starting 100 pods per node takes 1m50s to 2m20s\nwith 100qps, starting 100 pods per node takes ~1m20s\n",
        "createdAt" : "2016-03-05T01:06:14Z",
        "updatedAt" : "2016-03-05T01:06:14Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "0dc6af49-8b3e-43d1-b213-e150e5ac72e7",
        "parentId" : "acade396-86c7-43bb-8272-45b2439687d6",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "@wojtek-t - we probably should increase this.\n",
        "createdAt" : "2016-03-05T10:03:40Z",
        "updatedAt" : "2016-03-05T10:03:40Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "531bc884-d6c5-44f7-a561-70ffa79e776c",
        "parentId" : "acade396-86c7-43bb-8272-45b2439687d6",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "100 is too much in my opinion\nBut probably increasing to 10 or 20 should be ok - we should make some experiments.\n",
        "createdAt" : "2016-03-05T10:15:49Z",
        "updatedAt" : "2016-03-05T10:15:49Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "43a8f04193596599a182bea95de818d5e0635235",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +116,120 @@// IMPORTANT: This test is designed to work on large (>= 100 Nodes) clusters. For smaller ones\n// results will not be representative for control-plane performance as we'll start hitting\n// limits on Docker's concurrent container startup.\nvar _ = Describe(\"Density\", func() {\n\tvar c *client.Client"
  },
  {
    "id" : "ac42a593-1dc0-46ae-b6b5-af6ab30e86d8",
    "prId" : 22055,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8bcd03c-333f-4459-aae1-a2fb1cbd7d5d",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Please make this 10 a constant (minPodThroughput or sth like that)\n",
        "createdAt" : "2016-02-26T15:04:09Z",
        "updatedAt" : "2016-03-01T08:31:51Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9abcfa5122dd5bb734d0600aa6b71ff8171031f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +502,506 @@\n\t\t\tsaturationThreshold := time.Duration((totalPods / MinPodsPerSecondThroughput)) * time.Second\n\t\t\tif saturationThreshold < MinSaturationThreshold {\n\t\t\t\tsaturationThreshold = MinSaturationThreshold\n\t\t\t}"
  },
  {
    "id" : "a326cf15-d1ce-49e3-b844-0f1ccfced418",
    "prId" : 21882,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "535099e1-7f4d-4d1c-9c79-62c5bffa1583",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "remove empty line after it\n",
        "createdAt" : "2016-02-25T10:42:13Z",
        "updatedAt" : "2016-02-29T08:32:40Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "110340c4675e2c065eef6fbebaa11aea7113affd",
    "line" : null,
    "diffHunk" : "@@ -1,1 +140,144 @@\n\tBeforeEach(func() {\n\t\tc = framework.Client\n\t\tns = framework.Namespace.Name\n"
  },
  {
    "id" : "24a3d5f8-b85b-4d0f-b05c-8cce9451674d",
    "prId" : 19440,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78d59c08-54c0-4134-8dd6-39bf3a74bebf",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Why couting both Add and Delete?\n",
        "createdAt" : "2016-01-12T18:26:00Z",
        "updatedAt" : "2016-01-14T00:16:36Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "b3ca1197-6cb5-4c67-a1d3-0f5de939087f",
        "parentId" : "78d59c08-54c0-4134-8dd6-39bf3a74bebf",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "I should remove them to just count \"Update\" count, it should be more meaningful for status manager. :)\n",
        "createdAt" : "2016-01-12T19:04:44Z",
        "updatedAt" : "2016-01-14T00:16:36Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "97c6ecd0-1fa6-4958-b418-f3cf1b3fe376",
        "parentId" : "78d59c08-54c0-4134-8dd6-39bf3a74bebf",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Done\n",
        "createdAt" : "2016-01-12T19:54:45Z",
        "updatedAt" : "2016-01-14T00:16:36Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c36b611ea9185ceb11314f626fe22999fc2d4fa",
    "line" : null,
    "diffHunk" : "@@ -1,1 +279,283 @@\t\t\t\t\t\tupdateCount++\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t)\n\t\t\tgo updateController.Run(stop)"
  },
  {
    "id" : "f10d11de-2006-4448-ac12-af3597f1d734",
    "prId" : 19440,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "321bff32-f15e-413f-b0c6-5cf0b97d556a",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "As we discussed offline, `updateCount` is accessed by multiple goroutines and will need to be fixed. \n",
        "createdAt" : "2016-01-13T00:24:16Z",
        "updatedAt" : "2016-01-14T00:16:36Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "c53f24da-258f-4fcc-bff0-b4921cd1c7cf",
        "parentId" : "321bff32-f15e-413f-b0c6-5cf0b97d556a",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "ACK. :)\n",
        "createdAt" : "2016-01-13T01:01:21Z",
        "updatedAt" : "2016-01-14T00:16:36Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "8c40918c-4cc6-4c6b-96ac-a895aa34ae09",
        "parentId" : "321bff32-f15e-413f-b0c6-5cf0b97d556a",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Done.\n",
        "createdAt" : "2016-01-14T00:17:19Z",
        "updatedAt" : "2016-01-14T00:17:30Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c36b611ea9185ceb11314f626fe22999fc2d4fa",
    "line" : null,
    "diffHunk" : "@@ -1,1 +258,262 @@\t\t\t// uLock is a lock protects the updateCount\n\t\t\tvar uLock sync.Mutex\n\t\t\tupdateCount := 0\n\t\t\tlabel := labels.SelectorFromSet(labels.Set(map[string]string{\"name\": RCName}))\n\t\t\t_, updateController := controllerframework.NewInformer("
  },
  {
    "id" : "4340b7f9-1a0d-4a5f-b1c2-9816d7945a14",
    "prId" : 14055,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0271e5e1-b296-4e77-a3d3-7c38f34b9f07",
        "parentId" : null,
        "authorId" : null,
        "body" : "Given that you're doing this, and that this deletes the namespace and everything in it, can you perhaps delete the redundant replication controller cleanup code above?\n",
        "createdAt" : "2015-09-17T22:29:32Z",
        "updatedAt" : "2015-09-21T08:43:58Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "b952dbc7-99a0-4e53-aaa3-89ebac4b4fde",
        "parentId" : "0271e5e1-b296-4e77-a3d3-7c38f34b9f07",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "It's not that simple, because we're afraid of timeouts. On scalability cluster we need to delete 3000 pods, which takes some time, and I'm not certain how close to namespace deletion timeout we're running when deleting only events. I recall having this kind of problem with 'scheduler_predicates' test.\n",
        "createdAt" : "2015-09-19T19:01:10Z",
        "updatedAt" : "2015-09-21T08:43:58Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "393baa51574e5fdb21ff60396117d98cbffceef3",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +144,148 @@\t\tExpect(highLatencyRequests).NotTo(BeNumerically(\">\", 0), \"There should be no high-latency requests\")\n\n\t\tframework.afterEach()\n\t})\n"
  },
  {
    "id" : "d56dae4c-ea0f-408a-8945-1a95d18e0411",
    "prId" : 10020,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c7e7411-821a-42b1-8532-b755b59094b9",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Nit: if you're trying to rate limit to 5qps you can use util.throttle\n",
        "createdAt" : "2015-06-19T14:21:07Z",
        "updatedAt" : "2015-06-19T14:21:07Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "45263c3549ee908e6f59324c83ad065a808396f6",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +294,298 @@\t\t\t\t\tname := additionalPodsPrefix + \"-\" + strconv.Itoa(i)\n\t\t\t\t\tgo createRunningPod(&wg, c, name, ns, \"gcr.io/google_containers/pause:go\", podLabels)\n\t\t\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\t\t}\n\t\t\t\twg.Wait()"
  },
  {
    "id" : "091897c1-169e-4b45-9de6-505f2c7ccf80",
    "prId" : 8809,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0ff4942-ec22-45bc-a85c-86ceda32eaf4",
        "parentId" : null,
        "authorId" : null,
        "body" : "What units?  Better to make this a time.Duration.\n",
        "createdAt" : "2015-05-29T22:36:53Z",
        "updatedAt" : "2015-06-03T19:00:38Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "a248d0ccf92d8204c77a539bbdee809a9e296985",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +98,102 @@\t\tpodsPerMinion int\n\t\t/* Controls how often the apiserver is polled for pods */\n\t\tinterval int\n\t}\n"
  },
  {
    "id" : "45c1af87-cd0f-454b-9cf6-f340f10f4e5c",
    "prId" : 8809,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db8a5caf-f21f-499c-a163-555f599ed3fc",
        "parentId" : null,
        "authorId" : null,
        "body" : "See above. time.Second \\* 10\n",
        "createdAt" : "2015-05-29T22:37:28Z",
        "updatedAt" : "2015-06-03T19:00:38Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "a248d0ccf92d8204c77a539bbdee809a9e296985",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +103,107 @@\tdensityTests := []Density{\n\t\t// This test should always run, even if larger densities are skipped.\n\t\t{podsPerMinion: 3, skip: false, interval: 10},\n\t\t{podsPerMinion: 30, skip: false, interval: 10},\n\t\t// More than 30 pods per node is outside our v1.0 goals."
  },
  {
    "id" : "63e84b54-cf03-4f8a-a736-385dc5a20e8e",
    "prId" : 8809,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8823378e-7dfe-4234-b704-aeda1036e311",
        "parentId" : null,
        "authorId" : null,
        "body" : "As above.\n",
        "createdAt" : "2015-05-29T22:37:41Z",
        "updatedAt" : "2015-06-03T19:00:38Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "a248d0ccf92d8204c77a539bbdee809a9e296985",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +104,108 @@\t\t// This test should always run, even if larger densities are skipped.\n\t\t{podsPerMinion: 3, skip: false, interval: 10},\n\t\t{podsPerMinion: 30, skip: false, interval: 10},\n\t\t// More than 30 pods per node is outside our v1.0 goals.\n\t\t// We might want to enable those tests in the future."
  },
  {
    "id" : "2dae327a-9228-405c-9737-24a04c0179ec",
    "prId" : 8809,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59c78e69-14c2-4a41-91c0-c3694323ef12",
        "parentId" : null,
        "authorId" : null,
        "body" : "As above.\n",
        "createdAt" : "2015-05-29T22:37:48Z",
        "updatedAt" : "2015-06-03T19:00:38Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "a248d0ccf92d8204c77a539bbdee809a9e296985",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +107,111 @@\t\t// More than 30 pods per node is outside our v1.0 goals.\n\t\t// We might want to enable those tests in the future.\n\t\t{podsPerMinion: 50, skip: true, interval: 10},\n\t\t{podsPerMinion: 100, skip: true, interval: 1},\n\t}"
  },
  {
    "id" : "30716d5d-87c4-4000-8208-e4e5e77c1875",
    "prId" : 8809,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e12aaf4-3c43-4393-8bf2-2658e43a146a",
        "parentId" : null,
        "authorId" : null,
        "body" : "As above.\n",
        "createdAt" : "2015-05-29T22:37:55Z",
        "updatedAt" : "2015-06-03T19:00:39Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "a248d0ccf92d8204c77a539bbdee809a9e296985",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +108,112 @@\t\t// We might want to enable those tests in the future.\n\t\t{podsPerMinion: 50, skip: true, interval: 10},\n\t\t{podsPerMinion: 100, skip: true, interval: 1},\n\t}\n"
  },
  {
    "id" : "f12709cf-4674-4ed3-acc7-8ed43a2fb7c6",
    "prId" : 7505,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8359d8a-3002-4dac-89f0-34e98545ee6f",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Reset this before the test? otherwise could pass or fail depending on what you ran before running this test, no? (e.g. if you just ran the rest of the e2e suite, then there'd be enough extra OK requests that the offending requests are pushed into the 99th %ile.)\n",
        "createdAt" : "2015-04-29T16:16:11Z",
        "updatedAt" : "2015-05-05T09:51:32Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "0b8a8cce-2245-487d-91da-c10f218d3b52",
        "parentId" : "f8359d8a-3002-4dac-89f0-34e98545ee6f",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Currently it's impossible to reset those metrics. The only way would be to restart apiserver.\n\nIf we really want to count this properly we'd need to read the full histogram (not the summary) at the beginning, and subtract it from the one read at the end. Writing such code is of course doable, but not that straight forward.\n\nMy hope is that once we have prometheus server set up we will be able to write more advanced queries and read it easily.\n\nI think that currently the effect you described will not be that huge, because other density tests are much smaller than the biggest one and as a result should not affect overall metrics too much.\n\nIf you don't mind I'd submit this change as is (it's better than nothing) and improve this in time.\n",
        "createdAt" : "2015-04-30T19:11:18Z",
        "updatedAt" : "2015-05-05T09:51:32Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "bd61512c-a119-43a1-a107-2b750e3bb790",
        "parentId" : "f8359d8a-3002-4dac-89f0-34e98545ee6f",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "OK, can you add a TODO somewhere noting this?\n",
        "createdAt" : "2015-05-04T20:29:49Z",
        "updatedAt" : "2015-05-05T09:51:32Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "bb50df62-11f0-4502-8dca-8ddda3e7aadd",
        "parentId" : "f8359d8a-3002-4dac-89f0-34e98545ee6f",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Done.\n",
        "createdAt" : "2015-05-05T08:25:05Z",
        "updatedAt" : "2015-05-05T09:51:32Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "151bebaf-75fd-4315-943a-69136e5c9996",
        "parentId" : "f8359d8a-3002-4dac-89f0-34e98545ee6f",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "We only care about high percentiles, but rephrased.\n\nDone.\n",
        "createdAt" : "2015-05-05T09:52:29Z",
        "updatedAt" : "2015-05-05T09:52:29Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "551cec2a21c5480f2f55cde6cc69857b85e4031b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +159,163 @@\t\t\t// TODO: Update threshold to 1s once we reach this goal\n\t\t\t// TODO: We should reset metrics before the test. Currently previous tests influence latency metrics.\n\t\t\thighLatencyRequests, err := HighLatencyRequests(c, 10*time.Second)\n\t\t\texpectNoError(err)\n\t\t\tExpect(highLatencyRequests).NotTo(BeNumerically(\">\", 0))"
  },
  {
    "id" : "4858be17-7638-4abd-b6ca-a74eb01b2c1b",
    "prId" : 7235,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "887690fd-3373-4ee0-ad83-e01785573a6d",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Just curious: why do you replace it? The \"glog\" format seems to have more information.\n",
        "createdAt" : "2015-04-24T11:22:41Z",
        "updatedAt" : "2015-04-24T11:22:41Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "6f176634-5439-49d0-afe4-eefe121237e0",
        "parentId" : "887690fd-3373-4ee0-ad83-e01785573a6d",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "We want all logging to be done through ginkgo's logging facilities.  That's why Logf was written.\n",
        "createdAt" : "2015-04-24T11:29:28Z",
        "updatedAt" : "2015-04-24T11:29:28Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "9ed74407-0b01-4dec-92ec-574893a2a215",
        "parentId" : "887690fd-3373-4ee0-ad83-e01785573a6d",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I see - that's why we are using a different logging in the code itself than in e2e test, right?\n",
        "createdAt" : "2015-04-24T11:37:30Z",
        "updatedAt" : "2015-04-24T11:37:30Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "6fecda54-acd9-453a-b9f0-4ab56cbea83d",
        "parentId" : "887690fd-3373-4ee0-ad83-e01785573a6d",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Correct.  Ginkgo allows parallelizing tests and other features and using its logging facility ensures the logs stay with the test and get rolled up in reports generated from ginkgo logs.\n",
        "createdAt" : "2015-04-24T11:49:24Z",
        "updatedAt" : "2015-04-24T11:49:24Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "31684d187187c49ec0f84c4fd7abe59f14c2f861",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +199,203 @@\t\t\t\t\t\texpectNoError(RunRC(c, name, ns, \"gcr.io/google_containers/pause:go\", n))\n\t\t\t\t\t\tpodsLaunched += n\n\t\t\t\t\t\tLogf(\"Launched %v pods so far...\", podsLaunched)\n\t\t\t\t\t\terr := DeleteRC(c, ns, name)\n\t\t\t\t\t\texpectNoError(err)"
  },
  {
    "id" : "902b823b-fec9-4443-9150-7155d0095ac0",
    "prId" : 6638,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "This is a really inefficient way of getting all the events you want. I recommend waiting for #6546 to land and using the NewInformer() thing it adds to collect events you care about.\n",
        "createdAt" : "2015-04-09T18:05:26Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "6152ccb6-2a00-4e7f-b0da-6ea7939b431c",
        "parentId" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "How close is #6546 to landing? Can we accept as is and log another enh to convert to NewInformer?\n",
        "createdAt" : "2015-04-09T18:09:39Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "ce1d5608-006d-4138-99f2-49c3f95a348a",
        "parentId" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "#6546 should hopefully merge today. I am concerned that this method of getting events is going to be hard enough on apiserver that it will actually cause a performance problem. I'm also not sure about this stopping condition-- is it a guarantee that no events are generated in steady state? That may currently be the case but I don't think it's guaranteed...\n",
        "createdAt" : "2015-04-09T20:34:15Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "80837a1f-0511-4b18-9e73-8e17566502a0",
        "parentId" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "In my testing the only problems came about when a pod was continually restarted as that causes a never ending spew of events.  Even cases where there was an error the test went okay.  I dislike the notion of the test hanging because of a pod continually restarting, but atm I don't see a way to avoid it.  Either the test provides a false positive by shutting off the event collection too soon and potentially missing an issue, or it hangs.  The later seemed better because it would force an investigation, but neither are desirable imo.\n\nI could put in an insanely large timeout, like 20 minutes or something.  Thoughts @lavalamp ?\n",
        "createdAt" : "2015-04-10T14:40:28Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "8129bdcc-33b1-4e15-8345-8f8d431e82bd",
        "parentId" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Sorry for delay-- #6546 has merged!\n\nI recommend using that to collect events, run your assertion on every event you see, and stop the controller after the replication controller has shut down. I don't know that it's worth it to wait for every last event-- there's not really a firm bound on how long that could take.\n",
        "createdAt" : "2015-04-14T22:18:27Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "c123bc8f-fdb5-4f74-8fc5-195cf6f09059",
        "parentId" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "@lavalamp I'm not sure I see how using 6546 will help ensure that we inspect all the events that are logged.  I originally only cared about the start events, but there's no reason not to care about the shutdown events as well.  If I move the inspection of the events until after the rc is deleted I should be able to wait for all the events to be logged.  As long as the system is properly ensuring pods are shut down then the event stream should stop within a reasonable time after all the pods are stopped.  This should bound the test and prevent it from running forever.\n\nI'm not sure how to do that with 6546 though.  I don't see any way to use that change and know that events have all been generated and it's ok to stop the controller.\n",
        "createdAt" : "2015-04-20T16:54:59Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "9b522caa-54b0-4f28-b267-cae8afcd6898",
        "parentId" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "You could use exactly the same logic-- stop if you haven't gotten an event in 10 seconds. It's just that the current code does a lot of lists which are unfortunately quite heavyweight.\n",
        "createdAt" : "2015-04-21T00:31:55Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "ce8f138c-26c2-4780-b446-0e81a56cb76b",
        "parentId" : "eac2977b-0de3-4160-9118-4c6cf95dd68f",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "If the concern is the efficiency of the test, then I would rather get the change in now and enhance it later.  Ensuring the condition is tested is more important than ensuring it is done in the most efficient way imo.  I couldn't say that any other e2e test is performing its functions in the most efficient way possible.\n\nMy concern has been bounding the event collection loop, and I've done that in a change I can push.\n",
        "createdAt" : "2015-04-21T14:13:45Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "020ba6a6c7611e8418d6a7658ca4b1494c0a9ead",
    "line" : null,
    "diffHunk" : "@@ -1,1 +304,308 @@\t\t\ttimeout := 10 * time.Minute\n\t\t\tfor start := time.Now(); last < current && time.Since(start) < timeout; time.Sleep(10 * time.Second) {\n\t\t\t\tlast = current\n\t\t\t\tcurrent = len(events)\n\t\t\t}"
  },
  {
    "id" : "ff30275e-b43f-4f3e-aad3-28c26010dbec",
    "prId" : 6638,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2749ced2-8c9f-41c7-bda0-3e32f914b61b",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "This seems much better in the defer, which will ensure that it's cleaned up no matter what. If you want to not clean it up if it didn't get created, I think it's better to make an `var rcCreated bool` and conditioning the teardown on that.\n",
        "createdAt" : "2015-04-09T18:06:56Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "42203f88-b318-489b-b432-2c8160fe2b52",
        "parentId" : "2749ced2-8c9f-41c7-bda0-3e32f914b61b",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "What defer block?  The one for the RC, because that was removed in RunRC\n",
        "createdAt" : "2015-04-09T18:08:57Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "436e5c90-caf9-4e45-b8cd-014f64ebe3b8",
        "parentId" : "2749ced2-8c9f-41c7-bda0-3e32f914b61b",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Sorry, my diff collapsed some important lines, I didn't realize this WAS the clean up function. :)\n",
        "createdAt" : "2015-04-09T20:29:28Z",
        "updatedAt" : "2015-04-21T17:57:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "020ba6a6c7611e8418d6a7658ca4b1494c0a9ead",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +228,232 @@\t\trc, err := c.ReplicationControllers(ns).Get(RCName)\n\t\tif err == nil && rc.Spec.Replicas != 0 {\n\t\t\tBy(\"Cleaning up the replication controller\")\n\t\t\terr := DeleteRC(c, ns, RCName)\n\t\t\texpectNoError(err)"
  },
  {
    "id" : "6d6b855a-0a44-46ae-a135-c7639de0f196",
    "prId" : 6638,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7eadd731-5f3c-4722-bdd3-e71006742103",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I think it's a pretty harmless race, but you have the potential for concurrent reads and writes to events. Please send a follow-up PR that adds a lock.\n",
        "createdAt" : "2015-04-22T21:28:30Z",
        "updatedAt" : "2015-04-22T21:28:30Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "020ba6a6c7611e8418d6a7658ca4b1494c0a9ead",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +305,309 @@\t\t\tfor start := time.Now(); last < current && time.Since(start) < timeout; time.Sleep(10 * time.Second) {\n\t\t\t\tlast = current\n\t\t\t\tcurrent = len(events)\n\t\t\t}\n\t\t\tclose(stop)"
  },
  {
    "id" : "0f04a016-3bee-4e9e-a92e-fc2949ecad30",
    "prId" : 6459,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d37d90c8-5d29-4657-bc2f-05482ac4db8d",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I don't think this change is what we would like to do. The purpose of this test is to test how the performance looks in clusters with different sizes. So if we're testing a cluster with 2 nodes and a cluster with 100 nodes, the number of pods that we would like to start should be different (what I mean is that starting 30 pods on 100 nodes cluster doesn't expose Kubelets on heavy load at all).\n\nI will prepare a PR for it.\n\nBTW, I've just discovered that It() with function that takes parameters from \"outside\" world doesn't work (i.e. there are races - this works similarly to \"go func()\"). Will try to fix that too.\n",
        "createdAt" : "2015-04-07T09:05:45Z",
        "updatedAt" : "2015-04-07T09:06:57Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "699b37ff-f543-486c-93bc-cb52b30937e3",
        "parentId" : "d37d90c8-5d29-4657-bc2f-05482ac4db8d",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Regarding the race, this is the example of a log that shows it does not what we want:\n\nDensity \n  should allow starting 3 pods per node\n  /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/test/e2e/density.go:250\n[BeforeEach] Density\n  /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/test/e2e/density.go:197\n\n> > > testContext.KubeConfig: /usr/local/google/home/wojtekt/.kube/.kubeconfig\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > STEP: Making sure all 20 replicas exist\n> > > I0407 10:56:31.395932    1059 density.go:110] Controller my-short-lived-podfbc034b9-dd03-11e4-9e2b-a0481cabf39b: Found 0 pods out of 20\n",
        "createdAt" : "2015-04-07T09:12:00Z",
        "updatedAt" : "2015-04-07T09:12:00Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "88735f1d90156d41bc6620fa83c25add26e353ed",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +243,247 @@\t\tif dtest.podsPerMinion == 0 {\n\t\t\t//basic density tests\n\t\t\tname := fmt.Sprintf(\"should allow starting %d pods per node\", dtest.totalPods)\n\n\t\t\tif dtest.skip {"
  },
  {
    "id" : "6e3081b8-8a92-4b8b-b6fc-06cea5e536ab",
    "prId" : 6274,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f0837d1-f99d-4d3f-bd78-9230d8c8f579",
        "parentId" : null,
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Please add a TODO to increase it once we solve #6059\n",
        "createdAt" : "2015-04-01T07:20:31Z",
        "updatedAt" : "2015-04-01T07:23:17Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "89da2f9d-6b8d-4cd0-bff7-c99396b67f97",
        "parentId" : "3f0837d1-f99d-4d3f-bd78-9230d8c8f579",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Done.\n",
        "createdAt" : "2015-04-01T07:25:58Z",
        "updatedAt" : "2015-04-01T07:25:58Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5597efc0ce00a0433edc80262f1954902d6dccd",
    "line" : null,
    "diffHunk" : "@@ -1,1 +214,218 @@\t\tname := fmt.Sprintf(\"should allow starting %d pods per node\", count)\n\t\t// TODO(wojtek-t): Don't skip 30 pods per node test once #6059 if fixed.\n\t\tif count > 0 {\n\t\t\tname = \"[Skipped] \" + name\n\t\t}"
  },
  {
    "id" : "468a2a7b-8f13-4831-9b99-2a4f3fb57d1a",
    "prId" : 6051,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e28948af-86df-448e-a413-de72ee118e7c",
        "parentId" : null,
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Can you please add a comment that tests with \"Skipped\" in name will be skipped when running e2e without --ginkgo.focus flag? For me it's not obvious why you add this prefix.\n",
        "createdAt" : "2015-03-31T10:20:43Z",
        "updatedAt" : "2015-03-31T10:26:53Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "dde9220d-5ccc-451f-b446-151926f90bbd",
        "parentId" : "e28948af-86df-448e-a413-de72ee118e7c",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Done.\n",
        "createdAt" : "2015-03-31T10:27:08Z",
        "updatedAt" : "2015-03-31T10:27:08Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "467f400721cecb7dce117750af2dadc34bd84465",
    "line" : null,
    "diffHunk" : "@@ -1,1 +214,218 @@\t\tname := fmt.Sprintf(\"should allow starting %d pods per node\", count)\n\t\tif count > 30 {\n\t\t\tname = \"[Skipped] \" + name\n\t\t}\n\t\tIt(name, func() {"
  },
  {
    "id" : "34597b79-cfe2-4ab1-bbf7-1175081aca1f",
    "prId" : 5387,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99f2c3f4-14cc-4ac5-a816-8f6558e8d98c",
        "parentId" : null,
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "This could leak RCs/pods if there's an error communicating with the apiserver -- is that something that could break the other tests?\n",
        "createdAt" : "2015-03-12T16:39:22Z",
        "updatedAt" : "2015-03-12T16:39:22Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "9966eb66-f512-4132-93b2-8bd6d8de1316",
        "parentId" : "99f2c3f4-14cc-4ac5-a816-8f6558e8d98c",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "If there's a problem communicating with the apiserver then there would likely be an rc and its pods still in the system.  That could definitely impact other tests.  I'm not sure there's much we can do though.  If the apiserver is unresponsive I'm not sure how we clean up after the test.\n",
        "createdAt" : "2015-03-12T17:50:55Z",
        "updatedAt" : "2015-03-12T17:50:55Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "9d32ae84-21b9-4df4-8d39-82ed7f43280f",
        "parentId" : "99f2c3f4-14cc-4ac5-a816-8f6558e8d98c",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "Retries would be the typical answer, but I haven't checked how much we use retries in our e2e tests. If you give a request 3 tries to succeed instead of 1, flakes are less likely.\n",
        "createdAt" : "2015-03-12T17:54:24Z",
        "updatedAt" : "2015-03-12T17:54:24Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "46776510-b7cf-4f9a-a7a3-b22fc9fce572",
        "parentId" : "99f2c3f4-14cc-4ac5-a816-8f6558e8d98c",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "A retry might work but without a real world case to test against finding how many attempts and how long to wait between, or even if it will help, are all guesses.  I can add retries but I wouldn't be able to verify they'd actually solve a problem.  I'm also a little reluctant to cover up a communication problem.  The apiserver is going to have to be responsive under load.\n\nI don't recall there being a lot of retries for operations in the e2e tests.  That is also probably because not many would stress the system to the point of causing communication timeouts like this test suite could.\n\nATM this test is disabled and won't be run unless explicitly enabled because of the nature of the test.  This really belongs in a performance test suite.\n",
        "createdAt" : "2015-03-12T18:13:31Z",
        "updatedAt" : "2015-03-12T18:13:31Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c9a9a4fb035bdd8cc10bbd035ffcac0d46afa4c",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +193,197 @@\t\t// during the test so clean it up here\n\t\trc, err := c.ReplicationControllers(ns).Get(RCName)\n\t\tif err == nil && rc.Spec.Replicas != 0 {\n\t\t\tDeleteRC(c, ns, RCName)\n\t\t}"
  },
  {
    "id" : "ac4e61a4-3406-4d12-b5ef-65627d964ccf",
    "prId" : 4862,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "779c8e97-1faf-48d4-aa8d-2db8dde5ebf2",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "I would not fail on this condition.  We should be immune to transient errors here.\n",
        "createdAt" : "2015-02-26T22:27:51Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "a64f9fe0-b08f-43bd-b80e-a2aea670c559",
        "parentId" : "779c8e97-1faf-48d4-aa8d-2db8dde5ebf2",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Part of this test is not just that the system can handle x pods per node, but that it can do it without sacrificing other functionality.  We certainly wouldn't want the system to be running a large number of pods but be unresponsive to administrative functions like listing or removing pods.  If the system is running pods but it is too busy to respond to a simple query of the number of pods in the system then imo the system is too busy to be useful.\n",
        "createdAt" : "2015-03-02T18:21:07Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "798b3ee7fd52b44164774dd568fb3bf3c1063b00",
    "line" : null,
    "diffHunk" : "@@ -1,1 +121,125 @@\t\ttime.Sleep(5 * time.Second)\n\t\tpods, err = c.Pods(ns).List(label)\n\t\tExpect(err).NotTo(HaveOccurred())\n\t\tcurrent = len(pods.Items)\n\t}"
  },
  {
    "id" : "4152af16-62ce-4996-bdce-1f1cfa774eae",
    "prId" : 4862,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf71bc25-89cd-4af8-b0ab-13dbac114a2e",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "These are a lot of variables, perhaps use a struct?\n",
        "createdAt" : "2015-02-26T22:29:01Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "3e2069b9-5b54-4622-86a2-079101005005",
        "parentId" : "bf71bc25-89cd-4af8-b0ab-13dbac114a2e",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "I dislike the large number of variables as well, but I'm not sure a struct provides much improvement.  I still have to reset the counts.\n",
        "createdAt" : "2015-03-02T18:27:16Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "798b3ee7fd52b44164774dd568fb3bf3c1063b00",
    "line" : null,
    "diffHunk" : "@@ -1,1 +128,132 @@\n\tBy(\"Waiting for each pod to be running\")\n\tsame = 0\n\tlast = 0\n\tfailCount = 6"
  },
  {
    "id" : "74cae653-9511-4ac1-83df-3f300985899a",
    "prId" : 4862,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1e4fa3a-367e-4594-8c44-728ccc7ece6e",
        "parentId" : null,
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "So what's the behavior with Pending? It has to be focused on to work, or it's just always skipped or what? I haven't played with it yet.\n",
        "createdAt" : "2015-02-26T22:35:30Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "0ecff576-9c96-4b47-a294-3f25769ea448",
        "parentId" : "d1e4fa3a-367e-4594-8c44-728ccc7ece6e",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "I'm not sure I understand your question.  I added the pending and unknown counts to give clarity into what is happening with the scheduling.  I was running into issue where the number of running pods would drop while it was spinning up the pods.  The issue looked to be because a kubelet were too busy to respond to status queries, get marked unknown, and then the pods running on that node would move to unknown and get rescheduled.  Ideally I'd like to print out which nodes became unknown when a pod query was performed so the test can say which node when known and you have a system to look at.\n",
        "createdAt" : "2015-03-02T18:24:34Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "4d63a46e-b2b1-47a5-b4d6-fbc3fe58fd66",
        "parentId" : "d1e4fa3a-367e-4594-8c44-728ccc7ece6e",
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "I was asking about the PDescribe (which is a ginkgo feature of a Pending Describe), not the pending counts. :)\n",
        "createdAt" : "2015-03-06T18:46:54Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "a3b5d91a-a25d-404a-94ea-d01bc5133706",
        "parentId" : "d1e4fa3a-367e-4594-8c44-728ccc7ece6e",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "I don't think we want this test to run on every build, so I made the whole test suite pending.  If someone wants to run it then they would have to remove the P from Describe.  It's a manual process, but I didn't see another way to disable the test but still have it available to be run.  Ideally this kind of test should be in a load/stress suite rather than basic functionality.\n",
        "createdAt" : "2015-03-06T18:50:13Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "1d43c85a-51c8-4c0a-9f17-a8495e7a9456",
        "parentId" : "d1e4fa3a-367e-4594-8c44-728ccc7ece6e",
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "Yeah, that's all I was asking. I didn't know if it was a manual process of removing the 'P', or if Ginkgo was smart enough that it wouldn't run it on a regexp, but if you focused on it explicitly, it would.\n",
        "createdAt" : "2015-03-06T18:54:54Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "3b7753a3-91a9-4e80-9a2a-0f8745c3026e",
        "parentId" : "d1e4fa3a-367e-4594-8c44-728ccc7ece6e",
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "... some marker like \"if this test is the only test that matches, run it\" would be a reasonable feature request. (Obviously not in this PR.)\n",
        "createdAt" : "2015-03-06T18:55:39Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "dae6041a-e8ca-4772-b00a-0b51266ba87d",
        "parentId" : "d1e4fa3a-367e-4594-8c44-728ccc7ece6e",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "My reading of Focus is that it will run those tests to designate as focus, but I didn't find reverse functionality.  IE, run all tests except these unless explicitly asked for.\n",
        "createdAt" : "2015-03-06T18:58:14Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "798b3ee7fd52b44164774dd568fb3bf3c1063b00",
    "line" : null,
    "diffHunk" : "@@ -1,1 +170,174 @@// front of PDescribe (PDescribe->Describe) and then all tests will\n// be available\nvar _ = PDescribe(\"Density\", func() {\n\tvar c *client.Client\n\tvar minionCount int"
  },
  {
    "id" : "4647b5f5-64ed-4081-b7a9-55e341259053",
    "prId" : 4862,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdcb52d4-aa8f-434d-8d1d-7d8f1ac7e5c7",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Our goal is like 30-50 pods per node for v1, not 100.\n",
        "createdAt" : "2015-03-06T18:38:52Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "a147354a-9a14-464b-bc3a-47a6c4e526a4",
        "parentId" : "fdcb52d4-aa8f-434d-8d1d-7d8f1ac7e5c7",
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "I think it's reasonable to just beyond the envelope and see what breaks, no?\n",
        "createdAt" : "2015-03-06T18:47:17Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "b3846458-573c-4278-95be-7229b89e6a5f",
        "parentId" : "fdcb52d4-aa8f-434d-8d1d-7d8f1ac7e5c7",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Ideally this suite would be more than 1 test with the only difference being the number of pods/node.  The 100 pods/node level has exposed some issues, but certainly nothing wrong with also having a 30, 50, etc test.  I could add tests for 30 and 50 as well.\n",
        "createdAt" : "2015-03-06T18:52:29Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "2119c364-a8e1-4d8d-8d8e-5d9e789cfdd7",
        "parentId" : "fdcb52d4-aa8f-434d-8d1d-7d8f1ac7e5c7",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Some things I'd like to see, which don't have to be in the first version of this:\n- Multiple RCs at the same time\n- a run with kubernetes/pause as the image (to stress master components), hopefully with more pods than we start of nginx\n",
        "createdAt" : "2015-03-06T21:27:02Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "798b3ee7fd52b44164774dd568fb3bf3c1063b00",
    "line" : null,
    "diffHunk" : "@@ -1,1 +192,196 @@\t})\n\n\tIt(\"should allow starting 100 pods per node\", func() {\n\t\tRCName = \"my-hostname-density100-\" + string(util.NewUUID())\n\t\tRunRC(c, RCName, ns, \"dockerfile/nginx\", 100*minionCount)"
  },
  {
    "id" : "bb57ba54-a281-4f96-a451-34cc23a6ab9c",
    "prId" : 4862,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21c567ef-09df-4880-bd6a-2a4dd729a155",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Proper client behavior is to wait (poll or watch) until rc.Status.Replicas also equals zero. Otherwise, you'll leak pods if controller manager is temporarily down.\n",
        "createdAt" : "2015-03-06T20:23:02Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "458f4cbb-fcc1-4dc3-8311-6f7e0a4d45e5",
        "parentId" : "21c567ef-09df-4880-bd6a-2a4dd729a155",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "I noticed that kubectl.Stop does that.  imo that type of functionality should be inside the Delete rather then relying on clients to know the wait needs to be done.  I don't have a problem re-implementing Stop, but there should be a clean way to do this w/o needing to know that I have to wait for rc.Status.Replicas to equal 0.\n",
        "createdAt" : "2015-03-06T20:37:49Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "8c8d8a5c-fc61-4d6a-96d0-6a0c4f8e9afd",
        "parentId" : "21c567ef-09df-4880-bd6a-2a4dd729a155",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yeah, I don't know how hard it would be to call kubectl code from here, but calling kubectl.Stop would be ideal.\n",
        "createdAt" : "2015-03-06T20:40:41Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "95dc36da-13e2-4e62-a966-32a54ba8652a",
        "parentId" : "21c567ef-09df-4880-bd6a-2a4dd729a155",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Does it make sense to instead to refactor kubectl.Stop and rc.Delete so that the wait is inside rc.Delete rather than in kubectl.Stop?\n",
        "createdAt" : "2015-03-06T20:45:49Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "1531bdcf-73c8-4d5e-ad18-d7b2cc9400b3",
        "parentId" : "21c567ef-09df-4880-bd6a-2a4dd729a155",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Not necessarily; you might want to delete an rc because you've made another one to manage those pods, not because you're stopping it.\n",
        "createdAt" : "2015-03-06T21:23:58Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "7e1480e4-6930-4d54-8e58-6ccb1a1db57c",
        "parentId" : "21c567ef-09df-4880-bd6a-2a4dd729a155",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "Ya, I realized this morning that moving the wait into the Delete isn't the right path.  I still think the wait needs to be enshrined in an api call.  Maybe move kubectl.stop to rc.stop?\n",
        "createdAt" : "2015-03-09T12:21:40Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "44723698-552c-423d-9120-cdebd91d159c",
        "parentId" : "21c567ef-09df-4880-bd6a-2a4dd729a155",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yeah, that's reasonable; eventually, we're going to move it all the way into the server. But I'm not sure if we should block this PR on that-- a lot of people are blocked on needing a load generator right now. :)\n",
        "createdAt" : "2015-03-09T17:30:47Z",
        "updatedAt" : "2015-03-10T17:41:41Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "798b3ee7fd52b44164774dd568fb3bf3c1063b00",
    "line" : null,
    "diffHunk" : "@@ -1,1 +42,46 @@\tif _, err := c.ReplicationControllers(ns).Update(rc); err != nil {\n\t\treturn fmt.Errorf(\"Failed to resize replication controller %s to zero: %v\", name, err)\n\t}\n\n\tif err := wait.Poll(time.Second, time.Minute*20, client.ControllerHasDesiredReplicas(c, rc)); err != nil {"
  }
]