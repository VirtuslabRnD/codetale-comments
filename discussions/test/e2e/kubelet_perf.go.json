[
  {
    "id" : "82f7e24d-e994-4fe0-a76a-f5c3199c40bf",
    "prId" : 24003,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93cbd3ce-a7e1-493e-8c76-dfcba6770569",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Since RSS is not included in today's metrics API, should we validate workingset here as a temporary solution?  \n",
        "createdAt" : "2016-04-07T23:52:53Z",
        "updatedAt" : "2016-04-12T16:27:53Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "bedb6694-9aca-439c-bc63-d48f96f55289",
        "parentId" : "93cbd3ce-a7e1-493e-8c76-dfcba6770569",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Isn't workingset strictly larger than RSS? I'm worried that will make the kueblet_perf tests flaky without a corresponding bump to stats there.\n",
        "createdAt" : "2016-04-08T00:39:06Z",
        "updatedAt" : "2016-04-12T16:27:53Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "ac995c80-17c5-48c4-8aa1-66897d2e246b",
        "parentId" : "93cbd3ce-a7e1-493e-8c76-dfcba6770569",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Yes, my initial thought is making the threshold much larger here, at least we tested the API and checked stats for both Kubelet and Docker until we introduced RSS. Anyway, @roberthbailey, the current release czar :-), is ok with patching both #24003 and #24015 to release 1.2, I am fine with this. \n",
        "createdAt" : "2016-04-08T17:40:49Z",
        "updatedAt" : "2016-04-12T16:27:53Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8c685921f908894be46e8934b37fd70c11d2e4e",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +218,222 @@\t\t\t\t// of the addon pods affect the memory usage on each node.\n\t\t\t\tmemLimits: resourceUsagePerContainer{\n\t\t\t\t\tstats.SystemContainerKubelet: &containerResourceUsage{MemoryRSSInBytes: 70 * 1024 * 1024},\n\t\t\t\t\tstats.SystemContainerRuntime: &containerResourceUsage{MemoryRSSInBytes: 85 * 1024 * 1024},\n\t\t\t\t},"
  },
  {
    "id" : "49aef55c-6333-44fb-9bbb-0f681f27552b",
    "prId" : 21785,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7298c4a-3ed0-4746-82d9-0f6ff69d1da8",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Should we update the test to run `100` pods? Since the limit is lesser than that as of now, we can do it in a separate PR.\n",
        "createdAt" : "2016-02-24T23:20:27Z",
        "updatedAt" : "2016-02-24T23:20:27Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "5079f3f1-77a1-43fe-a255-b95666fc7af5",
        "parentId" : "e7298c4a-3ed0-4746-82d9-0f6ff69d1da8",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Yes, I plan to do it in another PR, after --max-pods is bumped.\n",
        "createdAt" : "2016-02-25T00:01:21Z",
        "updatedAt" : "2016-02-25T00:01:21Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdece9922b36248bdd216b4c595b2732212b3247",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +219,223 @@\t\t\t// TODO(yujuhong): change this test to ~100 pods per node after\n\t\t\t// --max-pods have been changed.\n\t\t\t{podsPerNode: 35,\n\t\t\t\tcpuLimits: containersCPUSummary{\n\t\t\t\t\t\"/kubelet\":       {0.50: 0.12, 0.95: 0.14},"
  },
  {
    "id" : "fd13af90-62d6-4e8f-b224-13f04529ca97",
    "prId" : 21020,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bf90c2f-24e3-4743-9e70-5231f9475baa",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "RSS of 500MB is pretty large. Do you plan on updating this soon? If not, we might not get much value out of this test.\nNot for this PR: How about collecting these metrics for a week and then coming with a number? \n",
        "createdAt" : "2016-02-12T21:06:00Z",
        "updatedAt" : "2016-02-12T21:19:54Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "d2574291-8aa3-4cd9-9011-491e6289588c",
        "parentId" : "6bf90c2f-24e3-4743-9e70-5231f9475baa",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "See TODO two lines above\n",
        "createdAt" : "2016-02-12T21:08:57Z",
        "updatedAt" : "2016-02-12T21:19:54Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "fe29f338-9618-49b1-a6d7-a36bbd8084bb",
        "parentId" : "6bf90c2f-24e3-4743-9e70-5231f9475baa",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Just checking if you plan on resolving that TODO anytime soon ;)\n\nOn Fri, Feb 12, 2016 at 1:09 PM, Yu-Ju Hong notifications@github.com\nwrote:\n\n> In test/e2e/kubelet_perf.go\n> https://github.com/kubernetes/kubernetes/pull/21020#discussion_r52798566\n> :\n> \n> > @@ -95,15 +95,53 @@ func runResourceTrackingTest(framework *Framework, podsPerNode int, nodeNames se\n> >     By(\"Reporting overall resource usage\")\n> >     logPodsOnNodes(framework.Client, nodeNames.List())\n> > \n> > ##     rm.LogLatest()\n> > -   summary := rm.GetCPUSummary()\n> > -   Logf(\"%s\", rm.FormatCPUSummary(summary))\n> > -   verifyCPULimits(expected, summary)\n> > -   usageSummary, err := rm.GetLatest()\n> > -   Expect(err).NotTo(HaveOccurred())\n> > -   Logf(\"%s\", rm.FormatResourceUsage(usageSummary))\n> > -   // TODO(yujuhong): Set realistic values after gathering enough data.\n> > -   verifyMemoryLimits(resourceUsagePerContainer{\n> > -       \"/kubelet\":       &containerResourceUsage{MemoryRSSInBytes: 500 \\* 1024 \\* 1024},\n> \n> See TODO two lines above\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/21020/files#r52798566.\n",
        "createdAt" : "2016-02-12T21:10:47Z",
        "updatedAt" : "2016-02-12T21:19:54Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "4939bdfc-34f4-4c69-8197-148c88112a44",
        "parentId" : "6bf90c2f-24e3-4743-9e70-5231f9475baa",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Even getting numbers is better than nothing, but yes, I plan to resolve that in a week or so :-)\n",
        "createdAt" : "2016-02-12T21:25:54Z",
        "updatedAt" : "2016-02-12T21:25:54Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "082da18e8a96f1df73cc9b951bae9370dd331bb6",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +101,105 @@\t// TODO(yujuhong): Set realistic values after gathering enough data.\n\tverifyMemoryLimits(resourceUsagePerContainer{\n\t\t\"/kubelet\":       &containerResourceUsage{MemoryRSSInBytes: 500 * 1024 * 1024},\n\t\t\"/docker-daemon\": &containerResourceUsage{MemoryRSSInBytes: 500 * 1024 * 1024},\n\t}, usageSummary)"
  },
  {
    "id" : "ca6b1928-919f-4917-b378-5ff04ccb5e77",
    "prId" : 19667,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3665fd28-6621-47f6-9a1f-a61dabd17131",
        "parentId" : null,
        "authorId" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "body" : "since this test is marked `[Slow]`, I think it's already skipped on gke-ci. Do we need to label it as a feature, too?\n",
        "createdAt" : "2016-01-20T01:54:16Z",
        "updatedAt" : "2016-01-28T00:10:39Z",
        "lastEditedBy" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "tags" : [
        ]
      },
      {
        "id" : "00f2af6a-beb3-44c6-93bb-502d25e0612d",
        "parentId" : "3665fd28-6621-47f6-9a1f-a61dabd17131",
        "authorId" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "body" : "Ah, I guess this is still an issue for the GKE soak cluster.\n\nWhy don't we just skip running this on GKE then, rather than labeling this feature?\n",
        "createdAt" : "2016-01-20T01:58:20Z",
        "updatedAt" : "2016-01-28T00:10:39Z",
        "lastEditedBy" : "719d0e19-fcef-4b47-afac-404318b9514f",
        "tags" : [
        ]
      },
      {
        "id" : "735663b9-0bd8-44a4-96cf-110ce7cae64b",
        "parentId" : "3665fd28-6621-47f6-9a1f-a61dabd17131",
        "authorId" : "d513ff43-94d3-4f43-8358-1fb8132b6aae",
        "body" : "> since this test is marked [Slow], I think it's already skipped on gke-ci. Do we need to label it as a feature, too?\n\nLogically the reason for it to be running/not running somewhere is that it's a special feature.  The fact that it's not running on GKE is side-effect.\n",
        "createdAt" : "2016-01-20T22:52:09Z",
        "updatedAt" : "2016-01-28T00:10:39Z",
        "lastEditedBy" : "d513ff43-94d3-4f43-8358-1fb8132b6aae",
        "tags" : [
        ]
      }
    ],
    "commit" : "a51f291b914cb76a6003bad27aa30193e49bf804",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +185,189 @@\t\t}\n\t})\n\tDescribe(\"experimental resource usage tracking [Feature:ExperimentalResourceUsageTracking]\", func() {\n\t\tdensity := []int{100}\n\t\tfor i := range density {"
  },
  {
    "id" : "163773ac-d4c9-477f-b34d-5c3444e6c1a9",
    "prId" : 18382,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76429097-5fef-4af4-add1-1daf9461cfa3",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Why we change this interval? \n",
        "createdAt" : "2015-12-11T21:06:47Z",
        "updatedAt" : "2015-12-11T21:06:47Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "90ace180-0f8f-4ee8-b24b-d06acfd7ca1e",
        "parentId" : "76429097-5fef-4af4-add1-1daf9461cfa3",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Because this interval has not actually being picked up by the underlying resource collector. We've been mistakenly using the 1s interval. I've decided to bump it to 3s to not generate too much traffic, while still capture the occasional spikes. In fact, we'll have to change it again once we bump the cadvisor housekeeping interval. \n",
        "createdAt" : "2015-12-11T23:16:17Z",
        "updatedAt" : "2015-12-11T23:16:17Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e8f4d831de1d499d6c4ae87ed5e9e7c8e67ccf0",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +33,37 @@const (\n\t// Interval to poll /stats/container on a node\n\tcontainerStatsPollingPeriod = 3 * time.Second\n\t// The monitoring time for one test.\n\tmonitoringTime = 20 * time.Minute"
  }
]