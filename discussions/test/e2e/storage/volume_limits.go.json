[
  {
    "id" : "e79a692a-df65-4e6a-a09d-7fa4de13b7fb",
    "prId" : 69225,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69225#pullrequestreview-160481009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa951cf4-d5c8-4e77-9c54-8822ffc43414",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "can we also launch a pod that uses the entire volume limit?  You could potentially reuse this test case and just set a different numVolumes: https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/test/e2e/storage/persistent_volumes.go#L319",
        "createdAt" : "2018-10-01T18:45:15Z",
        "updatedAt" : "2018-10-02T17:59:15Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "0a7d755b-5ec6-416f-bb6c-b0c240b76fef",
        "parentId" : "fa951cf4-d5c8-4e77-9c54-8822ffc43414",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "We could but for gce - it appears that the limit on the selected node starts at 64. So, we will have to create a pod with 64+ volumes ",
        "createdAt" : "2018-10-01T18:55:15Z",
        "updatedAt" : "2018-10-02T17:59:15Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "8fe3856e-c504-4c2b-8ab8-82f54d0febc5",
        "parentId" : "fa951cf4-d5c8-4e77-9c54-8822ffc43414",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "64 1gi volumes should be within our quota...",
        "createdAt" : "2018-10-01T19:10:17Z",
        "updatedAt" : "2018-10-02T17:59:15Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "a51de823-308d-44f8-8d32-285039723dcd",
        "parentId" : "fa951cf4-d5c8-4e77-9c54-8822ffc43414",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "okay, I think that will also make this test serial because no other workload(guess just with volumes but we don't have that level of granularity) can be scheduled on the node while this test runs on it.  I can do that as a follow up PR while fixing https://github.com/kubernetes/kubernetes/issues/68912 ",
        "createdAt" : "2018-10-01T20:54:05Z",
        "updatedAt" : "2018-10-02T17:59:15Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "e38cfb61be5ebed71407149aae468b769a3ade4e",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +47,51 @@\t\t\t\tframework.Failf(\"Expected volume limits to be set\")\n\t\t\t}\n\t\t}\n\n\t})"
  }
]