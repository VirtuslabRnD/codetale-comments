[
  {
    "id" : "5c635e4f-0a6f-4fc5-a820-629c15c3183e",
    "prId" : 102682,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102682#pullrequestreview-690161784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85f81da1-f8f5-4b14-b0e6-8537c0c40e3b",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Do you want the test to fail, or be skipped?",
        "createdAt" : "2021-06-22T17:23:19Z",
        "updatedAt" : "2021-06-22T17:23:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "56e69602-9c09-4ee9-9052-7a421fc6bfd4",
        "parentId" : "85f81da1-f8f5-4b14-b0e6-8537c0c40e3b",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "Should fail.\r\nI should explained this more clearly, let me clarify why this change is needed.\r\n\r\nOriginally, the testcase will fail with the local provider, this does not make sense since this testcase should be skipped instead, code snippet right above this line have that logic, \r\n```\r\nframework.ProviderIs(test.CloudProviders...)   // local provider is not belonged to `test.CloudProviders` which should be the provider like `gce` as an example.\r\n```\r\n\r\nBut the testcase failed instead of skipped because the `getRandomClusterZone` is called before the checking of the provider. \r\n\r\n`getRandomClusterZone` check the `zones` which generate a map based on the below node labels,\r\n```\r\nfailure-domain.beta.kubernetes.io/zone\r\ntopology.kubernetes.io/zone\r\n```\r\n\r\nlocal provider doesn't have necessary have the labels set on the host, so the testcase failed.\r\n\r\nWhat I did was extracting the line of the code for zones checking out and defer this check to the end of the provider validation.\r\n\r\nThe motivation behind this is we are trying to enable a CI system locally and run all those storage testcases with local provider as the first step, so we need to cleanup all failures of testcases which should not fail at all.\r\n\r\n\r\n",
        "createdAt" : "2021-06-23T02:34:23Z",
        "updatedAt" : "2021-06-23T02:34:23Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "5e30dbba-de08-4958-9a53-e4b856bd9ebf",
        "parentId" : "85f81da1-f8f5-4b14-b0e6-8537c0c40e3b",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "when the testcase hit this line means the provider is not `local` and zone label is not set, so the testcase should fail, this is an expected behavior.\r\n\r\nand if the provider is `local`, the testcase won't hit this line at all, it should be skipped due to the provider check.",
        "createdAt" : "2021-06-23T02:38:50Z",
        "updatedAt" : "2021-06-23T02:38:50Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d0936b74362b54dece6a660caa187b1a8ce4438",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +353,357 @@\n\t\t\t\tif zone, ok := test.Parameters[\"zone\"]; ok {\n\t\t\t\t\tframework.ExpectNotEqual(len(zone), 0, \"expect at least one zone\")\n\t\t\t\t}\n"
  },
  {
    "id" : "7cc9e0a7-9e69-4775-b13c-8abf6e7fe040",
    "prId" : 99888,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99888#pullrequestreview-610119349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3269a8d-4b7d-4e42-baf7-2027aee875d1",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "if this logic is useful for other tests too, maybe put it in a util function",
        "createdAt" : "2021-03-08T18:30:38Z",
        "updatedAt" : "2021-03-11T07:26:18Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "e5ad6658-adb9-4895-9887-259af41f396d",
        "parentId" : "a3269a8d-4b7d-4e42-baf7-2027aee875d1",
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "I created a function in e2epv called `GetDefaultFSType`",
        "createdAt" : "2021-03-09T00:35:36Z",
        "updatedAt" : "2021-03-11T07:26:18Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      },
      {
        "id" : "731b29a8-da04-4b34-81c1-d6c60e50edea",
        "parentId" : "a3269a8d-4b7d-4e42-baf7-2027aee875d1",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "this logic only applied to windows? Seems like Linux should also use the same logic here?",
        "createdAt" : "2021-03-11T06:22:05Z",
        "updatedAt" : "2021-03-11T07:26:18Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "e099c4f8-c478-4bda-9565-e658cb76d3db",
        "parentId" : "a3269a8d-4b7d-4e42-baf7-2027aee875d1",
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "Correct, I removed the if statement but there was an issue with this test among others:\r\n\r\n```\r\n[sig-storage] Dynamic Provisioning GlusterDynamicProvisioner should create and delete persistent volumes [fast]\r\n\r\nWarning  ProvisioningFailed  0s (x5 over 47s)  persistentvolume-controller  Failed to provision volume with StorageClass \"volume-provisioning-7307-glusterdptestpjc72\": invalid option \"fstype\" for volume plugin kubernetes.io/glusterfs\r\n```\r\n\r\nI was checking the parameters allowed in https://kubernetes.io/docs/concepts/storage/storage-classes/ and some of these StorageClasses don't accept `fsType` (one of them is glusterfs among others), before, the storage class fsType wasn't overriden from the default value so I think that we should preserve that, what if we add only the exception for windows?",
        "createdAt" : "2021-03-11T18:55:14Z",
        "updatedAt" : "2021-03-11T18:55:15Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d9053014e0e8edf655d7bac774fa4eb9fb83950",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +952,956 @@\t}\n\n\tif framework.NodeOSDistroIs(\"windows\") {\n\t\t// fstype might be forced from outside, in that case skip setting a default\n\t\tif _, exists := t.Parameters[\"fstype\"]; !exists {"
  },
  {
    "id" : "36a11d80-7a4b-4d90-81cd-b53c1ea51811",
    "prId" : 76625,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76625#pullrequestreview-228864811",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afd00593-9b11-495f-8c14-e04e71be41e2",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "same thing, skip and remove provider check?",
        "createdAt" : "2019-04-18T16:41:42Z",
        "updatedAt" : "2019-04-22T18:51:26Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "37de169a-fea9-4d5b-86e3-2c84f2e4299d",
        "parentId" : "afd00593-9b11-495f-8c14-e04e71be41e2",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Do we know that whatever default SC that will exist will support dynamic provisioning?",
        "createdAt" : "2019-04-19T14:25:41Z",
        "updatedAt" : "2019-04-22T18:51:26Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "7b9289ce-1310-461c-ad59-39be1365cf15",
        "parentId" : "afd00593-9b11-495f-8c14-e04e71be41e2",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "it seems odd to define a default storageclass that cannot support dynamic provisioning. There is movement in the conformance area to also add some \"validation\" suites using the default storage class",
        "createdAt" : "2019-04-19T20:10:20Z",
        "updatedAt" : "2019-04-22T18:51:26Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "d5ba98bb-3657-4fa7-b762-450500602597",
        "parentId" : "afd00593-9b11-495f-8c14-e04e71be41e2",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Given current defaults it isn't out of question to have a SC that does not support dynamic provisioning. For example - a bare metal cluster that uses only local PVCs.  The cloudprovider tests explicitly allow us to skip these tests but a default SC won't. \r\n\r\n",
        "createdAt" : "2019-04-20T03:51:53Z",
        "updatedAt" : "2019-04-22T18:51:26Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "6fee8a77279dcf11d559c3eaadfa7b3805c77d1a",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +812,816 @@\t\t\tscName, scErr := framework.GetDefaultStorageClassName(c)\n\t\t\tif scErr != nil {\n\t\t\t\tframework.Failf(scErr.Error())\n\t\t\t}\n\t\t\ttest := testsuites.StorageClassTest{"
  },
  {
    "id" : "9a44d991-6461-4d23-91d8-8361b1e941ec",
    "prId" : 70941,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70941#pullrequestreview-177015595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4abf311a-3353-4de8-a509-51b22230452e",
        "parentId" : null,
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "what was wrong with `PopAny()`?",
        "createdAt" : "2018-11-17T00:46:58Z",
        "updatedAt" : "2019-01-12T02:36:54Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "c9648751-b883-41e3-b7aa-a9acd6720683",
        "parentId" : "4abf311a-3353-4de8-a509-51b22230452e",
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "`PopAny()` is actually deterministic, so it's possible some cases are never tested",
        "createdAt" : "2018-11-20T23:07:13Z",
        "updatedAt" : "2019-01-12T02:36:54Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      }
    ],
    "commit" : "12ece9dacbbe8b3f9727c5136912de9089b54b27",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +1173,1177 @@\n\tzonesList := zones.UnsortedList()\n\treturn zonesList[rand.Intn(zones.Len())]\n}"
  },
  {
    "id" : "cc3122fd-f2ed-409c-bb63-593d5ee73ec9",
    "prId" : 70362,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70362#pullrequestreview-170465489",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2232efb5-108b-4e91-b045-b6d040624a2f",
        "parentId" : null,
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "`Expect(len(pvs)).ToNot(Equal(0))`",
        "createdAt" : "2018-10-31T20:54:40Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      }
    ],
    "commit" : "923df464fb431fe9d37270bde031258daf986cca",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +61,65 @@func testBindingWaitForFirstConsumer(client clientset.Interface, claim *v1.PersistentVolumeClaim, class *storage.StorageClass) (*v1.PersistentVolume, *v1.Node) {\n\tpvs, node := testBindingWaitForFirstConsumerMultiPVC(client, []*v1.PersistentVolumeClaim{claim}, class)\n\treturn pvs[0], node\n}\n"
  },
  {
    "id" : "9f5be338-2ccc-4432-99b2-4a66aa3198da",
    "prId" : 70362,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70362#pullrequestreview-171007101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be570a20-bdc2-4866-8b41-f37a6870a6b4",
        "parentId" : null,
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "nit: combine the two slices into a map",
        "createdAt" : "2018-10-31T20:56:13Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      },
      {
        "id" : "479b955e-25e4-442f-aa45-ea32f7f5008d",
        "parentId" : "be570a20-bdc2-4866-8b41-f37a6870a6b4",
        "authorId" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "body" : "The functions used below like `WaitForPersistentVolumeClaimsPhase`, `CreatePod` need access to the slices `createdClaims` and `claimNames`. So putting them into a map will require extracting the keys and values as slices multiple times down below. To avoid doing that and given we don't need to look up claim through names, I am keeping it as two separate slices that can be directly passed to the functions used below.",
        "createdAt" : "2018-11-02T08:33:22Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "tags" : [
        ]
      }
    ],
    "commit" : "923df464fb431fe9d37270bde031258daf986cca",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +75,79 @@\n\tBy(\"creating claims\")\n\tvar claimNames []string\n\tvar createdClaims []*v1.PersistentVolumeClaim\n\tfor _, claim := range claims {"
  },
  {
    "id" : "510c1877-10ff-4b2e-b97a-7ec15dfeb302",
    "prId" : 70362,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70362#pullrequestreview-172821072",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e789a0a-9280-4ec0-b3fe-eb7496a7a896",
        "parentId" : null,
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "Was this intentionally moved below `By(\"checking the claims are in pending state\")`?",
        "createdAt" : "2018-10-31T21:02:01Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      },
      {
        "id" : "c5a4a15d-6526-40b2-abbf-f214a832ab29",
        "parentId" : "2e789a0a-9280-4ec0-b3fe-eb7496a7a896",
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "The `By()` used to be below `framework.WaitForPersistentVolumeClaimsPhase(v1.ClaimBound, client, namespace, claimNames, 2*time.Second, framework.ClaimProvisionShortTimeout, true)`",
        "createdAt" : "2018-11-05T22:25:48Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      },
      {
        "id" : "cd28e257-5a5b-442b-8c0b-f63e30f35a1e",
        "parentId" : "2e789a0a-9280-4ec0-b3fe-eb7496a7a896",
        "authorId" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "body" : "Ah yes - that was intentional since I did not want to print out identical messages through the `By()` for each PVC.",
        "createdAt" : "2018-11-08T06:40:59Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "tags" : [
        ]
      }
    ],
    "commit" : "923df464fb431fe9d37270bde031258daf986cca",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +98,102 @@\t}()\n\n\t// Wait for ClaimProvisionTimeout (across all PVCs in parallel) and make sure the phase did not become Bound i.e. the Wait errors out\n\tBy(\"checking the claims are in pending state\")\n\terr = framework.WaitForPersistentVolumeClaimsPhase(v1.ClaimBound, client, namespace, claimNames, 2*time.Second, framework.ClaimProvisionShortTimeout, true)"
  },
  {
    "id" : "0ef0c5fe-d62c-408d-8679-da2160b01d72",
    "prId" : 70362,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70362#pullrequestreview-170989867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f8c09da-77bf-45d3-86cd-3791b106d23c",
        "parentId" : null,
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "could this be simplified to `framework.WaitForPersistentVolumeClaimsPhase(v1.ClaimPending)` and a very short timeout? If there's another function that just checks the instantaneous phase that'd be even better",
        "createdAt" : "2018-10-31T21:15:54Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      },
      {
        "id" : "9838fec1-8192-4f62-a65b-236fcf432046",
        "parentId" : "0f8c09da-77bf-45d3-86cd-3791b106d23c",
        "authorId" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "body" : "Waiting and checking that the claim did not bind for a while is necessary to verify that the WaitForFirstConsumer is indeed taking effect. If we wait for an extremely short period (through something like `framework.WaitForPersistentVolumeClaimsPhase(v1.ClaimPending)`), the verification that `WaitForFirstConsumer` is indeed taking effect will not be effective as a PVC with Immediate binding mode also stay Pending for a little while (as the PV is created, attached, mounted etc.) before switching to bound.",
        "createdAt" : "2018-11-02T07:14:07Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "tags" : [
        ]
      }
    ],
    "commit" : "923df464fb431fe9d37270bde031258daf986cca",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +102,106 @@\terr = framework.WaitForPersistentVolumeClaimsPhase(v1.ClaimBound, client, namespace, claimNames, 2*time.Second, framework.ClaimProvisionShortTimeout, true)\n\tExpect(err).To(HaveOccurred())\n\tfor _, claim := range createdClaims {\n\t\t// Get new copy of the claim\n\t\tclaim, err = client.CoreV1().PersistentVolumeClaims(claim.Namespace).Get(claim.Name, metav1.GetOptions{})"
  },
  {
    "id" : "4f111409-57ea-4f48-91f6-1d677bc274c6",
    "prId" : 70362,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70362#pullrequestreview-170992824",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "311f96d4-b765-498b-9977-abdff5188945",
        "parentId" : null,
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "If the label and node affinity check is left here, update the `By()` message",
        "createdAt" : "2018-10-31T21:24:27Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      },
      {
        "id" : "44fbfa06-e6a4-4dd3-ad7c-03cf00de79c1",
        "parentId" : "311f96d4-b765-498b-9977-abdff5188945",
        "authorId" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "body" : "Not necessary as the node affinity/label checks have been moved out. ",
        "createdAt" : "2018-11-02T07:30:12Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "tags" : [
        ]
      }
    ],
    "commit" : "923df464fb431fe9d37270bde031258daf986cca",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +120,124 @@\tExpect(err).NotTo(HaveOccurred())\n\n\tBy(\"re-checking the claims to see they binded\")\n\tvar pvs []*v1.PersistentVolume\n\tfor _, claim := range createdClaims {"
  },
  {
    "id" : "61c599ac-55a9-4320-96c9-d6f59f916ca4",
    "prId" : 70362,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70362#pullrequestreview-171793912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b4422de-329c-4029-bd96-ac9709566ad1",
        "parentId" : null,
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "`testZonalDelayedBinding()` and `testZonalDelayedBindingWithAllowedTopology()` share a lot of code, and their `StorageClassTest`s are the same too. I was thinking we could combine these two functions",
        "createdAt" : "2018-11-05T22:36:55Z",
        "updatedAt" : "2018-11-09T20:18:23Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      }
    ],
    "commit" : "923df464fb431fe9d37270bde031258daf986cca",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +268,272 @@\t\tstorageClassTestNameSuffix += \" with AllowedTopologies\"\n\t}\n\ttests := []testsuites.StorageClassTest{\n\t\t{\n\t\t\tName:           fmt.Sprintf(storageClassTestNameFmt, \"EBS\", storageClassTestNameSuffix),"
  },
  {
    "id" : "2b7e158b-eafd-4c29-996d-39734d664e86",
    "prId" : 68138,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/68138#pullrequestreview-151522483",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1b46233-5d67-49bb-8d3c-9284d2d55e67",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "We recently added more nfs provisioner tests in test/e2e/storage/drivers/in_tree.go.  Could you update the specs there too?",
        "createdAt" : "2018-08-31T19:24:46Z",
        "updatedAt" : "2018-08-31T20:26:21Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d877436bf67c030a739a070982aac4595a9e020",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +788,792 @@\t\t\t// external dynamic provisioner pods need additional permissions provided by the\n\t\t\t// persistent-volume-provisioner clusterrole and a leader-locking role\n\t\t\tserviceAccountName := \"default\"\n\t\t\tsubject := rbacv1beta1.Subject{\n\t\t\t\tKind:      rbacv1beta1.ServiceAccountKind,"
  },
  {
    "id" : "e5a02ef2-f87c-49ef-ac44-1031fd7611a5",
    "prId" : 67102,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67102#pullrequestreview-144619169",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ef5a5e8-5efe-4156-a638-15c097831058",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "should we validate that the pod came up running?",
        "createdAt" : "2018-08-08T19:35:21Z",
        "updatedAt" : "2018-08-10T23:34:09Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "21307db7-6d03-483a-988e-5aa9006783a7",
        "parentId" : "6ef5a5e8-5efe-4156-a638-15c097831058",
        "authorId" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "body" : "`CreateClientPod` -> `CreatePod` -> `WaitForPodNameRunningInNamespace` waits and polls for pod to get to a `PodRunning` state. So I think that should be good?",
        "createdAt" : "2018-08-08T21:37:34Z",
        "updatedAt" : "2018-08-10T23:34:09Z",
        "lastEditedBy" : "98986d5f-c846-4478-8363-8a016e3d6b89",
        "tags" : [
        ]
      }
    ],
    "commit" : "0417b21d509022591b7b96181bed211f3304cd03",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +205,209 @@\t\tframework.DeletePodOrFail(client, pod.Namespace, pod.Name)\n\t}()\n\n\tBy(\"re-checking the claim to see it binded\")\n\t// Get new copy of the claim"
  },
  {
    "id" : "2136d9a7-e1fd-453e-be5e-54a384d94a59",
    "prId" : 59879,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59879#pullrequestreview-113102088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75a1a31e-03d4-4a83-b4ae-495b0fbaa659",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "is it really fast?",
        "createdAt" : "2018-02-16T14:28:30Z",
        "updatedAt" : "2018-04-27T09:52:48Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "07409f53-a5cf-49c9-b3b2-7b835bdca51d",
        "parentId" : "75a1a31e-03d4-4a83-b4ae-495b0fbaa659",
        "authorId" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "body" : "Not fast if there is potential for 20m wait.",
        "createdAt" : "2018-02-16T19:13:47Z",
        "updatedAt" : "2018-04-27T09:52:48Z",
        "lastEditedBy" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "tags" : [
        ]
      },
      {
        "id" : "94c10d25-433e-4ee1-8133-6b6b723b16e6",
        "parentId" : "75a1a31e-03d4-4a83-b4ae-495b0fbaa659",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "ok. I will mark it as `slow` then.",
        "createdAt" : "2018-02-27T07:12:53Z",
        "updatedAt" : "2018-04-27T09:52:48Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "fd73e195-6660-4466-b954-574434cbd3ea",
        "parentId" : "75a1a31e-03d4-4a83-b4ae-495b0fbaa659",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "While thinking some more on this, I feel we dont need to mark it as `[slow]`, because the heketi server is running in mock mode and pvc creation should be really fast, the PVC is not attached to a pod, so PVC deletion should also be fast. Becasue of these reasons I am marking this as `fast`.",
        "createdAt" : "2018-04-18T07:22:40Z",
        "updatedAt" : "2018-04-27T09:52:48Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b550134dfea7333fafdbe982108f3d033cd8df6b",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +792,796 @@\n\tframework.KubeDescribe(\"GlusterDynamicProvisioner\", func() {\n\t\tIt(\"should create and delete persistent volumes [fast]\", func() {\n\t\t\tBy(\"creating a Gluster DP server Pod\")\n\t\t\tpod := startGlusterDpServerPod(c, ns)"
  },
  {
    "id" : "758cda7d-74d6-4601-bcd3-c6a0ac1ad114",
    "prId" : 59879,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59879#pullrequestreview-113105816",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "881fe774-9998-477e-962a-ae65a44516d9",
        "parentId" : null,
        "authorId" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "body" : "May need `[Slow]` here.  If all test provisioners (gce, gke, aws?) support native Gluster then no extra tag is needed",
        "createdAt" : "2018-02-16T19:12:36Z",
        "updatedAt" : "2018-04-27T09:52:48Z",
        "lastEditedBy" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "tags" : [
        ]
      },
      {
        "id" : "e3947d29-0e96-4c93-a760-a1e70ab16fd9",
        "parentId" : "881fe774-9998-477e-962a-ae65a44516d9",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "As mentioned in inital PR, the plan is to not  mix this with other cloud providers and the operation should be fast enough to complete. ",
        "createdAt" : "2018-04-18T07:36:08Z",
        "updatedAt" : "2018-04-27T09:52:48Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b550134dfea7333fafdbe982108f3d033cd8df6b",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +791,795 @@\t})\n\n\tframework.KubeDescribe(\"GlusterDynamicProvisioner\", func() {\n\t\tIt(\"should create and delete persistent volumes [fast]\", func() {\n\t\t\tBy(\"creating a Gluster DP server Pod\")"
  },
  {
    "id" : "4adaf1c4-af3a-4c33-8b2c-dd4cc7d611fc",
    "prId" : 52350,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/52350#pullrequestreview-62256483",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ab950fc-6c24-42ee-8fea-2600fd2e9afa",
        "parentId" : null,
        "authorId" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "body" : "Sorry for lack of knowledge here, but  isn't the default reclaim policy \"Delete\" and  the pv automatically deleted as part of dynamic provisioning when the pvc is deleted. If this is true then the e2e test should not have to explicitly delete the pv, but just wait some and check that the pv was deleted (for reclaimPolicy == \"Delete\"). And, the `if` here should test for reclaimPolicy == \"Retain\" so that the e2e cleans up resources it created?",
        "createdAt" : "2017-09-12T17:37:43Z",
        "updatedAt" : "2017-09-13T14:59:52Z",
        "lastEditedBy" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "tags" : [
        ]
      },
      {
        "id" : "11eb0d42-9fe4-4ddc-9b86-523cc819fa1d",
        "parentId" : "5ab950fc-6c24-42ee-8fea-2600fd2e9afa",
        "authorId" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "body" : "the if statement is correct but confusingly inverted because it's saying: if reclaim policy is delete, we expect the pv to be deleted by kube and so we wait for it; else if reclaim policy is retain, there's nothing to do or check for because the job of the kube dynamic provisioner is done.",
        "createdAt" : "2017-09-12T20:21:51Z",
        "updatedAt" : "2017-09-13T14:59:52Z",
        "lastEditedBy" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "tags" : [
        ]
      },
      {
        "id" : "06aa5b6d-0e0f-4acc-b6cc-d48aa5ec8f65",
        "parentId" : "5ab950fc-6c24-42ee-8fea-2600fd2e9afa",
        "authorId" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "body" : "ack. makes sense and I read the code below too quickly...",
        "createdAt" : "2017-09-12T20:32:20Z",
        "updatedAt" : "2017-09-13T14:59:52Z",
        "lastEditedBy" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "tags" : [
        ]
      }
    ],
    "commit" : "69b51a685a84a749d48643c534de52a664438fa4",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +143,147 @@\t// in a couple of minutes. Wait 20 minutes to recover from random cloud\n\t// hiccups.\n\tif pv.Spec.PersistentVolumeReclaimPolicy == v1.PersistentVolumeReclaimDelete {\n\t\tBy(fmt.Sprintf(\"deleting the claim's PV %q\", pv.Name))\n\t\tframework.ExpectNoError(framework.WaitForPersistentVolumeDeleted(client, pv.Name, 5*time.Second, 20*time.Minute))"
  },
  {
    "id" : "06d4a6f2-97b6-4ab3-982c-fb1cf303e2f2",
    "prId" : 52350,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/52350#pullrequestreview-62289379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eea121e9-29b0-43ea-ab6f-b334228c1a7a",
        "parentId" : null,
        "authorId" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "body" : "It is clearer to delete the pv here but curious as to why you delete it here when it could have been unconditionally deleted in `testDynamicProvisioning()` ",
        "createdAt" : "2017-09-12T17:46:11Z",
        "updatedAt" : "2017-09-13T14:59:52Z",
        "lastEditedBy" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "tags" : [
        ]
      },
      {
        "id" : "147aae84-d8fc-4aa5-a5f5-28b539d4fdf8",
        "parentId" : "eea121e9-29b0-43ea-ab6f-b334228c1a7a",
        "authorId" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "body" : "imo it makes sense because the manual deletion is cloudprovider-specific, and outside of the scope of the kube dynamic provisioning process, so i kept it out of testDynamicProvisioning",
        "createdAt" : "2017-09-12T20:22:43Z",
        "updatedAt" : "2017-09-13T14:59:52Z",
        "lastEditedBy" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "tags" : [
        ]
      },
      {
        "id" : "c5e7d895-3ced-4cb5-9e28-82c5234f95f6",
        "parentId" : "eea121e9-29b0-43ea-ab6f-b334228c1a7a",
        "authorId" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "body" : "I agree. But do you want to wait for the pv to be deleted, as is done in `testDynamicProvisioning`?",
        "createdAt" : "2017-09-12T20:34:31Z",
        "updatedAt" : "2017-09-13T14:59:52Z",
        "lastEditedBy" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "tags" : [
        ]
      },
      {
        "id" : "0885afda-3f37-40ef-a6db-cefac9890afd",
        "parentId" : "eea121e9-29b0-43ea-ab6f-b334228c1a7a",
        "authorId" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "body" : "okay, wouldn't hurt: will add WaitForPersistentVolumeDeleted after this line",
        "createdAt" : "2017-09-12T22:53:49Z",
        "updatedAt" : "2017-09-13T14:59:52Z",
        "lastEditedBy" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "tags" : [
        ]
      }
    ],
    "commit" : "69b51a685a84a749d48643c534de52a664438fa4",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +445,449 @@\n\t\t\tBy(fmt.Sprintf(\"deleting the PV %q\", pv.Name))\n\t\t\tframework.ExpectNoError(framework.DeletePersistentVolume(c, pv.Name), \"Failed to delete PV \", pv.Name)\n\t\t\tframework.ExpectNoError(framework.WaitForPersistentVolumeDeleted(c, pv.Name, 1*time.Second, 30*time.Second))\n\t\t})"
  }
]