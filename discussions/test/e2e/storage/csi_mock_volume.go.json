[
  {
    "id" : "91c8a8ef-0f06-4c30-9973-d03c6ca6c9b2",
    "prId" : 100183,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100183#pullrequestreview-611130578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5282162-57c1-4ac6-887a-f2f89fa46f40",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "For InvalidArgument, do we need to mark as uncertain?",
        "createdAt" : "2021-03-12T19:52:09Z",
        "updatedAt" : "2021-03-12T19:52:09Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "d7a9eff2-be0d-45b4-bdce-1bfc71d62c3d",
        "parentId" : "a5282162-57c1-4ac6-887a-f2f89fa46f40",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "It's an example of a \"final\" response, where we can be reasonably sure that e.g. provisioning is not in progress. Not sure if we can apply the same logic to Unstage.",
        "createdAt" : "2021-03-12T20:32:44Z",
        "updatedAt" : "2021-03-12T20:32:44Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5da73032f7882afe180af77a5fcc8f441d36807",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +931,935 @@\t\t\t\texpectedCalls: []csiCall{\n\t\t\t\t\t{expectedMethod: \"NodeStageVolume\", expectedError: codes.OK},\n\t\t\t\t\t{expectedMethod: \"NodeUnstageVolume\", expectedError: codes.InvalidArgument},\n\t\t\t\t\t{expectedMethod: \"NodeStageVolume\", expectedError: codes.OK},\n\t\t\t\t\t{expectedMethod: \"NodeUnstageVolume\", expectedError: codes.OK},"
  },
  {
    "id" : "6c246eaa-d020-461f-a218-f778fcf6b5d3",
    "prId" : 99641,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99641#pullrequestreview-605740926",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "107ab87f-5b56-499f-8711-615b01d158c0",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "The feature tag should be removed so it always runs",
        "createdAt" : "2021-03-05T23:48:57Z",
        "updatedAt" : "2021-03-08T20:00:14Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "8ce681a1-27b8-481d-9627-cb5c1b91aeb5",
        "parentId" : "107ab87f-5b56-499f-8711-615b01d158c0",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "Funny, this also occured to me this morning during breakfast :sweat_smile: \r\n\r\nDone.",
        "createdAt" : "2021-03-06T08:43:47Z",
        "updatedAt" : "2021-03-08T20:00:14Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c7e4c6316077aaf7822b1d4166ef54218ecee80",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +32,36 @@\tv1 \"k8s.io/api/core/v1\"\n\tstoragev1 \"k8s.io/api/storage/v1\"\n\tstoragev1beta1 \"k8s.io/api/storage/v1beta1\"\n\tapierrors \"k8s.io/apimachinery/pkg/api/errors\"\n\t\"k8s.io/apimachinery/pkg/api/resource\""
  },
  {
    "id" : "63ad9971-d176-4e69-b31c-96d2544680f2",
    "prId" : 99167,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99167#pullrequestreview-592729340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bf34131-a259-433a-aed7-30c7077cbfb5",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "this can avoid test failure. But I roughly checked the test code, it is kind of strange for me this test creates snapshot first, and then PVC? Could you please double check?",
        "createdAt" : "2021-02-17T23:16:03Z",
        "updatedAt" : "2021-02-18T19:32:46Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "4789538f-1578-42c9-b47e-e51e51482db1",
        "parentId" : "6bf34131-a259-433a-aed7-30c7077cbfb5",
        "authorId" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "body" : "Yes, they are creating snapshot first and then PVC. Does that matter? I think it is okay. The snapshot will just waiting I guess?",
        "createdAt" : "2021-02-17T23:19:57Z",
        "updatedAt" : "2021-02-18T19:32:46Z",
        "lastEditedBy" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "tags" : [
        ]
      },
      {
        "id" : "6501bd68-f7a9-4a01-8ca5-3463ce246c84",
        "parentId" : "6bf34131-a259-433a-aed7-30c7077cbfb5",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "ok, I think the original design is snapshot will fail, but now I kind of remember the design is changed to retry forever..",
        "createdAt" : "2021-02-17T23:57:28Z",
        "updatedAt" : "2021-02-18T19:32:46Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "95bded5193241e1c1d8aea72272e5b0487b4e0ca",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1302,1306 @@\t\t\t\t\t}\n\t\t\t\t\tframework.Logf(\"PVC not found. Continuing to test VolumeSnapshotContent finalizer\")\n\t\t\t\t} else if claim.DeletionTimestamp == nil {\n\t\t\t\t\tframework.Failf(\"Expected deletion timestamp to be set on PVC: %v\", claim)\n\t\t\t\t}"
  },
  {
    "id" : "bb147f92-7cda-49ec-ac79-bf3b4b2a74ab",
    "prId" : 99167,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99167#pullrequestreview-592735681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "524f7271-d58c-4e01-abf4-ad1289389385",
        "parentId" : null,
        "authorId" : "ffc1d568-c5ed-4932-82f4-3ae9d3bee69a",
        "body" : "Alternatively, is it reasonable to `framework.Failf()` if we get back *any* error (including not found)? ",
        "createdAt" : "2021-02-17T23:43:43Z",
        "updatedAt" : "2021-02-18T19:32:46Z",
        "lastEditedBy" : "ffc1d568-c5ed-4932-82f4-3ae9d3bee69a",
        "tags" : [
        ]
      },
      {
        "id" : "5e2fff9b-4f02-4e6c-92a4-395ed55e76b4",
        "parentId" : "524f7271-d58c-4e01-abf4-ad1289389385",
        "authorId" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "body" : "I think the test here is that if we get not found error, the test will continue. So not found err is part of the expecting scenario that we can encounter",
        "createdAt" : "2021-02-18T00:12:30Z",
        "updatedAt" : "2021-02-18T19:32:46Z",
        "lastEditedBy" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "tags" : [
        ]
      }
    ],
    "commit" : "95bded5193241e1c1d8aea72272e5b0487b4e0ca",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +1301,1305 @@\t\t\t\t\t\tframework.ExpectNoError(err, \"Failed to get claim: %v\", err)\n\t\t\t\t\t}\n\t\t\t\t\tframework.Logf(\"PVC not found. Continuing to test VolumeSnapshotContent finalizer\")\n\t\t\t\t} else if claim.DeletionTimestamp == nil {\n\t\t\t\t\tframework.Failf(\"Expected deletion timestamp to be set on PVC: %v\", claim)"
  },
  {
    "id" : "ac674280-9b01-4259-a094-f32b9f2c7851",
    "prId" : 97069,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97069#pullrequestreview-600723260",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d153e4ab-6dd3-4b7a-ba8e-a672ec8b4e9f",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "This comment is not true, I can see javascript replaced by proxy in many (all?) tests.",
        "createdAt" : "2021-03-01T11:38:22Z",
        "updatedAt" : "2021-03-01T18:23:55Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "b696fae8-d084-4179-80d7-5d2ff26a185e",
        "parentId" : "d153e4ab-6dd3-4b7a-ba8e-a672ec8b4e9f",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "Yes, and that's what I meant here: all usage of JavaScript gets replaced by hooks, and only those tests then run with the embedded CSI mock driver.\r\n",
        "createdAt" : "2021-03-01T13:22:23Z",
        "updatedAt" : "2021-03-01T18:23:55Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ef648d9360afafa847633149032e0ed4e29a556",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +159,163 @@\t\t// the driver inside the cluster although they could\n\t\t// changed to use embedding merely by setting\n\t\t// driverOpts.embedded to true.\n\t\t//\n\t\t// Not enabling it for all tests minimizes"
  },
  {
    "id" : "916bcbab-a9da-4be8-b069-eac991be7812",
    "prId" : 96042,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96042#pullrequestreview-538502367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "975301a5-f72d-4b53-a89c-13c71ffef5c4",
        "parentId" : null,
        "authorId" : "70e0dd56-8907-46d7-8381-c173d3c02d47",
        "body" : "10*time.Second replace this with podStartShortTimeout?",
        "createdAt" : "2020-11-24T19:22:57Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "70e0dd56-8907-46d7-8381-c173d3c02d47",
        "tags" : [
        ]
      },
      {
        "id" : "8dfc18bd-4c9b-4abe-b45e-feed18eb28ac",
        "parentId" : "975301a5-f72d-4b53-a89c-13c71ffef5c4",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "I believe this is 10 seconds because the test expects `WaitTimeoutForPodRunningInNamespace` to time out (see next line). A longer timeout, like `podStartShortTimeout` would hold the test run for too long. ",
        "createdAt" : "2020-11-25T15:56:34Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ee082985c2d8568989400d214f56c020ec8b20f9",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +387,391 @@\t\t\terr = e2eevents.WaitTimeoutForEvent(m.cs, pod.Namespace, eventSelector, msg, f.Timeouts.PodStart)\n\t\t\tif err != nil {\n\t\t\t\tpodErr := e2epod.WaitTimeoutForPodRunningInNamespace(m.cs, pod.Name, pod.Namespace, 10*time.Second)\n\t\t\t\tframework.ExpectError(podErr, \"Pod should not be in running status because attaching should failed\")\n\t\t\t\t// Events are unreliable, don't depend on the event. It's used only to speed up the test."
  },
  {
    "id" : "ee47a159-f1ed-4827-aec8-2739c16aa961",
    "prId" : 96042,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96042#pullrequestreview-542197287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04d1a98e-a7af-4098-9de3-ab28e3d0fa1d",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Hm so if a plugin decides to set a non-default timeout, then that will impact the timeout for non driver specific tests?\r\n\r\nIs that what we want?",
        "createdAt" : "2020-12-01T00:21:00Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "221f9a8a-0db4-4b25-a2c4-cf6c0ce3e36b",
        "parentId" : "04d1a98e-a7af-4098-9de3-ab28e3d0fa1d",
        "authorId" : "70e0dd56-8907-46d7-8381-c173d3c02d47",
        "body" : "can you point to an e.g for non-driver specific test?",
        "createdAt" : "2020-12-01T00:53:31Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "70e0dd56-8907-46d7-8381-c173d3c02d47",
        "tags" : [
        ]
      },
      {
        "id" : "00a31529-a12a-4344-a7ef-c60f19190b2a",
        "parentId" : "04d1a98e-a7af-4098-9de3-ab28e3d0fa1d",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I was thinking specifically about this csi mock driver test. If you run the csi mock driver test along with a cloud plugin that takes minutes to provision/attach, etc, then that increases the timeout on the mock driver tests, which aren't expected to take very long, and could mask other problems in the system.",
        "createdAt" : "2020-12-01T01:05:50Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "7a98d596-5fde-4961-bdcb-444633c266d1",
        "parentId" : "04d1a98e-a7af-4098-9de3-ab28e3d0fa1d",
        "authorId" : "70e0dd56-8907-46d7-8381-c173d3c02d47",
        "body" : "I see, this seems to be a csi mock plugin specific problem. For the testsuites, they are run in the context of a driver (intree, csi or external). We can remove the non-default timeouts for the mock driver. thoughts? (we dont expect any variable configurable timeouts for csi mock plugin anyway)",
        "createdAt" : "2020-12-01T01:14:26Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "70e0dd56-8907-46d7-8381-c173d3c02d47",
        "tags" : [
        ]
      },
      {
        "id" : "68d517cf-b9f4-40d3-80a5-8e9ac98b2e95",
        "parentId" : "04d1a98e-a7af-4098-9de3-ab28e3d0fa1d",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I guess right now these timeouts are only configurable via external storage interface or a custom ginkgo suite, so it's unlikely for someone to be running both csi mock and external storage tests in the same run. I would be more concerned if we want to extend this to all e2es in the future.",
        "createdAt" : "2020-12-01T01:25:57Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "67309953-f961-4239-9f2e-522b2ccf10a4",
        "parentId" : "04d1a98e-a7af-4098-9de3-ab28e3d0fa1d",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Notice that this csi mock driver test is not using custom timeouts, it's using the default ones:\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/96042/files/f594e0f7737a0d281e9b08ddf1b1ec770755ee6a#diff-ffda3f0c41f167e7deedad89b23cf9b89d0e9805f910c81e075baa19d667f21fR137\r\n\r\nEven if it was using the custom timeouts, it would get the values returned by the mock driver's `GetTimeouts()` method, as in:\r\n\r\n```diff\r\ndiff --git a/test/e2e/storage/drivers/csi.go b/test/e2e/storage/drivers/csi.go\r\nindex 03ec35e557c..d4956d47389 100644\r\n--- a/test/e2e/storage/drivers/csi.go\r\n+++ b/test/e2e/storage/drivers/csi.go\r\n@@ -278,6 +278,7 @@ type CSIMockDriverOpts struct {\r\n var _ testsuites.TestDriver = &mockCSIDriver{}\r\n var _ testsuites.DynamicPVTestDriver = &mockCSIDriver{}\r\n var _ testsuites.SnapshottableTestDriver = &mockCSIDriver{}\r\n+var _ testsuites.CustomTimeoutsTestDriver = &mockCSIDriver{}\r\n\r\n // InitMockCSIDriver returns a mockCSIDriver that implements TestDriver interface\r\n func InitMockCSIDriver(driverOpts CSIMockDriverOpts) testsuites.TestDriver {\r\n@@ -336,6 +337,12 @@ func InitMockCSIDriver(driverOpts CSIMockDriverOpts) testsuites.TestDriver {\r\n        }\r\n }\r\n\r\n+func (m *mockCSIDriver) GetTimeouts() *framework.TimeoutContext {\r\n+       timeouts := framework.NewTimeoutContextWithDefaults()\r\n+       timeouts.PodStart = time.Second * 5\r\n+       return timeouts\r\n+}\r\n+\r\n func (m *mockCSIDriver) GetDriverInfo() *testsuites.DriverInfo {\r\n        return &m.driverInfo\r\n }\r\n```",
        "createdAt" : "2020-12-01T18:38:43Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "5bab4d7c-2aea-4a0d-8634-f9eb27a94264",
        "parentId" : "04d1a98e-a7af-4098-9de3-ab28e3d0fa1d",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Awesome, this addresses my concern for now thanks!",
        "createdAt" : "2020-12-01T18:49:26Z",
        "updatedAt" : "2020-12-02T19:15:54Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ee082985c2d8568989400d214f56c020ec8b20f9",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +385,389 @@\t\t\tmsg := \"AttachVolume.Attach failed for volume\"\n\n\t\t\terr = e2eevents.WaitTimeoutForEvent(m.cs, pod.Namespace, eventSelector, msg, f.Timeouts.PodStart)\n\t\t\tif err != nil {\n\t\t\t\tpodErr := e2epod.WaitTimeoutForPodRunningInNamespace(m.cs, pod.Name, pod.Namespace, 10*time.Second)"
  },
  {
    "id" : "19b570be-076c-4110-93de-f0397b54b955",
    "prId" : 95863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95863#pullrequestreview-519291800",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3749543b-6423-45da-8331-357ee1e0f2e1",
        "parentId" : null,
        "authorId" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "body" : "Snapshots should be deleted too.  I don't see a DeleteSnapshot call.\r\n\r\nFor deleting snapshot, can you add a regular way to delete VolumeSnapshot first and another test case to delete VolumeSnapshotContent first?",
        "createdAt" : "2020-10-28T21:00:51Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "tags" : [
        ]
      },
      {
        "id" : "cf5a44f2-dd39-45db-8af7-fed4dc59e354",
        "parentId" : "3749543b-6423-45da-8331-357ee1e0f2e1",
        "authorId" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "body" : "Currently i have `DeleteAndWaitSnapshot` called in test cleanup on line 1217. Do you want to move that here? ",
        "createdAt" : "2020-10-29T02:11:07Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "tags" : [
        ]
      },
      {
        "id" : "e87ba4e6-b210-4040-81df-774498949709",
        "parentId" : "3749543b-6423-45da-8331-357ee1e0f2e1",
        "authorId" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "body" : "It's fine to keep that there.  I think you can add a negative test here to delete VolumeSnapshotContent.  Since they are bound, it should only add the deletionTimestamp on VolumeSnapshotContent but it will not be deleted.  Just try to retrieve the VolumesnapshotContent from API server again after the delete call and check that the deletionTimestamp is added.  Then it will be cleaned up on line 1217.",
        "createdAt" : "2020-10-29T02:43:33Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "tags" : [
        ]
      }
    ],
    "commit" : "e95af138b538d4beace1eceaeb454c57072130d9",
    "line" : 194,
    "diffHunk" : "@@ -1,1 +1300,1304 @@\t\t\t\tframework.ExpectNoError(err, fmt.Sprintf(\"failed to delete VolumeSnapshotContent %s\", volumeSnapshotContentName))\n\t\t\t})\n\t\t}\n\t})\n})"
  },
  {
    "id" : "959dfa98-65cb-46ed-be62-28ac0e813ad0",
    "prId" : 95863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95863#pullrequestreview-522835538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f745a1e1-605a-4d1e-b075-752d8ebf10e1",
        "parentId" : null,
        "authorId" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "body" : "Can you also check the finalizer on the content?",
        "createdAt" : "2020-11-03T17:29:38Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "tags" : [
        ]
      },
      {
        "id" : "fe12c3d8-ae68-4cf8-bbf0-be16d763b0e4",
        "parentId" : "f745a1e1-605a-4d1e-b075-752d8ebf10e1",
        "authorId" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "body" : "done",
        "createdAt" : "2020-11-03T20:03:48Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "tags" : [
        ]
      }
    ],
    "commit" : "e95af138b538d4beace1eceaeb454c57072130d9",
    "line" : 174,
    "diffHunk" : "@@ -1,1 +1280,1284 @@\t\t\t\tif snapshotContent.GetDeletionTimestamp() == nil {\n\t\t\t\t\tframework.Failf(\"Expected deletion timestamp to be set on snapshotcontent\")\n\t\t\t\t}\n\n\t\t\t\tif claim != nil {"
  },
  {
    "id" : "59e6eccb-5f42-4930-a235-21c5fb131b42",
    "prId" : 95863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95863#pullrequestreview-522973108",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4143894f-75e9-4871-8e42-8502bd5c194d",
        "parentId" : null,
        "authorId" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "body" : "This is outside of the snapshot feature tag?",
        "createdAt" : "2020-11-03T21:56:28Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "tags" : [
        ]
      },
      {
        "id" : "5846a5e4-65f9-401c-80cc-f845453b0803",
        "parentId" : "4143894f-75e9-4871-8e42-8502bd5c194d",
        "authorId" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "body" : "yeah i kept it here to be consistent with cleaning up of storage classes. Ideally, `vsc` would only be populated in tests that support the feature",
        "createdAt" : "2020-11-03T22:00:54Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "90cfc0c5-e945-4584-be7e-3411fd993507",
        "tags" : [
        ]
      },
      {
        "id" : "61cfbef5-8d34-4c3a-8515-1a8c4d43463d",
        "parentId" : "4143894f-75e9-4871-8e42-8502bd5c194d",
        "authorId" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "body" : "ok",
        "createdAt" : "2020-11-04T00:46:52Z",
        "updatedAt" : "2020-11-04T22:08:44Z",
        "lastEditedBy" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "tags" : [
        ]
      }
    ],
    "commit" : "e95af138b538d4beace1eceaeb454c57072130d9",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +255,259 @@\t\t\tginkgo.By(fmt.Sprintf(\"Deleting volumesnapshotclass %s\", vsc.GetName()))\n\t\t\tm.config.Framework.DynamicClient.Resource(testsuites.SnapshotClassGVR).Delete(context.TODO(), vsc.GetName(), metav1.DeleteOptions{})\n\t\t}\n\t\tginkgo.By(\"Cleaning up resources\")\n\t\tfor _, cleanupFunc := range m.testCleanups {"
  },
  {
    "id" : "887bdf56-8f38-4cfc-b6c2-74725f4cd62c",
    "prId" : 95848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95848#pullrequestreview-534052103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7ba557d-479f-47d5-a2b7-024de0978e96",
        "parentId" : null,
        "authorId" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "body" : "Add a wait here to make sure PVC is bound before moving forward.  Search for the usage of WaitForPVClaimBoundPhase.",
        "createdAt" : "2020-11-19T03:22:47Z",
        "updatedAt" : "2021-02-04T05:16:28Z",
        "lastEditedBy" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6ec62f76d0fda6bef1b41f9eec93ec6377a4988",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1560,1564 @@\t\t\t\tpvc, err = m.cs.CoreV1().PersistentVolumeClaims(f.Namespace.Name).Create(context.TODO(), pvc, metav1.CreateOptions{})\n\t\t\t\tframework.ExpectNoError(err, \"Failed to create claim: %v\", err)\n\n\t\t\t\tginkgo.By(\"Wait for PVC to be Bound\")\n\t\t\t\t_, err = e2epv.WaitForPVClaimBoundPhase(m.cs, []*v1.PersistentVolumeClaim{pvc}, 1*time.Minute)"
  },
  {
    "id" : "a9d94629-b4b3-440b-bcca-45e6263cd883",
    "prId" : 95848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95848#pullrequestreview-567583051",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e8d71fb-115d-4b9b-b156-467b051d12d8",
        "parentId" : null,
        "authorId" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "body" : "This is CSISnapshotterSecretNameAnnotation, but comment is about CSISnapshotterSecretNamespaceAnnotation",
        "createdAt" : "2021-01-13T20:15:52Z",
        "updatedAt" : "2021-02-04T05:16:28Z",
        "lastEditedBy" : "275dd783-53c3-4fed-8434-96ed6a2e0331",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6ec62f76d0fda6bef1b41f9eec93ec6377a4988",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1500,1504 @@\n\t\t\t// CSISnapshotterSecretNameAnnotation is the annotation key for the CSI snapshotter secret name in VolumeSnapshotClass.parameters\n\t\t\tCSISnapshotterSecretNameAnnotation string = \"csi.storage.k8s.io/snapshotter-secret-name\"\n\n\t\t\t// CSISnapshotterSecretNamespaceAnnotation is the annotation key for the CSI snapshotter secret namespace in VolumeSnapshotClass.parameters"
  },
  {
    "id" : "9cc0a3e7-4094-4f74-adfa-6e78c9187665",
    "prId" : 95848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95848#pullrequestreview-583042737",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18497398-6726-422d-805a-9526c338421e",
        "parentId" : null,
        "authorId" : "ffc1d568-c5ed-4932-82f4-3ae9d3bee69a",
        "body" : "Can you explain what is going on here? Why is the deadline is exceeded for the first 8 calls and then successful? (If I am reading this corectly)",
        "createdAt" : "2021-01-29T22:25:31Z",
        "updatedAt" : "2021-02-04T05:16:28Z",
        "lastEditedBy" : "ffc1d568-c5ed-4932-82f4-3ae9d3bee69a",
        "tags" : [
        ]
      },
      {
        "id" : "f6577d96-8842-44ac-9592-51eff69bb03f",
        "parentId" : "18497398-6726-422d-805a-9526c338421e",
        "authorId" : "f4f4ad43-9853-4a8a-82f6-88f811124122",
        "body" : "this is to test, volume snapshot should be created using secrets successfully even if there is a failure in the first few attempts. Added comments explaining the same. ",
        "createdAt" : "2021-02-04T05:14:21Z",
        "updatedAt" : "2021-02-04T05:16:28Z",
        "lastEditedBy" : "f4f4ad43-9853-4a8a-82f6-88f811124122",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6ec62f76d0fda6bef1b41f9eec93ec6377a4988",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +1521,1525 @@\t\t\t\tcreateVolumeScript: `OK`,\n\t\t\t\t// Fail the first 8 calls to create snapshot and succeed the  9th call.\n\t\t\t\tcreateSnapshotScript: `console.log(\"Counter:\", ++counter); if (counter < 8) { DEADLINEEXCEEDED; } else { OK; }`,\n\t\t\t},\n\t\t}"
  },
  {
    "id" : "7a6cbe6d-48c3-4ba0-98df-6ae49c53269a",
    "prId" : 95739,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95739#pullrequestreview-529547349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3084d305-9fe4-48b8-a562-1672bacaf1f7",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I am worried about flakes. Maybe we can try running this test a few times in a loop?",
        "createdAt" : "2020-11-07T01:14:47Z",
        "updatedAt" : "2020-11-12T22:09:57Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "e2aebb6c-3fac-4c43-b7cd-bbfcb10197d7",
        "parentId" : "3084d305-9fe4-48b8-a562-1672bacaf1f7",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can you open a bug to investigate this later?",
        "createdAt" : "2020-11-12T20:26:34Z",
        "updatedAt" : "2020-11-12T22:09:57Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "52065682-9424-43d7-b03a-a51a2a9240ef",
        "parentId" : "3084d305-9fe4-48b8-a562-1672bacaf1f7",
        "authorId" : "1a46cba5-dd9b-4c03-b94c-4dbfb64198e4",
        "body" : "Opened https://github.com/kubernetes/kubernetes/issues/96540",
        "createdAt" : "2020-11-12T22:21:52Z",
        "updatedAt" : "2020-11-12T22:21:53Z",
        "lastEditedBy" : "1a46cba5-dd9b-4c03-b94c-4dbfb64198e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "38071e74cf993b67290e9a458537e0a8460e8846",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +1445,1449 @@\t\t\t\t// kube-scheduler may need some time before it gets the CSIDriver object.\n\t\t\t\t// Without them, scheduling doesn't run as expected by the test.\n\t\t\t\tsyncDelay := 5 * time.Second\n\t\t\t\ttime.Sleep(syncDelay)\n"
  },
  {
    "id" : "3da7c802-b2de-4371-aec6-bab8fc78d65f",
    "prId" : 93777,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93777#pullrequestreview-466758579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57692358-5e68-4d99-bb55-a38add9c1e42",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "is it intentional that this is omitting a ListFunc? should it implement one that errors and fails the test rather than getting a nil panic?",
        "createdAt" : "2020-08-12T14:03:43Z",
        "updatedAt" : "2020-08-12T15:56:53Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "a7554728-7669-473a-83e4-ca2df388454e",
        "parentId" : "57692358-5e68-4d99-bb55-a38add9c1e42",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "I was following the approach from `WatchEventSequenceVerifier` which also only specifies a `WatchFunc`: https://github.com/kubernetes/kubernetes/blob/b1b93e30131986319f1d46a62771feb9c0f097ae/test/e2e/framework/util.go#L1335-L1348\r\n\r\nTo be honest, I hadn't considered whether that is correct. Now that you asked, I had a closer look and came to the conclusion that just the `WatchFunc` is needed because `NewRetryWatcher` only needs an implementation of the `cache.Watcher` interface.",
        "createdAt" : "2020-08-13T13:13:16Z",
        "updatedAt" : "2020-08-13T13:13:17Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "92df1bf5-0b99-4e36-be14-e16dffab55a2",
        "parentId" : "57692358-5e68-4d99-bb55-a38add9c1e42",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "that's true at the moment, but if that implementation ever changes we'll get a panic, which might be tough to track down. for purposes of this test, this seems ok for now.",
        "createdAt" : "2020-08-13T13:26:21Z",
        "updatedAt" : "2020-08-13T13:26:21Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a510a1becfda4d59d6cf979af61e92dbb3d9de5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +832,836 @@\t\t\t\tinitResource, err := f.ClientSet.CoreV1().PersistentVolumeClaims(f.Namespace.Name).List(ctx, metav1.ListOptions{})\n\t\t\t\tframework.ExpectNoError(err, \"Failed to fetch initial PVC resource\")\n\t\t\t\tlistWatcher := &cachetools.ListWatch{\n\t\t\t\t\tWatchFunc: func(listOptions metav1.ListOptions) (watch.Interface, error) {\n\t\t\t\t\t\treturn f.ClientSet.CoreV1().PersistentVolumeClaims(f.Namespace.Name).Watch(ctx, listOptions)"
  },
  {
    "id" : "5fd23982-a577-4fdc-a882-50819cd34e1d",
    "prId" : 93658,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93658#pullrequestreview-463904898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "parentId" : null,
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "How often does this happen? I suppose it is caused by load on the apiserver?\r\n\r\nI'm wondering whether it would be better to skip the test when this happens. As proposed here, we still have a flaky test failure, it just fails without flooding the logs.\r\n",
        "createdAt" : "2020-08-04T06:32:45Z",
        "updatedAt" : "2020-08-04T06:32:45Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "e99afba6-3b9f-42f1-ae7c-5163a58abf31",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "body" : "@pohly your call, I'm happy to skip or fail. I've seen it a couple of times on our OpenShift tests on GCP. It may be related to leader elections on the apiserver (which can happen at any time), which can cause short blips where the watchers close their existing watch and relist everything when the apiserver comes back.",
        "createdAt" : "2020-08-04T14:46:20Z",
        "updatedAt" : "2020-08-04T14:46:21Z",
        "lastEditedBy" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "tags" : [
        ]
      },
      {
        "id" : "24d8c9a4-1876-4916-a52d-bb7cea0e4325",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "I prefer skipping the test in this case.\r\n",
        "createdAt" : "2020-08-04T15:29:28Z",
        "updatedAt" : "2020-08-04T15:29:28Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "9c18ec80-8dbb-4e67-9739-1bbc246c0d6e",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Watches can time out at ANY time, and both etcd and kube-apiserver are allowed to do this.\r\n\r\nNo one should ever read from a watch stream and assume how long that watch is open.  We reserve the right to close the watch at any point from kube-apiserver gracefully, from intervals of 5s to hours.\r\n\r\nSo please, fix the test to not assume things that aren't true :)",
        "createdAt" : "2020-08-04T18:06:09Z",
        "updatedAt" : "2020-08-04T18:09:51Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "128d9926-ac1f-49c1-b177-04636af76dff",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "> fix this to retry the watch (single shot watch is not safe)\r\n\r\nI think that restarting the watch is not going to work: what the test tries to verify is that rescheduling happened. The only indication of that through the API is that the pod annotation gets set and later removed. If the watch is closed while the annotation is set and later gets restarted when it is set again, the test misses the \"annotation was reset\" part and fails incorrectly.\r\n\r\nI see several options:\r\n- stop using watches and instead rely on log parsing to detect whether rescheduling happened (not sure though whether there is anything in the logs about it)\r\n- skip then when the watch is closed\r\n- remove the test\r\n\r\nI'm leaning towards skipping because log parsing may also be problematic and a test that usually (I know, that's an assumption) works is better than no test.\r\n\r\n",
        "createdAt" : "2020-08-05T09:00:36Z",
        "updatedAt" : "2020-08-05T09:00:36Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "012c417f-e228-4a87-b227-c6e40be976c3",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "body" : "Perhaps the test could use a SharedInformer on top of the watch which would handle restarts correctly via the Reflector, and the DeltaFifo would ensure that you have correct updates to the object itself. You don't care about actual watch events, you only care about changes to the PVC object itself, right?  SharedInformer would let you concentrate on just the object and not the watch.",
        "createdAt" : "2020-08-05T14:29:15Z",
        "updatedAt" : "2020-08-05T14:29:15Z",
        "lastEditedBy" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "tags" : [
        ]
      },
      {
        "id" : "5bc7fcad-8870-4753-bbca-b0ba3c29cc47",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "But does SharedInformer guarantee that I see all updates? My understanding of it is that it is possible to miss updates, i.e. the only guarantee is that the callbacks eventually get the final state of each object.\r\n",
        "createdAt" : "2020-08-05T17:20:10Z",
        "updatedAt" : "2020-08-05T17:20:10Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "0af3d956-4e32-4680-bb46-e4778bc16856",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "body" : "> But does SharedInformer guarantee that I see all updates? My understanding of it is that it is possible to miss updates, i.e. the only guarantee is that the callbacks eventually get the final state of each object.\r\n\r\n@pohly doesn't that indicate a flaw in the scheme of annotation removal/addition? Any client using shared informer might miss those PVC annotation changes too, if they are collapsed into a single update by DeltaFIFO and thus the annotation removal is elided.\r\n\r\nI tried the sharedinformer approach in https://github.com/kubernetes/kubernetes/pull/93720 but don't think I'd have the time to push it forward. But it still the flaw you allude to; I think clients and the testcase need to assume they will only see the *current* object state and perhaps not any intermediate states, and only act on that (eg reconciliation).",
        "createdAt" : "2020-08-05T17:39:51Z",
        "updatedAt" : "2020-08-05T17:39:52Z",
        "lastEditedBy" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "tags" : [
        ]
      },
      {
        "id" : "04b905ac-b176-4bc0-bca6-1c85f49483d9",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "No, the actual usage of the annotation is not affected because it's value-triggered: if set, the external-provisioner does something. If unset, the scheduler does something. Nothing else changes the annotation, therefore it is fine to just know the current state.\r\n\r\nThe difference is that test wants to verify *how* we got to the current state. If the code and/or test is flawed, then we might reach the same observable end-state without actually having gone through the rescheduling cycle in the middle.\r\n\r\n\r\n",
        "createdAt" : "2020-08-06T06:35:34Z",
        "updatedAt" : "2020-08-06T06:35:34Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "d2cfb64d-2ab6-44c4-975c-b151d19c4ab7",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "I ended up just skipping the additional checks, i.e. the test will still be considered a success: https://github.com/kubernetes/kubernetes/pull/93777\r\n\r\n@dcbw: perhaps you can review that PR?\r\n",
        "createdAt" : "2020-08-07T12:53:25Z",
        "updatedAt" : "2020-08-07T12:53:25Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "562bef48-51a4-4476-bbf5-d48d420b3f05",
        "parentId" : "6125947e-df9d-4c87-a27b-f6e4ce078daf",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "> If the watch is closed while the annotation is set and later gets restarted when it is set again, the test misses the \"annotation was reset\" part and fails incorrectly.\r\n\r\nThe watchtools helpers provide a watch interface backed by an informer which will restart a dropped watch from the last observed event, so it won’t miss any events as long as it gets an updated starting resourceVersion before compaction, which it generally should because watch bookmarks are now enabled. I’d recommend using that. ",
        "createdAt" : "2020-08-09T21:31:56Z",
        "updatedAt" : "2020-08-09T21:31:56Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0f9fe2cd33db1e7debdc7db41e4fae54536d584",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +895,899 @@\t\t\t\t\tcase event, ok := <-pvcWatch.ResultChan():\n\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\tframework.Failf(\"PVC watch ended prematurely\")\n\t\t\t\t\t\t}\n"
  },
  {
    "id" : "1aa6489c-0171-43e8-97fa-07430b554fa6",
    "prId" : 93130,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93130#pullrequestreview-525463353",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6316a5a-5f20-4dab-89f2-278d1faee011",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "For alpha feature tests, we'll need to update test-infra alpha job to run this.",
        "createdAt" : "2020-10-07T21:34:47Z",
        "updatedAt" : "2020-11-12T17:26:59Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "c2e75480-1031-4c2d-856f-83c540d993e9",
        "parentId" : "f6316a5a-5f20-4dab-89f2-278d1faee011",
        "authorId" : "f64c354b-61cb-4b89-b58f-f464e7ad4f94",
        "body" : "https://github.com/kubernetes/test-infra/pull/19571",
        "createdAt" : "2020-10-14T21:35:48Z",
        "updatedAt" : "2020-11-12T17:26:59Z",
        "lastEditedBy" : "f64c354b-61cb-4b89-b58f-f464e7ad4f94",
        "tags" : [
        ]
      },
      {
        "id" : "23a76033-5486-4140-90d6-6c92eb1b10cf",
        "parentId" : "f6316a5a-5f20-4dab-89f2-278d1faee011",
        "authorId" : "f64c354b-61cb-4b89-b58f-f464e7ad4f94",
        "body" : "PR merged",
        "createdAt" : "2020-11-06T20:11:30Z",
        "updatedAt" : "2020-11-12T17:26:59Z",
        "lastEditedBy" : "f64c354b-61cb-4b89-b58f-f464e7ad4f94",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2859cd89b0ec00054219ed9988c6dccff39a0c8",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1309,1313 @@\t})\n\n\tginkgo.Context(\"CSIServiceAccountToken [Feature:CSIServiceAccountToken]\", func() {\n\t\tvar (\n\t\t\terr error"
  },
  {
    "id" : "03e09134-3689-4809-a36f-d24683071cb2",
    "prId" : 92387,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92387#pullrequestreview-441490319",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4cb8ed0-ecb0-411c-83e9-98e378a445e7",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "What about the case where there's not enough capacity",
        "createdAt" : "2020-07-01T20:55:56Z",
        "updatedAt" : "2020-07-08T06:02:58Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "f71c7c4b-cf92-4fd7-87aa-6a6b8c44ca47",
        "parentId" : "a4cb8ed0-ecb0-411c-83e9-98e378a445e7",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "Added.",
        "createdAt" : "2020-07-02T12:33:08Z",
        "updatedAt" : "2020-07-08T06:02:58Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      }
    ],
    "commit" : "30f93802a70f988bee901a6d272883e20eafad57",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +981,985 @@\t\t\t},\n\t\t\t{\n\t\t\t\tname:            \"CSIStorageCapacity used, have capacity\",\n\t\t\t\tstorageCapacity: &yes,\n\t\t\t\tcapacities:      []string{\"100Gi\"},"
  },
  {
    "id" : "3e50310f-973e-4b2c-9d71-7d0e92e2deb7",
    "prId" : 90793,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90793#pullrequestreview-412900227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6195d0fa-2122-4742-afed-8d28aaca212a",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can we open an issue to track this problem?",
        "createdAt" : "2020-05-13T16:56:47Z",
        "updatedAt" : "2020-05-13T16:56:56Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "26a95afa-96a4-4fc6-9682-983731dc2e73",
        "parentId" : "6195d0fa-2122-4742-afed-8d28aaca212a",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "https://github.com/kubernetes/kubernetes/issues/91155",
        "createdAt" : "2020-05-15T19:10:08Z",
        "updatedAt" : "2020-05-15T19:10:16Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      }
    ],
    "commit" : "5aa3805a5ff11860437510ff9b4115c7a0afae9d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +780,784 @@\t\t// These calls are assumed to occur in this order for\n\t\t// each test run. NodeStageVolume and\n\t\t// NodePublishVolume should also be deterministic and\n\t\t// only get called once, but sometimes kubelet calls\n\t\t// both multiple times, which breaks this test"
  },
  {
    "id" : "12d9e2be-e7be-4478-bb54-402183cb48ee",
    "prId" : 89041,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89041#pullrequestreview-388279433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "parentId" : null,
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "This failed because \"the server rejected our request for an unknown reason (get pods csi-mockplugin-0)\" (https://prow.k8s.io/view/gcs/kubernetes-jenkins/pr-logs/pull/89041/pull-kubernetes-e2e-gce-storage-slow/1245284273221537792/).\r\n\r\nPerhaps a retry loop would help?",
        "createdAt" : "2020-04-01T11:07:42Z",
        "updatedAt" : "2020-04-06T13:07:04Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "e852f8c8-20f5-4592-a174-79e175f8cd38",
        "parentId" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "There is already a retry loop in the caller (via `wait.Poll`). Perhaps failures to retrieve log output should simply be treated here as \"no output\", i.e. return `nil, nil`?\r\n",
        "createdAt" : "2020-04-01T12:09:03Z",
        "updatedAt" : "2020-04-06T13:07:04Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "8327cb83-7b2f-44b4-973f-1fe4a9ecbb71",
        "parentId" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Maybe the pod is not Running yet. The new mock test calls `createPod()` and immediately after that it reads the pod logs. I added wait for PVC to get bound in between - the driver must be fully operational to provision a PV. ",
        "createdAt" : "2020-04-01T13:40:16Z",
        "updatedAt" : "2020-04-06T13:07:04Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "a85240f4-7635-4572-86f1-d4525e0afef1",
        "parentId" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Sorry about the failed build noise. Stupid typo...",
        "createdAt" : "2020-04-01T15:15:47Z",
        "updatedAt" : "2020-04-06T13:07:04Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "e5c83330-0ae7-496c-aaeb-8cc69a640e36",
        "parentId" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "I don't get it. The mock driver provisioned a PV, yet it gets error from API server:\r\n\r\n`\r\ncould not load CSI driver logs: the server rejected our request for an unknown reason (get pods csi-mockplugin-0)\r\n`",
        "createdAt" : "2020-04-01T15:40:33Z",
        "updatedAt" : "2020-04-06T13:07:04Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "a2a66250-317a-45e9-8fdf-bb036f0ca7ff",
        "parentId" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "> There is already a retry loop in the caller (via wait.Poll). Perhaps failures to retrieve log output should simply be treated here as \"no output\", i.e. return nil, nil?\r\n\r\nI've implemented that idea in https://github.com/kubernetes/kubernetes/pull/88114 (https://github.com/kubernetes/kubernetes/pull/88114/commits/3cf9ab134a3201880472c063e5b527b88f3b0ebb) and got all tests to pass. The log does indeed show that \"the server rejected our request for an unknown reason\" occurred, but tests succeeded after ignoring that error (https://storage.googleapis.com/kubernetes-jenkins/pr-logs/pull/88114/pull-kubernetes-e2e-kind/1245706940810530817/build-log.txt).\r\n",
        "createdAt" : "2020-04-02T18:26:26Z",
        "updatedAt" : "2020-04-06T13:07:04Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      },
      {
        "id" : "8e36ba73-7181-4dda-9cb6-9fa2ef7ed4ab",
        "parentId" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "I added your last commit from #88114, hoping it helps.\r\n\r\nI can't reproduce \"the server rejected our request for an unknown reason\" on any of my clusters.",
        "createdAt" : "2020-04-06T13:08:15Z",
        "updatedAt" : "2020-04-06T13:08:15Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "a613209d-4134-44a7-8283-b36a4c2f8134",
        "parentId" : "c2805342-b5c6-4003-a3cc-7a21f8ce1ca3",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "> I can't reproduce \"the server rejected our request for an unknown reason\" on any of my clusters.\r\n\r\nMe neither. I think saw it once while working on some other test, but as far as I remember, that then turned out to be because I was I was asking for logs after the pod had just been deleted. Perhaps here we have the inverse, asking for a very recently started pod? Just wondering.\r\n",
        "createdAt" : "2020-04-06T14:01:44Z",
        "updatedAt" : "2020-04-06T14:01:44Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      }
    ],
    "commit" : "981aae35dd91b9308e43dd63ad1e5440fa32933e",
    "line" : 294,
    "diffHunk" : "@@ -1,1 +943,947 @@\tlog, err := e2epod.GetPodLogs(cs, namespace, driverPodName, driverContainerName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not load CSI driver logs: %s\", err)\n\t}\n\tframework.Logf(\"CSI driver logs:\\n%s\", log)"
  },
  {
    "id" : "e20c050b-3669-4b65-8b73-9544d5c4abbc",
    "prId" : 88520,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88520#pullrequestreview-364104930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "151cef65-f98c-498a-aa3d-0d32b3b46bea",
        "parentId" : null,
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "From https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/conformance-tests.md#conformance-test-requirements:\r\n\r\n```\r\nanything that checks optional Condition fields, such as Reason or Message, as these may change over time (however it is reasonable to verify these fields exist or are non-empty) \r\n```",
        "createdAt" : "2020-02-25T12:33:10Z",
        "updatedAt" : "2020-02-25T12:33:10Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "34206a610a1bd40378595c3d746a2f13b267b301",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +570,574 @@\t\t\t// Conformance tests cannot rely on specific output of optional fields (e.g., Reason\n\t\t\t// and Message) because these fields are not suject to the deprecation policy.\n\t\t\tif c.Type == v1.PodScheduled && c.Status == v1.ConditionFalse && c.Reason != \"\" && c.Message != \"\" {\n\t\t\t\treturn true, nil\n\t\t\t}"
  },
  {
    "id" : "b94929ca-a987-4491-8f72-4e2efd2a7a23",
    "prId" : 86397,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86397#pullrequestreview-338918374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0aa45c7f-847e-4df0-b84c-85d7bf999cef",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "let's do this in a follow-up (and keep this PR focused on deflaking), but it is surprising to me that passing expectPodInfo makes this method return early without ensuring node publish and node unpublish are both called",
        "createdAt" : "2019-12-19T04:43:54Z",
        "updatedAt" : "2019-12-19T19:28:43Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "304b0020-c845-4359-b0c2-5d8a0de9d751",
        "parentId" : "0aa45c7f-847e-4df0-b84c-85d7bf999cef",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "yeah that seems like oversight.. I will fix that as a follow up.",
        "createdAt" : "2019-12-19T16:33:12Z",
        "updatedAt" : "2019-12-19T19:28:43Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "d00ea72d-6443-422f-95a7-f7fcdb464370",
        "parentId" : "0aa45c7f-847e-4df0-b84c-85d7bf999cef",
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "ping to see if this has been addressed",
        "createdAt" : "2020-01-06T21:25:38Z",
        "updatedAt" : "2020-01-06T21:25:38Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      },
      {
        "id" : "00c927be-b335-4532-a2dd-3e27eae96108",
        "parentId" : "0aa45c7f-847e-4df0-b84c-85d7bf999cef",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I just opened https://github.com/kubernetes/kubernetes/pull/86891 to fix it.",
        "createdAt" : "2020-01-06T21:52:44Z",
        "updatedAt" : "2020-01-06T21:52:45Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1fb0b57dfdc372ba0fbe053aeb2e3fd98263c54",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +791,795 @@\t\treturn nil\n\t}\n\tif foundAttributes.Len() != 0 {\n\t\treturn fmt.Errorf(\"some unexpected volume attributes were found: %+v\", foundAttributes.List())\n\t}"
  },
  {
    "id" : "9a087c75-2e72-48a0-8529-e8a2bd270c97",
    "prId" : 86396,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86396#pullrequestreview-334402079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "541e113b-17a3-4124-b0d8-fcdfe7557a30",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "the same logic exists in testsuites/volume_expand.go#191... does this need to be changed there as well?",
        "createdAt" : "2019-12-18T22:44:05Z",
        "updatedAt" : "2019-12-18T22:50:44Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "0f9781d7-1a3b-4465-aab6-fd0bb0473ac7",
        "parentId" : "541e113b-17a3-4124-b0d8-fcdfe7557a30",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I thought I changed that in - https://github.com/kubernetes/kubernetes/pull/85297 . line#191 points to something else now.",
        "createdAt" : "2019-12-18T22:57:06Z",
        "updatedAt" : "2019-12-18T22:57:07Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "b742a18b-d901-49a6-8aa5-a633d7c8f7c7",
        "parentId" : "541e113b-17a3-4124-b0d8-fcdfe7557a30",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "oops, I was on an old branch",
        "createdAt" : "2019-12-19T04:22:46Z",
        "updatedAt" : "2019-12-19T04:22:46Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "03df6320e594c3fd988cd48bff71df6affaa5ef0",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +504,508 @@\t\t\t\t\tcheckPVCSize()\n\t\t\t\t} else {\n\t\t\t\t\tginkgo.By(\"Checking for conditions on pvc\")\n\t\t\t\t\tnpvc, err := testsuites.WaitForPendingFSResizeCondition(pvc, m.cs)\n\t\t\t\t\tframework.ExpectNoError(err, \"While waiting for pvc to have fs resizing condition\")"
  },
  {
    "id" : "34e72b64-4e80-4e91-86ac-9d679f69718d",
    "prId" : 81960,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81960#pullrequestreview-280250379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68d29037-7966-4b2e-a5fc-1331b1c4a2ad",
        "parentId" : null,
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "I understand the reason why this is done this way: scraping pod logs for grpc calls to validate driver-side calls because there are no better of doing it. But, man it looks brittle and can be easily fail if log entry is missing.",
        "createdAt" : "2019-08-27T14:05:22Z",
        "updatedAt" : "2019-08-28T10:21:12Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "d0931632-4d0b-424d-84bb-891e3ec10bca",
        "parentId" : "68d29037-7966-4b2e-a5fc-1331b1c4a2ad",
        "authorId" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "body" : "Correct. The approach itself isn't new, though, and seems to have worked so far.\r\n",
        "createdAt" : "2019-08-27T14:44:47Z",
        "updatedAt" : "2019-08-28T10:21:12Z",
        "lastEditedBy" : "ba0b9c6e-ec4c-4d1b-832e-751e6109bf38",
        "tags" : [
        ]
      }
    ],
    "commit" : "5088b2ee2fbb99bdce42a19e59d9f172872ddf9a",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +767,771 @@\t\t\tcontinue\n\t\t}\n\t\tswitch call.Method {\n\t\tcase \"/csi.v1.Node/NodePublishVolume\":\n\t\t\tnumNodePublishVolume++"
  },
  {
    "id" : "5d194333-249e-4c94-8296-d412c31a0ecd",
    "prId" : 74993,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74993#pullrequestreview-211015715",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c55d4db-042c-4162-adeb-43b1caee51e3",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "why this change btw?",
        "createdAt" : "2019-03-06T02:03:40Z",
        "updatedAt" : "2019-03-06T04:25:15Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "7bd5ce61-81c3-4f8e-ba08-d9abe4ca856b",
        "parentId" : "1c55d4db-042c-4162-adeb-43b1caee51e3",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "i think that's my editor automatically running \"go fmt\". i'm on go version 1.11.1",
        "createdAt" : "2019-03-06T02:16:47Z",
        "updatedAt" : "2019-03-06T04:25:15Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "afbc8a5056642ac9fab0522743a599b7a2857e36",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +26,30 @@\n\tv1 \"k8s.io/api/core/v1\"\n\tstorage \"k8s.io/api/storage/v1\"\n\tstoragev1 \"k8s.io/api/storage/v1\"\n\t\"k8s.io/apimachinery/pkg/api/errors\""
  },
  {
    "id" : "341b2f2c-70b6-4a8b-9681-e9bf89f4fbfd",
    "prId" : 74863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74863#pullrequestreview-210932811",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cbfa6d6-25ff-467b-94df-a8595f84e7ee",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "also test cases for:\r\n* attach=off, nodeExpansion=on\r\n* driver doesn't support expansion at all",
        "createdAt" : "2019-03-04T23:20:10Z",
        "updatedAt" : "2019-03-08T14:23:30Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "25c6cb2f-dcf4-4ef7-bc2b-315def2f3e26",
        "parentId" : "9cbfa6d6-25ff-467b-94df-a8595f84e7ee",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "added more e2es that cover this.",
        "createdAt" : "2019-03-05T21:38:13Z",
        "updatedAt" : "2019-03-08T14:23:30Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "1bd9ed06d34efb328ce514551e1a23d708f1e526",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +397,401 @@\t\t\t},\n\t\t\t{\n\t\t\t\tname:                  \"should expand volume by restarting pod if attach=on, nodeExpansion=on\",\n\t\t\t\tnodeExpansionRequired: true,\n\t\t\t},"
  }
]