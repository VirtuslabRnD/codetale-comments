[
  {
    "id" : "55f2458d-b853-4682-8085-8988217c602b",
    "prId" : 93567,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93567#pullrequestreview-463169902",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ded19444-d29b-4ba7-8606-631d203efae6",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "I don't like part of storage functions implemented here and another part implemented in `ProviderInterface`: https://github.com/kubernetes/kubernetes/blob/e8a72dea370484103051bd5722713584fabcdd23/test/e2e/framework/provider.go#L100-L101\r\n\r\nLong term, attach/detachAndDeletePDs should move there. Can you please add at least TODO comment?\r\n",
        "createdAt" : "2020-08-06T12:27:31Z",
        "updatedAt" : "2020-08-07T12:23:43Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "aaf8e045-55aa-4477-9608-945419aa5275",
        "parentId" : "ded19444-d29b-4ba7-8606-631d203efae6",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "agreed. I added a TODO here. The thing is - if we do move these to providers then somebody will have to implement these for other cloudproviders too, but it can be done incrementally I think. ",
        "createdAt" : "2020-08-06T15:29:12Z",
        "updatedAt" : "2020-08-07T12:23:43Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "93d9c5ea-0891-4823-98bb-b97d40507fd6",
        "parentId" : "ded19444-d29b-4ba7-8606-631d203efae6",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "The only implemented provider apart from AWS and GCE is Azure, other providers are not implemented at all or use NullProvider, so it's not that complicated",
        "createdAt" : "2020-08-07T09:28:31Z",
        "updatedAt" : "2020-08-07T12:23:43Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "f91d448e1bb0894dc36e306374faa21b3a878128",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +546,550 @@\n// TODO: move attachPD to standard cloudprovider functions so as these tests can run on other cloudproviders too\nfunc attachPD(nodeName types.NodeName, pdName string) error {\n\tif framework.TestContext.Provider == \"gce\" || framework.TestContext.Provider == \"gke\" {\n\t\tgceCloud, err := gce.GetGCECloud()"
  },
  {
    "id" : "27d0ed93-16cf-4782-bca3-6f8ef13d107a",
    "prId" : 93567,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93567#pullrequestreview-462627519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7fc0707-b2e4-4d93-86a3-250f9528751e",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "`defer` should be at least one line above, so failed `attachPD` results in `detachAndDeletePDs`.",
        "createdAt" : "2020-08-06T12:58:07Z",
        "updatedAt" : "2020-08-07T12:23:43Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "d457b9df-a6f4-4bb0-b35f-f3e70b34c23e",
        "parentId" : "a7fc0707-b2e4-4d93-86a3-250f9528751e",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "fixed",
        "createdAt" : "2020-08-06T15:25:32Z",
        "updatedAt" : "2020-08-07T12:23:43Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "f91d448e1bb0894dc36e306374faa21b3a878128",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +463,467 @@\t\t// this should be safe to do because if attach fails then detach will be considered\n\t\t// successful and we will delete the volume.\n\t\tdefer func() {\n\t\t\tdetachAndDeletePDs(diskName, []types.NodeName{host0Name})\n\t\t}()"
  },
  {
    "id" : "61b8c5d9-00e4-46a3-bf21-c8e8d131bc3e",
    "prId" : 85973,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85973#pullrequestreview-327954305",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc4a1200-8d1d-4213-8efa-4d2d023a2025",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "Could we just block the code for disruptOp == deleteNode?",
        "createdAt" : "2019-12-06T01:02:08Z",
        "updatedAt" : "2019-12-06T01:02:09Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "d91dc86f-9ced-47fb-879e-0d26ce726618",
        "parentId" : "dc4a1200-8d1d-4213-8efa-4d2d023a2025",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I would rather not run the test instead of running the test but having it pass when it doesn't actually work",
        "createdAt" : "2019-12-06T01:08:57Z",
        "updatedAt" : "2019-12-06T01:08:57Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "c4de1a3a-3565-492d-ba70-480e1d013a4f",
        "parentId" : "dc4a1200-8d1d-4213-8efa-4d2d023a2025",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "I also mean to not run the test, just think this whole part of code is not needed. It is ok if just leaving it for future change. ",
        "createdAt" : "2019-12-06T01:29:00Z",
        "updatedAt" : "2019-12-06T01:30:08Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "4e58aad8-eb02-4615-928f-c9ef1e9ac20d",
        "parentId" : "dc4a1200-8d1d-4213-8efa-4d2d023a2025",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Yeah I do want to re-enable the test again in the future",
        "createdAt" : "2019-12-06T01:31:55Z",
        "updatedAt" : "2019-12-06T01:31:56Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e6d6d73e7b1917cfbba6216af1b4b63619d58ad",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +408,412 @@\t\t\t\t\toutput, err = gceCloud.ListInstanceNames(framework.TestContext.CloudConfig.ProjectID, framework.TestContext.CloudConfig.Zone)\n\t\t\t\t\tframework.ExpectNoError(err, fmt.Sprintf(\"Unable to get list of node instances err=%v output=%s\", err, output))\n\t\t\t\t\tframework.ExpectEqual(true, strings.Contains(string(output), string(host0Name)))\n\n\t\t\t\t} else if disruptOp == deleteNodeObj {"
  },
  {
    "id" : "db607d3f-3537-4002-aae1-7d17b32821bb",
    "prId" : 85770,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85770#pullrequestreview-326545144",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "466b1cbc-21dc-41f5-836c-cf44630bbbcf",
        "parentId" : null,
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "lets be consistent throughout with \"got before expect\"",
        "createdAt" : "2019-12-02T14:53:29Z",
        "updatedAt" : "2019-12-03T09:42:23Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "0942b27b-79a6-49af-9e5e-73602a10e8e9",
        "parentId" : "466b1cbc-21dc-41f5-836c-cf44630bbbcf",
        "authorId" : "cc740aab-0edd-4978-b80c-6ca3543a6188",
        "body" : "@davidz627 Thanks.\r\ncurrent i don't understand what you mean? \r\nyou mean gomega.Expect(false, strings.Contains(string(output), string(host0Name))) != framework.ExpectEqual(false, strings.Contains(string(output), string(host0Name)))?",
        "createdAt" : "2019-12-03T01:31:55Z",
        "updatedAt" : "2019-12-03T09:42:23Z",
        "lastEditedBy" : "cc740aab-0edd-4978-b80c-6ca3543a6188",
        "tags" : [
        ]
      },
      {
        "id" : "8f3a52f9-bcce-449e-9e04-7c6ed8b64490",
        "parentId" : "466b1cbc-21dc-41f5-836c-cf44630bbbcf",
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "some of the parameters put the \"expected value\" first, some put the \"got value\" first. For example this one is `false, strings.Contains(string(output), string(host0Name))` but there are other invocations with the ordering: `strings.Contains(string(output), string(host0Name)), false`. We should be consistent throughout on the ordering (which value we are getting from a call and which value is our expected value\"",
        "createdAt" : "2019-12-03T16:36:16Z",
        "updatedAt" : "2019-12-03T16:36:16Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "81544cff-7099-497c-97f3-7103efa89b39",
        "parentId" : "466b1cbc-21dc-41f5-836c-cf44630bbbcf",
        "authorId" : "cc740aab-0edd-4978-b80c-6ca3543a6188",
        "body" : "@davidz627 I get the point,Thanks\r\nyou mean:\r\ngomega.Expect(false, strings.Contains(string(output), string(host0Name)))\r\nframework.ExpectEqual(false, strings.Contains(string(output), string(host0Name)))\r\n==>\r\nframework.ExpectEqual(strings.Contains(string(output), string(host0Name)),false)\r\n/cc @oomichi \r\ni guess both are ok,but prefer framework.ExpectEqual(strings.Contains(string(output), string(host0Name)),false).What do you think?",
        "createdAt" : "2019-12-04T00:55:25Z",
        "updatedAt" : "2019-12-04T00:55:25Z",
        "lastEditedBy" : "cc740aab-0edd-4978-b80c-6ca3543a6188",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eda99793de11f3acd2716a939e15a6f5fde8ff6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +406,410 @@\t\t\t\t\toutput, err = gceCloud.ListInstanceNames(framework.TestContext.CloudConfig.ProjectID, framework.TestContext.CloudConfig.Zone)\n\t\t\t\t\tframework.ExpectNoError(err, fmt.Sprintf(\"Unable to get list of node instances err=%v output=%s\", err, output))\n\t\t\t\t\tframework.ExpectEqual(false, strings.Contains(string(output), string(host0Name)))\n\n\t\t\t\t} else if disruptOp == deleteNodeObj {"
  },
  {
    "id" : "948a8a70-577c-4dcb-80e6-416329cd7f33",
    "prId" : 84491,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84491#pullrequestreview-309375324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "429c34ec-2c98-4c62-9bea-2e6dd2ffcf34",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Since we're doing both sync + wait for pod to disappear, then we shouldn't need to retry the read?",
        "createdAt" : "2019-10-30T01:25:23Z",
        "updatedAt" : "2019-10-30T22:41:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "081b283c-8a97-4234-9e33-6ce32e345a6e",
        "parentId" : "429c34ec-2c98-4c62-9bea-2e6dd2ffcf34",
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "yeah I was leaning on the cautious side but I think we should remove this.",
        "createdAt" : "2019-10-30T17:09:05Z",
        "updatedAt" : "2019-10-30T22:41:19Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3d2432e01cd4da55a41a411ae5dd7d36f28ad4e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +190,194 @@\t\t\t\t\tframework.ExpectNoError(podClient.Delete(host0Pod.Name, podDelOpt), \"Failed to delete host0Pod\")\n\t\t\t\t\tframework.Logf(\"deleted host0Pod %q\", host0Pod.Name)\n\t\t\t\t\te2epod.WaitForPodToDisappear(cs, host0Pod.Namespace, host0Pod.Name, labels.Everything(), framework.Poll, framework.PodDeleteTimeout)\n\t\t\t\t\tframework.Logf(\"deleted host0Pod %q disappeared\", host0Pod.Name)\n\t\t\t\t}"
  },
  {
    "id" : "5a31069c-5fcc-4c90-b735-44d90e722e6e",
    "prId" : 84491,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84491#pullrequestreview-309542311",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6bd3fdc-d5cf-4f8a-b1eb-79c7dae864bd",
        "parentId" : null,
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "actually what if there is a temporary failure in `kubectl exec` - network issue or something\r\n/cc @msau42 ",
        "createdAt" : "2019-10-30T17:10:35Z",
        "updatedAt" : "2019-10-30T22:41:19Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "963fdcb5-ed45-4e30-9917-281f523382b2",
        "parentId" : "c6bd3fdc-d5cf-4f8a-b1eb-79c7dae864bd",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "It internally retries: https://github.com/kubernetes/kubernetes/blob/d6a3518ea2ad92fb48e1de6c10f1fbd7e61736c3/test/e2e/framework/framework.go#L496",
        "createdAt" : "2019-10-30T21:46:07Z",
        "updatedAt" : "2019-10-30T22:41:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3d2432e01cd4da55a41a411ae5dd7d36f28ad4e",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +457,461 @@\tfor filePath, expectedContents := range fileAndContentToVerify {\n\t\t// No retry loop as there should not be temporal based failures\n\t\tv, err := f.ReadFileViaContainer(podName, containerName, filePath)\n\t\tframework.ExpectNoError(err, \"Error reading file %s via container %s\", filePath, containerName)\n\t\tframework.Logf(\"Read file %q with content: %v\", filePath, v)"
  },
  {
    "id" : "a1364b35-6d10-431a-b24a-33a4806de87a",
    "prId" : 54207,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54207#pullrequestreview-70653999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "789d0d2a-261b-488c-b823-cf780e49bc5e",
        "parentId" : null,
        "authorId" : "1530c754-7850-4e09-a79b-a6555e28f729",
        "body" : "Just a note, since we talked about this earlier.  Using `framework.ExpectNoError` inside a defer is safe.  Typically, if 2 `Expect()` calls resolve false, only the 1st one is reported in the log.  The `framework.ExpectNoError()` wrapper calls `fmt.Logf()` if `err != nil` though, so it's not an issue.",
        "createdAt" : "2017-10-19T19:25:12Z",
        "updatedAt" : "2017-10-19T23:27:28Z",
        "lastEditedBy" : "1530c754-7850-4e09-a79b-a6555e28f729",
        "tags" : [
        ]
      }
    ],
    "commit" : "6520852020f9fdab05ecf7841e3e7c4348c3549d",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +355,359 @@\t\t\t\t\t\t\tBy(\"defer: re-create host0 node object\")\n\t\t\t\t\t\t\t_, err := nodeClient.Create(targetNode)\n\t\t\t\t\t\t\tframework.ExpectNoError(err, fmt.Sprintf(\"defer: Unable to re-create the deleted node object %q\", targetNode.Name))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tBy(\"defer: verify the number of ready nodes\")"
  },
  {
    "id" : "e13d03b3-0450-460a-a133-01ca04daa3cf",
    "prId" : 49334,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49334#pullrequestreview-52828527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe914bc9-70ba-4301-98f5-7909b249a022",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I wonder in general, would it be better to use the gcloud programatic APIs instead of the gcloud command line?  I know for some of the PD tests, we use the programming apis to create disks.",
        "createdAt" : "2017-07-20T22:59:51Z",
        "updatedAt" : "2017-07-20T22:59:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "fa5d90bc-7e01-4631-ac43-dd633081eaea",
        "parentId" : "fe914bc9-70ba-4301-98f5-7909b249a022",
        "authorId" : "3e4fca9c-38d2-48b7-8f51-aa84ce73e592",
        "body" : "We're using `gcloud` for a lot of networking tests too. Anyone know why we chose to run this way? What do we gain by this?",
        "createdAt" : "2017-07-21T04:14:58Z",
        "updatedAt" : "2017-07-21T04:15:08Z",
        "lastEditedBy" : "3e4fca9c-38d2-48b7-8f51-aa84ce73e592",
        "tags" : [
        ]
      },
      {
        "id" : "4d222ce6-bc4c-4d92-959d-8e82c27d82c0",
        "parentId" : "fe914bc9-70ba-4301-98f5-7909b249a022",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "I agree we should switch all gcloud CLI calls to API. When the E2E tests were first written the client API code wasn't integrated/initialized so everyone just used direct CLI gcloud commands. I added the integration (https://github.com/kubernetes/kubernetes/pull/17747), but never got around to switching everything over. ",
        "createdAt" : "2017-07-28T01:53:00Z",
        "updatedAt" : "2017-07-28T01:53:00Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "c861d088ac853a47c848eb1ae5603b0359229234",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +450,454 @@\t\tframework.ExpectNoError(waitForPDInVolumesInUse(nodeClient, diskName, host0Name, nodeStatusTimeout, true /* should exist*/))\n\n\t\toutput, err := exec.Command(\"gcloud\", \"compute\", \"instances\", \"list\", \"--project=\"+framework.TestContext.CloudConfig.ProjectID).CombinedOutput()\n\t\tframework.ExpectNoError(err, fmt.Sprintf(\"Unable to get list of node instances err=%v output=%s\", err, output))\n\t\tExpect(true, strings.Contains(string(output), string(host0Name)))"
  },
  {
    "id" : "40d3f8e3-6c85-4d74-b792-2772cc691626",
    "prId" : 48118,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48118#pullrequestreview-46690724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38496e91-7d10-410a-8660-e8697368a530",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Does this also wait for all nodes to be ready?",
        "createdAt" : "2017-06-27T02:53:44Z",
        "updatedAt" : "2017-06-27T02:53:47Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "4277cd6a-5234-431b-accf-a630a42d9c7a",
        "parentId" : "38496e91-7d10-410a-8660-e8697368a530",
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "Yes it does, but \"all nodes\" only include nodes that are known to kubelet, which may not include the deleted node. The previous line checks for that case.",
        "createdAt" : "2017-06-27T21:48:20Z",
        "updatedAt" : "2017-06-27T21:48:20Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      }
    ],
    "commit" : "78d111b75f08f25cf428c605aca40616ce97f626",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +430,434 @@\t\t\tdetachAndDeletePDs(diskName, []types.NodeName{host0Name})\n\t\t\tframework.WaitForNodeToBeReady(f.ClientSet, string(host0Name), nodeStatusTimeout)\n\t\t\tframework.WaitForAllNodesSchedulable(f.ClientSet, nodeStatusTimeout)\n\t\t\tnodes = framework.GetReadySchedulableNodesOrDie(f.ClientSet)\n\t\t\tExpect(len(nodes.Items)).To(Equal(initialGroupSize), \"Requires node count to return to initial group size.\")"
  },
  {
    "id" : "db62a1fb-3857-4f5e-b2a7-5a6317dab599",
    "prId" : 46746,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46746#pullrequestreview-41636329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0ebc504-b91e-4e6c-9827-c4a11746cc8a",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Should this go in the AfterEach() for the test suite instead of the specific test case, so that we make sure all the tests will properly wait?",
        "createdAt" : "2017-06-01T17:09:24Z",
        "updatedAt" : "2017-06-01T17:10:00Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "78091e13-e9ed-46c8-9efc-4442cb38edd0",
        "parentId" : "c0ebc504-b91e-4e6c-9827-c4a11746cc8a",
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "Probably a good idea. This is one of the only two tests that require nodes to become ready again, and given that I didn't spend time understanding the implications for the other tests yet, I decided to keep the scope small.\r\n\r\nAnother possibility is to put an \"all nodes ready\" check in the general AfterEach inside framework.go, possibly reducing flakes from other tests as well, but again it'd be harder to understand the scope of this. @kubernetes/sig-testing",
        "createdAt" : "2017-06-01T17:58:39Z",
        "updatedAt" : "2017-06-01T21:01:35Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      },
      {
        "id" : "cc0e9c7d-a67d-4efa-bc13-1b4b20b62eac",
        "parentId" : "c0ebc504-b91e-4e6c-9827-c4a11746cc8a",
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "Leaving this inside \"defer\" since it waits for a specific deleted node in the test to be ready.",
        "createdAt" : "2017-06-01T21:02:46Z",
        "updatedAt" : "2017-06-01T21:02:46Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2cba391d8129dc1b389876605f1f31653c07ed",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +429,433 @@\t\t\tpodClient.Delete(host0Pod.Name, metav1.NewDeleteOptions(0))\n\t\t\tdetachAndDeletePDs(diskName, []types.NodeName{host0Name})\n\t\t\tframework.WaitForNodeToBeReady(f.ClientSet, string(host0Name), nodeStatusTimeout)\n\t\t\tExpect(len(nodes.Items)).To(Equal(initialGroupSize))\n\t\t}()"
  },
  {
    "id" : "64907489-ab00-4077-b0a8-af1ce7b15047",
    "prId" : 46746,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46746#pullrequestreview-41576683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71cef650-1f68-4ce2-9a63-ce467fc22d37",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Do you need to refresh `nodes` again? Otherwise, this will pick up the old `nodes` array?",
        "createdAt" : "2017-06-01T17:09:56Z",
        "updatedAt" : "2017-06-01T17:10:11Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2cba391d8129dc1b389876605f1f31653c07ed",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +430,434 @@\t\t\tdetachAndDeletePDs(diskName, []types.NodeName{host0Name})\n\t\t\tframework.WaitForNodeToBeReady(f.ClientSet, string(host0Name), nodeStatusTimeout)\n\t\t\tExpect(len(nodes.Items)).To(Equal(initialGroupSize))\n\t\t}()\n"
  }
]