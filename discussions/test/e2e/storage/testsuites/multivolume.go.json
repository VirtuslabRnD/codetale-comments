[
  {
    "id" : "5bf7a3fe-75fc-4e56-b385-837eabea2675",
    "prId" : 102775,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102775#pullrequestreview-681142946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "493b71e7-9a19-49c7-a5c5-14a9624adc69",
        "parentId" : null,
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "created https://github.com/kubernetes/kubernetes/issues/102787 for it",
        "createdAt" : "2021-06-10T18:53:10Z",
        "updatedAt" : "2021-06-10T18:53:10Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      }
    ],
    "commit" : "29aa4c0ee81aafbd8a5a1a96163eb97b1f27bc8b",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +405,409 @@\t\t}\n\t\tif pattern.VolMode == v1.PersistentVolumeBlock {\n\t\t\t// TODO: refactor preparePVCDataSourceForProvisioning() below to use\n\t\t\t// utils.CheckWriteToPath / utils.CheckReadFromPath and remove\n\t\t\t// redundant InjectContent(). This will enable block volume tests."
  },
  {
    "id" : "aa242c74-c8b3-48e9-892c-f87d76430f0c",
    "prId" : 102538,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102538#pullrequestreview-677636570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57a643a2-2618-4876-9b89-9cd3a14c6ce0",
        "parentId" : null,
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "The following lines seem to just check the number of pods and pvcs, not deleting anything.",
        "createdAt" : "2021-06-04T00:10:59Z",
        "updatedAt" : "2021-06-04T00:25:44Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      },
      {
        "id" : "fb69f177-5150-403e-b011-d403781b4edc",
        "parentId" : "57a643a2-2618-4876-9b89-9cd3a14c6ce0",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Moved the comment few lines below.",
        "createdAt" : "2021-06-07T16:27:47Z",
        "updatedAt" : "2021-06-07T16:27:48Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "28511e82ad96cd0fdc54a8dfcf99b8a9c16f6ca9",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +730,734 @@\t}\n\n\t// Delete the last pod and remove from slice of pods\n\tif len(pods) < len(pvcs) {\n\t\tframework.Failf(\"Number of pods shouldn't be less than %d, but got %d\", len(pvcs), len(pods))"
  },
  {
    "id" : "a076e2af-bbdf-4899-aa91-ee3a4fba8843",
    "prId" : 102538,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102538#pullrequestreview-680766635",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aacb8ae0-15fb-4db2-86cf-f3f47e7acad2",
        "parentId" : null,
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "I see that `requiresSameNode` is never false in the tests, what happens when anti-affinity is set after the pod is created with `CreateSecPodWithNodeSelection`? is it rescheduled again?",
        "createdAt" : "2021-06-08T18:15:03Z",
        "updatedAt" : "2021-06-08T18:15:24Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      },
      {
        "id" : "0e221d8f-21e3-4f64-ad55-956d78272873",
        "parentId" : "aacb8ae0-15fb-4db2-86cf-f3f47e7acad2",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Removed `requiresSameNode` and `readOnly` param of `TestConcurrentAccessToRelatedVolumes` in https://github.com/kubernetes/kubernetes/pull/102775",
        "createdAt" : "2021-06-10T13:14:32Z",
        "updatedAt" : "2021-06-10T13:14:32Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "28511e82ad96cd0fdc54a8dfcf99b8a9c16f6ca9",
    "line" : 170,
    "diffHunk" : "@@ -1,1 +723,727 @@\n\t\t// Set affinity depending on requiresSameNode\n\t\tif requiresSameNode {\n\t\t\te2epod.SetAffinity(&node, actualNodeName)\n\t\t} else {"
  },
  {
    "id" : "8e3943b1-f277-4158-aa82-3b24aed020e7",
    "prId" : 102538,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102538#pullrequestreview-680767747",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7be7d4a-8026-403c-89a7-c08c2e706de1",
        "parentId" : null,
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "this looks like it's always going to be true because the for loop above is adding a pod for each pvc (if there's an error creating the pod then `framework.ExpectNoError(err)` exits earlier)",
        "createdAt" : "2021-06-08T18:17:53Z",
        "updatedAt" : "2021-06-08T18:43:10Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      },
      {
        "id" : "ef3292fd-4b11-4856-a216-6842545c8cda",
        "parentId" : "c7be7d4a-8026-403c-89a7-c08c2e706de1",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Removed\r\n\r\n",
        "createdAt" : "2021-06-10T13:15:28Z",
        "updatedAt" : "2021-06-10T13:15:28Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "28511e82ad96cd0fdc54a8dfcf99b8a9c16f6ca9",
    "line" : 178,
    "diffHunk" : "@@ -1,1 +731,735 @@\n\t// Delete the last pod and remove from slice of pods\n\tif len(pods) < len(pvcs) {\n\t\tframework.Failf(\"Number of pods shouldn't be less than %d, but got %d\", len(pvcs), len(pods))\n\t}"
  },
  {
    "id" : "05a70bfd-19af-46e7-98d3-bd124f5b262b",
    "prId" : 102538,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102538#pullrequestreview-680771699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a931b2e-bcef-44ce-8239-5d6832e39439",
        "parentId" : null,
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "would it be possible to add a check here to make sure that both snapshots have the same contents before calling `TestConcurrentAccessToRelatedVolumes`? I'm not sure if that's doable without spinning up some pods though",
        "createdAt" : "2021-06-08T18:22:18Z",
        "updatedAt" : "2021-06-08T18:22:18Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      },
      {
        "id" : "311af478-2279-4445-8bb8-db8d2c85f410",
        "parentId" : "3a931b2e-bcef-44ce-8239-5d6832e39439",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "It is, however, volume prepared using `preparePVCDataSourceForProvisioning()` / `prepareSnapshotDataSourceForProvisioning` is not \"compatible\" with volume checks used in `multivolume.go` (`utils.CheckReadFromPath` + `utils.CheckWriteToPath`). It would be cool to merge them, but I gave up after an hour, it requires bigger refactoring.\r\n\r\nI added code that checks if the volumes have the right content, however, it works only for filesystem volumes. And it does not check that the volumes are \"independent\", i.e. write to one does not affect the other one - that would be a nice test too, but not with this framework.",
        "createdAt" : "2021-06-10T13:18:47Z",
        "updatedAt" : "2021-06-10T13:18:47Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "28511e82ad96cd0fdc54a8dfcf99b8a9c16f6ca9",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +382,386 @@\n\t\t// Test access to both volumes on the same node.\n\t\tTestConcurrentAccessToRelatedVolumes(l.config.Framework, l.cs, l.ns.Name,\n\t\t\tl.config.ClientNodeSelection, pvcs, true /* sameNode */, false /* readOnly */)\n\t})"
  },
  {
    "id" : "fc1518ca-49fc-4249-b10b-b0e62f68227d",
    "prId" : 102538,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102538#pullrequestreview-678880754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "994f6bd3-d7f5-4fa1-aca4-95261cb448a6",
        "parentId" : null,
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "would it be possible to also do a match of the contents of the volumes too?",
        "createdAt" : "2021-06-08T18:37:35Z",
        "updatedAt" : "2021-06-08T18:37:35Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      }
    ],
    "commit" : "28511e82ad96cd0fdc54a8dfcf99b8a9c16f6ca9",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +695,699 @@// TestConcurrentAccessToRelatedVolumes tests access to multiple volumes from multiple pods.\n// Each provided PVC is used by a single pod. The test ensures that volumes created from\n// another volume (=clone) or volume snapshot can be used together with the original volume.\nfunc TestConcurrentAccessToRelatedVolumes(f *framework.Framework, cs clientset.Interface, ns string,\n\tnode e2epod.NodeSelection, pvcs []*v1.PersistentVolumeClaim, requiresSameNode bool,"
  },
  {
    "id" : "a4a42296-4181-40be-b79c-098feec72077",
    "prId" : 102538,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102538#pullrequestreview-678883703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "869e7b3a-6fed-4c89-949e-883162cebc01",
        "parentId" : null,
        "authorId" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "body" : "I was wondering if this is concurrent, it looks like each pod is created serially but I guess that the call to create the pod returns immediately and the volumes are created in the background, I think that at some point we should block and check the volume contents of both pods when they're ready",
        "createdAt" : "2021-06-08T18:41:11Z",
        "updatedAt" : "2021-06-08T18:41:11Z",
        "lastEditedBy" : "87fae8a2-4751-4356-ba8a-ce265708b853",
        "tags" : [
        ]
      }
    ],
    "commit" : "28511e82ad96cd0fdc54a8dfcf99b8a9c16f6ca9",
    "line" : 140,
    "diffHunk" : "@@ -1,1 +693,697 @@}\n\n// TestConcurrentAccessToRelatedVolumes tests access to multiple volumes from multiple pods.\n// Each provided PVC is used by a single pod. The test ensures that volumes created from\n// another volume (=clone) or volume snapshot can be used together with the original volume."
  },
  {
    "id" : "d8720d02-660e-455f-9118-7e9642920829",
    "prId" : 96644,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96644#pullrequestreview-532812749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbe25035-96b1-4655-802f-084269393167",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Is this failing just for block volmode or also filesystem volmode?",
        "createdAt" : "2020-11-17T18:01:00Z",
        "updatedAt" : "2020-11-17T18:03:03Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "c228c44c-8ab1-476e-a935-73e082bdd5ca",
        "parentId" : "bbe25035-96b1-4655-802f-084269393167",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "block test will not be triggered because pd driver for windows does not support block.\r\nOnly one negative test for blockvolume will be triggered since it is trying to test if block is not supported.",
        "createdAt" : "2020-11-17T18:44:10Z",
        "updatedAt" : "2020-11-17T18:44:10Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "1a8fa224-5d3c-43dc-9592-ffa2e1e43b05",
        "parentId" : "bbe25035-96b1-4655-802f-084269393167",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "This test case runs for both filesystem volmode and block volmode. By adding the Linux tag here, then the filesystem volmode test case will no longer run on Windows. Is that what we want?",
        "createdAt" : "2020-11-17T19:16:47Z",
        "updatedAt" : "2020-11-17T19:16:48Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "cb845c65-7c3a-4430-9fa6-6c7bccfabe4b",
        "parentId" : "bbe25035-96b1-4655-802f-084269393167",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "yes, the command used here is not available for windows no matter vol type. I need to work on finding similar commands for windows before enabling them.",
        "createdAt" : "2020-11-17T21:07:14Z",
        "updatedAt" : "2020-11-17T21:07:15Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "079a4ea30c5bffdc59317904771f03a4a3f91e6e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +128,132 @@\t//          /    \\      <- same volume mode                   /    \\\n\t//   [volume1]  [volume2]                              [volume1]  [volume2]\n\tginkgo.It(\"should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]\", func() {\n\t\t// Currently, multiple volumes are not generally available for pre-provisoined volume,\n\t\t// because containerized storage servers, such as iSCSI and rbd, are just returning"
  },
  {
    "id" : "86c7eb3d-3a2d-4e1d-967c-073b8ed36ed7",
    "prId" : 96644,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96644#pullrequestreview-532680004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64bff4cf-2171-4a44-a90d-1fe7c256fd9e",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "In general, we are inconsistent with how we skip tests based on OS. Sometimes, we check node OS, and sometimes we use [LinuxOnly] tag. Can you clarify when do we use which method?",
        "createdAt" : "2020-11-17T18:02:54Z",
        "updatedAt" : "2020-11-17T18:03:03Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "f278867b-98f9-4ff9-b1ac-fb19801eb779",
        "parentId" : "64bff4cf-2171-4a44-a90d-1fe7c256fd9e",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "If the tests can only work on linux, we mark it as [linuxonly]\r\n\r\nfor some tests, the test can work on Windows but with certain fstypes (not all fstypes), there is a check here https://github.com/kubernetes/kubernetes/blob/02528ce91af8398999eb9a99b7abff314e4d8b83/test/e2e/storage/testsuites/base.go#L180 ",
        "createdAt" : "2020-11-17T18:42:22Z",
        "updatedAt" : "2020-11-17T18:42:22Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "079a4ea30c5bffdc59317904771f03a4a3f91e6e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +158,162 @@\t//          /    \\      <- same volume mode                   /    \\\n\t//   [volume1]  [volume2]                              [volume1]  [volume2]\n\tginkgo.It(\"should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]\", func() {\n\t\t// Currently, multiple volumes are not generally available for pre-provisoined volume,\n\t\t// because containerized storage servers, such as iSCSI and rbd, are just returning"
  },
  {
    "id" : "ce4c4001-3072-4454-8331-93fda231eec0",
    "prId" : 94881,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94881#pullrequestreview-493547721",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "parentId" : null,
        "authorId" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "body" : "I might not completely understand the issue, but can it be resolved by just adding \"sync command\" here and the similar code in recheck?",
        "createdAt" : "2020-09-18T16:49:54Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "tags" : [
        ]
      },
      {
        "id" : "3d7c4d02-8abf-418a-8e15-fd0d0b0be574",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "body" : "Maybe a real `sync` is sufficient, but the command that is part of BusyBox is not (no error, not synced). I tried it with a CentOS container, and there permissions prevented it from working.",
        "createdAt" : "2020-09-21T06:20:47Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "tags" : [
        ]
      },
      {
        "id" : "7ecd18df-2635-40d2-b57a-2bd0fb987a04",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "`sync` flushes pending writes to the disc, AFAICT it won't update the the page cache (at least on Linux).\r\n\r\nIdeally we'd just drop the page cache in the \"reading\" node by doing `echo 3 > /proc/sys/vm/drop_caches `) before calling `dd`, but I don't think that's possible inside a container.",
        "createdAt" : "2020-09-21T08:53:17Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "4e2dfdab-b983-4fc2-b92d-e83dd8069eee",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "body" : "Thank you for your explanation and testing.\r\n\r\nHow about `conv=fsync` option of dd command?\r\n\r\nThe dd command in busybox seems to support it and https://unix.stackexchange.com/questions/473854/block-device-cache-v-s-a-filesystem and https://stackoverflow.com/questions/49489798/is-running-sync-necessary-after-writing-a-disk-image seem to say that it will work well.",
        "createdAt" : "2020-09-21T14:37:33Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "tags" : [
        ]
      },
      {
        "id" : "d837162a-c133-4467-9d93-120e20b461d7",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "body" : "I tried that as well. Unfortunately it does not help with the reading part where some readahead seems to happen (detecting partition table?). Because reading needs the whole sector for direct-io, the same amount of bytes need to be written (to make the checksum match).",
        "createdAt" : "2020-09-21T15:13:07Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "tags" : [
        ]
      },
      {
        "id" : "87e641ae-eb95-424e-8e2a-c70be0a144c1",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "body" : "Now I understand that we need a way to avoid reader from reading from the cache, not just to guarantee the writer to write to the actual device before reader starts to read.\r\nAnd it seems that the only way to achieve it in container is to use direct I/O, so I agree to use direct I/O for this purpose.",
        "createdAt" : "2020-09-21T15:47:23Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "tags" : [
        ]
      },
      {
        "id" : "1c0f20cf-946a-4b71-b6c7-f74f226f52f7",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "body" : "I checked your codes again and I'm confused again.\r\n\r\nReader side's requirement for avoid reading from cache needs to be resolved by using `iflag=nocache` in reading, however does writer side really needs direct I/O?\r\n\r\n512 byte restriction seems to come from using `iflag=direct`, so I'm thinking about avoiding it by using other options in writing, like `conv=fsync` and/or `oflag=nocache` if we can really sync by these options.\r\n(Using debian image would be needed to use `iflag=nocache`, but avoiding direct I/O might still be possible. And it will potentially remove the TODOs in your codes.)\r\n\r\nDoes it make sense?\r\n\r\nIn addition, current codes use `iflag=direct` in reading and `oflag=nocache` in writing. Shouldn't it be `iflag=nocache` in reading and `oflag=direct` in writing, to achieve the original intention?",
        "createdAt" : "2020-09-21T16:47:07Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "tags" : [
        ]
      },
      {
        "id" : "412115e5-85aa-48e0-9d30-cdc997012fe4",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "body" : "> Reader side's requirement for avoid reading from cache needs to be resolved by using `iflag=nocache` in reading, however does writer side really needs direct I/O?\r\n\r\nUnfortunately, there is no `iflag=nocache` option, the `nocache` is only for the `oflag` used by the writer.\r\n\r\nYou are correct, the writer does not require direct I/O. That side only needs to flush/sync the data after writing. The `oflag=nocache` can be replaced by `oflag=fdatasync` or similar. However, I do expect that applications using RWX block-mode PVCs use direct I/O on both sides when they need the data available on all participating nodes (like QEMU live-migration).\r\n\r\n> 512 byte restriction seems to come from using `iflag=direct`, so I'm thinking about avoiding it by using other options in writing, like `conv=fsync` and/or `oflag=nocache` if we can really sync by these options.\r\n> (Using debian image would be needed to use `iflag=nocache`, but avoiding direct I/O might still be possible. And it will potentially remove the TODOs in your codes.)\r\n> \r\n> Does it make sense?\r\n\r\nIt really does, and you understood the issue perfectly! However, the missing support for `iflag=nocache` is preventing this from being an option. So the `iflag=direct` is the only alternative that is left. Direct I/O is the only option for readers to open a file/device without caching, see `man 2 open` and the `O_DIRECT` description, all the sync-flags for `open()` are related to writing only.\r\n",
        "createdAt" : "2020-09-22T06:46:55Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "tags" : [
        ]
      },
      {
        "id" : "333c4adf-5d45-415c-9087-895874712cfb",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "> Direct I/O is the only option for readers to open a file/device without caching, see man 2 open and the O_DIRECT description, all the sync-flags for open() are related to writing only.\r\n\r\nThis can be confirmed by running this little program in pods allocated in different nodes (and with a shared RWX volume): https://gist.github.com/bertinatto/6783734c636907e699d999b7a99bd622\r\n\r\nWith the O_DIRECT flag it prints the same output regardless the node where the container runs. Without the flag, the results are different for each node.",
        "createdAt" : "2020-09-22T07:29:58Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "e471db3f-0dff-4422-9538-580572603bf5",
        "parentId" : "3de105e7-b604-4654-81ab-998b6172fbb5",
        "authorId" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "body" : "@nixpanic @bertinatto\r\n\r\nThank you for your explanation.\r\n\r\nI understand that even though `iflag=nocache` can be passed to dd command, but won't be sufficient to skip cache.\r\nSo, I agree to apply this combination of options.",
        "createdAt" : "2020-09-22T14:52:01Z",
        "updatedAt" : "2020-09-25T12:38:50Z",
        "lastEditedBy" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2bf2ab76ecfb43519e64dea2eef6051a5abaf31",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +540,544 @@\t\tginkgo.By(fmt.Sprintf(\"Checking if write to the volume in pod%d works properly\", index))\n\t\tutils.CheckWriteToPath(f, pod, *pvc.Spec.VolumeMode, directIO, path, byteLen, seed)\n\n\t\tginkgo.By(fmt.Sprintf(\"Checking if read from the volume in pod%d works properly\", index))\n\t\tutils.CheckReadFromPath(f, pod, *pvc.Spec.VolumeMode, directIO, path, byteLen, seed)"
  },
  {
    "id" : "e3bfc89a-7360-4d9a-bcbc-a3f2efa98323",
    "prId" : 94881,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94881#pullrequestreview-496655792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f441056-5b64-426b-bc7e-56465595eb48",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "What happens if you make byteLen 512 in all cases?",
        "createdAt" : "2020-09-25T18:21:54Z",
        "updatedAt" : "2020-09-25T18:23:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2bf2ab76ecfb43519e64dea2eef6051a5abaf31",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +513,517 @@\tif *pvc.Spec.VolumeMode == v1.PersistentVolumeBlock {\n\t\t// byteLen should be the size of a sector to enable direct I/O\n\t\tbyteLen = 512\n\t\tdirectIO = true\n\t}"
  },
  {
    "id" : "23e5f62d-b590-4d88-8587-37a0b02630d6",
    "prId" : 94881,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94881#pullrequestreview-498287844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8eb1291-1029-45b1-b5a1-ead908945068",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can you explain why in the comment?",
        "createdAt" : "2020-09-25T18:22:24Z",
        "updatedAt" : "2020-09-25T18:23:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "5687e0a8-62b1-4fca-99f0-890e84b19344",
        "parentId" : "c8eb1291-1029-45b1-b5a1-ead908945068",
        "authorId" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "body" : "When attaching a block PV to a node, the Linux kernel tries to detect a partition table (reading the first sector(s) of the device). Because of read caching, writes to the device from an other node will not show up until the caches are dropped. Dropping caches is a privileged operation, and can not be done from within a (standard) pod. The alternative is to read with caches disabled, which is what direct I/O does.",
        "createdAt" : "2020-09-29T09:36:04Z",
        "updatedAt" : "2020-09-29T09:36:04Z",
        "lastEditedBy" : "1f54c5d1-c5cf-41c1-b046-4894d8619116",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2bf2ab76ecfb43519e64dea2eef6051a5abaf31",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +510,514 @@\tbyteLen := 64\n\tdirectIO := false\n\t// direct IO is needed for Block-mode PVs\n\tif *pvc.Spec.VolumeMode == v1.PersistentVolumeBlock {\n\t\t// byteLen should be the size of a sector to enable direct I/O"
  },
  {
    "id" : "f3c924f1-f7d1-4625-80d0-9a78368e3953",
    "prId" : 94881,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94881#pullrequestreview-496658184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b769badd-4326-463b-9566-3560a45732b3",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Should we also check AccessMode = rwx?",
        "createdAt" : "2020-09-25T18:22:39Z",
        "updatedAt" : "2020-09-25T18:23:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "029da300-3616-44c2-b650-3eee662c8f9a",
        "parentId" : "b769badd-4326-463b-9566-3560a45732b3",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "nm we only get here if it's rwx already",
        "createdAt" : "2020-09-25T18:23:37Z",
        "updatedAt" : "2020-09-25T18:23:38Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2bf2ab76ecfb43519e64dea2eef6051a5abaf31",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +511,515 @@\tdirectIO := false\n\t// direct IO is needed for Block-mode PVs\n\tif *pvc.Spec.VolumeMode == v1.PersistentVolumeBlock {\n\t\t// byteLen should be the size of a sector to enable direct I/O\n\t\tbyteLen = 512"
  },
  {
    "id" : "911259af-360e-4363-b04e-0a90b5230aba",
    "prId" : 85898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85898#pullrequestreview-340462142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f28019f5-e555-45cd-a705-921f1ff17aee",
        "parentId" : null,
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Logging the wrong error (declared in line 345).",
        "createdAt" : "2020-01-09T09:29:42Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "907086a7-27a3-41bc-8cb4-9a31149ea639",
        "parentId" : "f28019f5-e555-45cd-a705-921f1ff17aee",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Same for all checks of `ensureTopologyRequirements(...)`",
        "createdAt" : "2020-01-09T09:44:24Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "68621264-4fc8-4cc3-8210-8f4f7e29799e",
        "parentId" : "f28019f5-e555-45cd-a705-921f1ff17aee",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "Will fix it. Thanks.",
        "createdAt" : "2020-01-09T11:41:20Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      }
    ],
    "commit" : "c42e815fb5105379d79987cd35c2555c6f00f3fd",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +357,361 @@\t\tif len(topologyKeys) != 0 {\n\t\t\tif err = ensureTopologyRequirements(&nodeSelection, nodes, l.cs, topologyKeys, 2); err != nil {\n\t\t\t\tframework.Failf(\"Error setting topology requirements: %v\", err)\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "4cc7fa79-3b64-4db2-8159-d1c070b4b1bc",
    "prId" : 85898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85898#pullrequestreview-342575656",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4816ee9-8010-49e0-aecb-a05a1e069b2d",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "ClientNode is already set in nodeSelection. Do we end up overwriting it?",
        "createdAt" : "2020-01-11T01:01:12Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "7c3be09d-1ba9-43de-b241-6e9141dee245",
        "parentId" : "b4816ee9-8010-49e0-aecb-a05a1e069b2d",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "The function should add the required topology labels to the existing nodeSelection so not overwriting any existing fields there. Moreover the ClienNode seems to always be an empty string.",
        "createdAt" : "2020-01-13T09:32:09Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      },
      {
        "id" : "3984ae2d-8afe-4d82-85df-fdead5952be4",
        "parentId" : "b4816ee9-8010-49e0-aecb-a05a1e069b2d",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "ClientNode may be set by [external](https://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/external/external.go#L111) users. I guess they shouldn't also enable topology the same time. Otherwise, the two may conflict and you could end up with an unschedulable pod?",
        "createdAt" : "2020-01-14T02:57:46Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "9ff3daa6-7f88-44d7-aad7-2a81d57b0dca",
        "parentId" : "b4816ee9-8010-49e0-aecb-a05a1e069b2d",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "Yes. I meant that in the case the ClientNode is really set the test would be skipped (every multi-node test checks this) so we'd never get here.",
        "createdAt" : "2020-01-14T14:33:57Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      }
    ],
    "commit" : "c42e815fb5105379d79987cd35c2555c6f00f3fd",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +578,582 @@\t}\n\t// Take the first suitable topology\n\te2epod.SetNodeAffinityTopologyRequirement(nodeSelection, suitableTopologies[0])\n\n\treturn nil"
  },
  {
    "id" : "a434c6ae-f422-458c-918a-bbea58272ae9",
    "prId" : 85898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85898#pullrequestreview-342267985",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb118954-dc3f-46ad-ad35-2e813cfcdfd8",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "This logic is the same in many tests. Can we consolidate all the \"different-node test requirement\" checks into a common function?",
        "createdAt" : "2020-01-11T01:02:59Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "996b7d85-2421-42bf-99fb-12a7fd3232d8",
        "parentId" : "cb118954-dc3f-46ad-ad35-2e813cfcdfd8",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "Yes, @jsafrane asked about it too. These tests had a lot of repeated code before I started changing them and I don't think it's necessary bad -- it's very readable and makes it easy to debug when they fail. I tried to put the common code in the functions already: I can also move the `topologyKeys` length test out but I'm not sure it would be that much better.",
        "createdAt" : "2020-01-13T09:35:06Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      },
      {
        "id" : "90e487bf-256a-428e-bd7f-6342162609e3",
        "parentId" : "cb118954-dc3f-46ad-ad35-2e813cfcdfd8",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I think the whole section starting from \"// Check different-node test requirement\" could be consolidated to a common method. But it doesn't have to be in this pr.",
        "createdAt" : "2020-01-14T03:01:35Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c42e815fb5105379d79987cd35c2555c6f00f3fd",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +352,356 @@\t\t\te2eskipper.Skipf(\"Driver %q requires to deploy on a specific node - skipping\", l.driver.GetDriverInfo().Name)\n\t\t}\n\t\t// For multi-node tests there must be enough nodes with the same toopology to schedule the pods\n\t\tnodeSelection := e2epod.NodeSelection{Name: l.config.ClientNodeName}\n\t\ttopologyKeys := dInfo.TopologyKeys"
  },
  {
    "id" : "a8751cbe-1690-425a-ac2b-e8f3ec9e5a6a",
    "prId" : 85898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85898#pullrequestreview-342561754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44ad6933-63e0-4164-91af-2790cd115a83",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I guess the line above means that ClientNodeName should be empty. So let's just not set anything here?",
        "createdAt" : "2020-01-14T03:00:14Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "5740572b-3f4a-4594-bffc-da2d45e11f1f",
        "parentId" : "44ad6933-63e0-4164-91af-2790cd115a83",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Repeat throughout",
        "createdAt" : "2020-01-14T03:00:33Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "dd6ac823-bfa2-45bc-8dab-7622bfc58eba",
        "parentId" : "44ad6933-63e0-4164-91af-2790cd115a83",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "This is only re-doing what was already in the code before: check the next chunk -- I simply didn't want to change what was there already.",
        "createdAt" : "2020-01-14T14:15:24Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      }
    ],
    "commit" : "c42e815fb5105379d79987cd35c2555c6f00f3fd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +182,186 @@\t\t}\n\t\t// For multi-node tests there must be enough nodes with the same toopology to schedule the pods\n\t\tnodeSelection := e2epod.NodeSelection{Name: l.config.ClientNodeName}\n\t\ttopologyKeys := dInfo.TopologyKeys\n\t\tif len(topologyKeys) != 0 {"
  },
  {
    "id" : "bd273ccc-f03a-4926-bffc-e957ba750cf4",
    "prId" : 85898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85898#pullrequestreview-345268556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7ffb801-eaaa-4d3c-bf20-95c7707bd1f7",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "If a driver optionally supports topology (like vsphere), and this runs on a platform where topology is not used, are we going to end up skipping the test?",
        "createdAt" : "2020-01-14T03:04:53Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "b9ebda2f-3a86-46b0-89de-d732e2578a80",
        "parentId" : "e7ffb801-eaaa-4d3c-bf20-95c7707bd1f7",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "I didn't think about this... Is there a way to detect the topology support is optional? ",
        "createdAt" : "2020-01-14T08:11:08Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      },
      {
        "id" : "75b9016f-0fca-4236-a674-1f53d28473d5",
        "parentId" : "e7ffb801-eaaa-4d3c-bf20-95c7707bd1f7",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "On vSphere without topology support, nodes won't have topology labels. Looking at `getCurrentTopologiesNumber`, it may return some weird arrays. I added a comment above.",
        "createdAt" : "2020-01-14T15:03:18Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "7d1b62f7-fd6c-457e-86b9-a1854bb6ef79",
        "parentId" : "e7ffb801-eaaa-4d3c-bf20-95c7707bd1f7",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "And the test gets skipped, because no topology has more than 1 node.",
        "createdAt" : "2020-01-14T15:04:23Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "e02d3069-35fe-43b6-b959-9b50d50684ae",
        "parentId" : "e7ffb801-eaaa-4d3c-bf20-95c7707bd1f7",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "Nodes with the \"empty\" topologies get counted as if they had the same topology. I tried to re-run the tests on a cluster with no topology labels and the test run looks that same (since all the nodes were actually in the same zone). So I guess it's OK.",
        "createdAt" : "2020-01-20T12:15:04Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      }
    ],
    "commit" : "c42e815fb5105379d79987cd35c2555c6f00f3fd",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +575,579 @@\t}\n\tif len(suitableTopologies) == 0 {\n\t\tframework.Skipf(\"No topology with at least %d nodes found - skipping\", minCount)\n\t}\n\t// Take the first suitable topology"
  },
  {
    "id" : "3bfa5d29-7343-49d2-9f28-87419079fd8f",
    "prId" : 85898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85898#pullrequestreview-345268695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "120282ef-3bf4-4042-b46b-f19efe25bf44",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "What if a node does not have any topology label? `topo` will be empty and it will be added to `topos` below.",
        "createdAt" : "2020-01-14T15:02:59Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "bbed45a5-c3fc-43cc-b9e9-4fd31c12099a",
        "parentId" : "120282ef-3bf4-4042-b46b-f19efe25bf44",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "Yes. That's a situation I didn't count with. The driver specifies topology keys but nodes don't use them.",
        "createdAt" : "2020-01-14T15:45:16Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      },
      {
        "id" : "03773847-aa65-4b93-8007-21d9aa293460",
        "parentId" : "120282ef-3bf4-4042-b46b-f19efe25bf44",
        "authorId" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "body" : "See above...",
        "createdAt" : "2020-01-20T12:15:22Z",
        "updatedAt" : "2020-01-20T14:02:16Z",
        "lastEditedBy" : "d0f794d1-a9d4-4c31-aacc-fb35543cf586",
        "tags" : [
        ]
      }
    ],
    "commit" : "c42e815fb5105379d79987cd35c2555c6f00f3fd",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +540,544 @@\t\tfor _, k := range keys {\n\t\t\tv, ok := n.Labels[k]\n\t\t\tif ok {\n\t\t\t\ttopo[k] = v\n\t\t\t}"
  },
  {
    "id" : "fbbf27bb-6e65-4d19-a7e4-84b5c3ca374a",
    "prId" : 74693,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74693#pullrequestreview-211554116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3adfb7a-cb64-4ae0-a5f0-d833172af138",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "L323 - L340 are repeated. Can we put those into a function?",
        "createdAt" : "2019-03-06T22:27:05Z",
        "updatedAt" : "2019-03-09T00:29:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "124b0d41-53a2-4cc4-8c7b-40803318893e",
        "parentId" : "b3adfb7a-cb64-4ae0-a5f0-d833172af138",
        "authorId" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "body" : "@msau42 \r\n\r\nThank you for good advice, I fixed as suggested.\r\nAlso, I found that read test should be done before write test after pod recreation, but previous code doesn't do it properly.\r\nSo, I fixed it to work properly.\r\n\r\nPTAL\r\n",
        "createdAt" : "2019-03-07T00:55:39Z",
        "updatedAt" : "2019-03-09T00:29:19Z",
        "lastEditedBy" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4c88acec63ebd678d2e5a7a3db1b6da22d39b96",
    "line" : 322,
    "diffHunk" : "@@ -1,1 +320,324 @@func testAccessMultipleVolumes(f *framework.Framework, cs clientset.Interface, ns string,\n\tnode framework.NodeSelection, pvcs []*v1.PersistentVolumeClaim, readSeedBase int64, writeSeedBase int64) string {\n\tBy(fmt.Sprintf(\"Creating pod on %+v with multiple volumes\", node))\n\tpod, err := framework.CreateSecPodWithNodeSelection(cs, ns, pvcs,\n\t\tfalse, \"\", false, false, framework.SELinuxLabel,"
  },
  {
    "id" : "cad2df15-044a-4b19-99a0-435da5ef5f68",
    "prId" : 74693,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74693#pullrequestreview-211555774",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca709631-3923-4dd9-b75e-b161d84c2a86",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "every test case is doing init/cleanup.  Can this go in Before/AfterEach instead?",
        "createdAt" : "2019-03-06T22:27:39Z",
        "updatedAt" : "2019-03-09T00:29:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "653ee6b2-8ac3-4572-b36d-fab62cd1e9ef",
        "parentId" : "ca709631-3923-4dd9-b75e-b161d84c2a86",
        "authorId" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "body" : "Current code is a bit redundant, but it give us a chance to skip tests before driver initialization, per test basis. (Some test can be skipped without the information of driver, but others won't.)\r\nIf we prioritize readability over performance, we will be able to choose moving it to Before/AfterEach.",
        "createdAt" : "2019-03-07T00:59:04Z",
        "updatedAt" : "2019-03-09T00:29:19Z",
        "lastEditedBy" : "9ce7bd1a-5286-4173-88a4-039146bf0d46",
        "tags" : [
        ]
      },
      {
        "id" : "88773c1b-b152-4b29-9dc8-a45d65807b08",
        "parentId" : "ca709631-3923-4dd9-b75e-b161d84c2a86",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I guess for now it's fine, but I eventually want to move to a model where we bring up the driver during test cluster initialization instead of during the test case.",
        "createdAt" : "2019-03-07T01:02:31Z",
        "updatedAt" : "2019-03-09T00:29:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4c88acec63ebd678d2e5a7a3db1b6da22d39b96",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +119,123 @@\t\t}\n\n\t\tinit()\n\t\tdefer cleanup()\n"
  }
]