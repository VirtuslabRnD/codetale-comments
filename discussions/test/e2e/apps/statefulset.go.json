[
  {
    "id" : "4031dd26-503a-4631-bc53-1e7e82f6ee60",
    "prId" : 103073,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/103073#pullrequestreview-691184397",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8afa6db-5206-48f1-9f38-ccb8ff7cdb07",
        "parentId" : null,
        "authorId" : "0b8a340d-147f-4dd4-8384-35edecd298a1",
        "body" : "should we also test the minReadySeconds time itself in another test?",
        "createdAt" : "2021-06-23T11:32:31Z",
        "updatedAt" : "2021-06-23T11:32:49Z",
        "lastEditedBy" : "0b8a340d-147f-4dd4-8384-35edecd298a1",
        "tags" : [
        ]
      },
      {
        "id" : "a85ea66f-c18f-487c-b3a0-1e81d06c6f7c",
        "parentId" : "f8afa6db-5206-48f1-9f38-ccb8ff7cdb07",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "minReadySeconds may have issue because of clockskew between master and worker nodes",
        "createdAt" : "2021-06-23T12:24:28Z",
        "updatedAt" : "2021-06-23T12:26:26Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "8bd48e43-5732-483f-a75e-e1311802c96a",
        "parentId" : "f8afa6db-5206-48f1-9f38-ccb8ff7cdb07",
        "authorId" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "body" : "What is the purpose of this additional test?\r\nI cannot find any minReadySeconds specific part in that.\r\nIf that could not be stable, it would be better to write the specific part as commented-out.",
        "createdAt" : "2021-06-23T17:49:18Z",
        "updatedAt" : "2021-06-23T17:49:34Z",
        "lastEditedBy" : "c2df03b8-26df-4018-9f8f-4ddea7f8f6cc",
        "tags" : [
        ]
      },
      {
        "id" : "4b28117a-11ad-49a0-8bc5-33af2814ee6b",
        "parentId" : "f8afa6db-5206-48f1-9f38-ccb8ff7cdb07",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "This test ensures available replicas are matching what we have spec. minReadySeconds is the amount of time STS controller waits before marking a pod ready.",
        "createdAt" : "2021-06-23T22:07:26Z",
        "updatedAt" : "2021-06-23T22:07:26Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf9510751d51970948ccfa53f1e1332fd66c01a3",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1140,1144 @@\t\tframework.ExpectNoError(err)\n\t\te2estatefulset.WaitForStatusAvailableReplicas(c, ss, 1)\n\t})\n})\n"
  },
  {
    "id" : "bc8d1071-f6fb-415e-a563-2fe6fb1121a8",
    "prId" : 102344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102344#pullrequestreview-689617683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "792e16c4-2dd2-4cc5-8907-84c997e02c03",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "What's the rationale behind moving this earlier? Are these two test assertions (pod vs statefulset conflicts) interchangeable?",
        "createdAt" : "2021-06-21T21:43:03Z",
        "updatedAt" : "2021-06-21T22:22:44Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "c04d5247-7d51-4655-9e48-159fc3bccb41",
        "parentId" : "792e16c4-2dd2-4cc5-8907-84c997e02c03",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "The test is trying to verify that statefulPodName goes into failed at least once, and that the stateful controller sees that and recreates it, and that the trigger is the scheduling of a conflicting port.  If a human introduces a bug in the stateful set that causes it to never start, or a bug in the kubelet that fails pods incorrectly, or another random failure happens, it might not be the conflicting port (admission) that causes it to get recreated.  By waiting for the first pod to start successfully, we move those failures to earlier (we are more confident that the conflicting port is the cause).  This move means debugging is easier (if the pod fails to start it's more likely a bug in the test, or a bug in kubelet startup, if the pod starts but then conflict doesn't cause a restart, it's because someone broke admission in the kubelet)",
        "createdAt" : "2021-06-22T14:52:36Z",
        "updatedAt" : "2021-06-22T14:52:36Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eadd1a9ead7a009a9abfbd603a5efd0560473cc",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +760,764 @@\t\t\tif err := e2epod.WaitForPodNameRunningInNamespace(f.ClientSet, podName, f.Namespace.Name); err != nil {\n\t\t\t\tframework.Failf(\"Pod %v did not start running: %v\", podName, err)\n\t\t\t}\n\n\t\t\tginkgo.By(\"Creating statefulset with conflicting port in namespace \" + f.Namespace.Name)"
  },
  {
    "id" : "a16cfba7-8d69-4ee9-af24-9a67b064bd97",
    "prId" : 102256,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102256#pullrequestreview-677952594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e4d79fa-3f0e-4aed-9d85-9ba763b95b42",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "nit: Waiting for statefulset to have expected annotation but current annotation and Conditions are?",
        "createdAt" : "2021-06-07T00:05:10Z",
        "updatedAt" : "2021-06-07T00:07:48Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "152026ad-f70d-4b9a-9d8c-2241267109eb",
        "parentId" : "1e4d79fa-3f0e-4aed-9d85-9ba763b95b42",
        "authorId" : "23d83a23-a06a-4542-a52f-6096ea5648ef",
        "body" : "@ravisantoshgudimetla If the test was to flake at some point the idea was to have a clear reference point to help debug the test. For some context refer to this [comment](https://github.com/kubernetes/kubernetes/pull/96485#issuecomment-732335344).",
        "createdAt" : "2021-06-07T21:19:20Z",
        "updatedAt" : "2021-06-07T21:19:20Z",
        "lastEditedBy" : "23d83a23-a06a-4542-a52f-6096ea5648ef",
        "tags" : [
        ]
      },
      {
        "id" : "bf858e75-2bcd-4607-84bc-95f55168a4b7",
        "parentId" : "1e4d79fa-3f0e-4aed-9d85-9ba763b95b42",
        "authorId" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "body" : "The same pattern is followed in other merged conformance tests. We would like to keep it consistent. Thank you for the review.",
        "createdAt" : "2021-06-07T22:59:32Z",
        "updatedAt" : "2021-06-07T22:59:33Z",
        "lastEditedBy" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "tags" : [
        ]
      }
    ],
    "commit" : "bcfa3604a28cf0fea51776de10a45f9b9ab3e187",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +963,967 @@\t\t\t\t\t\te.ObjectMeta.Labels[\"e2e\"] == ss.ObjectMeta.Labels[\"e2e\"]\n\t\t\t\t\tif !found {\n\t\t\t\t\t\tframework.Logf(\"Observed Statefulset %v in namespace %v with annotations: %v & Conditions: %v\", ss.ObjectMeta.Name, ss.ObjectMeta.Namespace, ss.Annotations, ss.Status.Conditions)\n\t\t\t\t\t\treturn false, nil\n\t\t\t\t\t}"
  },
  {
    "id" : "221f1807-5923-48e8-a493-e63b5814116a",
    "prId" : 98126,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98126#pullrequestreview-589660959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f3a7bec-f887-4c32-80f5-943f3a7f2b58",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "should we similar to the original status, also add ?\r\ne2estatefulset.WaitForRunningAndReady(c, *ss.Spec.Replicas, ss)\r\nwaitForStatus(c, ss)",
        "createdAt" : "2021-01-19T02:39:36Z",
        "updatedAt" : "2021-02-11T20:44:38Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "04fe2fc1-4404-4291-a2df-939c18401981",
        "parentId" : "2f3a7bec-f887-4c32-80f5-943f3a7f2b58",
        "authorId" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "body" : "It is included on line 847-848. The pattern looks to be using this only at the \"create\" stage of the statefulset.\r\nDoes this address your concern?\r\n\r\n",
        "createdAt" : "2021-01-19T21:08:57Z",
        "updatedAt" : "2021-02-11T20:44:38Z",
        "lastEditedBy" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "tags" : [
        ]
      },
      {
        "id" : "38909bc5-a68c-4a5f-ae5f-05a78f610864",
        "parentId" : "2f3a7bec-f887-4c32-80f5-943f3a7f2b58",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "agree its not the pattern, but extra validation makes the test more robust, that indeed after patching the scale subresource, the StatefulSet pod is ready and good to go",
        "createdAt" : "2021-01-22T08:30:50Z",
        "updatedAt" : "2021-02-11T20:44:38Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "ba08c75b-6b65-47be-80ea-1c689a1d49cd",
        "parentId" : "2f3a7bec-f887-4c32-80f5-943f3a7f2b58",
        "authorId" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "body" : "Thank you for helping to make this test more solid.\r\nI believe `WaitForRunningAndReady` is for creating new resources.\r\nHowever, I think adding ` WaitForStatusReplicas` will add the value you would like to see.\r\nI update with ` WaitForStatusReplicas` and I trust this will make the test ready to be merged.",
        "createdAt" : "2021-01-24T18:14:24Z",
        "updatedAt" : "2021-02-11T20:44:38Z",
        "lastEditedBy" : "53d7f6d1-52e4-4b09-bbe9-0301c7d86286",
        "tags" : [
        ]
      },
      {
        "id" : "885fe4d0-5f9e-49f3-97b0-da5dc5bf6b4b",
        "parentId" : "2f3a7bec-f887-4c32-80f5-943f3a7f2b58",
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "> agree its not the pattern, but extra validation makes the test more robust, that indeed after patching the scale subresource, the StatefulSet pod is ready and good to go\r\n\r\n@krmayankk iirc there are other tests that ensure the scale is working as expected. The purpose of this test is to just ensure the update & patch API calls are working as expected. That additional scrutiny imo is unnecessary b/c we don't rely on the result at all and that wail will only make this test last longer. So I suggested dropping that additional wait. ",
        "createdAt" : "2021-02-11T14:09:28Z",
        "updatedAt" : "2021-02-11T20:44:38Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      },
      {
        "id" : "86b02842-35d0-448c-abab-afa6c2b664f2",
        "parentId" : "2f3a7bec-f887-4c32-80f5-943f3a7f2b58",
        "authorId" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "body" : "/hold\r\nwhere's the e2e/conformance test that verifies stateful sets scale as expected?  as an end user I'd be surprised if the API allowed me to update the spec, but the status never reconciled",
        "createdAt" : "2021-02-12T17:12:25Z",
        "updatedAt" : "2021-02-12T17:12:25Z",
        "lastEditedBy" : "5de211e4-9744-455e-9548-1a8e70ed1b2e",
        "tags" : [
        ]
      }
    ],
    "commit" : "866f658695d1ccead4475e4c565fa70c2c75bc57",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +888,892 @@\t\t\tss, err = c.AppsV1().StatefulSets(ns).Get(context.TODO(), ssName, metav1.GetOptions{})\n\t\t\tframework.ExpectNoError(err, \"Failed to get statefulset resource: %v\", err)\n\t\t\tframework.ExpectEqual(*(ss.Spec.Replicas), int32(4), \"statefulset should have 4 replicas\")\n\t\t})\n\t})"
  },
  {
    "id" : "ad46be92-dc0d-46f1-837b-a40996ac5805",
    "prId" : 90898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90898#pullrequestreview-408273042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "870d58bd-0fc9-4cd9-a381-820e489bbe9e",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "move this block after `Scale down will halt with unhealthy stateful pod` and revert to watchtools.Until starting at pl.ResourceVersion for this PR",
        "createdAt" : "2020-05-08T14:10:32Z",
        "updatedAt" : "2020-05-08T15:23:12Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ca96f0e1-be9f-467f-82d9-e8878f51d0b5",
        "parentId" : "870d58bd-0fc9-4cd9-a381-820e489bbe9e",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "done",
        "createdAt" : "2020-05-08T14:20:03Z",
        "updatedAt" : "2020-05-08T15:23:12Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "238dde4bdb602bd78ad6932ec38a9bf8a9cfd03b",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +643,647 @@\t\t\tframework.ExpectNoError(err)\n\n\t\t\t// Verify that statuful set will be scaled down in order.\n\t\t\twg.Add(1)\n\t\t\tgo func() {"
  },
  {
    "id" : "94a70df6-67c1-495c-8324-3f150fcf8c7a",
    "prId" : 90898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90898#pullrequestreview-408318605",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d81efcd2-b141-40bc-8ed0-caf37814740a",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "the timeout here probably isn't right, now that this starts before the statefulset does stuff",
        "createdAt" : "2020-05-08T15:23:43Z",
        "updatedAt" : "2020-05-08T15:23:56Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "9ac21753-9c8b-497d-a650-1e2944439fe7",
        "parentId" : "d81efcd2-b141-40bc-8ed0-caf37814740a",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "It's not the matter of timeout - the 10 minut timeout seems fine to me.\r\n\r\nIt was a matter of waiting for Added\" event in the second case.",
        "createdAt" : "2020-05-08T15:24:59Z",
        "updatedAt" : "2020-05-08T15:24:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "238dde4bdb602bd78ad6932ec38a9bf8a9cfd03b",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +598,602 @@\n\t\t\t\texpectedOrder := []string{ssName + \"-0\", ssName + \"-1\", ssName + \"-2\"}\n\t\t\t\tctx, cancel := watchtools.ContextWithOptionalTimeout(context.Background(), statefulSetTimeout)\n\t\t\t\tdefer cancel()\n"
  },
  {
    "id" : "7404df3f-7044-4309-a76d-20c89c6f9357",
    "prId" : 90898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90898#pullrequestreview-408317663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7d7ff31-e016-49b0-90f4-a3e315403800",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "same here ",
        "createdAt" : "2020-05-08T15:23:54Z",
        "updatedAt" : "2020-05-08T15:23:56Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "238dde4bdb602bd78ad6932ec38a9bf8a9cfd03b",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +649,653 @@\n\t\t\t\texpectedOrder := []string{ssName + \"-2\", ssName + \"-1\", ssName + \"-0\"}\n\t\t\t\tctx, cancel := watchtools.ContextWithOptionalTimeout(context.Background(), statefulSetTimeout)\n\t\t\t\tdefer cancel()\n"
  },
  {
    "id" : "e3cac79a-d044-490d-a71f-1d763ab53c2e",
    "prId" : 90898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90898#pullrequestreview-408579148",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5f3dd91-517a-4fca-a4d9-a2d9ec2b5040",
        "parentId" : null,
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "Ginkgo suggest adding `defer GinkgoRecover()` at the top of the go routine, should we need to add it?\r\nxref http://tonyyang132.blogspot.com/2018/08/ginkgo-and-goroutine.html",
        "createdAt" : "2020-05-08T23:33:36Z",
        "updatedAt" : "2020-05-08T23:33:36Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      }
    ],
    "commit" : "238dde4bdb602bd78ad6932ec38a9bf8a9cfd03b",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +646,650 @@\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\n\t\t\t\texpectedOrder := []string{ssName + \"-2\", ssName + \"-1\", ssName + \"-0\"}"
  },
  {
    "id" : "7332d069-e781-4d59-b750-57acd4943723",
    "prId" : 64350,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64350#pullrequestreview-124533614",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e516899f-1c5f-41f3-ad85-e3fc3301ee34",
        "parentId" : null,
        "authorId" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "body" : "From the test description, I'd have expected that we'd exercise some PVC option explicitly. I think the test name could use revising.",
        "createdAt" : "2018-05-30T17:52:31Z",
        "updatedAt" : "2018-07-02T02:17:19Z",
        "lastEditedBy" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "tags" : [
        ]
      },
      {
        "id" : "6cfd3bc3-6e2e-48cb-b408-ccbd223d0cb9",
        "parentId" : "e516899f-1c5f-41f3-ad85-e3fc3301ee34",
        "authorId" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "body" : "Hmm, from the linked issue, does this actually use an RWO PVC? ",
        "createdAt" : "2018-05-30T17:56:57Z",
        "updatedAt" : "2018-07-02T02:17:19Z",
        "lastEditedBy" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "tags" : [
        ]
      },
      {
        "id" : "e1d77193-ea99-4b8e-be9a-79b52b801611",
        "parentId" : "e516899f-1c5f-41f3-ad85-e3fc3301ee34",
        "authorId" : "9214959e-f9c5-43ab-a801-11ef9bc58371",
        "body" : "Yeah, the PVC spec is generated as RWO.",
        "createdAt" : "2018-05-30T18:26:07Z",
        "updatedAt" : "2018-07-02T02:17:19Z",
        "lastEditedBy" : "9214959e-f9c5-43ab-a801-11ef9bc58371",
        "tags" : [
        ]
      }
    ],
    "commit" : "db69638911894117b74875d4558092c7274ac263",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +251,255 @@\t\t// This can't be Conformance yet because it depends on a default\n\t\t// StorageClass and a dynamic provisioner.\n\t\tIt(\"should perform rolling updates and roll backs of template modifications with PVCs\", func() {\n\t\t\tBy(\"Creating a new StatefulSet with PVCs\")\n\t\t\t*(ss.Spec.Replicas) = 3"
  },
  {
    "id" : "9409eee5-afcf-4afb-9a43-1c6452a0a244",
    "prId" : 49289,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49289#pullrequestreview-51315957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1db147d2-17ca-464e-82a4-c703e9724b14",
        "parentId" : null,
        "authorId" : "a5ac91f3-1cbc-4fd9-af85-741ee28dd9c8",
        "body" : "Golint comments: should have a package comment, unless it's in another file for this package. [More info](https://golang.org/wiki/CodeReviewComments#package-comments). <!-- golint -->",
        "createdAt" : "2017-07-20T20:09:06Z",
        "updatedAt" : "2017-07-22T10:43:51Z",
        "lastEditedBy" : "a5ac91f3-1cbc-4fd9-af85-741ee28dd9c8",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e93ed27bd90728dc840b9bfd67b5c4c35affe48",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +15,19 @@*/\n\npackage apps\n\nimport ("
  },
  {
    "id" : "2af3e0ab-32be-40f5-9e69-c17adbcbf014",
    "prId" : 49168,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49168#pullrequestreview-52820155",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40d9bfa4-6f2b-4ed1-85f3-0c801bcd752d",
        "parentId" : null,
        "authorId" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "body" : "@liggitt I am not sure whether i catch your meaning. This e2e test will not work as the scale subresource cannot be found via `RESTClient` calls.\r\n\r\nWhat i want is to obtain a JSON blob located at:\r\n(1) for StatefulSet: e.g., `http://localhost:8001/apis/apps/v1beta2/namespaces/default/statefulsets/`\r\n(2) for StatefulSet's scale subresource: e.g., `http://localhost:8001/apis/apps/v1beta2/namespaces/default/statefulsets/web/scale`\r\n\r\nI am able to test it manually using `kubectl proxy` and `curl`. For example, the subresource `Spec.Replicas` attribute can be modified, and the change will be reflected to the StatefulSet `Spec.Replicas` attribute.\r\n\r\nI am not sure whether i miss something when setting up `RESTClient`, or `RESTClient` is not the suitable tool for this. I will look more into the library, but do you have any recommended person that i can ask for help with regarding this? FYI, e2e testing is something new to me and i have spent more than one week to come out with a working e2e test. I am more than happy if there is a person who can shed some lights on this. Thanks!",
        "createdAt" : "2017-07-27T23:58:53Z",
        "updatedAt" : "2017-08-07T19:18:59Z",
        "lastEditedBy" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "tags" : [
        ]
      },
      {
        "id" : "4f6c4687-d192-4048-beda-18c852161bcc",
        "parentId" : "40d9bfa4-6f2b-4ed1-85f3-0c801bcd752d",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "> I am able to test it manually using kubectl proxy and curl\r\n\r\nall `kubectl proxy` does is add credentials from your local kubeconfig file to requests made to `http://localhost:8001/...` and pass the request along to the API server. if this is working for you, it must mean you have `apps/v1beta2` enabled on your API server, and could make the same request directly against the API server, without `kubectl proxy` being involved at all.\r\n\r\nif all you want to do in the e2e is to submit a request to an API endpoint, there's no need to use curl, or to run `kubectl proxy`.\r\n\r\nI'd expect the e2e test to do something more like this:\r\n```\r\nBy(\"getting scale subresource\")\r\nscale := &extensionsv1beta1.Scale{}\r\nerr := c.AppsV1beta2().RESTClient().Get().AbsPath(\"/apis/apps/v1beta2\").Namespace(ns).Resource(\"statefulsets\").Name(ssName).SubResource(\"scale\").Do().Into(scale)\r\nif err != nil {\r\n\tframework.Failf(\"Failed to get scale subresource: %v\", err)\r\n}\r\nExpect(scale.Spec.Replicas).To(Equal(1))\r\nExpect(scale.Status.Replicas).To(Equal(1))\r\n\r\nBy(\"updating a scale subresource\")\r\n// unconditionally update to 2 replicas\r\nscale.ResourceVersion = \"\"\r\nscale.Spec.Replicas = 2\r\nscaleResult := &extensionsv1beta1.Scale{}\r\nerr := c.AppsV1beta2().RESTClient().Put().AbsPath(\"/apis/apps/v1beta2\").Namespace(ns).Resource(\"statefulsets\").Name(ssName).SubResource(\"scale\").Body(scale).Do().Into(scaleResult)\r\nif err != nil {\r\n\tframework.Failf(\"Failed to put scale subresource: %v\", err)\r\n}\r\nExpect(scaleResult.Spec.Replicas).To(Equal(2))\r\n\r\nBy(\"verifying the stateful set spec was modified\")\r\nss, err := c.AppsV1beta2().StatefulSets(ns).Get(ssName, metav1.GetOptions{})\r\nif err != nil {\r\n\tframework.Failf(\"Failed to get stateful set resource: %v\", err)\r\n}\r\nExpect(ss.Spec.Replicas).To(Equal(2))\r\n```\r\n\r\nThe above code assumes the scale resource expects/returns extensions/v1beta1 Scale objects, which I know is being debated. (it also was typed directly without testing... making it compile is left as an exercise for the reader :) )",
        "createdAt" : "2017-07-28T00:22:21Z",
        "updatedAt" : "2017-08-07T19:18:59Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "4f05f1dd-c3c7-4550-a450-61a9f1748d2a",
        "parentId" : "40d9bfa4-6f2b-4ed1-85f3-0c801bcd752d",
        "authorId" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "body" : "@liggitt Thank you very much for your detailed explanation! i will dive deep into each of the components carefully, and get back to you tomorrow. Thanks!",
        "createdAt" : "2017-07-28T00:28:51Z",
        "updatedAt" : "2017-08-07T19:18:59Z",
        "lastEditedBy" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "tags" : [
        ]
      }
    ],
    "commit" : "91f100b50178e1f46b46df21ea11a2f60e5d04aa",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +845,849 @@\t\t})\n\n\t\tIt(\"should have a working scale subresource\", func() {\n\t\t\tBy(\"Creating statefulset \" + ssName + \" in namespace \" + ns)\n\t\t\tss := framework.NewStatefulSet(ssName, ns, headlessSvcName, 1, nil, nil, labels)"
  }
]