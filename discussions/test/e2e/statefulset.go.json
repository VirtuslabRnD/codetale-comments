[
  {
    "id" : "bf43edc7-a525-4015-81e0-7bcb2ea02247",
    "prId" : 47693,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/47693#pullrequestreview-44721413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec6245cb-bf7c-4644-a365-9d80765bb3c6",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "rolling back is not supported, you mean rolling forward to a previous revision ?",
        "createdAt" : "2017-06-18T09:06:13Z",
        "updatedAt" : "2017-06-19T13:54:15Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "cfaffae9-2a86-4b74-bc60-03b17897ca14",
        "parentId" : "ec6245cb-bf7c-4644-a365-9d80765bb3c6",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "Rolling back is supported. This test ensures that the controller correctly detects a rollback and selects the previous revision instead of creating a new one.",
        "createdAt" : "2017-06-18T13:30:54Z",
        "updatedAt" : "2017-06-19T13:54:15Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      }
    ],
    "commit" : "b898cf81a50d7b8db53373bafa9a1a0377c0e394",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +320,324 @@\t\t\t}\n\n\t\t\tBy(\"Rolling back to a previous revision\")\n\t\t\tsst.BreakPodProbe(ss, &pods.Items[1], testProbe)\n\t\t\tExpect(err).NotTo(HaveOccurred())"
  },
  {
    "id" : "21957c97-257d-4c14-9fe7-eb717cb74fb2",
    "prId" : 47693,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/47693#pullrequestreview-44853122",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5097a53-ce79-4e92-8549-365317938b87",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "are these both in one side of partition or opposite sides ? worth putting a comment.",
        "createdAt" : "2017-06-19T07:16:50Z",
        "updatedAt" : "2017-06-19T13:54:15Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "7eadf47e-1906-4483-9e0d-ab833ee6193d",
        "parentId" : "a5097a53-ce79-4e92-8549-365317938b87",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "They are on opposite sides. I think that, as the partition is set to 1 above, and as 0 and 2 are clearly the Pods being deleted. This is obvious.",
        "createdAt" : "2017-06-19T13:57:17Z",
        "updatedAt" : "2017-06-19T13:58:00Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      }
    ],
    "commit" : "b898cf81a50d7b8db53373bafa9a1a0377c0e394",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +490,494 @@\t\t\tBy(\"Restoring Pods to the correct revision when they are deleted\")\n\t\t\tsst.DeleteStatefulPodAtIndex(0, ss)\n\t\t\tsst.DeleteStatefulPodAtIndex(2, ss)\n\t\t\tsst.WaitForRunningAndReady(3, ss)\n\t\t\tss = sst.GetStatefulSet(ss.Namespace, ss.Name)"
  },
  {
    "id" : "23bdb3d6-7f56-4b39-a3a0-abaefc1bb59a",
    "prId" : 47693,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/47693#pullrequestreview-44943877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6b3f6b1-f1fe-4906-8115-f939c9090c07",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "curious, why do you initialize Partition in this way and not Partition: &somePartitionVar",
        "createdAt" : "2017-06-19T07:39:37Z",
        "updatedAt" : "2017-06-19T13:54:15Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "e5c1f5e8-b972-4733-a0cb-a19d669a5c72",
        "parentId" : "a6b3f6b1-f1fe-4906-8115-f939c9090c07",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "I feel that both are idiomatic go code for in place initialization of a struct value with a pointer. In this case I used function that returns a pointer so that the pointer is explicitly immutable. ",
        "createdAt" : "2017-06-19T19:10:33Z",
        "updatedAt" : "2017-06-19T19:10:33Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      }
    ],
    "commit" : "b898cf81a50d7b8db53373bafa9a1a0377c0e394",
    "line" : 205,
    "diffHunk" : "@@ -1,1 +440,444 @@\t\t\t\t\t\tPartition: func() *int32 {\n\t\t\t\t\t\t\ti := int32(2)\n\t\t\t\t\t\t\treturn &i\n\t\t\t\t\t\t}()}\n\t\t\t\t}(),"
  },
  {
    "id" : "09f51e5c-4254-47cb-9bcf-cf868952c58b",
    "prId" : 45103,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45103#pullrequestreview-35367798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94ed499e-62f3-44f1-af9d-712be180939b",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "This calls `RunHostCmd` which is a simple exec into the pod though, no real host command as it was long ago. So this shouldn't take long (only does a file move). Hence, I guess this change doesn't make much of a difference. The issue we see is that `watch.Until` can return with `Keep waiting, received error from watch event: too old resource version: 7504 (7561)`.",
        "createdAt" : "2017-04-28T14:01:19Z",
        "updatedAt" : "2017-04-28T14:01:19Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "47ce53df056774c532d608b5b62ad8912624aad5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +318,322 @@\n\t\t\tBy(\"Verifying the 2nd pod is removed only when it becomes running and ready\")\n\t\t\tsst.RestoreProbe(ss, testProbe)\n\t\t\twatcher, err := f.ClientSet.Core().Pods(ns).Watch(metav1.SingleObject(\n\t\t\t\tmetav1.ObjectMeta{"
  },
  {
    "id" : "dbfd1589-6b00-4c63-a6c3-effa7a753f9e",
    "prId" : 42367,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42367#pullrequestreview-24686117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a23ba259-1e74-4771-a6f6-02dd81776b5b",
        "parentId" : null,
        "authorId" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "body" : "Shouldn't this be `WaitForStatus(ss, 2)`?",
        "createdAt" : "2017-03-02T05:49:33Z",
        "updatedAt" : "2017-03-03T19:25:47Z",
        "lastEditedBy" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "tags" : [
        ]
      },
      {
        "id" : "d50f4ba9-282c-4df3-94cb-b2c2c04ae03b",
        "parentId" : "a23ba259-1e74-4771-a6f6-02dd81776b5b",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "status.Replicas should always be 1 here regardless of the pod being ready or not. It seems that this is not the case actually and we treat this field as ReadyReplicas for ReplicaSets/Deployments (DaemonSets have a different name but there is still a separation between created and ready in the status). So the test seems fine but API-wise I think there is an unnecessary incosistency between the workload apis.",
        "createdAt" : "2017-03-02T09:34:59Z",
        "updatedAt" : "2017-03-03T19:25:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "08f95aff0f81139ba9ddd7f37961a6e479ef6fe2",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +218,222 @@\t\t\tBy(\"Before scale up finished setting 2nd pod to be not ready by breaking readiness probe\")\n\t\t\tsst.BreakProbe(ss, testProbe)\n\t\t\tsst.WaitForStatus(ss, 0)\n\t\t\tsst.WaitForRunningAndNotReady(2, ss)\n"
  }
]