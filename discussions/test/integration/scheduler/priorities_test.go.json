[
  {
    "id" : "29f2cdf7-740a-42e2-b4e7-a7603f606e0b",
    "prId" : 96062,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96062#pullrequestreview-522928050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a6879b2-f6f6-4b4d-af0c-537dbc64e832",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "nit: revert. The extra context doesn't hurt. Same for other Fatal calls.",
        "createdAt" : "2020-11-03T19:41:01Z",
        "updatedAt" : "2020-11-03T23:26:06Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "b2f0dd4a-965b-4352-be3c-c28f331947fa",
        "parentId" : "8a6879b2-f6f6-4b4d-af0c-537dbc64e832",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I deliberately let `createAndWaitForNodesInCache()` return the error with context - failed to create nodes, or fail to obtain nodes in cache. So let's just use `err` in the caller side.",
        "createdAt" : "2020-11-03T22:43:49Z",
        "updatedAt" : "2020-11-03T23:26:06Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "a146cb0aa0884cefe22f47aea505dc4878adcd6f",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +74,78 @@\t_, err := createAndWaitForNodesInCache(testCtx, \"testnode\", st.MakeNode(), 4)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Add a label to one of the nodes."
  },
  {
    "id" : "dd1df2ba-bc52-49a8-b355-9fa91577718f",
    "prId" : 95777,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95777#pullrequestreview-514848305",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0652380a-6a1c-4027-b85f-662c54485cb4",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "So, the intention of \"appending\" pods is that for the 2nd iteration, we have 3 existing pods, and 12 existing pods on the 3rd iteration?",
        "createdAt" : "2020-10-22T00:01:58Z",
        "updatedAt" : "2020-10-22T17:43:19Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "0d67c1ce-4fb9-4887-bd5f-6eb2a27ef8a8",
        "parentId" : "0652380a-6a1c-4027-b85f-662c54485cb4",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Yeah. I initially intended to recreate all Pods from zero, but no need, as all the Pods are equal.",
        "createdAt" : "2020-10-22T15:27:02Z",
        "updatedAt" : "2020-10-22T17:43:19Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6fb0bd308fff8810e46f2e52f617a73f9dbcfdd",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +410,414 @@\ttotalPodCnt := 0\n\tfor _, nPods := range []int{3, 9, 15} {\n\t\t// Append nPods each iteration.\n\t\tt.Run(fmt.Sprintf(\"%d-pods\", totalPodCnt+nPods), func(t *testing.T) {\n\t\t\tfor i := 0; i < nPods; i++ {"
  },
  {
    "id" : "8cf13339-13cd-4070-9bb0-4ebd39d26b12",
    "prId" : 80011,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80011#pullrequestreview-268043035",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df8545c8-4cb0-4851-a682-bed1efde570f",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "To make it easy to follow, why not use a list where list[i]=N means `i`th node has N pods on it. E.g., [~3~,1,1,1] means there are 3 pods on node 0 and node 0 is infeasible.",
        "createdAt" : "2019-07-29T22:06:33Z",
        "updatedAt" : "2019-08-01T17:42:45Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "caab8b74ba4bfd065248592ebb4174bb20a70be2",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +296,300 @@\t}{\n\t\t// note: naming starts at index 0\n\t\t// the symbol ~X~ means that node is infeasible\n\t\t{\n\t\t\tname: \"place pod on a ~0~/1/2/3 cluster with MaxSkew=1, node-1 is the preferred fit\","
  },
  {
    "id" : "22815e17-8b28-4342-8b8c-79e69eecc079",
    "prId" : 48847,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48847#pullrequestreview-49964474",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8cc057dd-9318-4f0a-921c-d10f60587503",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Bless you, I think this may be the first file in the entire codebase that has a comment explaining the purpose of the file.",
        "createdAt" : "2017-07-14T05:21:02Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "2612422a7ff00633877e5f1ec89290a887d0cc3f",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +25,29 @@)\n\n// This file tests the scheduler priority functions.\n\n// TestNodeAffinity verifies that scheduler's node affinity priority function"
  },
  {
    "id" : "65c848b9-f5ee-4825-bf97-ecc41ebb53f4",
    "prId" : 48847,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48847#pullrequestreview-50165488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9358e371-09e9-4af7-8a84-b5c1841fea13",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Shouldn't this be S1? It seems that this test is passing only by luck. Multiple MatchExpressions are ANDed, so the overall pod affinity term should be false on all nodes.",
        "createdAt" : "2017-07-14T22:22:32Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "17936c13-8d62-4ab0-95ab-2b6c36f93492",
        "parentId" : "9358e371-09e9-4af7-8a84-b5c1841fea13",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "There are two values in the array. The content of the first one which is `labelValue` is 'S1'.",
        "createdAt" : "2017-07-14T22:30:49Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "a4f3282c-c195-4c3a-ab7d-f2f68c93ff2f",
        "parentId" : "9358e371-09e9-4af7-8a84-b5c1841fea13",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Oops! You're right.\r\n",
        "createdAt" : "2017-07-14T22:32:17Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "2612422a7ff00633877e5f1ec89290a887d0cc3f",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +139,143 @@\t\t\t\t\t\t\t\t\t\tKey:      labelKey,\n\t\t\t\t\t\t\t\t\t\tOperator: metav1.LabelSelectorOpIn,\n\t\t\t\t\t\t\t\t\t\tValues:   []string{labelValue, \"S3\"},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t{"
  },
  {
    "id" : "a3a84643-c163-400d-bfd0-f7b720b001cc",
    "prId" : 48847,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48847#pullrequestreview-51281859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc82e14e-efdf-4aef-acba-ebe60e00c033",
        "parentId" : null,
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "I'd prefer using some random string, if the node clean up failed and make the cluster messed up, with random string, we can make sure the tests keep going on.",
        "createdAt" : "2017-07-20T02:56:15Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "dd8e3b06-28c2-4e10-b3b4-e776f350c972",
        "parentId" : "fc82e14e-efdf-4aef-acba-ebe60e00c033",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "That would make the test more resilient, but I guess the tests are not supposed to be resilient. Tests are preferred to fail fast when a PR breaks anything in the test or in the integration cluster.",
        "createdAt" : "2017-07-20T04:48:29Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "2268286f-358d-43ed-915a-b30e4790b85b",
        "parentId" : "fc82e14e-efdf-4aef-acba-ebe60e00c033",
        "authorId" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "body" : "I am not sure how much this will be flaky, but using random string should be considered to reduce the flaky tests, anyway, not a big problem, and will not against if you think it's ok",
        "createdAt" : "2017-07-20T06:30:33Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "8f672b1e-0513-4363-b383-ad8d8de0cdb9",
        "tags" : [
        ]
      },
      {
        "id" : "2e788772-349e-497a-8ede-4525cedda7ff",
        "parentId" : "fc82e14e-efdf-4aef-acba-ebe60e00c033",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Given that we delete all the nodes at the end of each test, I am not sure why a random string should reduce flakes.",
        "createdAt" : "2017-07-20T17:55:20Z",
        "updatedAt" : "2017-07-20T21:59:33Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "2612422a7ff00633877e5f1ec89290a887d0cc3f",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +95,99 @@\t}\n\ttopologyKey := \"node-topologykey\"\n\ttopologyValue := \"topologyvalue\"\n\tnodeLabels := map[string]string{\n\t\ttopologyKey: topologyValue,"
  }
]