[
  {
    "id" : "b27094e4-edf3-4e1d-bae7-9ac1d2ed2c8b",
    "prId" : 86028,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86028#pullrequestreview-329897733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37c9732c-e938-4ceb-99a3-8fab22af657a",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "since the pods has anti affinity to each other, the number of pods to schedule can't exceed the number of nodes (the topology used in the test)",
        "createdAt" : "2019-12-07T22:49:11Z",
        "updatedAt" : "2019-12-10T15:00:37Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "0883207c-e510-4785-aadd-266f4efb824c",
        "parentId" : "37c9732c-e938-4ceb-99a3-8fab22af657a",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Ack. Can you add the comment in the code?",
        "createdAt" : "2019-12-10T14:25:05Z",
        "updatedAt" : "2019-12-10T15:00:37Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "a28666a3-7069-452e-866a-f8c83ecdaa2b",
        "parentId" : "37c9732c-e938-4ceb-99a3-8fab22af657a",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "done.",
        "createdAt" : "2019-12-10T14:57:43Z",
        "updatedAt" : "2019-12-10T15:00:37Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "a051c591a1f1c2ba21738c04efd01e6646916d9d",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +75,79 @@\ttests := []struct{ nodes, existingPods, minPods int }{\n\t\t{nodes: 500, existingPods: 100, minPods: 400},\n\t\t{nodes: 5000, existingPods: 1000, minPods: 1000},\n\t}\n\ttestBasePod := makeBasePodWithPodAntiAffinity("
  },
  {
    "id" : "fda89fe4-1e9f-4310-bda6-60aeb2bac1ae",
    "prId" : 86028,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86028#pullrequestreview-329897733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0866b00d-d87f-4d04-b92d-884d1686b5d8",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Should we add LabelHostName for this as well?",
        "createdAt" : "2019-12-10T14:30:10Z",
        "updatedAt" : "2019-12-10T15:00:37Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "6b92f0a6-e071-48e7-a645-c69552624cad",
        "parentId" : "0866b00d-d87f-4d04-b92d-884d1686b5d8",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "no because we will end up having to schedule all the pods on one node, which does not really make sense unless we create tuples of pods with affinity to each other rather than having all pods having affinity to each other.\r\n\r\nThe test is still useful in that all the pods will still have affinity to each other at the zone level.",
        "createdAt" : "2019-12-10T14:59:29Z",
        "updatedAt" : "2019-12-10T15:00:37Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "a051c591a1f1c2ba21738c04efd01e6646916d9d",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +205,209 @@\t\tname := fmt.Sprintf(\"%vNodes/%vPods\", test.nodes, test.existingPods)\n\t\tb.Run(name, func(b *testing.B) {\n\t\t\tnodeStrategies := []testutils.CountToStrategy{{Count: test.nodes, Strategy: nodeStrategy}}\n\t\t\tbenchmarkScheduling(test.existingPods, test.minPods, nodeStrategies, testStrategy, b)\n\t\t})"
  },
  {
    "id" : "dea17246-aa9c-479e-a2ea-6c05e0951a3c",
    "prId" : 86028,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86028#pullrequestreview-329897733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d76ccf8-21a4-4c57-85b5-64b573991564",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Can we use a single namespace?",
        "createdAt" : "2019-12-10T14:32:48Z",
        "updatedAt" : "2019-12-10T15:00:37Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "a334059c-17a0-4c09-9f1b-cee7a5b5cb2d",
        "parentId" : "3d76ccf8-21a4-4c57-85b5-64b573991564",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "We could, but some features work at the namespace level, so I find it useful to test that.",
        "createdAt" : "2019-12-10T15:00:20Z",
        "updatedAt" : "2019-12-10T15:00:37Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "a051c591a1f1c2ba21738c04efd01e6646916d9d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +50,54 @@\t\t{nodes: 5000, existingPods: 5000, minPods: 1000},\n\t}\n\ttestNamespace  = \"sched-test\"\n\tsetupNamespace = \"sched-setup\"\n)"
  },
  {
    "id" : "92f4649e-762b-4bbd-b293-441e4481e9a4",
    "prId" : 82330,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82330#pullrequestreview-283996567",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92d5f7b0-9bf0-40b9-932c-739fc8ea4902",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "LGTM. Let's put a comment explaining that without this line we're taking the overhead of `defer()` into account.",
        "createdAt" : "2019-09-04T23:02:20Z",
        "updatedAt" : "2019-09-05T02:57:27Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "d3e8d930-d452-4288-82e9-ddbc0b3c33af",
        "parentId" : "92d5f7b0-9bf0-40b9-932c-739fc8ea4902",
        "authorId" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "body" : "LGTM @draveness \r\nAgree with @Huang-Wei . For readability, it is necessary to note why this line was added :)",
        "createdAt" : "2019-09-05T02:24:05Z",
        "updatedAt" : "2019-09-05T02:57:27Z",
        "lastEditedBy" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "tags" : [
        ]
      },
      {
        "id" : "63739a39-cdcd-48f6-8c35-5067b396a735",
        "parentId" : "92d5f7b0-9bf0-40b9-932c-739fc8ea4902",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "done, PTAL",
        "createdAt" : "2019-09-05T03:00:57Z",
        "updatedAt" : "2019-09-05T03:00:57Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "4105fea6eb0cf1e5dd56146ccf94755445988b9a",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +412,416 @@\n\t// Note: without this line we're taking the overhead of defer() into account.\n\tb.StopTimer()\n}\n"
  },
  {
    "id" : "d018b1ae-5486-4153-ba0b-c3b64b7e1fde",
    "prId" : 77397,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77397#pullrequestreview-254742965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d05840e-cf2f-4ef6-bb71-866658cadedc",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can we make the limit used in the test configurable?",
        "createdAt" : "2019-05-03T23:48:54Z",
        "updatedAt" : "2019-06-26T12:13:05Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "4f69ce2e-2cbe-40ff-9f49-6716af41ea87",
        "parentId" : "5d05840e-cf2f-4ef6-bb71-866658cadedc",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "IMO it's quite useless. In the most 'dense' test, each node gets 10 pods.",
        "createdAt" : "2019-06-26T11:19:15Z",
        "updatedAt" : "2019-06-26T12:13:05Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "8e195c26-9cfc-4f16-8afc-13bbf8ed223b",
        "parentId" : "5d05840e-cf2f-4ef6-bb71-866658cadedc",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Interesting why we don't try to max out the 100 pods/node limit in the benchmark.  I would prefer if we can test our \"worst case\" limits, ie ~100 volumes per node.",
        "createdAt" : "2019-06-26T16:47:04Z",
        "updatedAt" : "2019-06-26T16:47:04Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6430c0159fb75a40ebacd3b4dcdeb8b429c6433",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +202,206 @@\tdriverKey := util.GetCSIAttachLimitKey(testCSIDriver)\n\tallocatable := map[v1.ResourceName]string{\n\t\tv1.ResourceName(driverKey): fmt.Sprintf(\"%d\", util.DefaultMaxEBSVolumes),\n\t}\n\tvar count int32 = util.DefaultMaxEBSVolumes"
  },
  {
    "id" : "794b36c9-e1a5-408c-a0a4-6c53edaf36ee",
    "prId" : 69898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69898#pullrequestreview-165373667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ec342aa-4f9f-4be8-bb59-a5f03205cc93",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I just put this simple scenario as a start.\r\n\r\nBut feel free to comment if we'd like a more typical scenario like \"all nodes are split into X regions, Y zones\", etc.",
        "createdAt" : "2018-10-16T21:22:29Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "5259d09c38ee96bde61667b071d6778fd9ecf586",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +96,100 @@\t// The test strategy creates pods with affinity for each other.\n\ttestStrategy := testutils.NewCustomCreatePodStrategy(testBasePod)\n\tnodeStrategy := testutils.NewLabelNodePrepareStrategy(apis.LabelZoneFailureDomain, \"zone1\")\n\tfor _, test := range tests {\n\t\tname := fmt.Sprintf(\"%vNodes/%vPods\", test.nodes, test.existingPods)"
  },
  {
    "id" : "73bde88d-a043-4183-876e-b422d43cd4a5",
    "prId" : 69898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69898#pullrequestreview-165373944",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c16ea00-a72d-4098-a8e2-dcc98abf875c",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "similar like https://github.com/kubernetes/kubernetes/pull/69898/files#r225712449, it's the most basic case. We can extend it to have multiple terms, like region in X, zone in Y, etc.",
        "createdAt" : "2018-10-16T21:23:20Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "5259d09c38ee96bde61667b071d6778fd9ecf586",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +165,169 @@\tbasePod.Spec.Affinity = &v1.Affinity{\n\t\tPodAffinity: &v1.PodAffinity{\n\t\t\tRequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{\n\t\t\t\t{\n\t\t\t\t\tLabelSelector: &metav1.LabelSelector{"
  },
  {
    "id" : "16e31f22-5cba-4fd3-9339-a08a45f55137",
    "prId" : 69898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69898#pullrequestreview-166204258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac783208-e356-4ae6-9769-e40359693d91",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I am not sure where the code for [the perf dashboard](http://perf-dash.k8s.io/) lives, but it is worth checking to make sure that renaming this function does not affect the dashboard.",
        "createdAt" : "2018-10-18T00:05:49Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "2b5d42ab-02ff-44d3-bf5b-327619ff846c",
        "parentId" : "ac783208-e356-4ae6-9769-e40359693d91",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Sure. From below snapshot, it's very likely it's running `-bench=.` option:\r\n\r\n![image](https://user-images.githubusercontent.com/1425903/47125165-b9149180-d236-11e8-95f5-2c36914eb44d.png)\r\n\r\n@wojtek-t @shyamjvs If function `BenchmarkSchedulingAntiAffinity` is renamed to  `BenchmarkSchedulingPodAntiAffinity`, I guess old datapoints would stop growing and become stale? And maybe we need to do some trick (manually rename the local dataset file) to make old data connect with new data, so that the graph is still consistent.",
        "createdAt" : "2018-10-18T01:11:23Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "4e9ba8aa-73da-46a7-bd7c-f780029dbd4a",
        "parentId" : "ac783208-e356-4ae6-9769-e40359693d91",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I'm not familiar with perf-dash.\r\n@krzysied - can you please suggest something?",
        "createdAt" : "2018-10-18T06:28:06Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "8da92a32-3ffe-45b3-b02a-0c8869b31b61",
        "parentId" : "ac783208-e356-4ae6-9769-e40359693d91",
        "authorId" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "body" : "> I guess old datapoints would stop growing and become stale\r\n\r\nYes. The new datapoints will by assigned to new function name. The old one will remain unchanged.\r\n\r\n> And maybe we need to do some trick (manually rename the local dataset file) to make old data connect with new data, so that the graph is still consistent.\r\n\r\nCan be done. The question is whether we need this.\r\nRegarding benchmarks, Perfdash presents 100 newest results. Assuming that there is one test every hour, after a week there will be only results with the new function name available.",
        "createdAt" : "2018-10-18T08:56:40Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "tags" : [
        ]
      },
      {
        "id" : "528eaee9-b7c0-457a-8810-3a7d37cde134",
        "parentId" : "ac783208-e356-4ae6-9769-e40359693d91",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Thanks @krzysied @wojtek-t , then don't bother to \"migrate\" old dataset :)",
        "createdAt" : "2018-10-18T17:14:59Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "5259d09c38ee96bde61667b071d6778fd9ecf586",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +58,62 @@// PodAntiAffinity rules when the cluster has various quantities of nodes and\n// scheduled pods.\nfunc BenchmarkSchedulingPodAntiAffinity(b *testing.B) {\n\ttests := []struct{ nodes, existingPods, minPods int }{\n\t\t{nodes: 500, existingPods: 250, minPods: 250},"
  },
  {
    "id" : "5b7129a0-7023-4b15-a2c7-f974f1478d09",
    "prId" : 69898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69898#pullrequestreview-166726208",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a37716ca-f11f-4e01-ac41-a6b72bf4a563",
        "parentId" : null,
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "I am a bit curious why we had different number of nodes and pods in `BenchmarkScheduling` VS `BenchmarkSchedulingPodAffinity/AntiAffinity` from the beginning? It should've been a good chance to do cross comparison.",
        "createdAt" : "2018-10-19T06:43:17Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "00ed71d2-022d-4558-8cfb-8e03ca5eba5b",
        "parentId" : "a37716ca-f11f-4e01-ac41-a6b72bf4a563",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "In the past, we could hardly go beyond 500 nodes for inter-pod affinity/anti-affinity benchmarks without having test timeouts. With the improved performance of the feature, I think we should be able to run the test for 1000 node clusters with no problem.",
        "createdAt" : "2018-10-19T23:48:06Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "5259d09c38ee96bde61667b071d6778fd9ecf586",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +85,89 @@\ttests := []struct{ nodes, existingPods, minPods int }{\n\t\t{nodes: 500, existingPods: 250, minPods: 250},\n\t\t{nodes: 500, existingPods: 5000, minPods: 250},\n\t\t{nodes: 1000, existingPods: 1000, minPods: 500},\n\t}"
  },
  {
    "id" : "c78b5948-bee3-487a-b041-b0dda9d415c7",
    "prId" : 69898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69898#pullrequestreview-168045679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46d3e2f9-fb1b-46f4-a810-65de919f62e5",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Now that you are at it, could you add nodes: 1000, existing: 1000, minPods: 500 test here and in the next test?",
        "createdAt" : "2018-10-19T23:50:55Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "a455a0d5-e049-47a7-a143-7449371cefae",
        "parentId" : "46d3e2f9-fb1b-46f4-a810-65de919f62e5",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "@bsalamat to confirm, would you like the following tests to be added this 1000-node test entry?\r\n\r\n- BenchmarkSchedulingPodAntiAffinity\r\n- BenchmarkSchedulingPodAffinity\r\n- BenchmarkSchedulingNodeAffinity\r\n\r\nAnd after that, our cases would be like this:\r\n\r\n- BenchmarkScheduling (have no affinity)\r\n    \r\n    ```go\r\n    {nodes: 100, existingPods: 0, minPods: 100},\r\n    {nodes: 100, existingPods: 1000, minPods: 100},\r\n    {nodes: 1000, existingPods: 0, minPods: 100},\r\n    {nodes: 1000, existingPods: 1000, minPods: 100},\r\n    ```\r\n\r\n- BenchmarkSchedulingPodAntiAffinity\r\n\r\n    ```go\r\n    {nodes: 500, existingPods: 250, minPods: 250},\r\n    {nodes: 500, existingPods: 5000, minPods: 250},\r\n    {nodes: 1000, existingPods: 1000, minPods: 500},\r\n    ```\r\n\r\n- BenchmarkSchedulingPodAffinity\r\n\r\n    ```go\r\n    {nodes: 500, existingPods: 250, minPods: 250},\r\n    {nodes: 500, existingPods: 5000, minPods: 250},\r\n    {nodes: 1000, existingPods: 1000, minPods: 500},\r\n    ```\r\n\r\n- BenchmarkSchedulingNodeAffinity\r\n\r\n    ```go\r\n    {nodes: 500, existingPods: 250, minPods: 250},\r\n    {nodes: 500, existingPods: 5000, minPods: 250},\r\n    {nodes: 1000, existingPods: 1000, minPods: 500},\r\n    ```",
        "createdAt" : "2018-10-20T00:05:51Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "14e698d5-19aa-4839-b3ad-679fde7f864c",
        "parentId" : "46d3e2f9-fb1b-46f4-a810-65de919f62e5",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "ping @bsalamat ^^",
        "createdAt" : "2018-10-24T05:23:19Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "ddfd0a56-b461-4059-936b-db68f415ffea",
        "parentId" : "46d3e2f9-fb1b-46f4-a810-65de919f62e5",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Seems reasonable from my side :-)",
        "createdAt" : "2018-10-24T14:56:10Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "cc52cb01-874a-4630-b8ea-8e498bac28b9",
        "parentId" : "46d3e2f9-fb1b-46f4-a810-65de919f62e5",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "@Huang-Wei Looks good for now. I want to be able to try a larger number of nodes, but it has the risk of timing out in our CI/CD. Let's go with 1000 for now.",
        "createdAt" : "2018-10-24T17:49:20Z",
        "updatedAt" : "2018-10-24T18:06:46Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "440da663-b967-4084-bdbe-62908fe10e73",
        "parentId" : "46d3e2f9-fb1b-46f4-a810-65de919f62e5",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Done. Thanks @resouer @bsalamat .",
        "createdAt" : "2018-10-24T18:06:52Z",
        "updatedAt" : "2018-10-24T18:06:53Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "5259d09c38ee96bde61667b071d6778fd9ecf586",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +61,65 @@\ttests := []struct{ nodes, existingPods, minPods int }{\n\t\t{nodes: 500, existingPods: 250, minPods: 250},\n\t\t{nodes: 500, existingPods: 5000, minPods: 250},\n\t\t{nodes: 1000, existingPods: 1000, minPods: 500},\n\t}"
  }
]