[
  {
    "id" : "50615eae-b784-49cc-a46d-d2c3fb2d2be7",
    "prId" : 99212,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99212#pullrequestreview-594326392",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e95558e9-e407-4e66-a2f8-e4fe74e53b92",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "This test is ensuring that the existing behavior somewhat still holds. That is good. We should run it with the feature gate enabled and disabled (I would remove the `Logarithmic` part from the test name)\r\n\r\nAnother test we could have is rather opposite: have all the pods run at roughly the same time, downscale, and see if some level of randomness holds. Do you think something like this is possible? Maybe we can run the same test X times and ensure that at least in one of them, the removed pod was not the absolute youngest.\r\n\r\nIn the test plan we also suggested emulating the story https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2185-random-pod-select-on-replicaset-downscale#test-plan\r\nWe can do that in a follow up.",
        "createdAt" : "2021-02-18T23:19:25Z",
        "updatedAt" : "2021-03-05T20:58:35Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "83146c83-44ca-4faf-be2e-3c1690dd5898",
        "parentId" : "e95558e9-e407-4e66-a2f8-e4fe74e53b92",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "> have all the pods run at roughly the same time, downscale, and see if some level of randomness holds ... Maybe we can run the same test X times\r\n\r\nThere is an inherent problem with randomness in that we can never be certain it will be the randomness we expect :) Something like this will introduce flakes, which can be minimized by increasing the number of test runs, but still always possible. Are there examples of other tests which take a similar approach?\r\n\r\nWhat we really wish to show from such a test is that pods created on a similar logarithmic time scale have an equal chance of being chosen for downscaling. In other words, that their ranks are calculated properly. I believe the unit tests cover this level of detail sufficiently. \r\n\r\nThe integration test is simply showing that the basic logic still works when removed from the vacuum of unit tests. I think that the user story you linked will make a good e2e, and I could actually add that to this PR (I don't see any reason to wait, and that will flesh out test cases here more comfortably). Wdyt?",
        "createdAt" : "2021-02-19T15:52:37Z",
        "updatedAt" : "2021-03-05T20:58:35Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "08840d05-c6c5-4a69-8010-c231333eeac4",
        "parentId" : "e95558e9-e407-4e66-a2f8-e4fe74e53b92",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "> I believe the unit tests cover this level of detail sufficiently.\r\n\r\nI agree, but thanks for giving it a thought.\r\n\r\nStill, modify the test to run for feature gate enabled and disabled.\r\n\r\nFine with me to add the 2e2 test for the user story in this PR.",
        "createdAt" : "2021-02-19T15:58:10Z",
        "updatedAt" : "2021-03-05T20:58:35Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8d105ab724ccdb45b4ff380d84fd356e833991e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +496,500 @@}\n\nfunc TestLogarithmicScaleDown(t *testing.T) {\n\tdefer featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.LogarithmicScaleDown, true)()\n\ts, closeFn, rm, informers, c := rmSetup(t)"
  },
  {
    "id" : "045bed85-965c-490d-a9d4-c0b90cdd86cc",
    "prId" : 49429,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49429#pullrequestreview-75508689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77d2ec13-4c3a-49fd-9437-7fe1c34161c2",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Do we need to keep rs and rc integration test in sync, I mean, do we need this comment that's removed from original rc/rs controller code? \r\n\r\n```go\r\n// If you make changes to this file, you should also make the corresponding change in xxx.\r\n```",
        "createdAt" : "2017-11-09T00:54:52Z",
        "updatedAt" : "2017-11-09T01:06:51Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "b8e120b0-fa64-4f75-93f4-fb2d7ae3d19a",
        "parentId" : "77d2ec13-4c3a-49fd-9437-7fe1c34161c2",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "At the time, I decided to leave it out because it made sense to me that a test of RC behavior should be sort of frozen in time, because RC is deprecated and we don't intend to add new behavior to it. Keeping the RC tests fixed could also help make sure we don't accidentally change guaranteed parts of RC behavior while adding new code to ReplicaSetController.\r\n\r\nHowever, if the goal is for RC to keep evolving alongside RS, just with the old selector type, then it would make sense to ensure we update their tests in sync.",
        "createdAt" : "2017-11-09T16:52:00Z",
        "updatedAt" : "2017-11-09T16:52:01Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      }
    ],
    "commit" : "97cef269bf7c2ff105071de6047388b367dc767c",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +40,44 @@\t\"k8s.io/kubernetes/test/integration/framework\"\n)\n\nconst (\n\tinterval = 100 * time.Millisecond"
  },
  {
    "id" : "9a258b7f-1cea-4b23-80ae-6214467f0639",
    "prId" : 29798,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2121c7ce-4002-47c2-9381-edf85a5b8cb1",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Won't block the fix on this, but it would be good to move this somewhere common so it's not duplicated.\n",
        "createdAt" : "2016-07-29T23:12:23Z",
        "updatedAt" : "2016-07-29T23:12:23Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "7797ff2ead252faa57de4fc99290a17b70336837",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +158,162 @@// running the RC manager to prevent the rc manager from creating new pods\n// rather than adopting the existing ones.\nfunc waitToObservePods(t *testing.T, podInformer controllerframwork.SharedIndexInformer, podNum int) {\n\tif err := wait.Poll(10*time.Second, 60*time.Second, func() (bool, error) {\n\t\tobjects := podInformer.GetIndexer().List()"
  }
]