[
  {
    "id" : "f5135a32-242a-484b-896f-6ee2984c22a3",
    "prId" : 19083,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb3bdfc2-8b96-4151-b53e-0fe316eb43be",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Shouldn't we remove the 1st pod as well?  \n",
        "createdAt" : "2016-01-18T15:28:48Z",
        "updatedAt" : "2016-01-20T07:36:37Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "cbc3563b-94f0-43a2-ad36-f6fbdef5ea20",
        "parentId" : "eb3bdfc2-8b96-4151-b53e-0fe316eb43be",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "+1\n",
        "createdAt" : "2016-01-18T22:16:57Z",
        "updatedAt" : "2016-01-20T07:36:37Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "0202a206b81ccc579feff77ea4593bfb8ed2c6e4",
    "line" : null,
    "diffHunk" : "@@ -1,1 +551,555 @@\t}\n\n\t// 6. Make another pod with different name, same resource request\n\tpodResource.ObjectMeta.Name = \"pod-test-allocatable2\"\n\ttestAllocPod2, err := restClient.Pods(api.NamespaceDefault).Create(podResource)"
  },
  {
    "id" : "1a30fd86-b865-4ad6-bc82-b4d230511310",
    "prId" : 17865,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "576402a6-0cc7-4576-8adf-5ce747aaa225",
        "parentId" : null,
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "This test is too long.\nWe can improve it by changing to use table test.\n\nHow about the following workflow:\n- create two schedulers\n- craft tables of pods of a variety of combinations\n  - default, \"foo\", \"non-exists\", combinatory ones (do we support it?)\n",
        "createdAt" : "2015-12-04T18:04:48Z",
        "updatedAt" : "2015-12-22T03:07:07Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      },
      {
        "id" : "e5082146-88be-43c1-89ec-8050fbbd5f0a",
        "parentId" : "576402a6-0cc7-4576-8adf-5ce747aaa225",
        "authorId" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "body" : "What does it mean by \"combinatory ones\"? With two scheduler annotations? No, I don't think we support this.\n",
        "createdAt" : "2015-12-07T07:22:45Z",
        "updatedAt" : "2015-12-22T03:07:07Z",
        "lastEditedBy" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9f3607292a244125145b14b57f11622817ad66c",
    "line" : null,
    "diffHunk" : "@@ -1,1 +276,280 @@\nfunc TestMultiScheduler(t *testing.T) {\n\tframework.DeleteAllEtcdKeys()\n\n\tvar m *master.Master"
  },
  {
    "id" : "33f9bfc1-6275-42af-b41f-60f320646661",
    "prId" : 17865,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe848bc0-9527-4a8c-957f-289eab538117",
        "parentId" : null,
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "Why do we create a node here? We just need to see if something is picked up by a scheduler. Nonetheless schedulers don't need to actually schedule it. Right?\n\nIf we create a node, does it need to health-checking it?\n",
        "createdAt" : "2015-12-04T18:05:24Z",
        "updatedAt" : "2015-12-22T03:07:07Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      },
      {
        "id" : "6d0c3fd3-93a0-453c-85e7-7534c9539c75",
        "parentId" : "fe848bc0-9527-4a8c-957f-289eab538117",
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "Looks like a node won't be deleted unless it has a cloud provider. But the node status will be changed and won't be listed to scheduler.\n\nNonetheless, the main question is whether we could save the steps on using nodes. The purpose here is to check if a pod is picked up by a scheduler. Correct?\n",
        "createdAt" : "2015-12-05T00:14:03Z",
        "updatedAt" : "2015-12-22T03:07:07Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      },
      {
        "id" : "c4880f9b-a915-40c3-ab47-95f3ebe0ce06",
        "parentId" : "fe848bc0-9527-4a8c-957f-289eab538117",
        "authorId" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "body" : "Hi, I agree with you on this. The true intention is to test if a pod is picked by the correct scheduler, and this makes the scheduling action unnecessary. However, under current mechanism (ignoring pods in the queue if it does not meet the scheduler's name), I cannot test if a pod is \"picked\" by its scheduler except for checking its \"picking condition\" which is already tested in the unit-test.\n\nIn a word, although the current integration test is \"over-tested\" (I cannot find better word, sorry), it does test the behaviors of multiple schedulers. I cannot come up with a better plan... @davidopp \n\nRegarding the node health checking, I think it is not needed in this scenario, as is shown in the test. (If the node the unhealthy by default when created, the test would not pass)\n",
        "createdAt" : "2015-12-07T07:29:40Z",
        "updatedAt" : "2015-12-22T03:07:07Z",
        "lastEditedBy" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9f3607292a244125145b14b57f11622817ad66c",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +320,324 @@\n\t// 2. create a node\n\tnode := &api.Node{\n\t\tObjectMeta: api.ObjectMeta{Name: \"node-multi-scheduler-test-node\"},\n\t\tSpec:       api.NodeSpec{Unschedulable: false},"
  },
  {
    "id" : "fcdf6286-fe39-48c8-957e-a754dfa4134b",
    "prId" : 15202,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4f11767-3f6d-47af-84d9-44f34c23ae9a",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Is it a good idea for the scheduler and the test to use the same client? I guess it works...\n",
        "createdAt" : "2015-10-15T08:58:20Z",
        "updatedAt" : "2015-10-15T08:58:20Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "377e5c533b64b56892a8fdf555206f69824a5d18",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +341,345 @@\tdefer close(schedulerConfig.StopEverything)\n\n\tmakeNNodes(c, 1000)\n\tN := b.N\n\tb.ResetTimer()"
  },
  {
    "id" : "6d0446c0-e512-4473-9afa-c63176325629",
    "prId" : 7668,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e8b645d-fa95-4cdc-830d-c234c8aacef1",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "To make the test faster, you could look for an event from the scheduler saying it can't schedule the pod.\n",
        "createdAt" : "2015-05-06T20:20:09Z",
        "updatedAt" : "2015-05-07T01:33:19Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "f0ed58e1-6ae2-459f-8470-d1d2506757de",
        "parentId" : "5e8b645d-fa95-4cdc-830d-c234c8aacef1",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Discussed offline, will do in a follow up pr\n",
        "createdAt" : "2015-05-06T21:31:56Z",
        "updatedAt" : "2015-05-07T01:33:19Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b0607cf0b7619faba71e2f30b5240c42e562e4a",
    "line" : 231,
    "diffHunk" : "@@ -1,1 +245,249 @@\t\t}\n\n\t\t// There are no schedulable nodes - the pod shouldn't be scheduled.\n\t\terr = wait.Poll(time.Second, time.Second*10, podScheduled(restClient, myPod.Namespace, myPod.Name))\n\t\tif err == nil {"
  },
  {
    "id" : "a1318f70-a3c3-445c-871a-721edcd6781c",
    "prId" : 6901,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9267467a-938e-44a7-840c-0801520dc318",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Does a `defer close(schedulerConfig.StopEverything)` help?\n",
        "createdAt" : "2015-04-17T21:05:38Z",
        "updatedAt" : "2015-04-20T10:53:55Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "da3cbac2-84c7-4f6d-8378-d3755ac070ae",
        "parentId" : "9267467a-938e-44a7-840c-0801520dc318",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Yes - it helps. Thanks!\n",
        "createdAt" : "2015-04-20T10:54:08Z",
        "updatedAt" : "2015-04-20T10:54:08Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "329d41828f5522264d6350e0cdb99d5491c0103d",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +79,83 @@\tschedulerConfig.Recorder = eventBroadcaster.NewRecorder(api.EventSource{Component: \"scheduler\"})\n\teventBroadcaster.StartRecordingToSink(client.Events(\"\"))\n\tscheduler.New(schedulerConfig).Run()\n\n\tdefer close(schedulerConfig.StopEverything)"
  }
]