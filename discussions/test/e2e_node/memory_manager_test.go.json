[
  {
    "id" : "91255d92-8042-4d55-af93-5a9acf945c0e",
    "prId" : 99072,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99072#pullrequestreview-606265077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36678cc3-102a-40dd-8684-e35eb9c0e835",
        "parentId" : null,
        "authorId" : "38db888a-e196-4a62-9357-0fcd42a50015",
        "body" : "Can we add a comment to make it more clear that the `expectedSize` here is size in bytes?",
        "createdAt" : "2021-03-03T17:42:57Z",
        "updatedAt" : "2021-03-08T13:04:14Z",
        "lastEditedBy" : "38db888a-e196-4a62-9357-0fcd42a50015",
        "tags" : [
        ]
      },
      {
        "id" : "d95226d0-f98f-4183-97e9-13f2815deb10",
        "parentId" : "36678cc3-102a-40dd-8684-e35eb9c0e835",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "done",
        "createdAt" : "2021-03-08T13:03:45Z",
        "updatedAt" : "2021-03-08T13:04:14Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      }
    ],
    "commit" : "cff9ecd31775fc199c8a2bf180d5d25c8bc24bc9",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +345,349 @@\n\t\t\t\t// 512 Mb, the expected size in bytes\n\t\t\t\texpectedSize := int64(hugepagesCount * hugepagesSize2M * 1024)\n\t\t\t\tif size != expectedSize {\n\t\t\t\t\treturn fmt.Errorf(\"the actual size %d is different from the expected one %d\", size, expectedSize)"
  },
  {
    "id" : "49c465ed-98fa-44f3-99f6-5da3a9bb4b76",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-527043353",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "889425cb-b623-4640-a0a6-96983208abc1",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "i would also add tests that do the following:\r\n\r\n1) create more than one pod\r\n2) create pods that dont fit\r\n3) pods that use init containers\r\n\r\nabove should be done before moving out of alpha",
        "createdAt" : "2020-11-03T17:43:24Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "ec5a6646-123a-4e6f-a6a2-8986c7df16d6",
        "parentId" : "889425cb-b623-4640-a0a6-96983208abc1",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "I will start to work on it.",
        "createdAt" : "2020-11-04T12:47:54Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "b3ff7d51-1b53-473a-863b-18922354a03c",
        "parentId" : "889425cb-b623-4640-a0a6-96983208abc1",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "> 2\\. create pods that dont fit\r\n\r\n@derekwaynecarr  This one will require the multi NUMA host because when we are talking about a single NUMA node host, the pod will be rejected by the scheduler and not by the topology manager. I can add it, but it will be always skipped under the CI because k8s CI does not have support for multi-NUMA tests. I have some plans to add such support via fake NUMA kernel arguments, but it will need to introduce new functionality to the e2e node remote runner. Please give me know what do you prefer:\r\n- to add it now, but it will be always skipped\r\n- wait until we will have some infrastructure support to run multiple NUMA nodes tests",
        "createdAt" : "2020-11-04T16:06:11Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "3c3b14f2-51cb-40fb-b3d7-8e89761a56a1",
        "parentId" : "889425cb-b623-4640-a0a6-96983208abc1",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "add it now, and skip (prior to exit alpha).  downstream testing on alternative infrastructures would benefit.",
        "createdAt" : "2020-11-09T19:48:11Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "680ecfe2-1af2-49d7-baf8-67c8cc225dad",
        "parentId" : "889425cb-b623-4640-a0a6-96983208abc1",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "Already added it under - https://github.com/kubernetes/kubernetes/pull/95479/commits/da84cf6fea8faf1db61524c54ba9ff63bf2a2853#diff-a059dfe1f8609ac5352a3b897b3a64c682c8bac087aad0483e6bf8dd72a99850R459",
        "createdAt" : "2020-11-10T10:06:44Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 247,
    "diffHunk" : "@@ -1,1 +245,249 @@// Serial because the test updates kubelet configuration.\nvar _ = SIGDescribe(\"Memory Manager [Serial] [Feature:MemoryManager][NodeAlphaFeature:MemoryManager]\", func() {\n\t// TODO: add more complex tests that will include interaction between CPUManager, MemoryManager and TopologyManager\n\tvar (\n\t\tallNUMANodes             []int"
  },
  {
    "id" : "0964eff6-c6c3-4850-ad7f-dbab39e1a8ef",
    "prId" : 104076,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/104076#pullrequestreview-720379672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af9fcbc4-e436-475c-90e2-294a7a1a59e2",
        "parentId" : null,
        "authorId" : "b7c04289-77b0-4d05-90b7-3bde23e2b0bf",
        "body" : "If you look at the `stopKubelet` func, you see the comment: `stopKubelet will kill the running kubelet, and returns a func that will restart the process again`.\r\n\r\nHave you tried using something like:\r\n\r\n```go\r\nstartKubelet := stopKubelet()\r\n// wait until the kubelet health check will fail\r\ngomega.Eventually(func() bool {\r\n\treturn kubeletHealthCheck(kubeletHealthCheckURL)\r\n}, time.Minute, time.Second).Should(gomega.BeFalse())\r\n\r\nstartKubelet()\r\n```\r\n\r\ninstead?",
        "createdAt" : "2021-08-02T15:43:01Z",
        "updatedAt" : "2021-08-02T15:47:13Z",
        "lastEditedBy" : "b7c04289-77b0-4d05-90b7-3bde23e2b0bf",
        "tags" : [
        ]
      },
      {
        "id" : "ea31f679-34f5-41d3-ba6e-998cfd1924fd",
        "parentId" : "af9fcbc4-e436-475c-90e2-294a7a1a59e2",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "@odinuge yes, i did. it did not work.",
        "createdAt" : "2021-08-02T15:59:06Z",
        "updatedAt" : "2021-08-02T15:59:06Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      },
      {
        "id" : "a2d703da-58be-4925-a748-4b01d56a2ca1",
        "parentId" : "af9fcbc4-e436-475c-90e2-294a7a1a59e2",
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "Do we know why?",
        "createdAt" : "2021-08-02T16:12:01Z",
        "updatedAt" : "2021-08-02T16:12:05Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "7b167c8a-99d3-4560-8b02-f7e448b4f07f",
        "parentId" : "af9fcbc4-e436-475c-90e2-294a7a1a59e2",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "`systemd` has `active` vs `exited` played some role in it. ( https://askubuntu.com/questions/1304142/systemctl-status-active-exited-vs-running ) i solely depend on the CI for running this, so `restart` basically worked fine.",
        "createdAt" : "2021-08-02T16:17:23Z",
        "updatedAt" : "2021-08-02T16:17:23Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "dab19517e5a7a7d8aea1b5f4310b164fa7c32a42",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +342,346 @@\n\t\t\t// stop the kubelet and wait until the server will restart it automatically\n\t\t\tstopKubelet()\n\n\t\t\t// wait until the kubelet health check will fail"
  }
]