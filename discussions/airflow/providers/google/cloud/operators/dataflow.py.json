[
  {
    "id" : "14c2903c-848a-4e76-98c8-cb9e28113297",
    "prId" : 8145,
    "prUrl" : "https://github.com/apache/airflow/pull/8145#pullrequestreview-391011434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f4abaa5-c7dc-4ddb-8bde-6cd453d42de8",
        "parentId" : null,
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "Nice! But needs UPDATING.md entry",
        "createdAt" : "2020-04-09T18:09:53Z",
        "updatedAt" : "2020-04-12T21:39:47Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f33e29ec38375da3d79fc3df5ce5e2b2a8a0bb7d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +173,177 @@            dataflow_default_options: Optional[dict] = None,\n            options: Optional[dict] = None,\n            project_id: Optional[str] = None,\n            gcp_conn_id: str = 'google_cloud_default',\n            delegate_to: Optional[str] = None,"
  },
  {
    "id" : "f98f38a1-d8e7-4d2f-ba34-73d6007388da",
    "prId" : 8145,
    "prUrl" : "https://github.com/apache/airflow/pull/8145#pullrequestreview-391011434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a45c7b6-aa35-4ad1-99ae-9b2ebf924d02",
        "parentId" : null,
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "Needs UPDATING.md entry",
        "createdAt" : "2020-04-09T18:11:50Z",
        "updatedAt" : "2020-04-12T21:39:47Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f33e29ec38375da3d79fc3df5ce5e2b2a8a0bb7d",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +444,448 @@            py_requirements: Optional[List[str]] = None,\n            py_system_site_packages: bool = False,\n            project_id: Optional[str] = None,\n            gcp_conn_id: str = 'google_cloud_default',\n            delegate_to: Optional[str] = None,"
  },
  {
    "id" : "27f2083d-3a0c-4619-bba0-2c3de5ad046b",
    "prId" : 8550,
    "prUrl" : "https://github.com/apache/airflow/pull/8550#pullrequestreview-499463601",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33a70c48-9ae6-4817-8185-2b6322f8eecd",
        "parentId" : null,
        "authorId" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "body" : "Do you need to call this if job is no longer running?",
        "createdAt" : "2020-04-27T20:42:33Z",
        "updatedAt" : "2020-10-16T10:12:46Z",
        "lastEditedBy" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "tags" : [
        ]
      },
      {
        "id" : "9158e1dd-31d4-4848-acff-c2f986563ddc",
        "parentId" : "33a70c48-9ae6-4817-8185-2b6322f8eecd",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "Good point. I will change it",
        "createdAt" : "2020-04-27T22:13:02Z",
        "updatedAt" : "2020-10-16T10:12:46Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "da33e0b6-aca3-40e7-9ef8-e1c090aaaebd",
        "parentId" : "33a70c48-9ae6-4817-8185-2b6322f8eecd",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "I changed it here https://github.com/apache/airflow/pull/8550/commits/1eac7722208d6f7e7723eee44c676c4f0bc5065a",
        "createdAt" : "2020-09-29T11:34:17Z",
        "updatedAt" : "2020-10-16T10:12:46Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      },
      {
        "id" : "2b4a0b16-aec4-44aa-b3b7-2ba1b6ace052",
        "parentId" : "33a70c48-9ae6-4817-8185-2b6322f8eecd",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "Improved it here https://github.com/apache/airflow/pull/8550/commits/4b78b27d855cd61d97ffd3d2c2591b34a1a2f226",
        "createdAt" : "2020-09-30T14:01:18Z",
        "updatedAt" : "2020-10-16T10:12:46Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "e204c619e44f8eaf60769273925bb60b968231d2",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +515,519 @@        self.log.info(\"On kill.\")\n        if self.job_id:\n            self.hook.cancel_job(job_id=self.job_id, project_id=self.project_id)\n\n"
  },
  {
    "id" : "8d8a2194-3802-47df-8838-901e5ffe6115",
    "prId" : 8553,
    "prUrl" : "https://github.com/apache/airflow/pull/8553#pullrequestreview-401331956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44359764-93d0-453f-becf-a6c23bf8e8e0",
        "parentId" : null,
        "authorId" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "body" : "What is gcp_conn_id?",
        "createdAt" : "2020-04-27T20:37:06Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "tags" : [
        ]
      },
      {
        "id" : "c2b87b24-7740-4853-b821-934110f0b332",
        "parentId" : "44359764-93d0-453f-becf-a6c23bf8e8e0",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "Airflow saves all credentials(MySQL, GCP, AWS, and other) in one table in the database. It's called `connection`. This is the entry ID in this table.",
        "createdAt" : "2020-04-27T21:15:00Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      }
    ],
    "commit" : "92d18b05df81205d9d36863dca57c4358b57ee53",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +548,552 @@    :param gcp_conn_id: The connection ID to use connecting to Google Cloud\n        Platform.\n    :type gcp_conn_id: str\n    :param delegate_to: The account to impersonate, if any.\n        For this to work, the service account making the request must have"
  },
  {
    "id" : "74598763-4136-4bc5-b24f-b79768f721db",
    "prId" : 8553,
    "prUrl" : "https://github.com/apache/airflow/pull/8553#pullrequestreview-512345580",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c01ebf60-b568-426c-b4db-71b11702e2f2",
        "parentId" : null,
        "authorId" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "body" : "Do you want to call this even if job is cancelled/stopped/finished?",
        "createdAt" : "2020-04-27T20:38:37Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "tags" : [
        ]
      },
      {
        "id" : "513c0a1e-faed-4692-88ff-552647f3b724",
        "parentId" : "c01ebf60-b568-426c-b4db-71b11702e2f2",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "Good point. I will skip jobs in the terminal state.",
        "createdAt" : "2020-04-27T21:16:53Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "46b4f475-12db-4f44-83b9-406ff0894bb7",
        "parentId" : "c01ebf60-b568-426c-b4db-71b11702e2f2",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "I fixed it in hook.",
        "createdAt" : "2020-10-20T05:40:32Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "92d18b05df81205d9d36863dca57c4358b57ee53",
    "line" : 215,
    "diffHunk" : "@@ -1,1 +618,622 @@        self.log.info(\"On kill.\")\n        if self.job_id:\n            self.hook.cancel_job(job_id=self.job_id, project_id=self.project_id)\n\n"
  },
  {
    "id" : "0823e826-7c00-46b3-a5cb-f262a37ff39f",
    "prId" : 8553,
    "prUrl" : "https://github.com/apache/airflow/pull/8553#pullrequestreview-401313690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be73e54c-0780-49dd-840b-c952b9154171",
        "parentId" : null,
        "authorId" : "eefb8405-68ea-4eae-a9d5-e1df1635fcee",
        "body" : "@ibzib would be a good reviewer here",
        "createdAt" : "2020-04-27T20:50:38Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "eefb8405-68ea-4eae-a9d5-e1df1635fcee",
        "tags" : [
        ]
      }
    ],
    "commit" : "92d18b05df81205d9d36863dca57c4358b57ee53",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +527,531 @@class DataflowStartSqlJobOperator(BaseOperator):\n    \"\"\"\n    Starts Dataflow SQL query.\n\n    :param job_name: The unique name to assign to the Cloud Dataflow job."
  },
  {
    "id" : "820b0fbd-181e-474e-a6cb-9af8802710f4",
    "prId" : 8553,
    "prUrl" : "https://github.com/apache/airflow/pull/8553#pullrequestreview-512423505",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4571af3-5066-43bc-92c6-340ad4fd879b",
        "parentId" : null,
        "authorId" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "body" : "Dataflow has deliberately been trying to move away from using a default location, because many users may not realize that their job is running in us-central1 even if that is not intended.\r\n",
        "createdAt" : "2020-05-05T19:20:43Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "tags" : [
        ]
      },
      {
        "id" : "2e80f38b-6fe8-4155-b65c-1b640a39525b",
        "parentId" : "a4571af3-5066-43bc-92c6-340ad4fd879b",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "@ibzib I think we have to change it to all operators in the future. To keep consistency across all dataflow operators I would like to keep it for now.",
        "createdAt" : "2020-10-20T07:50:32Z",
        "updatedAt" : "2020-11-04T10:40:45Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "92d18b05df81205d9d36863dca57c4358b57ee53",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +574,578 @@        query: str,\n        options: Dict[str, Any],\n        location: str = DEFAULT_DATAFLOW_LOCATION,\n        project_id: Optional[str] = None,\n        gcp_conn_id: str = \"google_cloud_default\","
  },
  {
    "id" : "30c6c348-bec7-43ef-83ab-75188ec98562",
    "prId" : 11726,
    "prUrl" : "https://github.com/apache/airflow/pull/11726#pullrequestreview-515116637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50f0c692-5bad-4e1a-80df-64562b50e22a",
        "parentId" : null,
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "You made the description a bit complicated.\r\n\r\n```\r\nIf True, wait for the end of pipeline execution before exiting. If False, it only waits for it to starts (``JOB_STATE_RUNNING``).\r\n\r\nThe default behavior  depends on the type of pipeline:\r\n* for the streaming pipeline, wait for jobs to start,\r\n* for the batch pipeline, wait for the jobs to complete.\r\n\r\n.. warning::\r\n\r\n    You cannot call ``PipelineResult.wait_until_finish`` method in your pipeline code for the operator to work properly. i. e. you must use asynchronous execution. Otherwise, your pipeline will always wait until finished. For more information, look at: `Asynchronous execution <https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#python_10>`__\r\n```",
        "createdAt" : "2020-10-22T21:02:57Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      }
    ],
    "commit" : "87f44cb711bb6236fbb3d33ebbe1da202f0c54e6",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +137,141 @@        successfully cancelled when task is being killed.\n    :type cancel_timeout: Optional[int]\n    :param wait_until_finished: (Optional)\n        If True, wait for the end of pipeline execution before exiting.\n        If False, only submits job."
  },
  {
    "id" : "dc5d2bc0-c38e-405f-83e9-e7bdd404cdb5",
    "prId" : 11726,
    "prUrl" : "https://github.com/apache/airflow/pull/11726#pullrequestreview-523425857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ae83ae1-a903-447d-be86-d52ed88935a8",
        "parentId" : null,
        "authorId" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "body" : "By \"always wait until finished,\" do you mean it will block forever?\r\n\r\nWhy does this happen?",
        "createdAt" : "2020-11-02T19:08:18Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "tags" : [
        ]
      },
      {
        "id" : "f39abe5a-2fdd-4948-8dae-76e6eb5e6544",
        "parentId" : "4ae83ae1-a903-447d-be86-d52ed88935a8",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "`always wait until finished` means that task will wait for the terminal state of the job. It is the current behaviour for batch jobs and it is kept as default behaviour for backward compatibility. But there may be a case that user doesn't want to wait (after successful job start) for the end of the batch job but go further in the DAG.",
        "createdAt" : "2020-11-04T14:42:08Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "87f44cb711bb6236fbb3d33ebbe1da202f0c54e6",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +387,391 @@            You cannot call ``PipelineResult.wait_until_finish`` method in your pipeline code for the operator\n            to work properly. i. e. you must use asynchronous execution. Otherwise, your pipeline will\n            always wait until finished. For more information, look at:\n            `Asynchronous execution\n            <https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#python_10>`__"
  },
  {
    "id" : "3efc29c4-e66a-410d-a461-c85eea36b007",
    "prId" : 11726,
    "prUrl" : "https://github.com/apache/airflow/pull/11726#pullrequestreview-526617359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9aede8e-086c-4a95-8755-93d32a378753",
        "parentId" : null,
        "authorId" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "body" : "Note that `wait_until_finish` is called implicitly by `with Pipeline() as p:` as well. Since `with` is the recommended way to run pipelines, I am hesitant about this part. Especially since the user is likely to have not even written the template's pipeline code themselves.",
        "createdAt" : "2020-11-02T19:09:18Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "tags" : [
        ]
      },
      {
        "id" : "812ed1bf-4af0-49c8-8895-563a6e73ad55",
        "parentId" : "a9aede8e-086c-4a95-8755-93d32a378753",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "Partially I agree, but partially not.\r\nDefault behaviour is:\r\n -  for the streaming pipeline, wait for jobs to start,\r\n  - for the batch pipeline, wait for the jobs to complete.\r\n\r\nBut there may be the specific cases like:\r\n\r\n- user doesn't want to wait for the end of the batch job and knows for sure that templated batch job not `wait_until_finish`\r\n- user want to wait until streaming job will be cancelled/drained (e.g by some external api call or web UI)\r\n\r\nIt will give possibility to conscious dataflow users for more flexible DAGs if needed. What do you think about it?",
        "createdAt" : "2020-11-04T14:50:39Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      },
      {
        "id" : "d804c0fd-569f-42a4-8390-7d5baf9dfa4c",
        "parentId" : "a9aede8e-086c-4a95-8755-93d32a378753",
        "authorId" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "body" : "I am still confused about this part: `You cannot call ``PipelineResult.wait_until_finish`` method in your pipeline code for the operator to work properly.` What exactly will happen if `pipeline.wait_until_finish()` is called and the `wait_until_finished` param is set?",
        "createdAt" : "2020-11-04T19:57:27Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "tags" : [
        ]
      },
      {
        "id" : "f6343afc-dd1b-4234-9eb1-ee98c61db583",
        "parentId" : "a9aede8e-086c-4a95-8755-93d32a378753",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "The process of starting the Dataf≈Çow job in Airflow consists of two:\r\n- running a subprocess and reading the stderr/stderr log for the job id.\r\n- loop waiting for the end of the job ID from the previous step. This loop checks the status of the job.\r\n\r\nStep two is started just after step one has finished, so if you have `wait_until_finished` in your pipeline code, step two will not start until the process stops. When this process stops, steps two will run, but it will only execute one iteration as the job will be in a terminal state.\r\n\r\nIf you in your pipeline do not call the `wait_for_pipeline` method but pass `wait_until_finish =True` to the operator,  the second loop will wait for the job's terminal state.\r\n\r\nIf you in your pipeline do not call the `wait_for_pipeline` method, and pass `wait_until_finish =False` to the operator, the second loop will wait for the running state only. \r\n\r\n\r\n\r\n\r\n",
        "createdAt" : "2020-11-04T20:24:08Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "ce55470d-f71c-4645-b9a4-71e546b7bcb6",
        "parentId" : "a9aede8e-086c-4a95-8755-93d32a378753",
        "authorId" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "body" : "Thanks for the explanation @mik-laj. The behaviors you describe seem acceptable.\r\n\r\n@TobKed Can you include details like Kamil's comment in the pydocs?",
        "createdAt" : "2020-11-06T16:51:54Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "5496d688-c1dc-47bc-9a8f-dff3cf0ab5e0",
        "tags" : [
        ]
      },
      {
        "id" : "eb23cc6d-9f9f-48d6-aed7-70f5b18c2400",
        "parentId" : "a9aede8e-086c-4a95-8755-93d32a378753",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "Thanks @mik-laj \r\n@ibzib sure, just pushed fixup with improved docs :)",
        "createdAt" : "2020-11-09T20:12:35Z",
        "updatedAt" : "2020-11-15T16:22:31Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "87f44cb711bb6236fbb3d33ebbe1da202f0c54e6",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +385,389 @@        .. warning::\n\n            You cannot call ``PipelineResult.wait_until_finish`` method in your pipeline code for the operator\n            to work properly. i. e. you must use asynchronous execution. Otherwise, your pipeline will\n            always wait until finished. For more information, look at:"
  },
  {
    "id" : "36dca2ac-4605-4636-9890-324f621f9c64",
    "prId" : 12814,
    "prUrl" : "https://github.com/apache/airflow/pull/12814#pullrequestreview-579901516",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f5ae5d2-4c59-4446-8e9b-4d046c61425a",
        "parentId" : null,
        "authorId" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "body" : "Should this have a deprecation warning?",
        "createdAt" : "2021-01-20T19:17:27Z",
        "updatedAt" : "2021-02-03T18:18:34Z",
        "lastEditedBy" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "tags" : [
        ]
      },
      {
        "id" : "efddc6ed-cc07-40de-bb87-47efe69cb0af",
        "parentId" : "5f5ae5d2-4c59-4446-8e9b-4d046c61425a",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "I added deprecation warnings in docstrings and logs.",
        "createdAt" : "2021-01-31T14:45:35Z",
        "updatedAt" : "2021-02-03T18:18:35Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a4844e23b28e6a47cd3f1a18fe813da2c4fad13",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +172,176 @@\n# pylint: disable=too-many-instance-attributes\nclass DataflowCreateJavaJobOperator(BaseOperator):\n    \"\"\"\n    Start a Java Cloud DataFlow batch job. The parameters of the operation"
  }
]