[
  {
    "id" : "182df5b6-f4cb-4ccc-876f-ea88ed5bb9c0",
    "prId" : 8477,
    "prUrl" : "https://github.com/apache/airflow/pull/8477#pullrequestreview-401116487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3627dc1-72fe-4eef-9758-9d4e4c7db24a",
        "parentId" : null,
        "authorId" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "body" : "This change in order of arguments probably needs a note in UPDATING.md as well",
        "createdAt" : "2020-04-27T14:50:13Z",
        "updatedAt" : "2020-04-27T16:21:25Z",
        "lastEditedBy" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "tags" : [
        ]
      },
      {
        "id" : "8cbf745a-f929-4de7-a020-5caeba8c5822",
        "parentId" : "f3627dc1-72fe-4eef-9758-9d4e4c7db24a",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "Added",
        "createdAt" : "2020-04-27T16:21:58Z",
        "updatedAt" : "2020-04-27T16:21:58Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      }
    ],
    "commit" : "52166852c38b0265ed097e67d824a5b04bf3794d",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1304,1308 @@    def __init__(self,\n                 dataset_resource: dict,\n                 dataset_id: Optional[str] = None,\n                 project_id: Optional[str] = None,\n                 gcp_conn_id: str = 'google_cloud_default',"
  },
  {
    "id" : "4bfe01e8-9f50-4d33-aece-0deca256df6d",
    "prId" : 8858,
    "prUrl" : "https://github.com/apache/airflow/pull/8858#pullrequestreview-413091356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e504a314-2197-43bb-a247-8add3c8da5f8",
        "parentId" : null,
        "authorId" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "body" : "Why not put new argument `table_resource` at the end? This way we do not brake backward compatibility if someone was using positional arguments.",
        "createdAt" : "2020-05-16T13:36:46Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "tags" : [
        ]
      },
      {
        "id" : "72661f2e-8f13-45b8-9ad6-71fbe7c5972c",
        "parentId" : "e504a314-2197-43bb-a247-8add3c8da5f8",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "Hint: ``@apply_defaults``",
        "createdAt" : "2020-05-16T14:35:25Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "bc9e0f19-f773-404a-944d-c1a68905ffb7",
        "parentId" : "e504a314-2197-43bb-a247-8add3c8da5f8",
        "authorId" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "body" : "Got it, thanks!",
        "createdAt" : "2020-05-16T16:50:26Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfedd51332a9f6e7d4637fc297802667b80ad91b",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +765,769 @@        dataset_id: str,\n        table_id: str,\n        table_resource: Optional[Dict[str, Any]] = None,\n        project_id: Optional[str] = None,\n        schema_fields: Optional[List] = None,"
  },
  {
    "id" : "2fc3415e-83d4-4ff3-896a-31cde716804a",
    "prId" : 8858,
    "prUrl" : "https://github.com/apache/airflow/pull/8858#pullrequestreview-413091368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "208d6289-3438-497f-a5dd-552cb058a57e",
        "parentId" : null,
        "authorId" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "body" : "Again, why not put `fields` as the last argument?",
        "createdAt" : "2020-05-16T14:19:13Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "tags" : [
        ]
      },
      {
        "id" : "b3acc737-eed6-4605-8073-75d8ad1c2940",
        "parentId" : "208d6289-3438-497f-a5dd-552cb058a57e",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "Hint: ``@apply_defaults``",
        "createdAt" : "2020-05-16T14:35:18Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "b58b49d0-b435-4274-8662-86add95736ec",
        "parentId" : "208d6289-3438-497f-a5dd-552cb058a57e",
        "authorId" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "body" : "Got it, thanks!",
        "createdAt" : "2020-05-16T16:50:33Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfedd51332a9f6e7d4637fc297802667b80ad91b",
    "line" : 663,
    "diffHunk" : "@@ -1,1 +1406,1410 @@        self,\n        dataset_resource: dict,\n        fields: Optional[List[str]] = None,\n        dataset_id: Optional[str] = None,\n        project_id: Optional[str] = None,"
  },
  {
    "id" : "c6cf4b32-08e0-45df-9159-81d52b660adf",
    "prId" : 8858,
    "prUrl" : "https://github.com/apache/airflow/pull/8858#pullrequestreview-414246008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "742b46ca-0f0d-45ce-b502-6d1c358af2c5",
        "parentId" : null,
        "authorId" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "body" : "Will it be able to catch `Conflict` exception here without specifying `exists_ok=False` when calling `create_empty_table` method?",
        "createdAt" : "2020-05-19T08:52:39Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "e98f6be9-f602-4d12-9ec7-3fcc6d61008a",
        "tags" : [
        ]
      },
      {
        "id" : "89dcbdfd-3cb0-403a-8b53-fc456f8288da",
        "parentId" : "742b46ca-0f0d-45ce-b502-6d1c358af2c5",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "Good catch! Will fix",
        "createdAt" : "2020-05-19T09:03:14Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfedd51332a9f6e7d4637fc297802667b80ad91b",
    "line" : 205,
    "diffHunk" : "@@ -1,1 +833,837 @@            self.log.info('Table %s.%s.%s created successfully',\n                          table.project, table.dataset_id, table.table_id)\n        except Conflict:\n            self.log.info('Table %s.%s already exists.', self.dataset_id, self.table_id)\n"
  },
  {
    "id" : "c1d1d744-4bdb-46f7-a481-afaaf6b5c01c",
    "prId" : 8858,
    "prUrl" : "https://github.com/apache/airflow/pull/8858#pullrequestreview-415110997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7bf75d1-4b74-4843-a418-4ee2334b8984",
        "parentId" : null,
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "Should we raise exception when user provide mutully exclussive parameters e.g. table_resource and source_uris?",
        "createdAt" : "2020-05-19T17:19:58Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "ac74cbff-78d3-4123-aaa0-2370d2fe6adf",
        "parentId" : "e7bf75d1-4b74-4843-a418-4ee2334b8984",
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "I agree. the behaviour is a bit undefined if wrong combination of  parameters is specified. If table_resources is undefined and schema_fields are defined and source_objects and bucket  are, I am not sure what happens here. I think all the \"superfluous\" combinations where same data is provided by different means  should result in exception",
        "createdAt" : "2020-05-19T18:34:05Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      },
      {
        "id" : "c9700113-f593-4153-991a-5bdc25989216",
        "parentId" : "e7bf75d1-4b74-4843-a418-4ee2334b8984",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "I've added an exception ",
        "createdAt" : "2020-05-20T08:35:38Z",
        "updatedAt" : "2020-05-25T12:14:54Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfedd51332a9f6e7d4637fc297802667b80ad91b",
    "line" : 327,
    "diffHunk" : "@@ -1,1 +985,989 @@        ])\n\n        if not table_resource:\n            warnings.warn(\n                \"Passing table parameters via keywords arguments will be deprecated. \""
  },
  {
    "id" : "c1b212aa-c3f6-4f90-85db-459e5f0d0b08",
    "prId" : 8868,
    "prUrl" : "https://github.com/apache/airflow/pull/8868#pullrequestreview-654110570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73a618cf-578a-4bc7-80f4-5578dd935ca3",
        "parentId" : null,
        "authorId" : "e77c229b-b3b9-4d73-839c-1daf194bf0dc",
        "body" : "@turbaszek is this correct? That `airflow.providers.google.cloud.operators.bigquery.BigQueryExecuteQueryOperator` is replaced by `BigQueryInsertJobOperator`? `BigQueryExecuteQueryOperator` seems so much more nice.",
        "createdAt" : "2021-04-14T13:24:52Z",
        "updatedAt" : "2021-04-14T13:24:53Z",
        "lastEditedBy" : "e77c229b-b3b9-4d73-839c-1daf194bf0dc",
        "tags" : [
        ]
      },
      {
        "id" : "0de52440-7da3-4f85-95f0-97866f859fa1",
        "parentId" : "73a618cf-578a-4bc7-80f4-5578dd935ca3",
        "authorId" : "08925dc3-5b7c-419a-a7e9-3212f90b4c4a",
        "body" : "@judoole @turbaszek This is something I've been wondering for a long while. It seems that `BigQueryExecuteQueryOperator` makes the simple case of writing a job that queries data from one table and inserts it onto another so much simpler than `BigQueryInsertJobOperator `. The fact that `BigQueryInsertJobOperator` doesn't provide destination table as a task level parameter, doesn't provide clustering fields as a task level parameter, doesn't provide time_partitioning as a task level parameter but instead forces you to construct your own config fi. Overall it just seems way less friendly to use than `BigQueryInsertJobOperator `.",
        "createdAt" : "2021-05-06T14:17:46Z",
        "updatedAt" : "2021-05-06T14:18:06Z",
        "lastEditedBy" : "08925dc3-5b7c-419a-a7e9-3212f90b4c4a",
        "tags" : [
        ]
      },
      {
        "id" : "c7e2be05-4f85-4112-84d8-12359fc46cc4",
        "parentId" : "73a618cf-578a-4bc7-80f4-5578dd935ca3",
        "authorId" : "e77c229b-b3b9-4d73-839c-1daf194bf0dc",
        "body" : "Hey there @telac. I also asked on the Slack channel, and the deprecation is real and `BigQueryInsertJobOperator` is to be preferred in the future. The gist of it being;\r\n> BigQueryInsertJobOperator  exposes the full capabilities of the API and is easy to maintain.\r\n> The old operator - BigQueryExecuteQueryOperator needs to be updated every time a new fields in API is added.\r\n\r\nI guess personally I will create an in-house `BigQueryExecuteQueryOperator` equivalent Operator that extends from `BigQueryInsertJobOperator` and sets the specific parts we are interested in. For the most parts destination, sql, time_partitioning and write_disposition.",
        "createdAt" : "2021-05-07T06:00:06Z",
        "updatedAt" : "2021-05-07T06:00:06Z",
        "lastEditedBy" : "e77c229b-b3b9-4d73-839c-1daf194bf0dc",
        "tags" : [
        ]
      }
    ],
    "commit" : "516ccead561d79613ea250edb946ad6f70f9fc81",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +549,553 @@            gcp_conn_id = bigquery_conn_id\n\n        warnings.warn(\n            \"This operator is deprecated. Please use `BigQueryInsertJobOperator`.\",\n            DeprecationWarning, stacklevel=3,"
  },
  {
    "id" : "a721ecb4-e813-405d-aa18-1b721ee02794",
    "prId" : 9590,
    "prUrl" : "https://github.com/apache/airflow/pull/9590#pullrequestreview-441193894",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7cfa8d52-9830-40ac-827a-f1504e5360c2",
        "parentId" : null,
        "authorId" : "e3fe1065-d55e-4251-b731-d633d946748b",
        "body" : "the job polls here",
        "createdAt" : "2020-07-01T21:01:01Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "e3fe1065-d55e-4251-b731-d633d946748b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7dd582dd7cc824221d60ffaad2887827fb84e003",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +1720,1724 @@        )\n        # Start the job and wait for it to complete and get the result.\n        job.result()\n        return job\n"
  },
  {
    "id" : "cb64eeb9-4157-4c30-bc32-f7de49bb1246",
    "prId" : 9590,
    "prUrl" : "https://github.com/apache/airflow/pull/9590#pullrequestreview-457910459",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "parentId" : null,
        "authorId" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "body" : "what does this do to the the behavior for re-running a DAG for an execution date? \r\nWould this not by default reattach to the originally succeeded job (rather than expected behavior or re-running the job)? Should this be configurable?",
        "createdAt" : "2020-07-10T00:16:56Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "tags" : [
        ]
      },
      {
        "id" : "9da71dbc-5e61-4152-b09a-aae4cc969a3f",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "My question is: if job succeded why should we rerun it instead of using the existing result?  ",
        "createdAt" : "2020-07-10T04:53:26Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      },
      {
        "id" : "011b02a9-d81c-40b7-a18c-5156ffe8bcd4",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "body" : "In short, the underlying table / partition could have changed between the original run and the new run and in many cases the user will want to ensure that the result of this job is based on a fresh run of the query.\r\n\r\nAirflow cannot know if the underlying table(s) have changed since the original run.\r\n\r\nTo give an example use case:\r\nImagine a streaming job in charge of inserting records to an hourly partitioned fact table and a DAG responsible for running some hourly analytics query (joins to dimension tables / aggregation). Once the processing time watermark passes the end of the partition interval original scheduled dag run runs (BigQueryInsertJobOperator task runs a query on the data at this processing time). Then late data (event time << processing time) arrives that should still go to the old partition (because partitioning is on event time). When the streaming job detects this it could use some mechanism (e.g. the REST API or an alert to a human who manually re-runs the dag for an execution date) to tell the DAG (and this BigQueryInsertJobOperator) to re-run because the original run for this interval / partition was actually against incomplete data. ",
        "createdAt" : "2020-07-10T17:15:50Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "tags" : [
        ]
      },
      {
        "id" : "ff154f12-6c12-4850-aca8-2773c5f9b310",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "243b44f0-ed59-492b-8b17-19b6e29cf2f2",
        "body" : "@turbaszek I tend to agree with @jaketf.  We frequently might want to rerun a DAG for a prior, successful execution date at some point in the future that would overwrite data in a partitioned BigQuery table.  Typically this might occur because of a change in the business logic.\r\n\r\nSo, in my opinion, the `job_id` needs to be more unique than just `airflow_{self.dag_id}_{self.task_id}_{exec_date}_` in order to account for this.\r\n\r\nIf I clear the status of a task in a DAG, I expect that task to be fully rerun.",
        "createdAt" : "2020-07-21T07:32:21Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "243b44f0-ed59-492b-8b17-19b6e29cf2f2",
        "tags" : [
        ]
      },
      {
        "id" : "d6eaf061-91e5-4d4a-a7f8-6e01f1a45519",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "I think we will not be able to create universal behavior that will keep idempotency. Some users want to always execute SQL query, others want to use predetermined results, and others want to re-query only when it is backfill. All the cases sound correct and they cannot always be combined into a single operator without additional parameters.",
        "createdAt" : "2020-07-29T09:58:34Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "d6db44f3-c1f9-4e4e-9617-28d634fa4a6c",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "243b44f0-ed59-492b-8b17-19b6e29cf2f2",
        "body" : "Then there should be additional parameters.  IMHO, I don't think it is controversial to suggest that, when the status of a BQ task is cleared, the default behaviour should be that the query if fully rerun.  Other cases sound like less common scenarios (in my experience at least).",
        "createdAt" : "2020-07-29T12:05:15Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "243b44f0-ed59-492b-8b17-19b6e29cf2f2",
        "tags" : [
        ]
      },
      {
        "id" : "e4e85dab-333b-4bed-8459-e2c60a6a9ea7",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "> the query if fully rerun.\r\n\r\nThat was the original behaviour as the `job_id` was always generated by discovery API. ",
        "createdAt" : "2020-07-29T12:22:09Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      },
      {
        "id" : "e70ce092-cd8a-41e0-bf41-9994d5231130",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "243b44f0-ed59-492b-8b17-19b6e29cf2f2",
        "body" : "Indeed.  I'd say that should be the default going forward too.",
        "createdAt" : "2020-07-29T16:24:57Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "243b44f0-ed59-492b-8b17-19b6e29cf2f2",
        "tags" : [
        ]
      },
      {
        "id" : "8a4f683e-014f-4356-9d76-f686d3b03b13",
        "parentId" : "df2f4d96-9289-41a0-ae8f-c02e32fa7a73",
        "authorId" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "body" : "> I think we will not be able to create universal behavior that will keep idempotency. Some users want to always execute SQL query, others want to use predetermined results, and others want to re-query only when it is backfill. All the cases sound correct and they cannot always be combined into a single operator without additional parameters.\r\n\r\nThis is an interesting point. Perhaps we consider bringing back the deprecated BigQueryOperator (contract is always runs a new query) and than have this BigQueryInsertJobOperator (contract is create job if not succeeded exists).\r\n\r\nIMHO we can have both behaviors in a single operator with a `force_rerun=True` parameter that controls how the job id is generated.\r\n```python3\r\n# choice of uuid \r\nuniqueness_hash = hash(uuid.uuid4()) if self.force_rerun else hash(job_config)\r\n\r\njob_id = f\"{dag_id}{task_id}{exec_date}{uniqueness_hash}\"\r\n```",
        "createdAt" : "2020-07-29T21:27:13Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "tags" : [
        ]
      }
    ],
    "commit" : "7dd582dd7cc824221d60ffaad2887827fb84e003",
    "line" : 151,
    "diffHunk" : "@@ -1,1 +1754,1758 @@            self._handle_job_error(job)\n        except Conflict:\n            # If the job already exists retrieve it\n            job = hook.get_job(\n                project_id=self.project_id,"
  },
  {
    "id" : "e3d8f46f-6f69-4e6a-a5b8-21b59c59e7a9",
    "prId" : 9590,
    "prUrl" : "https://github.com/apache/airflow/pull/9590#pullrequestreview-461180440",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68cfec2c-3f44-4123-9b03-f9a7e56fe49b",
        "parentId" : null,
        "authorId" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "body" : "```suggestion\r\n                    f\"want to force rerun it consider setting `force_rerun=True`.\"\r\n                    f\"Or, if you want to reattach in this scenario add {job.state} to `reattach_states`\"\r\n```",
        "createdAt" : "2020-08-04T21:19:54Z",
        "updatedAt" : "2020-08-10T09:51:31Z",
        "lastEditedBy" : "c6742e4c-543e-4eea-b495-c6ec28597226",
        "tags" : [
        ]
      }
    ],
    "commit" : "7dd582dd7cc824221d60ffaad2887827fb84e003",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +1768,1772 @@                raise AirflowException(\n                    f\"Job with id: {job_id} already exists and is in {job.state} state. If you \"\n                    f\"want to force rerun it consider setting `force_rerun=True`.\"\n                    f\"Or, if you want to reattach in this scenario add {job.state} to `reattach_states`\"\n                )"
  },
  {
    "id" : "86b25454-58ac-4ee9-a4f3-5ef574e1140e",
    "prId" : 11434,
    "prUrl" : "https://github.com/apache/airflow/pull/11434#pullrequestreview-533319858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28f5a2f7-1eb6-40be-aeb4-cb5a0eb6fd62",
        "parentId" : null,
        "authorId" : "c9147a70-b812-4a50-8d74-018c51033f59",
        "body" : "I got an error in CI. To fix it this modification is necessary. In this case GCSHook.download() returns always `bytes` and json.loads() calls decode internally.\r\n\r\nIf we preserve the call to `decode`, we need to cast the returned value from `download` to bytes. @turbaszek could you advise me about this point?",
        "createdAt" : "2020-10-14T02:13:18Z",
        "updatedAt" : "2020-10-14T02:13:19Z",
        "lastEditedBy" : "c9147a70-b812-4a50-8d74-018c51033f59",
        "tags" : [
        ]
      },
      {
        "id" : "fe2eb1e7-1a90-443d-bbce-89ee649925b0",
        "parentId" : "28f5a2f7-1eb6-40be-aeb4-cb5a0eb6fd62",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "Looks good to me. I'm surprised we didn't catch it earlier ",
        "createdAt" : "2020-10-16T17:51:36Z",
        "updatedAt" : "2020-10-16T17:51:37Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      },
      {
        "id" : "b1103e42-1e1a-4ed5-8b55-1343ad1f42aa",
        "parentId" : "28f5a2f7-1eb6-40be-aeb4-cb5a0eb6fd62",
        "authorId" : "c9147a70-b812-4a50-8d74-018c51033f59",
        "body" : "Thank you for your review. Static type checking is really cool because we can find like above the issues.",
        "createdAt" : "2020-10-16T19:23:32Z",
        "updatedAt" : "2020-10-16T19:23:33Z",
        "lastEditedBy" : "c9147a70-b812-4a50-8d74-018c51033f59",
        "tags" : [
        ]
      },
      {
        "id" : "b1ca31f9-dc2d-4109-8025-33a8b50fe333",
        "parentId" : "28f5a2f7-1eb6-40be-aeb4-cb5a0eb6fd62",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "Well, this never worked as per #12439 👀 ",
        "createdAt" : "2020-11-18T10:48:14Z",
        "updatedAt" : "2020-11-18T10:48:15Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea69a6359c4815cd4e92bcdad6bf67e31246f161",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +948,952 @@                impersonation_chain=self.impersonation_chain,\n            )\n            schema_fields = json.loads(gcs_hook.download(gcs_bucket, gcs_object))\n        else:\n            schema_fields = self.schema_fields"
  }
]