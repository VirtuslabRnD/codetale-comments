[
  {
    "id" : "381efc37-b51c-41e2-ab83-f45a7cb4a65a",
    "prId" : 618,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1772899a-650b-413c-ac51-0ce987d7cb35",
        "parentId" : null,
        "authorId" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "body" : "`choices=['sync', 'eventlet', 'gevent', 'tornado'],` We can constrain this since those are the only options.\n",
        "createdAt" : "2015-11-11T19:24:34Z",
        "updatedAt" : "2015-11-11T20:57:48Z",
        "lastEditedBy" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "tags" : [
        ]
      }
    ],
    "commit" : "01115af12e45acbc1401945477c6be274a196c1a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +602,606 @@    parser_webserver.add_argument(\n        \"-k\", \"--workerclass\",\n        default=configuration.get('webserver', 'WORKER_CLASS'),\n        choices=['sync', 'eventlet', 'gevent', 'tornado'],\n        help=\"The worker class to use for gunicorn\")"
  },
  {
    "id" : "a698382a-caa1-49ef-88b7-4b2a15066130",
    "prId" : 676,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7ecce3c-ba5e-4edf-8454-661263a3be4c",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "Should we `help=argparse.SUPPRESS`?\n\nI was almost thinking about removing `resetdb` altogether as we are growing our user base beyond control at Airbnb.\n",
        "createdAt" : "2015-11-20T21:34:04Z",
        "updatedAt" : "2015-11-20T21:34:04Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c76e4ee0e66554b663e9f7d12f2e1da7eeefd51",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +642,646 @@    ht = \"Burn down and rebuild the metadata database\"\n    parser_resetdb = subparsers.add_parser('resetdb', help=ht)\n    parser_resetdb.add_argument(\n            \"-y\", \"--yes\",\n            default=False,"
  },
  {
    "id" : "b17dce8c-f668-4878-931f-78ed6932273a",
    "prId" : 1506,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d171dffb-4dba-4109-8775-781b38b50eb3",
        "parentId" : null,
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "I wonder if we should use different exit codes for different errors statuses, also it looks like the other parts of the CLI raise instead of exiting (though exiting is the right call since this is user facing), maybe turn the raises in the top-level functions into sys.exits too to be consistent?\n",
        "createdAt" : "2016-05-17T19:01:50Z",
        "updatedAt" : "2016-05-18T10:31:06Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      },
      {
        "id" : "b8549075-28ab-45ea-9021-f69141fd32a8",
        "parentId" : "d171dffb-4dba-4109-8775-781b38b50eb3",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "@aoen I agree, but the full change should be part of a different PR I think. I can change to raising and create a Jira for it?\n",
        "createdAt" : "2016-05-17T20:08:24Z",
        "updatedAt" : "2016-05-18T10:31:06Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      },
      {
        "id" : "d7a78f19-c2e9-4b8c-98a4-b4194db00c33",
        "parentId" : "d171dffb-4dba-4109-8775-781b38b50eb3",
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "SGTM, and thanks!\n",
        "createdAt" : "2016-05-17T20:10:34Z",
        "updatedAt" : "2016-05-18T10:31:06Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      },
      {
        "id" : "41f89f11-516a-4b11-b150-dacf89d7136e",
        "parentId" : "d171dffb-4dba-4109-8775-781b38b50eb3",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "https://issues.apache.org/jira/browse/AIRFLOW-126\n",
        "createdAt" : "2016-05-17T20:28:44Z",
        "updatedAt" : "2016-05-18T10:31:06Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb56289743d6f28db667e2eec2509b9e245b37c7",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +123,127 @@    if not dag:\n        logging.error(\"Cannot find dag {}\".format(args.dag_id))\n        sys.exit(1)\n\n    execution_date = datetime.now()"
  },
  {
    "id" : "fdb8c9b8-4708-49ff-b44d-953feb4701c1",
    "prId" : 3730,
    "prUrl" : "https://github.com/apache/airflow/pull/3730#pullrequestreview-146450898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4389a34f-3bef-4399-a951-b70b2d22570e",
        "parentId" : null,
        "authorId" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "body" : "https://github.com/apache/incubator-airflow/blob/0ba510e238ebb197bce9afccdb06b88871c4a7e6/airflow/api/client/api_client.py#L53-L55\r\n\r\nðŸ¤” Did you test this?",
        "createdAt" : "2018-08-15T07:01:35Z",
        "updatedAt" : "2018-09-05T17:39:14Z",
        "lastEditedBy" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "tags" : [
        ]
      },
      {
        "id" : "784026bc-15f4-4320-900f-1896ce1b6178",
        "parentId" : "4389a34f-3bef-4399-a951-b70b2d22570e",
        "authorId" : "c90d6af3-9d7c-4a48-9a99-61926b9c6f27",
        "body" : "Yes, I did test on this and unit test passed. I used the default config for the api_client here https://github.com/apache/incubator-airflow/blob/00adade4163197fcff799168398edc629f989d34/airflow/config_templates/default_airflow.cfg#L183\r\n\r\nI think it is actually using this class https://github.com/apache/incubator-airflow/blob/da1f64cf805abaecda227fc8c43a15a92faff648/airflow/api/client/local_client.py#L40-L42",
        "createdAt" : "2018-08-15T13:45:54Z",
        "updatedAt" : "2018-09-05T17:39:14Z",
        "lastEditedBy" : "c90d6af3-9d7c-4a48-9a99-61926b9c6f27",
        "tags" : [
        ]
      }
    ],
    "commit" : "6956eafad7859567e4449215a426c7573fee2567",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +321,325 @@def pool_export_helper(filepath):\n    pool_dict = {}\n    pools = api_client.get_pools()\n    for pool in pools:\n        pool_dict[pool[0]] = {\"slots\": pool[1], \"description\": pool[2]}"
  },
  {
    "id" : "e75b3a34-3b9d-42c6-ad62-4a8d9f86eeaa",
    "prId" : 3730,
    "prUrl" : "https://github.com/apache/airflow/pull/3730#pullrequestreview-147463618",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dee72e70-f54c-49aa-bc73-8f75f0389fc6",
        "parentId" : null,
        "authorId" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "body" : "This code makes me a bit sad, I would expect a more descriptive response from the API.",
        "createdAt" : "2018-08-15T07:02:17Z",
        "updatedAt" : "2018-09-05T17:39:14Z",
        "lastEditedBy" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "tags" : [
        ]
      },
      {
        "id" : "5971b838-4ae2-4fc7-a067-44cdd40cdd1f",
        "parentId" : "dee72e70-f54c-49aa-bc73-8f75f0389fc6",
        "authorId" : "c90d6af3-9d7c-4a48-9a99-61926b9c6f27",
        "body" : "I totally felt the same way, however, since it returns https://github.com/apache/incubator-airflow/blob/da1f64cf805abaecda227fc8c43a15a92faff648/airflow/api/client/local_client.py#L42\r\n\r\nThe pool class doesn't seem have set and get as Variables, maybe it's good to have another PR to add them?\r\nhttps://github.com/apache/incubator-airflow/blob/da1f64cf805abaecda227fc8c43a15a92faff648/airflow/models.py#L5077",
        "createdAt" : "2018-08-15T13:47:03Z",
        "updatedAt" : "2018-09-05T17:39:14Z",
        "lastEditedBy" : "c90d6af3-9d7c-4a48-9a99-61926b9c6f27",
        "tags" : [
        ]
      },
      {
        "id" : "bc533020-87ed-4c5e-b9e9-52aea7b50e01",
        "parentId" : "dee72e70-f54c-49aa-bc73-8f75f0389fc6",
        "authorId" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "body" : "Yes, that would be my preference.",
        "createdAt" : "2018-08-19T14:42:04Z",
        "updatedAt" : "2018-09-05T17:39:14Z",
        "lastEditedBy" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "tags" : [
        ]
      },
      {
        "id" : "37c1ca3f-455d-46de-a207-2750d3ea2a36",
        "parentId" : "dee72e70-f54c-49aa-bc73-8f75f0389fc6",
        "authorId" : "c90d6af3-9d7c-4a48-9a99-61926b9c6f27",
        "body" : "@Fokko Do you think we can move forward PR or this needs to wait for adding set and get in Pool? Thanks!",
        "createdAt" : "2018-08-19T16:08:19Z",
        "updatedAt" : "2018-09-05T17:39:14Z",
        "lastEditedBy" : "c90d6af3-9d7c-4a48-9a99-61926b9c6f27",
        "tags" : [
        ]
      }
    ],
    "commit" : "6956eafad7859567e4449215a426c7573fee2567",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +323,327 @@    pools = api_client.get_pools()\n    for pool in pools:\n        pool_dict[pool[0]] = {\"slots\": pool[1], \"description\": pool[2]}\n    with open(filepath, 'w') as poolfile:\n        poolfile.write(json.dumps(pool_dict, sort_keys=True, indent=4))"
  },
  {
    "id" : "40d3e330-6932-460d-98d9-4caed236f2f5",
    "prId" : 3834,
    "prUrl" : "https://github.com/apache/airflow/pull/3834#pullrequestreview-152184437",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16506da4-f8e3-40e7-a554-db6b57ae4fc0",
        "parentId" : null,
        "authorId" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "body" : "@XD-DENG I just tested this with some of the example dags in https://github.com/apache/incubator-airflow/tree/master/airflow/example_dags. Can you test your code with different schedule types including `@once`, `daily/weekly`, `timedelta(hours=1)`, etc... in addition to the case you provide which is cron expressions. Also, can you add tests for these?\r\n\r\n```\r\n(venv) sianand@LM-SJN-21002367:~/Projects/airflow_incubator $ airflow next_execution latest_only\r\n[2018-09-04 10:52:19,613] {__init__.py:51} INFO - Using executor SequentialExecutor\r\n/Users/sianand/miniconda3/lib/python3.6/site-packages/apache_airflow-2.0.0.dev0+incubating-py3.6.egg/airflow/bin/cli.py:1724: DeprecationWarning: The celeryd_concurrency option in [celery] has been renamed to worker_concurrency - the old setting has been used, but please update your config.\r\n  default=conf.get('celery', 'worker_concurrency')),\r\n[2018-09-04 10:52:19,822] {models.py:260} INFO - Filling up the DagBag from /Users/sianand/Projects/airflow_incubator/dags\r\n[2018-09-04 10:52:19,882] {example_kubernetes_operator.py:55} WARNING - Could not import KubernetesPodOperator: No module named 'kubernetes'\r\n[2018-09-04 10:52:19,882] {example_kubernetes_operator.py:56} WARNING - Install kubernetes dependencies with:     pip install apache-airflow[kubernetes]\r\nTraceback (most recent call last):\r\n  File \"/Users/sianand/miniconda3/bin/airflow\", line 4, in <module>\r\n    __import__('pkg_resources').run_script('apache-airflow==2.0.0.dev0+incubating', 'airflow')\r\n  File \"/Users/sianand/miniconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 654, in run_script\r\n    self.require(requires)[0].run_script(script_name, ns)\r\n  File \"/Users/sianand/miniconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 1434, in run_script\r\n    exec(code, namespace, namespace)\r\n  File \"/Users/sianand/miniconda3/lib/python3.6/site-packages/apache_airflow-2.0.0.dev0+incubating-py3.6.egg/EGG-INFO/scripts/airflow\", line 32, in <module>\r\n    args.func(args)\r\n  File \"/Users/sianand/miniconda3/lib/python3.6/site-packages/apache_airflow-2.0.0.dev0+incubating-py3.6.egg/airflow/utils/cli.py\", line 74, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/Users/sianand/miniconda3/lib/python3.6/site-packages/apache_airflow-2.0.0.dev0+incubating-py3.6.egg/airflow/bin/cli.py\", line 562, in next_execution\r\n    print(dag.following_schedule(dag.latest_execution_date))\r\n  File \"/Users/sianand/miniconda3/lib/python3.6/site-packages/apache_airflow-2.0.0.dev0+incubating-py3.6.egg/airflow/models.py\", line 3371, in following_schedule\r\n    return dttm + self._schedule_interval\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'datetime.timedelta'\r\n\r\n\r\n```",
        "createdAt" : "2018-09-04T17:58:33Z",
        "updatedAt" : "2018-09-09T13:08:41Z",
        "lastEditedBy" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a3166d6269d834ef1fda622ffa5690c9d3cb860",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +552,556 @@\n\n@cli_utils.action_logging\ndef next_execution(args):\n    \"\"\""
  },
  {
    "id" : "b2867ee9-ead1-43e9-94ec-56ec79647dee",
    "prId" : 3872,
    "prUrl" : "https://github.com/apache/airflow/pull/3872#pullrequestreview-153592961",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e00d62f-927f-4a03-924d-9f42898ccc68",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Could we get around all of these changes by instead setting `export AIRFLOW_HOME='$AIRFLOW_HOME'` before we build the docs?",
        "createdAt" : "2018-09-09T20:05:07Z",
        "updatedAt" : "2018-09-09T20:06:04Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "cd86387c-ad76-4636-b637-19c269400922",
        "parentId" : "9e00d62f-927f-4a03-924d-9f42898ccc68",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "That is what is happening before we build the docs, we export `BUILDING_AIRFLOW_DOCS` environment variable, `DAGS_FOLDER = '[AIRFLOW_HOME]/dags'` will be only set when we are building docs",
        "createdAt" : "2018-09-09T20:22:51Z",
        "updatedAt" : "2018-09-09T20:22:51Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "2af6e012-6279-46fe-aedf-43ee1e275587",
        "parentId" : "9e00d62f-927f-4a03-924d-9f42898ccc68",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "After this fix, we would see as below:\r\n\r\n![image](https://user-images.githubusercontent.com/8811558/45268599-8f698d00-b476-11e8-9cc2-11f31569f540.png)\r\n",
        "createdAt" : "2018-09-09T20:23:16Z",
        "updatedAt" : "2018-09-09T20:23:16Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "36d996ce-beaa-4272-8717-7bf9daf94b0c",
        "parentId" : "9e00d62f-927f-4a03-924d-9f42898ccc68",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "Let me know if you however prefer `$AIRFLOW_HOME/dags` instead of `[AIRFLOW_HOME]/dags`. I have set `[AIRFLOW_HOME]` because we take this parameter from the config file. But I don't have a preference, so `$AIRFLOW_HOME/dags` is fine for me :)",
        "createdAt" : "2018-09-09T20:24:25Z",
        "updatedAt" : "2018-09-09T20:24:25Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "29b26a8b-4a2a-47c5-aadf-c0403a97ba86",
        "parentId" : "9e00d62f-927f-4a03-924d-9f42898ccc68",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "@ashb \r\n\r\nTo answer your question, no setting `AIRFLOW_HOME` variable won't solve it as we take this value from `airflow.cfg`, but if we change `airflow.cfg` before building docs, this solves it for self-hosted docs but still fails for ReadTheDocs .",
        "createdAt" : "2018-09-09T20:29:16Z",
        "updatedAt" : "2018-09-09T20:29:16Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "ed569152-b1e7-4d4f-9ede-c240ea865447",
        "parentId" : "9e00d62f-927f-4a03-924d-9f42898ccc68",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Then perhaps `export AIRFLOW__CORE__DAGS_FOLDER=...` then\r\n\r\nI feel that not having to change the code for this case just makes things a bit cleaner long term.",
        "createdAt" : "2018-09-09T20:45:15Z",
        "updatedAt" : "2018-09-09T20:45:15Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "921ef451-44c6-493a-8f6c-a5f3ebd858f9",
        "parentId" : "9e00d62f-927f-4a03-924d-9f42898ccc68",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "Exporting `AIRFLOW__CORE__DAGS_FOLDER=` would mess up with ReadTheDocs environment as it would change the Dags folder and hence can break it when parsing some modules that use example dags.\r\n\r\nWe are already using this for `apply_defaults` decorator: \r\nhttps://github.com/apache/incubator-airflow/blob/f7fd78b06b24c15af3773b41ae3ecab7e48bb1ea/airflow/utils/decorators.py#L102-L105\r\n\r\nAnd also in future when we might have more such issues where docs would need to be handled differently, I think this would be better than setting environment variables for each instance.",
        "createdAt" : "2018-09-09T20:56:40Z",
        "updatedAt" : "2018-09-09T20:57:00Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a5af716d35a190b22b08449cd5eb6d5e80e7ae5",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +84,88 @@\nif \"BUILDING_AIRFLOW_DOCS\" in os.environ:\n    DAGS_FOLDER = '[AIRFLOW_HOME]/dags'\n\n"
  }
]