[
  {
    "id" : "4998f3fb-c48d-447e-ad76-5339c7f1e357",
    "prId" : 5079,
    "prUrl" : "https://github.com/apache/airflow/pull/5079#pullrequestreview-242973474",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a695d6ec-1415-42c8-a049-debe5d603fa8",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Isn't the code below here (handling Pools) now handled by a Dep so this should be removed?",
        "createdAt" : "2019-05-28T16:11:47Z",
        "updatedAt" : "2019-08-13T00:04:27Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "ffe58df7-4a0c-4b62-a280-cfc325d16f4c",
        "parentId" : "a695d6ec-1415-42c8-a049-debe5d603fa8",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "This is the trickiest part: we want to handle this in a Dep in all places, but we do dependencies checks in batch w/o Dep here in the scheduler logic. The current Dep is on the task instance level and doesn't really scale very well in the use case here. I don't yet have a perfect solution to hard weird this part of logic with Deps :( Maybe add batch processing in the current Deps? Feels like belongs to another PR.",
        "createdAt" : "2019-07-07T08:58:39Z",
        "updatedAt" : "2019-08-13T00:04:27Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b82718b2d8e5367219f602212d930685d957f12f",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +891,895 @@\n        # Go through each pool, and queue up a task for execution if there are\n        # any open slots in the pool.\n        for pool, task_instances in pool_to_task_instances.items():\n            pool_name = pool"
  },
  {
    "id" : "0dddd880-7282-42e6-a703-11acea5a1fb4",
    "prId" : 5605,
    "prUrl" : "https://github.com/apache/airflow/pull/5605#pullrequestreview-263769431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae513bc5-6426-490d-9d50-ada44087db88",
        "parentId" : null,
        "authorId" : "73893400-eac1-4c2c-8fdc-49a7c6b2dae8",
        "body" : "IMO its good to have a fallback value here",
        "createdAt" : "2019-07-18T16:09:25Z",
        "updatedAt" : "2019-07-19T12:34:28Z",
        "lastEditedBy" : "73893400-eac1-4c2c-8fdc-49a7c6b2dae8",
        "tags" : [
        ]
      },
      {
        "id" : "217d50e9-61e1-4c49-b18c-55378fe14807",
        "parentId" : "ae513bc5-6426-490d-9d50-ada44087db88",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "This is already defined in ./airflow/config_templates/default_airflow.cfg so it's not possible to not have a value for this set.",
        "createdAt" : "2019-07-18T16:22:29Z",
        "updatedAt" : "2019-07-19T12:34:28Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff0878c867134a2f348d46c95fa37586868f4a1f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1276,1280 @@        async_mode = not self.using_sqlite\n\n        processor_timeout_seconds = conf.getint('core', 'dagbag_import_timeout')\n        processor_timeout = timedelta(seconds=processor_timeout_seconds)\n        self.processor_agent = DagFileProcessorAgent(self.subdir,"
  },
  {
    "id" : "de8f1246-fe68-4e40-a83c-37ce04d5c7fd",
    "prId" : 5615,
    "prUrl" : "https://github.com/apache/airflow/pull/5615#pullrequestreview-265323060",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9578167d-e8b3-4b3b-ab6a-f64d9c17e5a9",
        "parentId" : null,
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "I like that we have title for the process :).",
        "createdAt" : "2019-07-23T10:09:02Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      },
      {
        "id" : "2b37b1d8-592d-4e1e-82cb-326f0d8604b0",
        "parentId" : "9578167d-e8b3-4b3b-ab6a-f64d9c17e5a9",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Yeah, this was something I added to (try) and track down _what_ kind of process was leaking, and I decided to keep it anyway.",
        "createdAt" : "2019-07-23T10:46:21Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      }
    ],
    "commit" : "057c4628a64f64eb2772bbb184814e24fa8d9234",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +122,126 @@\n        set_context(log, file_path)\n        setproctitle(\"airflow scheduler - DagFileProcessor {}\".format(file_path))\n\n        try:"
  },
  {
    "id" : "efb242b7-b7a1-43f9-84c7-f0c2fdf93df5",
    "prId" : 5615,
    "prUrl" : "https://github.com/apache/airflow/pull/5615#pullrequestreview-265337855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99ad33d4-2673-4839-9c5e-93b7245ae753",
        "parentId" : null,
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "I think we do not have to join() after we found that _process.is_alive() is False. But it is no harm and it's the kind of sanity way of doing it.",
        "createdAt" : "2019-07-23T10:18:00Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      },
      {
        "id" : "41cb5161-845c-4572-a222-c4e8f03b17cf",
        "parentId" : "99ad33d4-2673-4839-9c5e-93b7245ae753",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "See next comment.",
        "createdAt" : "2019-07-23T10:56:21Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "5596490b-1c51-4869-8098-3bb4a0ffb889",
        "parentId" : "99ad33d4-2673-4839-9c5e-93b7245ae753",
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "Maybe we should then set timeout =0 here as well.",
        "createdAt" : "2019-07-23T11:18:07Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      }
    ],
    "commit" : "057c4628a64f64eb2772bbb184814e24fa8d9234",
    "line" : 212,
    "diffHunk" : "@@ -1,1 +261,265 @@            self._done = True\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            self._parent_channel.close()\n            return True"
  },
  {
    "id" : "926586c3-4e44-409c-90d6-ad6b070797df",
    "prId" : 6596,
    "prUrl" : "https://github.com/apache/airflow/pull/6596#pullrequestreview-324909918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b11a72cd-d516-40ec-bb6d-81fd4510ebc9",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Why did SimpleTI get moved but SimpleDag didn't?",
        "createdAt" : "2019-11-30T19:13:58Z",
        "updatedAt" : "2019-12-03T13:41:00Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "cc6b2f18-71e0-4bda-9207-0edeb4fc4400",
        "parentId" : "b11a72cd-d516-40ec-bb6d-81fd4510ebc9",
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "Because I realised that SimpleTaskInstance is actually extension of the TaskInstance. Placing it in the dag_processing package was part of the dependency problem. SimpleTaskInstance is just a simpler representation of the Task Instance and it belongs there - not in the DagProcessor. It uses TaskInstance but does not use DagProcessing. \r\n\r\nThere is a chain of dependencies that STI brings if you keep it in DagProcessing, because when you want to use STI you automatically use DagProcessing - which causes cyclic dependencies. Moving STI to TI made it so much simpler and got rid of a number of cyclic deps.",
        "createdAt" : "2019-11-30T21:46:57Z",
        "updatedAt" : "2019-12-03T13:41:00Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      }
    ],
    "commit" : "0904b516d3537e9ca52592972e6380ee6fb25125",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +49,53 @@from airflow.utils import asciiart, helpers, timezone\nfrom airflow.utils.dag_processing import (\n    AbstractDagFileProcessor, DagFileProcessorAgent, SimpleDag, SimpleDagBag,\n)\nfrom airflow.utils.db import provide_session"
  },
  {
    "id" : "5cebfef3-049b-4862-ba87-243952592991",
    "prId" : 6624,
    "prUrl" : "https://github.com/apache/airflow/pull/6624#pullrequestreview-336094470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7d93bbf-23e6-44ad-96b5-782d23c63340",
        "parentId" : null,
        "authorId" : "f85ef659-e88b-40c6-856b-c86350e0d001",
        "body" : "@mik-laj  @ashb  @kaxil  where is this line supposed to redirect the DagFileProcessor  logs/stdout/stderr to? In CI (using breeze), my observation is that for each DAG, it redirects to individual log files which are in this directory. Looks like it concatenated the absolute path to the dag definition file behind AIRFLOW_HOME. `/root/airflow/opt/airflow/airflow/example_dags/`. \r\n\r\nIs that expected?",
        "createdAt" : "2019-12-24T06:11:51Z",
        "updatedAt" : "2019-12-24T06:11:51Z",
        "lastEditedBy" : "f85ef659-e88b-40c6-856b-c86350e0d001",
        "tags" : [
        ]
      }
    ],
    "commit" : "75ff53b7e29ed28ab9c43dbca7cba7a11e85ea29",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +129,133 @@        try:\n            # redirect stdout/stderr to log\n            with redirect_stdout(StreamLogWriter(log, logging.INFO)),\\\n                    redirect_stderr(StreamLogWriter(log, logging.WARN)):\n"
  },
  {
    "id" : "5a842864-55d6-4670-84f4-d0b7bf340686",
    "prId" : 6649,
    "prUrl" : "https://github.com/apache/airflow/pull/6649#pullrequestreview-321912917",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2242d04-c96e-4af4-b0be-a13fdd285eed",
        "parentId" : null,
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "Hi @kaxil , seems there is the same error in line 758. Do you mind fixing it together?",
        "createdAt" : "2019-11-23T14:26:20Z",
        "updatedAt" : "2019-11-23T14:27:42Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      },
      {
        "id" : "f4ddcc66-6f68-4c35-bb14-3389fcfd6381",
        "parentId" : "b2242d04-c96e-4af4-b0be-a13fdd285eed",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "Done :)",
        "createdAt" : "2019-11-23T14:27:49Z",
        "updatedAt" : "2019-11-23T14:27:49Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a526f526cb8798429e23c7731d0baf2976cede4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +752,756 @@\n        :param old_states: examine TaskInstances in this state\n        :type old_states: list[airflow.utils.state.State]\n        :param new_state: set TaskInstances to this state\n        :type new_state: airflow.utils.state.State"
  },
  {
    "id" : "81c1074d-60ef-4669-944d-6b9009d8b9d2",
    "prId" : 6697,
    "prUrl" : "https://github.com/apache/airflow/pull/6697#pullrequestreview-325802581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a030803-301f-4f0c-ad0e-101bad7219dd",
        "parentId" : null,
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "```suggestion\r\n\r\n    :param dag_ids: If specified, only look at these DAG ID's\r\n```",
        "createdAt" : "2019-12-02T23:05:25Z",
        "updatedAt" : "2019-12-08T00:04:17Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      }
    ],
    "commit" : "5254e1869916febf54531eafc58d0ec6f3974b19",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +312,316 @@    the file\n\n    :param dag_ids: If specified, only look at these DAG ID's\n    :type dag_ids: List[str]\n    :param log: Logger to save the processing process"
  },
  {
    "id" : "314795bb-2ff0-4f39-86b4-b1282e574dce",
    "prId" : 6697,
    "prUrl" : "https://github.com/apache/airflow/pull/6697#pullrequestreview-328640892",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85d1e3f0-27a1-41c3-a5d5-d6ddebf5fd36",
        "parentId" : null,
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "What I think should happen here as well is to add type annotations and make those classes pylint compliant (and remove --coding--)? \r\n\r\nIt should likely be done in a separate PR. I am working on such PR :)",
        "createdAt" : "2019-12-08T09:52:42Z",
        "updatedAt" : "2019-12-08T22:47:50Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      },
      {
        "id" : "6fdaf038-7881-47c8-9433-949914c1d1cd",
        "parentId" : "85d1e3f0-27a1-41c3-a5d5-d6ddebf5fd36",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "I agree. We should harden the core and this is the first change that does it. Gradually, I will try to introduce further improvements that do this.",
        "createdAt" : "2019-12-08T23:24:04Z",
        "updatedAt" : "2019-12-08T23:24:04Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      }
    ],
    "commit" : "5254e1869916febf54531eafc58d0ec6f3974b19",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +313,317 @@\n    :param dag_ids: If specified, only look at these DAG ID's\n    :type dag_ids: List[str]\n    :param log: Logger to save the processing process\n    :type log: logging.Logger"
  }
]