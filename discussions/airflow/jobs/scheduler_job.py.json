[
  {
    "id" : "4998f3fb-c48d-447e-ad76-5339c7f1e357",
    "prId" : 5079,
    "prUrl" : "https://github.com/apache/airflow/pull/5079#pullrequestreview-242973474",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a695d6ec-1415-42c8-a049-debe5d603fa8",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Isn't the code below here (handling Pools) now handled by a Dep so this should be removed?",
        "createdAt" : "2019-05-28T16:11:47Z",
        "updatedAt" : "2019-08-13T00:04:27Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "ffe58df7-4a0c-4b62-a280-cfc325d16f4c",
        "parentId" : "a695d6ec-1415-42c8-a049-debe5d603fa8",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "This is the trickiest part: we want to handle this in a Dep in all places, but we do dependencies checks in batch w/o Dep here in the scheduler logic. The current Dep is on the task instance level and doesn't really scale very well in the use case here. I don't yet have a perfect solution to hard weird this part of logic with Deps :( Maybe add batch processing in the current Deps? Feels like belongs to another PR.",
        "createdAt" : "2019-07-07T08:58:39Z",
        "updatedAt" : "2019-08-13T00:04:27Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b82718b2d8e5367219f602212d930685d957f12f",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +891,895 @@\n        # Go through each pool, and queue up a task for execution if there are\n        # any open slots in the pool.\n        for pool, task_instances in pool_to_task_instances.items():\n            pool_name = pool"
  },
  {
    "id" : "0dddd880-7282-42e6-a703-11acea5a1fb4",
    "prId" : 5605,
    "prUrl" : "https://github.com/apache/airflow/pull/5605#pullrequestreview-263769431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae513bc5-6426-490d-9d50-ada44087db88",
        "parentId" : null,
        "authorId" : "73893400-eac1-4c2c-8fdc-49a7c6b2dae8",
        "body" : "IMO its good to have a fallback value here",
        "createdAt" : "2019-07-18T16:09:25Z",
        "updatedAt" : "2019-07-19T12:34:28Z",
        "lastEditedBy" : "73893400-eac1-4c2c-8fdc-49a7c6b2dae8",
        "tags" : [
        ]
      },
      {
        "id" : "217d50e9-61e1-4c49-b18c-55378fe14807",
        "parentId" : "ae513bc5-6426-490d-9d50-ada44087db88",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "This is already defined in ./airflow/config_templates/default_airflow.cfg so it's not possible to not have a value for this set.",
        "createdAt" : "2019-07-18T16:22:29Z",
        "updatedAt" : "2019-07-19T12:34:28Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff0878c867134a2f348d46c95fa37586868f4a1f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1276,1280 @@        async_mode = not self.using_sqlite\n\n        processor_timeout_seconds = conf.getint('core', 'dagbag_import_timeout')\n        processor_timeout = timedelta(seconds=processor_timeout_seconds)\n        self.processor_agent = DagFileProcessorAgent(self.subdir,"
  },
  {
    "id" : "de8f1246-fe68-4e40-a83c-37ce04d5c7fd",
    "prId" : 5615,
    "prUrl" : "https://github.com/apache/airflow/pull/5615#pullrequestreview-265323060",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9578167d-e8b3-4b3b-ab6a-f64d9c17e5a9",
        "parentId" : null,
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "I like that we have title for the process :).",
        "createdAt" : "2019-07-23T10:09:02Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      },
      {
        "id" : "2b37b1d8-592d-4e1e-82cb-326f0d8604b0",
        "parentId" : "9578167d-e8b3-4b3b-ab6a-f64d9c17e5a9",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Yeah, this was something I added to (try) and track down _what_ kind of process was leaking, and I decided to keep it anyway.",
        "createdAt" : "2019-07-23T10:46:21Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      }
    ],
    "commit" : "057c4628a64f64eb2772bbb184814e24fa8d9234",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +122,126 @@\n        set_context(log, file_path)\n        setproctitle(\"airflow scheduler - DagFileProcessor {}\".format(file_path))\n\n        try:"
  },
  {
    "id" : "efb242b7-b7a1-43f9-84c7-f0c2fdf93df5",
    "prId" : 5615,
    "prUrl" : "https://github.com/apache/airflow/pull/5615#pullrequestreview-265337855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99ad33d4-2673-4839-9c5e-93b7245ae753",
        "parentId" : null,
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "I think we do not have to join() after we found that _process.is_alive() is False. But it is no harm and it's the kind of sanity way of doing it.",
        "createdAt" : "2019-07-23T10:18:00Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      },
      {
        "id" : "41cb5161-845c-4572-a222-c4e8f03b17cf",
        "parentId" : "99ad33d4-2673-4839-9c5e-93b7245ae753",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "See next comment.",
        "createdAt" : "2019-07-23T10:56:21Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "5596490b-1c51-4869-8098-3bb4a0ffb889",
        "parentId" : "99ad33d4-2673-4839-9c5e-93b7245ae753",
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "Maybe we should then set timeout =0 here as well.",
        "createdAt" : "2019-07-23T11:18:07Z",
        "updatedAt" : "2019-07-29T15:20:34Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      }
    ],
    "commit" : "057c4628a64f64eb2772bbb184814e24fa8d9234",
    "line" : 212,
    "diffHunk" : "@@ -1,1 +261,265 @@            self._done = True\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            self._parent_channel.close()\n            return True"
  }
]