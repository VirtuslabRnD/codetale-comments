[
  {
    "id" : "608a72b6-e87f-44d9-b440-11ab3573d971",
    "prId" : 83,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94170efe-6be8-409c-afc0-730c0b7af575",
        "parentId" : null,
        "authorId" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "body" : "What is an example of the second inequality being strict?\n",
        "createdAt" : "2015-06-26T19:07:45Z",
        "updatedAt" : "2015-06-26T19:07:45Z",
        "lastEditedBy" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "tags" : [
        ]
      },
      {
        "id" : "1af0cff8-3034-431f-aed0-e7c16bc71085",
        "parentId" : "94170efe-6be8-409c-afc0-730c0b7af575",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "if should actually read  `if successes < done == len(task._upstream_list):` but I didn't know whether it would parse. But yeah, it's basically skipping tasks that have one or many `failed` or `upstream_failed` upstream and they may have other succeeded tasks, but they have to be all finished processing.\n",
        "createdAt" : "2015-06-26T19:46:36Z",
        "updatedAt" : "2015-06-26T19:46:36Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "447bb088b87c9ad5d9f5747e95ba011f3e29982c",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +659,663 @@            )\n            successes, done  = qry[0]\n            if successes < done >= len(task._upstream_list):\n                self.state = State.UPSTREAM_FAILED\n                self.start_date = datetime.now()"
  },
  {
    "id" : "14e08a15-92bc-4214-b00c-8ca32c9783bf",
    "prId" : 147,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1910225e-b83a-406d-8fa7-851a5cedcfb3",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "it's not super explicit here that list is used as a way to cast an iterable. btw:\n\n```\n>>> list('abc')\n['a', 'b', 'c']\n```\n\nwhat about, instead of the try block:\n\n```\nif not isinstance(task_list, (tuple, list):\n    task_list = [task_list]\n```\n",
        "createdAt" : "2015-07-17T19:11:44Z",
        "updatedAt" : "2015-07-17T19:11:44Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "52b33797-390b-404d-9dbe-6ef4e407494c",
        "parentId" : "1910225e-b83a-406d-8fa7-851a5cedcfb3",
        "authorId" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "body" : "I can change it to that if you prefer, however it doesn't allow you to have custom classes where **iter** is defined to be passed in here. Would a comment alleviate your concerns?\n",
        "createdAt" : "2015-07-17T19:18:28Z",
        "updatedAt" : "2015-07-17T19:18:28Z",
        "lastEditedBy" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "42d99d7dadb46ac0f4ad8cd15631e5d012aa6171",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1369,1373 @@    def _set_relatives(self, task_or_task_list, upstream=False):\n        try:\n            task_list = list(task_or_task_list)\n        except TypeError:\n            task_list = [task_or_task_list]"
  },
  {
    "id" : "e6afa9f7-3d3f-43e1-a3d9-e3677abea352",
    "prId" : 151,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b23d8e7-7feb-4482-b725-26610788049d",
        "parentId" : null,
        "authorId" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "body" : "would it be possible to make this a list of callbacks?\n",
        "createdAt" : "2015-07-17T18:16:55Z",
        "updatedAt" : "2015-07-18T01:10:16Z",
        "lastEditedBy" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "tags" : [
        ]
      },
      {
        "id" : "79e7f618-8a54-4719-be42-c5cefb99a21e",
        "parentId" : "9b23d8e7-7feb-4482-b725-26610788049d",
        "authorId" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "body" : "If we are going that route, maybe a dictionary would make more sense ? \n\n```\n{\"on_success\": callback_callable, \"on_failure\": notification_callback, \"on_retry\": email_callback}\n```\n\nBut I think it's makes the interface less clear.\n",
        "createdAt" : "2015-07-17T18:22:12Z",
        "updatedAt" : "2015-07-18T01:10:16Z",
        "lastEditedBy" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "tags" : [
        ]
      },
      {
        "id" : "300f21b3-8de0-42c7-818b-e3c4796d22b4",
        "parentId" : "9b23d8e7-7feb-4482-b725-26610788049d",
        "authorId" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "body" : "oh... I meant, instead of passing a single callback for on_failure, pass a list, so you can do multiple things if a task fails without wrapping wrapping callbacks inside themselves.\n",
        "createdAt" : "2015-07-17T18:37:00Z",
        "updatedAt" : "2015-07-18T01:10:16Z",
        "lastEditedBy" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "tags" : [
        ]
      },
      {
        "id" : "8a0d01c4-91bf-40ed-9dc2-28e995043136",
        "parentId" : "9b23d8e7-7feb-4482-b725-26610788049d",
        "authorId" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "body" : "Sorry, I misunderstood. Sounds reasonable to me.\n",
        "createdAt" : "2015-07-17T18:47:11Z",
        "updatedAt" : "2015-07-18T01:10:16Z",
        "lastEditedBy" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "tags" : [
        ]
      },
      {
        "id" : "4aaef6db-9ec0-4d16-ab54-bf5ae1cc0652",
        "parentId" : "9b23d8e7-7feb-4482-b725-26610788049d",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "I think it muddies the API to expect a list or either a callable or a list of callable and for the Airflow to have to deal with etheir using type inspection. If you need to call multiple things, write a function that calls multiple things, or wrap it in a lambda that calls something like this\n\n```\ndef calls(funcs, *args, **args):\n    return [f(*args, **args) for f in funcs]\n```\n",
        "createdAt" : "2015-07-17T18:59:16Z",
        "updatedAt" : "2015-07-18T01:10:16Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "00d0b155-0a71-421c-8505-004bbcdaf60b",
        "parentId" : "9b23d8e7-7feb-4482-b725-26610788049d",
        "authorId" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "body" : "that is fine for me. I can make it work either way. thanks!\n",
        "createdAt" : "2015-07-17T22:15:58Z",
        "updatedAt" : "2015-07-18T01:10:16Z",
        "lastEditedBy" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "4009a9e6b91afce2d8eedde1ed53b3b60d0d8e12",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +1152,1156 @@        self.on_failure_callback = on_failure_callback\n        self.on_success_callback = on_success_callback\n        self.on_retry_callback = on_retry_callback\n        if isinstance(retry_delay, timedelta):\n            self.retry_delay = retry_delay"
  },
  {
    "id" : "af7b867c-021e-4bee-8c7a-cb10208f6e90",
    "prId" : 151,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "749e289e-c355-4b66-8d65-b930a50a8442",
        "parentId" : null,
        "authorId" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "body" : "it might be nice to also always email when the email is set.\n\nWould it be possible to have callbacks be a list and always seed the list with the email callback?\n",
        "createdAt" : "2015-07-17T18:20:36Z",
        "updatedAt" : "2015-07-18T01:10:16Z",
        "lastEditedBy" : "1e336686-2e66-4dfa-a3d5-95009d8e01ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "4009a9e6b91afce2d8eedde1ed53b3b60d0d8e12",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +881,885 @@\n        # Handling callbacks pessimistically\n        try:\n            if self.state == State.UP_FOR_RETRY and task.on_retry_callback:\n                task.on_retry_callback(context)"
  },
  {
    "id" : "5cc8f157-443e-4611-b229-9a918abebcf9",
    "prId" : 169,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58711bf3-58a1-4f83-b89b-6f19e3e76bb8",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "This line right here\n",
        "createdAt" : "2015-07-23T06:53:24Z",
        "updatedAt" : "2015-07-23T06:53:24Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eeead06c72c638ad3f0451fbd727abcda2b1c46",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +936,940 @@            'ti': self,\n            'task_instance_key_str': ti_key_str,\n            'conf': conf,\n        }\n"
  },
  {
    "id" : "6b3ca3de-e517-497e-a684-162f17c874f9",
    "prId" : 254,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a08fd82d-bdb5-4985-bfd6-cfa29dccd08a",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "I'm surprised that `urlparse` doens't urldecode... I guess hostname is not expected to have slashes in it...\n",
        "createdAt" : "2015-08-31T20:56:51Z",
        "updatedAt" : "2015-08-31T22:37:26Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bc95e230c3740aa3d601a17670b72c23a1dcfa4",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +337,341 @@        temp_uri = urlparse(uri)\n        hostname = temp_uri.hostname or ''\n        if '%2f' in hostname:\n            hostname = hostname.replace('%2f', '/').replace('%2F', '/')\n        self.host = hostname"
  },
  {
    "id" : "24705773-e4f2-4600-a421-a645f67d887c",
    "prId" : 260,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "906106cc-26ba-4a9e-b2ac-ab89c0a0652b",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "the `@provide_session` decorator will commit for you if no external session is passed to the function, if a session is passed, it assumes the caller is in charge of the transaction and doesn't want the function to commit on its behalf. \n",
        "createdAt" : "2015-08-18T16:32:44Z",
        "updatedAt" : "2015-08-20T17:31:05Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "123fbd80-9a48-4dfc-b7eb-5d13f616c054",
        "parentId" : "906106cc-26ba-4a9e-b2ac-ab89c0a0652b",
        "authorId" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "body" : "got it -- that's great. I think I copied all this boilerplate from somewhere to be safe. I'll pull it out.\n",
        "createdAt" : "2015-08-19T01:52:32Z",
        "updatedAt" : "2015-08-20T17:31:05Z",
        "lastEditedBy" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "tags" : [
        ]
      },
      {
        "id" : "5da60613-4366-4fa3-8b48-13e4f5928e04",
        "parentId" : "906106cc-26ba-4a9e-b2ac-ab89c0a0652b",
        "authorId" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "body" : "Actually... looking at `@provide_session`, it calls `expunge_all()` _after_ the function runs (line 279 of `utils.py`), meaning the XCom is removed from the session before `commit` is called. I think that's why I needed to add these lines explicitly. It looks like all the other uses of `@provide_session` are for getting data, not setting it, so this hasn't been an issue in the past.\n",
        "createdAt" : "2015-08-19T02:46:12Z",
        "updatedAt" : "2015-08-20T17:31:05Z",
        "lastEditedBy" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0235a9bfc2a37e98f769228a2e50bc4dc8841e6b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2190,2194 @@            dag_id=dag_id))\n\n        session.commit()\n\n    @classmethod"
  },
  {
    "id" : "b22cfe63-87b4-4188-a009-774bd58a25dd",
    "prId" : 260,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "712aa793-65a9-45c3-b0c4-12cda8e27c86",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "As much as I like pandas dataframe, I still think that returning an array of expunged XCom objects would be more comprehensive. The XCom object has all the metadata and the value so it's ideal. \n\nAlso maybe there's a need for distinct methods to `get` one or `get_many`, and matching with `pull` and `pull_many`\n",
        "createdAt" : "2015-08-18T16:41:59Z",
        "updatedAt" : "2015-08-20T17:31:05Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "a41a8c87-dbdf-426a-b9b8-d1346c3aebc4",
        "parentId" : "712aa793-65a9-45c3-b0c4-12cda8e27c86",
        "authorId" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "body" : "Ok. the DataFrame was more useful in the previous (more complicated) version, where there were more fields to query. Let's drop it. And while we're at it, maybe we drop the whole `limit` concept entirely? It was great for debugging but I think it just confuses things -- I'm not sure what the real world use case would be. If users really want the XCom objects instead of values (though I'm not sure they will), then maybe a `return_xcom=True` argument could be added.\n\nSo there are two return types:\n\n``` python\nti.xcom_pull(key=optional, task_id=t) # returns a value\nti.xcom_pull(key=optional, task_id=[t1, t2, t3]) # returns a list of 3 values\n```\n",
        "createdAt" : "2015-08-19T02:02:20Z",
        "updatedAt" : "2015-08-20T17:31:05Z",
        "lastEditedBy" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0235a9bfc2a37e98f769228a2e50bc4dc8841e6b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2222,2226 @@            .order_by(cls.execution_date.desc(), cls.timestamp.desc())\n            .limit(1))\n\n        result = query.first()\n        if result:"
  },
  {
    "id" : "e6cb46ad-58e5-4f96-9db7-ac96b5b1b808",
    "prId" : 364,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2d31ae8-1541-4ea9-b1f5-e13cd10c61e7",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "nit: use double quotes for sentences so that you don't have to escape apostrophes\n",
        "createdAt" : "2015-09-07T23:25:18Z",
        "updatedAt" : "2015-09-07T23:25:18Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "b67a1198-6b49-4cc0-8fcc-9aa8354327f4",
        "parentId" : "d2d31ae8-1541-4ea9-b1f5-e13cd10c61e7",
        "authorId" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "body" : "will do!\n",
        "createdAt" : "2015-09-08T22:36:35Z",
        "updatedAt" : "2015-09-08T22:36:35Z",
        "lastEditedBy" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fe611498ccc837e18c227a4276c14905f007953",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +817,821 @@        )\n        if not pool:\n            raise ValueError('Task specified a pool ({}) but the pool '\n                             'doesn\\'t exist!').format(self.task.pool)\n        open_slots = pool.open_slots(session=session)"
  },
  {
    "id" : "7c11a311-4e7b-4b68-a4b1-83a80ce737e5",
    "prId" : 712,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cc45871-e569-42c7-b6c2-0cce51ce0bde",
        "parentId" : null,
        "authorId" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "body" : "Shouldn't the base table still be the job? Or I guess in a way I feel that we need to kill the full outer join of this : \nCase 1) Zombie task instance TI.state == State.Running and LJ.State != State.Running and \n         2) LJ.State == State.Running and TI.state != State.Running.\n\nStill need to think about some of this.\n",
        "createdAt" : "2015-12-02T07:46:09Z",
        "updatedAt" : "2015-12-11T01:43:39Z",
        "lastEditedBy" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a00f85c9467d5183878028e75485b843cb7aeba",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +228,232 @@            \"Failing jobs without heartbeat after {}\".format(limit_dttm))\n\n        tis = (\n            session.query(TI)\n            .join(LJ)"
  },
  {
    "id" : "1afbf258-5d30-40f3-af2e-a103f5bb2c0d",
    "prId" : 958,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81f34af0-94b0-41b6-8fa2-26bc071899b3",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "this line isn't needed anymore...\n",
        "createdAt" : "2016-02-05T23:56:35Z",
        "updatedAt" : "2016-02-05T23:56:35Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbed0b23506cc4dcb5f31089686b58dda57474d3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +985,989 @@                                                tot_tries)\n            self.try_number += 1\n            msg = msg.format(**locals())\n            logging.info(HR + msg + HR)\n            self.start_date = datetime.now()"
  },
  {
    "id" : "67fa3063-35ba-4928-96b7-8dbd35bb9253",
    "prId" : 976,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a054b14-0fe2-4f07-bdf7-032578b60adc",
        "parentId" : null,
        "authorId" : "c96e8c12-7762-41e2-aa7e-96b2c1ddbcd1",
        "body" : "Looks better. is it possible at this stage to get even more information e.g. which of (possibly many) dependencies failed.\nBTW. I got confused - isn't it upstream task that is a dependency here, not a downstream one?\n",
        "createdAt" : "2016-02-09T08:57:17Z",
        "updatedAt" : "2016-02-09T08:57:17Z",
        "lastEditedBy" : "c96e8c12-7762-41e2-aa7e-96b2c1ddbcd1",
        "tags" : [
        ]
      },
      {
        "id" : "3a6011d7-fa56-49e5-870c-96da82556eb7",
        "parentId" : "5a054b14-0fe2-4f07-bdf7-032578b60adc",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "Downstream jobs run after, so they depend on upstream jobs.\n",
        "createdAt" : "2016-02-09T19:37:24Z",
        "updatedAt" : "2016-02-09T19:37:24Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "48cb817c-49e1-4f8f-a61a-12667d6cdf51",
        "parentId" : "5a054b14-0fe2-4f07-bdf7-032578b60adc",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "wait_for_downstream is a very specific flag it's documented in the docs.\n",
        "createdAt" : "2016-02-09T19:38:00Z",
        "updatedAt" : "2016-02-09T19:38:00Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "013b1cf9-536e-4032-923b-e6f50fa5ee45",
        "parentId" : "5a054b14-0fe2-4f07-bdf7-032578b60adc",
        "authorId" : "c96e8c12-7762-41e2-aa7e-96b2c1ddbcd1",
        "body" : "yes, I get it now, 'wait_for_downstream' could be more precisely called 'wait_for_past_downstream'.\n",
        "createdAt" : "2016-02-10T20:46:04Z",
        "updatedAt" : "2016-02-10T20:46:04Z",
        "lastEditedBy" : "c96e8c12-7762-41e2-aa7e-96b2c1ddbcd1",
        "tags" : [
        ]
      },
      {
        "id" : "aa735709-14cd-4b56-8162-0e2dc41534a3",
        "parentId" : "5a054b14-0fe2-4f07-bdf7-032578b60adc",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "right, I added the feature early on and can't change the API easily anymore...\n",
        "createdAt" : "2016-02-10T21:11:42Z",
        "updatedAt" : "2016-02-10T21:11:42Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9101ea043b53b88722f2da70791761d4133af33c",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +842,846 @@                    previous_ti.are_dependents_done(session):\n                if verbose:\n                    logging.warning(\"wait_for_downstream not satisfied\")\n                return False\n"
  },
  {
    "id" : "6ab08f25-5588-4631-a1ea-966e3f252a56",
    "prId" : 976,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41f4cc9b-ab23-418a-8b51-95e3e57710b3",
        "parentId" : null,
        "authorId" : "c96e8c12-7762-41e2-aa7e-96b2c1ddbcd1",
        "body" : "here in task._upstream_list we have a list of upstream dependencies, but if some is not satisfied we would still just see e.g. 'Trigger rule `ALL_SUCCESS` not satisfied' while airflow user would more likely be interested in the list of upstream tasks missing. This does not matter for ONE_SUCCESS and ONE_FAILED but for ALL_\\* it does.\nGiven the way the query is done now (using just sum of tasks in a specific state) it is not that straightforward to print it all out here, but you could consider that at some point.\n",
        "createdAt" : "2016-02-10T20:50:12Z",
        "updatedAt" : "2016-02-10T20:50:12Z",
        "lastEditedBy" : "c96e8c12-7762-41e2-aa7e-96b2c1ddbcd1",
        "tags" : [
        ]
      },
      {
        "id" : "318cd684-98bb-40ce-88d4-ba7f2e6415ee",
        "parentId" : "41f4cc9b-ab23-418a-8b51-95e3e57710b3",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "Right, it'd be nice to show the details but would cost. This query above is optimize for a single scan and return the minimum amount of info to be able to do the job...\n\nI'm planning to get in this area of the code and change things a bit eventually to power a UI page detailing the dependencies and explaining which ones are met and which ones aren't. This would answer clearly the most common question \"Why isn't my task running?\"\n",
        "createdAt" : "2016-02-10T21:14:21Z",
        "updatedAt" : "2016-02-10T21:14:21Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9101ea043b53b88722f2da70791761d4133af33c",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +907,911 @@            session.close()\n        if verbose:\n            logging.warning(\"Trigger rule `{}` not satisfied\".format(tr))\n        return False\n"
  },
  {
    "id" : "c2e13176-86ec-4e58-be5f-c6a0c59d5321",
    "prId" : 1378,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6cb7e17-235a-4341-b39b-4b942c724b19",
        "parentId" : null,
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "document lock_for_update\n",
        "createdAt" : "2016-05-09T18:38:57Z",
        "updatedAt" : "2016-05-09T21:19:15Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      }
    ],
    "commit" : "c1aa93f1a7c9dbe88889b78b541b3abe05ded081",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +818,822 @@\n    @provide_session\n    def refresh_from_db(self, session=None, lock_for_update=False):\n        \"\"\"\n        Refreshes the task instance from the database based on the primary key"
  },
  {
    "id" : "3b4f4769-fee2-4893-82f2-ec73a54b396c",
    "prId" : 1406,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adb9b6de-c256-4857-97b7-1919ce5178ad",
        "parentId" : null,
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "maybe attempts_to_rerun_succeeded_tasks or something a bit more clear\n",
        "createdAt" : "2016-04-19T23:50:39Z",
        "updatedAt" : "2016-04-19T23:50:39Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      }
    ],
    "commit" : "211a85907e6def416b9f7465b4e3e488a226b46b",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1130,1134 @@                \" on {self.end_date}\".format(**locals())\n            )\n            Stats.incr('previously_succeeded', 1, 1)\n        elif (\n                not ignore_dependencies and"
  },
  {
    "id" : "1dec784d-a354-4803-be1f-6c77fb6e36af",
    "prId" : 1406,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d92a2969-e111-46df-aa46-8eb83826483d",
        "parentId" : null,
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "maybe collect_dags_seconds so that it's clear it's a time measurement\n",
        "createdAt" : "2016-04-19T23:50:44Z",
        "updatedAt" : "2016-04-19T23:50:44Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      },
      {
        "id" : "c4c52301-b3bf-4900-aaaf-7325dcaf5e72",
        "parentId" : "d92a2969-e111-46df-aa46-8eb83826483d",
        "authorId" : "08b40e03-e1f4-45e1-80c0-b73f75b29580",
        "body" : "dag_collection_time_seconds? Also, I'm not too familiar with how these metrics appear on the user end. Is it just this string or is there any additional context about which module it came from?\n",
        "createdAt" : "2016-04-20T00:12:08Z",
        "updatedAt" : "2016-04-20T00:12:08Z",
        "lastEditedBy" : "08b40e03-e1f4-45e1-80c0-b73f75b29580",
        "tags" : [
        ]
      }
    ],
    "commit" : "211a85907e6def416b9f7465b4e3e488a226b46b",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +408,412 @@                        logging.warning(e)\n        Stats.gauge(\n            'collect_dags', (datetime.now() - start_dttm).total_seconds(), 1)\n        Stats.gauge(\n            'dagbag_size', len(self.dags), 1)"
  },
  {
    "id" : "6e35f568-7314-48ca-9ac9-71ccbd940472",
    "prId" : 1406,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fdec160-1074-438b-a7e7-5e88945599d1",
        "parentId" : null,
        "authorId" : "08b40e03-e1f4-45e1-80c0-b73f75b29580",
        "body" : "Could you also add `Stats.gauge('import_errors', len(self.import_errors), 1)`? \n",
        "createdAt" : "2016-04-21T02:27:28Z",
        "updatedAt" : "2016-04-21T02:27:28Z",
        "lastEditedBy" : "08b40e03-e1f4-45e1-80c0-b73f75b29580",
        "tags" : [
        ]
      }
    ],
    "commit" : "211a85907e6def416b9f7465b4e3e488a226b46b",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +410,414 @@            'collect_dags', (datetime.now() - start_dttm).total_seconds(), 1)\n        Stats.gauge(\n            'dagbag_size', len(self.dags), 1)\n\n    def deactivate_inactive_dags(self):"
  },
  {
    "id" : "45e62c75-e2a0-4778-bdfe-ee1805edde4c",
    "prId" : 1468,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88d12924-0568-430e-ad3d-4c3145587500",
        "parentId" : null,
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "Nit: docstring\n",
        "createdAt" : "2016-05-05T21:34:01Z",
        "updatedAt" : "2016-05-06T15:17:31Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      }
    ],
    "commit" : "b942933458845012310dca07256a127b5347a61e",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +436,440 @@            stats, key=lambda x: x.duration, reverse=True)\n\n    def dagbag_report(self):\n        \"\"\"Prints a report around DagBag loading stats\"\"\"\n        report = textwrap.dedent(\"\"\"\\n"
  },
  {
    "id" : "b6e889a9-08ca-4af9-92df-4c01cf28660c",
    "prId" : 1490,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72e871d3-df71-4e63-b368-daaa5ccaea35",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "Very close to being redundant with `not ready_for_retry()` the difference is very subtle. \n",
        "createdAt" : "2016-05-12T04:19:47Z",
        "updatedAt" : "2016-05-13T01:40:10Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "bfd7d783-2b8e-44dc-aeb2-5d538eb207d2",
        "parentId" : "72e871d3-df71-4e63-b368-daaa5ccaea35",
        "authorId" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "body" : "LIne `930` was taken from line `901` above :-) What's the difference? \n",
        "createdAt" : "2016-05-12T04:39:17Z",
        "updatedAt" : "2016-05-13T01:40:10Z",
        "lastEditedBy" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "tags" : [
        ]
      },
      {
        "id" : "fdaa6a32-8837-40db-8811-16285d09d70f",
        "parentId" : "72e871d3-df71-4e63-b368-daaa5ccaea35",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "At first I thought you could use `not ready_for_retry()` wherever `is_premature()` is used, but that's actually different, which validates the existence of  `is_premature()`.\n",
        "createdAt" : "2016-05-12T05:56:55Z",
        "updatedAt" : "2016-05-13T01:40:10Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "ecc28e6b-039c-4343-bbc3-51fc619a33a1",
        "parentId" : "72e871d3-df71-4e63-b368-daaa5ccaea35",
        "authorId" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "body" : "Right and a minor logic fix.\n",
        "createdAt" : "2016-05-12T06:07:33Z",
        "updatedAt" : "2016-05-13T01:40:10Z",
        "lastEditedBy" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "tags" : [
        ]
      },
      {
        "id" : "b4e24216-dc32-4b38-8313-3ab69027855c",
        "parentId" : "72e871d3-df71-4e63-b368-daaa5ccaea35",
        "authorId" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "body" : "Done\n",
        "createdAt" : "2016-05-13T01:40:31Z",
        "updatedAt" : "2016-05-13T01:40:31Z",
        "lastEditedBy" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab5d445992617585a0ced1d81881a0728f49b13a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +922,926 @@\n\n    def is_premature(self):\n        \"\"\"\n        Returns whether a task is in UP_FOR_RETRY state and its retry interval"
  },
  {
    "id" : "503d46af-43f3-44b6-b067-4697bbce9d21",
    "prId" : 1506,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e29d2b2e-b20b-4af0-b17a-5b061fe80c66",
        "parentId" : null,
        "authorId" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "body" : "Nit: docs\n",
        "createdAt" : "2016-05-17T17:20:00Z",
        "updatedAt" : "2016-05-18T10:31:06Z",
        "lastEditedBy" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "tags" : [
        ]
      },
      {
        "id" : "64d57971-685b-480d-ad97-800670a11d9c",
        "parentId" : "e29d2b2e-b20b-4af0-b17a-5b061fe80c66",
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "about docstrings, I had pylint setup on landscape.io to only require a docstring if the method is 10+ lines which I thought was reasonable\n",
        "createdAt" : "2016-05-20T16:47:59Z",
        "updatedAt" : "2016-05-20T16:47:59Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb56289743d6f28db667e2eec2509b9e245b37c7",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +3421,3425 @@\n    @provide_session\n    def refresh_from_db(self, session=None):\n        \"\"\"\n        Reloads the current dagrun from the database"
  },
  {
    "id" : "5eb28492-1527-413d-bb7c-660787f5dfd4",
    "prId" : 1506,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2fe569a4-6826-41fd-96b1-a463744c854c",
        "parentId" : null,
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "What's the purpose of this (and maybe consider adding a comment in the code too with your answer)?\n",
        "createdAt" : "2016-05-17T20:09:26Z",
        "updatedAt" : "2016-05-18T10:31:06Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      },
      {
        "id" : "c6f24364-fc8b-4d4c-8668-a92a1330d0dc",
        "parentId" : "2fe569a4-6826-41fd-96b1-a463744c854c",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Ah the important bit: this create_dagrun creates the _full_ dagrun and thus includes creating the taskinstances for this dagrun. Considering that a Dag is an ordered combination of tasks a DagRun should also be this at any given point in time. Ie. the current situation violates this: DagRuns are created shallow, without TaskInstances.\n\nHaving this in resolves issues with some occurrences of deadlocks and depend_on_past and I do consider it important for the integrity of airflow going forward and is one of the reasons of using \"create_dagrun\" to make sure we will be consistent in creating dag runs.\n\nstate for the create TaskInstances will be None / NULL and that does not interfere with any of the checks (confirmed by inspection and tests)\n\nAnd yes I will add a comment in the code.\n",
        "createdAt" : "2016-05-17T20:36:22Z",
        "updatedAt" : "2016-05-18T10:31:06Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      },
      {
        "id" : "d83835b2-93e4-4c38-b8d4-cce8de942b77",
        "parentId" : "2fe569a4-6826-41fd-96b1-a463744c854c",
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "Makes sense, thanks for the thorough explanation!\n",
        "createdAt" : "2016-05-18T17:38:03Z",
        "updatedAt" : "2016-05-18T17:38:03Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb56289743d6f28db667e2eec2509b9e245b37c7",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +3122,3126 @@        session.commit()\n\n        run.refresh_from_db()\n        return run\n"
  },
  {
    "id" : "ef4926c1-8a50-4107-94fa-73b32f8a83e0",
    "prId" : 1506,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23e754e1-e72d-4ef1-aeaf-aff9161bf656",
        "parentId" : null,
        "authorId" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "body" : "Thanks for encapsulating the logic by the way, this is amazing.\n",
        "createdAt" : "2016-05-19T17:19:44Z",
        "updatedAt" : "2016-05-19T17:19:44Z",
        "lastEditedBy" : "0ff67aea-e186-487e-bd0f-83d1c809bd31",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb56289743d6f28db667e2eec2509b9e245b37c7",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +3439,3443 @@    @staticmethod\n    @provide_session\n    def find(dag_id, run_id=None, state=None, external_trigger=None, session=None):\n        \"\"\"\n        Returns a set of dag runs for the given search criteria."
  },
  {
    "id" : "56e3584b-11d6-4243-ab36-46d72a1ba98e",
    "prId" : 1506,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf4dbfad-ad62-4d30-ad24-3c1b2bb0cdbe",
        "parentId" : null,
        "authorId" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "body" : "oh first time I see `one()`. sqla has a huge api... I just looked it up and `one()` raises if empty, meaning the if bellow is not necessary.\n",
        "createdAt" : "2016-05-20T16:50:11Z",
        "updatedAt" : "2016-05-20T16:50:11Z",
        "lastEditedBy" : "0624866b-ba3a-41a3-abab-e487a9cfb78c",
        "tags" : [
        ]
      },
      {
        "id" : "a7b8572f-e2f2-4c06-9600-0dfc9928988c",
        "parentId" : "cf4dbfad-ad62-4d30-ad24-3c1b2bb0cdbe",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Ah yes, I was wondering why it wasn't used a bit more. Kept the 'if' in for consistency, but obviously it should be removed. I will provide a small update for that\n",
        "createdAt" : "2016-05-21T10:13:35Z",
        "updatedAt" : "2016-05-21T10:13:35Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb56289743d6f28db667e2eec2509b9e245b37c7",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +3432,3436 @@            DR.execution_date == self.execution_date,\n            DR.run_id == self.run_id\n        ).one()\n        if dr:\n            self.id = dr.id"
  },
  {
    "id" : "95255600-065b-4d21-8187-26a098756b9c",
    "prId" : 1514,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b29a0ae2-79a3-4a50-a206-92d18ad0433f",
        "parentId" : null,
        "authorId" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "body" : "WIP? or is this for a future refactor PR?\n",
        "createdAt" : "2016-05-25T22:47:56Z",
        "updatedAt" : "2016-06-01T20:58:27Z",
        "lastEditedBy" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "tags" : [
        ]
      },
      {
        "id" : "49d29684-fe09-42dc-b29e-e073259eea72",
        "parentId" : "b29a0ae2-79a3-4a50-a206-92d18ad0433f",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Future. For lineage purposes this needs to be done, now you would loose that. verify_integrity will recreate the tasks if required.\n",
        "createdAt" : "2016-05-26T04:54:10Z",
        "updatedAt" : "2016-06-01T20:58:27Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b18c9959142f3f1e2cb031c8709225af01192e32",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +107,111 @@                ti.state = State.SHUTDOWN\n                job_ids.append(ti.job_id)\n        # todo: this creates an issue with the webui tests\n        #elif ti.state != State.REMOVED:\n        #    ti.state = State.NONE"
  },
  {
    "id" : "4745562b-3938-4863-ac78-549d4f1763b3",
    "prId" : 1514,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4eab45e4-9528-45a5-9436-19a3fa3a7f6b",
        "parentId" : null,
        "authorId" : "08b40e03-e1f4-45e1-80c0-b73f75b29580",
        "body" : "Should this be `not t.task.depends_on_past`?\n",
        "createdAt" : "2016-06-29T04:09:50Z",
        "updatedAt" : "2016-06-29T04:09:50Z",
        "lastEditedBy" : "08b40e03-e1f4-45e1-80c0-b73f75b29580",
        "tags" : [
        ]
      }
    ],
    "commit" : "b18c9959142f3f1e2cb031c8709225af01192e32",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +3455,3459 @@            session=session\n        )\n        none_depends_on_past = all(t.task.depends_on_past for t in unfinished_tasks)\n\n        # small speed up"
  },
  {
    "id" : "ac4c5c8a-dfa4-4ba1-95f2-51f297c7bb81",
    "prId" : 1541,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0a222dc-85ed-4b87-bfcf-21cdf64852ac",
        "parentId" : null,
        "authorId" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "body" : "@bolkedebruin this touches your method. I think it's fine (even though landscape complains about >5 args), but wanted to FYI you, since you're messing with this stuff as part of the scheduler roadmap work as well.\n",
        "createdAt" : "2016-05-26T03:00:30Z",
        "updatedAt" : "2016-05-26T07:47:34Z",
        "lastEditedBy" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "tags" : [
        ]
      }
    ],
    "commit" : "9db00511da22c731d10cdc4ea40942c77b1b4008",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3440,3444 @@    @provide_session\n    def find(dag_id, run_id=None, state=None, external_trigger=None, session=None,\n             execution_date=None):\n        \"\"\"\n        Returns a set of dag runs for the given search criteria."
  },
  {
    "id" : "64c5ebda-059c-4390-810c-4fe410496d27",
    "prId" : 1568,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "020d232f-846f-4354-b045-7a61907749c3",
        "parentId" : null,
        "authorId" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "body" : "Wonder about this logic. Is it possible to get an AirflowException for another reason, and erroneously set the state to REMOVED?\n",
        "createdAt" : "2016-06-02T21:58:06Z",
        "updatedAt" : "2016-06-03T12:59:38Z",
        "lastEditedBy" : "366b1763-372e-4924-9c92-2ea8968443bd",
        "tags" : [
        ]
      },
      {
        "id" : "50f922c4-3032-44b7-8685-3df0e9b57615",
        "parentId" : "020d232f-846f-4354-b045-7a61907749c3",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Not really. Removed is set when the task\nDoes not exist in the DAG. This exception is thrown when the task does not exist in task dictionary of the DAG. \n",
        "createdAt" : "2016-06-03T08:36:43Z",
        "updatedAt" : "2016-06-03T12:59:38Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      },
      {
        "id" : "05840cd6-43b0-464b-a695-efe6ea6ba4c5",
        "parentId" : "020d232f-846f-4354-b045-7a61907749c3",
        "authorId" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "body" : "So is the guard (`if self.state is not State.RUNING`) against it running to protect against this situation:\n1. Task starts running\n2. While task is running, DAG is modified and task is removed\n3. We don't want to interrupt the running task, that's what this guard is for\n\nIf so do you mind just commenting the logic to clarify?\n",
        "createdAt" : "2016-06-03T13:13:11Z",
        "updatedAt" : "2016-06-03T13:13:33Z",
        "lastEditedBy" : "0a40f9fd-4a0a-426e-b6f5-f97a4905eb99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb5a3b3a5d7c2ac40b3907f9e5fdcf333a8e0892",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +3518,3522 @@            try:\n                dag.get_task(ti.task_id)\n            except AirflowException:\n                if self.state is not State.RUNNING:\n                    ti.state = State.REMOVED"
  },
  {
    "id" : "cf1f9abc-7b88-42fa-b606-a73e8dd10d63",
    "prId" : 2781,
    "prUrl" : "https://github.com/apache/airflow/pull/2781#pullrequestreview-76208735",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97e4d720-45bf-4978-8e2c-7fa8a12ae32d",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Docstring says local time, return type says UTC (which I think is what the code returns).",
        "createdAt" : "2017-11-13T18:50:20Z",
        "updatedAt" : "2017-11-27T14:54:40Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1ab56cc6ad3b9419af94aaa333661c105185883",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +3006,3010 @@        Calculates the following schedule for this dag in local time\n        :param dttm: utc datetime\n        :return: utc datetime\n        \"\"\"\n        if isinstance(self._schedule_interval, six.string_types):"
  },
  {
    "id" : "87eded28-956c-423b-95c6-56fea9638fff",
    "prId" : 2781,
    "prUrl" : "https://github.com/apache/airflow/pull/2781#pullrequestreview-78473922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "645b8d8b-30b9-46e0-9282-a8047c449b27",
        "parentId" : null,
        "authorId" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "body" : "The migration script currently does not include this field `last_modified`, so it's still a `DATETIME` object, maybe we want to modify it to timestamp as well",
        "createdAt" : "2017-11-22T01:35:29Z",
        "updatedAt" : "2017-11-27T14:54:40Z",
        "lastEditedBy" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "tags" : [
        ]
      },
      {
        "id" : "bddac5bf-7ebc-4d3e-9f07-d80f3814fbdd",
        "parentId" : "645b8d8b-30b9-46e0-9282-a8047c449b27",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Good catch. I'll fix that",
        "createdAt" : "2017-11-22T07:16:19Z",
        "updatedAt" : "2017-11-27T14:54:40Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      },
      {
        "id" : "0a0dff65-11d4-43a6-8a8e-a531a111da5e",
        "parentId" : "645b8d8b-30b9-46e0-9282-a8047c449b27",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Done",
        "createdAt" : "2017-11-22T15:02:58Z",
        "updatedAt" : "2017-11-27T14:54:40Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1ab56cc6ad3b9419af94aaa333661c105185883",
    "line" : 417,
    "diffHunk" : "@@ -1,1 +3925,3929 @@    x_is_date = Column(Boolean, default=True)\n    iteration_no = Column(Integer, default=0)\n    last_modified = Column(UtcDateTime, default=func.now())\n\n    def __repr__(self):"
  },
  {
    "id" : "eb0df424-9138-4e41-9887-723e767f08c3",
    "prId" : 3174,
    "prUrl" : "https://github.com/apache/airflow/pull/3174#pullrequestreview-111723733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6174a7b6-32ec-4807-b35e-6dfafbb84eaf",
        "parentId" : null,
        "authorId" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "body" : "maybe safer as `elif isinstance(self._schedule_interval, (timedelta, relativedelta))`",
        "createdAt" : "2018-04-11T01:58:00Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "tags" : [
        ]
      },
      {
        "id" : "fc93a008-5e78-42bf-8704-1446423981bf",
        "parentId" : "6174a7b6-32ec-4807-b35e-6dfafbb84eaf",
        "authorId" : "35fe4a2b-7800-4b80-8a96-529ada07237c",
        "body" : "I'd prefer not to hard code the types supported because it undermines Python duck typing and if some other class similar to relativedelta is created this code would need to be updated.  We actually have this use case for a custom fiscal calendar we have implemented.",
        "createdAt" : "2018-04-12T17:40:23Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "35fe4a2b-7800-4b80-8a96-529ada07237c",
        "tags" : [
        ]
      }
    ],
    "commit" : "11e37d37846b4cdb59c8377f1e4bafd2689b2cd3",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +3334,3338 @@            following = timezone.make_aware(cron.get_next(datetime), self.timezone)\n            return timezone.convert_to_utc(following)\n        elif self._schedule_interval is not None:\n            return dttm + self._schedule_interval\n"
  },
  {
    "id" : "059b2f1c-e8d6-429a-bcaa-49323a4053b3",
    "prId" : 3174,
    "prUrl" : "https://github.com/apache/airflow/pull/3174#pullrequestreview-111066519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2d4a785-7e05-4938-8bd9-171d79d40d1b",
        "parentId" : null,
        "authorId" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "body" : "same as above",
        "createdAt" : "2018-04-11T01:58:11Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "tags" : [
        ]
      }
    ],
    "commit" : "11e37d37846b4cdb59c8377f1e4bafd2689b2cd3",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +3349,3353 @@            prev = timezone.make_aware(cron.get_prev(datetime), self.timezone)\n            return timezone.convert_to_utc(prev)\n        elif self._schedule_interval is not None:\n            return dttm - self._schedule_interval\n"
  },
  {
    "id" : "492d0daa-23da-458d-864e-6662657fdc78",
    "prId" : 3174,
    "prUrl" : "https://github.com/apache/airflow/pull/3174#pullrequestreview-111747929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84083a51-d624-4d75-8ff8-3361307cb35e",
        "parentId" : null,
        "authorId" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "body" : "this seems unnecessary and unrelated, cron_presets are always hashable (key is string)",
        "createdAt" : "2018-04-11T02:06:39Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "ef543b7a-dab1-4240-9ec7-b2e2b40b9846",
        "tags" : [
        ]
      },
      {
        "id" : "95d09a59-07c3-451a-b955-c06d00f4d7dd",
        "parentId" : "84083a51-d624-4d75-8ff8-3361307cb35e",
        "authorId" : "35fe4a2b-7800-4b80-8a96-529ada07237c",
        "body" : "relativedelta is not hashable across all versions which causes this line to fail.",
        "createdAt" : "2018-04-12T17:21:37Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "35fe4a2b-7800-4b80-8a96-529ada07237c",
        "tags" : [
        ]
      },
      {
        "id" : "5de8a2df-b30c-4e43-a946-db2b29d1a460",
        "parentId" : "84083a51-d624-4d75-8ff8-3361307cb35e",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Of what? Otherwise increase the baseline version?",
        "createdAt" : "2018-04-12T17:23:10Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      },
      {
        "id" : "31ed47d3-d410-4363-a2a2-a4ba3de12af8",
        "parentId" : "84083a51-d624-4d75-8ff8-3361307cb35e",
        "authorId" : "35fe4a2b-7800-4b80-8a96-529ada07237c",
        "body" : "It looks like V2.7 of dateutil made relativedelta hashable with this commit:\r\n\r\nhttps://github.com/dateutil/dateutil/commit/9cf401f644de5471a843cd18247fbdc589910d86#diff-e4bc4db7d2c23278de20a5498f83fb17\r\n\r\nMaybe we can make dateutil > 2.7 a dependency instead?",
        "createdAt" : "2018-04-12T17:34:57Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "35fe4a2b-7800-4b80-8a96-529ada07237c",
        "tags" : [
        ]
      },
      {
        "id" : "f0aaf70f-6755-40b8-b7a3-07bdfcddce2e",
        "parentId" : "84083a51-d624-4d75-8ff8-3361307cb35e",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Go for it!",
        "createdAt" : "2018-04-12T18:48:42Z",
        "updatedAt" : "2018-07-27T14:45:42Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "11e37d37846b4cdb59c8377f1e4bafd2689b2cd3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +3234,3238 @@\n        self.schedule_interval = schedule_interval\n        if isinstance(schedule_interval, Hashable) and schedule_interval in cron_presets:\n            self._schedule_interval = cron_presets.get(schedule_interval)\n        elif schedule_interval == '@once':"
  },
  {
    "id" : "48594779-3c92-48ea-b407-50592aad7de0",
    "prId" : 3698,
    "prUrl" : "https://github.com/apache/airflow/pull/3698#pullrequestreview-143471272",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b525a671-4f6a-4919-bcee-8271e1c1f05f",
        "parentId" : null,
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "I may be missing something but do we need to perform the check twice?",
        "createdAt" : "2018-08-06T06:42:12Z",
        "updatedAt" : "2018-08-07T23:29:42Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      },
      {
        "id" : "0e5a8605-99ae-4184-920e-6e48ef525156",
        "parentId" : "b525a671-4f6a-4919-bcee-8271e1c1f05f",
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "One is for `.py` DAG file, and the another is for package DAG file (`.zip` file).",
        "createdAt" : "2018-08-06T06:50:34Z",
        "updatedAt" : "2018-08-07T23:29:42Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      },
      {
        "id" : "2781bb51-e1e6-4577-8551-78548400f0b1",
        "parentId" : "b525a671-4f6a-4919-bcee-8271e1c1f05f",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "So we append checked `m` to `mods` here, https://github.com/apache/incubator-airflow/pull/3698/files#diff-a32a363fa616685db3bfefba947535b2R370, and the if statement evaluating zip file or not ends here https://github.com/apache/incubator-airflow/pull/3698/files#diff-a32a363fa616685db3bfefba947535b2R416. So I think the `mods` we are iterating here, https://github.com/apache/incubator-airflow/pull/3698/files#diff-a32a363fa616685db3bfefba947535b2R418, is a combination of modules from `.py` file and modules from zip files.",
        "createdAt" : "2018-08-06T06:55:47Z",
        "updatedAt" : "2018-08-07T23:29:42Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      },
      {
        "id" : "9890d7dc-e7e2-40ae-8b86-f7de731131dc",
        "parentId" : "b525a671-4f6a-4919-bcee-8271e1c1f05f",
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "Hi @yrqls21 , you're right. Thanks for pointing this out.\r\n\r\nI'll modify accordingly.",
        "createdAt" : "2018-08-06T07:01:23Z",
        "updatedAt" : "2018-08-07T23:29:42Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a5936fa3274d1009657aa89f95c874f21eb98ef",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +426,430 @@                            \"Invalid Cron expression: \" + str(cron_e)\n                        self.file_last_changed[dag.full_filepath] = \\\n                            file_last_changed_on_disk\n                    except AirflowDagCycleException as cycle_exception:\n                        self.log.exception(\"Failed to bag_dag: %s\", dag.full_filepath)"
  },
  {
    "id" : "4af78427-3015-4562-b95c-9800dded672e",
    "prId" : 3939,
    "prUrl" : "https://github.com/apache/airflow/pull/3939#pullrequestreview-157956040",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ff66d4a-fe0f-46c0-9c1d-9292850afc63",
        "parentId" : null,
        "authorId" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "body" : "Where in the file system does this .airflowignore need to be? Why not have a placeholder (empty) .airflowignore committed or part of the distribution, so folks know to override it in place?",
        "createdAt" : "2018-09-23T17:46:53Z",
        "updatedAt" : "2018-09-27T13:51:17Z",
        "lastEditedBy" : "92e402fb-479e-4b08-88f2-3b82c356ecc3",
        "tags" : [
        ]
      },
      {
        "id" : "57be36e8-afa0-466c-a97f-c2486783f0d7",
        "parentId" : "2ff66d4a-fe0f-46c0-9c1d-9292850afc63",
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "This .airflowignore file needs to be in DAG_FOLDER, then it can be detected when we parse the DAG files.\r\n\r\nIt’s a great idea to have a placeholder .airflowignore. But it’s a bit tricky to do this since users normally need to create “dags” folder by themselves after initdb.",
        "createdAt" : "2018-09-23T23:03:15Z",
        "updatedAt" : "2018-09-27T13:51:17Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      },
      {
        "id" : "a88e60f9-5de7-4e9b-851b-8505f96abb53",
        "parentId" : "2ff66d4a-fe0f-46c0-9c1d-9292850afc63",
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "I didn’t know the existence of this nice feature either previously. that’s exactly why we need to mention it to users more explicitly in the documentation ;-)",
        "createdAt" : "2018-09-23T23:30:56Z",
        "updatedAt" : "2018-09-27T13:51:17Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      }
    ],
    "commit" : "52cb6b643c9a9bdcbea2fca5feb1b74779882d88",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +523,527 @@        imports them and adds them to the dagbag collection.\n\n        Note that if a ``.airflowignore`` file is found while processing\n        the directory, it will behave much like a ``.gitignore``,\n        ignoring files that match any of the regex patterns specified"
  },
  {
    "id" : "2ba63ff4-6bbb-4553-822e-a9910d44f108",
    "prId" : 3956,
    "prUrl" : "https://github.com/apache/airflow/pull/3956#pullrequestreview-159223197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35f7ecaf-3c9f-4645-8210-68b74d8eb037",
        "parentId" : null,
        "authorId" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "body" : "Nit: How about just context_manager_dags since it includes the current one?",
        "createdAt" : "2018-09-26T21:53:42Z",
        "updatedAt" : "2018-09-28T15:30:51Z",
        "lastEditedBy" : "06a356e2-8fb5-4f99-b8bb-46ce12d2daa0",
        "tags" : [
        ]
      },
      {
        "id" : "a69a5267-9f03-496a-ae97-d858f2aa3246",
        "parentId" : "35f7ecaf-3c9f-4645-8210-68b74d8eb037",
        "authorId" : "6e4118f4-d81c-432b-b037-36a4cba6d92e",
        "body" : "Actually this doesn't contain the current one. It just contains what was in `_CONTEXT_MANAGER_DAG` before it is set to the current DAG. The `old` naming convention was existing, and the only difference here is making into a list.",
        "createdAt" : "2018-09-26T22:28:58Z",
        "updatedAt" : "2018-09-28T15:30:51Z",
        "lastEditedBy" : "6e4118f4-d81c-432b-b037-36a4cba6d92e",
        "tags" : [
        ]
      }
    ],
    "commit" : "17a65122543f3d44f36118d14fbec527dd42a642",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +3392,3396 @@        self.on_failure_callback = on_failure_callback\n\n        self._old_context_manager_dags = []\n\n        self._comps = {"
  },
  {
    "id" : "d3c42bd2-33f2-48a5-b865-d94f758416ca",
    "prId" : 3994,
    "prUrl" : "https://github.com/apache/airflow/pull/3994#pullrequestreview-162738532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8645444e-eef2-4ec0-8332-004be3e132da",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Right now the PK columns of the TI table is (self.dag_id, self.task_id, self.execution_date) and try_number is adjusted in place.\r\n\r\nThis change might be a good idea but I think it is more complex than just this -- for instance it probably makes sense to store each individual try in the task instance table, but that is a much bigger change.",
        "createdAt" : "2018-10-03T08:51:56Z",
        "updatedAt" : "2018-10-04T13:58:51Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "414b0fba-847f-4f11-adbb-a39aee5b6c62",
        "parentId" : "8645444e-eef2-4ec0-8332-004be3e132da",
        "authorId" : "0c0a2937-4012-41eb-b4a4-4716de9ebc4a",
        "body" : "Thanks @ashb for your comment.\r\n\r\nStoring individual try information in task_instance table will actually connect more dots, currently we get individual task try information by joining with task_fail table, though not explicitly mentioned for which try job was failed but playing with timestamps, one can infer retry_number.\r\n\r\nOn the other side, I guess it's okay to keep schema as is because it will not add additional load on task_instance table instead it will keep on delegating load to task_fail only which has comparatively lesser or almost no reads, however adding retry_number in task_fail table will add value.\r\n\r\nIMHO Keeping **key** property only for TaskInstance class seems to be a special behaviour probably it was meant only for scheduler to convey eligible task information to executor queued_tasks, if that's not true, we can also introduce  executor_key as another property and make existing references to this new property, thoughts? \r\n",
        "createdAt" : "2018-10-04T12:35:01Z",
        "updatedAt" : "2018-10-04T13:58:51Z",
        "lastEditedBy" : "0c0a2937-4012-41eb-b4a4-4716de9ebc4a",
        "tags" : [
        ]
      },
      {
        "id" : "80ee58e9-3d6b-4db8-a251-b78f4139e4d7",
        "parentId" : "8645444e-eef2-4ec0-8332-004be3e132da",
        "authorId" : "0c0a2937-4012-41eb-b4a4-4716de9ebc4a",
        "body" : "@ashb Looking forward to your feedback!",
        "createdAt" : "2018-10-08T06:35:55Z",
        "updatedAt" : "2018-10-08T06:35:55Z",
        "lastEditedBy" : "0c0a2937-4012-41eb-b4a4-4716de9ebc4a",
        "tags" : [
        ]
      },
      {
        "id" : "7ecf9823-6dd0-4171-922e-25bfd172f97e",
        "parentId" : "8645444e-eef2-4ec0-8332-004be3e132da",
        "authorId" : "0c0a2937-4012-41eb-b4a4-4716de9ebc4a",
        "body" : "This collision situation is arising because of the following steps happening periodically by scheduler.\r\n\r\n1.  In the _execute_helper method, heartbeat method is being called, which calls execute_async in further and putting key(dag_id, task_id, execution_date) and command to executor queue and then from there onwards based on executor type, task starts executing asynchronously.\r\n2.  Sync methods runs followed by execute_async method which push keys from result_queue to event_buffer\r\n3.  _process_executor_events method being called followed by hearbeat method which detects external killing of task, if TI.state is queued and event_buffer key status is success/failed\r\n\r\nAnother suggestion for change could be taking out execute_async method from heartbeat method and call it after _process_executor_events method call in _execute_helper method in jobs.py\r\n",
        "createdAt" : "2018-10-09T06:57:43Z",
        "updatedAt" : "2018-10-09T06:57:43Z",
        "lastEditedBy" : "0c0a2937-4012-41eb-b4a4-4716de9ebc4a",
        "tags" : [
        ]
      }
    ],
    "commit" : "4eebf95e7ecd75dc476e0076dbf72245726dee5e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1231,1235 @@        Returns a tuple that identifies the task instance uniquely\n        \"\"\"\n        return self.dag_id, self.task_id, self.execution_date, self.try_number\n\n    @provide_session"
  },
  {
    "id" : "d1c6ca21-1c1a-4acb-997f-5fd7e7b948a6",
    "prId" : 4109,
    "prUrl" : "https://github.com/apache/airflow/pull/4109#pullrequestreview-169103612",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf2edd3c-b6ed-46ce-85a2-765acf736e6e",
        "parentId" : null,
        "authorId" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "body" : "Nice one",
        "createdAt" : "2018-10-28T18:28:37Z",
        "updatedAt" : "2018-10-28T22:29:52Z",
        "lastEditedBy" : "03e41e23-f438-4a06-9652-8f20638d2c3a",
        "tags" : [
        ]
      }
    ],
    "commit" : "cd9ac485cb7cd9f125b59bc63216b5a62b4b222c",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +820,824 @@                       self.schema,\n                       self.login,\n                       \"XXXXXXXX\" if self.password else None,\n                       self.extra_dejson))\n"
  }
]