[
  {
    "id" : "4f37d90a-7647-4f09-9077-d35ea872e1a0",
    "prId" : 4083,
    "prUrl" : "https://github.com/apache/airflow/pull/4083#pullrequestreview-191376567",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "9 characters is not long enough for a UUID - is this just a random identifier?",
        "createdAt" : "2018-10-23T09:26:05Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "a4e04a38-b99c-42ec-bda4-003fe7dcce46",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "body" : "The Airflow Dataproc job builder truncates the UUID to 8 characters [here](https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/hooks/gcp_dataproc_hook.py#L94), so the task name submitted to Airflow consists of the task ID + underscore + an 8 digit ID. The 9 characters in my PR represents the 8 UUID characters plus the underscore.\r\n\r\n",
        "createdAt" : "2018-10-23T14:26:52Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "tags" : [
        ]
      },
      {
        "id" : "a6fd417c-feb8-417b-87ad-fb31715ac801",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "1aceaa64-1d3b-44c8-b7b9-3404a109eb8d",
        "body" : "This seems to be a bit error prone. How about the following? Let's modify the submit job operator interface by taking a new parameter job_dedupe_regex, when set, the hook will match existing jobs with this job_dedupe_regex and skip a new submission if a new match is found and other conditions are met. \r\n\r\nThis way, the hook implementation is entirely decoupled from the operator implementation re: job-name and open to other ways of deduping dataproc jobs. ",
        "createdAt" : "2018-10-23T19:16:06Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "1aceaa64-1d3b-44c8-b7b9-3404a109eb8d",
        "tags" : [
        ]
      },
      {
        "id" : "d1a7afbe-7981-4f2d-a33b-0ed3047b4a72",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "body" : "Thank you @fenglu-g. Just to clarify, do you mean that I should modify the [`DataProcHook.submit`](https://github.com/apache/incubator-airflow/blob/0e8394fd23d067b7e226c011bb1825ff734219c5/airflow/contrib/hooks/gcp_dataproc_hook.py#L229) and [`_DataProcJob.__init__`](https://github.com/apache/incubator-airflow/blob/0e8394fd23d067b7e226c011bb1825ff734219c5/airflow/contrib/hooks/gcp_dataproc_hook.py#L31) functions to take in the new regex param like so?\r\n\r\n```\r\nclass _DataProcJob(LoggingMixin):\r\n    def __init__(other args..., job_dedupe_regex=None):\r\n\r\nclass DataProcHook(GoogleCloudBaseHook):\r\n    def submit(other args..., job_dedupe_regex=[by default, regex that dedupes by matching the task ID]):\r\n```\r\nAnd then have `submit()` pass the regex along to the _DataProcJob init [here](https://github.com/apache/incubator-airflow/blob/0e8394fd23d067b7e226c011bb1825ff734219c5/airflow/contrib/hooks/gcp_dataproc_hook.py#L230\r\n), because the deduping happens during the init:\r\n\r\n```\r\ndef submit(other args..., job_dedupe_regex=[by default, regex that dedupes by matching the task ID]):\r\n    submitted = _DataProcJob(other args..., job_dedupe_regex=job_dedupe_regex)\r\n```",
        "createdAt" : "2018-10-25T19:04:08Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "tags" : [
        ]
      },
      {
        "id" : "3bd59d61-01ad-4de0-8d03-452749b8536c",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "@fenglu-g Can you take a look at this and let me know if you have any further comments?",
        "createdAt" : "2018-10-26T08:34:29Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "6e3d600f-8db5-4d09-a608-1f8954ad3420",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "body" : "@fenglu-g Just wanted to clarify if my interpretation of your comment was in line with what you meant? If so I'll go ahead and implement it. Thanks!",
        "createdAt" : "2018-10-30T20:36:53Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "tags" : [
        ]
      },
      {
        "id" : "0577f48b-b9f9-46b1-85ea-67c09a50cbb9",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "1aceaa64-1d3b-44c8-b7b9-3404a109eb8d",
        "body" : "Sorry for the late reply, yes but we should also surface this job_dedupe_regex argument to dataproc submit job operators (e.g., [DataProcSparkOperator](https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/dataproc_operator.py#L982)), wdyt? ",
        "createdAt" : "2018-10-31T21:10:05Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "1aceaa64-1d3b-44c8-b7b9-3404a109eb8d",
        "tags" : [
        ]
      },
      {
        "id" : "689434bb-fa69-4d91-9091-68ec13144c2b",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "body" : "Oh gotcha @fenglu-g, so users can provide their own regex (e.g. `DataProcSparkOperator(other args, [my_dedupe_regex_here]`) in `DataprocSparkOperator`, `DataprocPigOperator`, etc. I'll make that change. And sorry about my (very) late reply!",
        "createdAt" : "2018-11-26T21:32:33Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "tags" : [
        ]
      },
      {
        "id" : "d83cbcf3-2790-4de4-a40d-86a8b2232171",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "body" : "Hi @fenglu-g , I started working on the regex idea and I don't think it makes sense to have the user define the deduping logic. \r\n\r\nIt seems more error prone to me to have the user define the job ID they want to use for deduping, since the job ID is not user defined, but is instead hardcoded by Airflow here as the task ID + first 8 characters of UUID: https://github.com/apache/incubator-airflow/blob/0e8394fd23d067b7e226c011bb1825ff734219c5/airflow/contrib/hooks/gcp_dataproc_hook.py#L94. Since Airflow sets the job ID, Airflow should also set the deduping logic.\r\n\r\nI understand your concern about not wanting to hardcode the deduping logic, though. A solution for this could be: instead of having Airflow hardcode the job ID as in the link above, to have it instead call a function that creates the job ID from the task ID. And then my deduping logic would reference an inverse of that function to extract the task ID for deduping instead of hardcoding the deduping logic. \r\n\r\nIn other words, Airflow would create the job ID with a function that maps the task ID -> job ID. And then the dedupe mechanism would call a function that maps from job ID back to task ID to use in deduping. Tests will be added to make sure that these two functions are inverses of each other. \r\n\r\nHow does that sound?",
        "createdAt" : "2018-12-06T20:03:26Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "tags" : [
        ]
      },
      {
        "id" : "866a2711-a8a5-418c-b0a0-5590c364d64d",
        "parentId" : "1a01ed8f-0512-4ea7-90c3-9b6d64dc2df2",
        "authorId" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "body" : "I'm going to go ahead with implementing the solution I proposed above @fenglu-g; if you have any reservations please let me know.",
        "createdAt" : "2019-01-10T19:29:23Z",
        "updatedAt" : "2019-06-12T02:17:22Z",
        "lastEditedBy" : "d982ef83-ba6e-46a3-a538-63f65ea432f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "73bd599e20d180a7933a0762fc089e84212f092f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +50,54 @@            clusterName=cluster_name).execute()\n\n        UUID_LENGTH = 9\n        jobs_on_cluster = jobs_on_cluster_response.get('jobs', [])\n        try:"
  }
]