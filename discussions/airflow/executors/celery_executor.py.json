[
  {
    "id" : "2f2ec912-311d-44c6-a19e-bb7a3037e4e2",
    "prId" : 3740,
    "prUrl" : "https://github.com/apache/airflow/pull/3740#pullrequestreview-148855599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "parentId" : null,
        "authorId" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "body" : "@bolkedebruin \r\nJust found an issue for this commit. The update for function \"apply_async\" will crash scheduler when using celery executor. Tested in local environment.\r\nConcern for update in line 87:\r\n>**Before**: \r\n>`args=[command]`  and command is a unicode type string . \r\n> samples: `command = \"airflow run dag323...\"` and at this time `args = [\"airflow run example\"]`\r\n>`execute_command(\"airflow run dag323\")` asynchronously\r\n>**After**: \r\n>`args=command` and command is a list of short unicode strings.\r\n> samples: `command = [\"airflow\", \"run\", \"dag323\",...]` and now`args = [\"airflow\", \"run\", \"dag323\", ...]`\r\n>`execute_command(\"airflow\", \"run\", \"dag323\",...)` ** Error here**\r\n\r\n`execute_command.apply_async(args = command, queue = queue)` will pass a list of arguments rather than one to function `execute_command` which is defined as taking only one argument. The test in test_celery_executor.py right now only have one item in the testing `command` list and can not cover this case.",
        "createdAt" : "2018-08-22T00:44:42Z",
        "updatedAt" : "2018-08-22T00:46:06Z",
        "lastEditedBy" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "tags" : [
        ]
      },
      {
        "id" : "59ee547c-d368-4b3e-891c-35a7cb1b6587",
        "parentId" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "@YingboWang , good find. Do you want to create a pr for this issue?",
        "createdAt" : "2018-08-22T03:41:54Z",
        "updatedAt" : "2018-08-22T03:41:55Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "575e7d36-2933-4812-af17-564e8ef577e7",
        "parentId" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "authorId" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "body" : "I would like to. ",
        "createdAt" : "2018-08-22T04:43:06Z",
        "updatedAt" : "2018-08-22T04:43:06Z",
        "lastEditedBy" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "tags" : [
        ]
      },
      {
        "id" : "bee26ca4-ed1b-40fc-8b7a-5b9304735143",
        "parentId" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Good catch! I do think renaming the parmeter is the right way. \r\n\r\nUpdate: strike that got why it happens with celery",
        "createdAt" : "2018-08-23T10:50:46Z",
        "updatedAt" : "2018-08-23T10:53:18Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7403b86f0543a673e9b74e592184853e25faae9",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +85,89 @@                      \"queue={queue}\".format(**locals()))\n        self.tasks[key] = execute_command.apply_async(\n            args=command, queue=queue)\n        self.last_state[key] = celery_states.PENDING\n"
  },
  {
    "id" : "723f8542-0a9a-440e-8d0b-fb67cfdbd816",
    "prId" : 3773,
    "prUrl" : "https://github.com/apache/airflow/pull/3773#pullrequestreview-147879344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03fa6cd5-97f2-4223-aa86-1739b2db285d",
        "parentId" : null,
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "- The indentation change is to fix **`potential bug-1`**.\r\n\r\n- Changing from `task.state` to `state` in both lines 107 and 108 are to fix **`potential bug-2`**\r\n",
        "createdAt" : "2018-08-21T01:40:18Z",
        "updatedAt" : "2018-08-21T01:40:39Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      }
    ],
    "commit" : "e62f9b1780d5676826aa2be43aa297d16750b23d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +106,110 @@                    else:\n                        self.log.info(\"Unexpected state: %s\", state)\n                        self.last_state[key] = state\n            except Exception as e:\n                self.log.error(\"Error syncing the celery executor, ignoring it:\")"
  },
  {
    "id" : "61f20387-1bb4-46d4-bac7-11f2a4467382",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-152303805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "605fc038-569d-4e90-810a-85e956204ea0",
        "parentId" : null,
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "the logging usage doesn't seem to be consistent(self.logger vs self.log)",
        "createdAt" : "2018-09-04T22:28:13Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "cd655492-5a05-4dc0-90d1-7fd50046952a",
        "parentId" : "605fc038-569d-4e90-810a-85e956204ea0",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "My bad here, we are still on 1.8. Will update.",
        "createdAt" : "2018-09-05T00:58:20Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +170,174 @@        chunksize = self._num_tasks_per_process()\n\n        self.log.debug(\"Waiting for inquiries to complete...\")\n        task_keys_to_states = self._sync_pool.map(\n            fetch_celery_task_state,"
  },
  {
    "id" : "2763324c-1951-4604-a201-9069d29592dc",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-152304933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9cf2fda-3361-4b48-9838-3bf2e4663117",
        "parentId" : null,
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "does the network call happen here? Based on http://docs.celeryproject.org/en/latest/reference/celery.result.html, it uses celery_task.collect to fetch the result. I don't quite get the how the network call happens in this func though.",
        "createdAt" : "2018-09-04T22:51:22Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "cfbb64fe-27c3-430f-8ddb-bd546585f53e",
        "parentId" : "f9cf2fda-3361-4b48-9838-3bf2e4663117",
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "And do we retry to gather the task state?",
        "createdAt" : "2018-09-04T23:01:15Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "3cb25e9c-34bc-4923-bd1b-996cf2a073af",
        "parentId" : "f9cf2fda-3361-4b48-9838-3bf2e4663117",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "celery.task.state will make the network call to fetch the task state from celery. collect() and get() are blocking methods that trying to get the result of the task. Here I don't retry but just expect the next fetching attempt will success, to make the logic simpler as we don't retry previously( tho fetching in parallel may have bigger chance to fail, wait for retry on the next loop does not hurt too much).",
        "createdAt" : "2018-09-05T01:06:25Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +84,88 @@\n\ndef fetch_celery_task_state(celery_task):\n    \"\"\"\n    Fetch and return the state of the given celery task. The scope of this function is"
  },
  {
    "id" : "3d86237c-9de3-4f05-bfe3-8758ae68873d",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-153530694",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bd67d45-1e3d-415e-8a44-dd90ad4a451b",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Does this have to be a single param. Could we instead have this as:\r\n\r\n```\r\ndef fetch_celery_task_state(task_id, async_result):\r\n```\r\n\r\n",
        "createdAt" : "2018-09-06T09:08:36Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "bf2e0797-d115-4564-9a98-625ca2665a94",
        "parentId" : "0bd67d45-1e3d-415e-8a44-dd90ad4a451b",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "No it does not have to be a single param. Doing it in this way so that we don't need to unpack the `tasks` dict and thus looks cleaner.",
        "createdAt" : "2018-09-07T23:32:49Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +84,88 @@\n\ndef fetch_celery_task_state(celery_task):\n    \"\"\"\n    Fetch and return the state of the given celery task. The scope of this function is"
  },
  {
    "id" : "2ef0a775-02b1-4e9c-8047-e55447226e0b",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-153536452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92c542ec-0caf-47bc-b713-485d1580c60f",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Do we need to do any special handling to ensure of the SQLA connection when using multiprocssing? (I guess to make sure we don't end up with 16 extra connections to the DB that we don't need)",
        "createdAt" : "2018-09-06T09:23:09Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "641213cc-95a5-436d-b2e4-586e93aac38a",
        "parentId" : "92c542ec-0caf-47bc-b713-485d1580c60f",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "I believe multiprocessing with use os.fork() on unix systems and thus we can take advantage of COW to reduce ram usage. However AFAIK on Windows child process will reimport all module level imports and thus may require some extra ram. But I don't think the extra imports will create a big burden on the ram usage( or maybe our scheduler box is just too big :P). I'll add an entry in UPDATING.md regarding this config line.\r\n\r\nAbout the SQLA connection, you actually have the point. On Windows we might ended up configuring extra 16 connection pools while reimporting. And since subprocesses spun up by multiprocessing module do not run atexit() we might leave some hanging connections there in theory. However from my observation and test, SQLA initializes connections lazily and thus we at most have empty pool in the subprocesses.\r\n\r\nI might be wrong about the Windows thing and SQLA lazy initialization thing, open to discuss better handling if that is the case.\r\n\r\nFYI this is the test script/result I was playing with:\r\n```\r\n▶ cat test.py\r\nimport os\r\nfrom multiprocessing import Pool\r\n\r\nprint('execute module code')\r\n\r\ndef test_func(num):\r\n    print(num)\r\n\r\nif __name__ == '__main__':\r\n    pool = Pool(4)\r\n    results = pool.map(test_func, [1,2,3,4], 1)\r\n    pool.close()\r\n    pool.join()\r\n▶ python test.py\r\nexecute module code\r\n1\r\n2\r\n3\r\n4\r\n\r\n-------------- mimic Windows behavior ----------------\r\n▶ cat test.py\r\nimport os\r\nfrom multiprocessing import Pool\r\n\r\nprint('execute module code')\r\n\r\ndef test_func(num):\r\n    from airflow import settings\r\n    print(num)\r\n    print(settings.engine.pool.status())\r\n\r\nif __name__ == '__main__':\r\n    pool = Pool(4)\r\n    results = pool.map(test_func, [1,2,3,4], 1)\r\n    pool.close()\r\n    pool.join()\r\n\r\n▶ python test.py\r\nexecute module code\r\nexecute module code\r\nexecute module code\r\nexecute module code\r\nexecute module code\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80204)\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80202)\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80201)\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80203)\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\n4\r\n1\r\n2\r\n3\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\n```",
        "createdAt" : "2018-09-08T00:38:20Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +164,168 @@\n        # Recreate the process pool each sync in case processes in the pool die\n        self._sync_pool = Pool(processes=num_processes)\n\n        # Use chunking instead of a work queue to reduce context switching since tasks are"
  },
  {
    "id" : "9a6cab7b-c76d-47ad-b8f8-17df521bef46",
    "prId" : 4234,
    "prUrl" : "https://github.com/apache/airflow/pull/4234#pullrequestreview-178737868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c0f3f19-e12c-4bef-985e-edae023ef406",
        "parentId" : null,
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "Minor: One empty line is expected before this line to render Sphinx properly",
        "createdAt" : "2018-11-27T11:44:37Z",
        "updatedAt" : "2018-11-27T21:39:13Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      }
    ],
    "commit" : "554b4cf08bcf371a95415d9a0cd88820f2d9272c",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +157,161 @@        How many Celery tasks should each worker process send.\n\n        :return: Number of tasks that should be sent per process\n        :rtype: int\n        \"\"\""
  }
]