[
  {
    "id" : "2f2ec912-311d-44c6-a19e-bb7a3037e4e2",
    "prId" : 3740,
    "prUrl" : "https://github.com/apache/airflow/pull/3740#pullrequestreview-148855599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "parentId" : null,
        "authorId" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "body" : "@bolkedebruin \r\nJust found an issue for this commit. The update for function \"apply_async\" will crash scheduler when using celery executor. Tested in local environment.\r\nConcern for update in line 87:\r\n>**Before**: \r\n>`args=[command]`  and command is a unicode type string . \r\n> samples: `command = \"airflow run dag323...\"` and at this time `args = [\"airflow run example\"]`\r\n>`execute_command(\"airflow run dag323\")` asynchronously\r\n>**After**: \r\n>`args=command` and command is a list of short unicode strings.\r\n> samples: `command = [\"airflow\", \"run\", \"dag323\",...]` and now`args = [\"airflow\", \"run\", \"dag323\", ...]`\r\n>`execute_command(\"airflow\", \"run\", \"dag323\",...)` ** Error here**\r\n\r\n`execute_command.apply_async(args = command, queue = queue)` will pass a list of arguments rather than one to function `execute_command` which is defined as taking only one argument. The test in test_celery_executor.py right now only have one item in the testing `command` list and can not cover this case.",
        "createdAt" : "2018-08-22T00:44:42Z",
        "updatedAt" : "2018-08-22T00:46:06Z",
        "lastEditedBy" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "tags" : [
        ]
      },
      {
        "id" : "59ee547c-d368-4b3e-891c-35a7cb1b6587",
        "parentId" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "@YingboWang , good find. Do you want to create a pr for this issue?",
        "createdAt" : "2018-08-22T03:41:54Z",
        "updatedAt" : "2018-08-22T03:41:55Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "575e7d36-2933-4812-af17-564e8ef577e7",
        "parentId" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "authorId" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "body" : "I would like to. ",
        "createdAt" : "2018-08-22T04:43:06Z",
        "updatedAt" : "2018-08-22T04:43:06Z",
        "lastEditedBy" : "b2325481-de41-4b6d-a7c1-7152249759ba",
        "tags" : [
        ]
      },
      {
        "id" : "bee26ca4-ed1b-40fc-8b7a-5b9304735143",
        "parentId" : "04b32474-41e0-47b4-834b-715db797e0a9",
        "authorId" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "body" : "Good catch! I do think renaming the parmeter is the right way. \r\n\r\nUpdate: strike that got why it happens with celery",
        "createdAt" : "2018-08-23T10:50:46Z",
        "updatedAt" : "2018-08-23T10:53:18Z",
        "lastEditedBy" : "537cec6b-32b7-4f8b-9f76-6932246f79b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7403b86f0543a673e9b74e592184853e25faae9",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +85,89 @@                      \"queue={queue}\".format(**locals()))\n        self.tasks[key] = execute_command.apply_async(\n            args=command, queue=queue)\n        self.last_state[key] = celery_states.PENDING\n"
  },
  {
    "id" : "723f8542-0a9a-440e-8d0b-fb67cfdbd816",
    "prId" : 3773,
    "prUrl" : "https://github.com/apache/airflow/pull/3773#pullrequestreview-147879344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03fa6cd5-97f2-4223-aa86-1739b2db285d",
        "parentId" : null,
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "- The indentation change is to fix **`potential bug-1`**.\r\n\r\n- Changing from `task.state` to `state` in both lines 107 and 108 are to fix **`potential bug-2`**\r\n",
        "createdAt" : "2018-08-21T01:40:18Z",
        "updatedAt" : "2018-08-21T01:40:39Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      }
    ],
    "commit" : "e62f9b1780d5676826aa2be43aa297d16750b23d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +106,110 @@                    else:\n                        self.log.info(\"Unexpected state: %s\", state)\n                        self.last_state[key] = state\n            except Exception as e:\n                self.log.error(\"Error syncing the celery executor, ignoring it:\")"
  },
  {
    "id" : "61f20387-1bb4-46d4-bac7-11f2a4467382",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-152303805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "605fc038-569d-4e90-810a-85e956204ea0",
        "parentId" : null,
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "the logging usage doesn't seem to be consistent(self.logger vs self.log)",
        "createdAt" : "2018-09-04T22:28:13Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "cd655492-5a05-4dc0-90d1-7fd50046952a",
        "parentId" : "605fc038-569d-4e90-810a-85e956204ea0",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "My bad here, we are still on 1.8. Will update.",
        "createdAt" : "2018-09-05T00:58:20Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +170,174 @@        chunksize = self._num_tasks_per_process()\n\n        self.log.debug(\"Waiting for inquiries to complete...\")\n        task_keys_to_states = self._sync_pool.map(\n            fetch_celery_task_state,"
  },
  {
    "id" : "2763324c-1951-4604-a201-9069d29592dc",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-152304933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9cf2fda-3361-4b48-9838-3bf2e4663117",
        "parentId" : null,
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "does the network call happen here? Based on http://docs.celeryproject.org/en/latest/reference/celery.result.html, it uses celery_task.collect to fetch the result. I don't quite get the how the network call happens in this func though.",
        "createdAt" : "2018-09-04T22:51:22Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "cfbb64fe-27c3-430f-8ddb-bd546585f53e",
        "parentId" : "f9cf2fda-3361-4b48-9838-3bf2e4663117",
        "authorId" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "body" : "And do we retry to gather the task state?",
        "createdAt" : "2018-09-04T23:01:15Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "38d80383-47b9-439a-9efe-9282f79f8b2f",
        "tags" : [
        ]
      },
      {
        "id" : "3cb25e9c-34bc-4923-bd1b-996cf2a073af",
        "parentId" : "f9cf2fda-3361-4b48-9838-3bf2e4663117",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "celery.task.state will make the network call to fetch the task state from celery. collect() and get() are blocking methods that trying to get the result of the task. Here I don't retry but just expect the next fetching attempt will success, to make the logic simpler as we don't retry previously( tho fetching in parallel may have bigger chance to fail, wait for retry on the next loop does not hurt too much).",
        "createdAt" : "2018-09-05T01:06:25Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +84,88 @@\n\ndef fetch_celery_task_state(celery_task):\n    \"\"\"\n    Fetch and return the state of the given celery task. The scope of this function is"
  },
  {
    "id" : "3d86237c-9de3-4f05-bfe3-8758ae68873d",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-153530694",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bd67d45-1e3d-415e-8a44-dd90ad4a451b",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Does this have to be a single param. Could we instead have this as:\r\n\r\n```\r\ndef fetch_celery_task_state(task_id, async_result):\r\n```\r\n\r\n",
        "createdAt" : "2018-09-06T09:08:36Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "bf2e0797-d115-4564-9a98-625ca2665a94",
        "parentId" : "0bd67d45-1e3d-415e-8a44-dd90ad4a451b",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "No it does not have to be a single param. Doing it in this way so that we don't need to unpack the `tasks` dict and thus looks cleaner.",
        "createdAt" : "2018-09-07T23:32:49Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +84,88 @@\n\ndef fetch_celery_task_state(celery_task):\n    \"\"\"\n    Fetch and return the state of the given celery task. The scope of this function is"
  },
  {
    "id" : "2ef0a775-02b1-4e9c-8047-e55447226e0b",
    "prId" : 3830,
    "prUrl" : "https://github.com/apache/airflow/pull/3830#pullrequestreview-153536452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92c542ec-0caf-47bc-b713-485d1580c60f",
        "parentId" : null,
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Do we need to do any special handling to ensure of the SQLA connection when using multiprocssing? (I guess to make sure we don't end up with 16 extra connections to the DB that we don't need)",
        "createdAt" : "2018-09-06T09:23:09Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "641213cc-95a5-436d-b2e4-586e93aac38a",
        "parentId" : "92c542ec-0caf-47bc-b713-485d1580c60f",
        "authorId" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "body" : "I believe multiprocessing with use os.fork() on unix systems and thus we can take advantage of COW to reduce ram usage. However AFAIK on Windows child process will reimport all module level imports and thus may require some extra ram. But I don't think the extra imports will create a big burden on the ram usage( or maybe our scheduler box is just too big :P). I'll add an entry in UPDATING.md regarding this config line.\r\n\r\nAbout the SQLA connection, you actually have the point. On Windows we might ended up configuring extra 16 connection pools while reimporting. And since subprocesses spun up by multiprocessing module do not run atexit() we might leave some hanging connections there in theory. However from my observation and test, SQLA initializes connections lazily and thus we at most have empty pool in the subprocesses.\r\n\r\nI might be wrong about the Windows thing and SQLA lazy initialization thing, open to discuss better handling if that is the case.\r\n\r\nFYI this is the test script/result I was playing with:\r\n```\r\nâ–¶ cat test.py\r\nimport os\r\nfrom multiprocessing import Pool\r\n\r\nprint('execute module code')\r\n\r\ndef test_func(num):\r\n    print(num)\r\n\r\nif __name__ == '__main__':\r\n    pool = Pool(4)\r\n    results = pool.map(test_func, [1,2,3,4], 1)\r\n    pool.close()\r\n    pool.join()\r\nâ–¶ python test.py\r\nexecute module code\r\n1\r\n2\r\n3\r\n4\r\n\r\n-------------- mimic Windows behavior ----------------\r\nâ–¶ cat test.py\r\nimport os\r\nfrom multiprocessing import Pool\r\n\r\nprint('execute module code')\r\n\r\ndef test_func(num):\r\n    from airflow import settings\r\n    print(num)\r\n    print(settings.engine.pool.status())\r\n\r\nif __name__ == '__main__':\r\n    pool = Pool(4)\r\n    results = pool.map(test_func, [1,2,3,4], 1)\r\n    pool.close()\r\n    pool.join()\r\n\r\nâ–¶ python test.py\r\nexecute module code\r\nexecute module code\r\nexecute module code\r\nexecute module code\r\nexecute module code\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80204)\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80202)\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80201)\r\nairflow.settings [2018-09-07 17:37:24,218] {{settings.py:148}} DEBUG - Setting up DB connection pool (PID 80203)\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nairflow.settings [2018-09-07 17:37:24,219] {{settings.py:176}} INFO - setting.configure_orm(): Using pool settings. pool_size=5, pool_recycle=3600\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\nairflow.utils.log.logging_mixin.LoggingMixin [2018-09-07 17:37:24,332] {{__init__.py:42}} DEBUG - Cannot import  due to  doesn't look like a module path\r\n4\r\n1\r\n2\r\n3\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\nPool size: 5  Connections in pool: 0 Current Overflow: -5 Current Checked out connections: 0\r\n```",
        "createdAt" : "2018-09-08T00:38:20Z",
        "updatedAt" : "2018-09-10T19:30:41Z",
        "lastEditedBy" : "a3c64a30-509c-4788-80d1-4f17cb3b5530",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b5b980e697ec8dde5beb043338e01a4ef8b7c6",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +164,168 @@\n        # Recreate the process pool each sync in case processes in the pool die\n        self._sync_pool = Pool(processes=num_processes)\n\n        # Use chunking instead of a work queue to reduce context switching since tasks are"
  },
  {
    "id" : "9a6cab7b-c76d-47ad-b8f8-17df521bef46",
    "prId" : 4234,
    "prUrl" : "https://github.com/apache/airflow/pull/4234#pullrequestreview-178737868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c0f3f19-e12c-4bef-985e-edae023ef406",
        "parentId" : null,
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "Minor: One empty line is expected before this line to render Sphinx properly",
        "createdAt" : "2018-11-27T11:44:37Z",
        "updatedAt" : "2018-11-27T21:39:13Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      }
    ],
    "commit" : "554b4cf08bcf371a95415d9a0cd88820f2d9272c",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +157,161 @@        How many Celery tasks should each worker process send.\n\n        :return: Number of tasks that should be sent per process\n        :rtype: int\n        \"\"\""
  },
  {
    "id" : "21583635-6f81-476c-bb75-ea7b7bed6bdc",
    "prId" : 6596,
    "prUrl" : "https://github.com/apache/airflow/pull/6596#pullrequestreview-324898239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc3430f8-ac3d-41a3-a3b9-3025f8930fa9",
        "parentId" : null,
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "I think this function is not required as we don't use it elsewhere unless of course I have not seen it yet :). `update_task_state` is fine.",
        "createdAt" : "2019-11-25T19:04:33Z",
        "updatedAt" : "2019-12-03T13:41:00Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "64b609a0-694d-4fad-a193-1dbfb9f062f6",
        "parentId" : "fc3430f8-ac3d-41a3-a3b9-3025f8930fa9",
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "But it's actually used there - List of tasks from map :).. So it is used.",
        "createdAt" : "2019-11-30T16:09:31Z",
        "updatedAt" : "2019-12-03T13:41:00Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      }
    ],
    "commit" : "0904b516d3537e9ca52592972e6380ee6fb25125",
    "line" : 210,
    "diffHunk" : "@@ -1,1 +273,277 @@        self.update_task_states(task_keys_to_states)\n\n    def update_task_states(self,\n                           task_keys_to_states: List[Union[TaskInstanceStateType,\n                                                           ExceptionWithTraceback]]) -> None:"
  },
  {
    "id" : "fad40235-62be-4e7e-bdf7-5cb12ccdfe52",
    "prId" : 6596,
    "prUrl" : "https://github.com/apache/airflow/pull/6596#pullrequestreview-323658639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9c0961f-6457-42aa-ac7e-7e8bb7884f2c",
        "parentId" : null,
        "authorId" : "e29ffafb-ac51-434b-b9e0-af262caae1ee",
        "body" : "missing an empty line between title and params",
        "createdAt" : "2019-11-27T11:01:41Z",
        "updatedAt" : "2019-12-03T13:41:00Z",
        "lastEditedBy" : "e29ffafb-ac51-434b-b9e0-af262caae1ee",
        "tags" : [
        ]
      },
      {
        "id" : "476080b0-93ac-4cb6-a13d-0c85c4d99484",
        "parentId" : "f9c0961f-6457-42aa-ac7e-7e8bb7884f2c",
        "authorId" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "body" : "Done.",
        "createdAt" : "2019-11-27T13:07:31Z",
        "updatedAt" : "2019-12-03T13:41:00Z",
        "lastEditedBy" : "e8563344-32ea-4c07-9731-a2fed8d2edf2",
        "tags" : [
        ]
      }
    ],
    "commit" : "0904b516d3537e9ca52592972e6380ee6fb25125",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +237,241 @@    def order_queued_tasks_by_priority(self) -> List[Tuple[TaskInstanceKeyType, QueuedTaskInstanceType]]:\n        \"\"\"\n        Orders the queued tasks by priority.\n\n        :return: List of tuples from the queued_tasks according to the priority."
  },
  {
    "id" : "4de7e7e3-bb57-4027-a5e0-42b7c9c28ae3",
    "prId" : 7542,
    "prUrl" : "https://github.com/apache/airflow/pull/7542#pullrequestreview-406513125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f959f8c-b970-497f-a2e1-9b27c1784f93",
        "parentId" : null,
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "I know it's not necessary but would you mind adding a short description in those protected methods? ",
        "createdAt" : "2020-05-06T11:02:51Z",
        "updatedAt" : "2020-05-11T07:46:45Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      },
      {
        "id" : "3068d48e-a5dc-46aa-b1c0-b930856f4329",
        "parentId" : "8f959f8c-b970-497f-a2e1-9b27c1784f93",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "In all cases, the description of these methods will be identical I am afraid. The description of the method should contain a description of the behavior, and the behavior is identical everywhere. Only the implementation details are different.",
        "createdAt" : "2020-05-06T11:14:03Z",
        "updatedAt" : "2020-05-11T07:46:45Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      }
    ],
    "commit" : "6730ff994b8152f4519f452f4540c0b73a1b6115",
    "line" : 245,
    "diffHunk" : "@@ -1,1 +317,321 @@        return result\n\n    def _get_many_from_kv_backend(self, async_tasks) -> Mapping[str, str]:\n        task_ids = _tasks_list_to_task_ids(async_tasks)\n        keys = [app.backend.get_key_for_task(k) for k in task_ids]"
  },
  {
    "id" : "a1331a90-b65d-4b45-9ceb-b7a1cbf21045",
    "prId" : 10033,
    "prUrl" : "https://github.com/apache/airflow/pull/10033#pullrequestreview-456555708",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f63b8d30-81b0-40fb-98e6-5f33e577cc77",
        "parentId" : null,
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "```suggestion\r\n    cls.validate_command(command_to_exec)\r\n```\r\n\r\nthat should work ðŸ¤·  - WDYT ?",
        "createdAt" : "2020-07-28T11:12:36Z",
        "updatedAt" : "2020-07-28T11:51:21Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "84aa5b53-36e7-4065-a6ee-df1168cc211e",
        "parentId" : "f63b8d30-81b0-40fb-98e6-5f33e577cc77",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "I think the `cls` will be undefined in this place",
        "createdAt" : "2020-07-28T11:49:26Z",
        "updatedAt" : "2020-07-28T11:51:21Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      },
      {
        "id" : "24b98494-4dc6-46e9-b07f-e742086f422f",
        "parentId" : "f63b8d30-81b0-40fb-98e6-5f33e577cc77",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "Oh yes, sry didnt realize it was not a part of the class",
        "createdAt" : "2020-07-28T11:51:21Z",
        "updatedAt" : "2020-07-28T11:51:21Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      },
      {
        "id" : "ef7b63ee-a5e6-4530-8622-f8b7c6ca5867",
        "parentId" : "f63b8d30-81b0-40fb-98e6-5f33e577cc77",
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "But I updated other usages to `self.` instead of `BaseExecutor.`",
        "createdAt" : "2020-07-28T11:52:03Z",
        "updatedAt" : "2020-07-28T11:52:03Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      }
    ],
    "commit" : "d179a47381e06b2178c10386c51e9c085ff1e35f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +72,76 @@def execute_command(command_to_exec: CommandType) -> None:\n    \"\"\"Executes command.\"\"\"\n    BaseExecutor.validate_command(command_to_exec)\n    log.info(\"Executing command in Celery: %s\", command_to_exec)\n    env = os.environ.copy()"
  },
  {
    "id" : "28717468-a442-4d91-a0d9-c65c3a56b9b9",
    "prId" : 10949,
    "prUrl" : "https://github.com/apache/airflow/pull/10949#pullrequestreview-489420984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5ab0913-3c9b-4b65-a571-8ff1faedbd3f",
        "parentId" : null,
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "Maybe we do not need to have `tis_by_celery_task_id = {ti.external_executor_id: ti for ti in tis}` (line 360) separately. Instead, we can have \r\n```python\r\ncelery_tasks = {\r\n    ti.external_executor_id: (AsyncResult(ti.external_executor_id), ti)\r\n    for ti in tis\r\n    if ti.external_executor_id is not None\r\n}\r\n```\r\n\r\nThen lines 369-370 can also be merged into\r\n```python\r\nresult, ti = celery_tasks[celery_task_id]\r\n```\r\n\r\nBy doing this, we can avoid traversing `tis` for two times (only one time would suffice), also save one query on the dict.",
        "createdAt" : "2020-09-15T19:38:49Z",
        "updatedAt" : "2020-09-16T17:45:11Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      },
      {
        "id" : "38471aa7-c63d-44d6-b01e-5d74ff44aaf8",
        "parentId" : "f5ab0913-3c9b-4b65-a571-8ff1faedbd3f",
        "authorId" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "body" : "In addition, `tis_by_celery_task_id` covers all `tis`, which is not necessary, because it will only be used to query `tis` which present in `celery_tasks` (essentially means `tis` whose `external_executor_id` is not `None`).\r\n\r\nSo my suggested revision above may save a little memory as well ;-) ",
        "createdAt" : "2020-09-15T19:41:49Z",
        "updatedAt" : "2020-09-16T17:45:11Z",
        "lastEditedBy" : "59d531be-9d1e-478d-99a0-6e20963d3e21",
        "tags" : [
        ]
      },
      {
        "id" : "13f02175-18af-4ada-b144-d99ddf95a130",
        "parentId" : "f5ab0913-3c9b-4b65-a571-8ff1faedbd3f",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Nice!",
        "createdAt" : "2020-09-16T08:50:26Z",
        "updatedAt" : "2020-09-16T17:45:11Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      },
      {
        "id" : "f37f36d5-e56c-4854-89ca-0af67dc7546b",
        "parentId" : "f5ab0913-3c9b-4b65-a571-8ff1faedbd3f",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Done.",
        "createdAt" : "2020-09-16T09:02:05Z",
        "updatedAt" : "2020-09-16T17:45:11Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      }
    ],
    "commit" : "092b9c0d0577f60f59534b4fd0914cc5f6da3080",
    "line" : 196,
    "diffHunk" : "@@ -1,1 +371,375 @@        for celery_task_id, (state, info) in states_by_celery_task_id.items():\n            result, ti = celery_tasks[celery_task_id]\n            result.backend = cached_celery_backend\n\n            # Set the correct elements of the state dicts, then update this"
  },
  {
    "id" : "3d5009e6-cc46-4f47-90ce-e642a82719b4",
    "prId" : 11372,
    "prUrl" : "https://github.com/apache/airflow/pull/11372#pullrequestreview-505606693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3d71e6f-3f04-4a7a-9b88-8199893a08fd",
        "parentId" : null,
        "authorId" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "body" : "Do we need to fork it? Shouldn't we just execute it in current process (celery worker process)?",
        "createdAt" : "2020-10-09T11:50:04Z",
        "updatedAt" : "2020-10-09T11:50:04Z",
        "lastEditedBy" : "0d4fd7c4-f8ab-4371-acfe-b9cca6decaf5",
        "tags" : [
        ]
      },
      {
        "id" : "c1c03a22-ddbe-47a1-91eb-b55387aed7a2",
        "parentId" : "e3d71e6f-3f04-4a7a-9b88-8199893a08fd",
        "authorId" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "body" : "Can't cos of the `logging.shutdown()` at the end of task_run (which we need to keep, as that's when remote logs are uploaded. https://github.com/apache/airflow/pull/11327#issuecomment-705139313",
        "createdAt" : "2020-10-09T11:59:47Z",
        "updatedAt" : "2020-10-09T11:59:47Z",
        "lastEditedBy" : "f73f66ab-2657-4a50-be7a-2ca3ca98c202",
        "tags" : [
        ]
      }
    ],
    "commit" : "39ec73b2158be12df5bcfe0ff2cc4fa2c1fc85a8",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +89,93 @@\ndef _execute_in_fork(command_to_exec: CommandType) -> None:\n    pid = os.fork()\n    if pid:\n        # In parent, wait for the child"
  },
  {
    "id" : "47e27e91-5b52-4772-a88c-09e8f7ae928c",
    "prId" : 11956,
    "prUrl" : "https://github.com/apache/airflow/pull/11956#pullrequestreview-520656998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a2f4e1e-8d42-4028-9314-7cd660d5dfe1",
        "parentId" : null,
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "This library is not defined in setup.py? Should we add it? https://github.com/apache/airflow/blob/b104516/setup.py\r\n",
        "createdAt" : "2020-10-30T11:09:54Z",
        "updatedAt" : "2020-10-30T11:09:54Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "af5cb7e8-d711-4933-afdc-1bb51cc3c9e6",
        "parentId" : "6a2f4e1e-8d42-4028-9314-7cd660d5dfe1",
        "authorId" : "c25957e2-1132-4c48-a536-3824307fd862",
        "body" : "`numpy` is a direct dependency of `pandas` which we have in out setup.py\r\n\r\nhttps://github.com/apache/airflow/blob/1faf985d83a15e0c2407e4d12a93437c299b44f2/setup.py#L719\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/4582c1c8701075a9f9f0bf75fbcd8546515b43e8/setup.py#L744-L750\r\n\r\nMaybe a good idea to add it in `try .. catch` similar to the `kubernetes.client`\r\n\r\nhttps://github.com/astronomer/airflow/blob/0b2906506bd39b8940787db145f0dffb2dd7f425/airflow/executors/celery_executor.py#L188-L191",
        "createdAt" : "2020-10-30T13:18:39Z",
        "updatedAt" : "2020-10-30T13:18:39Z",
        "lastEditedBy" : "c25957e2-1132-4c48-a536-3824307fd862",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b2906506bd39b8940787db145f0dffb2dd7f425",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +179,183 @@    \"\"\"\n    import jinja2.ext  # noqa: F401\n    import numpy  # noqa: F401\n\n    import airflow.jobs.local_task_job"
  }
]