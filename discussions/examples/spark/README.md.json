[
  {
    "id" : "99a14a06-bb6c-44dc-be5b-7762b268ffda",
    "prId" : 33604,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33604#pullrequestreview-2964604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66b31a66-40f5-4e1b-bf41-cc147fc8372a",
        "parentId" : null,
        "authorId" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "body" : "The same snippet is repeated below for zeppelin. Perhaps it would be sufficient to say that the same can be run non-interactively also.\n",
        "createdAt" : "2016-10-05T06:40:08Z",
        "updatedAt" : "2016-10-10T20:36:47Z",
        "lastEditedBy" : "e535b047-00fc-4269-992a-b8d65bd7c57b",
        "tags" : [
        ]
      },
      {
        "id" : "338d0fda-a9f2-43f9-8f0e-f8efb912d336",
        "parentId" : "66b31a66-40f5-4e1b-bf41-cc147fc8372a",
        "authorId" : "3553e8c4-7450-4274-b7f8-6dc02f45e043",
        "body" : "The pyspark console example needs the code raw, and the zeppelin example needs the `%pyspark` hint.  It's a repetition vs ease of use thing. I'll rewrite it to make it more clear why we're repeating.\n",
        "createdAt" : "2016-10-05T17:33:01Z",
        "updatedAt" : "2016-10-10T20:36:47Z",
        "lastEditedBy" : "3553e8c4-7450-4274-b7f8-6dc02f45e043",
        "tags" : [
        ]
      },
      {
        "id" : "51c97caf-c8ca-4b5d-a23b-052ca088368d",
        "parentId" : "66b31a66-40f5-4e1b-bf41-cc147fc8372a",
        "authorId" : "3553e8c4-7450-4274-b7f8-6dc02f45e043",
        "body" : "I think what would really help is an example that is actually graphical, but I don't know enough about spark to cook up a cloud-agnostic example that'll show a graph or something.\n",
        "createdAt" : "2016-10-05T17:39:33Z",
        "updatedAt" : "2016-10-10T20:36:47Z",
        "lastEditedBy" : "3553e8c4-7450-4274-b7f8-6dc02f45e043",
        "tags" : [
        ]
      }
    ],
    "commit" : "94014159914845135a6bf507bddfa220156b9324",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +256,260 @@graphical with the Spark cluster, or you can stay in the CLI.\n\nFor both choices, we will be working with this Python snippet:\n\n```python"
  },
  {
    "id" : "b7d73503-ea77-4729-b82b-48f505fa64b9",
    "prId" : 21634,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e476c7b4-114b-4134-b8b5-63dabb2698e6",
        "parentId" : null,
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "This bullet doesn't very clearly state \"The SkyDNS add-on is running\" to me, whereas 90% of the complaints I've seen, that's what they need to know. I was actually planning on putting up a PR that just checked in the Dockerfile (i.e. tried to resolve `spark-master`, complained that DNS wasn't running.)\n",
        "createdAt" : "2016-02-22T16:56:02Z",
        "updatedAt" : "2016-02-22T16:56:02Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "a38327ed-f201-4cf9-8a46-4f83bba1c829",
        "parentId" : "e476c7b4-114b-4134-b8b5-63dabb2698e6",
        "authorId" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "body" : "Agree, another PR would be great on the docker image itself\n",
        "createdAt" : "2016-02-22T18:15:31Z",
        "updatedAt" : "2016-02-22T18:15:31Z",
        "lastEditedBy" : "a5be0b3b-3db2-4c99-a598-55f8708db5df",
        "tags" : [
        ]
      }
    ],
    "commit" : "6cbcb5d6c7d1ebaed064d360ce5605aaaaacbfcb",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +54,58 @@- You have a Kubernetes cluster installed and running.\n- That you have installed the ```kubectl``` command line tool somewhere in your path.\n- That a spark-master service which spins up will be automatically discoverable by your kube DNS impl, as 'spark-master'\n\nFor details, you can look at the Dockerfiles in the Sources section."
  },
  {
    "id" : "9caacdd1-78c2-44a0-9e9d-9036cf7b6921",
    "prId" : 16320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72d1d1d7-33b3-4461-bbea-70b5d77fc692",
        "parentId" : null,
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Is there any authentication on this? If not, I think it's worth mentioning that this exposes the unprotected UI to the world.\n",
        "createdAt" : "2015-10-27T18:30:41Z",
        "updatedAt" : "2015-10-27T22:12:19Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "2483ccf8-ec78-42e5-b6ec-310b53465ddc",
        "parentId" : "72d1d1d7-33b3-4461-bbea-70b5d77fc692",
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "I'll add a note. I guess we should throw an nginx proxy in front of this but it makes the example just that much more obnoxious. :/\n",
        "createdAt" : "2015-10-27T21:26:52Z",
        "updatedAt" : "2015-10-27T22:12:19Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "829c767a-8fd8-4aea-90c6-c4e6485bd28f",
        "parentId" : "72d1d1d7-33b3-4461-bbea-70b5d77fc692",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "I think the doc comment is sufficient.\n",
        "createdAt" : "2015-10-27T21:29:08Z",
        "updatedAt" : "2015-10-27T22:12:19Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "777d3a6ec0acfbf6a517d5b3fb62ff64d8222e15",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +145,149 @@```\n\nYou should now be able to visit `http://104.197.147.190:8080` and see the Spark\nMaster UI. *Note:* After workers connect, this UI has links to worker Web\nUIs. The worker UI links do not work (the links attempt to connect to cluster"
  },
  {
    "id" : "167396aa-c55f-4bd7-ba45-9602d8bb9d9f",
    "prId" : 16320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8794e41d-b090-497a-8d70-0f054829887e",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "imho pyspark is the exception and not the norm, examples should probably reference spark-shell repl.  \n",
        "createdAt" : "2015-10-30T13:58:02Z",
        "updatedAt" : "2015-10-30T13:58:02Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "d3f97eb8-6362-49ce-8822-4e969b6fae33",
        "parentId" : "8794e41d-b090-497a-8d70-0f054829887e",
        "authorId" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "body" : "My next goal is actually to get Zeppelin up (because in terms of visual demos, it works much, much better), so don't stress too much about this one. I _believe_ the driver pod also has spark-shell on it, but I need to poke at it.\n",
        "createdAt" : "2015-10-30T16:27:00Z",
        "updatedAt" : "2015-10-30T16:27:00Z",
        "lastEditedBy" : "a92f8f9e-31fd-4510-b4d9-3553f7025485",
        "tags" : [
        ]
      },
      {
        "id" : "365dbd51-e522-4829-a091-edf473d8d27f",
        "parentId" : "8794e41d-b090-497a-8d70-0f054829887e",
        "authorId" : "03b61a2c-385a-4e3a-9c84-96fa464c31a4",
        "body" : "don't switch from pyspark.\n\npyspark isn't just an exception. it's a rapidly growing interface and the easiest for non-scala developers to approach.\n\nhaving the zeppelin front end will be the best, but won't provide a simple cli interface.\n\nthe purpose of the driver pod was to provide the shells, so the scala shell is also there.\n",
        "createdAt" : "2015-11-03T00:47:35Z",
        "updatedAt" : "2015-11-03T00:47:35Z",
        "lastEditedBy" : "03b61a2c-385a-4e3a-9c84-96fa464c31a4",
        "tags" : [
        ]
      }
    ],
    "commit" : "777d3a6ec0acfbf6a517d5b3fb62ff64d8222e15",
    "line" : 221,
    "diffHunk" : "@@ -1,1 +211,215 @@\n```console\n$ kubectl exec spark-driver-controller-vwb9c -it pyspark\nPython 2.7.9 (default, Mar  1 2015, 12:57:24)\n[GCC 4.9.2] on linux2"
  },
  {
    "id" : "bfede89c-19ce-4108-b40d-268ce910ffdc",
    "prId" : 10407,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef1622ac-6941-4dae-baea-11fb3b717f6a",
        "parentId" : null,
        "authorId" : "d513ff43-94d3-4f43-8358-1fb8132b6aae",
        "body" : "At the very least, we should explain how to `ssh` into a node.\n",
        "createdAt" : "2015-06-30T17:33:26Z",
        "updatedAt" : "2015-07-07T17:37:04Z",
        "lastEditedBy" : "d513ff43-94d3-4f43-8358-1fb8132b6aae",
        "tags" : [
        ]
      },
      {
        "id" : "810387be-43bf-4f8c-8ec8-19cc87f8b962",
        "parentId" : "ef1622ac-6941-4dae-baea-11fb3b717f6a",
        "authorId" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "body" : "Done.\n",
        "createdAt" : "2015-06-30T18:15:10Z",
        "updatedAt" : "2015-07-07T17:37:04Z",
        "lastEditedBy" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc4ee789df61c99b6304003c12dfd6398c7446a",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +145,149 @@that sets up the environment based on the provided IP and port of the Master.\n\n```\ncluster-node $ sudo docker run -it gcr.io/google_containers/spark-base\nroot@f12a6fec45ce:/# . /setup_client.sh 10.0.204.187 7077"
  }
]