[
  {
    "id" : "f5ac4991-fe48-4ce5-a151-f80c9f797752",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1d42d70-a46d-4d43-9b08-ae8dd6fc6b9f",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "With the docs move this link is now just a reference to the new location.  `[getting started guides](../../docs/getting-started-guides/)`.  I wonder if there is as better way of handling this.  As a reader I'd prefer the link to point directly at the new docs.\n",
        "createdAt" : "2016-03-14T18:28:12Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "b639d1ff-80a8-4b8a-9d29-0c320ea8b5ee",
        "parentId" : "a1d42d70-a46d-4d43-9b08-ae8dd6fc6b9f",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "This is issue is nothing specific to this document, but important context.\n\nShortest path to creating a GKE cluster from here:\n- Click link\n- Click moved to link\n- Scroll to hosted solution, click Google Container Engine\n- Click \"View My Console\"\n- Click \"Create a container cluster\"\n- Click create (Accepting the defaults)\n- Wait for cluster to spin up\n- Configure kubectl (assuming alread installed)\n  - Click on the cluster name\n  - Click show credentials\n  - Click fetching credentials for kubectl\n  - run the gcloud command in a terminal\n\nIt doesn't take that long, but is many hops away from this example and some of the steps weren't immediately obvious to me - how do I configure kubectl once I have a running gke cluster?  How to I create a running GKE cluster from the GKE landing page.\n\nMy main concern is that the user shouldn't have to figure out how to do some orthogonal task after starting the example.  A bunch of cut-paste steps is probably ok, but having to switch contexts into \"How do I setup a k8s cluster?\" is too much to ask.\n",
        "createdAt" : "2016-03-14T22:01:55Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "62381213-bf91-453a-93b7-32d09331d80d",
        "parentId" : "a1d42d70-a46d-4d43-9b08-ae8dd6fc6b9f",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "Yes, this is probably a general issue when people visit any of these examples.  OTOH we don't want to repro those cluster startup instructions for every example, especially since different users will be wanting to use different cloud providers (AWS etc.). Perhaps we should do more to create a kind of \"guided path\" through the examples, starting with the \"setup\" ones. We do this to an extent.\n",
        "createdAt" : "2016-04-10T19:51:24Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +69,73 @@command line tool somewhere in your path.  Please see the\n[getting started guides](../../docs/getting-started-guides/)\nfor installation instructions for your platform.\n\nThis example also has a few code and configuration files needed.  To avoid"
  },
  {
    "id" : "87eed922-99ab-4c67-bbad-e3ac3a0836a3",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ef0c233-3f53-4801-900d-c87f332501db",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "This paragraph reads a bit awkward and is wordy.  Since it is the first paragraph in the document we should work to make sure it is clean and clear.  I would also prefer if as much as possible, our examples have the same wording and structure to minimize the amount of mental effort required when switch between examples.\n\nSee https://github.com/jeffmendoza/kubernetes/blob/wordpress-update/examples/mysql-wordpress-pd/README.md\n\n```\nThis example describes how to run a persistent installation of WordPress and MySQL on Kubernetes. We'll use the mysql and wordpress official Docker images for this installation. (The WordPress image includes an Apache server).\n\nDemonstrated Kubernetes Concepts:\n\nPersistent Volumes to define persistent disks (disk lifecycle not tied to the Pods).\nServices to enable Pods to locate one another.\nExternal Load Balancers to expose Services externally.\nDeployments to ensure Pods stay up and running.\nSecrets to store sensitive passwords.\n```\n\nIf this were to match the wordpress example more closely, it might read something like\n\n```\nThis example describes how to run a cloud native Cassandra deployment on Kubernetes.  We'll use a custom Cassandra image built with a Kubernetes-native Cassandra `SeedProvider`.\n\nDemonstrated  Kubernetes Concepts:\nServices for X\n...\n```\n\nAlso the layout of the various sections differs from wordpress example.  If we think one structuring is better than the other, I am fine with changing the canonical format, but would like to see the structures more standardized.  Once we settle on the exact details, we should probably update the guidelines with a template that everything should match.\n",
        "createdAt" : "2016-03-14T18:51:04Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "6afe7f45-1799-46ce-b77e-e9b5505b1bcb",
        "parentId" : "1ef0c233-3f53-4801-900d-c87f332501db",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "While I didn't write this myself, I'd vote to keep the 1st pp as is (at least for this PR-- we can discuss further).  I don't think everyone knows what \"cloud native\" means, and it's an interesting and rather subtle concept.\n",
        "createdAt" : "2016-04-10T20:37:02Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +48,52 @@  - [Seed Provider Source](#seed-provider-source)\n\nThe following document describes the development of a _cloud native_\n[Cassandra](http://cassandra.apache.org/) deployment on Kubernetes.  When we say\n_cloud native_, we mean an application which understands that it is running"
  },
  {
    "id" : "ebb0aed1-82a2-4b8a-a125-1e0c156dc885",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fcc3f94e-fc7b-4e13-8268-5f95ab5a4b87",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What do we think of standardizing on using deployments?  I'd prefer all of our examples do things the same way for consistency.\n",
        "createdAt" : "2016-03-14T18:54:01Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "5961236a-43bc-49bf-ac6c-13507226a3bd",
        "parentId" : "fcc3f94e-fc7b-4e13-8268-5f95ab5a4b87",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "That sounds good to me too, but I'd like to do that in a following PR, as I'm having enough trouble chasing down the cascade of test errors caused by simply removing cassandra.yaml.  (I didn't write the tests myself). \n",
        "createdAt" : "2016-03-15T01:03:52Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +82,86 @@```sh\n# create a service to track all cassandra nodes\nkubectl create -f examples/cassandra/cassandra-service.yaml\n\n# create a replication controller to replicate cassandra nodes"
  },
  {
    "id" : "aa3a4a37-f4f4-44ff-8ea6-6f66ba1c1617",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91cb8272-367e-416a-8faf-6f5811041343",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What does the selector query do / control in the RC?  What happens if this doesn't match the pod template labels?  Why would you want to override this?\n",
        "createdAt" : "2016-03-14T19:50:35Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "8b51aab3-8b5c-4efb-ad6f-73c32133bfa8",
        "parentId" : "91cb8272-367e-416a-8faf-6f5811041343",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "Same as with the comment above, see https://github.com/kubernetes/kubernetes/issues/2210 , and a discussion of where you might want to use manual selectors.\nBut as to the .yaml comment on the selector label -- suspect this may in part have been added as fyi for longer-term Kubernetes users, since this is new-ish.\n",
        "createdAt" : "2016-04-10T20:21:01Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 307,
    "diffHunk" : "@@ -1,1 +229,233 @@There are a few things to note in this description.\n\nThe `selector` attribute contains the controller's selector query. It can be\nexplicitly specified, or applied automatically from the labels in the pod\ntemplate if not set, as is done here."
  },
  {
    "id" : "7740fefc-7bef-4e78-84f0-6c4212f4d73f",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b53b4b5f-9ce4-40a2-ae09-a992050cd7fb",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What are the members of its set?  How does this differ from the pods it is replicating?  Does this mean it identifies the pods it has created from the template using a selector?\n",
        "createdAt" : "2016-03-14T20:19:28Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "9e78a4dd-d10e-439f-ba6b-47a34390425b",
        "parentId" : "b53b4b5f-9ce4-40a2-ae09-a992050cd7fb",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "Well, sort of :).  See https://github.com/kubernetes/kubernetes/issues/22158 , https://github.com/kubernetes/kubernetes/issues/2210\n",
        "createdAt" : "2016-04-10T19:58:46Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 258,
    "diffHunk" : "@@ -1,1 +162,166 @@_[Replication Controller](../../docs/user-guide/replication-controller.md)_\nis responsible for replicating sets of identical pods.  Like a\nService, it has a selector query which identifies the members of its set.\nUnlike a Service, it also has a desired number of replicas, and it will create\nor delete Pods to ensure that the number of Pods matches up with its"
  },
  {
    "id" : "dbbf6e99-b1b8-4772-8efb-ec31be15f846",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "706fc2f2-34af-4be0-9c60-0cae6aa2d069",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "In this case, are the members of its set pods or nodes?  What it mean to be a member of a DS?\n",
        "createdAt" : "2016-03-14T20:20:43Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "5adb8876-3b76-4874-ac7d-3df8a520ae9d",
        "parentId" : "706fc2f2-34af-4be0-9c60-0cae6aa2d069",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "It's a set of pods.\n",
        "createdAt" : "2016-04-10T20:26:45Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 487,
    "diffHunk" : "@@ -1,1 +382,386 @@In Kubernetes, a [_Daemon Set_](../../docs/admin/daemons.md) can distribute pods\nonto Kubernetes nodes, one-to-one.  Like a _ReplicationController_, it has a\nselector query which identifies the members of its set.  Unlike a\n_ReplicationController_, it has a node selector to limit which nodes are\nscheduled with the templated pods, and replicates not based on a set target"
  },
  {
    "id" : "2002c709-ae56-4cdc-b45f-f0ecfddc5e3f",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36b06546-6c5e-45dc-9273-4237c2582c1a",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Doesn't the Service need to create a load balancer to accomplish resolving the ip in the `standing query` task?  Perhaps we should just state what we are using the service for \"We will use a Service to resolve an ip address to a dynamically changing set of Cassandra Pods.  Traffic will be sent to Pods matching the label selector.\"\n",
        "createdAt" : "2016-03-14T20:31:28Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "b99ee5de-0be1-49ce-9032-97c343b18b74",
        "parentId" : "36b06546-6c5e-45dc-9273-4237c2582c1a",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "I think in this case Brendan did mean something a bit different, which that the seedprovider uses the endpoint API. So in that case it's not doing load balancing. \n",
        "createdAt" : "2016-04-10T19:54:33Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 168,
    "diffHunk" : "@@ -1,1 +113,117 @@that _must_ be scheduled onto the same host.\n\nAn important use for a Service is to create a load balancer which\ndistributes traffic across members of the set of Pods.  But a Service can also\nbe used as a standing query which makes a dynamically changing set of Pods"
  },
  {
    "id" : "889067b6-499f-4b4c-a22c-921fb0b0de8f",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2326447-e04d-48d7-871a-c6eb3f07206a",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Should we inline much of this in the yaml?\n",
        "createdAt" : "2016-03-14T23:07:31Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "9673ed77-1b8f-432e-ba2b-3b154f6e0ddf",
        "parentId" : "b2326447-e04d-48d7-871a-c6eb3f07206a",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "To me, this reads okay as is...\n",
        "createdAt" : "2016-04-10T02:52:21Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : null,
    "diffHunk" : "@@ -1,1 +236,240 @@from Step 1. This is how pods created by this replication controller are picked up\nby the Service.\"\n\nThe `replicas` attribute specifies the desired number of replicas, in this\ncase 2 initially.  We'll scale up to more shortly."
  },
  {
    "id" : "9e2c9b55-e697-4708-89ba-77569ee9650e",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68758f89-098a-4bb8-b452-daf73ea2fe46",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "nit: \"logic is working as intended\" clarify what is intended: \"locating all Cassandra replicas\"?\n",
        "createdAt" : "2016-03-14T23:26:50Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "2d979b4f-a71e-4687-ba5b-e8a07ab0b0ed",
        "parentId" : "68758f89-098a-4bb8-b452-daf73ea2fe46",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "Let's just leave this for now, and address it in a future iteration (meaning, I'm not sure, but as per our email thread on this, there are indications that the current example may not be using best practices with the SeedProvider.)\n",
        "createdAt" : "2016-04-10T02:50:08Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : null,
    "diffHunk" : "@@ -1,1 +317,321 @@```\n\nTo show that the `SeedProvider` logic is working as intended, you can use the\n`nodetool` command to examine the status of the Cassandra cluster.  To do this,\nuse the `kubectl exec` command, which lets you run `nodetool` in one of your"
  },
  {
    "id" : "be447c0c-4b9c-425c-9423-2ea391fd1ccd",
    "prId" : 22810,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30611397-147d-4d6f-a67f-d3f2679b8945",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "This failed for me using the example checked into head:\n\n``` console\n$ kubectl create -f examples/cassandra/cassandra-daemonset.yaml --validate=false\nError from server: error when creating \"examples/cassandra/cassandra-daemonset.yaml\": the server could not find the requested resource\n```\n",
        "createdAt" : "2016-03-14T23:53:08Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "634b03ff-e180-4abd-9610-aae210cef794",
        "parentId" : "30611397-147d-4d6f-a67f-d3f2679b8945",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "The daemonset requires 1.2 -- is that what you were running?\n",
        "createdAt" : "2016-03-15T00:41:15Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      },
      {
        "id" : "91c7de38-4a89-4e39-b1ec-f4ba97377009",
        "parentId" : "30611397-147d-4d6f-a67f-d3f2679b8945",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "No, just kicked off a GKE cluster which is 1.1.  We should update the Prereq section to explicitly state 1.2 is required.\n",
        "createdAt" : "2016-03-15T17:43:20Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "48209b68-d623-43b4-b874-dd60e7f834c7",
        "parentId" : "30611397-147d-4d6f-a67f-d3f2679b8945",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "done.\n",
        "createdAt" : "2016-04-10T02:42:44Z",
        "updatedAt" : "2016-04-15T04:55:44Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "8846b313dc7aeab96a830d3e7dbe4412940e9488",
    "line" : 519,
    "diffHunk" : "@@ -1,1 +466,470 @@\n```console\n$ kubectl create -f examples/cassandra/cassandra-daemonset.yaml --validate=false\n```\n"
  },
  {
    "id" : "2dc93bae-f1b1-406c-b4e7-bc4e97cbcc8b",
    "prId" : 21645,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "659fb492-9072-46ce-9751-3f8eacd8b8f3",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Update name to app in the console text.\n",
        "createdAt" : "2016-02-26T20:43:04Z",
        "updatedAt" : "2016-03-01T16:59:04Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "402752bc-1a9b-487d-ab3f-5929daec86a8",
        "parentId" : "659fb492-9072-46ce-9751-3f8eacd8b8f3",
        "authorId" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "body" : "done\n",
        "createdAt" : "2016-02-29T02:28:53Z",
        "updatedAt" : "2016-03-01T16:59:04Z",
        "lastEditedBy" : "d27cb3b5-92e1-4729-8eea-9a3b04663404",
        "tags" : [
        ]
      }
    ],
    "commit" : "04a585458c01684cac3067489ff5e853a36bc1d8",
    "line" : null,
    "diffHunk" : "@@ -1,1 +325,329 @@Now if you list the pods in your cluster, and filter to the label `app=cassandra`, you should see two cassandra pods:\n\n```console\n$ kubectl get pods -l=\"app=cassandra\"\nNAME              READY     STATUS    RESTARTS   AGE"
  },
  {
    "id" : "94e890ce-0c05-4865-a6d1-1ae8a0690143",
    "prId" : 21645,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c005f819-e0da-433b-8a8c-c431780e6e7e",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Change the label key from name to app below.\n",
        "createdAt" : "2016-02-26T20:43:38Z",
        "updatedAt" : "2016-03-01T16:59:04Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "04a585458c01684cac3067489ff5e853a36bc1d8",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +395,399 @@`DaemonSet` is designed to place a single pod on each node in the Kubernetes\ncluster. If you're looking for data redundancy with Cassandra, let's create a\ndaemonset to start our storage cluster:\n\n<!-- BEGIN MUNGE: EXAMPLE cassandra-daemonset.yaml -->"
  },
  {
    "id" : "decbb668-8e9b-47dd-b169-04ab2dc249c3",
    "prId" : 21645,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08fe8643-9fb2-4b74-9632-5403f06994e8",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "s/name/app/\n",
        "createdAt" : "2016-02-26T20:44:09Z",
        "updatedAt" : "2016-03-01T16:59:04Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "04a585458c01684cac3067489ff5e853a36bc1d8",
    "line" : null,
    "diffHunk" : "@@ -1,1 +472,476 @@network.\n\n```console\n$ kubectl get pods -l=\"app=cassandra\"\nNAME              READY     STATUS    RESTARTS   AGE"
  },
  {
    "id" : "985909d4-e45f-4805-a2bd-e11bb5d3335d",
    "prId" : 20746,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01d9cf03-45ff-4287-aaa8-925f234d0232",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "This change is not appropriate. What are you trying to do?\n\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/config-best-practices.md#naked-pods-vs-replication-controllers-and-jobs\n",
        "createdAt" : "2016-02-06T06:59:12Z",
        "updatedAt" : "2016-02-12T08:32:19Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "59ec61f8-0b9d-4562-9d12-ac5205a88b9b",
        "parentId" : "01d9cf03-45ff-4287-aaa8-925f234d0232",
        "authorId" : "0a790a9e-0f93-4562-9a2c-2e678e5408ad",
        "body" : "Have you read issue #19098? The readme for the [cassandra example](https://github.com/kubernetes/kubernetes/tree/master/examples/cassandra) starts by deploying a single pod (instead of a rc) but the code for the pod is incorrect (it's an rc). Just read the issue and the readme...\n",
        "createdAt" : "2016-02-06T07:35:04Z",
        "updatedAt" : "2016-02-12T08:32:19Z",
        "lastEditedBy" : "0a790a9e-0f93-4562-9a2c-2e678e5408ad",
        "tags" : [
        ]
      },
      {
        "id" : "df6afe64-a6a5-48e1-b335-7d9c51327e46",
        "parentId" : "01d9cf03-45ff-4287-aaa8-925f234d0232",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "@jtblin Did you read the document I posted a link to? Please do, and also:\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/pods.md#durability-of-pods-or-lack-thereof\n\nI would prefer not to show an example that lead users astray. If we show a pod as the first example, we need to make it clear that a pod is insufficient to ensure self-healing, as part of the motivation for using a ReplicationController in the following section. Nobody should run with a naked pod in production. Even `kubectl run` doesn't create naked pods.\n",
        "createdAt" : "2016-02-08T00:04:39Z",
        "updatedAt" : "2016-02-12T08:32:19Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "e33e076e-91bd-4be9-8fd3-1d5ceac38fe6",
        "parentId" : "01d9cf03-45ff-4287-aaa8-925f234d0232",
        "authorId" : "0a790a9e-0f93-4562-9a2c-2e678e5408ad",
        "body" : "@bgrant0607 yes I did and I am familiar with the restrictions about `Pods`, but I think this is somewhat orthogonal. I didn't create the example, I am just fixing the inconsistencies according to an existing issue #19098.\n\nIt seems there are ~24 other examples that use a Pod that so I'd suggest opening a different issue about this.\n\nIn any case, the change is appropriate as it just makes the readme instructions and the example code consistent with each other, again I didn't create the example in the first place. But if you prefer I can just remove the all Pod part, only keep the `ReplicationController` example and update the readme accordingly.\n",
        "createdAt" : "2016-02-08T08:03:43Z",
        "updatedAt" : "2016-02-12T08:32:19Z",
        "lastEditedBy" : "0a790a9e-0f93-4562-9a2c-2e678e5408ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ef6580ea3ca5f0314026e210649860373b69b50",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +58,62 @@```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:"
  },
  {
    "id" : "23fbff42-d889-4aac-abe3-4d7528a58492",
    "prId" : 16004,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4dcd90a9-8d54-4e84-a3d4-0bce5fd4be96",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "I think the above sentence applies whether or not you are using replication controller or daemonset, so it doesn't need to be here.\n",
        "createdAt" : "2015-10-30T17:48:53Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "8864e53a-75f7-4313-a092-2dfed4349bf0",
        "parentId" : "4dcd90a9-8d54-4e84-a3d4-0bce5fd4be96",
        "authorId" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "body" : "I don't think it's quite the same - using a rc you could get multiple Cassandra nodes on a single Kubernetes node which wouldn't give you data redundancy. I'm trying to stress the utility of using a daemonset in terms of redundancy and best practice.\n",
        "createdAt" : "2015-10-30T19:31:38Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "tags" : [
        ]
      },
      {
        "id" : "b81ca064-8dc1-4dae-a640-69e9a2ba1e51",
        "parentId" : "4dcd90a9-8d54-4e84-a3d4-0bce5fd4be96",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "We don't want to encourage people to use a DaemonSet solely as a way to get at most one pod per node.  \n\nIf that is all you need, then the short-term fix is to add a nodePort to your Pod (you don't have to use it, just pick  one and that will force max one per node).   Longer term, we plan to add less hacky support for expressing your spreading requirements.\n",
        "createdAt" : "2015-10-30T20:07:34Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "c86cc766-00cb-449d-baae-6c79aa12915e",
        "parentId" : "4dcd90a9-8d54-4e84-a3d4-0bce5fd4be96",
        "authorId" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "body" : "@erictune It's not just about enforcing only one pod per node. It's about enforcing one node per pod, no more, no less. So, to achieve this by setting nodePort, I'd have to constantly manually update the replicationController to have replicas == number of nodes. With a DaemonSet this is automatic. I don't see what the problem is with encouraging this - the purpose of DaemonSet is to place a single pod on each selected node, correct? So, if you want to have data replication over your entire cluster, and allocate Cassandra onto newly created instances automatically, is this not what DaemonSet is designed to do?\n",
        "createdAt" : "2015-10-30T21:51:30Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "tags" : [
        ]
      }
    ],
    "commit" : "0365ef9ef3ad367c427b17aa5d18b01bdeaa266f",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +310,314 @@In Kubernetes a _[Daemon Set](../../docs/admin/daemons.md)_ can distribute pods onto Kubernetes nodes, one-to-one.  Like a _ReplicationController_ it has a selector query which identifies the members of it's set.  Unlike a _ReplicationController_ it has a node selector to limit which nodes are scheduled with the templated pods, and replicates not based on a set target number of pods, but rather assigns a single pod to each targeted node.\n\nAn example use case: when deploying to the cloud, the expectation is that instances are ephemeral and might die at any time. Cassandra is built to replicate data across the cluster to facilitate data redundancy, so that in the case that an instance dies, the data stored on the instance does not, and the cluster can react by re-replicating the data to other running nodes.\n\nDaemonSet is designed to place a single pod on each node in the Kubernetes cluster. If you're looking for data redundancy with Cassandra, let's create a daemonset to start our storage cluster:"
  },
  {
    "id" : "6d9d441c-084b-42a0-bb8c-6bb5a79f8238",
    "prId" : 16004,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd04cb2f-67eb-4482-a0c3-1c2acd3774dc",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "The case where I would use this is if I was using cassandra on bare metal, and I was storing the data in a hostDir instead of an emptyDir, and I wanted to ensure that a cassandra daemon started on all nodes (or all matching nodes), and that it used any existing files in /var/lib/cassandra from the previous pod, thus saving the network cost of reconstruction, when perfectly good data is still there.  This allows you to have less downtime due to reconstruction after a node reboot/power-down.  If you want to change it to use a hostDir, and can verify that a cassandra node will come back after reboot without reconstruction, then I will be happy to take this PR.  If you don't have access to bare metal, simulating with PD is fine too. \n",
        "createdAt" : "2015-10-30T18:05:13Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "6cf9f126-2765-4f65-9113-88aa9af97c6b",
        "parentId" : "dd04cb2f-67eb-4482-a0c3-1c2acd3774dc",
        "authorId" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "body" : "Actually this is how I'm using it in production right now. I didn't remember to update the example to use hostDir. \n\nIn my case I use aws spot instances which quite frequently die so using Cassandra in this way is the only way I have data redundancy and prevent losses due to unexpected instance combustion. \n\nI'll update the PR.\n",
        "createdAt" : "2015-10-30T19:29:49Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "tags" : [
        ]
      },
      {
        "id" : "5e5613ff-2d47-4c66-bb40-7f50dd9393ba",
        "parentId" : "dd04cb2f-67eb-4482-a0c3-1c2acd3774dc",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Cool use case with spot instances that I had not considered.\n\nI guess there is still a possibility that all your spot instances go away at once, but that is uncommon enough that you live with that risk?   \n",
        "createdAt" : "2015-11-15T15:53:21Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "b807f28d-fc57-4023-a2e2-054981d4a772",
        "parentId" : "dd04cb2f-67eb-4482-a0c3-1c2acd3774dc",
        "authorId" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "body" : "@erictune I presume you could create a few \"stable\" instances, mark those as being in a different datacenter inside Cassandra, and then ask Cassandra to have two-datacenter reliability. Then the data would be replicated on both a spot instance and a stable instance at all times.\n",
        "createdAt" : "2015-11-15T19:55:13Z",
        "updatedAt" : "2015-11-16T21:51:09Z",
        "lastEditedBy" : "17d89203-4a2c-458a-a2eb-55108e56a2cd",
        "tags" : [
        ]
      }
    ],
    "commit" : "0365ef9ef3ad367c427b17aa5d18b01bdeaa266f",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +312,316 @@An example use case: when deploying to the cloud, the expectation is that instances are ephemeral and might die at any time. Cassandra is built to replicate data across the cluster to facilitate data redundancy, so that in the case that an instance dies, the data stored on the instance does not, and the cluster can react by re-replicating the data to other running nodes.\n\nDaemonSet is designed to place a single pod on each node in the Kubernetes cluster. If you're looking for data redundancy with Cassandra, let's create a daemonset to start our storage cluster:\n\n<!-- BEGIN MUNGE: EXAMPLE cassandra-daemonset.yaml -->"
  },
  {
    "id" : "303a0e72-49fc-4a27-84ac-8cbc6c60c6f2",
    "prId" : 5857,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ee4ba7b-b9f5-4355-8f64-9358e75422e8",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Thank you for adding explicit tags. \n",
        "createdAt" : "2015-03-24T18:20:08Z",
        "updatedAt" : "2015-03-24T18:20:08Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab0ae0a64e2e97176957078bcd8a89e170d4b344",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +24,28 @@    containers:\n      - name: cassandra\n        image: kubernetes/cassandra:v2\n        command:\n          - /run.sh"
  },
  {
    "id" : "58106bbb-8529-4621-9ee4-ed07e9d2f851",
    "prId" : 3217,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00c27fa9-1593-460e-bbf0-1cfc9170b8bb",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "\"This is the way that we initially use Services with Cassandra\"\n",
        "createdAt" : "2015-01-05T18:13:11Z",
        "updatedAt" : "2015-01-05T18:13:11Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "b06458e0ea25765f4ff9f51dca618e2e97e37ed3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +62,66 @@\n### Adding a Cassandra Service\nIn Kubernetes a _Service_ describes a set of Pods that perform the same task.  For example, the set of nodes in a Cassandra cluster, or even the single node we created above.  An important use for a Service is to create a load balancer which distributes traffic across members of the set.  But a _Service_ can also be used as a standing query which makes a dynamically changing set of Pods (or the single Pod we've already created) available via the Kubernetes API.  This is the way that we use initially use Services with Cassandra.\n\nHere is the service description:"
  },
  {
    "id" : "bfd6f66f-6a82-44f9-8659-3d8eebd6a055",
    "prId" : 3211,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a1d936c-c677-439e-8825-7d9aad0403be",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "maybe say \"Pods (or a single Pod in this case)\" to clarify that even when there is no load to balance, you want a Service.\n",
        "createdAt" : "2015-01-05T16:40:50Z",
        "updatedAt" : "2015-01-05T16:40:50Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "04f51b60de3dc222786311629c75990052014e17",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +62,66 @@\n### Adding a Cassandra Service\nIn Kubernetes a _Service_ describes a set of Pods that perform the same task.  For example, the set of nodes in a Cassandra cluster.  An important use for a Service is to create a load balancer which distributes traffic across members of the set.  But a _Service_ can also be used as a standing query which makes a dynamically changing set of Pods available via the Kubernetes API.  This is the way that we use initially use Services with Cassandra.\n\nHere is the service description:"
  },
  {
    "id" : "1292de93-24af-458e-b901-2aef5ac5152d",
    "prId" : 3211,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c72ba00-1d5e-4ceb-9c96-004dfa79ae86",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "english language parse error near \"... is the selector a label ...\" \n",
        "createdAt" : "2015-01-05T16:41:55Z",
        "updatedAt" : "2015-01-05T16:41:55Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "04f51b60de3dc222786311629c75990052014e17",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +75,79 @@```\n\nThe important thing to note here is the ```selector``` a label selector is a query that identifies the set of _Pods_ contained by the _Service_.  In this case the selector is ```name=cassandra```.  If you look back at the Pod specification above, you'll see that the pod has the corresponding label, so it will be selected for membership in this Service.\n\nCreate this service as follows:"
  }
]