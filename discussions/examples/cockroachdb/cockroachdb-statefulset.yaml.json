[
  {
    "id" : "f4358aa2-965a-4ce6-ae5c-3df208231a7e",
    "prId" : 39103,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39103#pullrequestreview-14710639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8718ab54-b4c5-4fe6-bf5a-715a3e49d42c",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "What happens if you scale the statefulset to run more pods than the available nodes? @smarterclayton @bprashanth do we have an issue somewhere about adding Conditions in StatefulSets and surface any failures?",
        "createdAt" : "2016-12-23T14:46:51Z",
        "updatedAt" : "2016-12-29T21:11:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "d3282581-24c4-47f0-9d45-b032f98cd713",
        "parentId" : "8718ab54-b4c5-4fe6-bf5a-715a3e49d42c",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "It actually doesn't appear to be getting strictly enforced :/ Is there an existing bug around this?\r\n\r\nI had initially tried this on minikube (1 node, 1.5.1) and all the pods were scheduled/started just fine. I also just tried it on a 3 node cluster on GKE (master on 1.5.1, nodes on 1.4.7) and found that it put two of the replicas on the same node even when there were only 3 replicas. Even worse, there was sufficient space to have put one of the replicas on the third node, given that when I scaled up to 4 pods the fourth got placed on the third node.",
        "createdAt" : "2016-12-27T22:54:39Z",
        "updatedAt" : "2016-12-29T21:11:47Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "4178b68e-c2a7-4177-807a-8cd6c1f2afad",
        "parentId" : "8718ab54-b4c5-4fe6-bf5a-715a3e49d42c",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Based on the [design proposal](https://github.com/kubernetes/kubernetes/blob/9ac1bcaa0337e8a54554c894a07366ae0eae52e4/docs/design/podaffinity.md), `requiredDuringSchedulingRequiredDuringExecution` should avoid from scheduling the pod. Can you open a bug about it? ",
        "createdAt" : "2016-12-28T20:13:58Z",
        "updatedAt" : "2016-12-29T21:11:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "434a6420-24a6-42fb-b5f3-41c29674060e",
        "parentId" : "8718ab54-b4c5-4fe6-bf5a-715a3e49d42c",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "We don't have an issue for SS having conditions, but that should be something we *must* do before graduation from beta.",
        "createdAt" : "2016-12-29T15:45:14Z",
        "updatedAt" : "2016-12-29T21:11:47Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "96f44f9a-37fe-44ce-8676-b42c4381132a",
        "parentId" : "8718ab54-b4c5-4fe6-bf5a-715a3e49d42c",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "Thanks, I've opened https://github.com/kubernetes/kubernetes/issues/39310\r\n\r\nAlso, I've switched to `preferredDuringSchedulingIgnoredDuringExecution`, since it won't break anything to have multiple instances on the same node, it's just strongly preferred not to.",
        "createdAt" : "2016-12-29T20:59:11Z",
        "updatedAt" : "2016-12-29T21:11:47Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff2ddbf208ffa6d7e3b69cf43af418bcd2af980c",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +79,83 @@        app: cockroachdb\n      annotations:\n        scheduler.alpha.kubernetes.io/affinity: >\n            {\n              \"podAntiAffinity\": {"
  },
  {
    "id" : "d6335263-2ae8-4bd8-8eb0-8b2b8e3531ad",
    "prId" : 39103,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39103#pullrequestreview-14639683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "074ef9fe-f365-49d7-a250-43f373c433c8",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Does  PodDisruptionBudget work for Deployments also ? Looking at http://kubernetes.io/docs/admin/disruptions/ its not obvious to me.",
        "createdAt" : "2016-12-29T07:57:48Z",
        "updatedAt" : "2016-12-29T21:11:47Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff2ddbf208ffa6d7e3b69cf43af418bcd2af980c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +64,68 @@  selector:\n    matchLabels:\n      app: cockroachdb\n  minAvailable: 67%\n---"
  }
]