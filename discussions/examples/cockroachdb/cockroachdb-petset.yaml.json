[
  {
    "id" : "9b2d359d-551e-41b9-af32-b3619d328ab8",
    "prId" : 28446,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "518b8b56-46cf-4281-a0ad-275f2afb1627",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "please add a comment elaborating what these ports are used for (pg protocol and raft, right?)\n",
        "createdAt" : "2016-07-04T04:58:20Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "2e18ce5e-f55b-4bee-8594-823d878b3c17",
        "parentId" : "518b8b56-46cf-4281-a0ad-275f2afb1627",
        "authorId" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "body" : "Not quite. I added a comment.\n",
        "createdAt" : "2016-07-04T23:01:51Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "tags" : [
        ]
      }
    ],
    "commit" : "6889f83a00069f63a1d5b1f4c5f41dafe6faccfc",
    "line" : null,
    "diffHunk" : "@@ -1,1 +11,15 @@  # The main port, served by gRPC, serves Postgres-flavor SQL, internode\n  # traffic and the cli.\n  - port: 26257\n    targetPort: 26257\n    name: grpc"
  },
  {
    "id" : "c6a5ef10-7607-4db6-b39b-8a3404509a5f",
    "prId" : 28446,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ccbd318-7ce6-4dbc-8216-4807ddbcc754",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "please include a \"limitations\" section in the README above\n",
        "createdAt" : "2016-07-04T04:58:24Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "7b2270ba-b59c-4caa-b091-a7fa8dff701c",
        "parentId" : "2ccbd318-7ce6-4dbc-8216-4807ddbcc754",
        "authorId" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "body" : "Done.\n",
        "createdAt" : "2016-07-04T23:01:53Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "tags" : [
        ]
      },
      {
        "id" : "05a19240-7edc-4c35-bc97-fbcfbf737867",
        "parentId" : "2ccbd318-7ce6-4dbc-8216-4807ddbcc754",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "This problem (it can also occur before a cluster has been initialized the first time) is something we need to solve by providing a strongly consistent config change mechanism available to nodes.  I think we'll make that a criteria for beta.\n",
        "createdAt" : "2016-07-05T00:48:43Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "6889f83a00069f63a1d5b1f4c5f41dafe6faccfc",
    "line" : null,
    "diffHunk" : "@@ -1,1 +71,75 @@            # There are likely ways out. For example, the init container could\n            # query the kubernetes API and see whether any other nodes are\n            # around, etc. Or, of course, the admin can pre-seed the lost\n            # volume somehow (and in that case we should provide a better way,\n            # for example a marker file)."
  },
  {
    "id" : "94197ebc-3f99-4b31-bb99-fd337517f49d",
    "prId" : 28446,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98fe3e71-478a-4390-83fd-38339c3237f1",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "there are 2 ways to run this:\n1. Assume people have dynamic provisioners\n2. Make something that works out of the box\n\n2 seems like the best approach for now, since dynamic provisioning is in alpha, but I suspect most people serious about this will run it with 1. Please include some text in the README saying something like \"if you have a dynamic provisioner running in your cluster [link] you don't need to create the volumes\".\n",
        "createdAt" : "2016-07-04T04:58:29Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "1dba9e59-4b58-4812-831c-b98f3ba2c82d",
        "parentId" : "98fe3e71-478a-4390-83fd-38339c3237f1",
        "authorId" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "body" : "2 puts me in unfamiliar terrain. Is there another example that demonstrates what you mean?\n",
        "createdAt" : "2016-07-04T23:01:55Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "tags" : [
        ]
      },
      {
        "id" : "e6a59073-f603-4a17-80e6-847fc6d74133",
        "parentId" : "98fe3e71-478a-4390-83fd-38339c3237f1",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "@thockin if we allowed EmptyDir to be added to PV then users could demonstrate this without gyrations today (the whole discussion about it being too hard to actually use PV for dev / test).\n\nThe alternative for now (which you could do here) is to have users create N hostPath PVs with a unique path per, and then hope that folks are running as root / have SELinux turned off.  We really need to document that.\n",
        "createdAt" : "2016-07-05T00:50:37Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "fc4a69d7-1224-43ae-92af-209120eff174",
        "parentId" : "98fe3e71-478a-4390-83fd-38339c3237f1",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Or always deploy a default throwaway directory storage provisioner https://github.com/kubernetes/kubernetes/issues/27732 \n",
        "createdAt" : "2016-07-05T01:06:22Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "533ab8b2-9252-4a74-8985-9dc3778944e3",
        "parentId" : "98fe3e71-478a-4390-83fd-38339c3237f1",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Yeah, but I'd rather do it with emptyDir.  I don't think tmpfs solves the\nproblem for medium sized directory sets.\n\nUltimately this is a storage class (\"free\") and a durability guarantee\n(\"instance\") and a quota (based on request capacity).  Since we already\nhave an alpha empty dir quota solution I want to sketch out the path that\ngets us to where users _always_ just create PVCs and they never have design\nwithout them.\n\nOn Jul 4, 2016, at 9:06 PM, Prashanth B notifications@github.com wrote:\n\nIn examples/cockroachdb/cockroachdb-petset.yaml\nhttps://github.com/kubernetes/kubernetes/pull/28446#discussion_r69497879:\n\n> -            # Of course this isn't without danger - if node0 loses its data,\n> -            # it will upon restarting it will simply bootstrap a new cluster\n> -            # and smack it into our existing cluster.\n> -            # There are likely ways out. For example, the init container could\n> -            # query the kubernetes API and see whether any other nodes are\n> -            # around, etc. Or, of course, the admin can pre-seed the lost\n> -            # volume somehow (and in that case we should provide a better way,\n> -            # for example a marker file).\n> -            if [ ! \"$(hostname)\" == \"cockroachdb-0\" ] || \\\n> -               [ -e \"/cockroach/cockroach-data/COCKROACHDB_VERSION\" ]\n> -            then\n> -              CRARGS+=(\"--join\" \"cockroachdb\")\n> -            fi\n> -            /cockroach/cockroach ${CRARGS[*]}\n> -      volumes:\n> -      - name: datadir\n\nOr always deploy a default throwaway directory storage provisioner #27732\nhttps://github.com/kubernetes/kubernetes/issues/27732\n\nâ€”\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/28446/files/a402b08521bc41a113668d2f4f1fab5f10622302#r69497879,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ABG_p_fTiXcbkOHDG-WXI00TpfBCt2wnks5qSa4bgaJpZM4JEBSZ\n.\n",
        "createdAt" : "2016-07-05T01:26:17Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "abd3b21f-c35d-4d18-ae7d-d04709c1e7ae",
        "parentId" : "98fe3e71-478a-4390-83fd-38339c3237f1",
        "authorId" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "body" : "Anything left to do here? I'm waiting for feedback (read that PRs are\nclosed after 2wks of inactivity).\n\nOn Tue, Jul 5, 2016, 03:27 Clayton Coleman notifications@github.com wrote:\n\n> In examples/cockroachdb/cockroachdb-petset.yaml\n> https://github.com/kubernetes/kubernetes/pull/28446#discussion_r69498643\n> :\n> \n> > -            # Of course this isn't without danger - if node0 loses its data,\n> > -            # it will upon restarting it will simply bootstrap a new cluster\n> > -            # and smack it into our existing cluster.\n> > -            # There are likely ways out. For example, the init container could\n> > -            # query the kubernetes API and see whether any other nodes are\n> > -            # around, etc. Or, of course, the admin can pre-seed the lost\n> > -            # volume somehow (and in that case we should provide a better way,\n> > -            # for example a marker file).\n> > -            if [ ! \"$(hostname)\" == \"cockroachdb-0\" ] || \\\n> > -               [ -e \"/cockroach/cockroach-data/COCKROACHDB_VERSION\" ]\n> > -            then\n> > -              CRARGS+=(\"--join\" \"cockroachdb\")\n> > -            fi\n> > -            /cockroach/cockroach ${CRARGS[*]}\n> > -      volumes:\n> > -      - name: datadir\n> \n> Yeah, but I'd rather do it with emptyDir. I don't think tmpfs solves the\n> problem for medium sized directory sets. Ultimately this is a storage class\n> (\"free\") and a durability guarantee (\"instance\") and a quota (based on\n> request capacity). Since we already have an alpha empty dir quota solution\n> I want to sketch out the path that gets us to where users _always_ just\n> create PVCs and they never have design without them. On Jul 4, 2016, at\n> 9:06 PM, Prashanth B notifications@github.com wrote: In\n> examples/cockroachdb/cockroachdb-petset.yaml <#28446 (comment)\n> https://github.com/kubernetes/kubernetes/pull/28446#discussion_r69497879\n> \n> > :\n> > - # Of course this isn't without danger - if node0 loses its data, + # it\n> >   will upon restarting it will simply bootstrap a new cluster + # and smack\n> >   it into our existing cluster. + # There are likely ways out. For example,\n> >   the init container could + # query the kubernetes API and see whether any\n> >   other nodes are + # around, etc. Or, of course, the admin can pre-seed the\n> >   lost + # volume somehow (and in that case we should provide a better way, +\n> >   # for example a marker file). + if [ ! \"$(hostname)\" == \"cockroachdb-0\" ]\n> >   || \\ + [ -e \"/cockroach/cockroach-data/COCKROACHDB_VERSION\" ] + then +\n> >   CRARGS+=(\"--join\" \"cockroachdb\") + fi + /cockroach/cockroach ${CRARGS[*]} +\n> >   volumes: + - name: datadir\n> >   Or always deploy a default throwaway directory storage provisioner #27732\n> >   https://github.com/kubernetes/kubernetes/issues/27732 <#27732\n> >   https://github.com/kubernetes/kubernetes/issues/27732> â€” You are\n> >   receiving this because you were mentioned. Reply to this email directly,\n> >   view it on GitHub <\n> >   https://github.com/kubernetes/kubernetes/pull/28446/files/a402b08521bc41a113668d2f4f1fab5f10622302#r69497879>,\n> >   or mute the thread <\n> >   https://github.com/notifications/unsubscribe/ABG_p_fTiXcbkOHDG-WXI00TpfBCt2wnks5qSa4bgaJpZM4JEBSZ>\n> >   .\n> \n> â€”\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/28446/files/a402b08521bc41a113668d2f4f1fab5f10622302#r69498643,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AE135M6Jf7QmzgU6JciYxe9S1-yVyLhSks5qSbLpgaJpZM4JEBSZ\n> .\n",
        "createdAt" : "2016-07-17T20:15:10Z",
        "updatedAt" : "2016-08-18T20:00:03Z",
        "lastEditedBy" : "459f4d66-1273-4f83-9c66-0a9795d02dfd",
        "tags" : [
        ]
      }
    ],
    "commit" : "6889f83a00069f63a1d5b1f4c5f41dafe6faccfc",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +81,85 @@            /cockroach/cockroach ${CRARGS[*]}\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir"
  }
]