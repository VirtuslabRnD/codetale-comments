[
  {
    "id" : "41cdc3fb-385d-48d7-9909-e087c7ae4c1d",
    "prId" : 722,
    "prUrl" : "https://github.com/typelevel/cats-effect/pull/722#pullrequestreview-329033025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1dfd58e3-ae37-4b07-b9d5-bc4fa0528801",
        "parentId" : null,
        "authorId" : "34e8cbcb-0f79-491e-8a9a-a47b1269128c",
        "body" : "Inlined the thread name argument as per the `BlockerPlatform` implementation.\r\n\r\nCan revert this change if requested.",
        "createdAt" : "2019-12-09T12:52:57Z",
        "updatedAt" : "2019-12-10T00:22:55Z",
        "lastEditedBy" : "34e8cbcb-0f79-491e-8a9a-a47b1269128c",
        "tags" : [
        ]
      },
      {
        "id" : "80f458df-0fde-437f-8bac-49e413d0bbe9",
        "parentId" : "1dfd58e3-ae37-4b07-b9d5-bc4fa0528801",
        "authorId" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "body" : "Seems better this way to me!",
        "createdAt" : "2019-12-09T16:07:37Z",
        "updatedAt" : "2019-12-10T00:22:56Z",
        "lastEditedBy" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c346dbbdfa3dc0b0dccda595e04ba821c75f335",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +35,39 @@        val ctr = new AtomicInteger(0)\n        def newThread(r: Runnable): Thread = {\n          val back = new Thread(r, s\"ioapp-compute-${ctr.getAndIncrement()}\")\n          back.setDaemon(true)\n          back"
  },
  {
    "id" : "7624df9e-6490-4076-a1b8-3297f58ec518",
    "prId" : 547,
    "prUrl" : "https://github.com/typelevel/cats-effect/pull/547#pullrequestreview-242920981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5241e31f-57b7-4582-835f-4db799d5acee",
        "parentId" : null,
        "authorId" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "body" : "I continue to dislike daemon threads for important work.  `IOApp.WithContext` is the existing compromise, though it's inconvenient enough I'm not so vigilant about using it.\r\n\r\nI don't know how to avoid the threads and shut down the context properly without a breaking change.  This comment may be more of a 3.0 discussion.",
        "createdAt" : "2019-05-27T14:39:10Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "tags" : [
        ]
      },
      {
        "id" : "d097c8ce-d27b-4705-a421-ee07821ac8bf",
        "parentId" : "5241e31f-57b7-4582-835f-4db799d5acee",
        "authorId" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "body" : "I went back and forth on this. I think it's okay for these to be daemon specifically *because* we have other control flow mechanisms which guarantee everything gets wound down before exiting under normal circumstances. In the event that something is exiting without tracing through the control flow graph all the way to the end of `IOApp`, then something horrible has happened (like OOM), and we want the threads to just die.\r\n\r\nTLDR, I think that threads blocking shutdown is a feature that isn't relevant or necessary in applications constructed with explicit monadic control flow.",
        "createdAt" : "2019-05-27T16:24:58Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "tags" : [
        ]
      },
      {
        "id" : "0319dcf4-d038-44f1-b7e0-6039447ca784",
        "parentId" : "5241e31f-57b7-4582-835f-4db799d5acee",
        "authorId" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "body" : "And I suppose we can say that any service that relies on its thread pools' graceful shutdown needs to be run on a separate, non-daemon pool.",
        "createdAt" : "2019-05-27T23:30:11Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "tags" : [
        ]
      },
      {
        "id" : "f2c8cd19-84f4-4d8e-a3d6-05ade50f9750",
        "parentId" : "5241e31f-57b7-4582-835f-4db799d5acee",
        "authorId" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "body" : "Exactly.",
        "createdAt" : "2019-05-28T21:04:14Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "tags" : [
        ]
      }
    ],
    "commit" : "57e81923886611a4cdd55d52736b07602ffb03ff",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +36,40 @@        val back = new Thread(r)\n        back.setName(s\"ioapp-compute-${ctr.getAndIncrement()}\")\n        back.setDaemon(true)\n        back\n      }"
  },
  {
    "id" : "b01fc33b-3b86-49d5-a4a3-f7b937b9aba4",
    "prId" : 547,
    "prUrl" : "https://github.com/typelevel/cats-effect/pull/547#pullrequestreview-242342302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a36b8398-6baf-44e2-9aac-23dd1fb458e7",
        "parentId" : null,
        "authorId" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "body" : "One false positive an http4s user ran into recently: rendering a very large String causes the charset encoder to allocate an array greater than Int.MaxValue and throws an `OutOfMemoryError(\"Requested array size exceeds VM limit\")` but everything (but that response) is fine.  It's irritating that the JVM throws this non-fatal error as the most inarguably fatal of error types.\r\n\r\nI think this approach is more correct than not, but I'm interested to hear if there are other horror stories of non-fatal errors that aren't caught by this.  The `System.exit` raises the cost of false negatives.",
        "createdAt" : "2019-05-27T15:04:41Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "tags" : [
        ]
      },
      {
        "id" : "1be20fd4-651b-4aab-b3eb-b0dbc6b19249",
        "parentId" : "a36b8398-6baf-44e2-9aac-23dd1fb458e7",
        "authorId" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "body" : "That's a fair point. Also that's a good example of an OOM which is legit recoverable. Another false-positive, as a note, could be `AssertionError`. It's another problem that I went back and forth on.\r\n\r\nI think the thing is that, the *moment* some sort of truly fatal error happens, I really do want to know it and explode spectacularly. The worst feeling in the world is knowing that your system *probably* went OOM (or had a link error, or such), but it was swallowed and now your only way to figure it out is to dump the running system and look for orphans.",
        "createdAt" : "2019-05-27T16:28:16Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "tags" : [
        ]
      }
    ],
    "commit" : "57e81923886611a4cdd55d52736b07602ffb03ff",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +51,55 @@            r.run()\n          } catch {\n            case NonFatal(t) =>\n              reportFailure(t)\n"
  },
  {
    "id" : "be6ffa43-2ed5-4129-a4b2-4d6da6d6fb6d",
    "prId" : 547,
    "prUrl" : "https://github.com/typelevel/cats-effect/pull/547#pullrequestreview-242307718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "991e9497-617e-4b6c-8b94-ce153287c101",
        "parentId" : null,
        "authorId" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "body" : ":+1:",
        "createdAt" : "2019-05-27T15:25:46Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "tags" : [
        ]
      }
    ],
    "commit" : "57e81923886611a4cdd55d52736b07602ffb03ff",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +29,33 @@  val ioAppGlobal: ExecutionContext = {\n    // lower-bound of 2 to prevent pathological deadlocks on virtual machines\n    val bound = math.max(2, Runtime.getRuntime().availableProcessors())\n\n    val executor = Executors.newFixedThreadPool(bound, new ThreadFactory {"
  },
  {
    "id" : "7e303151-1fb1-4882-ab0e-b4e711e43bfb",
    "prId" : 547,
    "prUrl" : "https://github.com/typelevel/cats-effect/pull/547#pullrequestreview-242921568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40b8a958-82cb-45d6-bc16-ac2cc939c9df",
        "parentId" : null,
        "authorId" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "body" : "Is the basic argument against ForkJoin that most people aren't doing divide-and-conquer tasks, and therefore we're all paying for the overhead of the layered queues without reaping its rewards?  I intuitively agree, but Scala standard library chose otherwise, and the [`ForkJoinPool` javadoc](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html) claims:\r\n\r\n> efficient processing ... when many small tasks are submitted to the pool from external clients\r\n\r\nand\r\n\r\n> Especially when setting asyncMode to true in constructors, ForkJoinPools may also be appropriate for use with event-style tasks that are never joined.\r\n\r\nIn _theory_, I agree with this change.  Empirically, I've swapped them out on real world workloads and not seen much difference either way.  I'm curious if you have apps where a FJ pool is noticeably detrimental to performance.  (Other points, like catching fatals and a minimum of one, are well taken and discussed elsewhere.  I'm strictly speaking performance here.)\r\n\r\nI imagine most cats-effect users implement blocking with `evalOn` and a second pool, but those who don't and whose blocking libraries save them by assuming a `blocking`-integrated executor will be in for an unpleasant surprise.\r\n",
        "createdAt" : "2019-05-27T16:07:47Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "tags" : [
        ]
      },
      {
        "id" : "d5676dcc-c8f3-4d83-8aef-13b4136edb80",
        "parentId" : "40b8a958-82cb-45d6-bc16-ac2cc939c9df",
        "authorId" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "body" : "> I'm curious if you have apps where a FJ pool is noticeably detrimental to performance.\r\n\r\nVery yes. There isn't a good tldr response to this comment, so I'll write something up in a gist instead.",
        "createdAt" : "2019-05-27T16:29:43Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "tags" : [
        ]
      },
      {
        "id" : "4eb5e964-16c6-43fc-b568-dc589e14a423",
        "parentId" : "40b8a958-82cb-45d6-bc16-ac2cc939c9df",
        "authorId" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "body" : "Oh actually, #509 contains a halfway-decent description. I can flesh it out more if you like. It's a lot less about the layered queues and more about the spawning semantics being optimized for bad thread pool usage. ForkJoin is a fantastic *general purpose* pool for Java. It's not a terrible choice for `Future`, either, since people tend to abuse `Future` for blocking side effects and such. Users of cats-effect though tend to be a lot more scrupulous about their pools, since the API is very carefully (and very effectively) designed to carrot them to do so.\r\n\r\nWhen you use ForkJoin to exclusively perform CPU-bound tasks that are throughput-optimized, it will behave *horrifically* poorly and you'll end up with a zillion threads.",
        "createdAt" : "2019-05-27T16:32:41Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "tags" : [
        ]
      },
      {
        "id" : "f62e2c8e-d52d-43bb-93d7-efb60cbb755e",
        "parentId" : "40b8a958-82cb-45d6-bc16-ac2cc939c9df",
        "authorId" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "body" : "Do you tend to see this with longer-running CPU intensive tasks, rather than a many small ones?",
        "createdAt" : "2019-05-27T23:45:21Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "19f1b7b0-eaed-49a1-9ac5-80ac65598c75",
        "tags" : [
        ]
      },
      {
        "id" : "7c394e47-c4aa-42a5-901f-c5d999e26444",
        "parentId" : "40b8a958-82cb-45d6-bc16-ac2cc939c9df",
        "authorId" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "body" : "Very much so, yes. Many small ones are generally okay because you're yielding back to the pool frequently. Heavily fairness-optimized workflows also don't see this issue. I'm relatively certain, based on the symptoms, that fork-join's heuristics are simply mistakenly identifying throughput-optimized workflows as being blocking IO, and thus spinning up new threads to avoid starvation.",
        "createdAt" : "2019-05-28T21:05:27Z",
        "updatedAt" : "2019-06-07T16:42:49Z",
        "lastEditedBy" : "5b44dcf3-8819-42b0-9a49-4cdbf5a8a305",
        "tags" : [
        ]
      }
    ],
    "commit" : "57e81923886611a4cdd55d52736b07602ffb03ff",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +31,35 @@    val bound = math.max(2, Runtime.getRuntime().availableProcessors())\n\n    val executor = Executors.newFixedThreadPool(bound, new ThreadFactory {\n      val ctr = new AtomicInteger(0)\n      def newThread(r: Runnable): Thread = {"
  }
]