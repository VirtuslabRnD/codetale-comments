[
  {
    "id" : "dd189997-392f-49a3-9620-335c307255c2",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-565778020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d55d72f3-19b6-4da4-8d83-297a411ef60e",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Does the log recovery logic need to be updated at all for the presence of snapshots?",
        "createdAt" : "2021-01-07T20:22:16Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ca3a75f2-f755-45f0-9058-6dcded4c510e",
        "parentId" : "d55d72f3-19b6-4da4-8d83-297a411ef60e",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "This PR changes the recovery logic so that the log is `truncateFullyToLatestSnapshot()` (https://github.com/apache/kafka/pull/9816/files#diff-b332f85b04775c821226b6f704e91d51f9647f29ba73dace65b99cf36f6b9ceaR298) if the `latestSnapshotId` is greater than `new OffsetAndEpoch(endOffset().offset, log.lastFetchedEpoch)`.\r\n\r\nThis can happen when a follower crashed after downloading a snapshot from the leader but before calling `truncateFullyToLatestSnapshot()`. Or https://github.com/apache/kafka/pull/9816/files#diff-b332f85b04775c821226b6f704e91d51f9647f29ba73dace65b99cf36f6b9ceaR155\r\n\r\nI think this is correct because `KafkaRaftClient` guarantees that any offset less then high watermark has been flushed and after https://issues.apache.org/jira/browse/KAFKA-10800 any snapshot generated by the state machine will be less than or equal to the high-watermark.",
        "createdAt" : "2021-01-11T22:07:41Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +43,47 @@import scala.compat.java8.OptionConverters._\n\nfinal class KafkaMetadataLog private (\n  log: Log,\n  // This object needs to be thread-safe because it is used by the snapshotting thread to notify the"
  },
  {
    "id" : "f33484bd-90e6-4a98-a08c-af498d017c57",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-568421679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69674b23-9c1c-40b7-938b-884bbc483261",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do we need to delete older snapshots?",
        "createdAt" : "2021-01-13T02:50:24Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e18cadae-2c22-4b5d-9271-3dd4b3be31cf",
        "parentId" : "69674b23-9c1c-40b7-938b-884bbc483261",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Yes. Up to this point we don't delete any snapshot. I was thinking of doing this in a future PR.\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-12205",
        "createdAt" : "2021-01-14T16:44:35Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +179,183 @@        // Truncate the log fully if the latest snapshot is greater than the log end offset\n\n        log.truncateFullyAndStartAt(snapshotId.offset)\n        oldestSnapshotId = latestSnapshotId\n"
  },
  {
    "id" : "e34dee6f-a83d-4ea3-b872-29e5dda71183",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-568449564",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d77c808-5629-4d88-933e-27319531109e",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Man, this is annoying. There is also a `pollLast` which returns null if the set is empty.",
        "createdAt" : "2021-01-13T02:57:31Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "321fea21-b767-40fe-9d97-d41ba34e4f6c",
        "parentId" : "8d77c808-5629-4d88-933e-27319531109e",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I agree. Unfortunately, `pollLast` removes the element from the set. I don't think we have a choice but to do this. ",
        "createdAt" : "2021-01-14T17:14:00Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 232,
    "diffHunk" : "@@ -1,1 +263,267 @@      Optional.of(snapshotIds.last)\n    } catch {\n      case _: NoSuchElementException =>\n        Optional.empty()\n    }"
  },
  {
    "id" : "df1993ee-2314-4238-a159-d7e55f3f90aa",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-566845278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "542e86ed-ad43-480d-be69-204614aa803d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Kind of a general note, but I think there are some logging gaps in here. I think we should tend toward the verbose side initially since snapshot events will be relatively infrequent.",
        "createdAt" : "2021-01-13T03:12:23Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 266,
    "diffHunk" : "@@ -1,1 +297,301 @@}\n\nobject KafkaMetadataLog {\n  def apply(\n    log: Log,"
  },
  {
    "id" : "5a4efc22-ea4d-428a-a3d8-4bfa16be6383",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-576636032",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18d29206-a1ac-4637-974d-c40421843b62",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess it's probably better to throw here. I was trying to think how we could end up here, but the only way I came up with is an invalid state transition which left the start or end offset inconsistent with the segment data. Sadly we have had a number of those bugs in the past. I debated whether we should just delete the snapshot, but I'm not sure that helps if we are left with inconsistent start/end offsets.",
        "createdAt" : "2021-01-15T01:54:08Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "39c34e04-cadf-47f8-8992-cf96ef9c1245",
        "parentId" : "18d29206-a1ac-4637-974d-c40421843b62",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I don't think we can delete the snapshot if `startOffset` is greater than 0.\r\n\r\nI think we need multiple bugs to throw this exception. Like you said the leader epoch cache would need to be inconsistent with respect to the log itself. Or we don't have a snapshot at the log start offset.\r\n\r\nI think at this point the best we can do is throw an exception. I think that deleting the snapshots and/or log could result in data loss. Maybe in the future we can do a `truncateFullyAtLastestSnapshot` but I would like to have a concrete reason for doing this.",
        "createdAt" : "2021-01-26T18:23:05Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +126,130 @@        } else {\n          throw new KafkaException(\n            s\"Log doesn't have a last fetch epoch and there is a snapshot ($snapshotId). \" +\n            s\"Expected the snapshot's end offset to match the log's end offset ($logEndOffset) \" +\n            s\"and the log start offset ($startOffset)\""
  },
  {
    "id" : "5862de13-fb69-46f1-83b7-f4263dd48843",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-576674479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80990245-c9b1-4fd8-9528-2bea8973b31f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Is it useful here to ensure that `snapshotId` is indeed lower than the high watermark?",
        "createdAt" : "2021-01-15T02:12:19Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "22f26789-45ac-4e26-a634-ffd61e137d66",
        "parentId" : "80990245-c9b1-4fd8-9528-2bea8973b31f",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Yes but only when it is the state machine that is creating the snapshot. When the raft client is creating the snapshot because it is downloading it from the leader we don't want to validate this.\r\n\r\nWe'll add this check to the `RaftClient::createSnapshot` API when we implement https://issues.apache.org/jira/browse/KAFKA-10800",
        "createdAt" : "2021-01-26T19:15:29Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +236,240 @@    latestSnapshotId().ifPresent { latest =>\n      if (latest.epoch > snapshotId.epoch || latest.offset > snapshotId.offset) {\n        // Since snapshots are less than the high-watermark absolute offset comparison is okay.\n        throw new IllegalArgumentException(\n          s\"Attemting to create a snapshot ($snapshotId) that is not greater than the latest snapshot ($latest)\""
  },
  {
    "id" : "ae222e9f-b5f0-4213-9c85-b13b90e7534f",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-576925096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c64d79ee-0df0-44fb-bdbf-377e24619df2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do we need to keep this as a var or could we access it from `snapshotIds` when needed (as we do for `latestSnapshotId`)?",
        "createdAt" : "2021-01-15T02:24:14Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b075083f-bd7e-40ce-be91-b15c37edd7bd",
        "parentId" : "c64d79ee-0df0-44fb-bdbf-377e24619df2",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Sorry for the delay reply. I missed this comment for some reason.\r\n\r\nAt the moment we need to keep this variable/reference. In this PR, the `oldestSnapshotId` can be anywhere in the `snapshotIds` set. I think that when we implement https://issues.apache.org/jira/browse/KAFKA-12205 can remove this variable.\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-12205 tracks the work needed to delete any snapshot that is less than the log start offset.",
        "createdAt" : "2021-01-26T17:46:18Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "958babe5-7377-403f-b873-9660bbd31112",
        "parentId" : "c64d79ee-0df0-44fb-bdbf-377e24619df2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm... It might be helpful to document this fact somewhere since it seems non-obvious. It surprised me anyway.",
        "createdAt" : "2021-01-27T02:18:22Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +56,60 @@   * snapshot id in the snapshotIds set.\n   */\n  private[this] var oldestSnapshotId = snapshotIds\n    .stream()\n    .filter(_.offset == startOffset)"
  },
  {
    "id" : "ca1eae25-6717-46de-b2b4-2031666616cb",
    "prId" : 10021,
    "prUrl" : "https://github.com/apache/kafka/pull/10021#pullrequestreview-599231367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "050cfbfa-efe5-420e-aa9e-5640e451e281",
        "parentId" : null,
        "authorId" : "51186e61-891a-464c-8c1d-9cf13522da98",
        "body" : "why we needs `false` here, it seems the default value of calling `headSet`?",
        "createdAt" : "2021-02-26T02:27:23Z",
        "updatedAt" : "2021-03-11T01:24:32Z",
        "lastEditedBy" : "51186e61-891a-464c-8c1d-9cf13522da98",
        "tags" : [
        ]
      },
      {
        "id" : "c5279077-c53c-4afc-8360-836e4ead5dcb",
        "parentId" : "050cfbfa-efe5-420e-aa9e-5640e451e281",
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "I add `false` to remind us that this headSet is not inclusive, I think it doesn't matter whether add or not, both are OK.",
        "createdAt" : "2021-02-26T02:36:58Z",
        "updatedAt" : "2021-03-11T01:24:32Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      },
      {
        "id" : "8ad2a5ce-9f59-4ada-90d6-6826f4e9e1d0",
        "parentId" : "050cfbfa-efe5-420e-aa9e-5640e451e281",
        "authorId" : "51186e61-891a-464c-8c1d-9cf13522da98",
        "body" : "makes sense, ty!",
        "createdAt" : "2021-02-26T03:44:59Z",
        "updatedAt" : "2021-03-11T01:24:32Z",
        "lastEditedBy" : "51186e61-891a-464c-8c1d-9cf13522da98",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce539fa117584682bd25c719e93ed5f371772b34",
    "line" : 140,
    "diffHunk" : "@@ -1,1 +287,291 @@   */\n  private def removeSnapshotFilesBefore(logStartSnapshotId: OffsetAndEpoch): Unit = {\n    val expiredSnapshotIdsIter = snapshotIds.headSet(logStartSnapshotId, false).iterator()\n    while (expiredSnapshotIdsIter.hasNext) {\n      val snapshotId = expiredSnapshotIdsIter.next()"
  },
  {
    "id" : "cbb836b4-e7dc-40d3-8ae5-91f6ad3d87ec",
    "prId" : 10256,
    "prUrl" : "https://github.com/apache/kafka/pull/10256#pullrequestreview-604483374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45575fe7-9b00-4ce8-be6f-48b032800d20",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "👍  Makes sense to move this here since KafkaMetadataLog fully owns the Log's lifecycle",
        "createdAt" : "2021-03-04T19:41:55Z",
        "updatedAt" : "2021-03-04T20:36:43Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "fad668873765974413cef49f63688665702983c6",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +305,309 @@    val defaultLogConfig = LogConfig(props)\n\n    val log = Log(\n      dir = dataDir,\n      config = defaultLogConfig,"
  },
  {
    "id" : "c23b8618-368a-4186-8ac9-b03f150695a0",
    "prId" : 10431,
    "prUrl" : "https://github.com/apache/kafka/pull/10431#pullrequestreview-654087535",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "085a5ece-1715-43fd-9efa-b15c46848bd2",
        "parentId" : null,
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "Is it necessary to put read operations in synchronized code? ",
        "createdAt" : "2021-05-06T05:58:33Z",
        "updatedAt" : "2021-05-11T17:08:50Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      },
      {
        "id" : "753ad310-c2d2-4029-8677-b8a9cba59bfc",
        "parentId" : "085a5ece-1715-43fd-9efa-b15c46848bd2",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Most modern hardware and Java's memory model require that values used by multiple core/threads is read from RAM and written to RAM. In Java, you can force this by using either using `volatile`, `synchronized` or a lot of the types in the `java.util.concurrent` package. The important observation is that this is needed for both reads and writes. This is a decent summary of the issue: https://medium.com/javarevisited/java-concurrency-java-memory-model-96e3ac36ec6b",
        "createdAt" : "2021-05-06T18:12:07Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "7b45380d-9fe9-4411-ae7f-82a2e0a394c1",
        "parentId" : "085a5ece-1715-43fd-9efa-b15c46848bd2",
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "Thanks for the explanations",
        "createdAt" : "2021-05-07T05:17:02Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "8617df7c84ecc2356e9c0e7c0a3489feba826e7d",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +278,282 @@\n  override def latestSnapshotId(): Optional[OffsetAndEpoch] = {\n    snapshots synchronized {\n      snapshots.lastOption.map { case (snapshotId, _) => snapshotId }.asJava\n    }"
  },
  {
    "id" : "cb1a0724-5265-458c-9b7d-ffecb288adde",
    "prId" : 10431,
    "prUrl" : "https://github.com/apache/kafka/pull/10431#pullrequestreview-653756452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e152c08-0e48-440d-8432-2412b013d549",
        "parentId" : null,
        "authorId" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "body" : "why not put this write operation in synchronized code?",
        "createdAt" : "2021-05-06T06:00:30Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "12f98c82-b9ef-4668-aa6f-5a62bf3439bb",
        "tags" : [
        ]
      },
      {
        "id" : "b0aa1874-880d-4b40-8049-16fadae995c9",
        "parentId" : "3e152c08-0e48-440d-8432-2412b013d549",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "We do. This method has a comment saying that it assumes that `synchronized` was used before calling this method. I think this is okay since the method is private. Let me know what you think.\r\n\r\nIf you look at the two callers of this method, they use different locking strategies.",
        "createdAt" : "2021-05-06T18:15:35Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "8617df7c84ecc2356e9c0e7c0a3489feba826e7d",
    "line" : 186,
    "diffHunk" : "@@ -1,1 +328,332 @@  ): mutable.TreeMap[OffsetAndEpoch, Option[FileRawSnapshotReader]] = {\n    val expiredSnapshots = snapshots.until(logStartSnapshotId).clone()\n    snapshots --= expiredSnapshots.keys\n\n    expiredSnapshots"
  },
  {
    "id" : "d9259b4b-10c3-4661-a97b-2475f29960cc",
    "prId" : 10431,
    "prUrl" : "https://github.com/apache/kafka/pull/10431#pullrequestreview-655875882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b947f16-5fb7-4c7d-a531-a84a3d087147",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Can we avoid adding new code that uses a deprecated method?",
        "createdAt" : "2021-05-07T18:16:34Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "88a92c5c-384f-4222-bdfa-91ef2e07adb8",
        "parentId" : "6b947f16-5fb7-4c7d-a531-a84a3d087147",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "The issue is that Kafka needs to compile against both Scala 2.12 and 2.13. In Scala 2.13 a lot of the collection methods were deprecated. The community created [scala-collection-compat](https://github.com/scala/scala-collection-compat) to allow the use of 2.13 functionality in 2.12. Apache Kafka depends on that project. Unfortunately, there is a pretty annoying bug in the latest stable version of `scala-collection-compat` that generates \"unused import warning\" when used in 2.13. The Kafka project turns those warnings into errors and the Scala compiler doesn't allow the use of `nowarn` in imports.\r\n\r\nThe best solution I can find is to use 2.12 methods that are deprecated and add this nowarn annotation.\r\n\r\nIt looks like this problem has been fixed in the devel version of `scala-collection-compat`. When that becomes a release version, we can remove this and few other `nowarn`.\r\n\r\n:dizzy: ",
        "createdAt" : "2021-05-07T21:36:12Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "bd4629cc-f54b-496a-8b46-869a092cbc67",
        "parentId" : "6b947f16-5fb7-4c7d-a531-a84a3d087147",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "👍 thanks for the explanation ",
        "createdAt" : "2021-05-10T17:08:23Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "8617df7c84ecc2356e9c0e7c0a3489feba826e7d",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +323,327 @@   * This method assumes that the lock for `snapshots` is already held.\n   */\n  @nowarn(\"cat=deprecation\") // Needed for TreeMap.until\n  private def forgetSnapshotsBefore(\n    logStartSnapshotId: OffsetAndEpoch"
  },
  {
    "id" : "932317d4-59c3-4758-a557-f3005deb3255",
    "prId" : 10431,
    "prUrl" : "https://github.com/apache/kafka/pull/10431#pullrequestreview-654895799",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d896ff78-5d22-4003-974c-07c683a26e7a",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Just to clarify my understanding -- this is the main change in this PR right? The FileRawSnapshotReader-s are now owned (opened and closed) by KafkaMetadataLog whereas previously they were opened here, but never closed. \r\n\r\nNow in KafkaMetadataLog#close and KafkaMetadataLog#deleteSnapshotFiles, we are closing the FileRawSnapshotReader instances that we opened.",
        "createdAt" : "2021-05-07T18:41:04Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "103d3e8a-a947-4e4e-a736-d5f08735b3eb",
        "parentId" : "d896ff78-5d22-4003-974c-07c683a26e7a",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Correct.",
        "createdAt" : "2021-05-07T21:57:45Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "8617df7c84ecc2356e9c0e7c0a3489feba826e7d",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +259,263 @@            val snapshotReader = Some(FileRawSnapshotReader.open(log.dir.toPath, snapshotId))\n            snapshots.put(snapshotId, snapshotReader)\n            snapshotReader\n          } catch {\n            case _: NoSuchFileException =>"
  },
  {
    "id" : "f522bf68-df82-4eec-91da-27fe84df590d",
    "prId" : 10431,
    "prUrl" : "https://github.com/apache/kafka/pull/10431#pullrequestreview-656994060",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "134d6b50-5862-4ac0-a72c-a09dd63a4634",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Should we grab the `snapshots` lock for this whole match expression like we do in deleteBeforeSnapshot? Is there possible a race between this block and deleteBeforeSnapshot?",
        "createdAt" : "2021-05-10T17:26:59Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "1b8e9283-d137-4f11-895b-9e3b8f342dc1",
        "parentId" : "134d6b50-5862-4ac0-a72c-a09dd63a4634",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Synchronizing `snapshots` is only needed when accessing that object. In `deleteBeforeSnapshot` it is grabbed because the `match` expression accesses `snapshots` in one of the `case`/branch.\r\n\r\nIn this method I think it is safe to only grab the log where we currently do.",
        "createdAt" : "2021-05-11T17:06:13Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "8617df7c84ecc2356e9c0e7c0a3489feba826e7d",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +163,167 @@  override def truncateToLatestSnapshot(): Boolean = {\n    val latestEpoch = log.latestEpoch.getOrElse(0)\n    val (truncated, forgottenSnapshots) = latestSnapshotId().asScala match {\n      case Some(snapshotId) if (\n          snapshotId.epoch > latestEpoch ||"
  },
  {
    "id" : "19c88297-9712-4ce9-9455-a81e8be5f71a",
    "prId" : 10431,
    "prUrl" : "https://github.com/apache/kafka/pull/10431#pullrequestreview-656997010",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a81048c-74f0-4754-b067-0aa38cd9aab3",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "nit: IntelliJ complains about this line if the `() => ...` isn't there. It compiles fine, it just doesn't like it for some reason.",
        "createdAt" : "2021-05-10T17:35:33Z",
        "updatedAt" : "2021-05-11T17:08:51Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "154bf248-443c-4daa-b8e9-453973e229d3",
        "parentId" : "0a81048c-74f0-4754-b067-0aa38cd9aab3",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Okay. Changed it to use a different syntax that should get around this IntelliJ error.",
        "createdAt" : "2021-05-11T17:09:21Z",
        "updatedAt" : "2021-05-11T17:09:21Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "8617df7c84ecc2356e9c0e7c0a3489feba826e7d",
    "line" : 225,
    "diffHunk" : "@@ -1,1 +347,351 @@      scheduler.schedule(\n        \"delete-snapshot-files\",\n        KafkaMetadataLog.deleteSnapshotFiles(log.dir.toPath, expiredSnapshots),\n        fileDeleteDelayMs\n      )"
  },
  {
    "id" : "c5ed1131-57a2-4927-93f7-0e98492d55ed",
    "prId" : 10431,
    "prUrl" : "https://github.com/apache/kafka/pull/10431#pullrequestreview-658566412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bbf710d-6ed4-4cb7-aaf2-01f6defc958c",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Is the above comment still accurate since snapshots is no longer thread safe?",
        "createdAt" : "2021-05-12T20:08:55Z",
        "updatedAt" : "2021-05-12T20:40:00Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0d54c4ac-782a-438b-b0e2-d1501e5f016b",
        "parentId" : "1bbf710d-6ed4-4cb7-aaf2-01f6defc958c",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "No. I updated the comment. I'll push a commit tomorrow after a few other changes.",
        "createdAt" : "2021-05-13T04:15:38Z",
        "updatedAt" : "2021-05-13T04:15:39Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "8617df7c84ecc2356e9c0e7c0a3489feba826e7d",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +40,44 @@  // Access to this object needs to be synchronized because it is used by the snapshotting thread to notify the\n  // polling thread when snapshots are created. This object is also used to store any opened snapshot reader.\n  snapshots: mutable.TreeMap[OffsetAndEpoch, Option[FileRawSnapshotReader]],\n  topicPartition: TopicPartition,\n  maxFetchSizeInBytes: Int,"
  }
]