[
  {
    "id" : "07833fb6-b522-4d1c-a097-f603b4ae8025",
    "prId" : 4907,
    "prUrl" : "https://github.com/apache/kafka/pull/4907#pullrequestreview-115377119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I wonder if we should allow `LogContext` to be passed to `KafkaException` to avoid a bunch of duplication. Doesn't have to be in this PR, but is it a good idea?",
        "createdAt" : "2018-04-25T04:59:21Z",
        "updatedAt" : "2018-04-25T22:24:58Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "642431b9-3275-4760-877b-00f03adeea2c",
        "parentId" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's worth considering for sure. It does seem more awkward since you'd have to add overloads to all the sub-type constructors as well. And many of the exception types are public ðŸ˜ž . If we can find a nice way to do it, it's a good idea.",
        "createdAt" : "2018-04-25T22:28:50Z",
        "updatedAt" : "2018-04-25T22:28:50Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2777ab489b6bd3fbaf0733fb85eaf8619d43acdf",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +1382,1386 @@        if (prev != null)\n          throw new KafkaException(s\"Trying to roll a new log segment for topic partition $topicPartition with \" +\n            s\"start offset $newOffset while it already exists.\")\n        // We need to update the segment base offset and append position data of the metadata when log rolls.\n        // The next offset should not change."
  },
  {
    "id" : "000ebd10-0ca5-483f-8f47-87146d732626",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126872964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe we can have a separate `splitAndReplace` method so that we can call this from `loadSegments` without needing to mutate the internal state. Gives an excuse to reduce the size of this big function as well (even if just a little bit).",
        "createdAt" : "2018-06-06T21:24:36Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f7c5caec-6013-45df-b48f-f154ed03c06f",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I don't fully understand the intent here. Do you want to avoid the call to `replaceSegments` when `splitSegmentOnOffsetOverflow` is called from `loadSegments`?",
        "createdAt" : "2018-06-07T00:35:10Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "8b0e976f-138c-4427-a789-3785803f2f3e",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, that's right. The expectation in `replaceSegments` is that the segments have already been loaded. It is easy to avoid violating this expectation and makes the code easier to reason about. ",
        "createdAt" : "2018-06-07T17:03:21Z",
        "updatedAt" : "2018-06-07T17:07:52Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 539,
    "diffHunk" : "@@ -1,1 +1944,1948 @@\n    // replace old segment with new ones\n    replaceSegments(newSegments.toList, List(segment), isRecoveredSwapFile = false)\n    newSegments.toList\n  }"
  },
  {
    "id" : "dbbcb00d-97a1-4e47-830b-6805dfe19e1f",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126881769",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Any reason not to make this a case class?",
        "createdAt" : "2018-06-06T21:24:59Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9fbee259-6a47-412f-8aa9-c120e8d4cf1f",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "findbugs does not seem to like it. Detailed response here: https://github.com/apache/kafka/pull/4975#discussion_r187756280",
        "createdAt" : "2018-06-07T00:36:54Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "217d7d7f-6a61-46e1-ba29-808e6beaf1e0",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, I see. I think this is kind of an indirect way of telling us that what we're doing is a little odd. I'd probably suggest pulling `copyRecordsToSegment` up--perhaps even moving it into `LogSegment`--but we can leave that for a follow-up.",
        "createdAt" : "2018-06-07T17:28:40Z",
        "updatedAt" : "2018-06-07T17:28:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 448,
    "diffHunk" : "@@ -1,1 +1853,1857 @@    var readBuffer = ByteBuffer.allocate(1024 * 1024)\n\n    class CopyResult(val bytesRead: Int, val overflowOffset: Option[Long])\n\n    // Helper method to copy `records` into `segment`. Makes sure records being appended do not result in offset overflow."
  },
  {
    "id" : "5fba58e6-2ade-4edc-bd5c-029dbb195e8d",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-127408782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b495922-fadf-436f-856e-4c02d97d388a",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "It seems we use the traversal of sortedNewSegments in descending order below.\r\n```\r\nnewSegments.sortBy(- _.baseOffset)\r\n```\r\nIf we use the above to sort, the code would be more readable.\r\n\r\nsortedNewSegments.tail should be used on line 1778",
        "createdAt" : "2018-06-10T20:19:56Z",
        "updatedAt" : "2018-06-10T20:20:49Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 359,
    "diffHunk" : "@@ -1,1 +1762,1766 @@   */\n  private[log] def replaceSegments(newSegments: Seq[LogSegment], oldSegments: Seq[LogSegment], isRecoveredSwapFile: Boolean = false) {\n    val sortedNewSegments = newSegments.sortBy(_.baseOffset)\n    val sortedOldSegments = oldSegments.sortBy(_.baseOffset)\n"
  }
]