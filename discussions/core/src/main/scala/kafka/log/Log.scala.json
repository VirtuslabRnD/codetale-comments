[
  {
    "id" : "07833fb6-b522-4d1c-a097-f603b4ae8025",
    "prId" : 4907,
    "prUrl" : "https://github.com/apache/kafka/pull/4907#pullrequestreview-115377119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I wonder if we should allow `LogContext` to be passed to `KafkaException` to avoid a bunch of duplication. Doesn't have to be in this PR, but is it a good idea?",
        "createdAt" : "2018-04-25T04:59:21Z",
        "updatedAt" : "2018-04-25T22:24:58Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "642431b9-3275-4760-877b-00f03adeea2c",
        "parentId" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's worth considering for sure. It does seem more awkward since you'd have to add overloads to all the sub-type constructors as well. And many of the exception types are public ðŸ˜ž . If we can find a nice way to do it, it's a good idea.",
        "createdAt" : "2018-04-25T22:28:50Z",
        "updatedAt" : "2018-04-25T22:28:50Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2777ab489b6bd3fbaf0733fb85eaf8619d43acdf",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +1382,1386 @@        if (prev != null)\n          throw new KafkaException(s\"Trying to roll a new log segment for topic partition $topicPartition with \" +\n            s\"start offset $newOffset while it already exists.\")\n        // We need to update the segment base offset and append position data of the metadata when log rolls.\n        // The next offset should not change."
  },
  {
    "id" : "000ebd10-0ca5-483f-8f47-87146d732626",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126872964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe we can have a separate `splitAndReplace` method so that we can call this from `loadSegments` without needing to mutate the internal state. Gives an excuse to reduce the size of this big function as well (even if just a little bit).",
        "createdAt" : "2018-06-06T21:24:36Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f7c5caec-6013-45df-b48f-f154ed03c06f",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I don't fully understand the intent here. Do you want to avoid the call to `replaceSegments` when `splitSegmentOnOffsetOverflow` is called from `loadSegments`?",
        "createdAt" : "2018-06-07T00:35:10Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "8b0e976f-138c-4427-a789-3785803f2f3e",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, that's right. The expectation in `replaceSegments` is that the segments have already been loaded. It is easy to avoid violating this expectation and makes the code easier to reason about. ",
        "createdAt" : "2018-06-07T17:03:21Z",
        "updatedAt" : "2018-06-07T17:07:52Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 539,
    "diffHunk" : "@@ -1,1 +1944,1948 @@\n    // replace old segment with new ones\n    replaceSegments(newSegments.toList, List(segment), isRecoveredSwapFile = false)\n    newSegments.toList\n  }"
  },
  {
    "id" : "dbbcb00d-97a1-4e47-830b-6805dfe19e1f",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126881769",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Any reason not to make this a case class?",
        "createdAt" : "2018-06-06T21:24:59Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9fbee259-6a47-412f-8aa9-c120e8d4cf1f",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "findbugs does not seem to like it. Detailed response here: https://github.com/apache/kafka/pull/4975#discussion_r187756280",
        "createdAt" : "2018-06-07T00:36:54Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "217d7d7f-6a61-46e1-ba29-808e6beaf1e0",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, I see. I think this is kind of an indirect way of telling us that what we're doing is a little odd. I'd probably suggest pulling `copyRecordsToSegment` up--perhaps even moving it into `LogSegment`--but we can leave that for a follow-up.",
        "createdAt" : "2018-06-07T17:28:40Z",
        "updatedAt" : "2018-06-07T17:28:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 448,
    "diffHunk" : "@@ -1,1 +1853,1857 @@    var readBuffer = ByteBuffer.allocate(1024 * 1024)\n\n    class CopyResult(val bytesRead: Int, val overflowOffset: Option[Long])\n\n    // Helper method to copy `records` into `segment`. Makes sure records being appended do not result in offset overflow."
  },
  {
    "id" : "5fba58e6-2ade-4edc-bd5c-029dbb195e8d",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-127408782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b495922-fadf-436f-856e-4c02d97d388a",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "It seems we use the traversal of sortedNewSegments in descending order below.\r\n```\r\nnewSegments.sortBy(- _.baseOffset)\r\n```\r\nIf we use the above to sort, the code would be more readable.\r\n\r\nsortedNewSegments.tail should be used on line 1778",
        "createdAt" : "2018-06-10T20:19:56Z",
        "updatedAt" : "2018-06-10T20:20:49Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 359,
    "diffHunk" : "@@ -1,1 +1762,1766 @@   */\n  private[log] def replaceSegments(newSegments: Seq[LogSegment], oldSegments: Seq[LogSegment], isRecoveredSwapFile: Boolean = false) {\n    val sortedNewSegments = newSegments.sortBy(_.baseOffset)\n    val sortedOldSegments = oldSegments.sortBy(_.baseOffset)\n"
  },
  {
    "id" : "614ab52f-32e1-4c0f-bf0e-eca2ac021af1",
    "prId" : 5133,
    "prUrl" : "https://github.com/apache/kafka/pull/5133#pullrequestreview-128601412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa002b9f-675b-4776-b974-6d6b1d1780d4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Need to update the parameter list above",
        "createdAt" : "2018-06-13T23:38:48Z",
        "updatedAt" : "2018-06-14T05:35:55Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "486f83e3422d512e3660225606caeb067a58b41f",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +87,91 @@                         validBytes: Int,\n                         offsetsMonotonic: Boolean,\n                         lastOffsetOfFirstBatch: Long) {\n  /**\n   * Get the first offset if it exists, else get the last offset of the first batch"
  },
  {
    "id" : "bec53497-6112-4f18-a32a-2726ab00cef8",
    "prId" : 5169,
    "prUrl" : "https://github.com/apache/kafka/pull/5169#pullrequestreview-127652268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I am not sure if we want to throw an exception in this case. Could we have an overflown segment (may be produced by the log cleaner) such that messages appear only after the overflow offset?",
        "createdAt" : "2018-06-08T22:29:38Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "dead1cc7-93aa-467d-9a57-470ea47967f3",
        "parentId" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. That's an interesting point. It might be possible for a compacted topic since the base offset is only determined by the input segments. ",
        "createdAt" : "2018-06-09T05:12:28Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "22893160-cd61-4128-9303-e66304e93fa9",
        "parentId" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "On second thought, I think this assertion is fine. We should always have at least one batch that we can append since we are starting the new segment from the base offset of the first batch. What needs to change for this case is the assertion below that `newSegments` is greater than one. I will fix this and add a test case.",
        "createdAt" : "2018-06-11T16:52:45Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "c71b1e5937a33e525f483cd5d2edaa925fa68ccb",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +1878,1882 @@        val bytesAppended = newSegment.appendFromFile(sourceRecords, position)\n        if (bytesAppended == 0)\n          throw new IllegalStateException(s\"Failed to append records from position $position in $segment\")\n\n        position += bytesAppended"
  },
  {
    "id" : "382ef6b2-1084-4b1f-a433-11358d8df7ff",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-180933235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I wonder if `newOffset <= activeSegment.baseOffset` would be a stricter validation?",
        "createdAt" : "2018-12-02T00:40:47Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "43e8beea-a38e-4838-bb85-47dbafea5eff",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Since we are already validating `newOffset == activeSegment.baseOffset`, `newOffset < activeSegment.baseOffset` means that logEndOffset is lower than base offset of the active segment (based on `val newOffset = math.max(expectedNextOffset, logEndOffset)`). We probably validate this in other places, but could be useful to validate here as well. In that case, I would still validate `if (segments.containsKey(newOffset)) {` separately, and `else if `newOffset < activeSegment.baseOffset` throw another KafkaException. What do you think?",
        "createdAt" : "2018-12-03T17:22:17Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      },
      {
        "id" : "77553851-7e25-446f-8ea7-ba4500f449db",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Actually I think we don't need that additional validation. That would cause \"out of order offsets\" earlier when we try to do an append.",
        "createdAt" : "2018-12-03T18:33:24Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      },
      {
        "id" : "bf1686d0-347d-4629-8af3-6c0865006948",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "On a third thought, does not hurt to check that we are not rolling to offset < base offset of the active segment, even though a segment with this base offset does not exist.",
        "createdAt" : "2018-12-03T19:14:38Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1571,1575 @@        val logFile = Log.logFile(dir, newOffset)\n\n        if (segments.containsKey(newOffset)) {\n          // segment with the same base offset already exists and loaded\n          if (activeSegment.baseOffset == newOffset && activeSegment.size == 0) {"
  },
  {
    "id" : "cfa90ac8-9e57-47a9-9dfb-2318936cfdbf",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-181034261",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7947c3d9-da4c-4a3c-b53f-8ccb79fcecb2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: missing a space before `exists`",
        "createdAt" : "2018-12-03T21:59:57Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b4de25a9-2321-4434-882a-97c3eea54758",
        "parentId" : "7947c3d9-da4c-4a3c-b53f-8ccb79fcecb2",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "added space after \"already\"",
        "createdAt" : "2018-12-03T23:42:03Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1578,1582 @@            warn(s\"Trying to roll a new log segment with start offset $newOffset \" +\n                 s\"=max(provided offset = $expectedNextOffset, LEO = $logEndOffset) while it already \" +\n                 s\"exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},\" +\n                 s\" size of offset index: ${activeSegment.offsetIndex.entries}.\")\n            deleteSegment(activeSegment)"
  },
  {
    "id" : "0a07c6cb-0777-4f4a-87b5-3d066a636864",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-181034798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29d76210-488b-44dc-9291-c2429e8dbc3f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess another option would be to resize the index files. Did you feel this would be more reliable?",
        "createdAt" : "2018-12-03T23:25:04Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a9a7a8a6-6a77-4908-a181-e2b0c15953eb",
        "parentId" : "29d76210-488b-44dc-9291-c2429e8dbc3f",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "I am concerned about resizing index, because it does not seem like the issue happens because we explicitly resize the index. Seems like something happens to mmaped file, which we don't understand yet. So, it seems more reliable to remove and recreate the segment.",
        "createdAt" : "2018-12-03T23:44:11Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1580,1584 @@                 s\"exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},\" +\n                 s\" size of offset index: ${activeSegment.offsetIndex.entries}.\")\n            deleteSegment(activeSegment)\n          } else {\n            throw new KafkaException(s\"Trying to roll a new log segment for topic partition $topicPartition with start offset $newOffset\" +"
  },
  {
    "id" : "e52d2abc-9126-4bbf-bab6-5e45bd58c943",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-236337100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "parentId" : null,
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "One Optimization we can have here is to lock only when we need to populate firsBatchTimestamp, for example:\r\nin LogSegment.scala , we can add:\r\ndef isFirstBatchTimestampLoaded : Boolean = {\r\n    rollingBasedTimestamp match {\r\n      case Some(t) => true\r\n      case _ => false\r\n    }\r\n  }\r\n\r\nIn log.scala, we can have:\r\n\r\n@threadsafe\r\n  private[log] def getFirstBatchTimestampForSegments2(segments: Iterable[LogSegment]): Iterable[Long] = {\r\n\r\n    val needLoadTimestampSegments = segments.filter {\r\n      segment =>\r\n        !segment.isFirstBatchTimestampLoaded\r\n    }\r\n\r\n    if (needLoadTimestampSegments.nonEmpty) {\r\n      lock synchronized {\r\n        needLoadTimestampSegments.foreach {\r\n          segment =>\r\n            segment.loadFirstBatchTimestamp()\r\n        }\r\n      }\r\n    }\r\n\r\n    segments.map {\r\n      segment =>\r\n        segment.getFirstBatchTimestampWithoutLoading()\r\n    }\r\n  }\r\n\r\nThis optimization will reduce lock contention on the log.  However, each time we call log.logSegments,  we are acquiring the locks.  log.logSegments has been called multiple times (e.g, cleanableOffsets, LogToClean, etc.) during the log compaction . \r\n\r\nThe choice here is :  reduced potential lock contention versus simplified code. ",
        "createdAt" : "2019-01-29T02:03:24Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "002cf52f-5f28-4fd8-b422-e15cb14a808e",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "This isn't exactly introduced by your patch, but I'm not very clear on the thread-safety semantics offered by the log lock. E.g., in `getEstimatedEarliestTime` the first line grabs log segments using the underlying log lock but then relinquishes it.\r\n\r\nThat said, do you even need to lock? What is the worst that can happen? e.g., this could result in a failed attempt to read the earliest batch timestamp of a segment that may have been deleted after listing all the segments but I don't think this is actually an issue.",
        "createdAt" : "2019-02-05T06:21:48Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "d006bc93-d06c-4c5d-85ad-5c5a8cb7bb98",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "This is really good point.  I don't see lock is hold when logcleaner performs the actual compaction IOs : cleanSegments -> cleanInto.   Maybe we can skip the lock. Let me double check.",
        "createdAt" : "2019-02-05T18:47:09Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "a32f87a8-f237-40dc-b1af-eebf9619683f",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "can you add a comment here explaining why this is safe? per your explanation offline, segments cannot be removed outside of the cleaner (for log compacted topics).",
        "createdAt" : "2019-05-10T23:04:50Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2027,2031 @@    * to ensure no other logcleaner threads and retention thread can work on the same segment.\n    */\n  private[log] def getFirstBatchTimestampForSegments(segments: Iterable[LogSegment]): Iterable[Long] = {\n    segments.map {\n      segment =>"
  },
  {
    "id" : "5a66ed57-e891-4fb6-b1fc-0529462ef435",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-200724441",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "156cbc73-b4cb-4107-8d61-a3b87a036270",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "nice spotting of the duplication here ðŸ‘ ",
        "createdAt" : "2019-02-06T18:17:11Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +984,988 @@  def latestEpoch: Option[Int] = leaderEpochCache.flatMap(_.latestEpoch)\n\n  def endOffsetForEpoch(leaderEpoch: Int): Option[OffsetAndEpoch] = {\n    leaderEpochCache.flatMap { cache =>\n      val (foundEpoch, foundOffset) = cache.endOffsetFor(leaderEpoch)"
  },
  {
    "id" : "ca6e06fe-a0f0-4d32-8682-32588a21295f",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-201505566",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Nit: there is a possibility of creating and then immediately deleting the same file. If `LeaderEpochFile` had a `getPath(dir: Dir): Path` method which returns the file's path then with `Files.deleteIfExists(LeaderEpochFile.getPath(dir))` we could avoid that.\r\nEven more, `LeaderEpochFile` object I think could be merged into `LeaderEpochCheckpointFile`'s companion object as `LeaderEpochFile` used only here.",
        "createdAt" : "2019-02-07T17:31:04Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "678e5043-8c26-40e0-980b-e0e0a09b35f8",
        "parentId" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, `LeaderEpochFile` seems unnecessary. But can you clarify how the file would be created and then deleted? I think the call to `newFile` does not create the file itself. ",
        "createdAt" : "2019-02-07T21:42:10Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d077e147-9af3-4834-9ff4-d1328fa826df",
        "parentId" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Havin a second look it seems I overlooked it regarding `newFile`, so I take back that comment :)",
        "createdAt" : "2019-02-08T10:08:02Z",
        "updatedAt" : "2019-02-08T10:08:02Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +369,373 @@        warn(s\"Deleting non-empty leader epoch cache due to incompatible message format $recordVersion\")\n\n      Files.deleteIfExists(leaderEpochFile.toPath)\n      leaderEpochCache = None\n    } else {"
  },
  {
    "id" : "71dca4f4-2638-414c-afc9-56f76e4d7feb",
    "prId" : 6298,
    "prUrl" : "https://github.com/apache/kafka/pull/6298#pullrequestreview-206963671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22cf0607-ad66-4592-9da4-817e48205134",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "@hachikuji \r\nI had to add this because we would still rebuild the cache in the case of an unclean shutdown and restart with `log.message.format=0.10`\r\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogSegment.scala#L378\r\n\r\nNote that I removed the last test case in `testOldMessageFormatDeletesEpochCacheIfUnsupported()`. If we re-initialize the log with the new message format and unclean shutdown, it **will** rebuild it from the batches. This further exemplifies that we don't properly support message downgrades I think.\r\n\r\nCan you think of any implications here? I believe we might be fine",
        "createdAt" : "2019-02-22T11:44:42Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "bbe4dfa5-13d8-41cf-ac3a-93b4d0bdf1e1",
        "parentId" : "22cf0607-ad66-4592-9da4-817e48205134",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it seems ok. We can open a separate JIRA for the problem you raised since it affects trunk as well.",
        "createdAt" : "2019-02-22T18:18:34Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "043c83a9-7d08-4c5d-8974-7eb5859011ca",
        "parentId" : "22cf0607-ad66-4592-9da4-817e48205134",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Yep. Opened https://issues.apache.org/jira/browse/KAFKA-7984",
        "createdAt" : "2019-02-22T18:29:06Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "aaa4a352963294db81c011d5baf1afb101792595",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +559,563 @@        val truncatedBytes =\n          try {\n            recoverSegment(segment, if (supportsLeaderEpoch) Some(_leaderEpochCache) else None)\n          } catch {\n            case _: InvalidOffsetException =>"
  }
]