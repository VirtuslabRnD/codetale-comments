[
  {
    "id" : "07833fb6-b522-4d1c-a097-f603b4ae8025",
    "prId" : 4907,
    "prUrl" : "https://github.com/apache/kafka/pull/4907#pullrequestreview-115377119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I wonder if we should allow `LogContext` to be passed to `KafkaException` to avoid a bunch of duplication. Doesn't have to be in this PR, but is it a good idea?",
        "createdAt" : "2018-04-25T04:59:21Z",
        "updatedAt" : "2018-04-25T22:24:58Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "642431b9-3275-4760-877b-00f03adeea2c",
        "parentId" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's worth considering for sure. It does seem more awkward since you'd have to add overloads to all the sub-type constructors as well. And many of the exception types are public 😞 . If we can find a nice way to do it, it's a good idea.",
        "createdAt" : "2018-04-25T22:28:50Z",
        "updatedAt" : "2018-04-25T22:28:50Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2777ab489b6bd3fbaf0733fb85eaf8619d43acdf",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +1382,1386 @@        if (prev != null)\n          throw new KafkaException(s\"Trying to roll a new log segment for topic partition $topicPartition with \" +\n            s\"start offset $newOffset while it already exists.\")\n        // We need to update the segment base offset and append position data of the metadata when log rolls.\n        // The next offset should not change."
  },
  {
    "id" : "000ebd10-0ca5-483f-8f47-87146d732626",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126872964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe we can have a separate `splitAndReplace` method so that we can call this from `loadSegments` without needing to mutate the internal state. Gives an excuse to reduce the size of this big function as well (even if just a little bit).",
        "createdAt" : "2018-06-06T21:24:36Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f7c5caec-6013-45df-b48f-f154ed03c06f",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I don't fully understand the intent here. Do you want to avoid the call to `replaceSegments` when `splitSegmentOnOffsetOverflow` is called from `loadSegments`?",
        "createdAt" : "2018-06-07T00:35:10Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "8b0e976f-138c-4427-a789-3785803f2f3e",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, that's right. The expectation in `replaceSegments` is that the segments have already been loaded. It is easy to avoid violating this expectation and makes the code easier to reason about. ",
        "createdAt" : "2018-06-07T17:03:21Z",
        "updatedAt" : "2018-06-07T17:07:52Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 539,
    "diffHunk" : "@@ -1,1 +1944,1948 @@\n    // replace old segment with new ones\n    replaceSegments(newSegments.toList, List(segment), isRecoveredSwapFile = false)\n    newSegments.toList\n  }"
  },
  {
    "id" : "dbbcb00d-97a1-4e47-830b-6805dfe19e1f",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126881769",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Any reason not to make this a case class?",
        "createdAt" : "2018-06-06T21:24:59Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9fbee259-6a47-412f-8aa9-c120e8d4cf1f",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "findbugs does not seem to like it. Detailed response here: https://github.com/apache/kafka/pull/4975#discussion_r187756280",
        "createdAt" : "2018-06-07T00:36:54Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "217d7d7f-6a61-46e1-ba29-808e6beaf1e0",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, I see. I think this is kind of an indirect way of telling us that what we're doing is a little odd. I'd probably suggest pulling `copyRecordsToSegment` up--perhaps even moving it into `LogSegment`--but we can leave that for a follow-up.",
        "createdAt" : "2018-06-07T17:28:40Z",
        "updatedAt" : "2018-06-07T17:28:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 448,
    "diffHunk" : "@@ -1,1 +1853,1857 @@    var readBuffer = ByteBuffer.allocate(1024 * 1024)\n\n    class CopyResult(val bytesRead: Int, val overflowOffset: Option[Long])\n\n    // Helper method to copy `records` into `segment`. Makes sure records being appended do not result in offset overflow."
  },
  {
    "id" : "5fba58e6-2ade-4edc-bd5c-029dbb195e8d",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-127408782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b495922-fadf-436f-856e-4c02d97d388a",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "It seems we use the traversal of sortedNewSegments in descending order below.\r\n```\r\nnewSegments.sortBy(- _.baseOffset)\r\n```\r\nIf we use the above to sort, the code would be more readable.\r\n\r\nsortedNewSegments.tail should be used on line 1778",
        "createdAt" : "2018-06-10T20:19:56Z",
        "updatedAt" : "2018-06-10T20:20:49Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 359,
    "diffHunk" : "@@ -1,1 +1762,1766 @@   */\n  private[log] def replaceSegments(newSegments: Seq[LogSegment], oldSegments: Seq[LogSegment], isRecoveredSwapFile: Boolean = false) {\n    val sortedNewSegments = newSegments.sortBy(_.baseOffset)\n    val sortedOldSegments = oldSegments.sortBy(_.baseOffset)\n"
  },
  {
    "id" : "614ab52f-32e1-4c0f-bf0e-eca2ac021af1",
    "prId" : 5133,
    "prUrl" : "https://github.com/apache/kafka/pull/5133#pullrequestreview-128601412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa002b9f-675b-4776-b974-6d6b1d1780d4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Need to update the parameter list above",
        "createdAt" : "2018-06-13T23:38:48Z",
        "updatedAt" : "2018-06-14T05:35:55Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "486f83e3422d512e3660225606caeb067a58b41f",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +87,91 @@                         validBytes: Int,\n                         offsetsMonotonic: Boolean,\n                         lastOffsetOfFirstBatch: Long) {\n  /**\n   * Get the first offset if it exists, else get the last offset of the first batch"
  },
  {
    "id" : "bec53497-6112-4f18-a32a-2726ab00cef8",
    "prId" : 5169,
    "prUrl" : "https://github.com/apache/kafka/pull/5169#pullrequestreview-127652268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I am not sure if we want to throw an exception in this case. Could we have an overflown segment (may be produced by the log cleaner) such that messages appear only after the overflow offset?",
        "createdAt" : "2018-06-08T22:29:38Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "dead1cc7-93aa-467d-9a57-470ea47967f3",
        "parentId" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. That's an interesting point. It might be possible for a compacted topic since the base offset is only determined by the input segments. ",
        "createdAt" : "2018-06-09T05:12:28Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "22893160-cd61-4128-9303-e66304e93fa9",
        "parentId" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "On second thought, I think this assertion is fine. We should always have at least one batch that we can append since we are starting the new segment from the base offset of the first batch. What needs to change for this case is the assertion below that `newSegments` is greater than one. I will fix this and add a test case.",
        "createdAt" : "2018-06-11T16:52:45Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "c71b1e5937a33e525f483cd5d2edaa925fa68ccb",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +1878,1882 @@        val bytesAppended = newSegment.appendFromFile(sourceRecords, position)\n        if (bytesAppended == 0)\n          throw new IllegalStateException(s\"Failed to append records from position $position in $segment\")\n\n        position += bytesAppended"
  },
  {
    "id" : "382ef6b2-1084-4b1f-a433-11358d8df7ff",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-180933235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I wonder if `newOffset <= activeSegment.baseOffset` would be a stricter validation?",
        "createdAt" : "2018-12-02T00:40:47Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "43e8beea-a38e-4838-bb85-47dbafea5eff",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Since we are already validating `newOffset == activeSegment.baseOffset`, `newOffset < activeSegment.baseOffset` means that logEndOffset is lower than base offset of the active segment (based on `val newOffset = math.max(expectedNextOffset, logEndOffset)`). We probably validate this in other places, but could be useful to validate here as well. In that case, I would still validate `if (segments.containsKey(newOffset)) {` separately, and `else if `newOffset < activeSegment.baseOffset` throw another KafkaException. What do you think?",
        "createdAt" : "2018-12-03T17:22:17Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      },
      {
        "id" : "77553851-7e25-446f-8ea7-ba4500f449db",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Actually I think we don't need that additional validation. That would cause \"out of order offsets\" earlier when we try to do an append.",
        "createdAt" : "2018-12-03T18:33:24Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      },
      {
        "id" : "bf1686d0-347d-4629-8af3-6c0865006948",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "On a third thought, does not hurt to check that we are not rolling to offset < base offset of the active segment, even though a segment with this base offset does not exist.",
        "createdAt" : "2018-12-03T19:14:38Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1571,1575 @@        val logFile = Log.logFile(dir, newOffset)\n\n        if (segments.containsKey(newOffset)) {\n          // segment with the same base offset already exists and loaded\n          if (activeSegment.baseOffset == newOffset && activeSegment.size == 0) {"
  },
  {
    "id" : "cfa90ac8-9e57-47a9-9dfb-2318936cfdbf",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-181034261",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7947c3d9-da4c-4a3c-b53f-8ccb79fcecb2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: missing a space before `exists`",
        "createdAt" : "2018-12-03T21:59:57Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b4de25a9-2321-4434-882a-97c3eea54758",
        "parentId" : "7947c3d9-da4c-4a3c-b53f-8ccb79fcecb2",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "added space after \"already\"",
        "createdAt" : "2018-12-03T23:42:03Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1578,1582 @@            warn(s\"Trying to roll a new log segment with start offset $newOffset \" +\n                 s\"=max(provided offset = $expectedNextOffset, LEO = $logEndOffset) while it already \" +\n                 s\"exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},\" +\n                 s\" size of offset index: ${activeSegment.offsetIndex.entries}.\")\n            deleteSegment(activeSegment)"
  },
  {
    "id" : "0a07c6cb-0777-4f4a-87b5-3d066a636864",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-181034798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29d76210-488b-44dc-9291-c2429e8dbc3f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess another option would be to resize the index files. Did you feel this would be more reliable?",
        "createdAt" : "2018-12-03T23:25:04Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a9a7a8a6-6a77-4908-a181-e2b0c15953eb",
        "parentId" : "29d76210-488b-44dc-9291-c2429e8dbc3f",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "I am concerned about resizing index, because it does not seem like the issue happens because we explicitly resize the index. Seems like something happens to mmaped file, which we don't understand yet. So, it seems more reliable to remove and recreate the segment.",
        "createdAt" : "2018-12-03T23:44:11Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1580,1584 @@                 s\"exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},\" +\n                 s\" size of offset index: ${activeSegment.offsetIndex.entries}.\")\n            deleteSegment(activeSegment)\n          } else {\n            throw new KafkaException(s\"Trying to roll a new log segment for topic partition $topicPartition with start offset $newOffset\" +"
  },
  {
    "id" : "e52d2abc-9126-4bbf-bab6-5e45bd58c943",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-236337100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "parentId" : null,
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "One Optimization we can have here is to lock only when we need to populate firsBatchTimestamp, for example:\r\nin LogSegment.scala , we can add:\r\ndef isFirstBatchTimestampLoaded : Boolean = {\r\n    rollingBasedTimestamp match {\r\n      case Some(t) => true\r\n      case _ => false\r\n    }\r\n  }\r\n\r\nIn log.scala, we can have:\r\n\r\n@threadsafe\r\n  private[log] def getFirstBatchTimestampForSegments2(segments: Iterable[LogSegment]): Iterable[Long] = {\r\n\r\n    val needLoadTimestampSegments = segments.filter {\r\n      segment =>\r\n        !segment.isFirstBatchTimestampLoaded\r\n    }\r\n\r\n    if (needLoadTimestampSegments.nonEmpty) {\r\n      lock synchronized {\r\n        needLoadTimestampSegments.foreach {\r\n          segment =>\r\n            segment.loadFirstBatchTimestamp()\r\n        }\r\n      }\r\n    }\r\n\r\n    segments.map {\r\n      segment =>\r\n        segment.getFirstBatchTimestampWithoutLoading()\r\n    }\r\n  }\r\n\r\nThis optimization will reduce lock contention on the log.  However, each time we call log.logSegments,  we are acquiring the locks.  log.logSegments has been called multiple times (e.g, cleanableOffsets, LogToClean, etc.) during the log compaction . \r\n\r\nThe choice here is :  reduced potential lock contention versus simplified code. ",
        "createdAt" : "2019-01-29T02:03:24Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "002cf52f-5f28-4fd8-b422-e15cb14a808e",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "This isn't exactly introduced by your patch, but I'm not very clear on the thread-safety semantics offered by the log lock. E.g., in `getEstimatedEarliestTime` the first line grabs log segments using the underlying log lock but then relinquishes it.\r\n\r\nThat said, do you even need to lock? What is the worst that can happen? e.g., this could result in a failed attempt to read the earliest batch timestamp of a segment that may have been deleted after listing all the segments but I don't think this is actually an issue.",
        "createdAt" : "2019-02-05T06:21:48Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "d006bc93-d06c-4c5d-85ad-5c5a8cb7bb98",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "This is really good point.  I don't see lock is hold when logcleaner performs the actual compaction IOs : cleanSegments -> cleanInto.   Maybe we can skip the lock. Let me double check.",
        "createdAt" : "2019-02-05T18:47:09Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "a32f87a8-f237-40dc-b1af-eebf9619683f",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "can you add a comment here explaining why this is safe? per your explanation offline, segments cannot be removed outside of the cleaner (for log compacted topics).",
        "createdAt" : "2019-05-10T23:04:50Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2027,2031 @@    * to ensure no other logcleaner threads and retention thread can work on the same segment.\n    */\n  private[log] def getFirstBatchTimestampForSegments(segments: Iterable[LogSegment]): Iterable[Long] = {\n    segments.map {\n      segment =>"
  },
  {
    "id" : "5a66ed57-e891-4fb6-b1fc-0529462ef435",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-200724441",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "156cbc73-b4cb-4107-8d61-a3b87a036270",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "nice spotting of the duplication here 👍 ",
        "createdAt" : "2019-02-06T18:17:11Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +984,988 @@  def latestEpoch: Option[Int] = leaderEpochCache.flatMap(_.latestEpoch)\n\n  def endOffsetForEpoch(leaderEpoch: Int): Option[OffsetAndEpoch] = {\n    leaderEpochCache.flatMap { cache =>\n      val (foundEpoch, foundOffset) = cache.endOffsetFor(leaderEpoch)"
  },
  {
    "id" : "ca6e06fe-a0f0-4d32-8682-32588a21295f",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-201505566",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Nit: there is a possibility of creating and then immediately deleting the same file. If `LeaderEpochFile` had a `getPath(dir: Dir): Path` method which returns the file's path then with `Files.deleteIfExists(LeaderEpochFile.getPath(dir))` we could avoid that.\r\nEven more, `LeaderEpochFile` object I think could be merged into `LeaderEpochCheckpointFile`'s companion object as `LeaderEpochFile` used only here.",
        "createdAt" : "2019-02-07T17:31:04Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "678e5043-8c26-40e0-980b-e0e0a09b35f8",
        "parentId" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, `LeaderEpochFile` seems unnecessary. But can you clarify how the file would be created and then deleted? I think the call to `newFile` does not create the file itself. ",
        "createdAt" : "2019-02-07T21:42:10Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d077e147-9af3-4834-9ff4-d1328fa826df",
        "parentId" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Havin a second look it seems I overlooked it regarding `newFile`, so I take back that comment :)",
        "createdAt" : "2019-02-08T10:08:02Z",
        "updatedAt" : "2019-02-08T10:08:02Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +369,373 @@        warn(s\"Deleting non-empty leader epoch cache due to incompatible message format $recordVersion\")\n\n      Files.deleteIfExists(leaderEpochFile.toPath)\n      leaderEpochCache = None\n    } else {"
  },
  {
    "id" : "71dca4f4-2638-414c-afc9-56f76e4d7feb",
    "prId" : 6298,
    "prUrl" : "https://github.com/apache/kafka/pull/6298#pullrequestreview-206963671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22cf0607-ad66-4592-9da4-817e48205134",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "@hachikuji \r\nI had to add this because we would still rebuild the cache in the case of an unclean shutdown and restart with `log.message.format=0.10`\r\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogSegment.scala#L378\r\n\r\nNote that I removed the last test case in `testOldMessageFormatDeletesEpochCacheIfUnsupported()`. If we re-initialize the log with the new message format and unclean shutdown, it **will** rebuild it from the batches. This further exemplifies that we don't properly support message downgrades I think.\r\n\r\nCan you think of any implications here? I believe we might be fine",
        "createdAt" : "2019-02-22T11:44:42Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "bbe4dfa5-13d8-41cf-ac3a-93b4d0bdf1e1",
        "parentId" : "22cf0607-ad66-4592-9da4-817e48205134",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it seems ok. We can open a separate JIRA for the problem you raised since it affects trunk as well.",
        "createdAt" : "2019-02-22T18:18:34Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "043c83a9-7d08-4c5d-8974-7eb5859011ca",
        "parentId" : "22cf0607-ad66-4592-9da4-817e48205134",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Yep. Opened https://issues.apache.org/jira/browse/KAFKA-7984",
        "createdAt" : "2019-02-22T18:29:06Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "aaa4a352963294db81c011d5baf1afb101792595",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +559,563 @@        val truncatedBytes =\n          try {\n            recoverSegment(segment, if (supportsLeaderEpoch) Some(_leaderEpochCache) else None)\n          } catch {\n            case _: InvalidOffsetException =>"
  },
  {
    "id" : "962ed4df-bf5b-4224-b149-5cdf1bdfc6dd",
    "prId" : 6652,
    "prUrl" : "https://github.com/apache/kafka/pull/6652#pullrequestreview-233173647",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2ec3975-3ccc-49d2-9a84-fde80fdc7dde",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Here we are trusting the consistency of the log start offset and the segments in the log. If a segment is corrupt and we have to truncate some data, could the log end offset be reset to an offset that is earlier than the log start offset?",
        "createdAt" : "2019-04-30T18:24:52Z",
        "updatedAt" : "2019-05-03T16:22:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "fa5b4cd7-03a8-4184-ad7a-2141d6d2c5e2",
        "parentId" : "c2ec3975-3ccc-49d2-9a84-fde80fdc7dde",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Good point. I made some changes to handle this case as well.",
        "createdAt" : "2019-05-02T18:39:48Z",
        "updatedAt" : "2019-05-03T16:22:53Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd0147f53eb4a7af432a773c222a16a4837a409c",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +578,582 @@      // reset the index size of the currently active log segment to allow more entries\n      activeSegment.resizeIndexes(config.maxIndexSize)\n      nextOffset\n    } else {\n      0"
  },
  {
    "id" : "6a0c5dc8-f460-4e5e-8520-4ce1fcc31665",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-257575024",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. I think we might need to do the lookup while holding the lock. Otherwise we may overwrite a different value than we are expecting. Same for `lso` below.\r\n\r\nAlso, we have a related method `maybeFetchHighWatermarkOffsetMetadata`. Perhaps we should try to consolidate?",
        "createdAt" : "2019-06-20T22:46:03Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "dff0e648-80c3-4780-87d3-08e24fbd55e1",
        "parentId" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "`maybeFetchHighWatermarkOffsetMetadata` is used when we initialized a replica as a leader. It loads the HW metadata and falls back to LSO if it's out of range. Seems like enough of a separate usage to keep separate maybe?\r\n\r\nBut now that you mention it, should `maybeFetchHighWatermarkOffsetMetadata` be synchronized as well? It probably only gets called by a single thread (Partition#makeLeader), but that doesn't guard against future usages.",
        "createdAt" : "2019-06-21T12:37:55Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "be8b0fd8-625b-4b1d-a3d3-fb4159de7830",
        "parentId" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess it depends on whether the fallback logic makes sense in this context. I guess it is reasonable to raise an out of range error here since we expect the high watermark to exist in the log. \r\n\r\nAgreed that `maybeFetchHighWatermarkOffsetMetadata` should be locked. Possibly we skipped it because this was only used in initialization. I don't see the harm adding it now.",
        "createdAt" : "2019-06-24T17:35:42Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a9e0812b-2193-4b92-8fdf-c505be6feb18",
        "parentId" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Were we going to add the locking to `initializeHighWatermarkOffsetMetadata`?",
        "createdAt" : "2019-07-03T15:49:58Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +370,374 @@      lock.synchronized {\n        val fullOffset = convertToOffsetMetadataOrThrow(_highWatermarkMetadata.messageOffset)\n        _highWatermarkMetadata = fullOffset\n        highWatermark = _highWatermarkMetadata\n      }"
  },
  {
    "id" : "eb677eba-05cd-4831-940e-ec59db2339ed",
    "prId" : 6841,
    "prUrl" : "https://github.com/apache/kafka/pull/6841#pullrequestreview-247308445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75e24aec-768d-4117-aae0-95ddf9abacbd",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : ":+1: ",
        "createdAt" : "2019-06-07T21:39:15Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6524debfbc275fe7f58c41b87c3daeb3f4c0aa6",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +1981,1985 @@    logString.append(s\", logEndOffset=$logEndOffset\")\n    logString.append(\")\")\n    logString.toString\n  }\n"
  },
  {
    "id" : "76b51a58-0114-4686-a0dc-7bc7d1b3f4b9",
    "prId" : 6841,
    "prUrl" : "https://github.com/apache/kafka/pull/6841#pullrequestreview-249513836",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97642443-5e44-49f1-9366-bca19ae5738c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think the intent of `LogOffsetSnapshot` is to ensure we get a consistent view of these offsets (e.g. to ensure that the high watermark must be less than or equal to the log end offset). As far as I can tell, the only lock that is held when accessing this method is the read-write lock `leaderAndIsrUpdateLock` in `Partition.fetchOffsetSnapshot`. However, we are only accessing the read side of the lock, so it seems these offsets can still be modified by other threads. I think this was already broken, so we can probably try to address it in a follow-up.",
        "createdAt" : "2019-06-11T23:43:31Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0e4358ea-fe89-49f5-b877-6d2b95c99374",
        "parentId" : "97642443-5e44-49f1-9366-bca19ae5738c",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Actually I was slightly mistaken about this. The main purpose of `fetchOffsetSnapshot` was to protect the epoch check when accessing these offsets. I think we never had any strict guarantee that they were consistent with each other. It may still be worthwhile following up on this and thinking through the implications.",
        "createdAt" : "2019-06-11T23:55:31Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "fded3218-071b-43e9-ad60-4e745fc93b8e",
        "parentId" : "97642443-5e44-49f1-9366-bca19ae5738c",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Ok. Leaving it out of this PR.",
        "createdAt" : "2019-06-13T18:00:56Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6524debfbc275fe7f58c41b87c3daeb3f4c0aa6",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +358,362 @@  def lastStableOffsetLag: Long = highWatermark - lastStableOffset\n\n  def offsetSnapshot: LogOffsetSnapshot = {\n    LogOffsetSnapshot(\n      logStartOffset = logStartOffset,"
  },
  {
    "id" : "4afdc2a1-b65e-456b-bd3a-785c87452db1",
    "prId" : 6841,
    "prUrl" : "https://github.com/apache/kafka/pull/6841#pullrequestreview-250051545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e8f56a8-f342-4399-a717-6c0f8ae73422",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Since we added high watermark and LSO, perhaps we may as well add the log start and end offsets as well.",
        "createdAt" : "2019-06-14T07:15:37Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "54480fad-8087-4a03-8438-7be2ec59209e",
        "parentId" : "7e8f56a8-f342-4399-a717-6c0f8ae73422",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Done.",
        "createdAt" : "2019-06-14T18:13:59Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6524debfbc275fe7f58c41b87c3daeb3f4c0aa6",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +1976,1980 @@    logString.append(s\", topic=${topicPartition.topic}\")\n    logString.append(s\", partition=${topicPartition.partition}\")\n    logString.append(s\", highWatermark=$highWatermarkMetadata\")\n    logString.append(s\", lastStableOffset=$lastStableOffsetMetadata\")\n    logString.append(s\", logStartOffset=$logStartOffset\")"
  },
  {
    "id" : "c605b77b-b215-47bf-bb26-e43b43625bb9",
    "prId" : 6847,
    "prUrl" : "https://github.com/apache/kafka/pull/6847#pullrequestreview-249670004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "you should also wait until the executor has finished running, with something like `ExecutorService#awaitTermination`",
        "createdAt" : "2019-06-04T21:09:50Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "3eb84865-9704-40ff-894f-446eaae73992",
        "parentId" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I don't think you can do that. The executor is shared with other components.",
        "createdAt" : "2019-06-04T21:10:54Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "e658733b-8693-44a8-8429-079b30718dfc",
        "parentId" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "So should I keep as is or change it?",
        "createdAt" : "2019-06-04T21:25:09Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      },
      {
        "id" : "c9b03199-3047-4175-bf49-23ed2081465f",
        "parentId" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "@ijuma: Good point.  We don't want to wait for the executor to terminate here.",
        "createdAt" : "2019-06-14T00:57:35Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a0be6532fabbe65ac4204596bfc5d353eb78b34",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +754,758 @@    lock synchronized {\n      checkIfMemoryMappedBufferClosed()\n      producerExpireCheck.cancel(true)\n      maybeHandleIOException(s\"Error while renaming dir for $topicPartition in dir ${dir.getParent}\") {\n        // We take a snapshot at the last written offset to hopefully avoid the need to scan the log"
  },
  {
    "id" : "d8e9496d-e916-4e72-a809-937af4070497",
    "prId" : 6943,
    "prUrl" : "https://github.com/apache/kafka/pull/6943#pullrequestreview-256044897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Can we move the comment here?\r\n\r\n```\r\n    // we will cache the log offset metadata if it corresponds to the starting offset of\t\r\n    // the last transaction that was started. This is optimized for leader appends where it\t\r\n    // is only possible to have one transaction started for each log append, and the log\t\r\n    // offset metadata will always match in that case since no data from other producers\t\r\n    // is mixed into the append\r\n```",
        "createdAt" : "2019-06-28T23:16:29Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f2cb5c2d-c33b-45cb-9700-4388f58a653f",
        "parentId" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Does the comment make sense any longer? Now we are caching the log offset metadata on both leader and follower using the same logic.",
        "createdAt" : "2019-06-28T23:50:39Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "6677053e-f2fe-450f-907c-3a27e2d073de",
        "parentId" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We should reword it for sure, but I think it's still worthy mentioning why we are updating producer state here.",
        "createdAt" : "2019-06-28T23:52:54Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "36581b97-87b0-44b0-b435-116d2b11e398",
        "parentId" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ack. I will reword",
        "createdAt" : "2019-06-29T17:34:05Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "80ebb55435216743bc82933f59d975b11a1989ac",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +1171,1175 @@          None\n\n        val maybeCompletedTxn = updateProducers(batch,\n          updatedProducers,\n          firstOffsetMetadata = firstOffsetMetadata,"
  },
  {
    "id" : "aa6c7b08-5688-4d38-908d-98078f23e2b6",
    "prId" : 7055,
    "prUrl" : "https://github.com/apache/kafka/pull/7055#pullrequestreview-260224917",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e49e5c7-92b0-4bb9-9db2-b86881768a4d",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Should we return the long value only or the whole `LogOffsetMetadata`?",
        "createdAt" : "2019-07-10T15:33:41Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "d675f12c-ddd2-4141-8436-96eed4244d5a",
        "parentId" : "5e49e5c7-92b0-4bb9-9db2-b86881768a4d",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We could, but I was trying to avoid exposing `LogOffsetMetadata` unless we were willing to materialize it.",
        "createdAt" : "2019-07-10T16:48:06Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd6409b01b62591b8e3d00b5b4c00425a8d0d108",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +325,329 @@      hw\n    updateHighWatermarkMetadata(LogOffsetMetadata(newHighWatermark))\n    newHighWatermark\n  }\n"
  },
  {
    "id" : "5528dee8-4b16-4557-898f-207396a671ec",
    "prId" : 7055,
    "prUrl" : "https://github.com/apache/kafka/pull/7055#pullrequestreview-260224665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8153d358-ce87-4076-bcd7-e925fd9c0155",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Should this be named `initializeHighWatermark` maybe?",
        "createdAt" : "2019-07-10T15:43:55Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "3dabeabd-4295-40f5-95b2-59a6ab88c3b9",
        "parentId" : "8153d358-ce87-4076-bcd7-e925fd9c0155",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I called it `update` since it is also used in the fetcher when updating the high watermark after a fetch response.",
        "createdAt" : "2019-07-10T16:47:35Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd6409b01b62591b8e3d00b5b4c00425a8d0d108",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +317,321 @@   * @return the updated high watermark offset\n   */\n  def updateHighWatermark(hw: Long): Long = {\n    val newHighWatermark = if (hw < logStartOffset)\n      logStartOffset"
  },
  {
    "id" : "b6697835-97d7-494b-94b6-562e07e791db",
    "prId" : 7264,
    "prUrl" : "https://github.com/apache/kafka/pull/7264#pullrequestreview-283222235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a31d4d28-65bc-4c4c-a3e7-2d20d291fb0d",
        "parentId" : null,
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Just for my understanding, looks like `floor < from` and if `to < floor`, doesn't it mean that `to < from`? ",
        "createdAt" : "2019-09-03T17:58:20Z",
        "updatedAt" : "2019-09-04T18:06:04Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      },
      {
        "id" : "e36b19cf-7b0a-409d-a187-5971d165c1f9",
        "parentId" : "a31d4d28-65bc-4c4c-a3e7-2d20d291fb0d",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, I think so. That's why I was considering cases where the dirty offset somehow got ahead of the log end offset.",
        "createdAt" : "2019-09-03T19:48:57Z",
        "updatedAt" : "2019-09-04T18:06:04Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9c25faf509ca5759496351a977dff05f49aad9b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2102,2106 @@    lock synchronized {\n      val view = Option(segments.floorKey(from)).map { floor =>\n        if (to < floor)\n          throw new IllegalArgumentException(s\"Invalid log segment range: requested segments from offset $from \" +\n            s\"mapping to segment with base offset $floor, which is greater than limit offset $to\")"
  },
  {
    "id" : "3d92885b-75ff-4c33-9155-4a99f7987a97",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337239604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ab7d84a-9570-45e0-8b24-51b669880969",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Isn't it safer not to have the defaults?",
        "createdAt" : "2019-12-11T13:55:19Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "7299dbd1-4ba1-40b7-94ff-32735e5409dd",
        "parentId" : "1ab7d84a-9570-45e0-8b24-51b669880969",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Probably. I preserved the default behavior from `isFromClient` to avoid having to update the ~250 references. Perhaps we can do this in a separate PR?",
        "createdAt" : "2019-12-30T21:09:21Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +980,984 @@                     leaderEpoch: Int,\n                     origin: AppendOrigin = AppendOrigin.Client,\n                     interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch)\n  }"
  },
  {
    "id" : "578311b6-a3c5-4ae0-942e-3699cba9bef8",
    "prId" : 7695,
    "prUrl" : "https://github.com/apache/kafka/pull/7695#pullrequestreview-318531930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14a49424-6bc4-4d0c-bcb5-b2266a1e86de",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Would it make sense to make this more explicit by the caller? It's clear that a hw that is ahead of the offset needs updating, but less so for the case where it's the same. It seems like we are relying on implicit implementation details that make the code harder to reason about.\r\n\r\nWould it help if the caller (there are only 4) specified if the log was rolled and/or truncated? Or is there risk of this condition happening independently of the caller?",
        "createdAt" : "2019-11-17T18:39:07Z",
        "updatedAt" : "2019-11-17T18:39:07Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "0d21edf7-6339-4832-8c3d-697094fb4de7",
        "parentId" : "14a49424-6bc4-4d0c-bcb5-b2266a1e86de",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a fair point. I was shooting for a minimal diff here to make merging easier, but there's room for improvement in making expectations more explicit. I'll think about this and submit a follow-up if I can come up with a nicer way to handle this.",
        "createdAt" : "2019-11-18T18:17:45Z",
        "updatedAt" : "2019-11-18T18:17:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "51d62e0fddd0a497bc22c633a635f0926d14a7f2",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +747,751 @@    // Update the high watermark in case it has gotten ahead of the log end offset following a truncation\n    // or if a new segment has been rolled and the offset metadata needs to be updated.\n    if (highWatermark >= messageOffset) {\n      updateHighWatermarkMetadata(nextOffsetMetadata)\n    }"
  },
  {
    "id" : "fccdd01e-7f8f-4012-8bc4-488c314c72a6",
    "prId" : 7773,
    "prUrl" : "https://github.com/apache/kafka/pull/7773#pullrequestreview-326548438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a little surprising to me that we don't _always_ call `close()` on the `Log` object. Perhaps we can add a few comments which emphasize that the log is considered closed after deletion and no further cleanup is needed. Even better would be if we could consolidate these two paths somehow so that we do not have this kind of bug in the future.",
        "createdAt" : "2019-12-03T22:13:47Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0ce8ae3e-298e-4f27-899d-6afb5c43f695",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Maybe we can do the simple thing in this PR so that we can backport.",
        "createdAt" : "2019-12-03T22:16:28Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "59e122f1-56d7-4ef8-ba55-1bc4fc0b501c",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm happy with that if improvements are not straightforward. Just whenever we punt these tech debts, they tend to never get done.",
        "createdAt" : "2019-12-03T22:59:16Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "3c417614-a0ca-47dc-b802-ed96c32710c9",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Done.",
        "createdAt" : "2019-12-03T23:55:11Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      },
      {
        "id" : "0e689431-c24b-46ad-a9bd-b2f8c78fe94f",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "As discussed, limited this PR to only leak fix.",
        "createdAt" : "2019-12-04T01:07:20Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0912969cf700a1307193f114567ed96a2dc882a4",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2006,2010 @@        checkIfMemoryMappedBufferClosed()\n        removeLogMetrics()\n        producerExpireCheck.cancel(true)\n        removeAndDeleteSegments(logSegments, asyncDelete = false)\n        leaderEpochCache.foreach(_.clear())"
  },
  {
    "id" : "418d78fe-e31a-42ef-a4cc-47c3c339f23a",
    "prId" : 8037,
    "prUrl" : "https://github.com/apache/kafka/pull/8037#pullrequestreview-353167289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7eb53319-0e52-49a0-b8fc-d0ec84463112",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Is there a case that recoveryPoint gets smaller than logStartOffset? If yes, is it ok to just move the recoveryPoint without flush? If not, is it worthy to throw exception or log this weird case?",
        "createdAt" : "2020-02-04T08:10:07Z",
        "updatedAt" : "2020-02-04T08:10:11Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "aeb2197e-afcc-4681-ae21-734294f3cc13",
        "parentId" : "7eb53319-0e52-49a0-b8fc-d0ec84463112",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Is a recovery point lower than the log start offset useful? All data below the log start offset is subject to deletion.",
        "createdAt" : "2020-02-04T17:23:36Z",
        "updatedAt" : "2020-02-04T17:23:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "94d5d772-8138-4f69-84bb-5895c8e641fd",
        "parentId" : "7eb53319-0e52-49a0-b8fc-d0ec84463112",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "U are right :)",
        "createdAt" : "2020-02-04T17:33:47Z",
        "updatedAt" : "2020-02-04T17:33:48Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c5b977813084661c071ad823e3e7e5555c0ca5a",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +743,747 @@    }\n\n    if (this.recoveryPoint < offset) {\n      this.recoveryPoint = offset\n    }"
  },
  {
    "id" : "bea72e4e-a256-4388-a0c1-e9ce9c1f26e6",
    "prId" : 8037,
    "prUrl" : "https://github.com/apache/kafka/pull/8037#pullrequestreview-353217061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f92e968-8ef0-4e6f-87c3-cb8662a5ff10",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Previous behavior is ```recoveryPoint = math.min(newOffset, recoveryPoint))``` but this patch changes it to\r\n```\r\nif (this.recoveryPoint < offset) {\r\n  this.recoveryPoint = offset\r\n}\r\n```\r\nwhich is equal to ```recoveryPoint = math.max(newOffset, recoveryPoint))```. Is it a bug?",
        "createdAt" : "2020-02-04T17:43:20Z",
        "updatedAt" : "2020-02-04T17:43:24Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "62ca578d-aa8e-45e3-a177-503d47411b6d",
        "parentId" : "5f92e968-8ef0-4e6f-87c3-cb8662a5ff10",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I have also updated `updateLogEndOffset` to set the recovery point. In `truncateFully` where we delete all segments and set the log start offset to be equal to the log end offset, this ensures recovery point is also set consistently.",
        "createdAt" : "2020-02-04T18:29:30Z",
        "updatedAt" : "2020-02-04T18:29:31Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "3b0d9df8-5f48-40bd-b192-0bd9f5139841",
        "parentId" : "5f92e968-8ef0-4e6f-87c3-cb8662a5ff10",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Thanks for the explanation! ",
        "createdAt" : "2020-02-04T18:51:03Z",
        "updatedAt" : "2020-02-04T18:51:03Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c5b977813084661c071ad823e3e7e5555c0ca5a",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +2097,2101 @@        producerStateManager.updateMapEndOffset(newOffset)\n        maybeIncrementFirstUnstableOffset()\n        updateLogStartOffset(newOffset)\n      }\n    }"
  }
]