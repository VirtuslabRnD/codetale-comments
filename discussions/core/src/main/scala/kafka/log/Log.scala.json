[
  {
    "id" : "07833fb6-b522-4d1c-a097-f603b4ae8025",
    "prId" : 4907,
    "prUrl" : "https://github.com/apache/kafka/pull/4907#pullrequestreview-115377119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I wonder if we should allow `LogContext` to be passed to `KafkaException` to avoid a bunch of duplication. Doesn't have to be in this PR, but is it a good idea?",
        "createdAt" : "2018-04-25T04:59:21Z",
        "updatedAt" : "2018-04-25T22:24:58Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "642431b9-3275-4760-877b-00f03adeea2c",
        "parentId" : "9c8308ee-297c-46da-b4bd-0454b4e0a490",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's worth considering for sure. It does seem more awkward since you'd have to add overloads to all the sub-type constructors as well. And many of the exception types are public 😞 . If we can find a nice way to do it, it's a good idea.",
        "createdAt" : "2018-04-25T22:28:50Z",
        "updatedAt" : "2018-04-25T22:28:50Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2777ab489b6bd3fbaf0733fb85eaf8619d43acdf",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +1382,1386 @@        if (prev != null)\n          throw new KafkaException(s\"Trying to roll a new log segment for topic partition $topicPartition with \" +\n            s\"start offset $newOffset while it already exists.\")\n        // We need to update the segment base offset and append position data of the metadata when log rolls.\n        // The next offset should not change."
  },
  {
    "id" : "000ebd10-0ca5-483f-8f47-87146d732626",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126872964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe we can have a separate `splitAndReplace` method so that we can call this from `loadSegments` without needing to mutate the internal state. Gives an excuse to reduce the size of this big function as well (even if just a little bit).",
        "createdAt" : "2018-06-06T21:24:36Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f7c5caec-6013-45df-b48f-f154ed03c06f",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I don't fully understand the intent here. Do you want to avoid the call to `replaceSegments` when `splitSegmentOnOffsetOverflow` is called from `loadSegments`?",
        "createdAt" : "2018-06-07T00:35:10Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "8b0e976f-138c-4427-a789-3785803f2f3e",
        "parentId" : "4fe2214d-f236-419d-9015-7769dac5d8b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, that's right. The expectation in `replaceSegments` is that the segments have already been loaded. It is easy to avoid violating this expectation and makes the code easier to reason about. ",
        "createdAt" : "2018-06-07T17:03:21Z",
        "updatedAt" : "2018-06-07T17:07:52Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 539,
    "diffHunk" : "@@ -1,1 +1944,1948 @@\n    // replace old segment with new ones\n    replaceSegments(newSegments.toList, List(segment), isRecoveredSwapFile = false)\n    newSegments.toList\n  }"
  },
  {
    "id" : "dbbcb00d-97a1-4e47-830b-6805dfe19e1f",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-126881769",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Any reason not to make this a case class?",
        "createdAt" : "2018-06-06T21:24:59Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "9fbee259-6a47-412f-8aa9-c120e8d4cf1f",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "findbugs does not seem to like it. Detailed response here: https://github.com/apache/kafka/pull/4975#discussion_r187756280",
        "createdAt" : "2018-06-07T00:36:54Z",
        "updatedAt" : "2018-06-07T00:53:17Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "217d7d7f-6a61-46e1-ba29-808e6beaf1e0",
        "parentId" : "76d1d82e-d138-47bf-b925-8f1a853ff6a1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, I see. I think this is kind of an indirect way of telling us that what we're doing is a little odd. I'd probably suggest pulling `copyRecordsToSegment` up--perhaps even moving it into `LogSegment`--but we can leave that for a follow-up.",
        "createdAt" : "2018-06-07T17:28:40Z",
        "updatedAt" : "2018-06-07T17:28:56Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 448,
    "diffHunk" : "@@ -1,1 +1853,1857 @@    var readBuffer = ByteBuffer.allocate(1024 * 1024)\n\n    class CopyResult(val bytesRead: Int, val overflowOffset: Option[Long])\n\n    // Helper method to copy `records` into `segment`. Makes sure records being appended do not result in offset overflow."
  },
  {
    "id" : "5fba58e6-2ade-4edc-bd5c-029dbb195e8d",
    "prId" : 4975,
    "prUrl" : "https://github.com/apache/kafka/pull/4975#pullrequestreview-127408782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b495922-fadf-436f-856e-4c02d97d388a",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "It seems we use the traversal of sortedNewSegments in descending order below.\r\n```\r\nnewSegments.sortBy(- _.baseOffset)\r\n```\r\nIf we use the above to sort, the code would be more readable.\r\n\r\nsortedNewSegments.tail should be used on line 1778",
        "createdAt" : "2018-06-10T20:19:56Z",
        "updatedAt" : "2018-06-10T20:20:49Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      }
    ],
    "commit" : "88121f86f4c732905a44c1bdb21b6383a2e073cf",
    "line" : 359,
    "diffHunk" : "@@ -1,1 +1762,1766 @@   */\n  private[log] def replaceSegments(newSegments: Seq[LogSegment], oldSegments: Seq[LogSegment], isRecoveredSwapFile: Boolean = false) {\n    val sortedNewSegments = newSegments.sortBy(_.baseOffset)\n    val sortedOldSegments = oldSegments.sortBy(_.baseOffset)\n"
  },
  {
    "id" : "614ab52f-32e1-4c0f-bf0e-eca2ac021af1",
    "prId" : 5133,
    "prUrl" : "https://github.com/apache/kafka/pull/5133#pullrequestreview-128601412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa002b9f-675b-4776-b974-6d6b1d1780d4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Need to update the parameter list above",
        "createdAt" : "2018-06-13T23:38:48Z",
        "updatedAt" : "2018-06-14T05:35:55Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "486f83e3422d512e3660225606caeb067a58b41f",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +87,91 @@                         validBytes: Int,\n                         offsetsMonotonic: Boolean,\n                         lastOffsetOfFirstBatch: Long) {\n  /**\n   * Get the first offset if it exists, else get the last offset of the first batch"
  },
  {
    "id" : "bec53497-6112-4f18-a32a-2726ab00cef8",
    "prId" : 5169,
    "prUrl" : "https://github.com/apache/kafka/pull/5169#pullrequestreview-127652268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I am not sure if we want to throw an exception in this case. Could we have an overflown segment (may be produced by the log cleaner) such that messages appear only after the overflow offset?",
        "createdAt" : "2018-06-08T22:29:38Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "dead1cc7-93aa-467d-9a57-470ea47967f3",
        "parentId" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. That's an interesting point. It might be possible for a compacted topic since the base offset is only determined by the input segments. ",
        "createdAt" : "2018-06-09T05:12:28Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "22893160-cd61-4128-9303-e66304e93fa9",
        "parentId" : "fe60d4b8-c631-4897-8a35-2ca0fe6136b2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "On second thought, I think this assertion is fine. We should always have at least one batch that we can append since we are starting the new segment from the base offset of the first batch. What needs to change for this case is the assertion below that `newSegments` is greater than one. I will fix this and add a test case.",
        "createdAt" : "2018-06-11T16:52:45Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "c71b1e5937a33e525f483cd5d2edaa925fa68ccb",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +1878,1882 @@        val bytesAppended = newSegment.appendFromFile(sourceRecords, position)\n        if (bytesAppended == 0)\n          throw new IllegalStateException(s\"Failed to append records from position $position in $segment\")\n\n        position += bytesAppended"
  },
  {
    "id" : "382ef6b2-1084-4b1f-a433-11358d8df7ff",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-180933235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I wonder if `newOffset <= activeSegment.baseOffset` would be a stricter validation?",
        "createdAt" : "2018-12-02T00:40:47Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "43e8beea-a38e-4838-bb85-47dbafea5eff",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Since we are already validating `newOffset == activeSegment.baseOffset`, `newOffset < activeSegment.baseOffset` means that logEndOffset is lower than base offset of the active segment (based on `val newOffset = math.max(expectedNextOffset, logEndOffset)`). We probably validate this in other places, but could be useful to validate here as well. In that case, I would still validate `if (segments.containsKey(newOffset)) {` separately, and `else if `newOffset < activeSegment.baseOffset` throw another KafkaException. What do you think?",
        "createdAt" : "2018-12-03T17:22:17Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      },
      {
        "id" : "77553851-7e25-446f-8ea7-ba4500f449db",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Actually I think we don't need that additional validation. That would cause \"out of order offsets\" earlier when we try to do an append.",
        "createdAt" : "2018-12-03T18:33:24Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      },
      {
        "id" : "bf1686d0-347d-4629-8af3-6c0865006948",
        "parentId" : "8b6639e2-612e-4127-b97d-da89475902e5",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "On a third thought, does not hurt to check that we are not rolling to offset < base offset of the active segment, even though a segment with this base offset does not exist.",
        "createdAt" : "2018-12-03T19:14:38Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1571,1575 @@        val logFile = Log.logFile(dir, newOffset)\n\n        if (segments.containsKey(newOffset)) {\n          // segment with the same base offset already exists and loaded\n          if (activeSegment.baseOffset == newOffset && activeSegment.size == 0) {"
  },
  {
    "id" : "cfa90ac8-9e57-47a9-9dfb-2318936cfdbf",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-181034261",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7947c3d9-da4c-4a3c-b53f-8ccb79fcecb2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: missing a space before `exists`",
        "createdAt" : "2018-12-03T21:59:57Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b4de25a9-2321-4434-882a-97c3eea54758",
        "parentId" : "7947c3d9-da4c-4a3c-b53f-8ccb79fcecb2",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "added space after \"already\"",
        "createdAt" : "2018-12-03T23:42:03Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1578,1582 @@            warn(s\"Trying to roll a new log segment with start offset $newOffset \" +\n                 s\"=max(provided offset = $expectedNextOffset, LEO = $logEndOffset) while it already \" +\n                 s\"exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},\" +\n                 s\" size of offset index: ${activeSegment.offsetIndex.entries}.\")\n            deleteSegment(activeSegment)"
  },
  {
    "id" : "0a07c6cb-0777-4f4a-87b5-3d066a636864",
    "prId" : 5986,
    "prUrl" : "https://github.com/apache/kafka/pull/5986#pullrequestreview-181034798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29d76210-488b-44dc-9291-c2429e8dbc3f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess another option would be to resize the index files. Did you feel this would be more reliable?",
        "createdAt" : "2018-12-03T23:25:04Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a9a7a8a6-6a77-4908-a181-e2b0c15953eb",
        "parentId" : "29d76210-488b-44dc-9291-c2429e8dbc3f",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "I am concerned about resizing index, because it does not seem like the issue happens because we explicitly resize the index. Seems like something happens to mmaped file, which we don't understand yet. So, it seems more reliable to remove and recreate the segment.",
        "createdAt" : "2018-12-03T23:44:11Z",
        "updatedAt" : "2018-12-04T17:22:20Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0cafe042f395068cab7dc3bdd079745960b4a37",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1580,1584 @@                 s\"exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},\" +\n                 s\" size of offset index: ${activeSegment.offsetIndex.entries}.\")\n            deleteSegment(activeSegment)\n          } else {\n            throw new KafkaException(s\"Trying to roll a new log segment for topic partition $topicPartition with start offset $newOffset\" +"
  },
  {
    "id" : "e52d2abc-9126-4bbf-bab6-5e45bd58c943",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-236337100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "parentId" : null,
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "One Optimization we can have here is to lock only when we need to populate firsBatchTimestamp, for example:\r\nin LogSegment.scala , we can add:\r\ndef isFirstBatchTimestampLoaded : Boolean = {\r\n    rollingBasedTimestamp match {\r\n      case Some(t) => true\r\n      case _ => false\r\n    }\r\n  }\r\n\r\nIn log.scala, we can have:\r\n\r\n@threadsafe\r\n  private[log] def getFirstBatchTimestampForSegments2(segments: Iterable[LogSegment]): Iterable[Long] = {\r\n\r\n    val needLoadTimestampSegments = segments.filter {\r\n      segment =>\r\n        !segment.isFirstBatchTimestampLoaded\r\n    }\r\n\r\n    if (needLoadTimestampSegments.nonEmpty) {\r\n      lock synchronized {\r\n        needLoadTimestampSegments.foreach {\r\n          segment =>\r\n            segment.loadFirstBatchTimestamp()\r\n        }\r\n      }\r\n    }\r\n\r\n    segments.map {\r\n      segment =>\r\n        segment.getFirstBatchTimestampWithoutLoading()\r\n    }\r\n  }\r\n\r\nThis optimization will reduce lock contention on the log.  However, each time we call log.logSegments,  we are acquiring the locks.  log.logSegments has been called multiple times (e.g, cleanableOffsets, LogToClean, etc.) during the log compaction . \r\n\r\nThe choice here is :  reduced potential lock contention versus simplified code. ",
        "createdAt" : "2019-01-29T02:03:24Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "002cf52f-5f28-4fd8-b422-e15cb14a808e",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "This isn't exactly introduced by your patch, but I'm not very clear on the thread-safety semantics offered by the log lock. E.g., in `getEstimatedEarliestTime` the first line grabs log segments using the underlying log lock but then relinquishes it.\r\n\r\nThat said, do you even need to lock? What is the worst that can happen? e.g., this could result in a failed attempt to read the earliest batch timestamp of a segment that may have been deleted after listing all the segments but I don't think this is actually an issue.",
        "createdAt" : "2019-02-05T06:21:48Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "d006bc93-d06c-4c5d-85ad-5c5a8cb7bb98",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "This is really good point.  I don't see lock is hold when logcleaner performs the actual compaction IOs : cleanSegments -> cleanInto.   Maybe we can skip the lock. Let me double check.",
        "createdAt" : "2019-02-05T18:47:09Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "a32f87a8-f237-40dc-b1af-eebf9619683f",
        "parentId" : "ac8e2528-370b-41a4-b314-2606ee16801c",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "can you add a comment here explaining why this is safe? per your explanation offline, segments cannot be removed outside of the cleaner (for log compacted topics).",
        "createdAt" : "2019-05-10T23:04:50Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2027,2031 @@    * to ensure no other logcleaner threads and retention thread can work on the same segment.\n    */\n  private[log] def getFirstBatchTimestampForSegments(segments: Iterable[LogSegment]): Iterable[Long] = {\n    segments.map {\n      segment =>"
  },
  {
    "id" : "5a66ed57-e891-4fb6-b1fc-0529462ef435",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-200724441",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "156cbc73-b4cb-4107-8d61-a3b87a036270",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "nice spotting of the duplication here 👍 ",
        "createdAt" : "2019-02-06T18:17:11Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +984,988 @@  def latestEpoch: Option[Int] = leaderEpochCache.flatMap(_.latestEpoch)\n\n  def endOffsetForEpoch(leaderEpoch: Int): Option[OffsetAndEpoch] = {\n    leaderEpochCache.flatMap { cache =>\n      val (foundEpoch, foundOffset) = cache.endOffsetFor(leaderEpoch)"
  },
  {
    "id" : "ca6e06fe-a0f0-4d32-8682-32588a21295f",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-201505566",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Nit: there is a possibility of creating and then immediately deleting the same file. If `LeaderEpochFile` had a `getPath(dir: Dir): Path` method which returns the file's path then with `Files.deleteIfExists(LeaderEpochFile.getPath(dir))` we could avoid that.\r\nEven more, `LeaderEpochFile` object I think could be merged into `LeaderEpochCheckpointFile`'s companion object as `LeaderEpochFile` used only here.",
        "createdAt" : "2019-02-07T17:31:04Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "678e5043-8c26-40e0-980b-e0e0a09b35f8",
        "parentId" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, `LeaderEpochFile` seems unnecessary. But can you clarify how the file would be created and then deleted? I think the call to `newFile` does not create the file itself. ",
        "createdAt" : "2019-02-07T21:42:10Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d077e147-9af3-4834-9ff4-d1328fa826df",
        "parentId" : "13e4021e-6be5-44ed-9bf2-c3d0095757e2",
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "Havin a second look it seems I overlooked it regarding `newFile`, so I take back that comment :)",
        "createdAt" : "2019-02-08T10:08:02Z",
        "updatedAt" : "2019-02-08T10:08:02Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +369,373 @@        warn(s\"Deleting non-empty leader epoch cache due to incompatible message format $recordVersion\")\n\n      Files.deleteIfExists(leaderEpochFile.toPath)\n      leaderEpochCache = None\n    } else {"
  },
  {
    "id" : "71dca4f4-2638-414c-afc9-56f76e4d7feb",
    "prId" : 6298,
    "prUrl" : "https://github.com/apache/kafka/pull/6298#pullrequestreview-206963671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22cf0607-ad66-4592-9da4-817e48205134",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "@hachikuji \r\nI had to add this because we would still rebuild the cache in the case of an unclean shutdown and restart with `log.message.format=0.10`\r\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogSegment.scala#L378\r\n\r\nNote that I removed the last test case in `testOldMessageFormatDeletesEpochCacheIfUnsupported()`. If we re-initialize the log with the new message format and unclean shutdown, it **will** rebuild it from the batches. This further exemplifies that we don't properly support message downgrades I think.\r\n\r\nCan you think of any implications here? I believe we might be fine",
        "createdAt" : "2019-02-22T11:44:42Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "bbe4dfa5-13d8-41cf-ac3a-93b4d0bdf1e1",
        "parentId" : "22cf0607-ad66-4592-9da4-817e48205134",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it seems ok. We can open a separate JIRA for the problem you raised since it affects trunk as well.",
        "createdAt" : "2019-02-22T18:18:34Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "043c83a9-7d08-4c5d-8974-7eb5859011ca",
        "parentId" : "22cf0607-ad66-4592-9da4-817e48205134",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Yep. Opened https://issues.apache.org/jira/browse/KAFKA-7984",
        "createdAt" : "2019-02-22T18:29:06Z",
        "updatedAt" : "2019-02-22T19:17:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "aaa4a352963294db81c011d5baf1afb101792595",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +559,563 @@        val truncatedBytes =\n          try {\n            recoverSegment(segment, if (supportsLeaderEpoch) Some(_leaderEpochCache) else None)\n          } catch {\n            case _: InvalidOffsetException =>"
  },
  {
    "id" : "962ed4df-bf5b-4224-b149-5cdf1bdfc6dd",
    "prId" : 6652,
    "prUrl" : "https://github.com/apache/kafka/pull/6652#pullrequestreview-233173647",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2ec3975-3ccc-49d2-9a84-fde80fdc7dde",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Here we are trusting the consistency of the log start offset and the segments in the log. If a segment is corrupt and we have to truncate some data, could the log end offset be reset to an offset that is earlier than the log start offset?",
        "createdAt" : "2019-04-30T18:24:52Z",
        "updatedAt" : "2019-05-03T16:22:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "fa5b4cd7-03a8-4184-ad7a-2141d6d2c5e2",
        "parentId" : "c2ec3975-3ccc-49d2-9a84-fde80fdc7dde",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Good point. I made some changes to handle this case as well.",
        "createdAt" : "2019-05-02T18:39:48Z",
        "updatedAt" : "2019-05-03T16:22:53Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd0147f53eb4a7af432a773c222a16a4837a409c",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +578,582 @@      // reset the index size of the currently active log segment to allow more entries\n      activeSegment.resizeIndexes(config.maxIndexSize)\n      nextOffset\n    } else {\n      0"
  },
  {
    "id" : "6a0c5dc8-f460-4e5e-8520-4ce1fcc31665",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-257575024",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. I think we might need to do the lookup while holding the lock. Otherwise we may overwrite a different value than we are expecting. Same for `lso` below.\r\n\r\nAlso, we have a related method `maybeFetchHighWatermarkOffsetMetadata`. Perhaps we should try to consolidate?",
        "createdAt" : "2019-06-20T22:46:03Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "dff0e648-80c3-4780-87d3-08e24fbd55e1",
        "parentId" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "`maybeFetchHighWatermarkOffsetMetadata` is used when we initialized a replica as a leader. It loads the HW metadata and falls back to LSO if it's out of range. Seems like enough of a separate usage to keep separate maybe?\r\n\r\nBut now that you mention it, should `maybeFetchHighWatermarkOffsetMetadata` be synchronized as well? It probably only gets called by a single thread (Partition#makeLeader), but that doesn't guard against future usages.",
        "createdAt" : "2019-06-21T12:37:55Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "be8b0fd8-625b-4b1d-a3d3-fb4159de7830",
        "parentId" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess it depends on whether the fallback logic makes sense in this context. I guess it is reasonable to raise an out of range error here since we expect the high watermark to exist in the log. \r\n\r\nAgreed that `maybeFetchHighWatermarkOffsetMetadata` should be locked. Possibly we skipped it because this was only used in initialization. I don't see the harm adding it now.",
        "createdAt" : "2019-06-24T17:35:42Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a9e0812b-2193-4b92-8fdf-c505be6feb18",
        "parentId" : "843c3dce-2c24-4eab-a77e-cb6d9df395ef",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Were we going to add the locking to `initializeHighWatermarkOffsetMetadata`?",
        "createdAt" : "2019-07-03T15:49:58Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +370,374 @@      lock.synchronized {\n        val fullOffset = convertToOffsetMetadataOrThrow(_highWatermarkMetadata.messageOffset)\n        _highWatermarkMetadata = fullOffset\n        highWatermark = _highWatermarkMetadata\n      }"
  },
  {
    "id" : "eb677eba-05cd-4831-940e-ec59db2339ed",
    "prId" : 6841,
    "prUrl" : "https://github.com/apache/kafka/pull/6841#pullrequestreview-247308445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75e24aec-768d-4117-aae0-95ddf9abacbd",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : ":+1: ",
        "createdAt" : "2019-06-07T21:39:15Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6524debfbc275fe7f58c41b87c3daeb3f4c0aa6",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +1981,1985 @@    logString.append(s\", logEndOffset=$logEndOffset\")\n    logString.append(\")\")\n    logString.toString\n  }\n"
  },
  {
    "id" : "76b51a58-0114-4686-a0dc-7bc7d1b3f4b9",
    "prId" : 6841,
    "prUrl" : "https://github.com/apache/kafka/pull/6841#pullrequestreview-249513836",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97642443-5e44-49f1-9366-bca19ae5738c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think the intent of `LogOffsetSnapshot` is to ensure we get a consistent view of these offsets (e.g. to ensure that the high watermark must be less than or equal to the log end offset). As far as I can tell, the only lock that is held when accessing this method is the read-write lock `leaderAndIsrUpdateLock` in `Partition.fetchOffsetSnapshot`. However, we are only accessing the read side of the lock, so it seems these offsets can still be modified by other threads. I think this was already broken, so we can probably try to address it in a follow-up.",
        "createdAt" : "2019-06-11T23:43:31Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0e4358ea-fe89-49f5-b877-6d2b95c99374",
        "parentId" : "97642443-5e44-49f1-9366-bca19ae5738c",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Actually I was slightly mistaken about this. The main purpose of `fetchOffsetSnapshot` was to protect the epoch check when accessing these offsets. I think we never had any strict guarantee that they were consistent with each other. It may still be worthwhile following up on this and thinking through the implications.",
        "createdAt" : "2019-06-11T23:55:31Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "fded3218-071b-43e9-ad60-4e745fc93b8e",
        "parentId" : "97642443-5e44-49f1-9366-bca19ae5738c",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Ok. Leaving it out of this PR.",
        "createdAt" : "2019-06-13T18:00:56Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6524debfbc275fe7f58c41b87c3daeb3f4c0aa6",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +358,362 @@  def lastStableOffsetLag: Long = highWatermark - lastStableOffset\n\n  def offsetSnapshot: LogOffsetSnapshot = {\n    LogOffsetSnapshot(\n      logStartOffset = logStartOffset,"
  },
  {
    "id" : "4afdc2a1-b65e-456b-bd3a-785c87452db1",
    "prId" : 6841,
    "prUrl" : "https://github.com/apache/kafka/pull/6841#pullrequestreview-250051545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e8f56a8-f342-4399-a717-6c0f8ae73422",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Since we added high watermark and LSO, perhaps we may as well add the log start and end offsets as well.",
        "createdAt" : "2019-06-14T07:15:37Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "54480fad-8087-4a03-8438-7be2ec59209e",
        "parentId" : "7e8f56a8-f342-4399-a717-6c0f8ae73422",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Done.",
        "createdAt" : "2019-06-14T18:13:59Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6524debfbc275fe7f58c41b87c3daeb3f4c0aa6",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +1976,1980 @@    logString.append(s\", topic=${topicPartition.topic}\")\n    logString.append(s\", partition=${topicPartition.partition}\")\n    logString.append(s\", highWatermark=$highWatermarkMetadata\")\n    logString.append(s\", lastStableOffset=$lastStableOffsetMetadata\")\n    logString.append(s\", logStartOffset=$logStartOffset\")"
  },
  {
    "id" : "c605b77b-b215-47bf-bb26-e43b43625bb9",
    "prId" : 6847,
    "prUrl" : "https://github.com/apache/kafka/pull/6847#pullrequestreview-249670004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "you should also wait until the executor has finished running, with something like `ExecutorService#awaitTermination`",
        "createdAt" : "2019-06-04T21:09:50Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "3eb84865-9704-40ff-894f-446eaae73992",
        "parentId" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I don't think you can do that. The executor is shared with other components.",
        "createdAt" : "2019-06-04T21:10:54Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "e658733b-8693-44a8-8429-079b30718dfc",
        "parentId" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "So should I keep as is or change it?",
        "createdAt" : "2019-06-04T21:25:09Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      },
      {
        "id" : "c9b03199-3047-4175-bf49-23ed2081465f",
        "parentId" : "9133aebe-30fe-492b-9e3a-886bbbaf26f3",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "@ijuma: Good point.  We don't want to wait for the executor to terminate here.",
        "createdAt" : "2019-06-14T00:57:35Z",
        "updatedAt" : "2019-06-17T16:34:23Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a0be6532fabbe65ac4204596bfc5d353eb78b34",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +754,758 @@    lock synchronized {\n      checkIfMemoryMappedBufferClosed()\n      producerExpireCheck.cancel(true)\n      maybeHandleIOException(s\"Error while renaming dir for $topicPartition in dir ${dir.getParent}\") {\n        // We take a snapshot at the last written offset to hopefully avoid the need to scan the log"
  },
  {
    "id" : "d8e9496d-e916-4e72-a809-937af4070497",
    "prId" : 6943,
    "prUrl" : "https://github.com/apache/kafka/pull/6943#pullrequestreview-256044897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Can we move the comment here?\r\n\r\n```\r\n    // we will cache the log offset metadata if it corresponds to the starting offset of\t\r\n    // the last transaction that was started. This is optimized for leader appends where it\t\r\n    // is only possible to have one transaction started for each log append, and the log\t\r\n    // offset metadata will always match in that case since no data from other producers\t\r\n    // is mixed into the append\r\n```",
        "createdAt" : "2019-06-28T23:16:29Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "f2cb5c2d-c33b-45cb-9700-4388f58a653f",
        "parentId" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Does the comment make sense any longer? Now we are caching the log offset metadata on both leader and follower using the same logic.",
        "createdAt" : "2019-06-28T23:50:39Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "6677053e-f2fe-450f-907c-3a27e2d073de",
        "parentId" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We should reword it for sure, but I think it's still worthy mentioning why we are updating producer state here.",
        "createdAt" : "2019-06-28T23:52:54Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "36581b97-87b0-44b0-b435-116d2b11e398",
        "parentId" : "ebfd4d26-b4bc-4cb6-9c34-df8eb6ab7ef1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ack. I will reword",
        "createdAt" : "2019-06-29T17:34:05Z",
        "updatedAt" : "2019-06-29T17:34:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "80ebb55435216743bc82933f59d975b11a1989ac",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +1171,1175 @@          None\n\n        val maybeCompletedTxn = updateProducers(batch,\n          updatedProducers,\n          firstOffsetMetadata = firstOffsetMetadata,"
  },
  {
    "id" : "aa6c7b08-5688-4d38-908d-98078f23e2b6",
    "prId" : 7055,
    "prUrl" : "https://github.com/apache/kafka/pull/7055#pullrequestreview-260224917",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e49e5c7-92b0-4bb9-9db2-b86881768a4d",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Should we return the long value only or the whole `LogOffsetMetadata`?",
        "createdAt" : "2019-07-10T15:33:41Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "d675f12c-ddd2-4141-8436-96eed4244d5a",
        "parentId" : "5e49e5c7-92b0-4bb9-9db2-b86881768a4d",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We could, but I was trying to avoid exposing `LogOffsetMetadata` unless we were willing to materialize it.",
        "createdAt" : "2019-07-10T16:48:06Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd6409b01b62591b8e3d00b5b4c00425a8d0d108",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +325,329 @@      hw\n    updateHighWatermarkMetadata(LogOffsetMetadata(newHighWatermark))\n    newHighWatermark\n  }\n"
  },
  {
    "id" : "5528dee8-4b16-4557-898f-207396a671ec",
    "prId" : 7055,
    "prUrl" : "https://github.com/apache/kafka/pull/7055#pullrequestreview-260224665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8153d358-ce87-4076-bcd7-e925fd9c0155",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Should this be named `initializeHighWatermark` maybe?",
        "createdAt" : "2019-07-10T15:43:55Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "3dabeabd-4295-40f5-95b2-59a6ab88c3b9",
        "parentId" : "8153d358-ce87-4076-bcd7-e925fd9c0155",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I called it `update` since it is also used in the fetcher when updating the high watermark after a fetch response.",
        "createdAt" : "2019-07-10T16:47:35Z",
        "updatedAt" : "2019-07-10T19:58:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd6409b01b62591b8e3d00b5b4c00425a8d0d108",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +317,321 @@   * @return the updated high watermark offset\n   */\n  def updateHighWatermark(hw: Long): Long = {\n    val newHighWatermark = if (hw < logStartOffset)\n      logStartOffset"
  },
  {
    "id" : "b6697835-97d7-494b-94b6-562e07e791db",
    "prId" : 7264,
    "prUrl" : "https://github.com/apache/kafka/pull/7264#pullrequestreview-283222235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a31d4d28-65bc-4c4c-a3e7-2d20d291fb0d",
        "parentId" : null,
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Just for my understanding, looks like `floor < from` and if `to < floor`, doesn't it mean that `to < from`? ",
        "createdAt" : "2019-09-03T17:58:20Z",
        "updatedAt" : "2019-09-04T18:06:04Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      },
      {
        "id" : "e36b19cf-7b0a-409d-a187-5971d165c1f9",
        "parentId" : "a31d4d28-65bc-4c4c-a3e7-2d20d291fb0d",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, I think so. That's why I was considering cases where the dirty offset somehow got ahead of the log end offset.",
        "createdAt" : "2019-09-03T19:48:57Z",
        "updatedAt" : "2019-09-04T18:06:04Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9c25faf509ca5759496351a977dff05f49aad9b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2102,2106 @@    lock synchronized {\n      val view = Option(segments.floorKey(from)).map { floor =>\n        if (to < floor)\n          throw new IllegalArgumentException(s\"Invalid log segment range: requested segments from offset $from \" +\n            s\"mapping to segment with base offset $floor, which is greater than limit offset $to\")"
  },
  {
    "id" : "3d92885b-75ff-4c33-9155-4a99f7987a97",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337239604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ab7d84a-9570-45e0-8b24-51b669880969",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Isn't it safer not to have the defaults?",
        "createdAt" : "2019-12-11T13:55:19Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "7299dbd1-4ba1-40b7-94ff-32735e5409dd",
        "parentId" : "1ab7d84a-9570-45e0-8b24-51b669880969",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Probably. I preserved the default behavior from `isFromClient` to avoid having to update the ~250 references. Perhaps we can do this in a separate PR?",
        "createdAt" : "2019-12-30T21:09:21Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +980,984 @@                     leaderEpoch: Int,\n                     origin: AppendOrigin = AppendOrigin.Client,\n                     interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch)\n  }"
  },
  {
    "id" : "578311b6-a3c5-4ae0-942e-3699cba9bef8",
    "prId" : 7695,
    "prUrl" : "https://github.com/apache/kafka/pull/7695#pullrequestreview-318531930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14a49424-6bc4-4d0c-bcb5-b2266a1e86de",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Would it make sense to make this more explicit by the caller? It's clear that a hw that is ahead of the offset needs updating, but less so for the case where it's the same. It seems like we are relying on implicit implementation details that make the code harder to reason about.\r\n\r\nWould it help if the caller (there are only 4) specified if the log was rolled and/or truncated? Or is there risk of this condition happening independently of the caller?",
        "createdAt" : "2019-11-17T18:39:07Z",
        "updatedAt" : "2019-11-17T18:39:07Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "0d21edf7-6339-4832-8c3d-697094fb4de7",
        "parentId" : "14a49424-6bc4-4d0c-bcb5-b2266a1e86de",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a fair point. I was shooting for a minimal diff here to make merging easier, but there's room for improvement in making expectations more explicit. I'll think about this and submit a follow-up if I can come up with a nicer way to handle this.",
        "createdAt" : "2019-11-18T18:17:45Z",
        "updatedAt" : "2019-11-18T18:17:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "51d62e0fddd0a497bc22c633a635f0926d14a7f2",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +747,751 @@    // Update the high watermark in case it has gotten ahead of the log end offset following a truncation\n    // or if a new segment has been rolled and the offset metadata needs to be updated.\n    if (highWatermark >= messageOffset) {\n      updateHighWatermarkMetadata(nextOffsetMetadata)\n    }"
  },
  {
    "id" : "fccdd01e-7f8f-4012-8bc4-488c314c72a6",
    "prId" : 7773,
    "prUrl" : "https://github.com/apache/kafka/pull/7773#pullrequestreview-326548438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a little surprising to me that we don't _always_ call `close()` on the `Log` object. Perhaps we can add a few comments which emphasize that the log is considered closed after deletion and no further cleanup is needed. Even better would be if we could consolidate these two paths somehow so that we do not have this kind of bug in the future.",
        "createdAt" : "2019-12-03T22:13:47Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0ce8ae3e-298e-4f27-899d-6afb5c43f695",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Maybe we can do the simple thing in this PR so that we can backport.",
        "createdAt" : "2019-12-03T22:16:28Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "59e122f1-56d7-4ef8-ba55-1bc4fc0b501c",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm happy with that if improvements are not straightforward. Just whenever we punt these tech debts, they tend to never get done.",
        "createdAt" : "2019-12-03T22:59:16Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "3c417614-a0ca-47dc-b802-ed96c32710c9",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Done.",
        "createdAt" : "2019-12-03T23:55:11Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      },
      {
        "id" : "0e689431-c24b-46ad-a9bd-b2f8c78fe94f",
        "parentId" : "99b042d8-ed28-468b-8c1a-a2a3a40aa104",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "As discussed, limited this PR to only leak fix.",
        "createdAt" : "2019-12-04T01:07:20Z",
        "updatedAt" : "2019-12-04T02:05:19Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0912969cf700a1307193f114567ed96a2dc882a4",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2006,2010 @@        checkIfMemoryMappedBufferClosed()\n        removeLogMetrics()\n        producerExpireCheck.cancel(true)\n        removeAndDeleteSegments(logSegments, asyncDelete = false)\n        leaderEpochCache.foreach(_.clear())"
  },
  {
    "id" : "418d78fe-e31a-42ef-a4cc-47c3c339f23a",
    "prId" : 8037,
    "prUrl" : "https://github.com/apache/kafka/pull/8037#pullrequestreview-353167289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7eb53319-0e52-49a0-b8fc-d0ec84463112",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Is there a case that recoveryPoint gets smaller than logStartOffset? If yes, is it ok to just move the recoveryPoint without flush? If not, is it worthy to throw exception or log this weird case?",
        "createdAt" : "2020-02-04T08:10:07Z",
        "updatedAt" : "2020-02-04T08:10:11Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "aeb2197e-afcc-4681-ae21-734294f3cc13",
        "parentId" : "7eb53319-0e52-49a0-b8fc-d0ec84463112",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Is a recovery point lower than the log start offset useful? All data below the log start offset is subject to deletion.",
        "createdAt" : "2020-02-04T17:23:36Z",
        "updatedAt" : "2020-02-04T17:23:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "94d5d772-8138-4f69-84bb-5895c8e641fd",
        "parentId" : "7eb53319-0e52-49a0-b8fc-d0ec84463112",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "U are right :)",
        "createdAt" : "2020-02-04T17:33:47Z",
        "updatedAt" : "2020-02-04T17:33:48Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c5b977813084661c071ad823e3e7e5555c0ca5a",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +743,747 @@    }\n\n    if (this.recoveryPoint < offset) {\n      this.recoveryPoint = offset\n    }"
  },
  {
    "id" : "bea72e4e-a256-4388-a0c1-e9ce9c1f26e6",
    "prId" : 8037,
    "prUrl" : "https://github.com/apache/kafka/pull/8037#pullrequestreview-353217061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f92e968-8ef0-4e6f-87c3-cb8662a5ff10",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Previous behavior is ```recoveryPoint = math.min(newOffset, recoveryPoint))``` but this patch changes it to\r\n```\r\nif (this.recoveryPoint < offset) {\r\n  this.recoveryPoint = offset\r\n}\r\n```\r\nwhich is equal to ```recoveryPoint = math.max(newOffset, recoveryPoint))```. Is it a bug?",
        "createdAt" : "2020-02-04T17:43:20Z",
        "updatedAt" : "2020-02-04T17:43:24Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "62ca578d-aa8e-45e3-a177-503d47411b6d",
        "parentId" : "5f92e968-8ef0-4e6f-87c3-cb8662a5ff10",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I have also updated `updateLogEndOffset` to set the recovery point. In `truncateFully` where we delete all segments and set the log start offset to be equal to the log end offset, this ensures recovery point is also set consistently.",
        "createdAt" : "2020-02-04T18:29:30Z",
        "updatedAt" : "2020-02-04T18:29:31Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "3b0d9df8-5f48-40bd-b192-0bd9f5139841",
        "parentId" : "5f92e968-8ef0-4e6f-87c3-cb8662a5ff10",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Thanks for the explanation! ",
        "createdAt" : "2020-02-04T18:51:03Z",
        "updatedAt" : "2020-02-04T18:51:03Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c5b977813084661c071ad823e3e7e5555c0ca5a",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +2097,2101 @@        producerStateManager.updateMapEndOffset(newOffset)\n        maybeIncrementFirstUnstableOffset()\n        updateLogStartOffset(newOffset)\n      }\n    }"
  },
  {
    "id" : "e001b916-6482-4966-b0ed-7a8dd3acc97a",
    "prId" : 8130,
    "prUrl" : "https://github.com/apache/kafka/pull/8130#pullrequestreview-500015753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e084593e-af45-47ee-bde1-c9467eed5d7d",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "@guozhangwang Is this ok? I am not familiar with this part of the code. If `false` the default anyway?",
        "createdAt" : "2020-09-30T18:35:48Z",
        "updatedAt" : "2020-10-07T16:02:14Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "7e9baf18-8fb6-4537-847e-d44219bfc902",
        "parentId" : "e084593e-af45-47ee-bde1-c9467eed5d7d",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "LGTM, since the method definition also has `isRecoveredSwapFile: Boolean = false` by default.",
        "createdAt" : "2020-10-01T00:04:38Z",
        "updatedAt" : "2020-10-07T16:02:14Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0191295d-bb0c-4c5c-bdd5-3c2a285f09b1",
        "parentId" : "e084593e-af45-47ee-bde1-c9467eed5d7d",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Thanks @guozhangwang for verifying.",
        "createdAt" : "2020-10-01T05:22:52Z",
        "updatedAt" : "2020-10-07T16:02:14Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f7a73ff0e1027602e7590cea21e2c5973a76973",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2448,2452 @@      // replace old segment with new ones\n      info(s\"Replacing overflowed segment $segment with split segments $newSegments\")\n      replaceSegments(newSegments.toList, List(segment))\n      newSegments.toList\n    } catch {"
  },
  {
    "id" : "1385e4da-60b1-4b43-9f12-d6e6280cb887",
    "prId" : 8476,
    "prUrl" : "https://github.com/apache/kafka/pull/8476#pullrequestreview-394143102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6c8ce4d-dbbb-47e0-851c-5fd0ba2aff84",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Why in this case we use `maxOffsetMetadata` and in the else if we use `startOffsetMetadata` as `emptyFetchDataInfo#fetchOffsetMetadata`? Is there a specific rationale for that?",
        "createdAt" : "2020-04-15T18:54:22Z",
        "updatedAt" : "2020-04-15T19:02:25Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "2b4af472-b8f1-4226-a586-f30d17ca9156",
        "parentId" : "b6c8ce4d-dbbb-47e0-851c-5fd0ba2aff84",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the start offset matches max offset, then we do not need to lookup the metadata as we already have it. Really this is the bug fix. Without this, then we will proceed to read from the log segment, which results in an error like the following:\r\n\r\n```\r\njava.util.concurrent.ExecutionException: org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@cc86429`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 85.\r\n\r\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n\tat kafka.log.LogConcurrencyTest.testUncommittedDataNotConsumed(LogConcurrencyTest.scala:75)\r\n\tat kafka.log.LogConcurrencyTest.testUncommittedDataNotConsumedFrequentSegmentRolls(LogConcurrencyTest.scala:61)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\r\n\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\r\n\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\r\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\r\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\r\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\r\n\tat com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)\r\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)\r\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)\r\nCaused by: org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@cc86429`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 85.\r\n\tat org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:40)\r\n\tat org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:24)\r\n\tat org.apache.kafka.common.utils.AbstractIterator.maybeComputeNext(AbstractIterator.java:79)\r\n\tat org.apache.kafka.common.utils.AbstractIterator.hasNext(AbstractIterator.java:45)\r\n\tat org.apache.kafka.common.record.FileRecords.searchForOffsetWithSize(FileRecords.java:302)\r\n\tat kafka.log.LogSegment.translateOffset(LogSegment.scala:275)\r\n\tat kafka.log.LogSegment.read(LogSegment.scala:298)\r\n\tat kafka.log.Log.$anonfun$read$2(Log.scala:1515)\r\n\tat kafka.log.Log.read(Log.scala:2333)\r\n\tat kafka.log.LogConcurrencyTest$ConsumerTask.call(LogConcurrencyTest.scala:95)\r\n\tat kafka.log.LogConcurrencyTest$ConsumerTask.call(LogConcurrencyTest.scala:85)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@cc86429`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 85.\r\n\tat org.apache.kafka.common.utils.Utils.readFullyOrFail(Utils.java:966)\r\n\tat org.apache.kafka.common.record.FileLogInputStream.nextBatch(FileLogInputStream.java:68)\r\n\tat org.apache.kafka.common.record.FileLogInputStream.nextBatch(FileLogInputStream.java:41)\r\n\tat org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:35)\r\n\t... 14 more\r\n```",
        "createdAt" : "2020-04-15T19:16:14Z",
        "updatedAt" : "2020-04-15T19:16:15Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "85ceeb4c-12a0-4ed3-83d5-ce540806ed25",
        "parentId" : "b6c8ce4d-dbbb-47e0-851c-5fd0ba2aff84",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yeah I understand this is the bug fix, I'm only curious to see why we used \r\n\r\n```\r\nemptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\r\n```\r\n\r\nnot\r\n\r\n```\r\nemptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\r\n```\r\n\r\nI realized it is just the same, so curious if you intentionally use `maxOffsetMetadata` for any other reasons, but I think it is just to avoid unnecessary `convertToOffsetMetadataOrThrow` :)",
        "createdAt" : "2020-04-15T20:24:51Z",
        "updatedAt" : "2020-04-15T20:24:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "8060e7f7-0b18-471c-95ac-d4ce568a205a",
        "parentId" : "b6c8ce4d-dbbb-47e0-851c-5fd0ba2aff84",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It lets us skip the call to `convertToOffsetMetadataOrThrow` in order to find `startOffsetMetadata`. I tried changing the check below to `>=` and hit a similar error when trying to translate the offset in `LogSegment`. We might also be able to fix this by adding some additional checks in `LogSegment` to avoid the offset translation when the max position matches the start position of the requested offset. ",
        "createdAt" : "2020-04-15T21:19:33Z",
        "updatedAt" : "2020-04-15T23:50:44Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0290c405efe3bdfe6357c350f82e189dbdbc822",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1487,1491 @@\n      if (startOffset == maxOffsetMetadata.messageOffset) {\n        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n      } else if (startOffset > maxOffsetMetadata.messageOffset) {\n        val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)"
  },
  {
    "id" : "a506836e-aa5c-4dcf-bfde-667508fdfcde",
    "prId" : 8714,
    "prUrl" : "https://github.com/apache/kafka/pull/8714#pullrequestreview-416962538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77d277ce-41ab-404e-ac9d-bc52d7f17683",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "This doesn't change behavior, but makes the code a bit less weird. Previously, we would special case the log, but rely on the called method to check again if `deletable` is empty.",
        "createdAt" : "2020-05-22T14:37:57Z",
        "updatedAt" : "2020-05-22T14:37:57Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbd9df44d6a0fe94e4aca4d21078be4d0815878c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1698,1702 @@        info(s\"Found deletable segments with base offsets [${deletable.map(_.baseOffset).mkString(\",\")}] due to $reason\")\n        deleteSegments(deletable)\n      } else 0\n    }\n  }"
  },
  {
    "id" : "0c523272-17dd-48f7-9056-031e58b3d49d",
    "prId" : 8850,
    "prUrl" : "https://github.com/apache/kafka/pull/8850#pullrequestreview-429185426",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "530f0df1-06f8-49bd-886f-6832ac18a6b9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "In addition, it might be useful to know the total log size. Maybe we could include `size - diff` as the size after deletion?",
        "createdAt" : "2020-06-11T18:36:19Z",
        "updatedAt" : "2020-06-19T00:21:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "068a9061f31a2536e43cb4f650b958342c0a9ae3",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +1812,1816 @@      if (diff - segment.size >= 0) {\n        diff -= segment.size\n        info(s\"Segment with base offset ${segment.baseOffset} will be deleted due to\" +\n          s\" retention size ${config.retentionSize} bytes breach. Segment size is\" +\n          s\" ${segment.size} and total log size after deletion will be ${size - diff}\")"
  },
  {
    "id" : "637d7428-074e-42b5-8f84-10508e4a2d1d",
    "prId" : 8850,
    "prUrl" : "https://github.com/apache/kafka/pull/8850#pullrequestreview-432675172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c374453e-9261-4d73-b6ea-cad293134249",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can we keep the reason? On second thought, maybe it's fine to leave this as is.",
        "createdAt" : "2020-06-11T18:45:42Z",
        "updatedAt" : "2020-06-19T00:21:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f1bfaa07-8735-4937-a996-fdc6a7ace481",
        "parentId" : "c374453e-9261-4d73-b6ea-cad293134249",
        "authorId" : "0437327f-f1bc-4c4a-b258-4c8dd6851ac9",
        "body" : "Wouldn't keeping the reason be a redundant? Since for every segment we delete we are logging exactly why we are deleting and the details surrounding the deleition.",
        "createdAt" : "2020-06-12T01:10:06Z",
        "updatedAt" : "2020-06-19T00:21:58Z",
        "lastEditedBy" : "0437327f-f1bc-4c4a-b258-4c8dd6851ac9",
        "tags" : [
        ]
      },
      {
        "id" : "3aeb7db3-bf4a-4a30-94eb-73a95756e198",
        "parentId" : "c374453e-9261-4d73-b6ea-cad293134249",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "In that case, perhaps we can remove the log message? We already have the following logging in the `deleteSegments` path:\r\n```scala\r\n  private def deleteSegmentFiles(segments: Iterable[LogSegment], asyncDelete: Boolean): Unit = {\r\n    segments.foreach(_.changeFileSuffixes(\"\", Log.DeletedFileSuffix))\r\n\r\n    def deleteSegments(): Unit = {\r\n      info(s\"Deleting segments ${segments.mkString(\",\")}\")\r\n      maybeHandleIOException(s\"Error while deleting segments for $topicPartition in dir ${dir.getParent}\") {\r\n        segments.foreach(_.deleteIfExists())\r\n      }\r\n    }\r\n\r\n    if (asyncDelete) {\r\n      info(s\"Scheduling segments for deletion ${segments.mkString(\",\")}\")\r\n      scheduler.schedule(\"delete-file\", () => deleteSegments, delay = config.fileDeleteDelayMs)\r\n    } else {\r\n      deleteSegments()\r\n    }\r\n  }\r\n```\r\nSo it seems this message is not adding any additional value.",
        "createdAt" : "2020-06-17T17:17:43Z",
        "updatedAt" : "2020-06-19T00:21:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "126f68e7-43ce-4288-9ec7-131fd0b09c6c",
        "parentId" : "c374453e-9261-4d73-b6ea-cad293134249",
        "authorId" : "0437327f-f1bc-4c4a-b258-4c8dd6851ac9",
        "body" : "Sure, will do.",
        "createdAt" : "2020-06-17T18:42:06Z",
        "updatedAt" : "2020-06-19T00:21:58Z",
        "lastEditedBy" : "0437327f-f1bc-4c4a-b258-4c8dd6851ac9",
        "tags" : [
        ]
      }
    ],
    "commit" : "068a9061f31a2536e43cb4f650b958342c0a9ae3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1703,1707 @@   * @return The number of segments deleted\n   */\n  private def deleteOldSegments(predicate: (LogSegment, Option[LogSegment]) => Boolean) = {\n    lock synchronized {\n      val deletable = deletableSegments(predicate)"
  },
  {
    "id" : "ffd8dc84-8c77-4099-ada5-3123906b63b4",
    "prId" : 9626,
    "prUrl" : "https://github.com/apache/kafka/pull/9626#pullrequestreview-552331548",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98977f41-8794-4338-bece-a2fce04bebe6",
        "parentId" : null,
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Seems neater to use `partitionMetadataFile.foreach` instead of using `.get` twice.",
        "createdAt" : "2020-12-15T11:20:42Z",
        "updatedAt" : "2020-12-18T21:37:05Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ff7840a592464cb18931b27d4ff1d6157a86b93",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +335,339 @@      if (!file.isEmpty())\n        topicId = file.read().topicId\n    }\n  }\n"
  },
  {
    "id" : "2316a884-0380-4b1f-ac1b-bbc603587ec3",
    "prId" : 9731,
    "prUrl" : "https://github.com/apache/kafka/pull/9731#pullrequestreview-549795162",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "15c8bf23-dff6-4590-b66b-b31917eb1b07",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "How about replacing ```messageFormatVersion``` by ```recordVersion.value``` for all usages in this method.",
        "createdAt" : "2020-12-11T03:30:35Z",
        "updatedAt" : "2020-12-11T04:32:59Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "866408eb-dae6-49d8-b5d8-9838a9f70b2f",
        "parentId" : "15c8bf23-dff6-4590-b66b-b31917eb1b07",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "This was already done. Please let me now if thats not the case.",
        "createdAt" : "2020-12-11T04:33:35Z",
        "updatedAt" : "2020-12-11T04:33:35Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d2b4a2e732b42397df62da141df30856e5c88db",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +893,897 @@    // (or later snapshots). Otherwise, if there is no snapshot file, then we have to rebuild producer state\n    // from the first segment.\n    if (recordVersion.value < RecordBatch.MAGIC_VALUE_V2 ||\n        (producerStateManager.latestSnapshotOffset.isEmpty && reloadFromCleanShutdown)) {\n      // To avoid an expensive scan through all of the segments, we take empty snapshots from the start of the"
  },
  {
    "id" : "ad34b823-b791-4001-becd-0e39a05246f8",
    "prId" : 9731,
    "prUrl" : "https://github.com/apache/kafka/pull/9731#pullrequestreview-549795183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27e3b572-989a-43e0-8d87-38161680d066",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "there is a same issue https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogCleaner.scala#L456",
        "createdAt" : "2020-12-11T03:33:45Z",
        "updatedAt" : "2020-12-11T04:32:59Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "fdf66c8a-f221-4514-8111-4bf4c2d3c721",
        "parentId" : "27e3b572-989a-43e0-8d87-38161680d066",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Done.",
        "createdAt" : "2020-12-11T04:33:39Z",
        "updatedAt" : "2020-12-11T04:33:40Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d2b4a2e732b42397df62da141df30856e5c88db",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +1969,1973 @@          baseOffset = newOffset,\n          config,\n          time = time,\n          initFileSize = initFileSize,\n          preallocate = config.preallocate)"
  },
  {
    "id" : "d72a8f2f-8c9a-491c-b2ab-d0b36110278c",
    "prId" : 9739,
    "prUrl" : "https://github.com/apache/kafka/pull/9739#pullrequestreview-569947776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "067f3cc8-bacc-428c-be96-ba990a561a45",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It would be helpful to have a test case which verifies `appendAsLeader` with the new append origin.",
        "createdAt" : "2021-01-08T00:14:21Z",
        "updatedAt" : "2021-02-01T15:22:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "1162ec3c-1cf3-42d6-b904-87f3ca6c632c",
        "parentId" : "067f3cc8-bacc-428c-be96-ba990a561a45",
        "authorId" : "410e5da8-f561-43d9-a4a1-a8ffb52d0269",
        "body" : "fixed",
        "createdAt" : "2021-01-16T15:49:20Z",
        "updatedAt" : "2021-02-01T15:22:53Z",
        "lastEditedBy" : "410e5da8-f561-43d9-a4a1-a8ffb52d0269",
        "tags" : [
        ]
      }
    ],
    "commit" : "89afaf3d8f72af4b5770bef9ae27989ad1a29860",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1077,1081 @@  def appendAsLeader(records: MemoryRecords,\n                     leaderEpoch: Int,\n                     origin: AppendOrigin = AppendOrigin.Client,\n                     interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n    val validateAndAssignOffsets = origin != AppendOrigin.RaftLeader"
  },
  {
    "id" : "16544d83-037e-453a-8f75-f54863f581c3",
    "prId" : 9739,
    "prUrl" : "https://github.com/apache/kafka/pull/9739#pullrequestreview-575680604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31ab01d5-91e4-4185-a735-19e064016d45",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It is a little odd here is that we have to pass through `leaderEpoch` even though we expect the Raft leader to have set it already. Perhaps we should be validating it in `analyzeAndValidateRecords`. We can verify for each batch that the leader epoch matches when the append origin is `RaftLeader`. Does that make sense?",
        "createdAt" : "2021-01-20T01:01:01Z",
        "updatedAt" : "2021-02-01T15:22:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "07dd9ee4-0354-4b68-bff6-a4a47ea53437",
        "parentId" : "31ab01d5-91e4-4185-a735-19e064016d45",
        "authorId" : "410e5da8-f561-43d9-a4a1-a8ffb52d0269",
        "body" : "Make sense to me, it is also a little bit odd to me, but I put it here because I think `assignOffsets`==true for `appendAsLeader` and ==false for `appendAsFollower`, which means normally `assignOffsets` is determined by the caller, the `RaftLeader` is just a special case for `appendAsLeader`, if we move the logic in `analyzeAndValidateRecords`, that means it need to determine whether to `assignOffsets` without caller info, does that doable? ",
        "createdAt" : "2021-01-21T04:08:31Z",
        "updatedAt" : "2021-02-01T15:22:53Z",
        "lastEditedBy" : "410e5da8-f561-43d9-a4a1-a8ffb52d0269",
        "tags" : [
        ]
      },
      {
        "id" : "436f9385-0db1-4822-a433-346ab73806bc",
        "parentId" : "31ab01d5-91e4-4185-a735-19e064016d45",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The call to `analyzeAndValidateRecords` is done both for leader as well as follower appends. Basically we do a shallow iteration over the batches in order to collect some information and validate the CRC. My thought was to pass `leaderEpoch` into `analyzeAndValidateRecords` and add a basic check like this:\r\n```java\r\nrecords.batches.forEach { batch =>\r\n  ...\r\n  if (origin === RaftLeader && batch.partitionLeaderEpoch != leaderEpoch) {\r\n    throw new InvalidRecordException(\"Append from Raft leader did not set the batch epoch correctly\")\r\n  }\r\n}  \r\n```",
        "createdAt" : "2021-01-25T18:19:08Z",
        "updatedAt" : "2021-02-01T15:22:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "89afaf3d8f72af4b5770bef9ae27989ad1a29860",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1080,1084 @@                     interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n    val validateAndAssignOffsets = origin != AppendOrigin.RaftLeader\n    append(records, origin, interBrokerProtocolVersion, validateAndAssignOffsets, leaderEpoch, ignoreRecordSize = false)\n  }\n"
  },
  {
    "id" : "72aecf7d-cbcc-4aaf-9085-9e6403cb2b2f",
    "prId" : 9816,
    "prUrl" : "https://github.com/apache/kafka/pull/9816#pullrequestreview-576925096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ca1f5c4-647b-4664-93a1-02ed675a4b09",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It is a little unfortunate to expose the truncation APIs from `Log`. For logs which are managed by `LogManager`, truncation calls are expected to go through `LogManager`. That makes me think it might be useful to have a distinction between the low-level `Log` object and that which is managed by `LogManager`. For example, maybe we could use `ManagedLog` or something like that. Not something to address here, just food for thought in the way of future improvement.",
        "createdAt" : "2021-01-27T02:16:28Z",
        "updatedAt" : "2021-01-29T19:24:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d7d4d25e92681ccdea18c2a388a367ac0993ab7",
    "line" : 185,
    "diffHunk" : "@@ -1,1 +2142,2146 @@   *  @param newOffset The new offset to start the log with\n   */\n  def truncateFullyAndStartAt(newOffset: Long): Unit = {\n    maybeHandleIOException(s\"Error while truncating the entire log for $topicPartition in dir ${dir.getParent}\") {\n      debug(s\"Truncate and start at offset $newOffset\")"
  },
  {
    "id" : "10d0cdfc-d2f8-436f-a391-d66eb979ccd3",
    "prId" : 10041,
    "prUrl" : "https://github.com/apache/kafka/pull/10041#pullrequestreview-586084871",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06877c0a-f46e-4a2c-8082-a5cde1051344",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add the new param to the javadoc? In the javadoc, it would be useful to explain a bit how this helps with re-upgrade.",
        "createdAt" : "2021-02-09T00:27:49Z",
        "updatedAt" : "2021-02-10T17:45:36Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "94ada5df-b4f0-4b11-9bde-98ad08e8c324",
        "parentId" : "06877c0a-f46e-4a2c-8082-a5cde1051344",
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "good point. will do!",
        "createdAt" : "2021-02-09T00:29:00Z",
        "updatedAt" : "2021-02-10T17:45:36Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      },
      {
        "id" : "89f38dae-031f-4142-99c7-404cc01442f6",
        "parentId" : "06877c0a-f46e-4a2c-8082-a5cde1051344",
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "Something like this work for an explanation?\r\n```\r\n* boolean flag to indicate whether the partition.metadata file should be kept in the \r\n* log directory. A partition.metadata file is only created when the controller's \r\n* inter-broker protocol version is at least 2.8. This file will persist the topic ID on\r\n* the broker. If inter-broker protocol is downgraded below 2.8, a topic ID may be lost\r\n* and a new ID generated upon re-upgrade. If the inter-broker protocol version is below\r\n* 2.8, partition.metadata will be deleted to avoid ID conflicts upon re-upgrade. \r\n ```",
        "createdAt" : "2021-02-09T00:40:32Z",
        "updatedAt" : "2021-02-10T17:45:36Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      },
      {
        "id" : "8e40076c-eec8-4b8e-a464-6998cb95d988",
        "parentId" : "06877c0a-f46e-4a2c-8082-a5cde1051344",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Yes, looks good to me.",
        "createdAt" : "2021-02-09T01:55:18Z",
        "updatedAt" : "2021-02-10T17:45:36Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1da8c724dcb89a355374bf84c9636eca04ce4b12",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +264,268 @@          logDirFailureChannel: LogDirFailureChannel,\n          private val hadCleanShutdown: Boolean = true,\n          val keepPartitionMetadataFile: Boolean = true) extends Logging with KafkaMetricsGroup {\n\n  import kafka.log.Log._"
  },
  {
    "id" : "52060533-5405-4b1e-9c67-a56f56ca41d8",
    "prId" : 10280,
    "prUrl" : "https://github.com/apache/kafka/pull/10280#pullrequestreview-680508670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40c7746d-eb5b-406b-9db3-94fbf1350c46",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "This is an existing issue. In this case, it seems that we should always update high watermark in completeTruncation() with localLog.logEndOffsetMetadata.",
        "createdAt" : "2021-06-09T21:07:15Z",
        "updatedAt" : "2021-06-09T21:07:56Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "30170320-0e4f-4105-a65f-a1d1f4eb11c5",
        "parentId" : "40c7746d-eb5b-406b-9db3-94fbf1350c46",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Done in 28bf22af168ca0db76796b5d3cd67a38ed8ed1c2.",
        "createdAt" : "2021-06-10T08:42:51Z",
        "updatedAt" : "2021-06-10T08:42:51Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "1752ea8c0b9537d7d66815e7619ba810a1cd141d",
    "line" : 959,
    "diffHunk" : "@@ -1,1 +1581,1585 @@      debug(s\"Truncate and start at offset $newOffset\")\n      lock synchronized {\n        localLog.truncateFullyAndStartAt(newOffset)\n        leaderEpochCache.foreach(_.clearAndFlush())\n        producerStateManager.truncateFullyAndStartAt(newOffset)"
  },
  {
    "id" : "571ee1e3-1f86-4385-8baf-8d4126f10f7a",
    "prId" : 10478,
    "prUrl" : "https://github.com/apache/kafka/pull/10478#pullrequestreview-627948550",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a76e388-71f3-4bcc-933c-1c372c39d229",
        "parentId" : null,
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Document the params",
        "createdAt" : "2021-04-05T16:33:20Z",
        "updatedAt" : "2021-04-20T07:10:51Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "eaf8519fa26ecfe6278edadee83e3604ae496be8",
    "line" : 1184,
    "diffHunk" : "@@ -1,1 +2536,2540 @@   * @return List of new segments that replace the input segment\n   */\n  private[log] def splitOverflowedSegment(segment: LogSegment,\n                                          existingSegments: LogSegments,\n                                          dir: File,"
  },
  {
    "id" : "db0f835d-521a-4fe2-bdd8-d0ebb18ce872",
    "prId" : 10478,
    "prUrl" : "https://github.com/apache/kafka/pull/10478#pullrequestreview-639412609",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25783a84-3bfb-42d5-9776-acbdfb2bd7a9",
        "parentId" : null,
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "Can we pass `LogComponents` as a single field instead of initializing with the respective fields? This makes the code easy to read, constructor simpler with less vars.   ",
        "createdAt" : "2021-04-12T02:13:26Z",
        "updatedAt" : "2021-04-20T07:10:51Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      },
      {
        "id" : "13b19a18-6987-46c7-8f18-48251425eda6",
        "parentId" : "25783a84-3bfb-42d5-9776-acbdfb2bd7a9",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "I see your point. It's good thinking. But the main reason I didn't take this route was that some of the fields of `LogComponents` are immutable within the context of the `LogLoader`, but are mutable by `Log`. Examples: `logStartOffset`, `recoveryPoint` and `leaderEpochCache ` are defined as `var` only in `Log`. It seemed hard to explain if we made it a `var` inside `LogComponents` just because the `Log` class wants to mutate them.",
        "createdAt" : "2021-04-12T09:07:03Z",
        "updatedAt" : "2021-04-20T07:10:51Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "12a9e5ff-93c1-44dc-a01b-d5a6206b8ea1",
        "parentId" : "25783a84-3bfb-42d5-9776-acbdfb2bd7a9",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "What I meant was we can have `Log` takes the argument as immutable `LogComponents` and it can initialize the vars inside `Log` with the respective fields from `LogComponents`. This will also set the right access of these vars by not giving write access by default. \r\n\r\n```\r\nclass Log(@volatile private var _dir: File,\r\n          @volatile var config: LogConfig,\r\n          val segments: LogSegments,\r\n          val logComponents: LogComponents,\r\n          scheduler: Scheduler,\r\n          brokerTopicStats: BrokerTopicStats,\r\n          val time: Time,\r\n          val maxProducerIdExpirationMs: Int,\r\n          val producerIdExpirationCheckIntervalMs: Int,\r\n          val topicPartition: TopicPartition,\r\n          logDirFailureChannel: LogDirFailureChannel,\r\n          @volatile var topicId: Option[Uuid],\r\n          val keepPartitionMetadataFile: Boolean = true) extends Logging with KafkaMetricsGroup {\r\n\r\n\r\n\r\n  @volatile private var logStartOffset: Long = logComponents.logStartOffset\r\n  @volatile private var recoveryPoint: Long = logComponents.recoveryPoint\r\n  @volatile private var nextOffsetMetadata: LogOffsetMetadata = logComponents.nextOffsetMetadata\r\n  @volatile var leaderEpochCache: Option[LeaderEpochFileCache] = logComponents.leaderEpochCache\r\n  private val producerStateManager: ProducerStateManager = logComponents.producerStateManager\r\n```",
        "createdAt" : "2021-04-12T16:41:27Z",
        "updatedAt" : "2021-04-20T07:10:51Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      },
      {
        "id" : "e355521c-4999-4f86-aed3-6d76d4321710",
        "parentId" : "25783a84-3bfb-42d5-9776-acbdfb2bd7a9",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Sounds good. I'll give this a shot.",
        "createdAt" : "2021-04-12T17:43:06Z",
        "updatedAt" : "2021-04-20T07:10:51Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "f1203e3c-392a-4a4e-8cd4-b1fcb1e5b35a",
        "parentId" : "25783a84-3bfb-42d5-9776-acbdfb2bd7a9",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "I took a stab at this. Things changed a bit after I did the refactoring that Jun suggested [here](https://github.com/apache/kafka/pull/10478#discussion_r610933966). In the latest version of the PR, `LogComponents` has been renamed to a class called `LoadedLogOffsets` which now only contains (logStartOffset, recoveryPoint, nextOffsetMetadata). It seemed reasonable to just pass these in as 3 separate parameters into `Log`, since the `LeaderEpochFileCache` and `ProducerStateManager` are no longer inside the `LoadedLogOffsets` object those have to anyway be passed in separately.",
        "createdAt" : "2021-04-15T06:41:22Z",
        "updatedAt" : "2021-04-20T07:10:51Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "f514c8e7-ee53-4af9-8e6e-718e0f2e3310",
        "parentId" : "25783a84-3bfb-42d5-9776-acbdfb2bd7a9",
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "Sure, that looks reasonable to me. ",
        "createdAt" : "2021-04-20T00:39:25Z",
        "updatedAt" : "2021-04-20T07:10:51Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      }
    ],
    "commit" : "eaf8519fa26ecfe6278edadee83e3604ae496be8",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +262,266 @@class Log(@volatile private var _dir: File,\n          @volatile var config: LogConfig,\n          val segments: LogSegments,\n          @volatile var logStartOffset: Long,\n          @volatile var recoveryPoint: Long,"
  },
  {
    "id" : "2ad12fa1-98f9-456b-b52a-f7aa0b5567d3",
    "prId" : 10742,
    "prUrl" : "https://github.com/apache/kafka/pull/10742#pullrequestreview-665963136",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32a3a886-43f2-4613-bf8a-da6e60200746",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add the new param to javadoc?",
        "createdAt" : "2021-05-21T21:18:11Z",
        "updatedAt" : "2021-05-21T21:26:00Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "86902269-6d12-4f7d-90c3-dc199124dc17",
        "parentId" : "32a3a886-43f2-4613-bf8a-da6e60200746",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Done. Addressed in cc8353fd9e2475e56488ade8d6f01a772830edc4.",
        "createdAt" : "2021-05-21T21:51:17Z",
        "updatedAt" : "2021-05-21T21:51:17Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b429dcb5a21580d1b8f5cca7183101c98a6faee",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +2343,2347 @@          logDirFailureChannel,\n          producerStateManager,\n          logPrefix)\n      }\n      // okay we are safe now, remove the swap suffix"
  },
  {
    "id" : "7e49ada5-7255-45c7-8216-a9f707460bab",
    "prId" : 10742,
    "prUrl" : "https://github.com/apache/kafka/pull/10742#pullrequestreview-666151610",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbf9389b-c6ae-4d4e-8acf-401a382bd0ca",
        "parentId" : null,
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "Could we add the default `logPrefix` parameter value to empty string? So all the tests don't need to update.",
        "createdAt" : "2021-05-22T02:14:33Z",
        "updatedAt" : "2021-05-22T02:17:09Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      },
      {
        "id" : "83fa617c-7645-4084-973d-55a6529ef7a6",
        "parentId" : "fbf9389b-c6ae-4d4e-8acf-401a382bd0ca",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Introducing a default value can lead to programming error, because we could forget to pass it when it is really needed to be passed.",
        "createdAt" : "2021-05-22T03:05:17Z",
        "updatedAt" : "2021-05-22T03:05:17Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "e8c23e8f-fb00-44ce-a2cb-f0b72b8d9aa7",
        "parentId" : "fbf9389b-c6ae-4d4e-8acf-401a382bd0ca",
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "Ok, good to me.",
        "createdAt" : "2021-05-22T04:41:51Z",
        "updatedAt" : "2021-05-22T04:41:51Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b429dcb5a21580d1b8f5cca7183101c98a6faee",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +2239,2243 @@                                  logDirFailureChannel: LogDirFailureChannel,\n                                  recordVersion: RecordVersion,\n                                  logPrefix: String): Option[LeaderEpochFileCache] = {\n    val leaderEpochFile = LeaderEpochCheckpointFile.newFile(dir)\n"
  },
  {
    "id" : "5c01735c-fd1c-4607-a93f-51d7a689b775",
    "prId" : 10754,
    "prUrl" : "https://github.com/apache/kafka/pull/10754#pullrequestreview-684346920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc081360-4208-4607-9c34-fbb65160247b",
        "parentId" : null,
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "This was a pre-existing issue but should we extract this whole if else block to a method? It's also a little bit easy to miss that it's setting the `_topicid` in https://github.com/apache/kafka/pull/10754/files#diff-eeafed82ed6a8600c397b108787fdf31e03191b0a192774a65c127d0d26edc44L330. Maybe that could benefit from an empty line prior to it.",
        "createdAt" : "2021-06-15T14:01:10Z",
        "updatedAt" : "2021-06-15T14:01:10Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      },
      {
        "id" : "5eb5a73d-5895-4456-82a2-a1a33b1ff27b",
        "parentId" : "fc081360-4208-4607-9c34-fbb65160247b",
        "authorId" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "body" : "Yeah. Unfortunately there is a lot going on here. I've edited the comment and spacing to make it a little clearer. If we still think it needs adjustment, I can try making a new method.",
        "createdAt" : "2021-06-15T17:52:13Z",
        "updatedAt" : "2021-06-15T17:52:13Z",
        "lastEditedBy" : "a31dcef8-b459-4b48-bb49-44e910fa9f34",
        "tags" : [
        ]
      },
      {
        "id" : "35a2ba03-4fa7-4bbd-b32e-50eb902d4f3f",
        "parentId" : "fc081360-4208-4607-9c34-fbb65160247b",
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "I think extracting it into a method named initializeTopicId or something like that would be good.",
        "createdAt" : "2021-06-15T18:17:15Z",
        "updatedAt" : "2021-06-15T18:17:15Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      }
    ],
    "commit" : "10eb9b259b46f1333f157000000d9a8e486e0aa1",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +349,353 @@    } else if (keepPartitionMetadataFile) {\n      _topicId.foreach(partitionMetadataFile.write)\n    } else {\n      // We want to keep the file and the in-memory topic ID in sync.\n      _topicId = None"
  },
  {
    "id" : "fcf0fb98-e990-4f63-81c8-0f56cfaaf6d2",
    "prId" : 10760,
    "prUrl" : "https://github.com/apache/kafka/pull/10760#pullrequestreview-689928562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e822ad8b-fbb7-4195-a418-0e06d88e1211",
        "parentId" : null,
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "Could we get a `maxTimestampSoFar` and `offsetOfMaxTimestampSoFar` which does not correspond to each others? It seems that we have no guarantee here. Is it an issue? ",
        "createdAt" : "2021-05-27T09:35:19Z",
        "updatedAt" : "2021-05-27T09:46:10Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      },
      {
        "id" : "6a1bcd7d-dec0-4f0f-8f07-1be384f4cbdd",
        "parentId" : "e822ad8b-fbb7-4195-a418-0e06d88e1211",
        "authorId" : "bb6c64f2-07d8-435d-afda-9f04874e90a0",
        "body" : "In all cases I can find the 2 are updated together so I think we can assume consistency. For the topic liveness case in the KIP absolute consistency is not required but there will be other cases that will need this (e.g. topic inspection).",
        "createdAt" : "2021-05-27T11:41:16Z",
        "updatedAt" : "2021-05-27T11:41:16Z",
        "lastEditedBy" : "bb6c64f2-07d8-435d-afda-9f04874e90a0",
        "tags" : [
        ]
      },
      {
        "id" : "61fbf0ee-2c2d-467d-a9b6-a400ba440ff8",
        "parentId" : "e822ad8b-fbb7-4195-a418-0e06d88e1211",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "I think that we should address this as well. I am fine with doing it in a follow-up PR though so we can keep this focused. Ok for you?",
        "createdAt" : "2021-06-15T14:23:23Z",
        "updatedAt" : "2021-06-15T14:41:40Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      },
      {
        "id" : "90d9f636-3165-4a96-8faf-257a46725914",
        "parentId" : "e822ad8b-fbb7-4195-a418-0e06d88e1211",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "Could we file a JIRA as a subtask in the Jira of the KIP to not forget about it?",
        "createdAt" : "2021-06-22T13:12:26Z",
        "updatedAt" : "2021-06-22T13:37:17Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      },
      {
        "id" : "3bd688ff-c6b4-49ab-b915-a703c69d2f60",
        "parentId" : "e822ad8b-fbb7-4195-a418-0e06d88e1211",
        "authorId" : "bb6c64f2-07d8-435d-afda-9f04874e90a0",
        "body" : "sure thing, raised: https://issues.apache.org/jira/browse/KAFKA-12981",
        "createdAt" : "2021-06-22T19:41:55Z",
        "updatedAt" : "2021-06-22T19:41:55Z",
        "lastEditedBy" : "bb6c64f2-07d8-435d-afda-9f04874e90a0",
        "tags" : [
        ]
      }
    ],
    "commit" : "19f98ab39f040f3072aa2242eedb3f76465fd65f",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1348,1352 @@        Some(new TimestampAndOffset(latestTimestampSegment.maxTimestampSoFar,\n          latestTimestampSegment.offsetOfMaxTimestampSoFar,\n          epochOptional))\n      } else {\n        // Cache to avoid race conditions. `toBuffer` is faster than most alternatives and provides"
  },
  {
    "id" : "7098bf66-e2da-462f-8947-449cf25662b2",
    "prId" : 10896,
    "prUrl" : "https://github.com/apache/kafka/pull/10896#pullrequestreview-687170435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b788acfb-f39d-4f45-8865-b4bfce9109ac",
        "parentId" : null,
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Could we add some logging similar to the one in `LogSegment`?  https://github.com/apache/kafka/blob/3fb836f507d35e2a1ab39df57edca671ffc66073/core/src/main/scala/kafka/log/LogSegment.scala#L612-L615",
        "createdAt" : "2021-06-18T08:43:19Z",
        "updatedAt" : "2021-06-18T08:49:29Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e2df3c068936a039468540723a07f141213ff88",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2405,2409 @@        }\n        snapshotsToDelete.foreach { snapshot =>\n          snapshot.deleteIfExists()\n        }\n      }"
  },
  {
    "id" : "071e9cf4-c166-4845-a35d-fc891adaa6e0",
    "prId" : 10960,
    "prUrl" : "https://github.com/apache/kafka/pull/10960#pullrequestreview-699717974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71df0411-d140-4e09-aa5c-918c5cd1eb5c",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Is this another instance of unnecessary copying? Are there others?",
        "createdAt" : "2021-07-05T16:33:14Z",
        "updatedAt" : "2021-07-05T16:33:14Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "34386fef-39f6-468b-afad-201654d3b953",
        "parentId" : "71df0411-d140-4e09-aa5c-918c5cd1eb5c",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Oh, this is a different type (`TimestampAndOffset`).",
        "createdAt" : "2021-07-05T16:35:06Z",
        "updatedAt" : "2021-07-05T16:35:06Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "04da7130-eed6-476d-9793-6531c90ad836",
        "parentId" : "71df0411-d140-4e09-aa5c-918c5cd1eb5c",
        "authorId" : "bb6c64f2-07d8-435d-afda-9f04874e90a0",
        "body" : "I had a check through and can't see any other copy instances",
        "createdAt" : "2021-07-06T09:09:17Z",
        "updatedAt" : "2021-07-06T09:09:18Z",
        "lastEditedBy" : "bb6c64f2-07d8-435d-afda-9f04874e90a0",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bb34bf3beb057dd7c29e26909fef67ec9a1ecb3",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1348,1352 @@        val latestTimestampAndOffset = latestTimestampSegment.maxTimestampAndOffsetSoFar\n        Some(new TimestampAndOffset(latestTimestampAndOffset.timestamp,\n          latestTimestampAndOffset.offset,\n          epochOptional))\n      } else {"
  }
]