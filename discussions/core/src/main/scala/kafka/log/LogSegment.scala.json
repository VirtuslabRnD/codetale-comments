[
  {
    "id" : "8d71046c-62ef-44f2-9d6c-1e20dd1646e7",
    "prId" : 5169,
    "prUrl" : "https://github.com/apache/kafka/pull/5169#pullrequestreview-127352873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e5d72f7-5941-4c10-971a-ff32d6ba25a9",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Hmm, I seem to have missed looking into this call in more detail. Do you think we need to handle a potential offset overflow when `onBecomeInactiveSegment` is called from `roll()`?",
        "createdAt" : "2018-06-08T22:25:17Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "2ef3625e-8536-41b6-84a8-c6d83783863e",
        "parentId" : "7e5d72f7-5941-4c10-971a-ff32d6ba25a9",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Double check to be sure, but I think we already protect updates to `offsetOfMaxTimestamp`. This field seems is updated in three cases 1) append, 2) recovery, and 3) loading from the index. We do validation in the first two cases and, in the third, the offset should already be in the right range (since it was already appended to the index).",
        "createdAt" : "2018-06-09T05:06:55Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "c71b1e5937a33e525f483cd5d2edaa925fa68ccb",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +498,502 @@   */\n  def onBecomeInactiveSegment() {\n    timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true)\n    offsetIndex.trimToValidSize()\n    timeIndex.trimToValidSize()"
  },
  {
    "id" : "2ff78c19-5482-4f7b-9f1a-46839a0f3f8d",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205499964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87641e20-0100-4911-a264-b93dc9345a00",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Perhaps add a comment why we skip the sanity check just for offsetIndex and timeIndex?",
        "createdAt" : "2019-02-16T02:17:03Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "4fde406b-ef21-4b82-9fe0-911401bd55b1",
        "parentId" : "87641e20-0100-4911-a264-b93dc9345a00",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2019-02-19T23:03:12Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +80,84 @@  def sanityCheck(timeIndexFileNewlyCreated: Boolean): Unit = {\n    if (lazyOffsetIndex.file.exists) {\n      // Resize the time index file to 0 if it is newly created.\n      if (timeIndexFileNewlyCreated)\n        timeIndex.resize(0)"
  },
  {
    "id" : "8b29916a-5fea-4223-9ae8-87794eb06325",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-236355891",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "135f2463-a91b-45db-894c-4bbd24990ca4",
        "parentId" : null,
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "can you add a comment stating why this needs to be volatile.",
        "createdAt" : "2019-05-10T23:08:17Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "3c92e1cd-dab2-4b89-99c4-1a9ebf008e01",
        "parentId" : "135f2463-a91b-45db-894c-4bbd24990ca4",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "added",
        "createdAt" : "2019-05-11T01:12:57Z",
        "updatedAt" : "2019-05-11T01:12:57Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +98,102 @@  // The timestamp we used for time based log rolling and for ensuring max compaction delay\n  // volatile for LogCleaner to see the update\n  @volatile private var rollingBasedTimestamp: Option[Long] = None\n\n  /* The maximum timestamp we see so far */"
  },
  {
    "id" : "bf5716d1-0566-431a-83f3-31b213ceb40f",
    "prId" : 9110,
    "prUrl" : "https://github.com/apache/kafka/pull/9110#pullrequestreview-460194909",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "366ec1b0-7656-48cb-8178-9566ad0e24ab",
        "parentId" : null,
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Should we keep `largestTime` from LHS, and, print both `largestRecordTimestamp` and `largestTime` ?",
        "createdAt" : "2020-08-03T06:16:34Z",
        "updatedAt" : "2020-08-06T18:18:29Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "82f51788-a613-471a-b61a-023d459648e2",
        "parentId" : "366ec1b0-7656-48cb-8178-9566ad0e24ab",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm ok with the change. I think it's better to reflect the underlying fields directly and redundant information just adds noise to the logs.",
        "createdAt" : "2020-08-03T17:16:29Z",
        "updatedAt" : "2020-08-06T18:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "6df3797e99ad6354c69bab545862c9590e4a9aad",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +414,418 @@    \", size=\" + size +\n    \", lastModifiedTime=\" + lastModified +\n    \", largestRecordTimestamp=\" + largestRecordTimestamp +\n    \")\"\n"
  }
]