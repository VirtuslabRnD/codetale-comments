[
  {
    "id" : "f8073033-34d4-47b7-b477-15d30a97942f",
    "prId" : 4663,
    "prUrl" : "https://github.com/apache/kafka/pull/4663#pullrequestreview-103270139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7f99e40-a935-40f4-a9ac-d4c850ab4218",
        "parentId" : null,
        "authorId" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "body" : "There may be more than one entry in logsToBeDeleted.\r\nCan we pick an entry whose wait time is short so that the efficiency of the while loop is higher ?",
        "createdAt" : "2018-03-12T19:10:26Z",
        "updatedAt" : "2018-03-13T01:02:28Z",
        "lastEditedBy" : "0c73d886-f3da-4107-8045-92d8e3c8fb75",
        "tags" : [
        ]
      },
      {
        "id" : "d533bc29-c777-46d3-ba83-3618d129ce52",
        "parentId" : "a7f99e40-a935-40f4-a9ac-d4c850ab4218",
        "authorId" : "220f032c-6592-42d9-9042-aed276632816",
        "body" : "@tedyu Thanks for the comment. This is a FIFO queue and each log should be deleted with the same delay. So the first element in the queue should always have the smallest wait time.",
        "createdAt" : "2018-03-13T01:05:42Z",
        "updatedAt" : "2018-03-13T01:05:51Z",
        "lastEditedBy" : "220f032c-6592-42d9-9042-aed276632816",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f0cd959e9a7c641b1f9bb35a027d347d6ec60d5",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +715,719 @@            val waitingTimeMs = scheduleTimeMs + currentDefaultConfig.fileDeleteDelayMs - time.milliseconds()\n            if (waitingTimeMs > 0)\n              Thread.sleep(waitingTimeMs)\n            removedLog.delete()\n            info(s\"Deleted log for partition ${removedLog.topicPartition} in ${removedLog.dir.getAbsolutePath}.\")"
  },
  {
    "id" : "a10d2909-ee78-4fee-bfe8-faf07dec84e4",
    "prId" : 4663,
    "prUrl" : "https://github.com/apache/kafka/pull/4663#pullrequestreview-103264577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b7631f8-e0f5-4b7a-a3e8-d19f452b35ae",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Hmm, if you look at the code in line 726. We already have the logic of delaying the deletion by currentDefaultConfig.fileDeleteDelayMs. So, it's kind of weird to delay it again here.\r\n\r\nI am also not sure if this completely solves the problem since people may customize fileDeleteDelayMs. It would be useful to have a solution regardless of fileDeleteDelayMs and the flush time.",
        "createdAt" : "2018-03-12T23:00:46Z",
        "updatedAt" : "2018-03-13T01:02:28Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "c1b08028-40dd-4065-8354-c57710645ffa",
        "parentId" : "8b7631f8-e0f5-4b7a-a3e8-d19f452b35ae",
        "authorId" : "220f032c-6592-42d9-9042-aed276632816",
        "body" : "@junrao Thanks for the comment Jun. The line 726 determines when broker tries to delete files next time. It is still possible that a file is scheduled for deletion right before the next task is run. And then the file will be deleted immediately instead of waiting for `file.delete.delay.ms`. So it may be reasonable to make this change to correctly enforce `file.delete.delay.ms` according to its Java doc.\r\n\r\nYeah the delay does not completely solve the problem. In the rare cases that log.flush takes more than 60 seconds, broker will still see IOException. In order to prevent this completely, we can either use a flag to avoid IOException, or grab the lock when flushing the log. If we have time, I would recommend we do the performance test with the lock-based approach -- in general we need to do such performance benchmark for each major release anyway. If we don't have time, and we think it is important to completely prevent this (after we have the 60 seconds delay), I will implement the flag-based approach. What do you think?",
        "createdAt" : "2018-03-12T23:16:44Z",
        "updatedAt" : "2018-03-13T01:02:28Z",
        "lastEditedBy" : "220f032c-6592-42d9-9042-aed276632816",
        "tags" : [
        ]
      },
      {
        "id" : "719ca1e3-1006-4438-b592-818dc81f87bb",
        "parentId" : "8b7631f8-e0f5-4b7a-a3e8-d19f452b35ae",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@lindong28 : Thanks for the explanation. It makes sense. In the interest of time, we can just take what you had for now and work on a more complete fix in a separate jira. ",
        "createdAt" : "2018-03-13T00:48:03Z",
        "updatedAt" : "2018-03-13T01:02:28Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f0cd959e9a7c641b1f9bb35a027d347d6ec60d5",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +713,717 @@        if (removedLog != null) {\n          try {\n            val waitingTimeMs = scheduleTimeMs + currentDefaultConfig.fileDeleteDelayMs - time.milliseconds()\n            if (waitingTimeMs > 0)\n              Thread.sleep(waitingTimeMs)"
  },
  {
    "id" : "d3a5f1b4-ad48-4ec1-ae44-5fd42a8fad86",
    "prId" : 5848,
    "prUrl" : "https://github.com/apache/kafka/pull/5848#pullrequestreview-173250903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23e4e7f8-c9c1-4764-92d3-15fcb1176e97",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It's probably better to fold the logic in checkpointLogRecoveryOffsetsInDir(dir: File) here and let all callers go through checkpointRecoveryOffsetsAndCleanSnapshot(). Currently, there are still a couple of callers to  checkpointLogRecoveryOffsetsInDir(dir: File) directly. The issue is that IOException is not handled properly there as in checkpointRecoveryOffsetsAndCleanSnapshot().",
        "createdAt" : "2018-11-09T02:05:26Z",
        "updatedAt" : "2018-11-12T01:02:55Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e796447e049ab4eb9a3f9ba1f1b14791f88705d0",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +594,598 @@  private[log] def checkpointRecoveryOffsetsAndCleanSnapshot(dir: File, logsToCleanSnapshot: Seq[Log]): Unit = {\n    try {\n      checkpointLogRecoveryOffsetsInDir(dir)\n      logsToCleanSnapshot.foreach(_.deleteSnapshotsAfterRecoveryPointCheckpoint())\n    } catch {"
  },
  {
    "id" : "2f47aaf7-2870-4eb7-ab93-1c3b786982ef",
    "prId" : 6841,
    "prUrl" : "https://github.com/apache/kafka/pull/6841#pullrequestreview-250010220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f58b004f-82c4-4173-816e-0737cb404456",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Hmm. Not related to this change but the locking patterns are not clear. At a glance one memory address seems to be protected by multiple locks. In this case that is the `logCreationOrDeletionLock` and the `lock` in `Log`.",
        "createdAt" : "2019-06-13T18:37:34Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "86e2b841-e383-4f4a-a796-53e522886715",
        "parentId" : "f58b004f-82c4-4173-816e-0737cb404456",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Yes. In this case the invocation can hold as many as 3 locks: logCreationOrDeletionLock, leaderIsrUpdateLock and partitionMapLock. While holding many locks isn't bad by itself (increases probablity of things going wrong and deadlocks), I don't have clear sense of data structure protected by them.\r\n\r\nWhile moving code I have avoided changing locking discipline. If it was protected before, it should be protected even after my change and in the same order.",
        "createdAt" : "2019-06-13T21:02:12Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      },
      {
        "id" : "be45b94a-876e-43d7-9edd-795418beb66f",
        "parentId" : "f58b004f-82c4-4173-816e-0737cb404456",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "+1 on not changing the locking here. We'll need to look at this in isolation to have any hope of getting it right. ",
        "createdAt" : "2019-06-13T23:43:45Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ff0edee7-6ccd-4661-8996-353314d48019",
        "parentId" : "f58b004f-82c4-4173-816e-0737cb404456",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Closing as per last comment. Lets take a look at this outside of this PR.",
        "createdAt" : "2019-06-14T16:41:49Z",
        "updatedAt" : "2019-06-14T23:20:28Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6524debfbc275fe7f58c41b87c3daeb3f4c0aa6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +804,808 @@\n      destLog.renameDir(Log.logDirName(topicPartition))\n      destLog.highWatermarkMetadata = sourceLog.highWatermarkMetadata\n      // Now that future replica has been successfully renamed to be the current replica\n      // Update the cached map and log cleaner as appropriate."
  },
  {
    "id" : "bac56330-3cf4-4efe-9ff5-f9b1a902ead6",
    "prId" : 6969,
    "prUrl" : "https://github.com/apache/kafka/pull/6969#pullrequestreview-264559881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e79ba54-3f7c-4272-9c1b-190bb0b1c330",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Unfortunately, the `map` here doesn't work because it will create the partition directory for all directories even when there are no failures. I think we only want to call `createLogDirectory` on a new directory if the previous one failed. ",
        "createdAt" : "2019-07-13T01:59:45Z",
        "updatedAt" : "2019-07-19T17:26:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "7e937ff0-a613-49a3-8161-4009251fb788",
        "parentId" : "4e79ba54-3f7c-4272-9c1b-190bb0b1c330",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "I agree that we only want to call `createLogDirectory` if the previous one failed.",
        "createdAt" : "2019-07-21T23:21:07Z",
        "updatedAt" : "2019-07-21T23:21:07Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      },
      {
        "id" : "e5669e66-3b4b-4bf3-ac38-daf06ea42cb6",
        "parentId" : "4e79ba54-3f7c-4272-9c1b-190bb0b1c330",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Oh this code actually looks right, nm. Since we call find(), it will stop on the first success.",
        "createdAt" : "2019-07-21T23:27:40Z",
        "updatedAt" : "2019-07-21T23:27:40Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d72dd7a04c83ecc8b28654b5d6807792ab373cc",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +703,707 @@        val logDir = logDirs\n          .toStream // to prevent actually mapping the whole list, lazy map\n          .map(createLogDirectory(_, logDirName))\n          .find(_.isSuccess)\n          .getOrElse(Failure(new KafkaStorageException(\"No log directories available. Tried \" + logDirs.map(_.getAbsolutePath).mkString(\", \"))))"
  },
  {
    "id" : "2d71a13b-fc36-478c-81c3-8b472727f52d",
    "prId" : 7305,
    "prUrl" : "https://github.com/apache/kafka/pull/7305#pullrequestreview-301578195",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c5ec8cb-81d9-4d31-bd37-a59e8ebc309a",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Can you add a comment indicating that the value indicates that config was updated while the partition was being initialized?",
        "createdAt" : "2019-10-14T21:47:42Z",
        "updatedAt" : "2019-10-14T22:52:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "838a339d-89e6-4e0d-a499-2773ce2f70e0",
        "parentId" : "4c5ec8cb-81d9-4d31-bd37-a59e8ebc309a",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Done.",
        "createdAt" : "2019-10-14T22:54:05Z",
        "updatedAt" : "2019-10-14T22:54:05Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      }
    ],
    "commit" : "95b1f122353c335ff2f598ad50b4b2c44237580a",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +88,92 @@  // See KAFKA-8813 for more detail on the race condition\n  // Visible for testing\n  private[log] val partitionsInitializing = new ConcurrentHashMap[TopicPartition, Boolean]().asScala\n\n  def reconfigureDefaultLogConfig(logConfig: LogConfig): Unit = {"
  },
  {
    "id" : "a697379e-8f3d-420a-8634-0c1f52e95e60",
    "prId" : 8257,
    "prUrl" : "https://github.com/apache/kafka/pull/8257#pullrequestreview-376279458",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67ccf8c3-3cb4-4705-b3a9-9c609867ca8c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The `iff` was probably intentional, if a bit pedantic. I'm fine with the change though.",
        "createdAt" : "2020-03-17T00:42:05Z",
        "updatedAt" : "2020-04-14T13:16:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "cf221c45-471d-4354-b6a3-24c994fa5bc2",
        "parentId" : "67ccf8c3-3cb4-4705-b3a9-9c609867ca8c",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "Oh.. I did not know that `iff` was actually valid. I thought that it was a typo :)",
        "createdAt" : "2020-03-17T10:24:03Z",
        "updatedAt" : "2020-04-14T13:16:08Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      },
      {
        "id" : "da013ad7-5226-4b8f-858f-33d66b9757ab",
        "parentId" : "67ccf8c3-3cb4-4705-b3a9-9c609867ca8c",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it's a mathematical short-hand for \"if and only if.\" But the \"only if\" part is implicit here.",
        "createdAt" : "2020-03-17T18:02:07Z",
        "updatedAt" : "2020-04-14T13:16:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "49360d6d46bf292c7b9a774be923073fd049c19d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +708,712 @@   * @param config The configuration of the log that should be applied for log creation\n   * @param isNew Whether the replica should have existed on the broker or not\n   * @param isFuture True if the future log of the specified partition should be returned or created\n   * @throws KafkaStorageException if isNew=false, log is not found in the cache and there is offline log directory on the broker\n   */"
  },
  {
    "id" : "94b79c84-f191-44cd-b35f-1d852ce58573",
    "prId" : 9680,
    "prUrl" : "https://github.com/apache/kafka/pull/9680#pullrequestreview-549577307",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b54a8ef-bbaa-419d-a1f1-15ffe0da75c8",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "@chia7712 This changed behavior. Previously we would call `get` on every `Future` and now we don't. @kowshik @junrao Seems like we didn't add a test that verifies the recent fix in this area?",
        "createdAt" : "2020-12-10T13:44:39Z",
        "updatedAt" : "2020-12-10T13:44:39Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "1e1d74b3-02db-4c61-b561-139598418a3b",
        "parentId" : "9b54a8ef-bbaa-419d-a1f1-15ffe0da75c8",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Oh, my bad. Will revert it with suitable test.",
        "createdAt" : "2020-12-10T14:59:19Z",
        "updatedAt" : "2020-12-10T14:59:20Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "2c9837b3-9f5a-4d46-a776-78366ee38548",
        "parentId" : "9b54a8ef-bbaa-419d-a1f1-15ffe0da75c8",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "see #9728",
        "createdAt" : "2020-12-10T17:26:09Z",
        "updatedAt" : "2020-12-10T17:26:10Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "b1e4d058-4f79-4ad4-95ac-446278492a92",
        "parentId" : "9b54a8ef-bbaa-419d-a1f1-15ffe0da75c8",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "@chia7712 @ijuma I think you were referring to https://github.com/apache/kafka/pull/9596 ?\r\n\r\nThe test added in #9596 to verify this was `LogManagerTest.testHandlingExceptionsDuringShutdown` ([link](https://github.com/apache/kafka/blob/dcbd28da53bfc6edb78fd1735993163fc6f4c7c7/core/src/test/scala/unit/kafka/log/LogManagerTest.scala#L94)). I agree that the test wasn't effective enough to catch the problem. The issue was that fundamentally this area of the code was a little bit difficult to test due to the asynchronous behavior.\r\n\r\nSo for example, although @chia7712 has a fix in #9728, I still think it is not easy to write a test that reliably fails if the bug is introduced again.\r\n\r\nBelow is my explanation on why it is difficult to improve the existing `testHandlingExceptionsDuringShutdown` to catch such issues. Let us imagine that the test setup introduced multiple logs for each log dir, and setup an expectation (in the test code) that if one of the logs failed during `close()`, then, the other logs under the same log directory should still be closed by the time `LogManager.shutdown()` returns. Such a kind of test expectation would have caught the problem introduced in this PR. Now, writing that kind of a test was not easy, because, even if one of the logs failed to close, the other futures can still be completed by the time `LogManager.shutdown()` returns (there is no strong need to call `future.get` for the future to pass). Thus, the test can pass despite the bug.\r\n\r\nOne potential solution (a bigger change) is to see if we can inject a different thread pool executor that only during test executes the threads lazily i.e. it executes the submitted threads only when `future.get` is called, otherwise it will never execute them. If we succeed in doing this, then, the test can be improved such that unless `future.get` is called on all logs, some of the log `close()` will never happen and will fail the test in the presence of the bug introduced by this PR.\r\n\r\ncc @junrao ",
        "createdAt" : "2020-12-10T20:31:19Z",
        "updatedAt" : "2020-12-10T20:31:55Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "ed2f6d10c2b8e47bb2da31349565fb84e9e770af",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +487,491 @@              true\n          }\n        }\n\n        if (!hasErrors) {"
  }
]