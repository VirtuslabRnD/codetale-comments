[
  {
    "id" : "d379f64f-0835-41f3-afe8-1566461deb46",
    "prId" : 6785,
    "prUrl" : "https://github.com/apache/kafka/pull/6785#pullrequestreview-247340535",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c239631-105a-4e80-a393-9862f186f09a",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Could be simplified as \r\n```\r\nif (batchIterator.hasNext && \r\n!(batch.magic < RecordBatch.MAGIC_VALUE_V2 && sourceCodec == NoCompressionCodec)) {\r\n  throw new InvalidRecordException(\"Compressed outer record has more than one batch\")\r\n} else {\r\n  batch\r\n}\r\n```\r\n",
        "createdAt" : "2019-06-04T18:20:39Z",
        "updatedAt" : "2019-06-21T01:39:59Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "573441ec-db48-429a-b10c-c0dbf87785e3",
        "parentId" : "0c239631-105a-4e80-a393-9862f186f09a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I feel it is easier to read in this way, though a bit verbose.",
        "createdAt" : "2019-06-07T23:37:21Z",
        "updatedAt" : "2019-06-21T01:39:59Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2877059086b62a6a36170174e0d2c924a1a6503",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +85,89 @@\n    // if the format is v2 and beyond, or if the messages are compressed, we should check there's only one batch.\n    if (batch.magic() >= RecordBatch.MAGIC_VALUE_V2 || sourceCodec != NoCompressionCodec) {\n      if (batchIterator.hasNext) {\n        throw new InvalidRecordException(\"Compressed outer record has more than one batch\")"
  },
  {
    "id" : "1e54e266-fa27-4b3a-966e-045e299ebc20",
    "prId" : 6816,
    "prUrl" : "https://github.com/apache/kafka/pull/6816#pullrequestreview-242893943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3597f92-7ee3-413f-b609-1c0487ee624d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do you think it's worth calling `hasNext()` first so that we can print a nice exception if the buffer reaches this point empty? I think we have a check for emptiness already in `analyzeAndValidateRecords`, but maybe no harm being a little defensive.",
        "createdAt" : "2019-05-28T20:23:23Z",
        "updatedAt" : "2019-05-29T04:35:12Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "7fa8c5522d00b97583dd5d8dd54b1391e209625a",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +83,87 @@    }\n\n    val batch = batchIterator.next()\n\n    if (batchIterator.hasNext) {"
  },
  {
    "id" : "660928be-c84e-49c3-833a-f5ec6e8ba821",
    "prId" : 7142,
    "prUrl" : "https://github.com/apache/kafka/pull/7142#pullrequestreview-283832953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6699cf86-fde8-4f88-8100-9cb2e418c757",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "In line 104,, 112, 119, 124 and 131 137 152 and 461 we should also include the topic-partition information.",
        "createdAt" : "2019-09-04T18:57:48Z",
        "updatedAt" : "2019-09-04T19:21:52Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "31e48845-02df-4b0e-b424-c17dacb64d7e",
        "parentId" : "6699cf86-fde8-4f88-8100-9cb2e418c757",
        "authorId" : "38aea9c5-d7f1-4e61-920e-b35370a0109c",
        "body" : "ack",
        "createdAt" : "2019-09-04T19:08:00Z",
        "updatedAt" : "2019-09-04T19:21:52Z",
        "lastEditedBy" : "38aea9c5-d7f1-4e61-920e-b35370a0109c",
        "tags" : [
        ]
      }
    ],
    "commit" : "09a79610109d36eaca221f0651cc74d53aa26440",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +101,105 @@    // batch magic byte should have the same magic as the first batch\n    if (firstBatch.magic() != batch.magic()) {\n      brokerTopicStats.allTopicsStats.invalidMagicNumberRecordsPerSec.mark()\n      throw new InvalidRecordException(s\"Batch magic ${batch.magic()} is not the same as the first batch'es magic byte ${firstBatch.magic()} in topic partition $topicPartition.\")\n    }"
  },
  {
    "id" : "2f7e4a99-e477-4620-9b0c-956c021e4e8f",
    "prId" : 7167,
    "prUrl" : "https://github.com/apache/kafka/pull/7167#pullrequestreview-297715595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c7b7706-7cdf-42d8-9108-a5eca8fdae1d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I am not sure why we don't do this validation for both compressed and non-compressed batches. Seems like we should.",
        "createdAt" : "2019-10-04T21:14:32Z",
        "updatedAt" : "2019-10-09T03:47:51Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca821c98f7e78ea2dadf8271000d4d084d1be6f5",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +378,382 @@            // inner records offset should always be continuous\n            val expectedOffset = expectedInnerOffset.getAndIncrement()\n            if (record.offset != expectedOffset) {\n              brokerTopicStats.allTopicsStats.invalidOffsetOrSequenceRecordsPerSec.mark()\n              throw new RecordValidationException("
  },
  {
    "id" : "cf474404-a21f-4b03-9e19-69bcfca5478c",
    "prId" : 7167,
    "prUrl" : "https://github.com/apache/kafka/pull/7167#pullrequestreview-299679539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5e7ad44-75e5-4053-8c38-7ae14f84fe3d",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Nice catch here.",
        "createdAt" : "2019-10-09T20:32:45Z",
        "updatedAt" : "2019-10-09T21:02:11Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca821c98f7e78ea2dadf8271000d4d084d1be6f5",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +254,258 @@        val expectedOffset = expectedInnerOffset.getAndIncrement()\n\n        // inner records offset should always be continuous\n        if (record.offset != expectedOffset) {\n          brokerTopicStats.allTopicsStats.invalidOffsetOrSequenceRecordsPerSec.mark()"
  }
]