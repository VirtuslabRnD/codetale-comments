[
  {
    "id" : "f2fd6acf-e70c-4aff-9408-e47bd9614fa0",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337249973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7061df7f-c43c-4937-8b9a-6e519ec25427",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Foo the case where we log, should we indicate that we're ignoring it and whether this is safe or the user has to take some action?",
        "createdAt" : "2019-12-11T13:58:23Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "7ae11ce1-84da-4c2c-b99c-cdc49c9d6a1f",
        "parentId" : "7061df7f-c43c-4937-8b9a-6e519ec25427",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The damage (if any) is sort of already done at this point. I will change the level to INFO. I debated even using DEBUG, but I think this case is rare enough that we'd want to be sure it makes it to the logs.",
        "createdAt" : "2019-12-30T21:55:35Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +289,293 @@        info(s\"Detected invalid coordinator epoch for producerId $producerId at \" +\n          s\"offset $offset in partition $topicPartition: ${endTxnMarker.coordinatorEpoch} \" +\n          s\"is older than previously known coordinator epoch ${updatedEntry.coordinatorEpoch}\")\n      } else {\n        throw new TransactionCoordinatorFencedException(s\"Invalid coordinator epoch for producerId $producerId at \" +"
  },
  {
    "id" : "b167c1ca-ffe1-4e75-ab04-70e2b66d3bb5",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-340012720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This maybe out of the scope of this PR but I found the call trace of `addBatch` and `update` are basically the same: the latter is from recover, append, and load, and the former is also from append and load, can we consolidate them into a single function then, or are there any scenarios that would only get on but not the other?",
        "createdAt" : "2019-12-11T20:09:07Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0ee0338d-59af-4ff5-8cef-7aba1ba1cfc2",
        "parentId" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it's a fair point. I looked into this, but found it not very straightforward. If it's ok, let's do this in a follow-up.",
        "createdAt" : "2019-12-30T21:40:50Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "71f209a6-fe03-443c-b60f-40dde9cef052",
        "parentId" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "SG",
        "createdAt" : "2020-01-08T17:09:05Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +125,129 @@\n  def update(nextEntry: ProducerStateEntry): Unit = {\n    maybeUpdateProducerEpoch(nextEntry.producerEpoch)\n    while (nextEntry.batchMetadata.nonEmpty)\n      addBatchMetadata(nextEntry.batchMetadata.dequeue())"
  },
  {
    "id" : "d8ab1f08-b0a9-451d-a465-b4575dcec557",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-340012938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: maybe rename this `append` to `appendRecordBatch`?",
        "createdAt" : "2019-12-11T20:31:05Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "99413305-fafe-4382-9cc1-a91486751d8a",
        "parentId" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I went with `appendDataBatch`. Sound ok?",
        "createdAt" : "2019-12-30T21:46:18Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "81d2d6ac-1ae9-45ff-8e3c-26006aa739b4",
        "parentId" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yea",
        "createdAt" : "2020-01-08T17:09:27Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 211,
    "diffHunk" : "@@ -1,1 +266,270 @@                      isTransactional: Boolean): Unit = {\n    val firstOffset = firstOffsetMetadata.messageOffset\n    maybeValidateDataBatch(epoch, firstSeq, firstOffset)\n    updatedEntry.addBatch(epoch, lastSeq, lastOffset, (lastOffset - firstOffset).toInt, lastTimestamp)\n"
  },
  {
    "id" : "e225a832-d20f-46e0-99ae-5c508f63450d",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337250450",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "003e9bcf-ca92-4770-8b1a-30fdce048c10",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is another behavior change that is not covered in the description, is that a piggy-backed cleanup too?",
        "createdAt" : "2019-12-11T20:39:20Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "e3dc6da9-f9ae-45f5-b5f0-728c4ce3f667",
        "parentId" : "003e9bcf-ca92-4770-8b1a-30fdce048c10",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, another issue I found during testing. I will mention this in the description.",
        "createdAt" : "2019-12-30T21:57:32Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 285,
    "diffHunk" : "@@ -1,1 +395,399 @@        val currentTxnFirstOffset = producerEntryStruct.getLong(CurrentTxnFirstOffsetField)\n        val lastAppendedDataBatches = mutable.Queue.empty[BatchMetadata]\n        if (offset >= 0)\n          lastAppendedDataBatches += BatchMetadata(seq, offset, offsetDelta, timestamp)\n"
  },
  {
    "id" : "1a10f25f-fe74-4959-b4d6-20fbc1e8408d",
    "prId" : 7929,
    "prUrl" : "https://github.com/apache/kafka/pull/7929#pullrequestreview-505075905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1929efa3-f0fe-42d3-ba04-16d23b2bfabd",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we also make sure that the offset for latestStraySnapshot is > the largest offset in segmentBaseOffsets?\r\n",
        "createdAt" : "2020-10-07T21:45:31Z",
        "updatedAt" : "2020-10-09T13:34:11Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "8ae12923-f6a9-4697-8d19-72687b9f8cc5",
        "parentId" : "1929efa3-f0fe-42d3-ba04-16d23b2bfabd",
        "authorId" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "body" : "We perform a check below which may cover this case. After setting the `snapshots` map, we look at the latest snapshot in the map. If the latest snapshot in the map is not equal to the `latestStraySnapshot`, we delete the `latestStraySnapshot`. \r\n\r\nI think this is a bit confusing though, so it might be better if instead we directly check that the `latestStraySnapshot` is larger than the largest offset in `segmentBaseOffsets`. ",
        "createdAt" : "2020-10-08T18:56:04Z",
        "updatedAt" : "2020-10-09T13:34:11Z",
        "lastEditedBy" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae99b4ebf20875bf69ce4d992586cc5707c6a0ee",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +529,533 @@        case None =>\n          if (!baseOffsets.contains(key)) {\n            latestStraySnapshot = Some(snapshot)\n          }\n      }"
  }
]