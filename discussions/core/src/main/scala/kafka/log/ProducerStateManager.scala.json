[
  {
    "id" : "f2fd6acf-e70c-4aff-9408-e47bd9614fa0",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337249973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7061df7f-c43c-4937-8b9a-6e519ec25427",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Foo the case where we log, should we indicate that we're ignoring it and whether this is safe or the user has to take some action?",
        "createdAt" : "2019-12-11T13:58:23Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "7ae11ce1-84da-4c2c-b99c-cdc49c9d6a1f",
        "parentId" : "7061df7f-c43c-4937-8b9a-6e519ec25427",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The damage (if any) is sort of already done at this point. I will change the level to INFO. I debated even using DEBUG, but I think this case is rare enough that we'd want to be sure it makes it to the logs.",
        "createdAt" : "2019-12-30T21:55:35Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +289,293 @@        info(s\"Detected invalid coordinator epoch for producerId $producerId at \" +\n          s\"offset $offset in partition $topicPartition: ${endTxnMarker.coordinatorEpoch} \" +\n          s\"is older than previously known coordinator epoch ${updatedEntry.coordinatorEpoch}\")\n      } else {\n        throw new TransactionCoordinatorFencedException(s\"Invalid coordinator epoch for producerId $producerId at \" +"
  },
  {
    "id" : "b167c1ca-ffe1-4e75-ab04-70e2b66d3bb5",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-340012720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This maybe out of the scope of this PR but I found the call trace of `addBatch` and `update` are basically the same: the latter is from recover, append, and load, and the former is also from append and load, can we consolidate them into a single function then, or are there any scenarios that would only get on but not the other?",
        "createdAt" : "2019-12-11T20:09:07Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0ee0338d-59af-4ff5-8cef-7aba1ba1cfc2",
        "parentId" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it's a fair point. I looked into this, but found it not very straightforward. If it's ok, let's do this in a follow-up.",
        "createdAt" : "2019-12-30T21:40:50Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "71f209a6-fe03-443c-b60f-40dde9cef052",
        "parentId" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "SG",
        "createdAt" : "2020-01-08T17:09:05Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +125,129 @@\n  def update(nextEntry: ProducerStateEntry): Unit = {\n    maybeUpdateProducerEpoch(nextEntry.producerEpoch)\n    while (nextEntry.batchMetadata.nonEmpty)\n      addBatchMetadata(nextEntry.batchMetadata.dequeue())"
  },
  {
    "id" : "d8ab1f08-b0a9-451d-a465-b4575dcec557",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-340012938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: maybe rename this `append` to `appendRecordBatch`?",
        "createdAt" : "2019-12-11T20:31:05Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "99413305-fafe-4382-9cc1-a91486751d8a",
        "parentId" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I went with `appendDataBatch`. Sound ok?",
        "createdAt" : "2019-12-30T21:46:18Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "81d2d6ac-1ae9-45ff-8e3c-26006aa739b4",
        "parentId" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yea",
        "createdAt" : "2020-01-08T17:09:27Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 211,
    "diffHunk" : "@@ -1,1 +266,270 @@                      isTransactional: Boolean): Unit = {\n    val firstOffset = firstOffsetMetadata.messageOffset\n    maybeValidateDataBatch(epoch, firstSeq, firstOffset)\n    updatedEntry.addBatch(epoch, lastSeq, lastOffset, (lastOffset - firstOffset).toInt, lastTimestamp)\n"
  },
  {
    "id" : "e225a832-d20f-46e0-99ae-5c508f63450d",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337250450",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "003e9bcf-ca92-4770-8b1a-30fdce048c10",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is another behavior change that is not covered in the description, is that a piggy-backed cleanup too?",
        "createdAt" : "2019-12-11T20:39:20Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "e3dc6da9-f9ae-45f5-b5f0-728c4ce3f667",
        "parentId" : "003e9bcf-ca92-4770-8b1a-30fdce048c10",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, another issue I found during testing. I will mention this in the description.",
        "createdAt" : "2019-12-30T21:57:32Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 285,
    "diffHunk" : "@@ -1,1 +395,399 @@        val currentTxnFirstOffset = producerEntryStruct.getLong(CurrentTxnFirstOffsetField)\n        val lastAppendedDataBatches = mutable.Queue.empty[BatchMetadata]\n        if (offset >= 0)\n          lastAppendedDataBatches += BatchMetadata(seq, offset, offsetDelta, timestamp)\n"
  },
  {
    "id" : "1a10f25f-fe74-4959-b4d6-20fbc1e8408d",
    "prId" : 7929,
    "prUrl" : "https://github.com/apache/kafka/pull/7929#pullrequestreview-505075905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1929efa3-f0fe-42d3-ba04-16d23b2bfabd",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we also make sure that the offset for latestStraySnapshot is > the largest offset in segmentBaseOffsets?\r\n",
        "createdAt" : "2020-10-07T21:45:31Z",
        "updatedAt" : "2020-10-09T13:34:11Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "8ae12923-f6a9-4697-8d19-72687b9f8cc5",
        "parentId" : "1929efa3-f0fe-42d3-ba04-16d23b2bfabd",
        "authorId" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "body" : "We perform a check below which may cover this case. After setting the `snapshots` map, we look at the latest snapshot in the map. If the latest snapshot in the map is not equal to the `latestStraySnapshot`, we delete the `latestStraySnapshot`. \r\n\r\nI think this is a bit confusing though, so it might be better if instead we directly check that the `latestStraySnapshot` is larger than the largest offset in `segmentBaseOffsets`. ",
        "createdAt" : "2020-10-08T18:56:04Z",
        "updatedAt" : "2020-10-09T13:34:11Z",
        "lastEditedBy" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae99b4ebf20875bf69ce4d992586cc5707c6a0ee",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +529,533 @@        case None =>\n          if (!baseOffsets.contains(key)) {\n            latestStraySnapshot = Some(snapshot)\n          }\n      }"
  },
  {
    "id" : "d507a23a-a9d3-45af-a28e-24eda54d2146",
    "prId" : 9569,
    "prUrl" : "https://github.com/apache/kafka/pull/9569#pullrequestreview-529776045",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea3e1beb-9511-4472-8026-7f67fbb353de",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Let's add a comment why we change this to the InvalidProducerEpochException.",
        "createdAt" : "2020-11-13T06:17:24Z",
        "updatedAt" : "2020-11-17T23:15:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8def4510257b11cd1a4caaeb916944c44b3d0ec3",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +207,211 @@        // producer send response callback to differentiate from the former fatal exception,\n        // letting client abort the ongoing transaction and retry.\n        throw new InvalidProducerEpochException(message)\n      }\n    }"
  },
  {
    "id" : "157127f5-df7d-42b2-8b49-d1865e6e9dcb",
    "prId" : 9632,
    "prUrl" : "https://github.com/apache/kafka/pull/9632#pullrequestreview-535822820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e62ce3de-31c8-4eb5-b74a-100254a51dd7",
        "parentId" : null,
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "Could you check my understanding? If we have a a non-empty currentTxnFirstOffset value (indicating a non-empty transaction), we'll return a valid CompletedTxn, otherwise we will return None. For the empty transactions this means that we aren't accumulating completed transactions. This saves us from having to call lastStableOffset on every empty completed transaction https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/Log.scala#L1240?",
        "createdAt" : "2020-11-20T22:35:44Z",
        "updatedAt" : "2020-11-30T19:59:36Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      },
      {
        "id" : "0dd64ff1-4bda-4f6f-bd63-93f535a4335f",
        "parentId" : "e62ce3de-31c8-4eb5-b74a-100254a51dd7",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, that is right. Additionally, we are not adding the transaction to the list of started transactions which are accumulated in the `ProducerAppendInfo`.",
        "createdAt" : "2020-11-20T22:37:53Z",
        "updatedAt" : "2020-11-30T19:59:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "99894fcfa96d090e8042085a45227b94d07fc872",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +318,322 @@    // and would not need to be reflected in the transaction index.\n    val completedTxn = updatedEntry.currentTxnFirstOffset.map { firstOffset =>\n      CompletedTxn(producerId, firstOffset, offset, endTxnMarker.controlType == ControlRecordType.ABORT)\n    }\n"
  }
]