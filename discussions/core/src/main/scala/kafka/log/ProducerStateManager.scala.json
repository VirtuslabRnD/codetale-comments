[
  {
    "id" : "f2fd6acf-e70c-4aff-9408-e47bd9614fa0",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337249973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7061df7f-c43c-4937-8b9a-6e519ec25427",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Foo the case where we log, should we indicate that we're ignoring it and whether this is safe or the user has to take some action?",
        "createdAt" : "2019-12-11T13:58:23Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "7ae11ce1-84da-4c2c-b99c-cdc49c9d6a1f",
        "parentId" : "7061df7f-c43c-4937-8b9a-6e519ec25427",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The damage (if any) is sort of already done at this point. I will change the level to INFO. I debated even using DEBUG, but I think this case is rare enough that we'd want to be sure it makes it to the logs.",
        "createdAt" : "2019-12-30T21:55:35Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +289,293 @@        info(s\"Detected invalid coordinator epoch for producerId $producerId at \" +\n          s\"offset $offset in partition $topicPartition: ${endTxnMarker.coordinatorEpoch} \" +\n          s\"is older than previously known coordinator epoch ${updatedEntry.coordinatorEpoch}\")\n      } else {\n        throw new TransactionCoordinatorFencedException(s\"Invalid coordinator epoch for producerId $producerId at \" +"
  },
  {
    "id" : "b167c1ca-ffe1-4e75-ab04-70e2b66d3bb5",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-340012720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This maybe out of the scope of this PR but I found the call trace of `addBatch` and `update` are basically the same: the latter is from recover, append, and load, and the former is also from append and load, can we consolidate them into a single function then, or are there any scenarios that would only get on but not the other?",
        "createdAt" : "2019-12-11T20:09:07Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0ee0338d-59af-4ff5-8cef-7aba1ba1cfc2",
        "parentId" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it's a fair point. I looked into this, but found it not very straightforward. If it's ok, let's do this in a follow-up.",
        "createdAt" : "2019-12-30T21:40:50Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "71f209a6-fe03-443c-b60f-40dde9cef052",
        "parentId" : "7c4f38ff-44b9-41be-a6e2-1de3b4397e9b",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "SG",
        "createdAt" : "2020-01-08T17:09:05Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +125,129 @@\n  def update(nextEntry: ProducerStateEntry): Unit = {\n    maybeUpdateProducerEpoch(nextEntry.producerEpoch)\n    while (nextEntry.batchMetadata.nonEmpty)\n      addBatchMetadata(nextEntry.batchMetadata.dequeue())"
  },
  {
    "id" : "d8ab1f08-b0a9-451d-a465-b4575dcec557",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-340012938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nit: maybe rename this `append` to `appendRecordBatch`?",
        "createdAt" : "2019-12-11T20:31:05Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "99413305-fafe-4382-9cc1-a91486751d8a",
        "parentId" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I went with `appendDataBatch`. Sound ok?",
        "createdAt" : "2019-12-30T21:46:18Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "81d2d6ac-1ae9-45ff-8e3c-26006aa739b4",
        "parentId" : "f470d912-dcdd-4f96-9437-f11b4a27e9fe",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yea",
        "createdAt" : "2020-01-08T17:09:27Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 211,
    "diffHunk" : "@@ -1,1 +266,270 @@                      isTransactional: Boolean): Unit = {\n    val firstOffset = firstOffsetMetadata.messageOffset\n    maybeValidateDataBatch(epoch, firstSeq, firstOffset)\n    updatedEntry.addBatch(epoch, lastSeq, lastOffset, (lastOffset - firstOffset).toInt, lastTimestamp)\n"
  },
  {
    "id" : "e225a832-d20f-46e0-99ae-5c508f63450d",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-337250450",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "003e9bcf-ca92-4770-8b1a-30fdce048c10",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is another behavior change that is not covered in the description, is that a piggy-backed cleanup too?",
        "createdAt" : "2019-12-11T20:39:20Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "e3dc6da9-f9ae-45f5-b5f0-728c4ce3f667",
        "parentId" : "003e9bcf-ca92-4770-8b1a-30fdce048c10",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, another issue I found during testing. I will mention this in the description.",
        "createdAt" : "2019-12-30T21:57:32Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 285,
    "diffHunk" : "@@ -1,1 +395,399 @@        val currentTxnFirstOffset = producerEntryStruct.getLong(CurrentTxnFirstOffsetField)\n        val lastAppendedDataBatches = mutable.Queue.empty[BatchMetadata]\n        if (offset >= 0)\n          lastAppendedDataBatches += BatchMetadata(seq, offset, offsetDelta, timestamp)\n"
  },
  {
    "id" : "1a10f25f-fe74-4959-b4d6-20fbc1e8408d",
    "prId" : 7929,
    "prUrl" : "https://github.com/apache/kafka/pull/7929#pullrequestreview-505075905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1929efa3-f0fe-42d3-ba04-16d23b2bfabd",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we also make sure that the offset for latestStraySnapshot is > the largest offset in segmentBaseOffsets?\r\n",
        "createdAt" : "2020-10-07T21:45:31Z",
        "updatedAt" : "2020-10-09T13:34:11Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "8ae12923-f6a9-4697-8d19-72687b9f8cc5",
        "parentId" : "1929efa3-f0fe-42d3-ba04-16d23b2bfabd",
        "authorId" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "body" : "We perform a check below which may cover this case. After setting the `snapshots` map, we look at the latest snapshot in the map. If the latest snapshot in the map is not equal to the `latestStraySnapshot`, we delete the `latestStraySnapshot`. \r\n\r\nI think this is a bit confusing though, so it might be better if instead we directly check that the `latestStraySnapshot` is larger than the largest offset in `segmentBaseOffsets`. ",
        "createdAt" : "2020-10-08T18:56:04Z",
        "updatedAt" : "2020-10-09T13:34:11Z",
        "lastEditedBy" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae99b4ebf20875bf69ce4d992586cc5707c6a0ee",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +529,533 @@        case None =>\n          if (!baseOffsets.contains(key)) {\n            latestStraySnapshot = Some(snapshot)\n          }\n      }"
  },
  {
    "id" : "d507a23a-a9d3-45af-a28e-24eda54d2146",
    "prId" : 9569,
    "prUrl" : "https://github.com/apache/kafka/pull/9569#pullrequestreview-529776045",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea3e1beb-9511-4472-8026-7f67fbb353de",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Let's add a comment why we change this to the InvalidProducerEpochException.",
        "createdAt" : "2020-11-13T06:17:24Z",
        "updatedAt" : "2020-11-17T23:15:10Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8def4510257b11cd1a4caaeb916944c44b3d0ec3",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +207,211 @@        // producer send response callback to differentiate from the former fatal exception,\n        // letting client abort the ongoing transaction and retry.\n        throw new InvalidProducerEpochException(message)\n      }\n    }"
  },
  {
    "id" : "157127f5-df7d-42b2-8b49-d1865e6e9dcb",
    "prId" : 9632,
    "prUrl" : "https://github.com/apache/kafka/pull/9632#pullrequestreview-535822820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e62ce3de-31c8-4eb5-b74a-100254a51dd7",
        "parentId" : null,
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "Could you check my understanding? If we have a a non-empty currentTxnFirstOffset value (indicating a non-empty transaction), we'll return a valid CompletedTxn, otherwise we will return None. For the empty transactions this means that we aren't accumulating completed transactions. This saves us from having to call lastStableOffset on every empty completed transaction https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/Log.scala#L1240?",
        "createdAt" : "2020-11-20T22:35:44Z",
        "updatedAt" : "2020-11-30T19:59:36Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      },
      {
        "id" : "0dd64ff1-4bda-4f6f-bd63-93f535a4335f",
        "parentId" : "e62ce3de-31c8-4eb5-b74a-100254a51dd7",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, that is right. Additionally, we are not adding the transaction to the list of started transactions which are accumulated in the `ProducerAppendInfo`.",
        "createdAt" : "2020-11-20T22:37:53Z",
        "updatedAt" : "2020-11-30T19:59:36Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "99894fcfa96d090e8042085a45227b94d07fc872",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +318,322 @@    // and would not need to be reflected in the transaction index.\n    val completedTxn = updatedEntry.currentTxnFirstOffset.map { firstOffset =>\n      CompletedTxn(producerId, firstOffset, offset, endTxnMarker.controlType == ControlRecordType.ABORT)\n    }\n"
  },
  {
    "id" : "37565658-434e-45a7-adea-0092877f8354",
    "prId" : 9827,
    "prUrl" : "https://github.com/apache/kafka/pull/9827#pullrequestreview-562915630",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9912bbdf-134e-45cc-85f5-3298b75cff18",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "it needs a space.",
        "createdAt" : "2021-01-06T17:47:56Z",
        "updatedAt" : "2021-01-07T10:50:56Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "c979bd1e1c0b16659807505afa9ad5b4b6314088",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +203,207 @@    if (producerEpoch < updatedEntry.producerEpoch) {\n      val message = s\"Epoch of producer $producerId at offset $offset in $topicPartition is $producerEpoch, \" +\n        s\"which is smaller than the last seen epoch ${updatedEntry.producerEpoch}\"\n\n      if (origin == AppendOrigin.Replication) {"
  },
  {
    "id" : "bd3de1fc-41bd-4047-8a41-02fab41c957d",
    "prId" : 9827,
    "prUrl" : "https://github.com/apache/kafka/pull/9827#pullrequestreview-562915630",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2abc7a1-4c4f-41ca-b20f-8c8f053100db",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "It seems to me ```updatedEntry.producerEpoch``` is also useful for this error.",
        "createdAt" : "2021-01-06T17:54:25Z",
        "updatedAt" : "2021-01-07T10:50:56Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "c979bd1e1c0b16659807505afa9ad5b4b6314088",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +220,224 @@      if (appendFirstSeq != 0) {\n        if (updatedEntry.producerEpoch != RecordBatch.NO_PRODUCER_EPOCH) {\n          throw new OutOfOrderSequenceException(s\"Invalid sequence number for new epoch of producer $producerId \" +\n            s\"at offset $offset in partition $topicPartition: $producerEpoch (request epoch), $appendFirstSeq (seq. number), \" +\n            s\"${updatedEntry.producerEpoch} (current producer epoch)\")"
  },
  {
    "id" : "5819024c-f0ef-4343-8f46-df4c1d5933ad",
    "prId" : 10896,
    "prUrl" : "https://github.com/apache/kafka/pull/10896#pullrequestreview-688999009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "parentId" : null,
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "I'm not entirely sure I understood this comment. Looking at the `LogLoader` code, it doesn't appear that we use the [intermediate](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogLoader.scala#L327) producer state manager to issue async deletions of snapshot files.\r\n\r\nSo, is it still possible that a missing file is a valid case?",
        "createdAt" : "2021-06-18T08:48:25Z",
        "updatedAt" : "2021-06-18T08:49:29Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "f255e00d-87fe-4c6c-8366-ad84b9d12d4b",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Agree with Kowshik. It's not clear if the comment is still valid.",
        "createdAt" : "2021-06-21T16:01:31Z",
        "updatedAt" : "2021-06-21T16:13:44Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "d65eac92-d16a-41ae-88b7-2201ac676855",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "body" : "@junrao @kowshik I accidentally included the word `async` here, the deletion performed by the intermediate ProducerStateManager is done synchronously.\r\n\r\nI'm referring to the case where we go through `LogLoader.recoverSegment`. We construct a new \"intermediate\" `ProducerStateManager` for segment recovery which is separate from the \"real\" `ProducerStateManager` captured in `LoadLogParams`. \r\n\r\nSegment recovery can use the intermediate `ProducerStateManager` to truncate snapshot files via `ProducerStateManager.truncateAndReload` in `Log.rebuildProducerState`. The `SnapshotFile` instances will be removed from the in-memory map for the \"intermediate\" `ProducerStateManager` in this case, but will remain for the \"real\" `ProducerStateManager` captured in `LoadLogParams`. \r\n\r\n",
        "createdAt" : "2021-06-21T16:43:38Z",
        "updatedAt" : "2021-06-21T16:43:38Z",
        "lastEditedBy" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "tags" : [
        ]
      },
      {
        "id" : "73bb0772-e9f7-45dc-8eaf-e6ae2f430646",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@gardnervickers : Good point. It seems that LogLoader.recoverSegment() can both remove and add snapshots, both of which will be missing in the \"real\" ProducerStateManager captured in LoadLogParams. This can lead to the missing file issue you pointed out and also potentially cause LogLoad.load() to do an unnecessary expensive Log.rebuildProducerState().\r\n\r\n@kowshik : I am wondering if we should let LoadLoader reload the snapshots in the \"real\" ProducerStateManager before calling Log.rebuildProducerState() in LogLoad.load().",
        "createdAt" : "2021-06-21T21:24:10Z",
        "updatedAt" : "2021-06-21T21:24:56Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "291adca8-cc34-4e71-8000-d05615bc6d3a",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@gardnervickers : @kowshik mentioned that params.producerStateManager.removeStraySnapshots(params.segments.baseOffsets.toSeq)\r\nin LogLoad.load() actually reloads the snapshots after log recovery. So, it seems that the issue you mentioned may not be a problem?",
        "createdAt" : "2021-06-21T21:50:32Z",
        "updatedAt" : "2021-06-21T21:51:03Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "faadf03b-87bd-4df9-815d-cce9da295b73",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "body" : "@kowshik @junrao Right, we'll still end up with a a fully in-sync `ProducerStateManager` after `LogLoader.load(..)` runs. \r\n\r\nThe problem can still occur though because we delete snapshots using both the \"intermediate\" and \"real\" `ProducerStateManager` prior to `removeStraySnapshots` at the end of `LogLoader.load`. \r\n\r\n1. `recoverSegment` can delete snapshots with the intermediate `ProducerStateManager`\r\n2. `removeAndDeleteSegmentsAsync`Â will use the \"real\" `ProducerStateManager` to schedule async deletion. It may have a stale view of the present snapshots on the filesystem if #1 deleted snapshots, causing the rename to fail.\r\n3. At the end of `LogLoader.load`, we will `removeStraySnapshots`, which will fix up any discrepancies between the contents of the log dir and the \"real\" `ProducerStateManager`.\r\n",
        "createdAt" : "2021-06-21T22:35:14Z",
        "updatedAt" : "2021-06-21T22:35:14Z",
        "lastEditedBy" : "851026f6-ef9e-43cc-bb20-4145295d1b95",
        "tags" : [
        ]
      },
      {
        "id" : "4479ef86-0260-41c7-ba38-abf5780fb2a6",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Lets assume `PSM` refers to `ProducerStateManager`.\r\n\r\n@junrao @gardnervickers That feels right to me, thanks for the explanation! Couple things I wanted to ask:\r\n\r\n1. Should we update the comment [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogLoader.scala#L144-L146) to say:\r\n\r\n```\r\n// Reload all snapshots into the ProducerStateManager cache, the intermediate ProducerStateManager used\r\n// during log recovery may have created or deleted some snapshots\r\n// without the LoadLogParams.producerStateManager instance witnessing the changes.\r\n```\r\n2. `PSM.removeStraySnapshots` and its params could have a better name. Should we call it differently, like `PSM.reloadEssentialSnapshots(essentialSegmentBaseOffsets: Seq[Long])`?\r\n\r\n=== SUMMARY OF CASES ===\r\n\r\nI thought it's useful to summarize. There are few different cases that arise whenever`PSM.removeAndMarkSnapshotForDeletion()` is invoked on the \"real\" PSM instance. I believe all cases are handled with the current code as explained below:\r\n\r\n**Straightforward cases:**\r\n1. Snapshot entry is present in real `PSM` instance and snapshot file is present. This is a straightforward case where we remove the entry and rename the file.\r\n2. Snapshot entry is absent in real `PSM` instance and snapshot file is absent. This is also a more straightforward case where we do nothing.\r\n\r\n**Corner cases:**\r\n1. Snapshot entry is present in the real `PSM` instance, but snapshot file absent. This can happen because intermediate PSM deleted the snapshot file. In this case, we ignore the failure in the file rename.\r\n2. Snapshot entry is absent in the real `PSM` instance, but snapshot file present. This can happen when intermediate PSM takes a snapshot, but the real `PSM` doesn't have the entry (yet). This is handled by the call to `PSM.removeStraySnapshots` [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogLoader.scala#L147) which corrects such discrepancies by loading all snapshots from disk and eliminating those that don't match the list of segment base offsets post recovery.",
        "createdAt" : "2021-06-21T23:54:21Z",
        "updatedAt" : "2021-06-22T00:26:11Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "37fd20b8-1fc8-4036-ad51-d60d1acc8fe6",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@gardnervickers : Thanks for the explanation. Makes sense.\r\n\r\n@kowshik : The source of all the confusing is that we use the real PSM in some cases while using a temporary PSM in some other cases during recovery. The temporary PSM in recoverSegment() is used in 4 different places.\r\n\r\n1. In recoverLog(). this is the case that we could just pass in the real PSM.\r\n2. In completeSwapOperations(). We try to avoid recovering segment here in https://github.com/apache/kafka/pull/10763.\r\n3 and 4. In loadSegmentFiles(). We probably need to clean this part of the logic a bit. If we are missing index file or the index file is corrupted, typically we can just rebuild the index without changing PSM. If the segment is truncated while rebuilding the index, we actually want to follow the process in step 1, by just removing the rest of the segments. So, we could also get rid of the temporary PSM in this case.\r\n\r\nI am wondering if we could have a separate PR to get rid of the temporary PSM complete?\r\n",
        "createdAt" : "2021-06-22T01:04:39Z",
        "updatedAt" : "2021-06-22T01:05:08Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "384cd5f2-e34b-4d8f-8be9-eb50624e3ebb",
        "parentId" : "118aac07-5e65-44b3-b36e-e8b3b397c8c3",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "@junrao That's a good point. Yes, we should get rid of the temporary PSM. I've created a jira tracking this improvement: https://issues.apache.org/jira/browse/KAFKA-12977. It is currently assigned to myself and I'll follow up on it.",
        "createdAt" : "2021-06-22T02:09:20Z",
        "updatedAt" : "2021-06-22T02:09:20Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e2df3c068936a039468540723a07f141213ff88",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +850,854 @@    Option(snapshots.remove(snapshotOffset)).flatMap { snapshot => {\n      // If the file cannot be renamed, it likely means that the file was deleted already.\n      // This can happen due to the way we construct an intermediate producer state manager\n      // during log recovery, and use it to issue deletions prior to creating the \"real\"\n      // producer state manager."
  }
]