[
  {
    "id" : "253aa45e-60b5-47ca-94b1-28942525694f",
    "prId" : 4580,
    "prUrl" : "https://github.com/apache/kafka/pull/4580#pullrequestreview-97363748",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d97a490-e229-476f-89f2-f0660378fa90",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "It would be a bit confusing if `s` is `None`. Maybe we'd want to have an explicit message for that case. Same for the other method.",
        "createdAt" : "2018-02-17T03:25:52Z",
        "updatedAt" : "2018-02-17T03:27:05Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "19f1d2d181c5a8b26066cb88392e690a3fad878b",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +330,334 @@          pausedCleaningCond.signalAll()\n        case s =>\n          throw new IllegalStateException(s\"In-progress partition $topicPartition cannot be in $s state.\")\n      }\n    }"
  },
  {
    "id" : "b898c540-cd4b-4dd2-be89-9011f4ecb597",
    "prId" : 5439,
    "prUrl" : "https://github.com/apache/kafka/pull/5439#pullrequestreview-148141828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bd042dd-8f88-46da-9ecb-12455ce0a466",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Also, I could not figure out how often this method is called. Wondering if it might take too much CPU if there are many uncleanable partitions",
        "createdAt" : "2018-08-21T16:22:17Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3efb8ceab14c3414c7cf1d08512eb40b128515a",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +101,105 @@        \"uncleanable-bytes\",\n        new Gauge[Long] {\n          def value = {\n            inLock(lock) {\n              uncleanablePartitions.get(dir.getAbsolutePath) match {"
  },
  {
    "id" : "9f42cbd8-9742-41eb-9a48-b1cefc1edbfb",
    "prId" : 5439,
    "prUrl" : "https://github.com/apache/kafka/pull/5439#pullrequestreview-155284989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc686eab-ae7f-4c71-be5b-4dc85be12b47",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Can we take the lock explicitly here?",
        "createdAt" : "2018-09-13T22:29:13Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3efb8ceab14c3414c7cf1d08512eb40b128515a",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +469,473 @@  }\n\n  private def isUncleanablePartition(log: Log, topicPartition: TopicPartition): Boolean = {\n    inLock(lock) {\n      uncleanablePartitions.get(log.dir.getParent).exists(partitions => partitions.contains(topicPartition))"
  },
  {
    "id" : "c5cdef93-9803-4550-9b43-34aace3c0f30",
    "prId" : 5439,
    "prUrl" : "https://github.com/apache/kafka/pull/5439#pullrequestreview-159033059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a271527-c84a-4c4b-94f6-c158861f8518",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Hmm, I wonder what impact taking the lock has on log cleaner. I think it depends on how frequently this is called which I am unsure of. @hachikuji thoughts?",
        "createdAt" : "2018-09-26T03:27:48Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "8ad0256c-f11c-46f9-b3a2-1955bc19efc6",
        "parentId" : "9a271527-c84a-4c4b-94f6-c158861f8518",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Yeah I'm also a bit concerned with this. An alternative would be to not hold the lock and risk the ConcurrentModificationException in the `Gauge`.",
        "createdAt" : "2018-09-26T14:39:36Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3efb8ceab14c3414c7cf1d08512eb40b128515a",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +91,95 @@    newGauge(\n      \"uncleanable-partitions-count\",\n      new Gauge[Int] { def value = inLock(lock) { uncleanablePartitions.get(dir.getAbsolutePath).map(_.size).getOrElse(0) } },\n      Map(\"logDirectory\" -> dir.getAbsolutePath)\n    )"
  },
  {
    "id" : "0c6f04ae-9d53-49f3-a903-6d83d8baec10",
    "prId" : 5439,
    "prUrl" : "https://github.com/apache/kafka/pull/5439#pullrequestreview-161970869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d2a9e4b-855e-4f37-acd0-df14f734e9e2",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "A partition can potentially be moved to a different disk through KIP-113. Should we keep this map consistent with disk movement?",
        "createdAt" : "2018-10-02T19:05:15Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "54e9791a-a008-4a13-9d2e-c12d8aa9424a",
        "parentId" : "2d2a9e4b-855e-4f37-acd0-df14f734e9e2",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Absolutely. Great catch!",
        "createdAt" : "2018-10-05T10:11:30Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3efb8ceab14c3414c7cf1d08512eb40b128515a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +76,80 @@  /* the set of uncleanable partitions (partitions that have raised an unexpected error during cleaning)\n   *   for each log directory */\n  private val uncleanablePartitions = mutable.HashMap[String, mutable.Set[TopicPartition]]()\n\n  /* the set of directories marked as uncleanable and therefore offline */"
  },
  {
    "id" : "a2141a6f-2127-415c-9645-a797cbd8eb49",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-236355983",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f176dbd-aa9f-48ad-968e-15b6bfb6db41",
        "parentId" : null,
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "I think this might bother some users in that the main motivation of this feature is timely deletion. Ideally, a partition that ends up in an uncleanable state due to a cleaner error for e.g., should still be counted toward the `max-compaction-delay` metric. Although there is a separate metric that counts uncleanable partitions I don't think we want to overload the meaning of that metric to convey that we may be violating max compaction lag. i.e., it is reasonable and clearer to allow the `max-compaction-delay` metric to keep climbing since the records in those partitions are still exposed for consumption.",
        "createdAt" : "2019-02-04T23:36:58Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "72ebe47a-3aa3-4b3b-8d94-73e697a6b86c",
        "parentId" : "9f176dbd-aa9f-48ad-968e-15b6bfb6db41",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "Uncleanable partitions may come a result of IOException.  It is not always safe to obtain compaction lag for those uncleanable partitions since obtaining the firstbatchtime stamp might require some IOs.  We already have metrics for uncleanable partitions. I think it is safe not to account for uncleanable partitions in this case.",
        "createdAt" : "2019-02-06T02:17:25Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "35895986-e189-4d6c-a25a-0b8eaf61e877",
        "parentId" : "9f176dbd-aa9f-48ad-968e-15b6bfb6db41",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "Which metrics?",
        "createdAt" : "2019-05-10T22:37:46Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "ef94ad4e-a64f-4724-925e-af28325d4821",
        "parentId" : "9f176dbd-aa9f-48ad-968e-15b6bfb6db41",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "uncleanable-partitions-count",
        "createdAt" : "2019-05-11T01:14:59Z",
        "updatedAt" : "2019-05-11T01:14:59Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +176,180 @@        case (topicPartition, log) =>\n          // skip any logs already in-progress and uncleanable partitions\n          inProgress.contains(topicPartition) || isUncleanablePartition(log, topicPartition)\n      }.map {\n        case (topicPartition, log) => // create a LogToClean instance for each"
  },
  {
    "id" : "226af44f-a076-478e-8847-57f25a77bcd2",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-236337100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fab2fd0f-7b71-4c0d-a90a-d4843221e5d9",
        "parentId" : null,
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "This works, but I think we should try and avoid passing in a new `preCleanStats` param with default.\r\nE.g., we could separate out updating max compaction delay (i.e., separate function from this) and all it does is update the stat; alternately just have a volatile `maxCompactionDelay` member and update that from this method. A minor disadvantage of a snapshot stats object is that it is necessary to drive its progress from the cleaner thread - for bulk stats such as cleaner stats it is okay. For a metric that you might rely on for alerting, its true value would be delayed by up to `log.cleaner.backoff` in low volume scenarios.",
        "createdAt" : "2019-02-05T00:58:12Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "87530a46-3ac0-49c2-8077-d5ce446f15fd",
        "parentId" : "fab2fd0f-7b71-4c0d-a90a-d4843221e5d9",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "Since grabFilthiestCompactedLog and cleaner stats are defined in two classes. Volatile global variables doesn't make our life easier.  The default \"new PreCleanStats()\" is mainly for the purposes not to change many test cases that use this function directly.   \r\nIn terms of delay,  the max delay can only be safely populated in log cleaner thread, and it reflects the correct view when the delay is calculated.  So the next update of maxdelay might be delayed by the backoff and the time spent in the actual compaction. ",
        "createdAt" : "2019-02-06T02:25:24Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "c716e1b1-f4b7-4d07-b4a1-8fe071fb51eb",
        "parentId" : "fab2fd0f-7b71-4c0d-a90a-d4843221e5d9",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "I still dislike the \"step\"-effect that this has. i.e., from the point in time any log is due for compaction, the `maxCompactionDelay` metric should be increasing with time. This is minor in the sense that you will record it the next time the cleaner gets around to computing it. I think this can be addressed in a follow-up.",
        "createdAt" : "2019-05-10T22:36:05Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +166,170 @@    * the log manager maintains.\n    */\n  def grabFilthiestCompactedLog(time: Time, preCleanStats: PreCleanStats = new PreCleanStats()): Option[LogToClean] = {\n    inLock(lock) {\n      val now = time.milliseconds"
  },
  {
    "id" : "c59a806e-9e9b-4753-a267-98e73838f5bf",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-200393193",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d4a4c6b-217a-452a-b141-45d52a6dd586",
        "parentId" : null,
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "I think it would also be useful to log a count of how many logs are unclean and of those, how many logs are cleanable due to violating the max compaction lag constraint.",
        "createdAt" : "2019-02-05T23:46:36Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "f29c9fdc-91ba-49c5-b582-f95c29e6f1a5",
        "parentId" : "8d4a4c6b-217a-452a-b141-45d52a6dd586",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "I added related logs : see PreCleanStats",
        "createdAt" : "2019-02-06T02:26:36Z",
        "updatedAt" : "2019-05-11T01:11:28Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +190,194 @@      this.dirtiestLogCleanableRatio = if (dirtyLogs.nonEmpty) dirtyLogs.max.cleanableRatio else 0\n      // and must meet the minimum threshold for dirty byte ratio or have some bytes required to be compacted\n      val cleanableLogs = dirtyLogs.filter { ltc =>\n        (ltc.needCompactionNow && ltc.cleanableBytes > 0) || ltc.cleanableRatio > ltc.log.config.minCleanableRatio\n      }"
  },
  {
    "id" : "c156dbbf-c23c-4938-a3e5-ff9f887a41a5",
    "prId" : 6009,
    "prUrl" : "https://github.com/apache/kafka/pull/6009#pullrequestreview-236773239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "981c0961-a9ae-4051-a1bb-b92408620e72",
        "parentId" : null,
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "Sorry I didn't notice earlier: should we actually prioritize a log that is past its max compaction delay over a log that is more dirty?",
        "createdAt" : "2019-05-11T06:03:28Z",
        "updatedAt" : "2019-05-11T06:03:28Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      },
      {
        "id" : "69bb80ad-e8e9-4bb5-b212-c1e835067e36",
        "parentId" : "981c0961-a9ae-4051-a1bb-b92408620e72",
        "authorId" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "body" : "The original idea is to sort the log based on the compaction delay that passed the the max delay. But a log with a very short compaction delay may always takes priority over a very dirty log (with high dirty ratio).  I think it is better not to prioritize it since the the compaction finish time is not actually guaranteed since log cleaner thread can take a long time for compaction, and it can work on other log when a log go beyond the max compaction lag.",
        "createdAt" : "2019-05-11T07:07:20Z",
        "updatedAt" : "2019-05-11T07:07:20Z",
        "lastEditedBy" : "bb1ec2c6-60d3-4dc3-99ad-5798eedf84b2",
        "tags" : [
        ]
      },
      {
        "id" : "d721fd53-b225-4999-8a41-7fb4c8bb9a9e",
        "parentId" : "981c0961-a9ae-4051-a1bb-b92408620e72",
        "authorId" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "body" : "This really depends on the interpretation of the config. For PII data for e.g., you should be able to provide some guarantee. Either way, there is the possibility of starvation. However, we do have sensors to indicate this situation, so I think we can leave it as is and revisit if people want harder guarantees.",
        "createdAt" : "2019-05-13T16:04:07Z",
        "updatedAt" : "2019-05-13T16:04:07Z",
        "lastEditedBy" : "aa2f3d2f-93f8-421b-a14a-37805d03ea87",
        "tags" : [
        ]
      }
    ],
    "commit" : "e66773e6c3db8cb602bb365130ef2345eaf36123",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +197,201 @@      } else {\n        preCleanStats.recordCleanablePartitions(cleanableLogs.size)\n        val filthiest = cleanableLogs.max\n        inProgress.put(filthiest.topicPartition, LogCleaningInProgress)\n        Some(filthiest)"
  },
  {
    "id" : "a143ec00-9cde-4125-be21-4e986c848670",
    "prId" : 8089,
    "prUrl" : "https://github.com/apache/kafka/pull/8089#pullrequestreview-357034750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "540e27bc-2488-4b22-a031-a464d552c562",
        "parentId" : null,
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "I agree with your comment on this PR that it is better to move the call to `updateCheckpoints` to `cleanableOffsets` / closer to the logic that resets the first dirty offset, since you know exactly when the offset was reset and there is no need to check again. Otherwise, we are opening more possibilities for future bugs if the logic changes but only one place gets updated.",
        "createdAt" : "2020-02-11T22:11:18Z",
        "updatedAt" : "2020-02-18T19:05:34Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "90f350679fa01f4613b1e6cfeca25ec412086817",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +181,185 @@            val lastCleanOffset = lastClean.get(topicPartition)\n            val offsetsToClean = cleanableOffsets(log, lastCleanOffset, now)\n            // update checkpoint for logs with invalid checkpointed offsets\n            if (offsetsToClean.forceUpdateCheckpoint)\n              updateCheckpoints(log.dir.getParentFile(), Option(topicPartition, offsetsToClean.firstDirtyOffset))"
  }
]