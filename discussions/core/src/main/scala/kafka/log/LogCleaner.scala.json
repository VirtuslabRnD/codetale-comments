[
  {
    "id" : "e8347b90-d1c1-4826-b3ea-ec95058a3a8b",
    "prId" : 4465,
    "prUrl" : "https://github.com/apache/kafka/pull/4465#pullrequestreview-91713221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83cb9907-922a-4b0c-87af-1a5a3b5c808f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe I'm missing it, but don't we need to clear `cleaners`? Perhaps there should be a test assertion somewhere.",
        "createdAt" : "2018-01-25T18:29:20Z",
        "updatedAt" : "2018-01-26T18:29:53Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ebc72613-ad9a-4687-894c-c3fda24efc50",
        "parentId" : "83cb9907-922a-4b0c-87af-1a5a3b5c808f",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Oops, fixed and added test",
        "createdAt" : "2018-01-26T00:00:44Z",
        "updatedAt" : "2018-01-26T18:29:53Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "e696e7748e9a85dff78cf067f5de03d6342ca713",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +151,155 @@  def shutdown() {\n    info(\"Shutting down the log cleaner.\")\n    cleaners.foreach(_.shutdown())\n    cleaners.clear()\n  }"
  },
  {
    "id" : "f9dcaa9a-3757-40f9-866f-5f8096b9cee9",
    "prId" : 5439,
    "prUrl" : "https://github.com/apache/kafka/pull/5439#pullrequestreview-159407927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e4d570c-f073-47d5-950c-b952c4799b2f",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "I'd have loved to replace `filthiest` with something better but I can't think of much. `cleanDirtyLog`, may be?",
        "createdAt" : "2018-09-26T03:10:43Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      },
      {
        "id" : "47ba6587-b34b-4019-9e3c-cfd6fd6f5d62",
        "parentId" : "7e4d570c-f073-47d5-950c-b952c4799b2f",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "`filthiest` and `filthy` is used in multiple places to denote a dirty compacted log:\r\nhttps://github.com/apache/kafka/blob/9f7267dd2fedde86bf15aabdbc5256e5fc617184/core/src/main/scala/kafka/log/LogCleanerManager.scala#L122\r\nhttps://github.com/apache/kafka/blob/9f7267dd2fedde86bf15aabdbc5256e5fc617184/core/src/main/scala/kafka/log/LogCleanerManager.scala#L144\r\n\r\nI think if we'd change the naming we'd need to change some of those occurrences as well. May I ask why you prefer dirty over filthiest? I see both as synonyms",
        "createdAt" : "2018-09-27T11:55:47Z",
        "updatedAt" : "2018-10-08T18:35:36Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3efb8ceab14c3414c7cf1d08512eb40b128515a",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +307,311 @@      * @return whether a log was cleaned\n      */\n    private def cleanFilthiestLog(): Boolean = {\n      var currentLog: Option[Log] = None\n"
  },
  {
    "id" : "848ed32e-5e5d-4e95-aacb-5fc81ad5a1e2",
    "prId" : 6715,
    "prUrl" : "https://github.com/apache/kafka/pull/6715#pullrequestreview-236802698",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b838b46f-ca1f-4dd6-add5-38b57387477a",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just being paranoid here: a txn marker should always follow some data (from the same producer-id) on the partition -- i.e. if there is no data produced to this partition, there should be no txn marker.\r\n\r\nWith that, I'm not sure if the second condition should really happen or not: a producer has never produced any data, hence has no latest offsets, but still have a control batch. Even with compaction, the producer's latest offset should still be preserved right?",
        "createdAt" : "2019-05-11T19:29:17Z",
        "updatedAt" : "2019-05-11T19:32:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "b7371c24-0d4a-4765-934f-73d8be2bae6b",
        "parentId" : "b838b46f-ca1f-4dd6-add5-38b57387477a",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "A producer can abort after adding the partition to the transaction but before sending any data. In this case, there could be markers without any data.",
        "createdAt" : "2019-05-12T23:21:24Z",
        "updatedAt" : "2019-05-12T23:21:24Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e02e080d-8d03-42e9-a38a-3844ab42a01c",
        "parentId" : "b838b46f-ca1f-4dd6-add5-38b57387477a",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Ack, makes sense.",
        "createdAt" : "2019-05-13T17:02:06Z",
        "updatedAt" : "2019-05-13T17:02:06Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "dab0403780dfd593303ece81a647672491f67544",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +630,634 @@            lastRecord.lastDataOffset match {\n              case Some(offset) => batch.lastOffset == offset\n              case None => batch.isControlBatch && batch.producerEpoch == lastRecord.producerEpoch\n            }\n          }"
  },
  {
    "id" : "37bfbcd2-a5e4-47c7-b199-672e263a1ad9",
    "prId" : 6722,
    "prUrl" : "https://github.com/apache/kafka/pull/6722#pullrequestreview-237434887",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c2f23cf-f4e3-41d6-876a-48ea195ac8c9",
        "parentId" : null,
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Would it be worth adding a comment that we're creating a min-heap? I was a bit confused initially because I assumed the default behavior would be to create a min-heap, similar to Java's `PriorityQueue`, though it's clear from Scala documentation that it instead builds a max-heap.",
        "createdAt" : "2019-05-14T18:47:16Z",
        "updatedAt" : "2019-05-24T23:24:37Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "e525b63fa7f5b8f0054a9382b45cf116b3aa9310",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +1062,1066 @@  private val ongoingAbortedTxns = mutable.Map.empty[Long, AbortedTransactionMetadata]\n  // Minheap of aborted transactions sorted by the transaction first offset\n  private var abortedTransactions = mutable.PriorityQueue.empty[AbortedTxn](new Ordering[AbortedTxn] {\n    override def compare(x: AbortedTxn, y: AbortedTxn): Int = x.firstOffset compare y.firstOffset\n  }.reverse)"
  },
  {
    "id" : "3631e15c-1716-4a50-a1ba-1ec27b1323e9",
    "prId" : 6722,
    "prUrl" : "https://github.com/apache/kafka/pull/6722#pullrequestreview-241515648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30a6a410-b29d-46aa-a727-b03c4a3283ce",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add transactionMetadata to the javadoc?",
        "createdAt" : "2019-05-24T01:12:19Z",
        "updatedAt" : "2019-05-24T23:24:37Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e525b63fa7f5b8f0054a9382b45cf116b3aa9310",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +552,556 @@                                 deleteHorizonMs: Long,\n                                 stats: CleanerStats,\n                                 transactionMetadata: CleanedTransactionMetadata): Unit = {\n    // create a new segment with a suffix appended to the name of the log and indexes\n    val cleaned = LogCleaner.createNewCleanedSegment(log, segments.head.baseOffset)"
  },
  {
    "id" : "94ce16a9-33fd-4c42-aff0-f85d20c8f526",
    "prId" : 7859,
    "prUrl" : "https://github.com/apache/kafka/pull/7859#pullrequestreview-336002345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d35dcf1a-1ecd-4725-9203-2e54d5f8272e",
        "parentId" : null,
        "authorId" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "body" : "This will still create an intermediate collection, no? Why not use `cleaners.iterator.map`, turn stats into an iterator... and avoid creating the extra collection completely?",
        "createdAt" : "2019-12-23T15:33:31Z",
        "updatedAt" : "2019-12-25T22:00:28Z",
        "lastEditedBy" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "tags" : [
        ]
      },
      {
        "id" : "68fab32b-ec15-4ad6-9295-104f3d039ab4",
        "parentId" : "d35dcf1a-1ecd-4725-9203-2e54d5f8272e",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Yes, good catch. I missed that in the initial change. Will update.",
        "createdAt" : "2019-12-23T16:05:09Z",
        "updatedAt" : "2019-12-25T22:00:28Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "fc9cee6e-bd73-4c1d-98f9-c68cc0161fe3",
        "parentId" : "d35dcf1a-1ecd-4725-9203-2e54d5f8272e",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "I went to make this change and then I remembered why I didn't do it in the first place. The iterator can only be used once and we need two iterations. We would need to allocate two separator iterators. But `cleaners` should be a small collection (size 1 by default) so the additional code complexity didn't seem worth it.",
        "createdAt" : "2019-12-23T20:33:39Z",
        "updatedAt" : "2019-12-25T22:00:28Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "61ab584d2774171ed97fc947e709bde4928768f9",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +118,122 @@  /* a metric to track the recopy rate of each thread's last cleaning */\n  newGauge(\"cleaner-recopy-percent\", () => {\n    val stats = cleaners.map(_.lastStats)\n    val recopyRate = stats.iterator.map(_.bytesWritten).sum.toDouble / math.max(stats.iterator.map(_.bytesRead).sum, 1)\n    (100 * recopyRate).toInt"
  }
]