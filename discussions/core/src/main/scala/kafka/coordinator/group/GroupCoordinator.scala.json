[
  {
    "id" : "b6990de4-9f31-4f12-bf51-9a449776d85e",
    "prId" : 4479,
    "prUrl" : "https://github.com/apache/kafka/pull/4479#pullrequestreview-92814289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "864242f1-c7d2-4ce6-b7d1-1e82f2c730c8",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This still feels a bit hacky. As an alternative, maybe we can let the offset selector be provided as a function. Something like this:\r\n\r\n```scala\r\ndef cleanupGroupMetadata(\r\n groups: Iterable[GroupMetadata], \r\n collectOffsetsToRemove: Group => Map[TopicPartition, OffsetAndMetadata])\r\n```\r\n\r\nWhat do you think?",
        "createdAt" : "2018-01-30T22:20:49Z",
        "updatedAt" : "2018-01-31T19:11:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "580caacb-72a1-46b4-a3e5-e98b6213c967",
        "parentId" : "864242f1-c7d2-4ce6-b7d1-1e82f2c730c8",
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "I'm not sure which part you consider hacky, and am trying to understand your suggestion.\r\n\r\nFor the sake of `deleteGroups` functionality, we can use `group.allOffsets` that conforms to the function signature above. But how about the existing functionality, where we want to delete specific topic partitions from a group: `groupManager.cleanupGroupMetadata(Some(topicPartitions), groupManager.currentGroups, time.milliseconds())` and populate the corresponding `OffsetAndMetadata` values? I'm assuming we want to reuse the same `cleanupGroupMetadata` method for both cases.\r\n\r\nOn the same assumption, we also need to factor in the concept of current time so we can determine the expired offsets for the existing functionality.\r\n\r\nOn the other hand if you are proposing to create A new `cleanupGroupMetadata` method that calls on the existing method, we should make this call once per group (since topic partitions are group-specific).\r\n\r\nOr maybe I'm missing the point :)",
        "createdAt" : "2018-01-31T03:40:55Z",
        "updatedAt" : "2018-01-31T19:11:21Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      },
      {
        "id" : "57e4e53d-bfb3-4c49-807d-d506de269e23",
        "parentId" : "864242f1-c7d2-4ce6-b7d1-1e82f2c730c8",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's not that big of a deal. I just thought it was a mild abuse to reuse the expiration logic to delete all offsets. Alternatively, what I was suggesting is to let the caller choose the offsets to delete.",
        "createdAt" : "2018-01-31T06:09:39Z",
        "updatedAt" : "2018-01-31T19:11:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca60b651ee8c2c38a6122b0300504aad8d2972b9",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +376,380 @@\n      if (eligibleGroups.nonEmpty) {\n        groupManager.cleanupGroupMetadata(None, eligibleGroups, Long.MaxValue)\n        groupErrors ++= eligibleGroups.map(_.groupId -> Errors.NONE).toMap\n        info(s\"The following groups were deleted: ${eligibleGroups.map(_.groupId).mkString(\", \")}\")"
  },
  {
    "id" : "4aef60da-fd15-476b-9a0b-5a0ac737d789",
    "prId" : 4504,
    "prUrl" : "https://github.com/apache/kafka/pull/4504#pullrequestreview-98375530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa5117ad-8727-4f3e-afee-fb1ecb1e0efe",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm debating whether we should grab the group lock in these functions. I know the caller is holding the lock already when it invokes them, but I'm wondering if we should be more defensive in case we change the logic. At a minimum, we can document the fact that the function is invoked while holding the lock.",
        "createdAt" : "2018-02-21T19:48:15Z",
        "updatedAt" : "2018-02-22T00:13:13Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "195d8b5c-d438-4b92-aae7-1b482eb00779",
        "parentId" : "fa5117ad-8727-4f3e-afee-fb1ecb1e0efe",
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "Thanks for the quick feedback. Seems fair. I added some comments to clarify this matter. Please let me know if you think they're not adequate. Thanks.",
        "createdAt" : "2018-02-21T21:55:12Z",
        "updatedAt" : "2018-02-22T00:13:13Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      }
    ],
    "commit" : "05a6eb50b5de80647cd6a8cae72551f5e94ccb70",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +376,380 @@\n      if (eligibleGroups.nonEmpty) {\n        val offsetsRemoved = groupManager.cleanupGroupMetadata(eligibleGroups, group => {\n          group.removeAllOffsets()\n        })"
  },
  {
    "id" : "ad5f89db-775b-4516-8ed0-dc72ff8d06ff",
    "prId" : 4788,
    "prUrl" : "https://github.com/apache/kafka/pull/4788#pullrequestreview-108229349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3b3e556-eb66-4147-aa29-d8533ee2ab1d",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "The indention below seems not aligned?",
        "createdAt" : "2018-03-29T22:59:34Z",
        "updatedAt" : "2018-03-30T01:01:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ccc3e85feaf510a8730f6dddb63fb9fc8c592124",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +249,253 @@\n      case None =>\n        groupManager.getGroup(groupId) match {\n          case None => responseCallback(Array.empty, Errors.UNKNOWN_MEMBER_ID)\n          case Some(group) => doSyncGroup(group, generation, memberId, groupAssignment, responseCallback)"
  },
  {
    "id" : "d0b3d0ca-5474-4e78-b608-a9b292eeec69",
    "prId" : 5962,
    "prUrl" : "https://github.com/apache/kafka/pull/5962#pullrequestreview-189504164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a038565b-0342-4349-b055-b7215cd346a5",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Why do we define a new `NewMemberJoinTimeoutMs` instead of using the member's brought-in session timeout?",
        "createdAt" : "2018-12-28T23:23:24Z",
        "updatedAt" : "2018-12-28T23:23:24Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "4597285d-31b4-4e67-ab91-5f50ddcb0c10",
        "parentId" : "a038565b-0342-4349-b055-b7215cd346a5",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "As stated above, for new members they do not have member ids and cannot start sending heartbeats, so session timeout would not matter here. Thus we need a separate value just for this purpose.",
        "createdAt" : "2019-01-04T19:47:03Z",
        "updatedAt" : "2019-01-04T19:47:03Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "259f70653f00eaeec3377abc30580ca801974b78",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +722,726 @@    // members in the rebalance. To prevent this going on indefinitely, we timeout JoinGroup requests\n    // for new members. If the new member is still there, we expect it to retry.\n    completeAndScheduleNextExpiration(group, member, NewMemberJoinTimeoutMs)\n\n    maybePrepareRebalance(group, s\"Adding new member $memberId\")"
  },
  {
    "id" : "e8792bbe-1ebe-4b00-af40-bc9210f37875",
    "prId" : 6058,
    "prUrl" : "https://github.com/apache/kafka/pull/6058#pullrequestreview-188880567",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "70c1c09c-e589-4067-94b6-72519ac91008",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "With the changes here, I think the `Empty` case shouldn't be possible. We know we have one member at least. ",
        "createdAt" : "2019-01-03T16:40:09Z",
        "updatedAt" : "2019-01-15T03:26:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "29f668ef1a9b2ee5939791b9bd5b89bf1a1660bb",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +231,235 @@\n          case Stable =>\n            val member = group.get(memberId)\n            if (group.isLeader(memberId) || !member.matches(protocols)) {\n              // force a rebalance if a member has changed metadata or if the leader sends JoinGroup."
  },
  {
    "id" : "d8e0fc98-ccb0-4c16-8159-85241ee5f157",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-194603468",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b7f3c2d-49b2-459b-9715-65d0596937ee",
        "parentId" : null,
        "authorId" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "body" : "Do we really need to shrink? If we detect a size issue and force a rebalance, everyone is forced to re-join and then the \"group too large\" logic in the JoinGroup path will kick in.\r\n\r\nPersonally, I'd leave existing groups alone. If the size causes issues, they'll timeout and rebalance themselves anyway. And if there are no issues, why introduce new issues.\r\n\r\n@hachikuji should probably check the logic here, since reloading of groups has a bunch of logic (including transactions) that I'm not comfortable with.\r\n\r\n",
        "createdAt" : "2019-01-17T19:05:10Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "tags" : [
        ]
      },
      {
        "id" : "647b2f6b-be07-421b-993c-e3a9f97badeb",
        "parentId" : "7b7f3c2d-49b2-459b-9715-65d0596937ee",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Yeah, this is the trickiest part of the KIP. My naive initial reasoning was that we might want to maintain the same leader but now that I think of it that is more or less pointless (unless it loads some state in its initial PartitionAssignor) and we most likely will pick another leader from the subsequent rebalance",
        "createdAt" : "2019-01-18T11:47:50Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "d9943fbc-2cd9-4506-8a2b-53b589063f73",
        "parentId" : "7b7f3c2d-49b2-459b-9715-65d0596937ee",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's not a bad idea to preserve the leader, but I think Gwen's suggestion seems simpler. I think the main thing I was hoping is that the resizing could be graceful. The existing members should be able to commit their offsets before getting kicked out. It seems like we don't get that with this approach since we are forcefully removing some members.",
        "createdAt" : "2019-01-18T23:56:45Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "7b541dfe-f81e-460e-b4e0-6e065663162d",
        "parentId" : "7b7f3c2d-49b2-459b-9715-65d0596937ee",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "By kicking out the extra members, like Jason said those members lose their opportunities to commit the last batch. However on the other hand, this should only happen when the server operator believes that an absurd group size is configured that should be prevented. In this sense, the group is interpreted as `over capacity`, so allowing rest of consumers take over the jobs should be very light lifting, which I don't see an obvious negative impact. Thoughts?",
        "createdAt" : "2019-01-20T07:01:51Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "93b49433-913e-4750-867f-87462e0d9729",
        "parentId" : "7b7f3c2d-49b2-459b-9715-65d0596937ee",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "> In this sense, the group is interpreted as over capacity, so allowing rest of consumers take over the jobs should be very light lifting\r\nSorry, I couldn't understand this exactly. Do you mean that it's reasonable to \"shrink\" the group in one way or another and not let it continue over capacity?\r\n\r\nMy thinking is to go with a forced rebalance",
        "createdAt" : "2019-01-21T13:39:44Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +660,664 @@      info(s\"Loading group metadata for ${group.groupId} with generation ${group.generationId}\")\n      assert(group.is(Stable) || group.is(Empty))\n      if (groupIsOverCapacity(group)) {\n        prepareRebalance(group, s\"Freshly-loaded group is over capacity ($groupConfig.groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets\")\n      }"
  },
  {
    "id" : "9e3805c8-305d-4b40-b232-8f1972565910",
    "prId" : 6177,
    "prUrl" : "https://github.com/apache/kafka/pull/6177#pullrequestreview-225334147",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b3fdf4e-285f-4a6c-9a51-ac5a100b3130",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Hmm... this is not what I was thinking. Maybe we can elaborate a bit more on the KIP wiki?",
        "createdAt" : "2019-03-28T18:23:53Z",
        "updatedAt" : "2019-04-26T15:12:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "3b054691-3b09-4e70-b0a7-b18b9d0bd709",
        "parentId" : "6b3fdf4e-285f-4a6c-9a51-ac5a100b3130",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Yea, updated explicitly in the KIP",
        "createdAt" : "2019-04-11T05:35:38Z",
        "updatedAt" : "2019-04-26T15:12:49Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "932118542844c01e3caf657e2eee9c16dfd269d7",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +241,245 @@        responseCallback(joinError(memberId, Errors.INCONSISTENT_GROUP_PROTOCOL))\n      } else if (group.isPendingMember(memberId)) {\n        // A rejoining pending member will be accepted. Note that pending member will never be a static member.\n        if (groupInstanceId.isDefined) {\n          throw new IllegalStateException(s\"the static member $groupInstanceId was unexpectedly to be assigned \" +"
  },
  {
    "id" : "ca724c4f-54b6-4701-ae5b-37af41e27cfd",
    "prId" : 6177,
    "prUrl" : "https://github.com/apache/kafka/pull/6177#pullrequestreview-220841470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b5f0e9b-02ce-4d23-b0da-926ac35b3628",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "What if two consumers joining with the same instance id and member id?",
        "createdAt" : "2019-03-28T18:26:51Z",
        "updatedAt" : "2019-04-26T15:12:49Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "d8c2387d-bfcc-4ebd-9fca-465c5f3eeb15",
        "parentId" : "8b5f0e9b-02ce-4d23-b0da-926ac35b3628",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "As we have discussed, we shall generate a new member id each time the static member rejoins. So the former consumer will not have the same member.id as the previous one",
        "createdAt" : "2019-03-30T23:46:18Z",
        "updatedAt" : "2019-04-26T15:12:49Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "932118542844c01e3caf657e2eee9c16dfd269d7",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +261,265 @@            // it reset its member id and retry.\n          responseCallback(joinError(memberId, Errors.UNKNOWN_MEMBER_ID))\n        } else {\n          val member = group.get(memberId)\n"
  },
  {
    "id" : "195e8783-53c9-4eb8-b9c4-aae648880486",
    "prId" : 6251,
    "prUrl" : "https://github.com/apache/kafka/pull/6251#pullrequestreview-202264025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22d694fb-59c3-4679-b59d-e204962df204",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "For logical consistency, I'd suggest this line go above `maybePrepareRebalance`. Removal of the pending member may resolve the rebalance.",
        "createdAt" : "2019-02-11T18:23:45Z",
        "updatedAt" : "2019-02-19T06:13:25Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "2102b7c04ffb64ba163904304f9ee7fb9dab3432",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +787,791 @@    completeAndScheduleNextExpiration(group, member, NewMemberJoinTimeoutMs)\n\n    group.removePendingMember(memberId)\n    maybePrepareRebalance(group, s\"Adding new member $memberId\")\n  }"
  },
  {
    "id" : "4b38dae2-29cc-4849-b5b0-451cdeff8bbd",
    "prId" : 6650,
    "prUrl" : "https://github.com/apache/kafka/pull/6650#pullrequestreview-234823576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c559ea1d-b8e5-410b-9447-9e7e0b9fae9a",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "nice catch.",
        "createdAt" : "2019-05-08T01:17:14Z",
        "updatedAt" : "2019-05-18T04:04:24Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a0c245434329139d931b0cc904704ef8c26a62c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +182,186 @@\n          if (group.is(Stable)) {\n            info(s\"Static member $groupInstanceId with unknown member id rejoins, assigning new member id $newMemberId, while \" +\n              s\"old member $oldMemberId will be removed. No rebalance will be triggered.\")\n"
  },
  {
    "id" : "0ab20383-3e9d-4b47-bc9e-47d47a7bf5b4",
    "prId" : 6650,
    "prUrl" : "https://github.com/apache/kafka/pull/6650#pullrequestreview-234823576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "097103b3-d83a-4520-8406-745c2235a818",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Nice refactoring here.",
        "createdAt" : "2019-05-08T01:27:39Z",
        "updatedAt" : "2019-05-18T04:04:24Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a0c245434329139d931b0cc904704ef8c26a62c",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +523,527 @@\n            case CompletingRebalance =>\n                responseCallback(Errors.REBALANCE_IN_PROGRESS)\n\n            case PreparingRebalance =>"
  },
  {
    "id" : "83939c67-8e19-4702-acba-a285f93c2b44",
    "prId" : 6650,
    "prUrl" : "https://github.com/apache/kafka/pull/6650#pullrequestreview-239027929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "304f1325-943b-4d00-a33e-70c3216c046b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think we always tried to make `Dead` the first check. Otherwise we may end up validating the request against stale state. I like the restructuring here, but maybe we still need to move this check to the top?",
        "createdAt" : "2019-05-17T15:42:36Z",
        "updatedAt" : "2019-05-18T04:04:24Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "cb4558dc-1f7c-4cb6-8023-2c6fb12247a7",
        "parentId" : "304f1325-943b-4d00-a33e-70c3216c046b",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Sounds good",
        "createdAt" : "2019-05-17T16:47:12Z",
        "updatedAt" : "2019-05-18T04:04:24Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a0c245434329139d931b0cc904704ef8c26a62c",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +518,522 @@          responseCallback(Errors.ILLEGAL_GENERATION)\n        } else {\n          group.currentState match {\n            case Empty =>\n              responseCallback(Errors.UNKNOWN_MEMBER_ID)"
  },
  {
    "id" : "56dd5b6a-fe97-4f52-a1eb-d3446c28478c",
    "prId" : 6666,
    "prUrl" : "https://github.com/apache/kafka/pull/6666#pullrequestreview-239064735",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc9b7c1a-c5b2-4d5f-b47d-6b93867e3194",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Personally I'd prefer to have such logic in DelayedJoin instead, i.e. to re-write the current \r\n\r\n```\r\noverride def onComplete() = coordinator.onCompleteJoin(group)\r\n```\r\n\r\ncode. Similar to the extended `InitialDelayedJoin#onComplete`.",
        "createdAt" : "2019-05-17T18:09:55Z",
        "updatedAt" : "2019-05-17T18:19:30Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "16de0b4222e65788435f0ffdb2c0bc8c3b32e74d",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +935,939 @@        // until session timeout removes all the non-responsive members.\n        error(s\"Group ${group.groupId} could not complete rebalance because no members rejoined\")\n        joinPurgatory.tryCompleteElseWatch(\n          new DelayedJoin(this, group, group.rebalanceTimeoutMs),\n          Seq(GroupKey(group.groupId)))"
  },
  {
    "id" : "99356a5d-716c-44d7-9f31-9e44a7fafd06",
    "prId" : 6714,
    "prUrl" : "https://github.com/apache/kafka/pull/6714#pullrequestreview-266357240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba4ee83c-606a-46f1-8871-a9cdc6049153",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "What is the expected behavior if both memberId and groupInstanceId are provided?",
        "createdAt" : "2019-07-24T06:28:17Z",
        "updatedAt" : "2019-07-26T04:38:45Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "305e4e36-4aee-4f4f-9a50-ae5481272140",
        "parentId" : "ba4ee83c-606a-46f1-8871-a9cdc6049153",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "We will deal them for the same way as join group, where we check member.id fencing if instance.id is defined, or unknown id if we don't recognize the member.id and instance.id is not known, too.",
        "createdAt" : "2019-07-24T16:48:05Z",
        "updatedAt" : "2019-07-26T04:38:45Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "97b6192e-0438-4722-b4ca-a5edd918501f",
        "parentId" : "ba4ee83c-606a-46f1-8871-a9cdc6049153",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe you explained already, but is that planned for a follow-up patch? I think the logic below ignores memberId if the instanceId is provided.",
        "createdAt" : "2019-07-25T00:44:07Z",
        "updatedAt" : "2019-07-26T04:38:45Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "c941ea28-dc9e-4757-a3a1-ace6a24b716c",
        "parentId" : "ba4ee83c-606a-46f1-8871-a9cdc6049153",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Yes, we will ignore `member.id` if instanceId is provided. This is the case for admin client batch removal. And we could catch the case where member.id doesn't exist in the `FENCED_INSTANCE_ID` check, as the instanceId must be pointing to some real `member.id`.",
        "createdAt" : "2019-07-25T01:07:19Z",
        "updatedAt" : "2019-07-26T04:38:45Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab085e9764aa35dece48891a8d707b743b5bf711",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +456,460 @@                      memberLeaveError(leavingMember, Errors.NONE)\n                    }\n                  } else if (!group.has(memberId) && !group.hasStaticMember(groupInstanceId)) {\n                    memberLeaveError(leavingMember, Errors.UNKNOWN_MEMBER_ID)\n                  } else {"
  },
  {
    "id" : "b27876d9-333e-4ae6-b58c-bab829a94f6a",
    "prId" : 6894,
    "prUrl" : "https://github.com/apache/kafka/pull/6894#pullrequestreview-246717254",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f7c3d9e-93a9-4119-93e1-239f34d31b3e",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Use the same `match / case` pattern, and add more log4j entries here: the rationale is that join-group events should not be very frequent so it's worth making more visibility out of it.",
        "createdAt" : "2019-06-06T17:41:26Z",
        "updatedAt" : "2019-06-11T01:42:06Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bb2b5f92caf46b4d78268db42075c829306fe64",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +182,186 @@\n          group.currentState match {\n            case Stable =>\n              info(s\"Static member $groupInstanceId with unknown member id rejoins group ${group.groupId} \" +\n                s\"in ${group.currentState} state. Assigning new member id $newMemberId, while old member $oldMemberId \" +"
  },
  {
    "id" : "114d4f42-3a09-4135-a78c-43cc881a966d",
    "prId" : 6894,
    "prUrl" : "https://github.com/apache/kafka/pull/6894#pullrequestreview-246717254",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87411e20-8631-49a7-94a3-ffa8eb2d7d96",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Again the match / case pattern, and for CompletingRebalance we still return REBALANCE_IN_PROGRESS just to be safe.",
        "createdAt" : "2019-06-06T17:42:17Z",
        "updatedAt" : "2019-06-11T01:42:06Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bb2b5f92caf46b4d78268db42075c829306fe64",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +629,633 @@        responseCallback(offsetMetadata.mapValues(_ => Errors.ILLEGAL_GENERATION))\n      } else {\n        group.currentState match {\n          case Stable | PreparingRebalance =>\n            // During PreparingRebalance phase, we still allow a commit request since we rely"
  },
  {
    "id" : "0f9d4dec-0754-42c8-a9ef-a4a9fc5baccd",
    "prId" : 6894,
    "prUrl" : "https://github.com/apache/kafka/pull/6894#pullrequestreview-246912923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42d4b322-dcc6-4254-b96c-ad6ab271b7e4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think you mentioned there are some implication for KIP-429. In theory, if the consumer knows it will continue to own some partitions, it _could_ continue to commit offsets before it has the assignment for the latest generation. It seems we should either explicitly allow this or treat it as an illegal state?",
        "createdAt" : "2019-06-06T20:38:55Z",
        "updatedAt" : "2019-06-11T01:42:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "faa9f7d7-de70-4988-8f93-48832e47848b",
        "parentId" : "42d4b322-dcc6-4254-b96c-ad6ab271b7e4",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I've thought about that, and it is actually a bit complicated for consumer to do so. Because if you get a retriable error code from commit response in commitSync (in commitAsync we do not retry but just set the error back to user), the re-try logic is:\r\n\r\n1) ensureCoordinatorReady.\r\n2) resend commit request\r\n\r\nWhich means that we would not re-join group since we do not check `ensureActiveGroup` at all. On the other hand, `ensureActiveGroup` makes sure that the whole rebalance is done including `onPartitionRevoked / onPartitionAssigned` callback, and therefore some partitions may already be revoked -- and in practice we are going to commit offsets in `onPartitionRevoked` successfully since the group state would be in `stable` anyways. Hence for this `commitSync`, it is better still fail and throw.\r\n\r\nHowever, I'm thinking that instead of throwing CommitFailed, we can instead throw a different exception (e.g. we can just throw `RebalanceInProgressException` all the way to the caller). In callers like Streams then, we should handle CommitFailed and RebalanceInProgress differently: e.g. for the former we treat it as \"task being migrated\", while for the latter, since we know we are going to call `commitSync` again in `onPartitionsRevoked` we can just swallow it and proceed.",
        "createdAt" : "2019-06-07T00:09:59Z",
        "updatedAt" : "2019-06-11T01:42:06Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "cc2ead0c-9b6a-4a54-b2f6-48ad52d92aa0",
        "parentId" : "42d4b322-dcc6-4254-b96c-ad6ab271b7e4",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Note though, it would require a minor public API change that `commitSync` could throw `RebalanceInProgressException`. I'd just piggy back with KIP-429.",
        "createdAt" : "2019-06-07T00:10:55Z",
        "updatedAt" : "2019-06-11T01:42:06Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "41128e95-3035-4bfc-9267-1a7aee89dad7",
        "parentId" : "42d4b322-dcc6-4254-b96c-ad6ab271b7e4",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, let's keep it as is.",
        "createdAt" : "2019-06-07T04:46:31Z",
        "updatedAt" : "2019-06-11T01:42:06Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bb2b5f92caf46b4d78268db42075c829306fe64",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +637,641 @@            groupManager.storeOffsets(group, memberId, offsetMetadata, responseCallback)\n\n          case CompletingRebalance =>\n            // We should not receive a commit request if the group has not completed rebalance;\n            // but since the consumer's member.id and generation is valid, it means it has received"
  },
  {
    "id" : "761f609b-9fd0-49b2-9074-fe8bc3b57e8e",
    "prId" : 6899,
    "prUrl" : "https://github.com/apache/kafka/pull/6899#pullrequestreview-247929583",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2a4526c-9ecc-46b6-8a89-408845f5205c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think it's worth a comment explaining the use of the old leaderId here.",
        "createdAt" : "2019-06-11T16:17:31Z",
        "updatedAt" : "2019-06-12T04:56:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3d2900439cab3b6134ba74dab5d231c1380edb6",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +206,210 @@                // as a leader based on the returned message, since the new member.id won't match\n                // returned leader id, therefore no assignment will be performed.\n                leaderId = currentLeader,\n                error = Errors.NONE))\n            case Empty | Dead =>"
  },
  {
    "id" : "48e37daf-265d-4af7-b9b5-9109655070c4",
    "prId" : 6899,
    "prUrl" : "https://github.com/apache/kafka/pull/6899#pullrequestreview-248315304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a620b155-d38a-4659-9753-ce4d1a40d372",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The call to `replaceGroupInstance` also updates the leaderId. If the leader is replaced twice, there will be a different leaderId each time. This is probably harmless, just mentioning it since the behavior is slightly inconsistent. The other option would be to lock in the leaderId when the generation is bumped. The advantage of the approach here is that the leaderId set in `GroupMetadata` always corresponds to one of the existing members, which may be less error-prone.",
        "createdAt" : "2019-06-11T16:39:51Z",
        "updatedAt" : "2019-06-12T04:56:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "dedcfbb6-f791-4381-9aff-fbb5871b3979",
        "parentId" : "a620b155-d38a-4659-9753-ce4d1a40d372",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Yes, agree it's a trade-off.",
        "createdAt" : "2019-06-11T17:04:27Z",
        "updatedAt" : "2019-06-12T04:56:07Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3d2900439cab3b6134ba74dab5d231c1380edb6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +184,188 @@\n          val currentLeader = group.leaderOrNull\n          val member = group.replaceGroupInstance(oldMemberId, newMemberId, groupInstanceId)\n          // Heartbeat of old member id will expire without effect since the group no longer contains that member id.\n          // New heartbeat shall be scheduled with new member id."
  },
  {
    "id" : "51e80296-78dd-437d-b4cb-d779e012c653",
    "prId" : 6899,
    "prUrl" : "https://github.com/apache/kafka/pull/6899#pullrequestreview-248439856",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00fbb064-ed30-4109-8d12-81eed02364e9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Perhaps if you want to move this, we may as well put it in the `JoinGroupResult` companion object.\r\n```scala\r\nobject JoinGroupResult {\r\n  def apply(memberId: String, error: Errors): JoinGroupResult = {\r\n...\r\n  }\r\n}\r\n```",
        "createdAt" : "2019-06-11T21:10:15Z",
        "updatedAt" : "2019-06-12T04:56:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3d2900439cab3b6134ba74dab5d231c1380edb6",
    "line" : 195,
    "diffHunk" : "@@ -1,1 +1095,1099 @@  }\n\n  def joinError(memberId: String, error: Errors): JoinGroupResult = {\n    JoinGroupResult(\n      members = List.empty,"
  },
  {
    "id" : "5bb365ff-7ac0-4c9a-a51e-cdf29fe1a63d",
    "prId" : 7276,
    "prUrl" : "https://github.com/apache/kafka/pull/7276#pullrequestreview-287351588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8dcce302-7f48-4961-927b-a7eea8f96946",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Leaving this here for lack of a better alternative. One thing that I didn't cover in the KIP is changes to the consumer group command to use this API. I think we can extend the KIP and do this in a separate PR. What do you think?",
        "createdAt" : "2019-09-11T18:45:24Z",
        "updatedAt" : "2019-09-13T19:33:00Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "189b6e20-faf9-405d-8a32-c866a8b49490",
        "parentId" : "8dcce302-7f48-4961-927b-a7eea8f96946",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "It makes sense. Let's do it in a separate PR.",
        "createdAt" : "2019-09-12T07:56:26Z",
        "updatedAt" : "2019-09-13T19:33:00Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      },
      {
        "id" : "2f74146b-158c-4ab1-b908-75a5851dd920",
        "parentId" : "8dcce302-7f48-4961-927b-a7eea8f96946",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "I have opened a JIRA for it: https://issues.apache.org/jira/browse/KAFKA-8901",
        "createdAt" : "2019-09-12T11:06:18Z",
        "updatedAt" : "2019-09-13T19:33:00Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      }
    ],
    "commit" : "d96c9908a2b95e75aa853ea79d04532c229d007a",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +543,547 @@  }\n\n  def handleDeleteOffsets(groupId: String, partitions: Seq[TopicPartition]): (Errors, Map[TopicPartition, Errors]) = {\n    var groupError: Errors = Errors.NONE\n    var partitionErrors: Map[TopicPartition, Errors] = Map()"
  },
  {
    "id" : "0d3dec00-9861-4e1a-bd2b-7a71c0fa9d77",
    "prId" : 7897,
    "prUrl" : "https://github.com/apache/kafka/pull/7897#pullrequestreview-342958606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57457978-51b6-4419-8719-b2fd26e0e936",
        "parentId" : null,
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Why do we need to check `generationId >= 0` ?",
        "createdAt" : "2020-01-14T22:37:48Z",
        "updatedAt" : "2020-01-14T23:13:52Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      },
      {
        "id" : "cee43dc3-3e81-45a1-b4a7-0732ae551765",
        "parentId" : "57457978-51b6-4419-8719-b2fd26e0e936",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "This is a validation on whether the generationId is set in the txn commit, as we only do the check when the request actually sets that field.",
        "createdAt" : "2020-01-15T02:31:59Z",
        "updatedAt" : "2020-01-15T02:31:59Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "943bdf3604b662dfd20e7cfd586ada10e3412bc8",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +727,731 @@        // Enforce member id when it is set.\n        responseCallback(offsetMetadata.map { case (k, _) => k -> Errors.UNKNOWN_MEMBER_ID })\n      } else if (generationId >= 0 && generationId != group.generationId) {\n        // Enforce generation check when it is set.\n        responseCallback(offsetMetadata.map { case (k, _) => k -> Errors.ILLEGAL_GENERATION })"
  },
  {
    "id" : "1e6d33c8-532b-4047-8167-6e6d94fe74c0",
    "prId" : 8339,
    "prUrl" : "https://github.com/apache/kafka/pull/8339#pullrequestreview-380651265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33007b9a-2830-4e3a-9ba1-58b9e3596be8",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "why to complete Heartbeat here?",
        "createdAt" : "2020-03-24T19:37:07Z",
        "updatedAt" : "2020-03-24T22:36:10Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "6f58801a-c795-4e29-8020-c6ce2eab3929",
        "parentId" : "33007b9a-2830-4e3a-9ba1-58b9e3596be8",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Heartbeat \"completion\" is a really confusing notion in this code. If the delayed heartbeat gets completed, that just means we do not need its expiration any longer. In this case if a member has already been removed from the group, then we do not need to wait for expiration of the delayed heartbeat to remove it.",
        "createdAt" : "2020-03-24T19:58:24Z",
        "updatedAt" : "2020-03-24T22:36:10Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "b64ad18e2413bcc0ae18fa134403bc2a594427ea",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +1183,1187 @@    } else {\n      info(s\"Member id $memberId was not found in ${group.groupId} during heartbeat completion check\")\n      true\n    }\n  }"
  },
  {
    "id" : "a96387ac-e1de-4612-8063-dc65ae0a39ab",
    "prId" : 8405,
    "prUrl" : "https://github.com/apache/kafka/pull/8405#pullrequestreview-386073993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fdd4724-517b-4393-8cef-7831b30c9307",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "We should add a comment about why we would like to trigger rebalance in this case, for example \"The rejoining static member will replace the assigned member.id, which could be unknown to the current leader. To avoid having an unknown member assignment returned, we need to trigger another rebalance\".",
        "createdAt" : "2020-04-02T02:55:19Z",
        "updatedAt" : "2020-04-07T17:54:37Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c44f148e48f1bbed4e5717680b17c2b0b5c53044",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +1030,1034 @@          leaderId = currentLeader,\n          error = Errors.NONE))\n      case CompletingRebalance =>\n        // if the group is in after-sync stage, upon getting a new join-group of a known static member\n        // we should still trigger a new rebalance, since the old member may already be sent to the leader"
  },
  {
    "id" : "d626d5d7-589c-4630-a56e-a39bbe21b694",
    "prId" : 8405,
    "prUrl" : "https://github.com/apache/kafka/pull/8405#pullrequestreview-387467187",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34333acc-0c3e-413b-bfcc-a990a9703b92",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The logic we have in `doSyncGroup` to fill in missing assignments seems dubious. Perhaps there's a compatibility argument to retain it, but it seems more likely to cause problems like the one here than to be useful.",
        "createdAt" : "2020-04-03T05:12:28Z",
        "updatedAt" : "2020-04-07T17:54:37Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d6430f3f-0d56-46cf-8d49-aed19242c4ee",
        "parentId" : "34333acc-0c3e-413b-bfcc-a990a9703b92",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Yup I agree, let me file a JIRA for this.",
        "createdAt" : "2020-04-03T18:07:28Z",
        "updatedAt" : "2020-04-07T17:54:37Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c44f148e48f1bbed4e5717680b17c2b0b5c53044",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +995,999 @@  }\n\n  private def updateStaticMemberAndRebalance(group: GroupMetadata,\n                                             newMemberId: String,\n                                             groupInstanceId: Option[String],"
  }
]