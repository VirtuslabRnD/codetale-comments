[
  {
    "id" : "773d4b80-c279-43b2-88bf-ebcb1c0b2cb4",
    "prId" : 3848,
    "prUrl" : "https://github.com/apache/kafka/pull/3848#pullrequestreview-191900203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef6953b8-aa74-4511-9ea1-e02ae2195547",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add a comment to describe the return value of the method?",
        "createdAt" : "2019-01-11T23:43:21Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df18c6b3232f3e8b9bf879b7a8dd762a5968c6c",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +642,646 @@    *         the exception that was thrown.\n    */\n  private def onPreferredReplicaElection(partitions: Set[TopicPartition],\n                                         electionType: ElectionType): Map[TopicPartition, Throwable] = {\n    info(s\"Starting preferred replica leader election for partitions ${partitions.mkString(\",\")}\")"
  },
  {
    "id" : "0e2059fe-ddcf-4678-891d-82db179bd47a",
    "prId" : 3848,
    "prUrl" : "https://github.com/apache/kafka/pull/3848#pullrequestreview-193898101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "This is an existing issue. When handing a ZK session expiration, we call beforeInitializingSession, which first clear the event queue before adding an Expire event. The issue is that the clear events including PreferredReplicaLeaderElection or ControlledShutdown and the callbacks on those events won't be called. This means the client will never receive a response and will time out. I am thinking that one way to fix this is the following: go through each cleared event and if the event has a callback, call the callback with an error.",
        "createdAt" : "2019-01-15T00:04:20Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "1db53abf-74aa-4ace-8b25-b7d539363c0c",
        "parentId" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "authorId" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "body" : "Makes sense. Do you want me to implement this for `PreferredReplicaLeaderElection` _and `ControlledShutdown`_? What testing would you like to see?",
        "createdAt" : "2019-01-16T11:16:09Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "tags" : [
        ]
      },
      {
        "id" : "46117d22-0e70-46dd-b273-62e25bfed6f3",
        "parentId" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Right now, only PreferredReplicaLeaderElection and ControlledShutdown have callbacks. So, we can just handle them. As for testing, perhaps we can queue up some events in ControllerEventManager and call clearAndPut(), and assert that the queued events' callback are called.",
        "createdAt" : "2019-01-17T23:55:56Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df18c6b3232f3e8b9bf879b7a8dd762a5968c6c",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +1562,1566 @@\n    override def handleProcess(): Unit = {\n      if (!isActive) {\n        callback(Map.empty, partitionsFromAdminClientOpt match {\n          case Some(partitions) => partitions.map(partition => partition -> new ApiError(Errors.NOT_CONTROLLER, null)).toMap"
  },
  {
    "id" : "d7394551-afc0-47cc-97c8-da207d93262b",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-93799718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c34c6cb-528c-491b-aa77-2d7dc1be7470",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Perhaps we should have a log message?",
        "createdAt" : "2018-02-02T22:21:32Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "785640b7-014e-49d0-9d26-e06e3453708c",
        "parentId" : "4c34c6cb-528c-491b-aa77-2d7dc1be7470",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Since we have a log message in the process() method of BrokerModifications, do we need another one here? For other events, logging is in `process()` method of the event.",
        "createdAt" : "2018-02-03T01:15:58Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +1520,1524 @@  override val path: String = BrokerIdZNode.path(brokerId)\n\n  override def handleDataChange(): Unit = {\n    eventManager.put(controller.BrokerModifications)\n  }"
  },
  {
    "id" : "549b8e7f-9665-4d71-973c-dbcb7ddb7ed6",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-102563161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df864efb-d0d6-44f7-8bfc-8a1086938226",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@rajinisivaram : Sorry for the late review. Just a minor comment. It seems that we can avoid reading all brokers from ZK by passing in brokerId to BrokerModifications and only read the info for that brokerId?",
        "createdAt" : "2018-03-01T22:31:33Z",
        "updatedAt" : "2018-03-01T22:31:33Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "b3abdce9-42e4-4377-b6f0-c316c5aa14eb",
        "parentId" : "df864efb-d0d6-44f7-8bfc-8a1086938226",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@junrao Thanks for the review. Opened PR #4670.",
        "createdAt" : "2018-03-09T07:34:43Z",
        "updatedAt" : "2018-03-09T07:34:43Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +1246,1250 @@      if (!isActive) return\n      val curBrokers = zkClient.getAllBrokersInCluster.toSet\n      val updatedBrokers = controllerContext.liveBrokers.filter { broker =>\n        val existingBroker = curBrokers.find(_.id == broker.id)\n        existingBroker match {"
  },
  {
    "id" : "7fe10739-e787-450a-a285-3e8b7d82c945",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-147796250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we fix the alignment?",
        "createdAt" : "2018-08-17T23:12:21Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "b6cfc76f-1721-4b29-bc10-9a75cf65a0d8",
        "parentId" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "Can you be more specific? If you mean the indentation by 2 spaces in the trailing conditions, it seems that's the correct alignment, as also being used in line 436 and line 1172 of this file.",
        "createdAt" : "2018-08-20T19:29:50Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "0f33fa2e-b287-4e45-8fb1-d650e1abf0b4",
        "parentId" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Sorry. If there is the standard, we can just leave it as it is.",
        "createdAt" : "2018-08-20T19:51:34Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1055,1059 @@      val partitionsToActOn = controllerContext.partitionsOnBroker(id).filter { partition =>\n        controllerContext.partitionReplicaAssignment(partition).size > 1 &&\n          controllerContext.partitionLeadershipInfo.contains(partition) &&\n          !topicDeletionManager.isTopicQueuedUpForDeletion(partition.topic)\n      }"
  },
  {
    "id" : "739f2f66-9c4c-4aee-9c1b-ae12d108ab9b",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-148091532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "I think we also need to exclude the partitions being deleted in the replicatedPartitionsBrokerLeads(). Otherwise, the shutting down broker will keep retrying the controlled shutdown request on those partitions since their leaders will never be moved.",
        "createdAt" : "2018-08-17T23:21:54Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "2ae9d7eb-784a-4678-9af2-e949164d6c57",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "For the partitions being deleted, the two behaviors are\r\n1. If we exclude them in the response, then controlled shutdown will be allowed immediately even though the controller might need to issue StopReplica requests for deletion on the given broker, hence possibly blocking the topic deletion until the broker is brought up again.\r\n2. If we instead include the partitions being deleted, the controller will block the controlled shutdown for a while (bound by controlled shutdown retries) and wait for topic deletion to finish. Once the topic deletion finishes, the controlled shutdown response will be empty. Or if the controlled shutdown retries are exhausted before the topic deletion can finish, the broker will be shutdown regardless of the response.\r\nHence I think the current behavior does not violate correctness. It's a matter of trade-off. Thoughts?",
        "createdAt" : "2018-08-20T19:54:58Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "39d97128-5da2-43e5-bd0e-2094c068a4d9",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Good analysis. This is my thought. Normally, topic deletion will happen very quickly. So, if we get to a state where the deletion of a topic is started, but not completed when a broker is being shut down, chances are that another broker is down, which prevents the topic deletion from completing. In this case, having the broker retry the controlled shutdown request won't be useful and only adds latency during shutdown. ",
        "createdAt" : "2018-08-21T00:36:58Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "f7e9c3c5-68ab-44fd-833f-da3a2ba08ab7",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "That makes sense. I've made the change. Thanks!",
        "createdAt" : "2018-08-21T14:36:18Z",
        "updatedAt" : "2018-08-21T14:36:18Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1056,1060 @@        controllerContext.partitionReplicaAssignment(partition).size > 1 &&\n          controllerContext.partitionLeadershipInfo.contains(partition) &&\n          !topicDeletionManager.isTopicQueuedUpForDeletion(partition.topic)\n      }\n      val (partitionsLedByBroker, partitionsFollowedByBroker) = partitionsToActOn.partition { partition =>"
  },
  {
    "id" : "9506db26-09dc-42f8-88c3-97710ecfff98",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-148194607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It seems that we should be checking for isTopicWithDeletionStarted() instead of isTopicQueuedUpForDeletion()?",
        "createdAt" : "2018-08-21T17:10:07Z",
        "updatedAt" : "2018-08-21T17:10:07Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "e32c6d18-a275-4404-a174-34ccb7d98c0e",
        "parentId" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "At the detailed level:\r\nLet's consider topics that are enqueued for deletion but haven't started the actual deletion, maybe they have ongoing partition reassignments that would still take many hours to finish. And for those partitions during a controlled shutdown we won't try to switch leaderships because we no longer care about their availability.\r\nAnd here even if the shutting down broker is still leader for those partitions, I think it's ok to shutdown the broker immediately because again we don't care about the partition availability anymore. Using !isTopicWithDeletionStarted will still return those partitions and block the controlled shutdown, which is probably not ideal.\r\n\r\nAt a higher level:\r\nisTopicQueuedUpForDeletion is used in many other places so that for other types of operations, once a topic is enqueued, it effectively no longer exists. I think that also applies here, and using the same method consistently simplifies reasoning. Thoughts?",
        "createdAt" : "2018-08-21T18:04:54Z",
        "updatedAt" : "2018-08-21T18:04:54Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "6e2324d6-5ed8-402e-8ccb-c2f5e5ef0c0d",
        "parentId" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Thanks for the explanation. This sounds good then.",
        "createdAt" : "2018-08-21T18:36:09Z",
        "updatedAt" : "2018-08-21T18:36:09Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1080,1084 @@        controllerContext.partitionLeadershipInfo.filter {\n          case (topicPartition, leaderIsrAndControllerEpoch) =>\n            !topicDeletionManager.isTopicQueuedUpForDeletion(topicPartition.topic) &&\n              leaderIsrAndControllerEpoch.leaderAndIsr.leader == id &&\n              controllerContext.partitionReplicaAssignment(topicPartition).size > 1"
  }
]