[
  {
    "id" : "773d4b80-c279-43b2-88bf-ebcb1c0b2cb4",
    "prId" : 3848,
    "prUrl" : "https://github.com/apache/kafka/pull/3848#pullrequestreview-191900203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef6953b8-aa74-4511-9ea1-e02ae2195547",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add a comment to describe the return value of the method?",
        "createdAt" : "2019-01-11T23:43:21Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df18c6b3232f3e8b9bf879b7a8dd762a5968c6c",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +642,646 @@    *         the exception that was thrown.\n    */\n  private def onPreferredReplicaElection(partitions: Set[TopicPartition],\n                                         electionType: ElectionType): Map[TopicPartition, Throwable] = {\n    info(s\"Starting preferred replica leader election for partitions ${partitions.mkString(\",\")}\")"
  },
  {
    "id" : "0e2059fe-ddcf-4678-891d-82db179bd47a",
    "prId" : 3848,
    "prUrl" : "https://github.com/apache/kafka/pull/3848#pullrequestreview-193898101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "This is an existing issue. When handing a ZK session expiration, we call beforeInitializingSession, which first clear the event queue before adding an Expire event. The issue is that the clear events including PreferredReplicaLeaderElection or ControlledShutdown and the callbacks on those events won't be called. This means the client will never receive a response and will time out. I am thinking that one way to fix this is the following: go through each cleared event and if the event has a callback, call the callback with an error.",
        "createdAt" : "2019-01-15T00:04:20Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "1db53abf-74aa-4ace-8b25-b7d539363c0c",
        "parentId" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "authorId" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "body" : "Makes sense. Do you want me to implement this for `PreferredReplicaLeaderElection` _and `ControlledShutdown`_? What testing would you like to see?",
        "createdAt" : "2019-01-16T11:16:09Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "tags" : [
        ]
      },
      {
        "id" : "46117d22-0e70-46dd-b273-62e25bfed6f3",
        "parentId" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Right now, only PreferredReplicaLeaderElection and ControlledShutdown have callbacks. So, we can just handle them. As for testing, perhaps we can queue up some events in ControllerEventManager and call clearAndPut(), and assert that the queued events' callback are called.",
        "createdAt" : "2019-01-17T23:55:56Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df18c6b3232f3e8b9bf879b7a8dd762a5968c6c",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +1562,1566 @@\n    override def handleProcess(): Unit = {\n      if (!isActive) {\n        callback(Map.empty, partitionsFromAdminClientOpt match {\n          case Some(partitions) => partitions.map(partition => partition -> new ApiError(Errors.NOT_CONTROLLER, null)).toMap"
  },
  {
    "id" : "d7394551-afc0-47cc-97c8-da207d93262b",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-93799718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c34c6cb-528c-491b-aa77-2d7dc1be7470",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Perhaps we should have a log message?",
        "createdAt" : "2018-02-02T22:21:32Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "785640b7-014e-49d0-9d26-e06e3453708c",
        "parentId" : "4c34c6cb-528c-491b-aa77-2d7dc1be7470",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Since we have a log message in the process() method of BrokerModifications, do we need another one here? For other events, logging is in `process()` method of the event.",
        "createdAt" : "2018-02-03T01:15:58Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +1520,1524 @@  override val path: String = BrokerIdZNode.path(brokerId)\n\n  override def handleDataChange(): Unit = {\n    eventManager.put(controller.BrokerModifications)\n  }"
  },
  {
    "id" : "549b8e7f-9665-4d71-973c-dbcb7ddb7ed6",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-102563161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df864efb-d0d6-44f7-8bfc-8a1086938226",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@rajinisivaram : Sorry for the late review. Just a minor comment. It seems that we can avoid reading all brokers from ZK by passing in brokerId to BrokerModifications and only read the info for that brokerId?",
        "createdAt" : "2018-03-01T22:31:33Z",
        "updatedAt" : "2018-03-01T22:31:33Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "b3abdce9-42e4-4377-b6f0-c316c5aa14eb",
        "parentId" : "df864efb-d0d6-44f7-8bfc-8a1086938226",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@junrao Thanks for the review. Opened PR #4670.",
        "createdAt" : "2018-03-09T07:34:43Z",
        "updatedAt" : "2018-03-09T07:34:43Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +1246,1250 @@      if (!isActive) return\n      val curBrokers = zkClient.getAllBrokersInCluster.toSet\n      val updatedBrokers = controllerContext.liveBrokers.filter { broker =>\n        val existingBroker = curBrokers.find(_.id == broker.id)\n        existingBroker match {"
  },
  {
    "id" : "7fe10739-e787-450a-a285-3e8b7d82c945",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-147796250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we fix the alignment?",
        "createdAt" : "2018-08-17T23:12:21Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "b6cfc76f-1721-4b29-bc10-9a75cf65a0d8",
        "parentId" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "Can you be more specific? If you mean the indentation by 2 spaces in the trailing conditions, it seems that's the correct alignment, as also being used in line 436 and line 1172 of this file.",
        "createdAt" : "2018-08-20T19:29:50Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "0f33fa2e-b287-4e45-8fb1-d650e1abf0b4",
        "parentId" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Sorry. If there is the standard, we can just leave it as it is.",
        "createdAt" : "2018-08-20T19:51:34Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1055,1059 @@      val partitionsToActOn = controllerContext.partitionsOnBroker(id).filter { partition =>\n        controllerContext.partitionReplicaAssignment(partition).size > 1 &&\n          controllerContext.partitionLeadershipInfo.contains(partition) &&\n          !topicDeletionManager.isTopicQueuedUpForDeletion(partition.topic)\n      }"
  },
  {
    "id" : "739f2f66-9c4c-4aee-9c1b-ae12d108ab9b",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-148091532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "I think we also need to exclude the partitions being deleted in the replicatedPartitionsBrokerLeads(). Otherwise, the shutting down broker will keep retrying the controlled shutdown request on those partitions since their leaders will never be moved.",
        "createdAt" : "2018-08-17T23:21:54Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "2ae9d7eb-784a-4678-9af2-e949164d6c57",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "For the partitions being deleted, the two behaviors are\r\n1. If we exclude them in the response, then controlled shutdown will be allowed immediately even though the controller might need to issue StopReplica requests for deletion on the given broker, hence possibly blocking the topic deletion until the broker is brought up again.\r\n2. If we instead include the partitions being deleted, the controller will block the controlled shutdown for a while (bound by controlled shutdown retries) and wait for topic deletion to finish. Once the topic deletion finishes, the controlled shutdown response will be empty. Or if the controlled shutdown retries are exhausted before the topic deletion can finish, the broker will be shutdown regardless of the response.\r\nHence I think the current behavior does not violate correctness. It's a matter of trade-off. Thoughts?",
        "createdAt" : "2018-08-20T19:54:58Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "39d97128-5da2-43e5-bd0e-2094c068a4d9",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Good analysis. This is my thought. Normally, topic deletion will happen very quickly. So, if we get to a state where the deletion of a topic is started, but not completed when a broker is being shut down, chances are that another broker is down, which prevents the topic deletion from completing. In this case, having the broker retry the controlled shutdown request won't be useful and only adds latency during shutdown. ",
        "createdAt" : "2018-08-21T00:36:58Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "f7e9c3c5-68ab-44fd-833f-da3a2ba08ab7",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "That makes sense. I've made the change. Thanks!",
        "createdAt" : "2018-08-21T14:36:18Z",
        "updatedAt" : "2018-08-21T14:36:18Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1056,1060 @@        controllerContext.partitionReplicaAssignment(partition).size > 1 &&\n          controllerContext.partitionLeadershipInfo.contains(partition) &&\n          !topicDeletionManager.isTopicQueuedUpForDeletion(partition.topic)\n      }\n      val (partitionsLedByBroker, partitionsFollowedByBroker) = partitionsToActOn.partition { partition =>"
  },
  {
    "id" : "9506db26-09dc-42f8-88c3-97710ecfff98",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-148194607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It seems that we should be checking for isTopicWithDeletionStarted() instead of isTopicQueuedUpForDeletion()?",
        "createdAt" : "2018-08-21T17:10:07Z",
        "updatedAt" : "2018-08-21T17:10:07Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "e32c6d18-a275-4404-a174-34ccb7d98c0e",
        "parentId" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "At the detailed level:\r\nLet's consider topics that are enqueued for deletion but haven't started the actual deletion, maybe they have ongoing partition reassignments that would still take many hours to finish. And for those partitions during a controlled shutdown we won't try to switch leaderships because we no longer care about their availability.\r\nAnd here even if the shutting down broker is still leader for those partitions, I think it's ok to shutdown the broker immediately because again we don't care about the partition availability anymore. Using !isTopicWithDeletionStarted will still return those partitions and block the controlled shutdown, which is probably not ideal.\r\n\r\nAt a higher level:\r\nisTopicQueuedUpForDeletion is used in many other places so that for other types of operations, once a topic is enqueued, it effectively no longer exists. I think that also applies here, and using the same method consistently simplifies reasoning. Thoughts?",
        "createdAt" : "2018-08-21T18:04:54Z",
        "updatedAt" : "2018-08-21T18:04:54Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "6e2324d6-5ed8-402e-8ccb-c2f5e5ef0c0d",
        "parentId" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Thanks for the explanation. This sounds good then.",
        "createdAt" : "2018-08-21T18:36:09Z",
        "updatedAt" : "2018-08-21T18:36:09Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1080,1084 @@        controllerContext.partitionLeadershipInfo.filter {\n          case (topicPartition, leaderIsrAndControllerEpoch) =>\n            !topicDeletionManager.isTopicQueuedUpForDeletion(topicPartition.topic) &&\n              leaderIsrAndControllerEpoch.leaderAndIsr.leader == id &&\n              controllerContext.partitionReplicaAssignment(topicPartition).size > 1"
  },
  {
    "id" : "bb5cde64-e46d-46a0-8337-c9067df552b7",
    "prId" : 5821,
    "prUrl" : "https://github.com/apache/kafka/pull/5821#pullrequestreview-179630545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5284d66-6829-45c1-9b98-bd47e80bf6c4",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we fix the javadoc above?",
        "createdAt" : "2018-11-13T23:11:36Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "d4e305f3-3bac-48e4-b386-043fc3e9ef71",
        "parentId" : "c5284d66-6829-45c1-9b98-bd47e80bf6c4",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Fixed.",
        "createdAt" : "2018-11-29T05:09:40Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "aec83c36f2b7f46b7ffc1990f56e60f0a9811149",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +201,205 @@   * @return The number of partitions that the broker still leads.\n   */\n  def controlledShutdown(id: Int, brokerEpoch: Long, controlledShutdownCallback: Try[Set[TopicPartition]] => Unit): Unit = {\n    val controlledShutdownEvent = ControlledShutdown(id, brokerEpoch, controlledShutdownCallback)\n    eventManager.put(controlledShutdownEvent)"
  },
  {
    "id" : "25bee507-7738-44c3-890e-0b94c610d6ce",
    "prId" : 5821,
    "prUrl" : "https://github.com/apache/kafka/pull/5821#pullrequestreview-179631110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Normally, when we call onBrokerFailure(), the passed in deadbrokers won't be in controllerContext.liveBrokers, which is used by onBrokerFailure() through PartitionStateMachine/ReplicaStateMachine. With this change, this may not be true. Will that have any impact?",
        "createdAt" : "2018-11-15T22:15:45Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "83b496dd-578d-417c-9077-9dd735494298",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "This is a very good point. I think it is fine because the bounced broker will reject the control requests anyway because the cached broker epoch has not been updated yet. \r\n\r\nHowever, this brings up another question: do we actually need to call `onBrokerFailure()` for bounced brokers? \r\nAfter a second thought, I think the answer is no because the end goal of controller handling bounced brokers in BrokerChange event is to make sure the quickly bounced brokers will be initialized correctly and the end partition/replica states will be the same with and without calling `onBrokerFailure` for the bounced brokers (if there are no new brokers and dead brokers). In this case, only calling `onBrokerStartup` is sufficient. Invoking `onBrokerFailure` first is a correct and safe option but it comes with some overhead because we need to perform leader election and send out the StopReplica/LeaderAndIsr/UpdateMetadata, which are not necessary. Previously I thought that missing `onBrokerFailure` will cause correctness issue because we might miss some state clean up but looks like it is not the case. Also note that if we use controlled shutdown to shutdown and restart the broker, the leadership election actually happens before processing the BrokerChangeEvent.\r\n\r\nTL;DR:\r\nTo be more specific for your original question (why updating the live brokers first then invoke `onBrokerFailure` is fine), there are three places where we use the live brokers informartion in `onBrokerFailure`:\r\n1. Determine whether we need to transition partition states to OfflinePartition: since at the time of the BrokerChange event processing, the bounced brokers are alive so there will not be offline partitions.\r\n2. Determine which brokers we want to consider when performing leader election in `partitionStateMachine.triggerOnlinePartitionStateChange()`: since the bounced brokers are online at that time so we should consider them.\r\n3. Determine which brokers we need to send out control requests and which brokers we need to include in the live brokers field in UpdateMetadataRequest: since the bounced brokers will not accept control requests anyway so the first point doesn't matter. For the second point, the bounced brokers are live so we don't want to exclude them in the UpdateMetadataRequest.",
        "createdAt" : "2018-11-16T09:48:12Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      },
      {
        "id" : "52c8db54-15b3-442d-9563-555dd5c67d6f",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@hzxa21 : Overall, I agree with your assessment that the onBrokerFailure() call seems redundant. The only thing is that it can force a leader epoch change. Suppose that broker 1 is a bounced broker and is the current leader. If we skip onBrokerFailure(), the controller just keeps broker 1 as the leader w/o bumping the leader epoch. This means that the follower won't go through leader epoch based log truncation, which maybe needed since broker 1 may not have all the data in its local log after the bounce. So, perhaps we can't skip onBrokerFailure(). The next question is should the live broker list exclude the bounced brokers when we call onBrokerFailure(). It seems that we should since live broker list influences which broker is the new leader. If the bounced brokers are still in the live broker list and are the current leaders, those leaders' epoch won't change. So, in summary, it seems that we should still call onBrokerFailure() but excluding bounced brokers from live broker list first. We then add the bounced brokers to live broker list and call onBrokerStartup().",
        "createdAt" : "2018-11-28T00:36:16Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "6b037fdc-c822-4a22-9eb8-0c8faf6c71b7",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "I agree. Also after an offline discussion with Dong, we agree that the benefit of optimizing for quickly bounced brokers is minor and since in normal scenario we will go through onBrokerFailure and then onBrokerStartup for the bounce brokers, it is better to do the same thing here ( invoke onBrokerFailure() and then update live brokers).\r\n\r\nThanks for the comment. I will update the PR accordingly.",
        "createdAt" : "2018-11-28T00:57:13Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      },
      {
        "id" : "36903fde-5687-4aa0-8f59-eceb2587e09a",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2018-11-29T05:13:31Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "aec83c36f2b7f46b7ffc1990f56e60f0a9811149",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +1292,1296 @@      if (bouncedBrokerIds.nonEmpty) {\n        controllerContext.removeLiveBrokersAndEpochs(bouncedBrokerIds)\n        onBrokerFailure(bouncedBrokerIdsSorted)\n        controllerContext.addLiveBrokersAndEpochs(bouncedBrokerAndEpochs)\n        onBrokerStartup(bouncedBrokerIdsSorted)"
  },
  {
    "id" : "1b1a0fd8-1a26-42d8-8642-531e97a68db5",
    "prId" : 5821,
    "prUrl" : "https://github.com/apache/kafka/pull/5821#pullrequestreview-179631276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68140097-d07a-4e5f-9275-f41e4017a392",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should we log the broker epoch in addition to the broker id?",
        "createdAt" : "2018-11-15T22:18:43Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0793d8e7-e7cd-487e-999a-d68dad7b99b3",
        "parentId" : "68140097-d07a-4e5f-9275-f41e4017a392",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "The broker epoch is already logged at the end of the broker change event: https://github.com/apache/kafka/pull/5821/files/88e4eb606fc72761a17378ee6bdf6732765808ef#diff-ed90e8ecc5439a5ede5e362255d11be1R1305",
        "createdAt" : "2018-11-29T05:14:32Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "aec83c36f2b7f46b7ffc1990f56e60f0a9811149",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +1280,1284 @@        s\"deleted brokers: ${deadBrokerIdsSorted.mkString(\",\")}, \" +\n        s\"bounced brokers: ${bouncedBrokerIdsSorted.mkString(\",\")}, \" +\n        s\"all live brokers: ${liveBrokerIdsSorted.mkString(\",\")}\")\n\n      newBrokerAndEpochs.keySet.foreach(controllerContext.controllerChannelManager.addBroker)"
  },
  {
    "id" : "e558401d-06f0-4d35-9caf-88d508102e19",
    "prId" : 5869,
    "prUrl" : "https://github.com/apache/kafka/pull/5869#pullrequestreview-171845373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a078c5c7-3e9d-4f6e-abf9-03cb62d61362",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should we change the comment in line 361 and 362 accordingly?",
        "createdAt" : "2018-11-05T23:36:05Z",
        "updatedAt" : "2018-11-06T02:47:03Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "1808e02f-c6dc-4a4b-8e6a-6a98029a97f1",
        "parentId" : "a078c5c7-3e9d-4f6e-abf9-03cb62d61362",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2018-11-06T02:31:49Z",
        "updatedAt" : "2018-11-06T02:47:03Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2093c8673a07898cf2271274efe351d2b8e64fa",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +358,362 @@    newBrokers.foreach(controllerContext.replicasOnOfflineDirs.remove)\n    val newBrokersSet = newBrokers.toSet\n    val existingBrokers = controllerContext.liveOrShuttingDownBrokerIds -- newBrokers\n    // Send update metadata request to all the existing brokers in the cluster so that they know about the new brokers\n    // via this update. No need to include any partition states in the request since there are no partition state changes."
  },
  {
    "id" : "f7109a2c-971c-47b1-a664-342760b5ad94",
    "prId" : 5869,
    "prUrl" : "https://github.com/apache/kafka/pull/5869#pullrequestreview-171847733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47c80126-ee04-407a-bcc0-36fef8390bbd",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Probably change the comment above from \"inform brokers of the offline replica\" to \"inform brokers of the offline brokers\"?",
        "createdAt" : "2018-11-06T02:35:25Z",
        "updatedAt" : "2018-11-06T02:47:03Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "5bb98877-1545-4f13-b3da-6435902ad3ed",
        "parentId" : "47c80126-ee04-407a-bcc0-36fef8390bbd",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2018-11-06T02:47:08Z",
        "updatedAt" : "2018-11-06T02:47:08Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2093c8673a07898cf2271274efe351d2b8e64fa",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +465,469 @@    // Note that during leader re-election, brokers update their metadata\n    if (partitionsWithoutLeader.isEmpty) {\n      sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, Set.empty)\n    }\n  }"
  },
  {
    "id" : "c7b18488-1fba-47b0-bc25-8321bbcd27d2",
    "prId" : 6588,
    "prUrl" : "https://github.com/apache/kafka/pull/6588#pullrequestreview-228014740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d37ceb5-39b7-4c95-8436-2497ec30a496",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : ":+1: ",
        "createdAt" : "2019-04-17T22:06:08Z",
        "updatedAt" : "2019-04-24T22:00:55Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b68fbe3cb047ef1c25bacb7c65955805306c96c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +74,78 @@  private val stateChangeLogger = new StateChangeLogger(config.brokerId, inControllerContext = true, None)\n  val controllerContext = new ControllerContext\n  var controllerChannelManager: ControllerChannelManager = _\n\n  // have a separate scheduler for the controller to be able to start and stop independently of the kafka server"
  },
  {
    "id" : "b62b2b2e-dfcc-41e6-a498-b4efea40f059",
    "prId" : 6588,
    "prUrl" : "https://github.com/apache/kafka/pull/6588#pullrequestreview-228445325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69ae78b9-188f-40a1-8387-ce968c6616cd",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "It is fine here but the nice thing of using `Option` or more precisely `Option.apply` is that it handles `null` and returns the type `Option[T]` instead of `Some[T]` by `Some.apply`.",
        "createdAt" : "2019-04-17T22:11:06Z",
        "updatedAt" : "2019-04-24T22:00:55Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "c6cc74e2-5d10-4bb6-8b9b-a8246a37848a",
        "parentId" : "69ae78b9-188f-40a1-8387-ce968c6616cd",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, I just changed to `Some` because we have a non-nullable object. I thought it is a little cleaner, but it's a bit subjective.",
        "createdAt" : "2019-04-18T18:39:25Z",
        "updatedAt" : "2019-04-24T22:00:55Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b68fbe3cb047ef1c25bacb7c65955805306c96c",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +493,497 @@    partitionStateMachine.handleStateChanges(newPartitions.toSeq, NewPartition)\n    replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, NewReplica)\n    partitionStateMachine.handleStateChanges(newPartitions.toSeq, OnlinePartition, Some(OfflinePartitionLeaderElectionStrategy))\n    replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, OnlineReplica)\n  }"
  },
  {
    "id" : "7027c590-07cf-4582-bc3e-50808aff7522",
    "prId" : 6642,
    "prUrl" : "https://github.com/apache/kafka/pull/6642#pullrequestreview-233272419",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63270a03-3454-473a-aa59-7714dc5cce24",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Unused import ApiKeys, StopReplicaResponse.",
        "createdAt" : "2019-05-02T22:51:38Z",
        "updatedAt" : "2019-05-13T23:04:49Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3d0db41a3c6ca30e89f9e83db0959b2bb88eef9",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +43,47 @@import scala.util.{Failure, Try}\n\nsealed trait ElectionType\nobject AutoTriggered extends ElectionType\nobject ZkTriggered extends ElectionType"
  },
  {
    "id" : "737d37d5-b24a-443e-bc7b-b70faf68c87f",
    "prId" : 6642,
    "prUrl" : "https://github.com/apache/kafka/pull/6642#pullrequestreview-233272419",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fa8c9fd-0bab-47f1-acc8-9f88e93f374d",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should we log this event too?",
        "createdAt" : "2019-05-07T21:08:19Z",
        "updatedAt" : "2019-05-13T23:04:49Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3d0db41a3c6ca30e89f9e83db0959b2bb88eef9",
    "line" : 244,
    "diffHunk" : "@@ -1,1 +999,1003 @@  private def processAutoPreferredReplicaLeaderElection(): Unit = {\n    if (!isActive) return\n    try {\n      info(\"Processing automatic preferred replica leader election\")\n      checkAndTriggerAutoLeaderRebalance()"
  },
  {
    "id" : "68d9e7ce-4a6e-481b-acfc-698af899136a",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-238627339",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74359d36-83cd-447f-b991-f8c4a77bb89a",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "These three imports can be combined with `{ }` (I think that's the preferred style with scala)",
        "createdAt" : "2019-05-13T21:44:52Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "a8d9ea9d-cd9e-4ec3-ae4e-b2963ae59470",
        "parentId" : "74359d36-83cd-447f-b991-f8c4a77bb89a",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I try to avoid that style as it makes it more difficult to deal with merge conflicts when adding or removing unrelated imports.",
        "createdAt" : "2019-05-16T20:42:38Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +30,34 @@import kafka.zk._\nimport kafka.zookeeper.{StateChangeHandler, ZNodeChangeHandler, ZNodeChildChangeHandler}\nimport org.apache.kafka.common.ElectionType\nimport org.apache.kafka.common.KafkaException\nimport org.apache.kafka.common.TopicPartition"
  },
  {
    "id" : "c5d30f3d-8c75-4147-8879-c48c8bb8643b",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-240919946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af905e25-c2d5-4984-b5bb-b473dc1640a0",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Seems the criteria for `electablePartitions` only accounts for preferred leader election. For unclean leader election, this seems to work since the set of partitions eligible for unclean election is a subset, but maybe it is better to select the replicas explicitly?",
        "createdAt" : "2019-05-22T16:35:20Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "63dd2e76-135f-45ba-b703-7c07c144ef35",
        "parentId" : "af905e25-c2d5-4984-b5bb-b473dc1640a0",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Yeah. I am hesitant to make this change in this PR. As it would duplicate code in the PartitionStateMachine here and as a result creating technical debt. In the long term, I  want to refactor this code so that all of these decisions are made in the PartitionStateMachine which returns enough information for this layer to either filter those \"errors\" or to pass them along to the client.\r\n\r\nWhat do you think?",
        "createdAt" : "2019-05-22T19:10:29Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "00019efe-e1f6-4cc5-827d-582eb26f929d",
        "parentId" : "af905e25-c2d5-4984-b5bb-b473dc1640a0",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I was considering something simple like just collecting all the offline partitions.",
        "createdAt" : "2019-05-22T19:53:25Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "de775856-4598-4da8-903f-ddcf897c72a6",
        "parentId" : "af905e25-c2d5-4984-b5bb-b473dc1640a0",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If we don't do this, then I think we won't return the `ELECTION_NOT_NEEDED` error consistently with the current logic. The only time you would get it would be for partitions whose preferred leader was already assigned, but that would be weird.",
        "createdAt" : "2019-05-23T00:47:37Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 289,
    "diffHunk" : "@@ -1,1 +1557,1561 @@        }\n\n        val results = onReplicaElection(electablePartitions, electionType, electionTrigger).mapValues {\n          case Left(ex) =>\n            if (ex.isInstanceOf[StateChangeFailedException]) {"
  },
  {
    "id" : "af526f4b-1a64-464e-ae4f-98f6f6a8ff49",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-241859922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "003a8f5a-e9ce-4698-b23a-d8926fffa9e9",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: could be `getOrElse(zkClient.getPreferredReplicaElection)`\r\n\r\nBy the way, I think we are assuming that `electionType` is PREFERRED if `partitionsFromAdminClientOpt` is not defined. This is true of course, but I'm wondering if we should validate the assumption.",
        "createdAt" : "2019-05-23T00:39:33Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0421c090-7bc6-45c1-aaaf-06eee911c5ee",
        "parentId" : "003a8f5a-e9ce-4698-b23a-d8926fffa9e9",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Yeah. I prefer (no pun intended :smile:) to leave it as is. I think the core of the issue is that this method is trying to unify to many different behavior. In the future I want to split this method into at least two different methods. What do you think?",
        "createdAt" : "2019-05-24T17:25:41Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 236,
    "diffHunk" : "@@ -1,1 +1525,1529 @@      // leader elections and we get the `path exists` check for free\n      if (electionTrigger == AdminClientTriggered || zkClient.registerZNodeChangeHandlerAndCheckExistence(preferredReplicaElectionHandler)) {\n        val partitions = partitionsFromAdminClientOpt match {\n          case Some(partitions) => partitions\n          case None => zkClient.getPreferredReplicaElection"
  },
  {
    "id" : "1e86755e-a927-465b-bcd5-5cfc42c2dc8c",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-241856547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47f11bc1-7e6c-457e-855e-53c8777748dc",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess we have a log message elsewhere?",
        "createdAt" : "2019-05-23T01:04:53Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d8086d1c-4d00-4953-9608-99cc53d0e0ba",
        "parentId" : "47f11bc1-7e6c-457e-855e-53c8777748dc",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I am reproducing the old code's behavior. Regarding logging, we would see a `LeaderAndIsrRequest` event from the `PartitionStateMachine`, right?",
        "createdAt" : "2019-05-24T17:17:41Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +676,680 @@              error(s\"Error completing replica leader election ($electionType) for partition $tp\", throwable)\n            }\n          case (_, Right(_)) => // Ignored; No need to log or throw exception for the success cases\n        }\n      }"
  },
  {
    "id" : "71b23faf-8c7f-4382-b31f-fa70365d4d54",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-240919946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5e5e1f1-9c6b-4f1d-b7e9-c8b159476ec2",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: seems misaligned",
        "createdAt" : "2019-05-23T01:07:30Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 210,
    "diffHunk" : "@@ -1,1 +1506,1510 @@    callback(\n      partitionsFromAdminClientOpt.fold(Map.empty[TopicPartition, Either[ApiError, Int]]) { partitions =>\n        partitions.map(partition => partition -> Left(new ApiError(Errors.NOT_CONTROLLER, null)))(breakOut)\n      }\n    )"
  },
  {
    "id" : "bbbc374c-363e-4180-8514-d25d0075d456",
    "prId" : 6767,
    "prUrl" : "https://github.com/apache/kafka/pull/6767#pullrequestreview-241827428",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e0081b7-e78b-4ac1-a371-0e898f8a2c2e",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Shouldn't we throw an exception here too?",
        "createdAt" : "2019-05-24T07:52:32Z",
        "updatedAt" : "2019-05-24T07:52:33Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "d830458a-e934-46f6-ac45-daa118586e00",
        "parentId" : "2e0081b7-e78b-4ac1-a371-0e898f8a2c2e",
        "authorId" : "462b08c4-4553-46cf-b1cf-8fd3349738fb",
        "body" : "Well, I have been still thinking about it; If we need an Exception, I will apply it in next cleanup. @abbccdda Could you tell me your opinion?",
        "createdAt" : "2019-05-24T13:27:18Z",
        "updatedAt" : "2019-05-24T13:27:18Z",
        "lastEditedBy" : "462b08c4-4553-46cf-b1cf-8fd3349738fb",
        "tags" : [
        ]
      },
      {
        "id" : "94193b87-02ff-4c33-905d-39bb7ebf4266",
        "parentId" : "2e0081b7-e78b-4ac1-a371-0e898f8a2c2e",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Previously we would throw an exception (`MatchError`) and now we ignore it. That seems worse, right?",
        "createdAt" : "2019-05-24T14:54:39Z",
        "updatedAt" : "2019-05-24T14:54:40Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "684542ba-af46-4860-805b-a10aafacb927",
        "parentId" : "2e0081b7-e78b-4ac1-a371-0e898f8a2c2e",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, you are right. I went ahead and merged because it was being addressed in #6686.",
        "createdAt" : "2019-05-24T15:57:21Z",
        "updatedAt" : "2019-05-24T15:57:22Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b9b6fbde-accf-4676-b692-e67e1fb43e9b",
        "parentId" : "2e0081b7-e78b-4ac1-a371-0e898f8a2c2e",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Makes sense, we can just do it in #6686 then.",
        "createdAt" : "2019-05-24T16:08:28Z",
        "updatedAt" : "2019-05-24T16:08:28Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "54e32f8ead515b4f8f82f25d5f63c0b53b3c45ed",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1604,1608 @@          processStartup()\n        case ShutdownEventThread =>\n          // not handled here\n      }\n    } catch {"
  },
  {
    "id" : "31008781-a60b-42ab-88cc-6072c5dfca48",
    "prId" : 7128,
    "prUrl" : "https://github.com/apache/kafka/pull/7128#pullrequestreview-267975719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4314db97-aa85-4acd-a8ad-f35fe332ae2b",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Just a quick comment to make things super clear:\r\n\r\nReassignment #1\r\n```\r\nORS = [1,2,3]\r\nTRS = [3,4,5]\r\n```\r\nReassignment #2 (overriding)\r\n```\r\nTRS = [3,5,6]\r\nOVRS = [5]\r\nURS = [4]\r\n```",
        "createdAt" : "2019-07-29T18:55:58Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "8538961c460c50be1828a050793f45d8558ea9b9",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +526,530 @@   *       (it is essentially (RS - ORS) - URS)\n   *\n   *   1 Set RS = ORS + OVRS, AR = OVRS, RS = [] in memory\n   *   2 Send LeaderAndIsr request with RS = ORS + OVRS, AR = [], RS = [] to all brokers in ORS + OVRS\n   *     (because the ongoing reassignment is in phase A, we know we wouldn't have a leader in URS"
  },
  {
    "id" : "caf2577d-ee31-471b-975c-24059daae009",
    "prId" : 7128,
    "prUrl" : "https://github.com/apache/kafka/pull/7128#pullrequestreview-284448239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "120d5111-b199-4ea7-a670-1df85cfc716f",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Do we want to guarantee that if controllerContext.partitionReplicaAssignment(tp).isBeingReassigned then controllerContext.partitionsBeingReassigned(tp).isPresent?\r\nAt this point, I don't know if we should update the in-memory and ZK replica assignment to one that is not being reassigned. Since we remove the partition from the partitionsBeingReassigned collection, it seems to make sense to also update in-memory and ZK to truly treat the reassignment as failed.\r\nYet, do we need to send LeaderAndIsr requests if we change those?\r\n\r\nThe one scenario where this divergence could happen (controllerContext.partitionReplicaAssignment(tp).isBeingReassigned is true and controllerContext.partitionsBeingReassigned(tp).isPresent is not)\r\nis if we have an UNKNOWN_SERVER_ERROR during the reassignment (line 723)",
        "createdAt" : "2019-07-29T18:57:18Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "fe9e21ba-c667-482c-aa2f-1a5dbdc9ae91",
        "parentId" : "120d5111-b199-4ea7-a670-1df85cfc716f",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I think we should try to avoid having an in-memory state that does not match the ZK state.  Can you think of a case where that might happen here?",
        "createdAt" : "2019-09-04T21:41:13Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "6012a3d7-81ee-4b37-a897-ec1117bf9648",
        "parentId" : "120d5111-b199-4ea7-a670-1df85cfc716f",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "I think that might happen if we throw an error somewhere inside `onPartitionReassignment` after it updates the in-memory state. This wasn't handled previously either\r\n\r\nThe question here is more about the two separate in-memory collections diverging - `partitionsBeingReassigned` and `partitionReplicaAssignment`. If we get an unknown error inside `onPartitionReassignment` we may have updated the in-memory `partitionReplicaAssignment` but in `maybeTriggerPartitionReassignment` we catch the error and remove the partition from `partitionsBeingReassigned`. So we may end up with an in-memory state indicating a reassignment but none scheduled due to some failure.\r\n\r\nI can't think of a great way to guard against this. It may be good to leave it as is",
        "createdAt" : "2019-09-05T00:06:41Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "010f756f-f9ef-46b5-9acd-f63b98eff469",
        "parentId" : "120d5111-b199-4ea7-a670-1df85cfc716f",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "It's difficult to handle arbitrary exceptions propagating from any line of code.  If we're confident that we've carefully checked the code in onPartitionReassignment, let's not block on this for now.",
        "createdAt" : "2019-09-05T18:13:54Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "8538961c460c50be1828a050793f45d8558ea9b9",
    "line" : 316,
    "diffHunk" : "@@ -1,1 +733,737 @@            error(s\"Ignoring request to reassign partition $tp that doesn't exist.\")\n            partitionsToBeRemovedFromReassignment.add(tp)\n            reassignmentResults.put(tp, new ApiError(Errors.UNKNOWN_TOPIC_OR_PARTITION, \"The partition does not exist.\"))\n        }\n      }"
  },
  {
    "id" : "1c1acec6-0e79-4568-8a28-8d1748760a39",
    "prId" : 7128,
    "prUrl" : "https://github.com/apache/kafka/pull/7128#pullrequestreview-283910297",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "145e1e33-df62-4bbf-a001-2b93bbeb050e",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "I've added in `assert()` calls throughout the code -- they helped me catch some bugs. I am wondering whether it's worth it to keep them for one AK release to catch issues we didn't expect or to remove them. I know we've had some before but I think I've heard we're moving away from them",
        "createdAt" : "2019-07-30T17:22:07Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "cb80bb83-2419-4af7-aa13-bcf826ac9aa7",
        "parentId" : "145e1e33-df62-4bbf-a001-2b93bbeb050e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "`assert` is turned off by certain JVM flags.  So it's not really helpful for catching errors, since it is often turned off.\r\n\r\nConsidering that we already have this problem in the controller, it may be worth doing the conversion to something else en masse, rather than worrying about it in this PR.",
        "createdAt" : "2019-09-04T21:36:55Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "8538961c460c50be1828a050793f45d8558ea9b9",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +585,589 @@      oldReplicas = oldReplicas,\n      newReplicas = reassignedPartitionContext.newReplicas)\n    assert(reassignedPartitionContext.newReplicas == partitionAssignment.targetReplicas,\n      s\"newReplicas ${reassignedPartitionContext.newReplicas} were not equal to the expected targetReplicas ${partitionAssignment.targetReplicas}\")\n    val targetReplicas = partitionAssignment.targetReplicas"
  },
  {
    "id" : "ad28d9ae-9dca-42a8-9e68-056b1f8657b9",
    "prId" : 7128,
    "prUrl" : "https://github.com/apache/kafka/pull/7128#pullrequestreview-283909206",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5cbe4c2-2157-465c-bd44-6d441c11067f",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Come to think of it this would get called on API every reassignment so it may be worth to not log it?",
        "createdAt" : "2019-07-30T17:45:15Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "9db64d50-e209-48b3-8643-73f2caf5ebc9",
        "parentId" : "e5cbe4c2-2157-465c-bd44-6d441c11067f",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I think it's OK to keep the log.",
        "createdAt" : "2019-09-04T21:34:19Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "8538961c460c50be1828a050793f45d8558ea9b9",
    "line" : 527,
    "diffHunk" : "@@ -1,1 +1038,1042 @@  private def removePartitionsFromZkReassignment(partitionsToBeRemoved: Set[TopicPartition]): Unit = {\n    if (!zkClient.reassignPartitionsInProgress()) {\n      debug(s\"Cannot remove partitions $partitionsToBeRemoved from ZooKeeper because there is no ZooKeeper reassignment present\")\n      return\n    }"
  },
  {
    "id" : "eeda4409-1e6e-4fe9-8b43-5ed542582b1d",
    "prId" : 7128,
    "prUrl" : "https://github.com/apache/kafka/pull/7128#pullrequestreview-285667222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40c83634-49c0-4203-b898-8ca95178d005",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "nit: can `replicaAssignment.targetReplicas` be different than `targetReplicas`? If not this log statement might be a bit redundant.",
        "createdAt" : "2019-09-09T15:38:12Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      },
      {
        "id" : "76a03605-f080-44e8-a000-685b0c6f6c44",
        "parentId" : "40c83634-49c0-4203-b898-8ca95178d005",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Yes, that's the reason of the log. It is called when we are overriding an existing reassignment. i.e an on-going reassignment is happening (`replicaAssignment`) and we call the function to save a new reassignment (`targetReplicas`)\r\n\r\nThey can be the same, sure, if the user sends the same request twice in a row - but it is worth logging regardless. At the very least, it would help diagnose bugs easier. Besides that, I think it's a very useful log",
        "createdAt" : "2019-09-09T17:32:47Z",
        "updatedAt" : "2019-09-09T17:48:38Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "8538961c460c50be1828a050793f45d8558ea9b9",
    "line" : 708,
    "diffHunk" : "@@ -1,1 +1675,1679 @@        if (reassignmentIsInProgress) {\n          info(s\"Overriding old reassignment for partition $partition \" +\n            s\"(with target replicas ${replicaAssignment.targetReplicas.mkString(\",\")}) \" +\n            s\"to new target replicas (${targetReplicas.mkString(\",\")})\")\n          assert(replicaAssignment.isBeingReassigned)"
  },
  {
    "id" : "89985be9-d75b-451c-8847-89930f9611b8",
    "prId" : 7562,
    "prUrl" : "https://github.com/apache/kafka/pull/7562#pullrequestreview-304504338",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b7078d1-09ac-429e-9db5-58f39c5fcb8b",
        "parentId" : null,
        "authorId" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "body" : "This is an important side effect of the function, I think we should add this to the javadoc comment.",
        "createdAt" : "2019-10-21T13:53:24Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "cb6f8432-d515-4113-87f8-14e555ab8ed1",
        "tags" : [
        ]
      }
    ],
    "commit" : "58544a8feea84b2b5759194172cd3cd0cc0b7f17",
    "line" : 784,
    "diffHunk" : "@@ -1,1 +1631,1635 @@      }\n\n      // The latest reassignment (whether by API or through zk) always takes precedence,\n      // so remove from active zk reassignment (if one exists)\n      maybeRemoveFromZkReassignment((tp, _) => partitionsToReassign.contains(tp))"
  },
  {
    "id" : "6bde8095-69ce-4841-9704-ab6e429a1912",
    "prId" : 7562,
    "prUrl" : "https://github.com/apache/kafka/pull/7562#pullrequestreview-306577741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14843dd8-17ac-4bd1-8c32-ea90d6af731f",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Turns out we didn't need the LeaderAndIsr here? I thought we needed it to prevent the leader from adding any replica outside the replica set back into ISR. I guess our very next action is to send a LeaderAndIsr in Phase A so it doesn't matter?",
        "createdAt" : "2019-10-21T14:16:29Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "fadcc7a1-26d5-4c3a-a2b4-36f466d73552",
        "parentId" : "14843dd8-17ac-4bd1-8c32-ea90d6af731f",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I was thinking it was unnecessary because we already get an epoch bump in A3, but in the case of a cancellation, we may not hit that case at all. So perhaps we still need it. It's a bit tough at the moment to see all of the epoch bumps because some of them are hidden beneath replica state changes. Let me look at this more carefully.",
        "createdAt" : "2019-10-21T21:56:39Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e6a83964-d4e8-494b-944d-4706fb53075f",
        "parentId" : "14843dd8-17ac-4bd1-8c32-ea90d6af731f",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, I think I convinced myself that it is unnecessary. Whether we go down the phase A path or phase B path, we will get an epoch bump. In phase B, the epoch will be bumped in `moveReassignedPartitionLeaderIfRequired`.  Does that seem right?",
        "createdAt" : "2019-10-22T06:52:48Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "c217db06-611a-4a2e-a8a4-a5ab7ca7aaa1",
        "parentId" : "14843dd8-17ac-4bd1-8c32-ea90d6af731f",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "I also traced it down - I think you're right.",
        "createdAt" : "2019-10-24T13:35:10Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "58544a8feea84b2b5759194172cd3cd0cc0b7f17",
    "line" : 234,
    "diffHunk" : "@@ -1,1 +682,686 @@      val unneededReplicas = currentAssignment.replicas.diff(reassignment.replicas)\n      if (unneededReplicas.nonEmpty)\n        stopRemovedReplicasOfReassignedPartition(topicPartition, unneededReplicas)\n    }\n"
  },
  {
    "id" : "8db077fa-8fb2-4e0a-8985-84f1cd500321",
    "prId" : 7562,
    "prUrl" : "https://github.com/apache/kafka/pull/7562#pullrequestreview-306578771",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "406d98b4-9215-488e-96d5-938c06369987",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "If the controller loses connectivity at this point (after updating ZK and before calling StopReplica), would we be left with extra replicas or would the leader kick them out?",
        "createdAt" : "2019-10-21T14:23:25Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "e4c134a8-76ba-4828-b037-a7028e01be71",
        "parentId" : "406d98b4-9215-488e-96d5-938c06369987",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a good question. I think we have this problem for replica removal in general even prior to KIP-455. The StopReplica requests are all sent asynchronously and we do not await their completion as we do with topic deletion. Generally our approach is pretty brittle. I think this will be fixed by KIP-500.",
        "createdAt" : "2019-10-21T21:33:36Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "4d3cb017-1baa-468f-aad9-4536fc0d6a06",
        "parentId" : "406d98b4-9215-488e-96d5-938c06369987",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Oh. Sounds good then",
        "createdAt" : "2019-10-24T13:36:35Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "58544a8feea84b2b5759194172cd3cd0cc0b7f17",
    "line" : 230,
    "diffHunk" : "@@ -1,1 +678,682 @@      controllerContext.updatePartitionFullReplicaAssignment(topicPartition, reassignment)\n\n      // If there is a reassignment already in progress, then some of the currently adding replicas\n      // may be eligible for immediate removal, in which case we need to stop the replicas.\n      val unneededReplicas = currentAssignment.replicas.diff(reassignment.replicas)"
  },
  {
    "id" : "60c04baa-51e1-4099-96d1-4dc154316b38",
    "prId" : 7562,
    "prUrl" : "https://github.com/apache/kafka/pull/7562#pullrequestreview-306922576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53666bb9-f6bd-4c28-8cf5-9ec361ea24f7",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Curious as to why we don't fall back to the ZK-written assignment? We sometimes update ZK first which would make this a non-issue but what happens if we fail in `B3`?\r\nIf I'm reading it right, we would have `RS=TRS` in memory while ZK would contain something else. If a user retries that reassignment, we would ignore the value in ZK and only retire the old TRS replicas.",
        "createdAt" : "2019-10-21T15:53:27Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "6fc7d790-3f74-473a-8972-25183ee298c0",
        "parentId" : "53666bb9-f6bd-4c28-8cf5-9ec361ea24f7",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I changed the logic so that the controller context is only updated after we have updated the assignment state in zookeeper. Unless I missed something, there shouldn't be any need to revert to a previous state.",
        "createdAt" : "2019-10-21T23:12:23Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "fb472e70-80f0-472f-b77e-bd87486ae23c",
        "parentId" : "53666bb9-f6bd-4c28-8cf5-9ec361ea24f7",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Not in B2->B6. So the opposite could happen - if we throw an exception in between the ZK update and the memory update, we would have stale things in memory.",
        "createdAt" : "2019-10-24T18:43:51Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "7606de5b-b233-47f0-ab30-bf500a839f13",
        "parentId" : "53666bb9-f6bd-4c28-8cf5-9ec361ea24f7",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I agree this is still a weak point. If we want to handle it, it would make more sense to do it in `onPartitionReassignment` since phase B is unlikely to execute in `maybeTriggerPartitionReassignment` unless it is a no-op move. Also, I think we would have to do more than just reset the assignment state. We probably need to figure out how to retry. The controller could track a failed reassignments collection, for example, which we could periodically retry.\r\n\r\nSince this is a pre-existing problem, I will file a JIRA to follow up with this.",
        "createdAt" : "2019-10-24T23:20:29Z",
        "updatedAt" : "2019-10-24T23:20:30Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0c304e7c-f9ed-4c23-804f-77608ee12e9b",
        "parentId" : "53666bb9-f6bd-4c28-8cf5-9ec361ea24f7",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "fyi: https://issues.apache.org/jira/browse/KAFKA-9099",
        "createdAt" : "2019-10-24T23:52:04Z",
        "updatedAt" : "2019-10-24T23:52:04Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "58544a8feea84b2b5759194172cd3cd0cc0b7f17",
    "line" : 339,
    "diffHunk" : "@@ -1,1 +723,727 @@              throw e\n            case e: Throwable =>\n              error(s\"Error completing reassignment of partition $tp\", e)\n              new ApiError(Errors.UNKNOWN_SERVER_ERROR)\n          }"
  },
  {
    "id" : "596b22b8-a0ff-433b-abb1-025fa9587f08",
    "prId" : 7562,
    "prUrl" : "https://github.com/apache/kafka/pull/7562#pullrequestreview-306763719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25e87b85-4091-4ed6-a64d-79fc694738ac",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "We probably want to use different states btw ZkPartitionReassignment and ApiPartitionReassignment?",
        "createdAt" : "2019-10-24T00:00:20Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "6855eac2-6f94-4ffe-9fb9-58111dd8293b",
        "parentId" : "25e87b85-4091-4ed6-a64d-79fc694738ac",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I thought the intent of `ControllerState` was to capture a higher level breakdown of where the controller was spending its time. For example, `AlterPartitionReassignment` state is used both for the initial reassignment trigger and for ISR state changes affecting a reassigning partition. It might be useful to see a finer-grained breakdown based on the trigger, but I think practically speaking only one of them will be used in a given cluster. ",
        "createdAt" : "2019-10-24T18:03:47Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "58544a8feea84b2b5759194172cd3cd0cc0b7f17",
    "line" : 995,
    "diffHunk" : "@@ -1,1 +2121,2125 @@\ncase object ZkPartitionReassignment extends ControllerEvent {\n  override def state: ControllerState = ControllerState.AlterPartitionReassignment\n}\n"
  },
  {
    "id" : "5ddad484-76af-40cc-9da7-6848c7d8b88f",
    "prId" : 7562,
    "prUrl" : "https://github.com/apache/kafka/pull/7562#pullrequestreview-306587022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "427a01ed-3afa-4ad0-887d-9ab22f36e7a7",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "The new logic guarantees that the reassignment is stored in ZK before it is added to `partitionsBeingReassigned` - nice!",
        "createdAt" : "2019-10-24T13:47:50Z",
        "updatedAt" : "2019-10-24T23:03:07Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "58544a8feea84b2b5759194172cd3cd0cc0b7f17",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +441,445 @@\n  private def maybeResumeReassignments(shouldResume: (TopicPartition, ReplicaAssignment) => Boolean): Unit = {\n    controllerContext.partitionsBeingReassigned.foreach { tp =>\n      val currentAssignment = controllerContext.partitionFullReplicaAssignment(tp)\n      if (shouldResume(tp, currentAssignment))"
  },
  {
    "id" : "506f143f-551c-48aa-972c-2627c58b5a3a",
    "prId" : 7574,
    "prUrl" : "https://github.com/apache/kafka/pull/7574#pullrequestreview-305343016",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "331873fe-3e28-4aae-a3d1-e9e455ae4859",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "Thanks @stanislavkozlovski . I think this only gets called for the API path. Should we perform a similar check for the ZK side? This is around line 1698 in this file.",
        "createdAt" : "2019-10-22T16:27:36Z",
        "updatedAt" : "2019-10-22T16:28:07Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f82d9868bb2e9481dbd782d6cf1ffc72ac54d67",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1717,1721 @@        val replicaSet = replicas.toSet\n\n        if (replicas.isEmpty || replicas.size != replicaSet.size)\n          false\n        else if (replicas.exists(_ < 0))"
  },
  {
    "id" : "202ca860-5bf7-4c65-a59d-33f744ed0117",
    "prId" : 7611,
    "prUrl" : "https://github.com/apache/kafka/pull/7611#pullrequestreview-309056698",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8ac1b62-83fe-4e75-9da6-f5f146abcf88",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "nit: for completeness, could you also add `U3. ....` around the comments on lines 678-680",
        "createdAt" : "2019-10-30T09:23:56Z",
        "updatedAt" : "2019-10-30T09:24:02Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "189ccf5f7d73cc71e5f51edc0a7a6e04bc6be690",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +666,670 @@   */\n  private def updateCurrentReassignment(topicPartition: TopicPartition, reassignment: ReplicaAssignment): Unit = {\n    val currentAssignment = controllerContext.partitionFullReplicaAssignment(topicPartition)\n\n    if (currentAssignment != reassignment) {"
  },
  {
    "id" : "392e8eb9-9573-4fa9-96df-658fc86f0baf",
    "prId" : 8524,
    "prUrl" : "https://github.com/apache/kafka/pull/8524#pullrequestreview-401366445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a72a87db-a548-402a-9c34-bc3c3be4730c",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "We also do `controllerContext.isReplicaOnline(leaderBroker, tp)` in the caller. Do we also need it here?",
        "createdAt" : "2020-04-27T21:11:09Z",
        "updatedAt" : "2020-04-27T21:11:09Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "11524426-3bd2-46ee-bc31-bf1d514f3522",
        "parentId" : "a72a87db-a548-402a-9c34-bc3c3be4730c",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Good point. We can get rid of the isReplicaOnline() check in the caller. @leonardge : Could you summit a followup minor PR?",
        "createdAt" : "2020-04-27T21:34:05Z",
        "updatedAt" : "2020-04-27T21:34:05Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "4b5f2bd9-704c-46b9-8f6d-27e048144b02",
        "parentId" : "a72a87db-a548-402a-9c34-bc3c3be4730c",
        "authorId" : "cc18232f-f142-44e6-9255-4f0ffe19f739",
        "body" : "Sure thing!",
        "createdAt" : "2020-04-27T22:16:01Z",
        "updatedAt" : "2020-04-27T22:16:01Z",
        "lastEditedBy" : "cc18232f-f142-44e6-9255-4f0ffe19f739",
        "tags" : [
        ]
      }
    ],
    "commit" : "2fa3ceed0322b90306393e0dffc1fdbdba779613",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1079,1083 @@  private def canPreferredReplicaBeLeader(tp: TopicPartition): Boolean = {\n    val assignment = controllerContext.partitionReplicaAssignment(tp)\n    val liveReplicas = assignment.filter(replica => controllerContext.isReplicaOnline(replica, tp))\n    val isr = controllerContext.partitionLeadershipInfo(tp).leaderAndIsr.isr\n    PartitionLeaderElectionAlgorithms"
  },
  {
    "id" : "fe2fda4a-2154-4912-8f6d-ee73fc7d7273",
    "prId" : 9001,
    "prUrl" : "https://github.com/apache/kafka/pull/9001#pullrequestreview-455261235",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67104dea-01c7-4806-9634-989100531b6c",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Is it ok for us to always do `updateFeatureZNode`, since this call is idempotent?",
        "createdAt" : "2020-07-20T05:46:02Z",
        "updatedAt" : "2020-10-06T22:58:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "8145a186-24be-4f21-b4fe-3ffcae2eb991",
        "parentId" : "67104dea-01c7-4806-9634-989100531b6c",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "Not sure I understood. We will only update the `FeatureZNode` if the status is not disabled currently (see the implementation below). What am I missing?",
        "createdAt" : "2020-07-21T07:02:03Z",
        "updatedAt" : "2020-10-06T22:58:39Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "3e47abb9-05ef-40f7-9dc5-e0834c21c30b",
        "parentId" : "67104dea-01c7-4806-9634-989100531b6c",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "My pt is that since we know the outcome (feature versioning will be disabled), we don't need to do one more lookup but just try to push the update. Anyway, I think this is a nit.",
        "createdAt" : "2020-07-22T18:58:04Z",
        "updatedAt" : "2020-10-06T22:58:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "960b9455-5828-4203-ad0e-43ff5995b398",
        "parentId" : "67104dea-01c7-4806-9634-989100531b6c",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "We can not just push the update, because, we have to decide if the node needs to be created or existing node should be updated. That is why we read the node first to understand if it exists or not, then we update the existing node only if the status does not match (this avoids a ZK write in the most common cases).",
        "createdAt" : "2020-07-25T06:57:15Z",
        "updatedAt" : "2020-10-06T22:58:40Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1c79cee2ab243d95647935d2b3e7abe371bf6ea",
    "line" : 191,
    "diffHunk" : "@@ -1,1 +411,415 @@   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n   */\n  private def disableFeatureVersioning(): Unit = {\n    val newNode = FeatureZNode(FeatureZNodeStatus.Disabled, Features.emptyFinalizedFeatures())\n    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)"
  },
  {
    "id" : "e71d19ae-4bc7-443b-8584-027aab16a685",
    "prId" : 9001,
    "prUrl" : "https://github.com/apache/kafka/pull/9001#pullrequestreview-456880892",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a57917b-f4d5-4421-ae25-b7ff05a60275",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "nit: `zkClient.getDataAndVersion(FeatureZNode.path)._2` should be suffice",
        "createdAt" : "2020-07-23T02:50:16Z",
        "updatedAt" : "2020-10-06T22:58:39Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "09179314-1746-46e6-b70a-8c8e3bee41ba",
        "parentId" : "2a57917b-f4d5-4421-ae25-b7ff05a60275",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "`newVersion` is more readable than `_2`.",
        "createdAt" : "2020-07-25T06:39:17Z",
        "updatedAt" : "2020-10-06T22:58:40Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      },
      {
        "id" : "272e2424-da22-4c36-b033-3811cc0062e1",
        "parentId" : "2a57917b-f4d5-4421-ae25-b7ff05a60275",
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "Yea, I mean you could use `val newVersion = zkClient.getDataAndVersion(FeatureZNode.path)._2`, but it's up to you.",
        "createdAt" : "2020-07-29T22:30:26Z",
        "updatedAt" : "2020-10-06T22:58:40Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1c79cee2ab243d95647935d2b3e7abe371bf6ea",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +285,289 @@    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n    zkClient.createFeatureZNode(newNode)\n    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n    newVersion\n  }"
  }
]