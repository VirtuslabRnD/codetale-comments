[
  {
    "id" : "773d4b80-c279-43b2-88bf-ebcb1c0b2cb4",
    "prId" : 3848,
    "prUrl" : "https://github.com/apache/kafka/pull/3848#pullrequestreview-191900203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef6953b8-aa74-4511-9ea1-e02ae2195547",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add a comment to describe the return value of the method?",
        "createdAt" : "2019-01-11T23:43:21Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df18c6b3232f3e8b9bf879b7a8dd762a5968c6c",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +642,646 @@    *         the exception that was thrown.\n    */\n  private def onPreferredReplicaElection(partitions: Set[TopicPartition],\n                                         electionType: ElectionType): Map[TopicPartition, Throwable] = {\n    info(s\"Starting preferred replica leader election for partitions ${partitions.mkString(\",\")}\")"
  },
  {
    "id" : "0e2059fe-ddcf-4678-891d-82db179bd47a",
    "prId" : 3848,
    "prUrl" : "https://github.com/apache/kafka/pull/3848#pullrequestreview-193898101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "This is an existing issue. When handing a ZK session expiration, we call beforeInitializingSession, which first clear the event queue before adding an Expire event. The issue is that the clear events including PreferredReplicaLeaderElection or ControlledShutdown and the callbacks on those events won't be called. This means the client will never receive a response and will time out. I am thinking that one way to fix this is the following: go through each cleared event and if the event has a callback, call the callback with an error.",
        "createdAt" : "2019-01-15T00:04:20Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "1db53abf-74aa-4ace-8b25-b7d539363c0c",
        "parentId" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "authorId" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "body" : "Makes sense. Do you want me to implement this for `PreferredReplicaLeaderElection` _and `ControlledShutdown`_? What testing would you like to see?",
        "createdAt" : "2019-01-16T11:16:09Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "491bcd91-bc8d-4f54-b5fd-d6c7be5e8693",
        "tags" : [
        ]
      },
      {
        "id" : "46117d22-0e70-46dd-b273-62e25bfed6f3",
        "parentId" : "9240abc8-4848-40d9-b918-7d981e4a2ed8",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Right now, only PreferredReplicaLeaderElection and ControlledShutdown have callbacks. So, we can just handle them. As for testing, perhaps we can queue up some events in ControllerEventManager and call clearAndPut(), and assert that the queued events' callback are called.",
        "createdAt" : "2019-01-17T23:55:56Z",
        "updatedAt" : "2019-01-25T09:57:17Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df18c6b3232f3e8b9bf879b7a8dd762a5968c6c",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +1562,1566 @@\n    override def handleProcess(): Unit = {\n      if (!isActive) {\n        callback(Map.empty, partitionsFromAdminClientOpt match {\n          case Some(partitions) => partitions.map(partition => partition -> new ApiError(Errors.NOT_CONTROLLER, null)).toMap"
  },
  {
    "id" : "d7394551-afc0-47cc-97c8-da207d93262b",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-93799718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c34c6cb-528c-491b-aa77-2d7dc1be7470",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Perhaps we should have a log message?",
        "createdAt" : "2018-02-02T22:21:32Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "785640b7-014e-49d0-9d26-e06e3453708c",
        "parentId" : "4c34c6cb-528c-491b-aa77-2d7dc1be7470",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Since we have a log message in the process() method of BrokerModifications, do we need another one here? For other events, logging is in `process()` method of the event.",
        "createdAt" : "2018-02-03T01:15:58Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +1520,1524 @@  override val path: String = BrokerIdZNode.path(brokerId)\n\n  override def handleDataChange(): Unit = {\n    eventManager.put(controller.BrokerModifications)\n  }"
  },
  {
    "id" : "549b8e7f-9665-4d71-973c-dbcb7ddb7ed6",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-102563161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df864efb-d0d6-44f7-8bfc-8a1086938226",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@rajinisivaram : Sorry for the late review. Just a minor comment. It seems that we can avoid reading all brokers from ZK by passing in brokerId to BrokerModifications and only read the info for that brokerId?",
        "createdAt" : "2018-03-01T22:31:33Z",
        "updatedAt" : "2018-03-01T22:31:33Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "b3abdce9-42e4-4377-b6f0-c316c5aa14eb",
        "parentId" : "df864efb-d0d6-44f7-8bfc-8a1086938226",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@junrao Thanks for the review. Opened PR #4670.",
        "createdAt" : "2018-03-09T07:34:43Z",
        "updatedAt" : "2018-03-09T07:34:43Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +1246,1250 @@      if (!isActive) return\n      val curBrokers = zkClient.getAllBrokersInCluster.toSet\n      val updatedBrokers = controllerContext.liveBrokers.filter { broker =>\n        val existingBroker = curBrokers.find(_.id == broker.id)\n        existingBroker match {"
  },
  {
    "id" : "7fe10739-e787-450a-a285-3e8b7d82c945",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-147796250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we fix the alignment?",
        "createdAt" : "2018-08-17T23:12:21Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "b6cfc76f-1721-4b29-bc10-9a75cf65a0d8",
        "parentId" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "Can you be more specific? If you mean the indentation by 2 spaces in the trailing conditions, it seems that's the correct alignment, as also being used in line 436 and line 1172 of this file.",
        "createdAt" : "2018-08-20T19:29:50Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "0f33fa2e-b287-4e45-8fb1-d650e1abf0b4",
        "parentId" : "49d66c68-4654-4c4f-b316-6ee90fe6ceb8",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Sorry. If there is the standard, we can just leave it as it is.",
        "createdAt" : "2018-08-20T19:51:34Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1055,1059 @@      val partitionsToActOn = controllerContext.partitionsOnBroker(id).filter { partition =>\n        controllerContext.partitionReplicaAssignment(partition).size > 1 &&\n          controllerContext.partitionLeadershipInfo.contains(partition) &&\n          !topicDeletionManager.isTopicQueuedUpForDeletion(partition.topic)\n      }"
  },
  {
    "id" : "739f2f66-9c4c-4aee-9c1b-ae12d108ab9b",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-148091532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "I think we also need to exclude the partitions being deleted in the replicatedPartitionsBrokerLeads(). Otherwise, the shutting down broker will keep retrying the controlled shutdown request on those partitions since their leaders will never be moved.",
        "createdAt" : "2018-08-17T23:21:54Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "2ae9d7eb-784a-4678-9af2-e949164d6c57",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "For the partitions being deleted, the two behaviors are\r\n1. If we exclude them in the response, then controlled shutdown will be allowed immediately even though the controller might need to issue StopReplica requests for deletion on the given broker, hence possibly blocking the topic deletion until the broker is brought up again.\r\n2. If we instead include the partitions being deleted, the controller will block the controlled shutdown for a while (bound by controlled shutdown retries) and wait for topic deletion to finish. Once the topic deletion finishes, the controlled shutdown response will be empty. Or if the controlled shutdown retries are exhausted before the topic deletion can finish, the broker will be shutdown regardless of the response.\r\nHence I think the current behavior does not violate correctness. It's a matter of trade-off. Thoughts?",
        "createdAt" : "2018-08-20T19:54:58Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "39d97128-5da2-43e5-bd0e-2094c068a4d9",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Good analysis. This is my thought. Normally, topic deletion will happen very quickly. So, if we get to a state where the deletion of a topic is started, but not completed when a broker is being shut down, chances are that another broker is down, which prevents the topic deletion from completing. In this case, having the broker retry the controlled shutdown request won't be useful and only adds latency during shutdown. ",
        "createdAt" : "2018-08-21T00:36:58Z",
        "updatedAt" : "2018-08-21T14:33:53Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "f7e9c3c5-68ab-44fd-833f-da3a2ba08ab7",
        "parentId" : "a7902b9f-a74d-4340-9cd7-8f5b29f8f797",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "That makes sense. I've made the change. Thanks!",
        "createdAt" : "2018-08-21T14:36:18Z",
        "updatedAt" : "2018-08-21T14:36:18Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1056,1060 @@        controllerContext.partitionReplicaAssignment(partition).size > 1 &&\n          controllerContext.partitionLeadershipInfo.contains(partition) &&\n          !topicDeletionManager.isTopicQueuedUpForDeletion(partition.topic)\n      }\n      val (partitionsLedByBroker, partitionsFollowedByBroker) = partitionsToActOn.partition { partition =>"
  },
  {
    "id" : "9506db26-09dc-42f8-88c3-97710ecfff98",
    "prId" : 5388,
    "prUrl" : "https://github.com/apache/kafka/pull/5388#pullrequestreview-148194607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It seems that we should be checking for isTopicWithDeletionStarted() instead of isTopicQueuedUpForDeletion()?",
        "createdAt" : "2018-08-21T17:10:07Z",
        "updatedAt" : "2018-08-21T17:10:07Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "e32c6d18-a275-4404-a174-34ccb7d98c0e",
        "parentId" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "authorId" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "body" : "At the detailed level:\r\nLet's consider topics that are enqueued for deletion but haven't started the actual deletion, maybe they have ongoing partition reassignments that would still take many hours to finish. And for those partitions during a controlled shutdown we won't try to switch leaderships because we no longer care about their availability.\r\nAnd here even if the shutting down broker is still leader for those partitions, I think it's ok to shutdown the broker immediately because again we don't care about the partition availability anymore. Using !isTopicWithDeletionStarted will still return those partitions and block the controlled shutdown, which is probably not ideal.\r\n\r\nAt a higher level:\r\nisTopicQueuedUpForDeletion is used in many other places so that for other types of operations, once a topic is enqueued, it effectively no longer exists. I think that also applies here, and using the same method consistently simplifies reasoning. Thoughts?",
        "createdAt" : "2018-08-21T18:04:54Z",
        "updatedAt" : "2018-08-21T18:04:54Z",
        "lastEditedBy" : "7d732cb7-eb6e-42e2-a773-db6c10d43f26",
        "tags" : [
        ]
      },
      {
        "id" : "6e2324d6-5ed8-402e-8ccb-c2f5e5ef0c0d",
        "parentId" : "6ce17d29-5272-46a2-bf61-fbd657f5f9f4",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Thanks for the explanation. This sounds good then.",
        "createdAt" : "2018-08-21T18:36:09Z",
        "updatedAt" : "2018-08-21T18:36:09Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a88df990ba29bfc85ca4ab2096d225a80a3a7641",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1080,1084 @@        controllerContext.partitionLeadershipInfo.filter {\n          case (topicPartition, leaderIsrAndControllerEpoch) =>\n            !topicDeletionManager.isTopicQueuedUpForDeletion(topicPartition.topic) &&\n              leaderIsrAndControllerEpoch.leaderAndIsr.leader == id &&\n              controllerContext.partitionReplicaAssignment(topicPartition).size > 1"
  },
  {
    "id" : "bb5cde64-e46d-46a0-8337-c9067df552b7",
    "prId" : 5821,
    "prUrl" : "https://github.com/apache/kafka/pull/5821#pullrequestreview-179630545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5284d66-6829-45c1-9b98-bd47e80bf6c4",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we fix the javadoc above?",
        "createdAt" : "2018-11-13T23:11:36Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "d4e305f3-3bac-48e4-b386-043fc3e9ef71",
        "parentId" : "c5284d66-6829-45c1-9b98-bd47e80bf6c4",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Fixed.",
        "createdAt" : "2018-11-29T05:09:40Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "aec83c36f2b7f46b7ffc1990f56e60f0a9811149",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +201,205 @@   * @return The number of partitions that the broker still leads.\n   */\n  def controlledShutdown(id: Int, brokerEpoch: Long, controlledShutdownCallback: Try[Set[TopicPartition]] => Unit): Unit = {\n    val controlledShutdownEvent = ControlledShutdown(id, brokerEpoch, controlledShutdownCallback)\n    eventManager.put(controlledShutdownEvent)"
  },
  {
    "id" : "25bee507-7738-44c3-890e-0b94c610d6ce",
    "prId" : 5821,
    "prUrl" : "https://github.com/apache/kafka/pull/5821#pullrequestreview-179631110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Normally, when we call onBrokerFailure(), the passed in deadbrokers won't be in controllerContext.liveBrokers, which is used by onBrokerFailure() through PartitionStateMachine/ReplicaStateMachine. With this change, this may not be true. Will that have any impact?",
        "createdAt" : "2018-11-15T22:15:45Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "83b496dd-578d-417c-9077-9dd735494298",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "This is a very good point. I think it is fine because the bounced broker will reject the control requests anyway because the cached broker epoch has not been updated yet. \r\n\r\nHowever, this brings up another question: do we actually need to call `onBrokerFailure()` for bounced brokers? \r\nAfter a second thought, I think the answer is no because the end goal of controller handling bounced brokers in BrokerChange event is to make sure the quickly bounced brokers will be initialized correctly and the end partition/replica states will be the same with and without calling `onBrokerFailure` for the bounced brokers (if there are no new brokers and dead brokers). In this case, only calling `onBrokerStartup` is sufficient. Invoking `onBrokerFailure` first is a correct and safe option but it comes with some overhead because we need to perform leader election and send out the StopReplica/LeaderAndIsr/UpdateMetadata, which are not necessary. Previously I thought that missing `onBrokerFailure` will cause correctness issue because we might miss some state clean up but looks like it is not the case. Also note that if we use controlled shutdown to shutdown and restart the broker, the leadership election actually happens before processing the BrokerChangeEvent.\r\n\r\nTL;DR:\r\nTo be more specific for your original question (why updating the live brokers first then invoke `onBrokerFailure` is fine), there are three places where we use the live brokers informartion in `onBrokerFailure`:\r\n1. Determine whether we need to transition partition states to OfflinePartition: since at the time of the BrokerChange event processing, the bounced brokers are alive so there will not be offline partitions.\r\n2. Determine which brokers we want to consider when performing leader election in `partitionStateMachine.triggerOnlinePartitionStateChange()`: since the bounced brokers are online at that time so we should consider them.\r\n3. Determine which brokers we need to send out control requests and which brokers we need to include in the live brokers field in UpdateMetadataRequest: since the bounced brokers will not accept control requests anyway so the first point doesn't matter. For the second point, the bounced brokers are live so we don't want to exclude them in the UpdateMetadataRequest.",
        "createdAt" : "2018-11-16T09:48:12Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      },
      {
        "id" : "52c8db54-15b3-442d-9563-555dd5c67d6f",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "@hzxa21 : Overall, I agree with your assessment that the onBrokerFailure() call seems redundant. The only thing is that it can force a leader epoch change. Suppose that broker 1 is a bounced broker and is the current leader. If we skip onBrokerFailure(), the controller just keeps broker 1 as the leader w/o bumping the leader epoch. This means that the follower won't go through leader epoch based log truncation, which maybe needed since broker 1 may not have all the data in its local log after the bounce. So, perhaps we can't skip onBrokerFailure(). The next question is should the live broker list exclude the bounced brokers when we call onBrokerFailure(). It seems that we should since live broker list influences which broker is the new leader. If the bounced brokers are still in the live broker list and are the current leaders, those leaders' epoch won't change. So, in summary, it seems that we should still call onBrokerFailure() but excluding bounced brokers from live broker list first. We then add the bounced brokers to live broker list and call onBrokerStartup().",
        "createdAt" : "2018-11-28T00:36:16Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "6b037fdc-c822-4a22-9eb8-0c8faf6c71b7",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "I agree. Also after an offline discussion with Dong, we agree that the benefit of optimizing for quickly bounced brokers is minor and since in normal scenario we will go through onBrokerFailure and then onBrokerStartup for the bounce brokers, it is better to do the same thing here ( invoke onBrokerFailure() and then update live brokers).\r\n\r\nThanks for the comment. I will update the PR accordingly.",
        "createdAt" : "2018-11-28T00:57:13Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      },
      {
        "id" : "36903fde-5687-4aa0-8f59-eceb2587e09a",
        "parentId" : "80234995-6e59-4a51-968b-4786e66fa0be",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2018-11-29T05:13:31Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "aec83c36f2b7f46b7ffc1990f56e60f0a9811149",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +1292,1296 @@      if (bouncedBrokerIds.nonEmpty) {\n        controllerContext.removeLiveBrokersAndEpochs(bouncedBrokerIds)\n        onBrokerFailure(bouncedBrokerIdsSorted)\n        controllerContext.addLiveBrokersAndEpochs(bouncedBrokerAndEpochs)\n        onBrokerStartup(bouncedBrokerIdsSorted)"
  },
  {
    "id" : "1b1a0fd8-1a26-42d8-8642-531e97a68db5",
    "prId" : 5821,
    "prUrl" : "https://github.com/apache/kafka/pull/5821#pullrequestreview-179631276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68140097-d07a-4e5f-9275-f41e4017a392",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should we log the broker epoch in addition to the broker id?",
        "createdAt" : "2018-11-15T22:18:43Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0793d8e7-e7cd-487e-999a-d68dad7b99b3",
        "parentId" : "68140097-d07a-4e5f-9275-f41e4017a392",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "The broker epoch is already logged at the end of the broker change event: https://github.com/apache/kafka/pull/5821/files/88e4eb606fc72761a17378ee6bdf6732765808ef#diff-ed90e8ecc5439a5ede5e362255d11be1R1305",
        "createdAt" : "2018-11-29T05:14:32Z",
        "updatedAt" : "2018-12-01T16:57:02Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "aec83c36f2b7f46b7ffc1990f56e60f0a9811149",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +1280,1284 @@        s\"deleted brokers: ${deadBrokerIdsSorted.mkString(\",\")}, \" +\n        s\"bounced brokers: ${bouncedBrokerIdsSorted.mkString(\",\")}, \" +\n        s\"all live brokers: ${liveBrokerIdsSorted.mkString(\",\")}\")\n\n      newBrokerAndEpochs.keySet.foreach(controllerContext.controllerChannelManager.addBroker)"
  },
  {
    "id" : "e558401d-06f0-4d35-9caf-88d508102e19",
    "prId" : 5869,
    "prUrl" : "https://github.com/apache/kafka/pull/5869#pullrequestreview-171845373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a078c5c7-3e9d-4f6e-abf9-03cb62d61362",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should we change the comment in line 361 and 362 accordingly?",
        "createdAt" : "2018-11-05T23:36:05Z",
        "updatedAt" : "2018-11-06T02:47:03Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "1808e02f-c6dc-4a4b-8e6a-6a98029a97f1",
        "parentId" : "a078c5c7-3e9d-4f6e-abf9-03cb62d61362",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2018-11-06T02:31:49Z",
        "updatedAt" : "2018-11-06T02:47:03Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2093c8673a07898cf2271274efe351d2b8e64fa",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +358,362 @@    newBrokers.foreach(controllerContext.replicasOnOfflineDirs.remove)\n    val newBrokersSet = newBrokers.toSet\n    val existingBrokers = controllerContext.liveOrShuttingDownBrokerIds -- newBrokers\n    // Send update metadata request to all the existing brokers in the cluster so that they know about the new brokers\n    // via this update. No need to include any partition states in the request since there are no partition state changes."
  },
  {
    "id" : "f7109a2c-971c-47b1-a664-342760b5ad94",
    "prId" : 5869,
    "prUrl" : "https://github.com/apache/kafka/pull/5869#pullrequestreview-171847733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47c80126-ee04-407a-bcc0-36fef8390bbd",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Probably change the comment above from \"inform brokers of the offline replica\" to \"inform brokers of the offline brokers\"?",
        "createdAt" : "2018-11-06T02:35:25Z",
        "updatedAt" : "2018-11-06T02:47:03Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "5bb98877-1545-4f13-b3da-6435902ad3ed",
        "parentId" : "47c80126-ee04-407a-bcc0-36fef8390bbd",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2018-11-06T02:47:08Z",
        "updatedAt" : "2018-11-06T02:47:08Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2093c8673a07898cf2271274efe351d2b8e64fa",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +465,469 @@    // Note that during leader re-election, brokers update their metadata\n    if (partitionsWithoutLeader.isEmpty) {\n      sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, Set.empty)\n    }\n  }"
  },
  {
    "id" : "c7b18488-1fba-47b0-bc25-8321bbcd27d2",
    "prId" : 6588,
    "prUrl" : "https://github.com/apache/kafka/pull/6588#pullrequestreview-228014740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d37ceb5-39b7-4c95-8436-2497ec30a496",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : ":+1: ",
        "createdAt" : "2019-04-17T22:06:08Z",
        "updatedAt" : "2019-04-24T22:00:55Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b68fbe3cb047ef1c25bacb7c65955805306c96c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +74,78 @@  private val stateChangeLogger = new StateChangeLogger(config.brokerId, inControllerContext = true, None)\n  val controllerContext = new ControllerContext\n  var controllerChannelManager: ControllerChannelManager = _\n\n  // have a separate scheduler for the controller to be able to start and stop independently of the kafka server"
  },
  {
    "id" : "b62b2b2e-dfcc-41e6-a498-b4efea40f059",
    "prId" : 6588,
    "prUrl" : "https://github.com/apache/kafka/pull/6588#pullrequestreview-228445325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69ae78b9-188f-40a1-8387-ce968c6616cd",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "It is fine here but the nice thing of using `Option` or more precisely `Option.apply` is that it handles `null` and returns the type `Option[T]` instead of `Some[T]` by `Some.apply`.",
        "createdAt" : "2019-04-17T22:11:06Z",
        "updatedAt" : "2019-04-24T22:00:55Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "c6cc74e2-5d10-4bb6-8b9b-a8246a37848a",
        "parentId" : "69ae78b9-188f-40a1-8387-ce968c6616cd",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, I just changed to `Some` because we have a non-nullable object. I thought it is a little cleaner, but it's a bit subjective.",
        "createdAt" : "2019-04-18T18:39:25Z",
        "updatedAt" : "2019-04-24T22:00:55Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b68fbe3cb047ef1c25bacb7c65955805306c96c",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +493,497 @@    partitionStateMachine.handleStateChanges(newPartitions.toSeq, NewPartition)\n    replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, NewReplica)\n    partitionStateMachine.handleStateChanges(newPartitions.toSeq, OnlinePartition, Some(OfflinePartitionLeaderElectionStrategy))\n    replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, OnlineReplica)\n  }"
  }
]