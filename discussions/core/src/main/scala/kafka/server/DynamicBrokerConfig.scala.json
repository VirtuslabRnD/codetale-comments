[
  {
    "id" : "430f27b5-a1f2-4fba-a2fc-7b55e2d982bb",
    "prId" : 4464,
    "prUrl" : "https://github.com/apache/kafka/pull/4464#pullrequestreview-92415619",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b60a8f49-e8e1-4042-a96c-972ada06c212",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's a little confusing that validation failure is signaled both by the return value and an exception. Do we need both?",
        "createdAt" : "2018-01-29T17:25:15Z",
        "updatedAt" : "2018-01-30T17:59:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f0d7184e-1152-4838-ace2-2d82a9fd18cb",
        "parentId" : "b60a8f49-e8e1-4042-a96c-972ada06c212",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "As an external API, I thought a boolean is better. For internal validation, I am using exceptions where the message gets propagated to clients (that doesn't feel very safe to do for custom code where exceptions may potentially contain sensitive information like passwords). I have modified `BrokerReconfigurable` to only throw exceptions.",
        "createdAt" : "2018-01-29T20:35:19Z",
        "updatedAt" : "2018-01-30T17:59:34Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "b437f6a6-7315-418d-bdf4-0e227373eeba",
        "parentId" : "b60a8f49-e8e1-4042-a96c-972ada06c212",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, I think that is fine. Of course plugins may want to propagate a message as well. I guess we could do something like we do with the policies and let the plugin raise `ConfigValidationException` (or something like that). ",
        "createdAt" : "2018-01-30T00:43:03Z",
        "updatedAt" : "2018-01-30T17:59:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f50215eb-bbd2-4d41-9989-7d3eec4cacbd",
        "parentId" : "b60a8f49-e8e1-4042-a96c-972ada06c212",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Internally, `ConfigException` is used, which was being translated to `InvalidRequestException` for other config updates (like topic configs). So plugins could throw that exception. I don't mind removing the return value from `validateReconfiguration` if it is better to document the exception instead. Let me know what you think - perhaps I could make that update in the next PR.",
        "createdAt" : "2018-01-30T01:15:51Z",
        "updatedAt" : "2018-01-30T17:59:34Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      },
      {
        "id" : "21799e07-ad82-42ce-b4a8-148138f89d18",
        "parentId" : "b60a8f49-e8e1-4042-a96c-972ada06c212",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, we can consider separately. I think I slightly prefer documenting the use of `ConfigException` for consistency, but I don't feel too strongly about it.",
        "createdAt" : "2018-01-30T01:18:49Z",
        "updatedAt" : "2018-01-30T17:59:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ec0d321aa79ae11696dc746b434c88b1bad60ff",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +540,544 @@  }\n\n  override def validateReconfiguration(configs: util.Map[String, _]): Boolean = {\n    val updatedMetricsReporters = metricsReporterClasses(configs)\n"
  },
  {
    "id" : "b6bb6d79-89c9-4022-a1af-ab7e233c8afa",
    "prId" : 4471,
    "prUrl" : "https://github.com/apache/kafka/pull/4471#pullrequestreview-92407576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6caa5ed0-a977-4bbc-b366-f12ae615f3dd",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Just so I understand, is the purpose of this check to ensure that configs are only changed \"gracefully.\"",
        "createdAt" : "2018-01-29T21:03:42Z",
        "updatedAt" : "2018-01-30T15:24:51Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "43ca3779-9d09-4cf5-8837-df2e59943999",
        "parentId" : "6caa5ed0-a977-4bbc-b366-f12ae615f3dd",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Yes, since threads can have a big impact, I thought it was better to limit the values. The updates will be more graceful and it will avoid a wrong config bringing down brokers.",
        "createdAt" : "2018-01-30T00:31:48Z",
        "updatedAt" : "2018-01-30T15:24:51Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c25c58922801e023bc1fe56919dfbab7f4565f5",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +476,480 @@        if (newValue <= 0)\n          throw new ConfigException(s\"$errorMsg, value should be at least 1\")\n        if (newValue < oldValue / 2)\n          throw new ConfigException(s\"$errorMsg, value should be at least half the current value $oldValue\")\n        if (newValue > oldValue * 2)"
  },
  {
    "id" : "37939082-bc56-4364-b638-5e95f5600ac4",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-93717949",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69039404-7e65-4968-abe7-107feef60b28",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "By the way, we should make a note to add some documentation on dynamic broker configs and key changes.",
        "createdAt" : "2018-02-01T21:30:09Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d50ed03a-40bd-43c3-bcf8-edded56cbda7",
        "parentId" : "69039404-7e65-4968-abe7-107feef60b28",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Added a note to KAFKA-6476",
        "createdAt" : "2018-02-02T18:49:07Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +267,271 @@    val props = persistentProps.clone().asInstanceOf[Properties]\n    if (!props.asScala.keySet.exists(DynamicPasswordConfigs.contains)) {\n      maybeCreatePasswordEncoder(kafkaConfig.passwordEncoderOldSecret).foreach { passwordDecoder =>\n        DynamicPasswordConfigs.foreach { configName =>\n          val value = props.getProperty(configName)"
  },
  {
    "id" : "34dd4cbb-7d7d-4d96-a353-223356ae25e1",
    "prId" : 4488,
    "prUrl" : "https://github.com/apache/kafka/pull/4488#pullrequestreview-93721378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6c120df-0e37-4dbb-b78f-ba84b889d5ae",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Just so I understand, is it possible to remove a listener which has been configured statically?",
        "createdAt" : "2018-02-01T22:07:59Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "76c60b55-fc67-4530-a2a2-c3ce1c885f24",
        "parentId" : "e6c120df-0e37-4dbb-b78f-ba84b889d5ae",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Yes, any listener can be removed apart from the current inter-broker listener.",
        "createdAt" : "2018-02-02T18:59:45Z",
        "updatedAt" : "2018-02-04T02:11:57Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0488d78e0ea02c3ddde1a31f6c1e0613a443e3d",
    "line" : 398,
    "diffHunk" : "@@ -1,1 +734,738 @@    val oldListeners = oldConfig.listeners\n    val oldListenerMap = listenersToMap(oldListeners)\n    val listenersRemoved = oldListeners.filterNot(e => newListenerMap.contains(e.listenerName))\n    val listenersAdded = newListeners.filterNot(e => oldListenerMap.contains(e.listenerName))\n"
  },
  {
    "id" : "f82c65f7-6847-488a-85a9-99a04c88c069",
    "prId" : 4731,
    "prUrl" : "https://github.com/apache/kafka/pull/4731#pullrequestreview-104882358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a97d6d10-df61-492b-b40e-f7df03899428",
        "parentId" : null,
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "In current code, we are not allowing to add a new listener with default listenerSecurityProtocolMap. \r\n\r\nFor example, say I have a broker with below configs\r\nlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\r\nlisteners=SASL_PLAINTEXT://:9092\r\n\r\nNow I want to add  \"PLAINTEXT://:9093\" listeners config.  listener addition will fail at above check.\r\n\r\n\r\n",
        "createdAt" : "2018-03-19T03:21:26Z",
        "updatedAt" : "2018-03-19T18:56:50Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "9fa68065-88d1-40f7-a0e4-9e9983cea643",
        "parentId" : "a97d6d10-df61-492b-b40e-f7df03899428",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "@omkreddy Thanks for the PR, the change looks good.",
        "createdAt" : "2018-03-19T09:15:27Z",
        "updatedAt" : "2018-03-19T18:56:50Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "59b53168b7ba38ad56564706f62a0f830c629bb1",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +721,725 @@    if (!newAdvertisedListeners.keySet.subsetOf(newListeners.keySet))\n      throw new ConfigException(s\"Advertised listeners '$newAdvertisedListeners' must be a subset of listeners '$newListeners'\")\n    if (!newListeners.keySet.subsetOf(newConfig.listenerSecurityProtocolMap.keySet))\n      throw new ConfigException(s\"Listeners '$newListeners' must be subset of listener map '${newConfig.listenerSecurityProtocolMap}'\")\n    newListeners.keySet.intersect(oldListeners.keySet).foreach { listenerName =>"
  },
  {
    "id" : "f4f8c4bc-25cc-4d37-b498-9d2944c30d0f",
    "prId" : 5334,
    "prUrl" : "https://github.com/apache/kafka/pull/5334#pullrequestreview-144503873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c40a4b1a-f49f-4c9f-82a2-cd283139936b",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe this would be a good time to check for valid IP addresses?\r\n\r\nAlso, there is a requirement in `KafkaConfig` that the default value can only be set to 0 if there are per-ip overrides. Should we check that?",
        "createdAt" : "2018-07-16T15:46:42Z",
        "updatedAt" : "2018-08-09T15:23:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "81bb501f-78b8-4b6d-99a9-ca2d4f48dc76",
        "parentId" : "c40a4b1a-f49f-4c9f-82a2-cd283139936b",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "Above requirement will get validated be during newConfig creation via kafkaConfig.validateValues.  newly added DynamicBrokerConfigTest.testConnectionQuota test validates this scenario.\r\n\r\nWe can add a check to validate IP addresses.  Would it be OK, If I use Apache commons validator library? This will give Ipv4, ipv6 validation util class.\r\nhttp://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/InetAddressValidator.html ",
        "createdAt" : "2018-07-16T18:03:40Z",
        "updatedAt" : "2018-08-09T15:23:21Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "01d21f53-cfe9-4e81-99a0-0cdb44064535",
        "parentId" : "c40a4b1a-f49f-4c9f-82a2-cd283139936b",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "@hachikuji Can you take one more look? Also above query?",
        "createdAt" : "2018-07-18T18:44:05Z",
        "updatedAt" : "2018-08-09T15:23:21Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "764edc45-479e-4ab4-a0dd-90e21d32a27a",
        "parentId" : "c40a4b1a-f49f-4c9f-82a2-cd283139936b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I wasn't imagining anything too elaborate. Just perhaps using something like `ClientUtils.parseAndValidateAddresses`.",
        "createdAt" : "2018-07-18T20:09:55Z",
        "updatedAt" : "2018-08-09T15:23:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "0f34dec2-a166-4ddb-90e8-94d5ce7834c5",
        "parentId" : "c40a4b1a-f49f-4c9f-82a2-cd283139936b",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "@hachikuji I have added basic validation for host address.  not sure, if this is what your  are thinking.  Please take a look.",
        "createdAt" : "2018-07-20T19:20:42Z",
        "updatedAt" : "2018-08-09T15:23:21Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      },
      {
        "id" : "34d09a22-7117-46e9-8433-893c257cfcaf",
        "parentId" : "c40a4b1a-f49f-4c9f-82a2-cd283139936b",
        "authorId" : "915b2f67-05e6-4824-939a-398e7be58870",
        "body" : "@hachikuji Can you please take look?",
        "createdAt" : "2018-08-08T16:32:33Z",
        "updatedAt" : "2018-08-09T15:23:21Z",
        "lastEditedBy" : "915b2f67-05e6-4824-939a-398e7be58870",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc3eea8581661ad263e08d0e430b7ce65cea7c2d",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +828,832 @@  }\n\n  override def validateReconfiguration(newConfig: KafkaConfig): Unit = {\n  }\n"
  }
]