[
  {
    "id" : "ad8ec671-f388-4f57-8009-54e4e2e9c93c",
    "prId" : 10095,
    "prUrl" : "https://github.com/apache/kafka/pull/10095#pullrequestreview-587919343",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ded3c51e-a966-49db-a742-c809dc40215a",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Worth adding a comment explaining the thread safety aspects of this class and also its general purpose. I see a few volatile fields, but `failedAttempts` is not volatile (for example).",
        "createdAt" : "2021-02-10T15:49:06Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "44257c50-6919-42b1-99c7-bc4091a658d3",
        "parentId" : "ded3c51e-a966-49db-a742-c809dc40215a",
        "authorId" : "e0554c25-f6f3-4e49-a325-bcc5d4dc5fb2",
        "body" : "I wonder if it might be helpful to assign each `var` into one of two buckets: stuff that can only be written to from the event queue thread but that can be read from any thread (must be `@volatile`), and stuff that is only used from within the event queue (doesn't need to be).  At a minimum put these into different commented sections, but maybe even create a single container object for the `@volatile` ones:\r\n\r\n\r\n```\r\n  private case class EventQueueThreadOwnedVars(@volatile var _brokerEpoch: Long = -1L,\r\n                                               @volatile var _state: BrokerState = BrokerState.NOT_RUNNING)\r\n  val eventQueueThreadOwnedVars = EventQueueThreadOwnedVars()\r\n```\r\n",
        "createdAt" : "2021-02-10T16:37:13Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "e0554c25-f6f3-4e49-a325-bcc5d4dc5fb2",
        "tags" : [
        ]
      },
      {
        "id" : "38ee48ff-12f6-424a-a798-f87d0d41f3aa",
        "parentId" : "ded3c51e-a966-49db-a742-c809dc40215a",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I added some JavaDoc for the class as a whole explaining the overall paradigm.\r\n\r\nI think putting all the mutable state into a separate class is a bit too extreme.  I added notes about how each mutable variable should be used, however.",
        "createdAt" : "2021-02-10T18:26:34Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc50f6e3b260837d51a5477e6d19b20fed8a86b",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +52,56 @@ * written from the event queue thread.\n */\nclass BrokerLifecycleManager(val config: KafkaConfig,\n                             val time: Time,\n                             val threadNamePrefix: Option[String]) extends Logging {"
  },
  {
    "id" : "da6002ac-94d0-42b1-80a4-3b042e87fbd5",
    "prId" : 10095,
    "prUrl" : "https://github.com/apache/kafka/pull/10095#pullrequestreview-587921861",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b906c4de-ebaa-4564-8ac0-2b1c8b2be056",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Why are we using a Java collection?",
        "createdAt" : "2021-02-10T15:51:39Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "a7b5f0bd-7ebb-40ab-919d-bddbab8fde49",
        "parentId" : "b906c4de-ebaa-4564-8ac0-2b1c8b2be056",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "supportedFeatures is a static final map which basically explains what features the software supports.  So the intention is for it to be defined in common code which happens to be written in Java (not included in this PR).  It seemed simpler not to translate the map into scala, although I guess it's not too difficult either way.",
        "createdAt" : "2021-02-10T18:29:23Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc50f6e3b260837d51a5477e6d19b20fed8a86b",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +185,189 @@            clusterId: Uuid,\n            advertisedListeners: ListenerCollection,\n            supportedFeatures: util.Map[String, VersionRange]): Unit = {\n    eventQueue.append(new StartupEvent(highestMetadataOffsetProvider,\n      channelManager, clusterId, advertisedListeners, supportedFeatures))"
  },
  {
    "id" : "cfe3aee7-def7-4f00-ae80-1c70422404e2",
    "prId" : 10095,
    "prUrl" : "https://github.com/apache/kafka/pull/10095#pullrequestreview-587935532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "103de7ca-839c-4b39-8656-5ce45907356b",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "We are scheduling the next heartbeat in the near future here so we don't have to wait for the full heartbeat interval in order to make it to RUNNING. But i think this will only happen if a SetReadyToUnfenceEvent is handled between the broker entering RECOVERY and the next heartbeat request, in which case that event handler will send a heartbeat right away. If that's the case, do we really need this 10ms here or should we just wait the normal heartbeat interval? \r\n\r\n",
        "createdAt" : "2021-02-10T16:05:18Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "deacf466-503c-41dd-8fea-c3f3e2031998",
        "parentId" : "103de7ca-839c-4b39-8656-5ce45907356b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "If there is no log recovery work to be done, a SetReadyToUnfenceEvent will probably be handled in this time.  Basically BrokerServer will just continue on and tell us it's ready to go.",
        "createdAt" : "2021-02-10T18:45:06Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc50f6e3b260837d51a5477e6d19b20fed8a86b",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +377,381 @@              // Schedule the heartbeat after only 10 ms so that in the case where\n              //there is no recovery work to be done, we start up a bit quicker.\n              scheduleNextCommunication(NANOSECONDS.convert(10, MILLISECONDS))\n            case BrokerState.RECOVERY =>\n              if (!message.data().isFenced()) {"
  },
  {
    "id" : "4a5690ca-fc99-4681-9e59-a68a34736411",
    "prId" : 10095,
    "prUrl" : "https://github.com/apache/kafka/pull/10095#pullrequestreview-588749296",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "243939b8-a04c-448f-a5e6-68c25eef65c7",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Since the heartbeat interval is configurable, maybe we should calculate this from the interval instead of a fixed value. Maybe something like (interval / 2) or sqrt(interval)? ",
        "createdAt" : "2021-02-10T16:22:32Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "2343cdb4-bc70-4530-a174-044d6d1a90da",
        "parentId" : "243939b8-a04c-448f-a5e6-68c25eef65c7",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Hmm.  I'm not sure.  Even if the heartbeat interval was 1 second or 3 seconds (just for example) I don't think we'd want this interval to change.",
        "createdAt" : "2021-02-10T18:47:01Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "84ee52f3-636c-4837-90f2-e4791c84a691",
        "parentId" : "243939b8-a04c-448f-a5e6-68c25eef65c7",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Yea, fair enough. Maybe we can at least consolidate on one magic \"short\" time and make it a constant? Now we have 10ms and 50ms hard coded.",
        "createdAt" : "2021-02-11T16:30:13Z",
        "updatedAt" : "2021-02-11T16:30:13Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc50f6e3b260837d51a5477e6d19b20fed8a86b",
    "line" : 400,
    "diffHunk" : "@@ -1,1 +398,402 @@                  // In the case where controlled shutdown completes quickly, this will\n                  // speed things up a little bit.\n                  scheduleNextCommunication(NANOSECONDS.convert(50, MILLISECONDS))\n                } else {\n                  scheduleNextCommunicationAfterSuccess()"
  },
  {
    "id" : "ed1713f9-a0e5-49dd-ae7c-c7d33c4e15b1",
    "prId" : 10095,
    "prUrl" : "https://github.com/apache/kafka/pull/10095#pullrequestreview-587948076",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2f08bb2-638e-4141-8429-82527cb48144",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "If this class is owning the lifecycle for `_channelManager`, should it not also instantiate it?",
        "createdAt" : "2021-02-10T16:31:04Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "4121bb64-c3d8-418c-9082-abf107d93147",
        "parentId" : "c2f08bb2-638e-4141-8429-82527cb48144",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "We do it this way to make unit tests easier, since we need to supply a mock channel manager at times",
        "createdAt" : "2021-02-10T18:56:40Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "2144445a-1b50-48ce-9621-ea470266b3af",
        "parentId" : "c2f08bb2-638e-4141-8429-82527cb48144",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I guess we could have the responsibility for closing the channel manager lie with the external callers.  Would that be better here?  It would be clearer, I guess.",
        "createdAt" : "2021-02-10T18:59:18Z",
        "updatedAt" : "2021-02-10T19:03:44Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc50f6e3b260837d51a5477e6d19b20fed8a86b",
    "line" : 254,
    "diffHunk" : "@@ -1,1 +252,256 @@      _highestMetadataOffsetProvider = highestMetadataOffsetProvider\n      _channelManager = channelManager\n      _channelManager.start()\n      _state = BrokerState.STARTING\n      _clusterId = clusterId"
  }
]