[
  {
    "id" : "25606d95-8bac-4a6b-ac0c-ec8e8b06c102",
    "prId" : 5206,
    "prUrl" : "https://github.com/apache/kafka/pull/5206#pullrequestreview-171309268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad6ea18d-8ee9-44f8-8cf8-d92894c11ca8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should map be foreach?",
        "createdAt" : "2018-11-03T00:41:58Z",
        "updatedAt" : "2018-11-03T00:42:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "27c8bdc1dd7272df7db14e886ec2bcdd2f77c75b",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +840,844 @@      val fetchPartitionStatus = new mutable.ArrayBuffer[(TopicPartition, FetchPartitionStatus)]\n      fetchInfos.foreach { case (topicPartition, partitionData) =>\n        logReadResultMap.get(topicPartition).map(logReadResult => {\n          val logOffsetMetadata = logReadResult.info.fetchOffsetMetadata\n          fetchPartitionStatus += (topicPartition -> FetchPartitionStatus(logOffsetMetadata, partitionData))"
  },
  {
    "id" : "a8e052e2-5b14-42c0-a3fc-9a31b92b93d5",
    "prId" : 5576,
    "prUrl" : "https://github.com/apache/kafka/pull/5576#pullrequestreview-149903578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "368c4a79-2e9a-439b-8c8b-05768037e675",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "If we do this, we can remove the unused import in line 38.",
        "createdAt" : "2018-08-27T22:09:16Z",
        "updatedAt" : "2018-08-27T22:24:59Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "210976e679533309f961e74a781124ff4e7ec6fd",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1481,1485 @@        case Some(partition) =>\n          if (partition eq ReplicaManager.OfflinePartition)\n            new EpochEndOffset(Errors.KAFKA_STORAGE_ERROR, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)\n          else\n            partition.lastOffsetForLeaderEpoch(leaderEpoch)"
  },
  {
    "id" : "a005cbb5-564f-4212-87b0-abe78ac184d5",
    "prId" : 5661,
    "prUrl" : "https://github.com/apache/kafka/pull/5661#pullrequestreview-161859700",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc0ac912-e63d-432c-948e-d3b3b3000c21",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "The issue of not calling replicaFetcherManager.removeFetcherForPartitions() in line 1278 is that in the case of controlled shutdown, we avoid adding the partitions to the fetcher. This means that existing partitions won't be removed from the fetcher. This may cause replicas removed from ISR during controlled shutdown to be added back to ISR again.\r\n\r\nAlso, if we do this, the state-change logging after replicaFetcherManager.removeFetcherForPartitions()  probably needs to be moved too.",
        "createdAt" : "2018-10-04T23:16:59Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "bed238d1-adaa-4654-ae96-3a96f1ecd3c5",
        "parentId" : "fc0ac912-e63d-432c-948e-d3b3b3000c21",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, let's revert this change. I think it was not strictly needed and I was not too happy about the additional bookkeeping in `AbstractFetcherManager`.",
        "createdAt" : "2018-10-05T00:48:21Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9bc468a7415831841a1203315789e683066dad4",
    "line" : 581,
    "diffHunk" : "@@ -1,1 +1301,1305 @@      else {\n        // we do not need to check if the leader exists again since this has been done at the beginning of this process\n        val partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map { partition =>\n          val leader = metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get\n            .brokerEndPoint(config.interBrokerListenerName)"
  },
  {
    "id" : "a3c2b965-eb01-4e60-b18f-d33c7327dae0",
    "prId" : 6036,
    "prUrl" : "https://github.com/apache/kafka/pull/6036#pullrequestreview-185717443",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55ebca03-80ac-46ac-800d-4c5d0b385ed8",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Offset is not longer included in this error message. Is that intentional?",
        "createdAt" : "2018-12-17T17:45:03Z",
        "updatedAt" : "2018-12-17T17:50:31Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "3aa61ef5-aea7-4f25-a373-51eb23f2b051",
        "parentId" : "55ebca03-80ac-46ac-800d-4c5d0b385ed8",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's part of `PartitionData`.",
        "createdAt" : "2018-12-17T17:49:25Z",
        "updatedAt" : "2018-12-17T17:50:31Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "aeec58e8aa05579b68027999fb2003a9a67ba699",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +950,954 @@          val fetchSource = Request.describeReplicaId(replicaId)\n          error(s\"Error processing fetch with max size $adjustedMaxBytes from $fetchSource \" +\n            s\"on partition $tp: $fetchInfo\", e)\n\n          LogReadResult(info = FetchDataInfo(LogOffsetMetadata.UnknownOffsetMetadata, MemoryRecords.EMPTY),"
  },
  {
    "id" : "61abe60a-5dbe-4ec0-9f22-9849cae2d6c7",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-238640855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "863ad7b6-d53e-46b4-bf32-495f553704fe",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Perhaps results can be renamed errorResults?",
        "createdAt" : "2019-05-15T01:10:43Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "e3d70e8a-a611-44bc-9b6a-439d8f2ce010",
        "parentId" : "863ad7b6-d53e-46b4-bf32-495f553704fe",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "We changed the API once more. Now the result is not a tuple and it includes both the successful and failed elections.",
        "createdAt" : "2019-05-16T21:11:31Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +1572,1576 @@\n      if (expectedLeaders.nonEmpty) {\n        val watchKeys: Seq[TopicPartitionOperationKey] = expectedLeaders.map{\n          case (tp, _) => TopicPartitionOperationKey(tp)\n        }(breakOut)"
  },
  {
    "id" : "62f4d588-4416-4f93-b349-3d07125c802c",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-240900241",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37b7e743-0da4-426a-af94-380f6869b82d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "There doesn't seem to be a good reason for this to be part of `ReplicaManager` instead of `KafkaController`. No need to do it here, but maybe we can move the election purgatory out of `ReplicaManager` in a follow-up patch.",
        "createdAt" : "2019-05-22T16:23:33Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "549b56f2-a199-408e-a493-6b9ae1ef3caa",
        "parentId" : "37b7e743-0da4-426a-af94-380f6869b82d",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "https://issues.apache.org/jira/browse/KAFKA-8408",
        "createdAt" : "2019-05-22T21:54:57Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +1553,1557 @@  }\n\n  def electLeaders(\n    controller: KafkaController,\n    partitions: Set[TopicPartition],"
  },
  {
    "id" : "ab79914e-ded4-477f-9c87-02d065950268",
    "prId" : 6814,
    "prUrl" : "https://github.com/apache/kafka/pull/6814#pullrequestreview-244594707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4b21898-7557-4037-8f23-586e7fac5d03",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just clarifying this is the actual fix, while all others are code cleaning right?",
        "createdAt" : "2019-05-31T23:53:39Z",
        "updatedAt" : "2019-06-01T17:35:35Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0a6660db-ae13-4fc5-b376-52eec5fc3a90",
        "parentId" : "e4b21898-7557-4037-8f23-586e7fac5d03",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, thanks for the patience. I was taking this opportunity to simplify the contract of `updateFollowerFetchState`, which made it easier to write new test cases. I also wanted to reduce the usage of `LogReadResult`. In future work, I think there is an opportunity to consolidate a lot of the fetch state. We currently have so many classes which are all related, but slightly different. There is `LogReadInfo`, `LogReadResult`, `FetchPartitionData`, `FetchPartitionStatus`, `FetchMetadata`, etc. I think we can replace all of those with a single `FetchContext` or something like that. Anyway, that is future work.",
        "createdAt" : "2019-06-01T17:14:17Z",
        "updatedAt" : "2019-06-01T17:35:35Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "fafdae4c452e5581bfa94f675b332da64741d4ac",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +1395,1399 @@    readResults.map { case (topicPartition, readResult) =>\n      val updatedReadResult = if (readResult.error != Errors.NONE) {\n        debug(s\"Skipping update of fetch state for follower $followerId since the \" +\n          s\"log read returned error ${readResult.error}\")\n        readResult"
  },
  {
    "id" : "8c4b62c7-767d-431e-bfe7-f47752aae0ba",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-248455347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f649ffdd-bf13-4fa1-a89b-af585e8c51bf",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: i think it's more common to use `None` directly",
        "createdAt" : "2019-06-12T00:06:13Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 284,
    "diffHunk" : "@@ -1,1 +1074,1078 @@      if (Request.isValidBrokerId(replicaId)) {\n        // Don't look up preferred for follower fetches via normal replication\n        Option.empty\n      } else {\n        replicaSelectorOpt.flatMap { replicaSelector =>"
  },
  {
    "id" : "e8e27167-6315-4cde-b1fd-3c3b87e09c17",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-253649790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a4ef8e0-ae05-4e94-93bb-6c86d4a1344c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the follower has no cached high watermark, we want to respond immediately. Currently we have this logic in DelayedFetch, but I think it makes more sense to avoid purgatory altogether and respond immediately after the initial fetch above in this function.",
        "createdAt" : "2019-06-24T20:52:44Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +905,909 @@      val fetchMetadata = FetchMetadata(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit, isFromFollower,\n        fetchIsolation, isFromFollower, replicaId, fetchPartitionStatus)\n      val delayedFetch = new DelayedFetch(timeout, fetchMetadata, this, quota, clientMetadata,\n        updateHwAndThenCallback)\n"
  },
  {
    "id" : "f4f8c412-c781-40bd-96c1-4b3f1103e73b",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-254948020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b24e2ad7-cde4-41b0-a92d-3c49ba5bc04a",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add the new param to the javadoc above?",
        "createdAt" : "2019-06-27T00:57:49Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +93,97 @@                         readSize: Int,\n                         lastStableOffset: Option[Long],\n                         preferredReadReplica: Option[Int] = None,\n                         followerNeedsHwUpdate: Boolean = false,\n                         exception: Option[Throwable] = None) {"
  },
  {
    "id" : "88e5f29f-0ac1-4f2d-9262-8a8f91fa21dc",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-254948020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c88c509c-8793-4274-ac61-a6dee9f887d4",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we change the comment above accordingly since we can fetch from any replica now?",
        "createdAt" : "2019-06-27T00:57:55Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +832,836 @@                    quota: ReplicaQuota,\n                    responseCallback: Seq[(TopicPartition, FetchPartitionData)] => Unit,\n                    isolationLevel: IsolationLevel,\n                    clientMetadata: Option[ClientMetadata]) {\n    val isFromFollower = Request.isValidBrokerId(replicaId)"
  },
  {
    "id" : "f9ac96e6-a87d-4360-8a32-7c3c0a4edcf7",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-257229089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5313dc0f-913e-47a7-a604-3197759b0bc0",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Just checking my understanding, but this doesn't account for hw changes as a result of the fetch, right?",
        "createdAt" : "2019-07-02T22:37:53Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f547aa3c-1ad0-4e18-8bc1-76e1f6140892",
        "parentId" : "5313dc0f-913e-47a7-a604-3197759b0bc0",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Correct, since the highWatermark returned in the readInfo is the HW _before_ the read occurs, that's what we're comparing here. If the read caused the HW to advance, we wouldn't know about it here.",
        "createdAt" : "2019-07-03T02:03:02Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +980,984 @@          // Check if the HW known to the follower is behind the actual HW\n          val followerNeedsHwUpdate: Boolean = partition.getReplica(replicaId)\n            .exists(replica => replica.lastSentHighWatermark < readInfo.highWatermark)\n\n          val fetchDataInfo = if (shouldLeaderThrottle(quota, tp, replicaId)) {"
  },
  {
    "id" : "fdfdfb8e-1518-4755-bdd4-bdc53d7ad22b",
    "prId" : 6989,
    "prUrl" : "https://github.com/apache/kafka/pull/6989#pullrequestreview-253584524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecaebf1a-fd34-48eb-b678-436785b41a99",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I find that one line imports like `some.package.{One, Two, Three}` lead to unnecessary merge conflicts. How about encouraging one line imports like:\r\n```\r\nimport some.package.One\r\nimport some.package.Two\r\n...\r\n```",
        "createdAt" : "2019-06-24T17:17:36Z",
        "updatedAt" : "2019-06-30T07:35:40Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "b21c711a-d31b-49ba-a758-ed62d3a0277d",
        "parentId" : "ecaebf1a-fd34-48eb-b678-436785b41a99",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "This is the current style. If we want to change it, let's start a discussion in the mailing list.",
        "createdAt" : "2019-06-24T18:30:39Z",
        "updatedAt" : "2019-06-30T07:35:40Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "36d72d5e6c54f763fa145ca508a203974337e852",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +46,50 @@import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction\nimport org.apache.kafka.common.requests.ProduceResponse.PartitionResponse\nimport org.apache.kafka.common.requests.{ApiError, DeleteRecordsResponse, DescribeLogDirsResponse, EpochEndOffset, IsolationLevel, LeaderAndIsrRequest, LeaderAndIsrResponse, OffsetsForLeaderEpochRequest, StopReplicaRequest, UpdateMetadataRequest}\nimport org.apache.kafka.common.utils.Time\n"
  },
  {
    "id" : "57269ca9-fe9c-4c6b-aac7-6baa19de8f1a",
    "prId" : 7650,
    "prUrl" : "https://github.com/apache/kafka/pull/7650#pullrequestreview-312452870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93ce70cd-4660-4ffe-a9a9-fd911ef38315",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Ah, good catch. +1 ",
        "createdAt" : "2019-11-06T14:05:19Z",
        "updatedAt" : "2019-11-06T15:00:24Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "acff32b11b43a0139b32df7c6bf26e477ed7eb7c",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +939,943 @@      }\n      val fetchMetadata: SFetchMetadata = SFetchMetadata(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit,\n        fetchOnlyFromLeader, fetchIsolation, isFromFollower, replicaId, fetchPartitionStatus)\n      val delayedFetch = new DelayedFetch(timeout, fetchMetadata, this, quota, clientMetadata,\n        maybeUpdateHwAndSendResponse)"
  }
]