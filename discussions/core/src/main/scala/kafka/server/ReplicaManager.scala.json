[
  {
    "id" : "25606d95-8bac-4a6b-ac0c-ec8e8b06c102",
    "prId" : 5206,
    "prUrl" : "https://github.com/apache/kafka/pull/5206#pullrequestreview-171309268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad6ea18d-8ee9-44f8-8cf8-d92894c11ca8",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should map be foreach?",
        "createdAt" : "2018-11-03T00:41:58Z",
        "updatedAt" : "2018-11-03T00:42:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "27c8bdc1dd7272df7db14e886ec2bcdd2f77c75b",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +840,844 @@      val fetchPartitionStatus = new mutable.ArrayBuffer[(TopicPartition, FetchPartitionStatus)]\n      fetchInfos.foreach { case (topicPartition, partitionData) =>\n        logReadResultMap.get(topicPartition).map(logReadResult => {\n          val logOffsetMetadata = logReadResult.info.fetchOffsetMetadata\n          fetchPartitionStatus += (topicPartition -> FetchPartitionStatus(logOffsetMetadata, partitionData))"
  },
  {
    "id" : "a8e052e2-5b14-42c0-a3fc-9a31b92b93d5",
    "prId" : 5576,
    "prUrl" : "https://github.com/apache/kafka/pull/5576#pullrequestreview-149903578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "368c4a79-2e9a-439b-8c8b-05768037e675",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "If we do this, we can remove the unused import in line 38.",
        "createdAt" : "2018-08-27T22:09:16Z",
        "updatedAt" : "2018-08-27T22:24:59Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "210976e679533309f961e74a781124ff4e7ec6fd",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1481,1485 @@        case Some(partition) =>\n          if (partition eq ReplicaManager.OfflinePartition)\n            new EpochEndOffset(Errors.KAFKA_STORAGE_ERROR, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)\n          else\n            partition.lastOffsetForLeaderEpoch(leaderEpoch)"
  },
  {
    "id" : "a005cbb5-564f-4212-87b0-abe78ac184d5",
    "prId" : 5661,
    "prUrl" : "https://github.com/apache/kafka/pull/5661#pullrequestreview-161859700",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc0ac912-e63d-432c-948e-d3b3b3000c21",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "The issue of not calling replicaFetcherManager.removeFetcherForPartitions() in line 1278 is that in the case of controlled shutdown, we avoid adding the partitions to the fetcher. This means that existing partitions won't be removed from the fetcher. This may cause replicas removed from ISR during controlled shutdown to be added back to ISR again.\r\n\r\nAlso, if we do this, the state-change logging after replicaFetcherManager.removeFetcherForPartitions()  probably needs to be moved too.",
        "createdAt" : "2018-10-04T23:16:59Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "bed238d1-adaa-4654-ae96-3a96f1ecd3c5",
        "parentId" : "fc0ac912-e63d-432c-948e-d3b3b3000c21",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, let's revert this change. I think it was not strictly needed and I was not too happy about the additional bookkeeping in `AbstractFetcherManager`.",
        "createdAt" : "2018-10-05T00:48:21Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9bc468a7415831841a1203315789e683066dad4",
    "line" : 581,
    "diffHunk" : "@@ -1,1 +1301,1305 @@      else {\n        // we do not need to check if the leader exists again since this has been done at the beginning of this process\n        val partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map { partition =>\n          val leader = metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get\n            .brokerEndPoint(config.interBrokerListenerName)"
  },
  {
    "id" : "a3c2b965-eb01-4e60-b18f-d33c7327dae0",
    "prId" : 6036,
    "prUrl" : "https://github.com/apache/kafka/pull/6036#pullrequestreview-185717443",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55ebca03-80ac-46ac-800d-4c5d0b385ed8",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Offset is not longer included in this error message. Is that intentional?",
        "createdAt" : "2018-12-17T17:45:03Z",
        "updatedAt" : "2018-12-17T17:50:31Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "3aa61ef5-aea7-4f25-a373-51eb23f2b051",
        "parentId" : "55ebca03-80ac-46ac-800d-4c5d0b385ed8",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "It's part of `PartitionData`.",
        "createdAt" : "2018-12-17T17:49:25Z",
        "updatedAt" : "2018-12-17T17:50:31Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "aeec58e8aa05579b68027999fb2003a9a67ba699",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +950,954 @@          val fetchSource = Request.describeReplicaId(replicaId)\n          error(s\"Error processing fetch with max size $adjustedMaxBytes from $fetchSource \" +\n            s\"on partition $tp: $fetchInfo\", e)\n\n          LogReadResult(info = FetchDataInfo(LogOffsetMetadata.UnknownOffsetMetadata, MemoryRecords.EMPTY),"
  },
  {
    "id" : "61abe60a-5dbe-4ec0-9f22-9849cae2d6c7",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-238640855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "863ad7b6-d53e-46b4-bf32-495f553704fe",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Perhaps results can be renamed errorResults?",
        "createdAt" : "2019-05-15T01:10:43Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "e3d70e8a-a611-44bc-9b6a-439d8f2ce010",
        "parentId" : "863ad7b6-d53e-46b4-bf32-495f553704fe",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "We changed the API once more. Now the result is not a tuple and it includes both the successful and failed elections.",
        "createdAt" : "2019-05-16T21:11:31Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +1572,1576 @@\n      if (expectedLeaders.nonEmpty) {\n        val watchKeys: Seq[TopicPartitionOperationKey] = expectedLeaders.map{\n          case (tp, _) => TopicPartitionOperationKey(tp)\n        }(breakOut)"
  },
  {
    "id" : "62f4d588-4416-4f93-b349-3d07125c802c",
    "prId" : 6686,
    "prUrl" : "https://github.com/apache/kafka/pull/6686#pullrequestreview-240900241",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37b7e743-0da4-426a-af94-380f6869b82d",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "There doesn't seem to be a good reason for this to be part of `ReplicaManager` instead of `KafkaController`. No need to do it here, but maybe we can move the election purgatory out of `ReplicaManager` in a follow-up patch.",
        "createdAt" : "2019-05-22T16:23:33Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "549b56f2-a199-408e-a493-6b9ae1ef3caa",
        "parentId" : "37b7e743-0da4-426a-af94-380f6869b82d",
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "https://issues.apache.org/jira/browse/KAFKA-8408",
        "createdAt" : "2019-05-22T21:54:57Z",
        "updatedAt" : "2019-05-29T01:18:29Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d53f267c91cc93478d817214f47a6fd2ec20691",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +1553,1557 @@  }\n\n  def electLeaders(\n    controller: KafkaController,\n    partitions: Set[TopicPartition],"
  },
  {
    "id" : "ab79914e-ded4-477f-9c87-02d065950268",
    "prId" : 6814,
    "prUrl" : "https://github.com/apache/kafka/pull/6814#pullrequestreview-244594707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4b21898-7557-4037-8f23-586e7fac5d03",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Just clarifying this is the actual fix, while all others are code cleaning right?",
        "createdAt" : "2019-05-31T23:53:39Z",
        "updatedAt" : "2019-06-01T17:35:35Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "0a6660db-ae13-4fc5-b376-52eec5fc3a90",
        "parentId" : "e4b21898-7557-4037-8f23-586e7fac5d03",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, thanks for the patience. I was taking this opportunity to simplify the contract of `updateFollowerFetchState`, which made it easier to write new test cases. I also wanted to reduce the usage of `LogReadResult`. In future work, I think there is an opportunity to consolidate a lot of the fetch state. We currently have so many classes which are all related, but slightly different. There is `LogReadInfo`, `LogReadResult`, `FetchPartitionData`, `FetchPartitionStatus`, `FetchMetadata`, etc. I think we can replace all of those with a single `FetchContext` or something like that. Anyway, that is future work.",
        "createdAt" : "2019-06-01T17:14:17Z",
        "updatedAt" : "2019-06-01T17:35:35Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "fafdae4c452e5581bfa94f675b332da64741d4ac",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +1395,1399 @@    readResults.map { case (topicPartition, readResult) =>\n      val updatedReadResult = if (readResult.error != Errors.NONE) {\n        debug(s\"Skipping update of fetch state for follower $followerId since the \" +\n          s\"log read returned error ${readResult.error}\")\n        readResult"
  },
  {
    "id" : "8c4b62c7-767d-431e-bfe7-f47752aae0ba",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-248455347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f649ffdd-bf13-4fa1-a89b-af585e8c51bf",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: i think it's more common to use `None` directly",
        "createdAt" : "2019-06-12T00:06:13Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 284,
    "diffHunk" : "@@ -1,1 +1074,1078 @@      if (Request.isValidBrokerId(replicaId)) {\n        // Don't look up preferred for follower fetches via normal replication\n        Option.empty\n      } else {\n        replicaSelectorOpt.flatMap { replicaSelector =>"
  },
  {
    "id" : "e8e27167-6315-4cde-b1fd-3c3b87e09c17",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-253649790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a4ef8e0-ae05-4e94-93bb-6c86d4a1344c",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "If the follower has no cached high watermark, we want to respond immediately. Currently we have this logic in DelayedFetch, but I think it makes more sense to avoid purgatory altogether and respond immediately after the initial fetch above in this function.",
        "createdAt" : "2019-06-24T20:52:44Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +905,909 @@      val fetchMetadata = FetchMetadata(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit, isFromFollower,\n        fetchIsolation, isFromFollower, replicaId, fetchPartitionStatus)\n      val delayedFetch = new DelayedFetch(timeout, fetchMetadata, this, quota, clientMetadata,\n        updateHwAndThenCallback)\n"
  },
  {
    "id" : "f4f8c412-c781-40bd-96c1-4b3f1103e73b",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-254948020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b24e2ad7-cde4-41b0-a92d-3c49ba5bc04a",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add the new param to the javadoc above?",
        "createdAt" : "2019-06-27T00:57:49Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +93,97 @@                         readSize: Int,\n                         lastStableOffset: Option[Long],\n                         preferredReadReplica: Option[Int] = None,\n                         followerNeedsHwUpdate: Boolean = false,\n                         exception: Option[Throwable] = None) {"
  },
  {
    "id" : "88e5f29f-0ac1-4f2d-9262-8a8f91fa21dc",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-254948020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c88c509c-8793-4274-ac61-a6dee9f887d4",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we change the comment above accordingly since we can fetch from any replica now?",
        "createdAt" : "2019-06-27T00:57:55Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +832,836 @@                    quota: ReplicaQuota,\n                    responseCallback: Seq[(TopicPartition, FetchPartitionData)] => Unit,\n                    isolationLevel: IsolationLevel,\n                    clientMetadata: Option[ClientMetadata]) {\n    val isFromFollower = Request.isValidBrokerId(replicaId)"
  },
  {
    "id" : "f9ac96e6-a87d-4360-8a32-7c3c0a4edcf7",
    "prId" : 6832,
    "prUrl" : "https://github.com/apache/kafka/pull/6832#pullrequestreview-257229089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5313dc0f-913e-47a7-a604-3197759b0bc0",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Just checking my understanding, but this doesn't account for hw changes as a result of the fetch, right?",
        "createdAt" : "2019-07-02T22:37:53Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f547aa3c-1ad0-4e18-8bc1-76e1f6140892",
        "parentId" : "5313dc0f-913e-47a7-a604-3197759b0bc0",
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Correct, since the highWatermark returned in the readInfo is the HW _before_ the read occurs, that's what we're comparing here. If the read caused the HW to advance, we wouldn't know about it here.",
        "createdAt" : "2019-07-03T02:03:02Z",
        "updatedAt" : "2019-07-04T02:59:14Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "4649c14a969cd8c88d06cad68f65d39a0cfdd905",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +980,984 @@          // Check if the HW known to the follower is behind the actual HW\n          val followerNeedsHwUpdate: Boolean = partition.getReplica(replicaId)\n            .exists(replica => replica.lastSentHighWatermark < readInfo.highWatermark)\n\n          val fetchDataInfo = if (shouldLeaderThrottle(quota, tp, replicaId)) {"
  },
  {
    "id" : "fdfdfb8e-1518-4755-bdd4-bdc53d7ad22b",
    "prId" : 6989,
    "prUrl" : "https://github.com/apache/kafka/pull/6989#pullrequestreview-253584524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecaebf1a-fd34-48eb-b678-436785b41a99",
        "parentId" : null,
        "authorId" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "body" : "I find that one line imports like `some.package.{One, Two, Three}` lead to unnecessary merge conflicts. How about encouraging one line imports like:\r\n```\r\nimport some.package.One\r\nimport some.package.Two\r\n...\r\n```",
        "createdAt" : "2019-06-24T17:17:36Z",
        "updatedAt" : "2019-06-30T07:35:40Z",
        "lastEditedBy" : "4a7c311c-0954-4671-a0d2-266cb67437ad",
        "tags" : [
        ]
      },
      {
        "id" : "b21c711a-d31b-49ba-a758-ed62d3a0277d",
        "parentId" : "ecaebf1a-fd34-48eb-b678-436785b41a99",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "This is the current style. If we want to change it, let's start a discussion in the mailing list.",
        "createdAt" : "2019-06-24T18:30:39Z",
        "updatedAt" : "2019-06-30T07:35:40Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "36d72d5e6c54f763fa145ca508a203974337e852",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +46,50 @@import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction\nimport org.apache.kafka.common.requests.ProduceResponse.PartitionResponse\nimport org.apache.kafka.common.requests.{ApiError, DeleteRecordsResponse, DescribeLogDirsResponse, EpochEndOffset, IsolationLevel, LeaderAndIsrRequest, LeaderAndIsrResponse, OffsetsForLeaderEpochRequest, StopReplicaRequest, UpdateMetadataRequest}\nimport org.apache.kafka.common.utils.Time\n"
  },
  {
    "id" : "57269ca9-fe9c-4c6b-aac7-6baa19de8f1a",
    "prId" : 7650,
    "prUrl" : "https://github.com/apache/kafka/pull/7650#pullrequestreview-312452870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93ce70cd-4660-4ffe-a9a9-fd911ef38315",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Ah, good catch. +1 ",
        "createdAt" : "2019-11-06T14:05:19Z",
        "updatedAt" : "2019-11-06T15:00:24Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      }
    ],
    "commit" : "acff32b11b43a0139b32df7c6bf26e477ed7eb7c",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +939,943 @@      }\n      val fetchMetadata: SFetchMetadata = SFetchMetadata(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit,\n        fetchOnlyFromLeader, fetchIsolation, isFromFollower, replicaId, fetchPartitionStatus)\n      val delayedFetch = new DelayedFetch(timeout, fetchMetadata, this, quota, clientMetadata,\n        maybeUpdateHwAndSendResponse)"
  },
  {
    "id" : "3ff35a14-b553-47ba-b173-f044d154cdc0",
    "prId" : 8257,
    "prUrl" : "https://github.com/apache/kafka/pull/8257#pullrequestreview-392914391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93329cb6-af08-4ef5-8844-df7afc2312ec",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Could potentially use `nonOfflinePartition(topicPartition).foreach`",
        "createdAt" : "2020-04-14T01:38:14Z",
        "updatedAt" : "2020-04-14T13:16:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "22048407-bbea-47f6-bc6a-b87dd38e5e5a",
        "parentId" : "93329cb6-af08-4ef5-8844-df7afc2312ec",
        "authorId" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "body" : "Actually, it does not work because we need both the reference to the hosted partition and the partition bellow: `hostedPartition` and `removedPartition`. `nonOfflinePartition` only provides the latter.",
        "createdAt" : "2020-04-14T13:15:31Z",
        "updatedAt" : "2020-04-14T13:16:08Z",
        "lastEditedBy" : "59ca7821-b29c-4f24-a9d5-cbd394145686",
        "tags" : [
        ]
      }
    ],
    "commit" : "49360d6d46bf292c7b9a774be923073fd049c19d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +327,331 @@  def stopReplica(topicPartition: TopicPartition, deletePartition: Boolean): Unit  = {\n    if (deletePartition) {\n      getPartition(topicPartition) match {\n        case hostedPartition @ HostedPartition.Online(removedPartition) =>\n          if (allPartitions.remove(topicPartition, hostedPartition)) {"
  },
  {
    "id" : "a3e1d40b-5179-4fa6-b641-0a1084a44329",
    "prId" : 8257,
    "prUrl" : "https://github.com/apache/kafka/pull/8257#pullrequestreview-392541165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "70684d2a-7e56-44c2-ab0b-1da713b57b02",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Good call updating this.",
        "createdAt" : "2020-04-14T01:40:30Z",
        "updatedAt" : "2020-04-14T13:16:08Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "49360d6d46bf292c7b9a774be923073fd049c19d",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +381,385 @@        (responseMap, Errors.STALE_CONTROLLER_EPOCH)\n      } else {\n        this.controllerEpoch = controllerEpoch\n\n        val stoppedPartitions = mutable.Map.empty[TopicPartition, StopReplicaPartitionState]"
  },
  {
    "id" : "4ecb20cc-8e2d-477b-9586-7d38a0d5e4b8",
    "prId" : 8320,
    "prUrl" : "https://github.com/apache/kafka/pull/8320#pullrequestreview-381992360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b6c5d9f-0350-4c72-ad3f-dacf8b244a79",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we change the following logging in line 1245 to info?\r\n\r\n`              stateChangeLogger.debug(s\"Ignoring LeaderAndIsr request from \"`",
        "createdAt" : "2020-03-26T01:54:11Z",
        "updatedAt" : "2020-03-26T18:05:26Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "f8db1632-6768-41ea-bf6f-809ddab2a9a6",
        "parentId" : "9b6c5d9f-0350-4c72-ad3f-dacf8b244a79",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Sounds good, done in https://github.com/apache/kafka/pull/8320/commits/a3469120ffb00fb04d5c4878d991ba685e3bd066",
        "createdAt" : "2020-03-26T13:12:14Z",
        "updatedAt" : "2020-03-26T18:05:26Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fc7ca7de7ffd00ed32fcd3ced1cc4774ad503c0",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +1358,1362 @@                          responseMap: mutable.Map[TopicPartition, Errors],\n                          highWatermarkCheckpoints: OffsetCheckpoints): Set[Partition] = {\n    val traceEnabled = stateChangeLogger.isTraceEnabled\n    partitionStates.keys.foreach { partition =>\n      if (traceEnabled)"
  },
  {
    "id" : "1f7b7771-a3fe-4af3-b2d7-13c06ae9a617",
    "prId" : 8484,
    "prUrl" : "https://github.com/apache/kafka/pull/8484#pullrequestreview-393935232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a66a7012-887f-4742-b3ef-7c47285f285b",
        "parentId" : null,
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "Avoids a partition lookup",
        "createdAt" : "2020-04-14T15:07:27Z",
        "updatedAt" : "2020-04-14T15:07:33Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      },
      {
        "id" : "2d5459c2-c00a-42f5-9b3c-ad8aabbec08b",
        "parentId" : "a66a7012-887f-4742-b3ef-7c47285f285b",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Are the semantics the same? Do we ensure that `Partition` is not offline? Or does that not matter here?",
        "createdAt" : "2020-04-15T16:22:24Z",
        "updatedAt" : "2020-04-15T16:22:39Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "a59c36e3-7388-4837-a0d0-d741d452c930",
        "parentId" : "a66a7012-887f-4742-b3ef-7c47285f285b",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "On second thought, I think this doesn't matter.",
        "createdAt" : "2020-04-15T16:23:12Z",
        "updatedAt" : "2020-04-15T16:23:13Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3aa0caa036cc21bfd4305558bf9a0421e3b5600e",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +1153,1157 @@   *  the quota is exceeded and the replica is not in sync.\n   */\n  def shouldLeaderThrottle(quota: ReplicaQuota, partition: Partition, replicaId: Int): Boolean = {\n    val isReplicaInSync = partition.inSyncReplicaIds.contains(replicaId)\n    !isReplicaInSync && quota.isThrottled(partition.topicPartition) && quota.isQuotaExceeded"
  },
  {
    "id" : "a5d7cf54-92a7-4876-846c-dcc4b5e06f84",
    "prId" : 8484,
    "prUrl" : "https://github.com/apache/kafka/pull/8484#pullrequestreview-393023358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4d8e659-457a-4c4a-934e-14e2f641095b",
        "parentId" : null,
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "Partition is already accessible, there's no need to look it it up in findPreferredReadReplica.",
        "createdAt" : "2020-04-14T15:08:03Z",
        "updatedAt" : "2020-04-14T15:08:04Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      }
    ],
    "commit" : "3aa0caa036cc21bfd4305558bf9a0421e3b5600e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +988,992 @@        // If we are the leader, determine the preferred read-replica\n        val preferredReadReplica = clientMetadata.flatMap(\n          metadata => findPreferredReadReplica(partition, metadata, replicaId, fetchInfo.fetchOffset, fetchTimeMs))\n\n        if (preferredReadReplica.isDefined) {"
  },
  {
    "id" : "056a8c4a-f278-48d4-92cf-833506462737",
    "prId" : 8709,
    "prUrl" : "https://github.com/apache/kafka/pull/8709#pullrequestreview-419430580",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1b5f3f9-01dd-4514-8abd-a233f61d0045",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Could we use the `toString` from the case class?",
        "createdAt" : "2020-05-26T21:06:16Z",
        "updatedAt" : "2020-05-27T17:39:46Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "333f28e1-808a-42cc-bedb-00a2543aa313",
        "parentId" : "f1b5f3f9-01dd-4514-8abd-a233f61d0045",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Do we get the labels from the default `toString`? In the past, I thought it would only show the values.",
        "createdAt" : "2020-05-26T22:24:22Z",
        "updatedAt" : "2020-05-27T17:39:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f44f9613-d39b-49aa-adce-47d412a329d0",
        "parentId" : "f1b5f3f9-01dd-4514-8abd-a233f61d0045",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "That's true. You have to write `toString` yourself if you want labels. We could write a utility method like so (didn't try too compile it and skipped some details like commas):\r\n\r\n```scala\r\ndef productToString(product: Product): String = {\r\n  val builder = new StringBuilder\r\n  sb.append(product.prefix)\r\n  for (i <- 0 until product.productArity) {\r\n    builder.append(product.productElementName(i))\r\n      .append(\"=\")\r\n      .append(product.productElement(i))\r\n  }\r\n  sb.build()\r\n}\r\n```\r\n\r\nThen we can call that method from any case class `toString` where we want this format. Avoids some duplication and forgetting to update `toString` when new fields are added.",
        "createdAt" : "2020-05-26T22:45:47Z",
        "updatedAt" : "2020-05-27T17:39:46Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "b046c73f-d7e8-4f13-845d-f1089d9a5edd",
        "parentId" : "f1b5f3f9-01dd-4514-8abd-a233f61d0045",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Ok, I added something like that.",
        "createdAt" : "2020-05-27T16:42:43Z",
        "updatedAt" : "2020-05-27T17:39:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d8734687-a637-4f2b-9a47-2f874e3c260f",
        "parentId" : "f1b5f3f9-01dd-4514-8abd-a233f61d0045",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Sweet!",
        "createdAt" : "2020-05-27T16:59:35Z",
        "updatedAt" : "2020-05-27T17:39:46Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1dd8a335f3ce082601b0609ac16d1ada0706d96",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +121,125 @@      s\"lastStableOffset=$lastStableOffset, \" +\n      s\"error=$error\" +\n      \")\"\n  }\n"
  },
  {
    "id" : "c4596e2d-0cc6-47cb-8c4c-f43728ebf3c0",
    "prId" : 8807,
    "prUrl" : "https://github.com/apache/kafka/pull/8807#pullrequestreview-425758899",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b867b1a-b82f-4deb-8681-ce07242199f2",
        "parentId" : null,
        "authorId" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "body" : "For ease of review, it's worth mentioning in the PR body that the only lines that changed here are 1373-1377 and 1233, with the response being captured in 1246.",
        "createdAt" : "2020-06-06T18:18:30Z",
        "updatedAt" : "2020-06-06T18:18:30Z",
        "lastEditedBy" : "6c4430fc-3795-49d6-9c36-cf6aa694824e",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1a91a410bdcbef7ccacec2a95f53c1e5009c962",
    "line" : 241,
    "diffHunk" : "@@ -1,1 +1371,1375 @@        }\n      }\n      val endMs = time.milliseconds()\n      val elapsedMs = endMs - startMs\n      stateChangeLogger.info(s\"Finished LeaderAndIsr request in ${elapsedMs}ms correlationId $correlationId from controller \" +"
  },
  {
    "id" : "46027de3-f919-401f-82d9-0a0a0cbd26fc",
    "prId" : 8979,
    "prUrl" : "https://github.com/apache/kafka/pull/8979#pullrequestreview-448060789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "022c73da-d1df-4fb4-ab2f-05408d0b9b74",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Would we want to change this for new IBPs?",
        "createdAt" : "2020-07-13T22:19:55Z",
        "updatedAt" : "2020-07-17T14:15:04Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "5ee4ba67-c499-4058-945b-a4900a02ac8c",
        "parentId" : "022c73da-d1df-4fb4-ab2f-05408d0b9b74",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Updated to change to NOT_LEADER_OR_FOLLOWER from 2.7 onwards. Also added note to upgrade docs.",
        "createdAt" : "2020-07-14T12:31:15Z",
        "updatedAt" : "2020-07-17T14:15:04Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "65d4f62be75ad44bb14910b178e073d56b5ea586",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +716,720 @@            // Retaining REPLICA_NOT_AVAILABLE exception for ALTER_REPLICA_LOG_DIRS for compatibility\n            warn(s\"Unable to alter log dirs for $topicPartition\", e)\n            (topicPartition, Errors.REPLICA_NOT_AVAILABLE)\n          case t: Throwable =>\n            error(\"Error while changing replica dir for partition %s\".format(topicPartition), t)"
  },
  {
    "id" : "9211d54c-ffd1-416c-b3c9-e9461b1bdd01",
    "prId" : 9382,
    "prUrl" : "https://github.com/apache/kafka/pull/9382#pullrequestreview-543336129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5032c66-c338-492a-b29a-96655be06335",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think this could be saved for a follow-up, but I wonder if we should consider similarly letting the initial offset be determined by the fetcher thread on initialization rather than being passed in. I find it confusing that we expect this to be the high watermark in some cases. It seems a little slippery the way we rely on it in `AbstractFetcherThread.truncateToHighWatermark`.",
        "createdAt" : "2020-12-02T20:05:04Z",
        "updatedAt" : "2020-12-03T01:21:35Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "06534aac-e290-4c75-8ed3-8f76593172a0",
        "parentId" : "f5032c66-c338-492a-b29a-96655be06335",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Yes, makes sense, will do that in a follow-up PR.",
        "createdAt" : "2020-12-02T23:58:00Z",
        "updatedAt" : "2020-12-03T01:21:35Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "be8b884ae2034c4ce6aef17d29a9a3f3429abe82",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1698,1702 @@   * OffsetForLeaderEpoch request.\n   */\n  private def initialFetchOffset(log: Log): Long = {\n    if (ApiVersion.isTruncationOnFetchSupported(config.interBrokerProtocolVersion) && log.latestEpoch.nonEmpty)\n      log.logEndOffset"
  }
]