[
  {
    "id" : "65ffa95d-5778-4f1a-9552-2ada11de3caa",
    "prId" : 5661,
    "prUrl" : "https://github.com/apache/kafka/pull/5661#pullrequestreview-162111376",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52311d51-ca7c-46d7-8c17-15db4768b513",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "The original version logs one partition per line. Perhaps that's easier to parse when debugging?",
        "createdAt" : "2018-10-05T15:22:46Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "255a35e6-22c7-403b-b002-6f047ed3e640",
        "parentId" : "52311d51-ca7c-46d7-8c17-15db4768b513",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. I've found the original message to be too big to be useful in practice. This was an attempt to break it down so that at least there would be a separate message per broker. It was also a tad annoying that we had to build a whole new collection just to print a log message.",
        "createdAt" : "2018-10-05T15:50:50Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "dacf06e3-7108-44e8-9d06-0e5ae22738a4",
        "parentId" : "52311d51-ca7c-46d7-8c17-15db4768b513",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Ok, we can keep it this way then.",
        "createdAt" : "2018-10-05T16:28:39Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9bc468a7415831841a1203315789e683066dad4",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +153,157 @@\n        fetcherThread.addPartitions(initialOffsetAndEpochs)\n        info(s\"Added fetcher to broker ${brokerAndFetcherId.broker} for partitions $initialOffsetAndEpochs\")\n      }\n    }"
  },
  {
    "id" : "6e02b5d0-3027-47c9-8f0d-5101fcfc0236",
    "prId" : 6716,
    "prUrl" : "https://github.com/apache/kafka/pull/6716#pullrequestreview-239082616",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0729ef1-565b-4bd8-ad96-5ce4438d65fd",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Would be nice to have a test case for this metric. We can verify that it increments after a partition failure and that it decrements after a call to `removeFetcherForPartitions`.",
        "createdAt" : "2019-05-16T21:07:06Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ae580be5-29b5-49b0-9c2b-dc1e5d5b9f44",
        "parentId" : "e0729ef1-565b-4bd8-ad96-5ce4438d65fd",
        "authorId" : "678ef46b-7a50-4214-891b-456966e84ed0",
        "body" : "Added a test for the metric to verify its behavior.",
        "createdAt" : "2019-05-17T18:50:12Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "678ef46b-7a50-4214-891b-456966e84ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e5b801039a364d5aa3f528c8591dd5d98f00f03",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +69,73 @@\n  val failedPartitionsCount = newGauge(\n    \"FailedPartitionsCount\", {\n      new Gauge[Int] {\n        def value: Int = failedPartitions.size"
  },
  {
    "id" : "bf6f45d1-226e-435d-b24a-a05c7acc184f",
    "prId" : 6716,
    "prUrl" : "https://github.com/apache/kafka/pull/6716#pullrequestreview-239082089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9290ce2-b0d4-45c2-b823-fa7c98d4b7b6",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "A brief doc for this class may be helpful. I think it's worth mentioning that this includes partitions which failed either during truncation or appending. The basic three classes of errors are:\r\n1) storage errors\r\n2) fenced epoch\r\n3) unexpected errors\r\n\r\nWe can mention that partitions which fail due to a storage error are eventually removed from this set after the log directory is taken offline.",
        "createdAt" : "2019-05-16T21:10:34Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "34ffc794-76a5-4945-974e-e3226b55ff78",
        "parentId" : "c9290ce2-b0d4-45c2-b823-fa7c98d4b7b6",
        "authorId" : "678ef46b-7a50-4214-891b-456966e84ed0",
        "body" : "I have added doc for this class now.",
        "createdAt" : "2019-05-17T18:49:01Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "678ef46b-7a50-4214-891b-456966e84ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e5b801039a364d5aa3f528c8591dd5d98f00f03",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +216,220 @@  * taken offline.\n  */\nclass FailedPartitions {\n  private val failedPartitionsSet = new mutable.HashSet[TopicPartition]\n"
  },
  {
    "id" : "370784a9-909a-4675-9727-b66f591218d8",
    "prId" : 6773,
    "prUrl" : "https://github.com/apache/kafka/pull/6773#pullrequestreview-240317884",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fa10540-1ed9-4e70-9fac-098fd3ea7fbd",
        "parentId" : null,
        "authorId" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "body" : "I do see many places we are doing logging based on certain conditions. Do you think it worths adding an API support in trait `Logging`?",
        "createdAt" : "2019-05-20T20:02:49Z",
        "updatedAt" : "2019-05-20T20:03:47Z",
        "lastEditedBy" : "3dfe0270-df82-43af-827f-0681ce1c6ad9",
        "tags" : [
        ]
      },
      {
        "id" : "dc375ce7-96ae-49bf-a29a-45be80279f54",
        "parentId" : "6fa10540-1ed9-4e70-9fac-098fd3ea7fbd",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "@abbccdda It's not a bad idea. This is something we struggle with particularly in the controller. I am a big fan of structured logging and it would be great to have standard logging APIs to enforce consistent conventions. If you're interested in following up on this, I'd be happy to help out. ",
        "createdAt" : "2019-05-21T21:48:30Z",
        "updatedAt" : "2019-05-21T21:48:47Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "0581c1d55e322ad25ff3a99a4e11d37c4bb03d1c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +175,179 @@      failedPartitions.removeAll(partitions)\n    }\n    if (partitions.nonEmpty)\n      info(s\"Removed fetcher for partitions $partitions\")\n  }"
  },
  {
    "id" : "f46f2e7c-d651-4078-a83e-782b0fbfcdd4",
    "prId" : 8412,
    "prUrl" : "https://github.com/apache/kafka/pull/8412#pullrequestreview-387385452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30fc9db0-e69f-4c24-8980-2b5cb5beb51b",
        "parentId" : null,
        "authorId" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "body" : "Do we have similar log messages for when fetchers are removed? Seems rather useful.",
        "createdAt" : "2020-04-03T14:31:25Z",
        "updatedAt" : "2020-04-03T16:21:18Z",
        "lastEditedBy" : "083f2f03-ca7e-48a1-9781-482564dfdf53",
        "tags" : [
        ]
      },
      {
        "id" : "7d4cbed4-95a9-4284-ae60-d40c8938e5da",
        "parentId" : "30fc9db0-e69f-4c24-8980-2b5cb5beb51b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yes, removals are logged below in `removeFetcherForPartitions`.",
        "createdAt" : "2020-04-03T16:00:24Z",
        "updatedAt" : "2020-04-03T16:21:18Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dd4ec7253154a2e2db31c28c0ba188434d20199",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +154,158 @@                                             initialOffsetAndEpochs: collection.Map[TopicPartition, OffsetAndEpoch]): Unit = {\n    fetcherThread.addPartitions(initialOffsetAndEpochs)\n    info(s\"Added fetcher to broker ${fetcherThread.sourceBroker.id} for partitions $initialOffsetAndEpochs\")\n  }\n"
  }
]