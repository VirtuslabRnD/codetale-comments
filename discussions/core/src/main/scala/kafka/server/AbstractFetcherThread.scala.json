[
  {
    "id" : "603cfdb0-f030-4e85-9b3a-447b81b9ac91",
    "prId" : 4501,
    "prUrl" : "https://github.com/apache/kafka/pull/4501#pullrequestreview-94918475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46952625-a587-4353-81c6-ab07106da854",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Do we still want this change? I think `handlePartitionsWithErrors` can be safely called with an empty collection. But maybe this is clearer. Leaving it to you to decide.",
        "createdAt" : "2018-02-08T00:03:48Z",
        "updatedAt" : "2018-02-08T00:03:53Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "eeb89387-9273-4c84-8456-0d843e40468a",
        "parentId" : "46952625-a587-4353-81c6-ab07106da854",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "No strong feeling. If we didn't already have the empty check for the log message, it wouldn't bother me. But since the check is there, it seemed more natural to skip the call to `handlePartitionsWithError`. Of course both implementations repeat the empty check anyway. I'll probably leave it this way.",
        "createdAt" : "2018-02-08T00:11:21Z",
        "updatedAt" : "2018-02-08T00:11:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c15aed3bf20e993d986ce1299955bc514879c3f",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +236,240 @@      debug(s\"Handling errors for partitions $partitionsWithError\")\n      handlePartitionsWithErrors(partitionsWithError)\n    }\n  }\n"
  },
  {
    "id" : "0cf4a43e-8a84-4aca-ad72-9c1242fa10ec",
    "prId" : 4882,
    "prUrl" : "https://github.com/apache/kafka/pull/4882#pullrequestreview-118473834",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff404894-5e23-4c37-924f-4079a082bc83",
        "parentId" : null,
        "authorId" : "220f032c-6592-42d9-9042-aed276632816",
        "body" : "nits: this replica can be either follower or future replica. Maybe the variable can be named `replicaName`?",
        "createdAt" : "2018-05-08T06:43:03Z",
        "updatedAt" : "2018-05-09T21:46:13Z",
        "lastEditedBy" : "220f032c-6592-42d9-9042-aed276632816",
        "tags" : [
        ]
      },
      {
        "id" : "125d6159-5dd0-4d9e-88f8-bfac462350d6",
        "parentId" : "ff404894-5e23-4c37-924f-4079a082bc83",
        "authorId" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "body" : "Yeah, I already went back and forth couple of times regarding \"replica\" vs. \"follower\" (also re: your comment below). Jun commented (in this PR) that replica is also confusing in a way that leader is also a replica. And in case of future replica, it is also a follower, but of a different type. I propose to keep this name as is, but replace `replicaEndOffset` with `followerEndOffset` re: your comment below.",
        "createdAt" : "2018-05-08T18:09:53Z",
        "updatedAt" : "2018-05-09T21:46:13Z",
        "lastEditedBy" : "e235ea82-83a9-41e5-8e3a-15b2e1b6f350",
        "tags" : [
        ]
      }
    ],
    "commit" : "544f4fdf83ce6f540124668d95a58d2cdebc1ea8",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +321,325 @@  def getOffsetTruncationState(tp: TopicPartition, leaderEpochOffset: EpochEndOffset, replica: Replica, isFutureReplica: Boolean = false): OffsetTruncationState = {\n    // to make sure we can distinguish log output for fetching from remote leader or local replica\n    val followerName = if (isFutureReplica) \"future replica\" else \"follower\"\n\n    if (leaderEpochOffset.endOffset == UNDEFINED_EPOCH_OFFSET) {"
  },
  {
    "id" : "c9506581-2f1c-4938-9b88-55b1038fa788",
    "prId" : 4882,
    "prUrl" : "https://github.com/apache/kafka/pull/4882#pullrequestreview-118836530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b83d15c-4e85-4ee2-aa64-c3186f648f46",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "In general, we don't expect the truncation point to be < local HW. So, it would be useful to log a warning when this happens. Not sure what's the easiest way since now we can have intermediate truncation point.",
        "createdAt" : "2018-05-09T17:50:28Z",
        "updatedAt" : "2018-05-09T21:46:13Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "544f4fdf83ce6f540124668d95a58d2cdebc1ea8",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +361,365 @@      } else {\n        val offsetToTruncateTo = min(followerEndOffset, leaderEpochOffset.endOffset)\n        OffsetTruncationState(min(offsetToTruncateTo, replica.logEndOffset.messageOffset), truncationCompleted = true)\n      }\n    }"
  },
  {
    "id" : "ec514950-b1d4-45d0-a07d-995856bb0a92",
    "prId" : 5587,
    "prUrl" : "https://github.com/apache/kafka/pull/5587#pullrequestreview-153462219",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84b2f51f-4db9-419a-bfc5-6df875993954",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "There is actually a reason why we fetch the partition state directly from partitionStates here again under the partitionMapLock. partitionStates can be modified after we release the lock in doWork() (e.g, due to a leadership change). When we get to processFetchRequest(),  a replica may have become the leader and therefore have been removed from partitionStates. In this case, we don't want to append the pending fetched data to the log since this partition could have appended new data from the producer to the log. This may cause leader epoch to be out of order in the log.",
        "createdAt" : "2018-09-07T00:33:48Z",
        "updatedAt" : "2018-09-07T00:58:37Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "8e5bbfcf-d699-45f6-ad44-d422795ede63",
        "parentId" : "84b2f51f-4db9-419a-bfc5-6df875993954",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "On another look, the change is fine. We still check partitionStates in processFetchRequest().",
        "createdAt" : "2018-09-07T18:49:23Z",
        "updatedAt" : "2018-09-07T18:49:23Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bcba833cdbe86d8a85c7d7109e66a19dc08615b2",
    "line" : 168,
    "diffHunk" : "@@ -1,1 +227,231 @@            // In this case, we only want to process the fetch response if the partition state is ready for fetch and\n            // the current offset is the same as the offset requested.\n            val fetchOffset = fetchStates(topicPartition).fetchOffset\n            if (fetchOffset == currentPartitionFetchState.fetchOffset && currentPartitionFetchState.isReadyForFetch) {\n              partitionData.error match {"
  },
  {
    "id" : "ca8555cf-37eb-4b60-8a27-28e0efb6d894",
    "prId" : 5661,
    "prUrl" : "https://github.com/apache/kafka/pull/5661#pullrequestreview-161019725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc394f80-bb95-4b8a-aee2-c4ee397a4059",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "If we remove tp here, should we update AbstractFetcherManager so that it can update partitionBrokerIdMap and also kills idle thread if possible?",
        "createdAt" : "2018-09-28T22:03:19Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "e31a6afa-6a90-4578-bde9-810213b20d11",
        "parentId" : "dc394f80-bb95-4b8a-aee2-c4ee397a4059",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "@junrao Thanks for the suggestion. This is tricky since we would need to acquire the lock in `AbstractFetcherManager` while holding the lock in `AbstractFetcherThread`. The usual locking order is the other way around. I will explore whether it is possible to give up the lock in `AbstractFetcherThread` prior to invoking `onPartitionFenced`.",
        "createdAt" : "2018-10-03T06:14:32Z",
        "updatedAt" : "2018-10-05T16:40:27Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9bc468a7415831841a1203315789e683066dad4",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +222,226 @@      info(s\"Partition $tp has an older epoch ($currentLeaderEpoch) than the current leader. Will await \" +\n        s\"the new LeaderAndIsr state before resuming fetching.\")\n      partitionStates.remove(tp)\n    }\n  }"
  },
  {
    "id" : "ec2a4605-02f1-41b6-b003-70dc89f25f78",
    "prId" : 6089,
    "prUrl" : "https://github.com/apache/kafka/pull/6089#pullrequestreview-189594783",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa6b024a-ba7b-4f8e-8e92-6c0cc0b3ba34",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Not sure if this part of the change was worth it. We still need a copy per element and the code is a bit harder to read. The rest looks great though.",
        "createdAt" : "2019-01-05T11:51:54Z",
        "updatedAt" : "2019-01-05T11:52:03Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "dff8a2a479f7a5da0a69b57e3631ecaded29e13a",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +146,150 @@    val partitionsWithEpochs = mutable.Map.empty[TopicPartition, EpochData]\n\n    partitionStates.stream().forEach(new Consumer[PartitionStates.PartitionState[PartitionFetchState]] {\n      override def accept(state: PartitionStates.PartitionState[PartitionFetchState]): Unit = {\n        val tp = state.topicPartition"
  },
  {
    "id" : "68b1b610-78d0-4554-8452-d0d43d56f387",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-200772349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a3575d6-65de-4401-a3b2-a39f34a6a844",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "nit: IntelliJ suggests to convert this to a `Single Abstract Method`, effectivelly removing the `new Consumer[PartitionStates.PartitionState[PartitionFetchState]]` and ` override def accept(state: PartitionStates.PartitionState[PartitionFetchState]): Unit = {` boilerplate",
        "createdAt" : "2019-02-06T18:24:19Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "1e440924-25d5-4264-b848-ea775fd984de",
        "parentId" : "8a3575d6-65de-4401-a3b2-a39f34a6a844",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I think that only works with scala 2.12. ",
        "createdAt" : "2019-02-06T19:50:39Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +148,152 @@    val partitionsWithoutEpochs = mutable.Set.empty[TopicPartition]\n\n    partitionStates.stream().forEach(new Consumer[PartitionStates.PartitionState[PartitionFetchState]] {\n      override def accept(state: PartitionStates.PartitionState[PartitionFetchState]): Unit = {\n        if (state.value.isTruncating) {"
  },
  {
    "id" : "0ecfdd0f-95e8-4710-bbec-744f77a6498f",
    "prId" : 6333,
    "prUrl" : "https://github.com/apache/kafka/pull/6333#pullrequestreview-208607641",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9527bc33-bf6b-4719-b825-7f717a4dd091",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "nit: Looking at the usage in the file, I think we would prefer:\r\n```\r\nOption(partitionStates.stateValue(topicPartition)).foreach { currentFetchState =\r\n...\r\n```\r\ninstead of the null check",
        "createdAt" : "2019-02-27T15:58:40Z",
        "updatedAt" : "2019-02-28T16:18:47Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b395b68b85ba67200f71c645b5002a4a04fb1c4",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +212,216 @@    for (tp <- partitions) {\n      val partitionState = partitionStates.stateValue(tp)\n      if (partitionState != null) {\n        try {\n          val highWatermark = partitionState.fetchOffset"
  },
  {
    "id" : "017804d4-7cdc-4a92-a4e9-60b346c85a72",
    "prId" : 6716,
    "prUrl" : "https://github.com/apache/kafka/pull/6716#pullrequestreview-238037566",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ccf02b1-4dd5-4857-a46c-a74473abba90",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Every time we call `markPartitionFailed`, we also call `removePartitions`. Maybe we could create a private method to do this?",
        "createdAt" : "2019-05-15T17:55:02Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "175a33c1-49c7-49cf-86b0-930c516d3836",
        "parentId" : "1ccf02b1-4dd5-4857-a46c-a74473abba90",
        "authorId" : "678ef46b-7a50-4214-891b-456966e84ed0",
        "body" : "Yeah, defined a new method `addFailedPartition` for it.",
        "createdAt" : "2019-05-15T19:55:35Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "678ef46b-7a50-4214-891b-456966e84ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e5b801039a364d5aa3f528c8591dd5d98f00f03",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +186,190 @@      case e: KafkaStorageException =>\n        error(s\"Failed to truncate $topicPartition at offset ${truncationState.offset}\", e)\n        markPartitionFailed(topicPartition)\n        false\n      case t: Throwable =>"
  },
  {
    "id" : "4e2f1bde-b757-42e1-a993-315468031d1e",
    "prId" : 6716,
    "prUrl" : "https://github.com/apache/kafka/pull/6716#pullrequestreview-238566300",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40940827-1758-4db5-a3ae-6e8c03e988c4",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Storage failures result in an event which eventually bubbles up to `ReplicaManager.handleLogDirFailure` by way of `LogDirFailureChannel`. This method will remove the partitions from the replica fetchers, which will consequently remove them from the `failedPartitions` set. I am wondering if it would be better not to do this so that `FailedPartitionsCount` also takes into account failed log directories.\r\n\r\ncc @junrao Does this make sense?",
        "createdAt" : "2019-05-15T18:03:39Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "290c3fae-20fc-4e46-8f76-3697028d976d",
        "parentId" : "40940827-1758-4db5-a3ae-6e8c03e988c4",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Thinking about this a little more, we already have a separate metric to track the offline partitions in `ReplicaManager`, so I think we do not need to change the behavior of `handleLogDirFailure`. If a partition fails due to a storage error, it will be reflected initially in `FailedPartitionsCount`, but later in the offline metric. I think that seems reasonable.",
        "createdAt" : "2019-05-16T18:31:20Z",
        "updatedAt" : "2019-05-17T19:53:38Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e5b801039a364d5aa3f528c8591dd5d98f00f03",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +184,188 @@    }\n    catch {\n      case e: KafkaStorageException =>\n        error(s\"Failed to truncate $topicPartition at offset ${truncationState.offset}\", e)\n        markPartitionFailed(topicPartition)"
  }
]