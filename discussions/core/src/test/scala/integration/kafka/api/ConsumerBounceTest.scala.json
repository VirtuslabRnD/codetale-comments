[
  {
    "id" : "33ed6c85-b2a3-411a-a20d-cea9b933ae67",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-194108163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb73a06d-e9d0-4967-bb05-f4046b38c79c",
        "parentId" : null,
        "authorId" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "body" : "I'm not sure I understand what the changes to receive records are doing here...",
        "createdAt" : "2019-01-17T19:34:13Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "tags" : [
        ]
      },
      {
        "id" : "7ff539c9-a1f3-4c65-97aa-891d8e9281ca",
        "parentId" : "cb73a06d-e9d0-4967-bb05-f4046b38c79c",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "`receiveRecords` used to accept a `topic` parameter which it did not use at all, so I removed it",
        "createdAt" : "2019-01-18T13:35:00Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "40d9d23e-5fae-4ca5-8498-b0d8607f76d9",
        "parentId" : "cb73a06d-e9d0-4967-bb05-f4046b38c79c",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "If you're asking in a more general sense - I added a way to handle exceptions in `receiveRecords` through a callback. This was to ease implementation since I needed to run `receiveRecords()` in parallel. I played a lot with these tests, now I clearly see that callback logic isn't needed.",
        "createdAt" : "2019-01-18T14:02:46Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +203,207 @@\n    sendRecords(numRecords, newtopic)\n    receiveRecords(consumer, numRecords, 10000)\n\n    servers.foreach(server => killBroker(server.config.brokerId))"
  },
  {
    "id" : "00cec450-0043-4d62-99f4-f47f77495028",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-195345542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a32daae-8654-4ad4-8979-908b58418461",
        "parentId" : null,
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "This test fails for me about 50% of the time when I run it locally. I wonder if you see this flakiness on your side.\r\nSample error messages:\r\n```\r\njava.lang.AssertionError: expected:<102> but was:<105>\r\n\r\norg.scalatest.junit.JUnitTestFailedError: Expected to only receive one exception of type\r\nclass org.apache.kafka.common.errors.GroupMaxSizeReachedExceptionduring consumption.\r\nReceived: ArrayBuffer(org.apache.kafka.clients.consumer.CommitFailedException: Commit\r\ncannot be completed since the group has already rebalanced and assigned the partitions to\r\nanother member. This means that the time between subsequent calls to poll() was longer\r\nthan the configured max.poll.interval.ms, which typically implies that the poll loop is\r\nspending too much time message processing. You can address this either by increasing\r\nmax.poll.interval.ms or by reducing the maximum size of batches returned in poll() with\r\nmax.poll.records.)\r\n\r\njava.lang.AssertionError: expected:<102> but was:<136>\r\n```",
        "createdAt" : "2019-01-20T07:11:33Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      },
      {
        "id" : "8a422ea4-4756-41c4-b5c7-e65afd5eb9cc",
        "parentId" : "4a32daae-8654-4ad4-8979-908b58418461",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Not 50% of the time but I do see some flakiness now. What do you think would be a good way to go around this? Checking for at least X records rather than exact?",
        "createdAt" : "2019-01-21T13:36:02Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "0e0df975-e7fa-4e9f-a81f-8c1f9efda144",
        "parentId" : "4a32daae-8654-4ad4-8979-908b58418461",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "After the changes, this test has passed 50/50 times for me",
        "createdAt" : "2019-01-22T09:34:01Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "63daf420-6c9c-43b5-a8cb-27849ab4bd64",
        "parentId" : "4a32daae-8654-4ad4-8979-908b58418461",
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "Yes, it now passes for me too (20/20). Thanks!",
        "createdAt" : "2019-01-23T06:00:44Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +297,301 @@    */\n  @Test\n  def testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup(): Unit = {\n    val topic = \"group-max-size-test\"\n    val maxGroupSize = 2"
  },
  {
    "id" : "ea9672dd-46e0-4956-9c26-bf94648b1f4f",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-195345542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad20d850-3fd5-42c2-b933-e9d7f53fc271",
        "parentId" : null,
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "It seems this test doesn't properly handle the failure. ",
        "createdAt" : "2019-01-23T05:51:19Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 167,
    "diffHunk" : "@@ -1,1 +384,388 @@    */\n  @Test\n  def testConsumerReceivesFatalExceptionWhenGroupPassesMaxSize(): Unit = {\n    val topic = \"group-max-size-test\"\n    val groupId = \"group1\""
  },
  {
    "id" : "5e8c29e9-95b7-48ca-a8f5-0bb4cb8b55fa",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-198937413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "709f83af-5be5-4612-bc6b-e9227b10429e",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We have similar logic in `PlaintextConsumerTest.subscribePollers`. Not sure how easy it is to factor out the common logic, but it would be preferable if possible.",
        "createdAt" : "2019-02-01T06:30:08Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +425,429 @@  }\n\n  def subscribeAndPoll(consumer: KafkaConsumer[Array[Byte], Array[Byte]], executor: ExecutorService, revokeSemaphore: Option[Semaphore] = None,\n                       onException: Exception => Unit = e => { throw e }, topic: String = topic, pollTimeout: Int = 1000): Future[Any] = {\n    executor.submit(CoreUtils.runnable {"
  },
  {
    "id" : "88100eed-81ea-4e38-838f-c9dfece1ed69",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-199302347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8648c514-a02e-4c0d-905d-0f3d6e0b4f04",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Leaving this here for lack of a clear place. Integration tests are definitely nice to have, but I'd have expected we could hit more cases with a unit test in `GroupCoordinatorTest`.",
        "createdAt" : "2019-02-01T06:46:33Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f34b7cf7-5765-4273-9501-00c0365ec710",
        "parentId" : "8648c514-a02e-4c0d-905d-0f3d6e0b4f04",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "In response to this comment and https://github.com/apache/kafka/pull/6163#discussion_r252945186,\r\n\r\nWe discussed offline that it would be best to refactor these tests a bit as well. I have opened a follow-up JIRA to address that just so that this KIP can make it into 2.2\r\nhttps://issues.apache.org/jira/browse/KAFKA-7893",
        "createdAt" : "2019-02-01T22:58:53Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +54,58 @@  }\n\n  private def generateKafkaConfigs(maxGroupSize: String = maxGroupSize.toString): Seq[KafkaConfig] = {\n    val properties = new Properties\n    properties.put(KafkaConfig.OffsetsTopicReplicationFactorProp, \"3\") // don't want to lose offset"
  }
]