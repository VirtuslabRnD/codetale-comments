[
  {
    "id" : "33ed6c85-b2a3-411a-a20d-cea9b933ae67",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-194108163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb73a06d-e9d0-4967-bb05-f4046b38c79c",
        "parentId" : null,
        "authorId" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "body" : "I'm not sure I understand what the changes to receive records are doing here...",
        "createdAt" : "2019-01-17T19:34:13Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "979e3650-ce20-4720-a0da-e44d283b558b",
        "tags" : [
        ]
      },
      {
        "id" : "7ff539c9-a1f3-4c65-97aa-891d8e9281ca",
        "parentId" : "cb73a06d-e9d0-4967-bb05-f4046b38c79c",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "`receiveRecords` used to accept a `topic` parameter which it did not use at all, so I removed it",
        "createdAt" : "2019-01-18T13:35:00Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "40d9d23e-5fae-4ca5-8498-b0d8607f76d9",
        "parentId" : "cb73a06d-e9d0-4967-bb05-f4046b38c79c",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "If you're asking in a more general sense - I added a way to handle exceptions in `receiveRecords` through a callback. This was to ease implementation since I needed to run `receiveRecords()` in parallel. I played a lot with these tests, now I clearly see that callback logic isn't needed.",
        "createdAt" : "2019-01-18T14:02:46Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +203,207 @@\n    sendRecords(numRecords, newtopic)\n    receiveRecords(consumer, numRecords, 10000)\n\n    servers.foreach(server => killBroker(server.config.brokerId))"
  },
  {
    "id" : "00cec450-0043-4d62-99f4-f47f77495028",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-195345542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a32daae-8654-4ad4-8979-908b58418461",
        "parentId" : null,
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "This test fails for me about 50% of the time when I run it locally. I wonder if you see this flakiness on your side.\r\nSample error messages:\r\n```\r\njava.lang.AssertionError: expected:<102> but was:<105>\r\n\r\norg.scalatest.junit.JUnitTestFailedError: Expected to only receive one exception of type\r\nclass org.apache.kafka.common.errors.GroupMaxSizeReachedExceptionduring consumption.\r\nReceived: ArrayBuffer(org.apache.kafka.clients.consumer.CommitFailedException: Commit\r\ncannot be completed since the group has already rebalanced and assigned the partitions to\r\nanother member. This means that the time between subsequent calls to poll() was longer\r\nthan the configured max.poll.interval.ms, which typically implies that the poll loop is\r\nspending too much time message processing. You can address this either by increasing\r\nmax.poll.interval.ms or by reducing the maximum size of batches returned in poll() with\r\nmax.poll.records.)\r\n\r\njava.lang.AssertionError: expected:<102> but was:<136>\r\n```",
        "createdAt" : "2019-01-20T07:11:33Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      },
      {
        "id" : "8a422ea4-4756-41c4-b5c7-e65afd5eb9cc",
        "parentId" : "4a32daae-8654-4ad4-8979-908b58418461",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "Not 50% of the time but I do see some flakiness now. What do you think would be a good way to go around this? Checking for at least X records rather than exact?",
        "createdAt" : "2019-01-21T13:36:02Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "0e0df975-e7fa-4e9f-a81f-8c1f9efda144",
        "parentId" : "4a32daae-8654-4ad4-8979-908b58418461",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "After the changes, this test has passed 50/50 times for me",
        "createdAt" : "2019-01-22T09:34:01Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "63daf420-6c9c-43b5-a8cb-27849ab4bd64",
        "parentId" : "4a32daae-8654-4ad4-8979-908b58418461",
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "Yes, it now passes for me too (20/20). Thanks!",
        "createdAt" : "2019-01-23T06:00:44Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +297,301 @@    */\n  @Test\n  def testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup(): Unit = {\n    val topic = \"group-max-size-test\"\n    val maxGroupSize = 2"
  },
  {
    "id" : "ea9672dd-46e0-4956-9c26-bf94648b1f4f",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-195345542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad20d850-3fd5-42c2-b933-e9d7f53fc271",
        "parentId" : null,
        "authorId" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "body" : "It seems this test doesn't properly handle the failure. ",
        "createdAt" : "2019-01-23T05:51:19Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "2a5e5a4d-e0e2-4e26-b139-0930dd63f949",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 167,
    "diffHunk" : "@@ -1,1 +384,388 @@    */\n  @Test\n  def testConsumerReceivesFatalExceptionWhenGroupPassesMaxSize(): Unit = {\n    val topic = \"group-max-size-test\"\n    val groupId = \"group1\""
  },
  {
    "id" : "5e8c29e9-95b7-48ca-a8f5-0bb4cb8b55fa",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-198937413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "709f83af-5be5-4612-bc6b-e9227b10429e",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "We have similar logic in `PlaintextConsumerTest.subscribePollers`. Not sure how easy it is to factor out the common logic, but it would be preferable if possible.",
        "createdAt" : "2019-02-01T06:30:08Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +425,429 @@  }\n\n  def subscribeAndPoll(consumer: KafkaConsumer[Array[Byte], Array[Byte]], executor: ExecutorService, revokeSemaphore: Option[Semaphore] = None,\n                       onException: Exception => Unit = e => { throw e }, topic: String = topic, pollTimeout: Int = 1000): Future[Any] = {\n    executor.submit(CoreUtils.runnable {"
  },
  {
    "id" : "88100eed-81ea-4e38-838f-c9dfece1ed69",
    "prId" : 6163,
    "prUrl" : "https://github.com/apache/kafka/pull/6163#pullrequestreview-199302347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8648c514-a02e-4c0d-905d-0f3d6e0b4f04",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Leaving this here for lack of a clear place. Integration tests are definitely nice to have, but I'd have expected we could hit more cases with a unit test in `GroupCoordinatorTest`.",
        "createdAt" : "2019-02-01T06:46:33Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "f34b7cf7-5765-4273-9501-00c0365ec710",
        "parentId" : "8648c514-a02e-4c0d-905d-0f3d6e0b4f04",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "In response to this comment and https://github.com/apache/kafka/pull/6163#discussion_r252945186,\r\n\r\nWe discussed offline that it would be best to refactor these tests a bit as well. I have opened a follow-up JIRA to address that just so that this KIP can make it into 2.2\r\nhttps://issues.apache.org/jira/browse/KAFKA-7893",
        "createdAt" : "2019-02-01T22:58:53Z",
        "updatedAt" : "2019-02-01T23:36:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "1421a43a3fcf28c99477bea7a1499a8fa17e50d9",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +54,58 @@  }\n\n  private def generateKafkaConfigs(maxGroupSize: String = maxGroupSize.toString): Seq[KafkaConfig] = {\n    val properties = new Properties\n    properties.put(KafkaConfig.OffsetsTopicReplicationFactorProp, \"3\") // don't want to lose offset"
  },
  {
    "id" : "7bc73344-f78f-4739-92a1-4ff401024e50",
    "prId" : 6238,
    "prUrl" : "https://github.com/apache/kafka/pull/6238#pullrequestreview-220826721",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31c1f16c-fb89-4499-824d-7756cd61d605",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Extending `BaseConsumerTest` means we inherit its test cases (i.e. `testSimpleConsumption` and `testCoordinatorFailover`). I wonder if we should move these tests somewhere else.",
        "createdAt" : "2019-02-13T19:02:50Z",
        "updatedAt" : "2019-03-30T16:16:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b0d77fc4-437c-42d9-8644-db4b80dcb343",
        "parentId" : "31c1f16c-fb89-4499-824d-7756cd61d605",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "We have 6 test classes that make use of those test cases. Only `ConsumerBounceTest` is redundant. I don't think it hurts that much.\r\n\r\nWe could create a new class `BasicConsumerTests` which defines those tests if you think that's a good approach",
        "createdAt" : "2019-02-22T15:04:58Z",
        "updatedAt" : "2019-03-30T16:16:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      },
      {
        "id" : "25ca76eb-d44f-4d68-acf3-ec783efc545e",
        "parentId" : "31c1f16c-fb89-4499-824d-7756cd61d605",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, it is not that big of a deal. Just it's a bit annoying to extend the build time even more because of tests we know are redundant. I think I slightly prefer your suggestion for `BasicConsumerTests`. Or maybe we can call the new class `BaseConsumerTest` and rename the existing one to `AbstractConsumerTest`?\r\n\r\nThe use of inheritance in these test cases has been a bit problematic in general. It would probably be better to split out any shared utility logic into a separate utility class. We don't have to do that here though.",
        "createdAt" : "2019-03-25T16:58:42Z",
        "updatedAt" : "2019-03-30T16:16:28Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "40547092-7fe5-46b6-a19a-eaec2de52f29",
        "parentId" : "31c1f16c-fb89-4499-824d-7756cd61d605",
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "I like your thinking. Let's leave this as it is for the time being so we can merge this PR and reduce the flakiness of ConsumerBounceTest.\r\n\r\nI've opened up a follow-up JIRA to track this - https://issues.apache.org/jira/browse/KAFKA-8176",
        "createdAt" : "2019-03-30T16:11:07Z",
        "updatedAt" : "2019-03-30T16:16:28Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2acc8ddf917981b24c61b84f36c4ec7ec4caeec",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +36,40 @@ * Integration tests for the consumer that cover basic usage as well as server failures\n */\nclass ConsumerBounceTest extends BaseConsumerTest with Logging {\n  val maxGroupSize = 5\n"
  },
  {
    "id" : "05bd8fda-61df-48e7-81c5-f9f535468857",
    "prId" : 6557,
    "prUrl" : "https://github.com/apache/kafka/pull/6557#pullrequestreview-226879472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2237f76c-ff88-4c81-8a42-09e09decff0f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I'm a little confused about the problem we're fixing. Why does it matter that the coordinator is restarted first? Overall, I wonder if we can simplify this. Our goal is to observe the effect of changing the max group size config. Could we restart all brokers first and then check the consumers for the exception we expect?",
        "createdAt" : "2019-04-15T21:03:37Z",
        "updatedAt" : "2019-04-15T21:03:54Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddf06c114b1572c9666e0246ebbcc7f00eec9e68",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +322,326 @@    val coordinator = holdingGroupBrokers.head\n    // ensure the coordinator broker will be restarted first\n    val orderedBrokersIds = List(coordinator) ++ servers.indices.toBuffer.filter(_ != coordinator)\n    // restart brokers until the group moves to a Coordinator with the new config\n    breakable { for (broker <- orderedBrokersIds) {"
  },
  {
    "id" : "a6d4ecaa-f164-4856-b4a3-cfc8843810d0",
    "prId" : 6884,
    "prUrl" : "https://github.com/apache/kafka/pull/6884#pullrequestreview-268505186",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6164d04a-1a52-4995-a9ab-42212cdda7e3",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "This is needed since it previously is based on a transient state that there are un-committed offsets yet to be sent upon closing, with this PR the likelihood of it is much smaller causing it to be flaky.",
        "createdAt" : "2019-07-30T16:08:05Z",
        "updatedAt" : "2019-08-08T21:28:14Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6041a792f58b0b9a38983a60e052e9018319a6e6",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +289,293 @@    servers.foreach(server => killBroker(server.config.brokerId))\n    val closeTimeout = 2000\n    val future1 = submitCloseAndValidate(consumer1, closeTimeout, None, Some(closeTimeout))\n    val future2 = submitCloseAndValidate(consumer2, Long.MaxValue, Some(requestTimeout), Some(requestTimeout))\n    future1.get"
  }
]