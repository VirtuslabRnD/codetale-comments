[
  {
    "id" : "c3344aba-991a-431e-863d-6bf677f19d01",
    "prId" : 9608,
    "prUrl" : "https://github.com/apache/kafka/pull/9608#pullrequestreview-533865280",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c68e7b60-5ed5-4289-bdad-b1d1bbc35230",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Sorry, but we have several identical comments in other test cases. Are those comments also wrong?",
        "createdAt" : "2020-11-18T02:02:44Z",
        "updatedAt" : "2020-11-18T21:03:11Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "ca5cea4a-3dc4-492b-a123-c0651318cbb8",
        "parentId" : "c68e7b60-5ed5-4289-bdad-b1d1bbc35230",
        "authorId" : "b1319cd9-5275-49d8-bc79-127ecb5f5fbd",
        "body" : "I took a look at the `testPartialSegmentClean` which also uses `makeCleaner(2)`, and noticed it does multiple clean attempts since it keeps filling its map, so the logic of the tests are ok. I think the comments are misleading, I traced the code to `LogCleaner.buildOffsetMapForSegment` , and there's this line:\r\n```\r\nval maxDesiredMapSize = (map.slots * this.dupBufferLoadFactor).toInt\r\n```\r\nSo we will only be able to put one offset in the map, and won't attempt to put anything else after that one even if it's the same key. I am going to change the comments",
        "createdAt" : "2020-11-18T20:51:39Z",
        "updatedAt" : "2020-11-18T21:03:11Z",
        "lastEditedBy" : "b1319cd9-5275-49d8-bc79-127ecb5f5fbd",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1e28982f952ac4b1c97fb9aa32f0b037f330bfa",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +818,822 @@  @Test\n  def testLogCleanerStats(): Unit = {\n    // because loadFactor is 0.75, this means we can fit 3 messages in the map\n    val cleaner = makeCleaner(4)\n    val logProps = new Properties()"
  }
]