[
  {
    "id" : "9b393a27-872c-4359-a427-feef27aa9260",
    "prId" : 5169,
    "prUrl" : "https://github.com/apache/kafka/pull/5169#pullrequestreview-128781406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99fab983-be5e-4e15-92be-01bc0f8c5994",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Good catch, this logic was previously broken.",
        "createdAt" : "2018-06-14T13:12:53Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c71b1e5937a33e525f483cd5d2edaa925fa68ccb",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +2250,2254 @@    // operation is aborted but the recovery process itself kicks off split which should complete.\n    newSegments.reverse.foreach { segment =>\n      if (segment != newSegments.last)\n        segment.changeFileSuffixes(\"\", Log.CleanedFileSuffix)\n      else"
  },
  {
    "id" : "8c30fb75-150b-4606-8c76-7a62a15e047a",
    "prId" : 5254,
    "prUrl" : "https://github.com/apache/kafka/pull/5254#pullrequestreview-132205809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48c1798d-21d4-4d0f-92e1-6793f5112f9f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This seems to be just an artifact of the refactor? I guess there is no harm taking a snapshot after every truncation since they are rare.",
        "createdAt" : "2018-06-26T17:57:26Z",
        "updatedAt" : "2018-06-26T21:04:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "2cd1e1bd-373f-4b67-b5d8-31b9dd61ef07",
        "parentId" : "48c1798d-21d4-4d0f-92e1-6793f5112f9f",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Right, we now take at least one snapshot after truncation.",
        "createdAt" : "2018-06-26T21:08:41Z",
        "updatedAt" : "2018-06-26T21:08:41Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4f27a7e5c2e3390ddd67ba245a1b236b2303116",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +678,682 @@\n    log.truncateTo(1)\n    assertEquals(Some(1), log.latestProducerSnapshotOffset)\n    assertEquals(1, log.latestProducerStateEndOffset)\n"
  },
  {
    "id" : "be7554bb-0adb-413f-bcd0-6dba39fd3bb9",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500758",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Do we need to explicitly create these 2 index files?",
        "createdAt" : "2019-02-15T00:43:42Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "fba113ee-5364-4a1f-b67e-adfadda11ff0",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "We need. Otherwise `recoverSegment()` will be invoked because the offset index is missing and truncation will happen so that the assertion here will fail: https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/log/LogTest.scala#L3690\r\n\r\nBtw, this happened to work without this patch because the index file will always get created before the offset index file existence check (See: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/AbstractIndex.scala#L112), which means we will never reach https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/Log.scala#L475 before this patch.",
        "createdAt" : "2019-02-15T05:39:55Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      },
      {
        "id" : "d9c1d9b6-9217-4cb9-8b03-325c09da4c24",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Ok, Probably add a comment why we need to explicitly create those files.",
        "createdAt" : "2019-02-19T16:57:09Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "d8796a05-d86e-436b-8f68-ac7372d4a9cb",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2019-02-19T23:05:29Z",
        "updatedAt" : "2019-02-19T23:05:30Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +3812,3816 @@      // Need to create the offset files explicitly to avoid triggering segment recovery to truncate segment.\n      Log.offsetIndexFile(logDir, baseOffset).createNewFile()\n      Log.timeIndexFile(logDir, baseOffset).createNewFile()\n      baseOffset + Int.MaxValue\n    }"
  },
  {
    "id" : "8ec065cb-7398-45c5-86f7-57a815a51077",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d988e9d0-f7c5-427d-8dea-5fc3dae0ab40",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Is this change intended? The default value for  recoveryPoint is not 200.",
        "createdAt" : "2019-02-19T16:40:54Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "9acde4aa-7dbd-4293-b5bd-66632ce1c4fc",
        "parentId" : "d988e9d0-f7c5-427d-8dea-5fc3dae0ab40",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Yes. After we get rid of the sanity check, the index files for segment below the recovery point will no longer get checked on startup so in order to test rebuild a corrupted index, we need to make sure the recovery point is below the base offset for the corrupted index.",
        "createdAt" : "2019-02-19T23:05:12Z",
        "updatedAt" : "2019-02-19T23:05:13Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +1799,1803 @@\n    // reopen the log with recovery point=0 so that the segment recovery can be triggered\n    log = createLog(logDir, logConfig)\n    assertEquals(\"Should have %d messages when log is reopened\".format(numMessages), numMessages, log.logEndOffset)\n    for(i <- 0 until numMessages) {"
  },
  {
    "id" : "086787c5-0cf4-4ec4-84ed-633079b84482",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01081bd4-eb36-46a2-972b-3a31e84592c9",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Probably add a comment why we need to explicitly create those files.",
        "createdAt" : "2019-02-19T16:57:55Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0ad0057a-24af-4dd1-a67c-7fa29c3b3c40",
        "parentId" : "01081bd4-eb36-46a2-972b-3a31e84592c9",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2019-02-19T23:05:23Z",
        "updatedAt" : "2019-02-19T23:05:23Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +2375,2379 @@    // Need to create the offset files explicitly to avoid triggering segment recovery to truncate segment.\n    Log.offsetIndexFile(logDir, segmentBaseOffset).createNewFile()\n    Log.timeIndexFile(logDir, segmentBaseOffset).createNewFile()\n    records.foreach(segment.append _)\n    segment.close()"
  },
  {
    "id" : "ef40953a-e6b1-4a60-8648-fb71ceb6b370",
    "prId" : 5646,
    "prUrl" : "https://github.com/apache/kafka/pull/5646#pullrequestreview-157506262",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbe9c673-0f0b-4f99-b193-5f9d965b52b3",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe we can add an assertion after these appends on the number of segments in the log and the base offset of the first one? Gives the test a bit more teeth.",
        "createdAt" : "2018-09-20T23:49:30Z",
        "updatedAt" : "2018-09-21T16:23:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e9e9b45e80a44ea59338dda5301822bf991ada3",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2755,2759 @@    val log = createLog(logDir, logConfig, brokerTopicStats, logStartOffset = 5L)\n\n    // append some messages to create some segments\n    for (_ <- 0 until 15)\n      log.appendAsLeader(createRecords, leaderEpoch = 0)"
  },
  {
    "id" : "3c036a38-ff5d-43e3-b433-acf4f3e5dc61",
    "prId" : 5678,
    "prUrl" : "https://github.com/apache/kafka/pull/5678#pullrequestreview-161040398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0aaf87af-6120-4718-a17a-6ba33265fc1b",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Not sure if this truly matters, but does createRecords(0, 0).sizeInBytes match the record size when records are created with different offsets?",
        "createdAt" : "2018-10-01T19:52:37Z",
        "updatedAt" : "2018-10-03T17:40:09Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "c1bbcd66-dc8d-4b00-917c-09e986c8ab4f",
        "parentId" : "0aaf87af-6120-4718-a17a-6ba33265fc1b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. I think they should be the same. The batch base offset is a fixed size and the first record will always have a relative offset of 0.",
        "createdAt" : "2018-10-03T07:37:58Z",
        "updatedAt" : "2018-10-03T17:40:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bece85a158a9e4ce023095dbf49fc5048bb964a",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +2851,2855 @@    }\n\n    val logConfig = LogTest.createLogConfig(segmentBytes = 10 * createRecords(0, 0).sizeInBytes)\n    val log = createLog(logDir, logConfig)\n    val cache = epochCache(log)"
  },
  {
    "id" : "7dab9806-fe08-4c66-9653-d2abe0e169ad",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-201383827",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f00e4388-1b09-4c45-a939-7364a029a16a",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Perhaps further assert that the epoch cache is cleared immediately after log.updateConfig()?",
        "createdAt" : "2019-02-08T00:07:07Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +2207,2211 @@    assertLeaderEpochCacheEmpty(log)\n\n    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"bar\".getBytes())),\n      magicValue = RecordVersion.V1.value), leaderEpoch = 5)\n    assertLeaderEpochCacheEmpty(log)"
  }
]