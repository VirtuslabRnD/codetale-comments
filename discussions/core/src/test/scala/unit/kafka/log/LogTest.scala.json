[
  {
    "id" : "9b393a27-872c-4359-a427-feef27aa9260",
    "prId" : 5169,
    "prUrl" : "https://github.com/apache/kafka/pull/5169#pullrequestreview-128781406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99fab983-be5e-4e15-92be-01bc0f8c5994",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Good catch, this logic was previously broken.",
        "createdAt" : "2018-06-14T13:12:53Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c71b1e5937a33e525f483cd5d2edaa925fa68ccb",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +2250,2254 @@    // operation is aborted but the recovery process itself kicks off split which should complete.\n    newSegments.reverse.foreach { segment =>\n      if (segment != newSegments.last)\n        segment.changeFileSuffixes(\"\", Log.CleanedFileSuffix)\n      else"
  },
  {
    "id" : "8c30fb75-150b-4606-8c76-7a62a15e047a",
    "prId" : 5254,
    "prUrl" : "https://github.com/apache/kafka/pull/5254#pullrequestreview-132205809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48c1798d-21d4-4d0f-92e1-6793f5112f9f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This seems to be just an artifact of the refactor? I guess there is no harm taking a snapshot after every truncation since they are rare.",
        "createdAt" : "2018-06-26T17:57:26Z",
        "updatedAt" : "2018-06-26T21:04:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "2cd1e1bd-373f-4b67-b5d8-31b9dd61ef07",
        "parentId" : "48c1798d-21d4-4d0f-92e1-6793f5112f9f",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Right, we now take at least one snapshot after truncation.",
        "createdAt" : "2018-06-26T21:08:41Z",
        "updatedAt" : "2018-06-26T21:08:41Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4f27a7e5c2e3390ddd67ba245a1b236b2303116",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +678,682 @@\n    log.truncateTo(1)\n    assertEquals(Some(1), log.latestProducerSnapshotOffset)\n    assertEquals(1, log.latestProducerStateEndOffset)\n"
  },
  {
    "id" : "be7554bb-0adb-413f-bcd0-6dba39fd3bb9",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500758",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Do we need to explicitly create these 2 index files?",
        "createdAt" : "2019-02-15T00:43:42Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "fba113ee-5364-4a1f-b67e-adfadda11ff0",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "We need. Otherwise `recoverSegment()` will be invoked because the offset index is missing and truncation will happen so that the assertion here will fail: https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/log/LogTest.scala#L3690\r\n\r\nBtw, this happened to work without this patch because the index file will always get created before the offset index file existence check (See: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/AbstractIndex.scala#L112), which means we will never reach https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/Log.scala#L475 before this patch.",
        "createdAt" : "2019-02-15T05:39:55Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      },
      {
        "id" : "d9c1d9b6-9217-4cb9-8b03-325c09da4c24",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Ok, Probably add a comment why we need to explicitly create those files.",
        "createdAt" : "2019-02-19T16:57:09Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "d8796a05-d86e-436b-8f68-ac7372d4a9cb",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2019-02-19T23:05:29Z",
        "updatedAt" : "2019-02-19T23:05:30Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +3812,3816 @@      // Need to create the offset files explicitly to avoid triggering segment recovery to truncate segment.\n      Log.offsetIndexFile(logDir, baseOffset).createNewFile()\n      Log.timeIndexFile(logDir, baseOffset).createNewFile()\n      baseOffset + Int.MaxValue\n    }"
  },
  {
    "id" : "8ec065cb-7398-45c5-86f7-57a815a51077",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d988e9d0-f7c5-427d-8dea-5fc3dae0ab40",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Is this change intended? The default value for  recoveryPoint is not 200.",
        "createdAt" : "2019-02-19T16:40:54Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "9acde4aa-7dbd-4293-b5bd-66632ce1c4fc",
        "parentId" : "d988e9d0-f7c5-427d-8dea-5fc3dae0ab40",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Yes. After we get rid of the sanity check, the index files for segment below the recovery point will no longer get checked on startup so in order to test rebuild a corrupted index, we need to make sure the recovery point is below the base offset for the corrupted index.",
        "createdAt" : "2019-02-19T23:05:12Z",
        "updatedAt" : "2019-02-19T23:05:13Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +1799,1803 @@\n    // reopen the log with recovery point=0 so that the segment recovery can be triggered\n    log = createLog(logDir, logConfig)\n    assertEquals(\"Should have %d messages when log is reopened\".format(numMessages), numMessages, log.logEndOffset)\n    for(i <- 0 until numMessages) {"
  },
  {
    "id" : "086787c5-0cf4-4ec4-84ed-633079b84482",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01081bd4-eb36-46a2-972b-3a31e84592c9",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Probably add a comment why we need to explicitly create those files.",
        "createdAt" : "2019-02-19T16:57:55Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0ad0057a-24af-4dd1-a67c-7fa29c3b3c40",
        "parentId" : "01081bd4-eb36-46a2-972b-3a31e84592c9",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2019-02-19T23:05:23Z",
        "updatedAt" : "2019-02-19T23:05:23Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +2375,2379 @@    // Need to create the offset files explicitly to avoid triggering segment recovery to truncate segment.\n    Log.offsetIndexFile(logDir, segmentBaseOffset).createNewFile()\n    Log.timeIndexFile(logDir, segmentBaseOffset).createNewFile()\n    records.foreach(segment.append _)\n    segment.close()"
  }
]