[
  {
    "id" : "9b393a27-872c-4359-a427-feef27aa9260",
    "prId" : 5169,
    "prUrl" : "https://github.com/apache/kafka/pull/5169#pullrequestreview-128781406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99fab983-be5e-4e15-92be-01bc0f8c5994",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Good catch, this logic was previously broken.",
        "createdAt" : "2018-06-14T13:12:53Z",
        "updatedAt" : "2018-06-19T15:58:05Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c71b1e5937a33e525f483cd5d2edaa925fa68ccb",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +2250,2254 @@    // operation is aborted but the recovery process itself kicks off split which should complete.\n    newSegments.reverse.foreach { segment =>\n      if (segment != newSegments.last)\n        segment.changeFileSuffixes(\"\", Log.CleanedFileSuffix)\n      else"
  },
  {
    "id" : "8c30fb75-150b-4606-8c76-7a62a15e047a",
    "prId" : 5254,
    "prUrl" : "https://github.com/apache/kafka/pull/5254#pullrequestreview-132205809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48c1798d-21d4-4d0f-92e1-6793f5112f9f",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "This seems to be just an artifact of the refactor? I guess there is no harm taking a snapshot after every truncation since they are rare.",
        "createdAt" : "2018-06-26T17:57:26Z",
        "updatedAt" : "2018-06-26T21:04:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "2cd1e1bd-373f-4b67-b5d8-31b9dd61ef07",
        "parentId" : "48c1798d-21d4-4d0f-92e1-6793f5112f9f",
        "authorId" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "body" : "Right, we now take at least one snapshot after truncation.",
        "createdAt" : "2018-06-26T21:08:41Z",
        "updatedAt" : "2018-06-26T21:08:41Z",
        "lastEditedBy" : "93b1c273-8917-4547-bd53-5101f22161c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4f27a7e5c2e3390ddd67ba245a1b236b2303116",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +678,682 @@\n    log.truncateTo(1)\n    assertEquals(Some(1), log.latestProducerSnapshotOffset)\n    assertEquals(1, log.latestProducerStateEndOffset)\n"
  },
  {
    "id" : "be7554bb-0adb-413f-bcd0-6dba39fd3bb9",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500758",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Do we need to explicitly create these 2 index files?",
        "createdAt" : "2019-02-15T00:43:42Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "fba113ee-5364-4a1f-b67e-adfadda11ff0",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "We need. Otherwise `recoverSegment()` will be invoked because the offset index is missing and truncation will happen so that the assertion here will fail: https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/log/LogTest.scala#L3690\r\n\r\nBtw, this happened to work without this patch because the index file will always get created before the offset index file existence check (See: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/AbstractIndex.scala#L112), which means we will never reach https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/Log.scala#L475 before this patch.",
        "createdAt" : "2019-02-15T05:39:55Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      },
      {
        "id" : "d9c1d9b6-9217-4cb9-8b03-325c09da4c24",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Ok, Probably add a comment why we need to explicitly create those files.",
        "createdAt" : "2019-02-19T16:57:09Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "d8796a05-d86e-436b-8f68-ac7372d4a9cb",
        "parentId" : "0526e32d-a9ea-4bf6-b5a2-3c26519bac3e",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2019-02-19T23:05:29Z",
        "updatedAt" : "2019-02-19T23:05:30Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +3812,3816 @@      // Need to create the offset files explicitly to avoid triggering segment recovery to truncate segment.\n      Log.offsetIndexFile(logDir, baseOffset).createNewFile()\n      Log.timeIndexFile(logDir, baseOffset).createNewFile()\n      baseOffset + Int.MaxValue\n    }"
  },
  {
    "id" : "8ec065cb-7398-45c5-86f7-57a815a51077",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d988e9d0-f7c5-427d-8dea-5fc3dae0ab40",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Is this change intended? The default value for  recoveryPoint is not 200.",
        "createdAt" : "2019-02-19T16:40:54Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "9acde4aa-7dbd-4293-b5bd-66632ce1c4fc",
        "parentId" : "d988e9d0-f7c5-427d-8dea-5fc3dae0ab40",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Yes. After we get rid of the sanity check, the index files for segment below the recovery point will no longer get checked on startup so in order to test rebuild a corrupted index, we need to make sure the recovery point is below the base offset for the corrupted index.",
        "createdAt" : "2019-02-19T23:05:12Z",
        "updatedAt" : "2019-02-19T23:05:13Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +1799,1803 @@\n    // reopen the log with recovery point=0 so that the segment recovery can be triggered\n    log = createLog(logDir, logConfig)\n    assertEquals(\"Should have %d messages when log is reopened\".format(numMessages), numMessages, log.logEndOffset)\n    for(i <- 0 until numMessages) {"
  },
  {
    "id" : "086787c5-0cf4-4ec4-84ed-633079b84482",
    "prId" : 5498,
    "prUrl" : "https://github.com/apache/kafka/pull/5498#pullrequestreview-205500722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01081bd4-eb36-46a2-972b-3a31e84592c9",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Probably add a comment why we need to explicitly create those files.",
        "createdAt" : "2019-02-19T16:57:55Z",
        "updatedAt" : "2019-02-19T23:03:15Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0ad0057a-24af-4dd1-a67c-7fa29c3b3c40",
        "parentId" : "01081bd4-eb36-46a2-972b-3a31e84592c9",
        "authorId" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "body" : "Done.",
        "createdAt" : "2019-02-19T23:05:23Z",
        "updatedAt" : "2019-02-19T23:05:23Z",
        "lastEditedBy" : "e9f2a5b6-a46b-4418-b0e8-3a587ddbbf67",
        "tags" : [
        ]
      }
    ],
    "commit" : "e818b20e6fd37ece4dbc2d39a0d872a1f523c1d5",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +2375,2379 @@    // Need to create the offset files explicitly to avoid triggering segment recovery to truncate segment.\n    Log.offsetIndexFile(logDir, segmentBaseOffset).createNewFile()\n    Log.timeIndexFile(logDir, segmentBaseOffset).createNewFile()\n    records.foreach(segment.append _)\n    segment.close()"
  },
  {
    "id" : "ef40953a-e6b1-4a60-8648-fb71ceb6b370",
    "prId" : 5646,
    "prUrl" : "https://github.com/apache/kafka/pull/5646#pullrequestreview-157506262",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbe9c673-0f0b-4f99-b193-5f9d965b52b3",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe we can add an assertion after these appends on the number of segments in the log and the base offset of the first one? Gives the test a bit more teeth.",
        "createdAt" : "2018-09-20T23:49:30Z",
        "updatedAt" : "2018-09-21T16:23:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e9e9b45e80a44ea59338dda5301822bf991ada3",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2755,2759 @@    val log = createLog(logDir, logConfig, brokerTopicStats, logStartOffset = 5L)\n\n    // append some messages to create some segments\n    for (_ <- 0 until 15)\n      log.appendAsLeader(createRecords, leaderEpoch = 0)"
  },
  {
    "id" : "3c036a38-ff5d-43e3-b433-acf4f3e5dc61",
    "prId" : 5678,
    "prUrl" : "https://github.com/apache/kafka/pull/5678#pullrequestreview-161040398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0aaf87af-6120-4718-a17a-6ba33265fc1b",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Not sure if this truly matters, but does createRecords(0, 0).sizeInBytes match the record size when records are created with different offsets?",
        "createdAt" : "2018-10-01T19:52:37Z",
        "updatedAt" : "2018-10-03T17:40:09Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "c1bbcd66-dc8d-4b00-917c-09e986c8ab4f",
        "parentId" : "0aaf87af-6120-4718-a17a-6ba33265fc1b",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Hmm.. I think they should be the same. The batch base offset is a fixed size and the first record will always have a relative offset of 0.",
        "createdAt" : "2018-10-03T07:37:58Z",
        "updatedAt" : "2018-10-03T17:40:09Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bece85a158a9e4ce023095dbf49fc5048bb964a",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +2851,2855 @@    }\n\n    val logConfig = LogTest.createLogConfig(segmentBytes = 10 * createRecords(0, 0).sizeInBytes)\n    val log = createLog(logDir, logConfig)\n    val cache = epochCache(log)"
  },
  {
    "id" : "7dab9806-fe08-4c66-9653-d2abe0e169ad",
    "prId" : 6232,
    "prUrl" : "https://github.com/apache/kafka/pull/6232#pullrequestreview-201383827",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f00e4388-1b09-4c45-a939-7364a029a16a",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Perhaps further assert that the epoch cache is cleared immediately after log.updateConfig()?",
        "createdAt" : "2019-02-08T00:07:07Z",
        "updatedAt" : "2019-02-08T04:13:43Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab429a3c1ea54d660cfae70d6c14c21ad8bf4c1e",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +2207,2211 @@    assertLeaderEpochCacheEmpty(log)\n\n    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"bar\".getBytes())),\n      magicValue = RecordVersion.V1.value), leaderEpoch = 5)\n    assertLeaderEpochCacheEmpty(log)"
  },
  {
    "id" : "eafbac71-d29a-48c3-83e4-f6ab547d9b12",
    "prId" : 6968,
    "prUrl" : "https://github.com/apache/kafka/pull/6968#pullrequestreview-251774814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6c365c0-7ffe-47c0-80fe-a825eb915141",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Can we assert something about the log? If not, then we should remove the `val`.",
        "createdAt" : "2019-06-19T15:15:39Z",
        "updatedAt" : "2019-06-19T16:16:58Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e69964bdf200fab68e17e6f817173057b855655",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +3748,3752 @@    logDir.mkdirs()\n    val logConfig = LogTest.createLogConfig()\n    val log = createLog(logDir, logConfig)\n    assertEquals(1, log.numberOfSegments)\n  }"
  },
  {
    "id" : "57d23442-1eb3-4e87-b3c3-41860ed6ad16",
    "prId" : 6968,
    "prUrl" : "https://github.com/apache/kafka/pull/6968#pullrequestreview-251788054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "582e5296-bb94-44b9-b6cd-651027200859",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Maybe add an assertion that there is exactly one log segment?",
        "createdAt" : "2019-06-19T15:35:02Z",
        "updatedAt" : "2019-06-19T16:16:58Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e69964bdf200fab68e17e6f817173057b855655",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +3748,3752 @@    logDir.mkdirs()\n    val logConfig = LogTest.createLogConfig()\n    val log = createLog(logDir, logConfig)\n    assertEquals(1, log.numberOfSegments)\n  }"
  },
  {
    "id" : "194631f0-a5bd-48e6-b8ec-e8a22cc00f3c",
    "prId" : 7081,
    "prUrl" : "https://github.com/apache/kafka/pull/7081#pullrequestreview-267442728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45898f10-a599-461a-8400-e8ba33850c5a",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Is this useful since assertValidLogOffsetMetadata() calls the same log.read() for verification? Ditto in assertEmptyFetch().",
        "createdAt" : "2019-07-25T01:40:22Z",
        "updatedAt" : "2019-07-30T00:20:27Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "e319be06-1222-41eb-b095-9dc273d950d3",
        "parentId" : "45898f10-a599-461a-8400-e8ba33850c5a",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, good point. I think the main thing I wanted to assert here was that the segment position is defined and valid.",
        "createdAt" : "2019-07-26T23:57:30Z",
        "updatedAt" : "2019-07-30T00:20:27Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      }
    ],
    "commit" : "15ddf031ec82cadf3dc040b726ddd0ec95f5ada2",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +136,140 @@\n    assertEquals(offset, readInfo.fetchOffsetMetadata.messageOffset)\n    assertValidLogOffsetMetadata(log, readInfo.fetchOffsetMetadata)\n  }\n"
  },
  {
    "id" : "f72c9ca1-d803-4974-b32e-6d56b55e87e1",
    "prId" : 7687,
    "prUrl" : "https://github.com/apache/kafka/pull/7687#pullrequestreview-330776721",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82732249-96fa-423d-82cd-b1869693f969",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Nice cleanup with `appendEndTxnMarkerAsLeader` and below!",
        "createdAt" : "2019-12-11T20:41:46Z",
        "updatedAt" : "2020-01-09T19:38:09Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9a1865479fccd5e7a00c3697248fce38d115d74",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +1345,1349 @@      new SimpleRecord(\"baz\".getBytes))\n    log.appendAsLeader(records, leaderEpoch = 0)\n    val abortAppendInfo = appendEndTxnMarkerAsLeader(log, pid, epoch, ControlRecordType.ABORT)\n    log.updateHighWatermark(abortAppendInfo.lastOffset + 1)\n"
  },
  {
    "id" : "d32fb493-1d9c-4839-8188-e6718cd59c03",
    "prId" : 8418,
    "prUrl" : "https://github.com/apache/kafka/pull/8418#pullrequestreview-387455391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd5d28dc-1fdc-42d2-8ad9-dcc9979bdfc1",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Should we use lambdas for `Runnable` and `Callable`?",
        "createdAt" : "2020-04-03T17:40:07Z",
        "updatedAt" : "2020-04-03T17:54:40Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef0058146efe1bbd9b848be3395c4be571b4842f",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +3689,3693 @@        log.updateHighWatermark(log.logEndOffset)\n      }\n    }\n\n    val executor = Executors.newFixedThreadPool(2)"
  },
  {
    "id" : "90652d20-1b5e-48ca-8b1e-a50ff8a88426",
    "prId" : 9364,
    "prUrl" : "https://github.com/apache/kafka/pull/9364#pullrequestreview-514411713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f3f25ab-c5ea-499f-a784-c22083c3e2ad",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "The callers of createLog() in line 652 and 2205 seem to need lastShutdownClean to be false.",
        "createdAt" : "2020-10-02T21:20:09Z",
        "updatedAt" : "2020-10-30T22:49:57Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "6002816d-fa45-4ae5-b8dc-510fd156c8a1",
        "parentId" : "6f3f25ab-c5ea-499f-a784-c22083c3e2ad",
        "authorId" : "6d290dc4-e10d-4c64-876f-9eaa3ee33f28",
        "body" : "Done",
        "createdAt" : "2020-10-22T06:57:29Z",
        "updatedAt" : "2020-10-30T22:49:57Z",
        "lastEditedBy" : "6d290dc4-e10d-4c64-876f-9eaa3ee33f28",
        "tags" : [
        ]
      }
    ],
    "commit" : "49f1b10c84c4fcb33afe930477a85fb54f606ab2",
    "line" : 416,
    "diffHunk" : "@@ -1,1 +4615,4619 @@                        maxProducerIdExpirationMs: Int = 60 * 60 * 1000,\n                        producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs,\n                        lastShutdownClean: Boolean = true): Log = {\n    LogTest.createLog(dir, config, brokerTopicStats, scheduler, time, logStartOffset, recoveryPoint,\n      maxProducerIdExpirationMs, producerIdExpirationCheckIntervalMs, lastShutdownClean)"
  },
  {
    "id" : "22d3b176-502c-4e62-90de-a343ac32a47c",
    "prId" : 9364,
    "prUrl" : "https://github.com/apache/kafka/pull/9364#pullrequestreview-516324140",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "537e6e60-d9d4-4b9c-b0ff-f3bcba89a747",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Hmm, it seems that this tests expects a clean shutdown.",
        "createdAt" : "2020-10-23T00:23:28Z",
        "updatedAt" : "2020-10-30T22:49:57Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0c10e887-50e3-4d0b-b5bb-cabba5cf898e",
        "parentId" : "537e6e60-d9d4-4b9c-b0ff-f3bcba89a747",
        "authorId" : "6d290dc4-e10d-4c64-876f-9eaa3ee33f28",
        "body" : "Jun, this test was not creating a clean shut down file before opening the log again. So, it would have been going through log recovery code path earlier as well.",
        "createdAt" : "2020-10-25T04:33:04Z",
        "updatedAt" : "2020-10-30T22:49:57Z",
        "lastEditedBy" : "6d290dc4-e10d-4c64-876f-9eaa3ee33f28",
        "tags" : [
        ]
      }
    ],
    "commit" : "49f1b10c84c4fcb33afe930477a85fb54f606ab2",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +1458,1462 @@\n    // After reloading log, producer state should not be regenerated\n    val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L, lastShutdownClean = false)\n    assertEquals(1, reloadedLog.activeProducersWithLastSequence.size)\n    val reloadedEntryOpt = log.activeProducersWithLastSequence.get(pid2)"
  },
  {
    "id" : "ee91a67f-bfff-4d11-b2d9-655a1cc270ef",
    "prId" : 9364,
    "prUrl" : "https://github.com/apache/kafka/pull/9364#pullrequestreview-516324043",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87e6c218-3cff-4486-9913-ad2422693928",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It seems that this expects a clean shutdown.",
        "createdAt" : "2020-10-23T16:10:39Z",
        "updatedAt" : "2020-10-30T22:49:57Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "f608707d-d793-4bca-a9c7-d961f7ee060c",
        "parentId" : "87e6c218-3cff-4486-9913-ad2422693928",
        "authorId" : "6d290dc4-e10d-4c64-876f-9eaa3ee33f28",
        "body" : "Jun, this test was not creating a clean shut down file before opening the log again. So, it would have gone through the recovery path. Hence, I have set `lastShutdownClean` parameter to `false`. Similarly, for line 2210.",
        "createdAt" : "2020-10-25T04:30:37Z",
        "updatedAt" : "2020-10-30T22:49:57Z",
        "lastEditedBy" : "6d290dc4-e10d-4c64-876f-9eaa3ee33f28",
        "tags" : [
        ]
      }
    ],
    "commit" : "49f1b10c84c4fcb33afe930477a85fb54f606ab2",
    "line" : 242,
    "diffHunk" : "@@ -1,1 +2329,2333 @@    }\n\n    log = createLog(logDir, logConfig, recoveryPoint = lastOffset, lastShutdownClean = false)\n    verifyRecoveredLog(log, lastOffset)\n    log.close()"
  },
  {
    "id" : "e25bcdc8-9ba6-4a9d-bb97-b3220c99ec7b",
    "prId" : 9364,
    "prUrl" : "https://github.com/apache/kafka/pull/9364#pullrequestreview-515201189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d4eefd9-7033-4901-a94c-c2e02cfaca61",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It seem the createLog() call on line 3976 inside testRecoverOnlyLastSegment() needs to have lastShutdownClean = false.",
        "createdAt" : "2020-10-23T16:18:11Z",
        "updatedAt" : "2020-10-30T22:49:57Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "49f1b10c84c4fcb33afe930477a85fb54f606ab2",
    "line" : 354,
    "diffHunk" : "@@ -1,1 +3817,3821 @@\n    // reopen the log and recover from the beginning\n    val recoveredLog = createLog(logDir, LogConfig(), lastShutdownClean = false)\n    val recoveredLeaderEpochCache = epochCache(recoveredLog)\n"
  }
]