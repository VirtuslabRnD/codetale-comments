[
  {
    "id" : "8a19f993-ce25-4b6a-a2ad-d345eeebad7b",
    "prId" : 7973,
    "prUrl" : "https://github.com/apache/kafka/pull/7973#pullrequestreview-344474762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51f97cfe-49a7-4240-8686-93111e29d4a1",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Just checking my understanding, but it seems there's no need to do this after the shrink semaphore is released. If we did it before, then we could assert that the append futures are blocked just like the update follower futures.",
        "createdAt" : "2020-01-17T07:34:31Z",
        "updatedAt" : "2020-01-17T10:03:34Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "55b4351f-45e3-40c7-b0c3-ea98f1ad2efa",
        "parentId" : "51f97cfe-49a7-4240-8686-93111e29d4a1",
        "authorId" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "body" : "Good idea, updated.",
        "createdAt" : "2020-01-17T10:04:57Z",
        "updatedAt" : "2020-01-17T10:04:58Z",
        "lastEditedBy" : "b4936a15-698a-496e-85a1-b1e229b4986b",
        "tags" : [
        ]
      }
    ],
    "commit" : "8edfe93407704a21ce7c286952e6dfe107c448a8",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +147,151 @@\n    assertFalse(stateUpdateFutures.exists(_.isDone))\n    appendSemaphore.release(numProducers * numRecordsPerProducer)\n    assertFalse(appendFutures.exists(_.isDone))\n"
  },
  {
    "id" : "a26ae1c4-28dd-4d41-be4d-0d1940810bf9",
    "prId" : 9065,
    "prUrl" : "https://github.com/apache/kafka/pull/9065#pullrequestreview-454331161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d470437-21cd-407b-a457-5a55551166bd",
        "parentId" : null,
        "authorId" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "body" : "This fails incredibly quickly 100/100 times without the Partition.scala changes.",
        "createdAt" : "2020-07-23T17:05:26Z",
        "updatedAt" : "2020-07-23T18:27:21Z",
        "lastEditedBy" : "df911192-d6c6-4af9-8568-f67bf2dcf926",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f813d69b07fb5cf8a85587576652e4a55cc7f3b",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +124,128 @@   */\n  @Test\n  def testGetReplicaWithUpdateAssignmentAndIsr(): Unit = {\n    val active = new AtomicBoolean(true)\n    val replicaToCheck = 3"
  },
  {
    "id" : "629c6be0-d488-440c-bdc0-263fee19fbd6",
    "prId" : 10742,
    "prUrl" : "https://github.com/apache/kafka/pull/10742#pullrequestreview-666146433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9cd246b-5cb1-490c-b34b-ed529d87001a",
        "parentId" : null,
        "authorId" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "body" : "As above mentioned, if the default logPrefix is empty string, then the following change in tests are not needed.",
        "createdAt" : "2021-05-22T02:16:05Z",
        "updatedAt" : "2021-05-22T02:17:09Z",
        "lastEditedBy" : "d520dc4e-6bae-4b0b-90d6-4c0a1cabb518",
        "tags" : [
        ]
      },
      {
        "id" : "7402daaf-e893-4ada-9e92-f059b3acebf6",
        "parentId" : "e9cd246b-5cb1-490c-b34b-ed529d87001a",
        "authorId" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "body" : "I've explained here: https://github.com/apache/kafka/pull/10742#discussion_r637344863",
        "createdAt" : "2021-05-22T03:05:28Z",
        "updatedAt" : "2021-05-22T03:05:36Z",
        "lastEditedBy" : "b4f52e78-c19e-46b4-b486-6da86e32e687",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b429dcb5a21580d1b8f5cca7183101c98a6faee",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +284,288 @@        val logDirFailureChannel = new LogDirFailureChannel(1)\n        val segments = new LogSegments(log.topicPartition)\n        val leaderEpochCache = Log.maybeCreateLeaderEpochCache(log.dir, log.topicPartition, logDirFailureChannel, log.config.messageFormatVersion.recordVersion, \"\")\n        val maxProducerIdExpirationMs = 60 * 60 * 1000\n        val producerStateManager = new ProducerStateManager(log.topicPartition, log.dir, maxProducerIdExpirationMs)"
  }
]