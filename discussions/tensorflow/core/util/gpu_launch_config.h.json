[
  {
    "id" : "9c44e061-624b-4a84-94f3-b50200365792",
    "prId" : 40954,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40954#pullrequestreview-446522389",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5064491b-131c-4351-8aae-b57b26d417f7",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "I think it would be cleaner to provide a forward-compatible overload of hipOccupancyMaxPotentialBlockSize.",
        "createdAt" : "2020-07-10T13:29:51Z",
        "updatedAt" : "2020-07-24T03:06:10Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "0849d9a9-817d-4836-8242-75ce8e529caf",
        "parentId" : "5064491b-131c-4351-8aae-b57b26d417f7",
        "authorId" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "body" : "Just trying to restore the code to its pre-existing state (for ROCm 3.3).  The code within the `#else` block will have a short shelf life. it will be gone with the switch to ROCm 3.5+, which should happen soon (within the next month).\r\n\r\nCan we leave it as is for now?",
        "createdAt" : "2020-07-10T16:06:38Z",
        "updatedAt" : "2020-07-24T03:06:10Z",
        "lastEditedBy" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddafc33a31db310352b656e0831ef2da8975cf87",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +182,186 @@  CHECK_GE(block_size_limit, 0);\n  uint32_t block_size_limit_uint = static_cast<uint32_t>(block_size_limit);\n  hipOccupancyMaxPotentialBlockSize(&block_count_uint, &thread_per_block_uint,\n                                    func, dynamic_shared_memory_size,\n                                    block_size_limit_uint);"
  },
  {
    "id" : "2976207d-3a2a-4966-acf4-293e897f14a3",
    "prId" : 40954,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40954#pullrequestreview-446522479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "696f81e7-6310-44ca-bc15-8aeff4cb1570",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Would it be cleaner to provide a forward-compatible implementation of hipOccupancyMaxActiveBlocksPerMultiprocessor?",
        "createdAt" : "2020-07-10T13:30:24Z",
        "updatedAt" : "2020-07-24T03:06:10Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "0a974d2d-be33-43ad-8825-9939fe7a111e",
        "parentId" : "696f81e7-6310-44ca-bc15-8aeff4cb1570",
        "authorId" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "body" : "same as above",
        "createdAt" : "2020-07-10T16:06:45Z",
        "updatedAt" : "2020-07-24T03:06:10Z",
        "lastEditedBy" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddafc33a31db310352b656e0831ef2da8975cf87",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +222,226 @@  CHECK_EQ(err, hipSuccess);\n#else\n  // Apply the heuristic in GetGpuLaunchConfig(int, const Eigen::GpuDevice&)\n  // that the kernel is quite simple and will largely be memory-limited.\n  const int physical_thread_count = std::min("
  },
  {
    "id" : "b072f737-f90d-45e1-bf9a-9c09bfc31fc9",
    "prId" : 28834,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28834#pullrequestreview-239838397",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c7b604e-26f4-4868-b2e5-15d8dfd87f6f",
        "parentId" : null,
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "Looks like this is already aliased in line 374 in the new file. I might have left this one behind when re-doing my changes after chsigg's. So it looks like we should just remove it.",
        "createdAt" : "2019-05-20T15:03:55Z",
        "updatedAt" : "2019-05-20T15:03:55Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      },
      {
        "id" : "948eb038-1908-4d1b-b825-4e01b53a083e",
        "parentId" : "2c7b604e-26f4-4868-b2e5-15d8dfd87f6f",
        "authorId" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "body" : "I thought about removing it, but was not sure about it.\r\n\r\nThere exists both a non-template version (line 253) and a template version (line 366) of the routine `GetGpu2DLaunchConfig`.\r\n\r\nThe `GetCuda2DLaunchConfig` wrapper that we create on line 374 will cover both the template and non-template version, I think, but was not sure.  So decided to create an explicit wrapper for the non-template version. \r\n\r\nwill leave it upto @chsigg to decide whether or not to leave this change in. I can remove the explicit wrapper for the non-template routine if that works better for upstreaming.",
        "createdAt" : "2019-05-20T15:27:58Z",
        "updatedAt" : "2019-05-20T15:27:58Z",
        "lastEditedBy" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "tags" : [
        ]
      },
      {
        "id" : "ed02ea76-aeea-4fbf-b525-9dcf08f9cba5",
        "parentId" : "2c7b604e-26f4-4868-b2e5-15d8dfd87f6f",
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Either way is fine with me. I'll start the merge as is. Eventually those should all be removed again anyway.",
        "createdAt" : "2019-05-21T06:02:31Z",
        "updatedAt" : "2019-05-21T06:02:47Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e9beeb2f8dc621ca5951e1ecc99c4b25b638b3c",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +279,283 @@}\n#ifndef TENSORFLOW_USE_ROCM\ninline Cuda2DLaunchConfig GetCuda2DLaunchConfig(int xdim, int ydim,\n                                                const Eigen::GpuDevice& d) {\n  return GetGpu2DLaunchConfig(xdim, ydim, d);"
  },
  {
    "id" : "042a2126-86b0-4881-888a-68f84269bc72",
    "prId" : 28834,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28834#pullrequestreview-239572264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80bd7c48-6bb8-4a4b-9c28-8dded3cd60cb",
        "parentId" : null,
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "Already defined in line 198, we should remove it",
        "createdAt" : "2019-05-20T15:04:53Z",
        "updatedAt" : "2019-05-20T15:04:53Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      },
      {
        "id" : "676b09c0-203e-4d48-8ec3-f229017e45b4",
        "parentId" : "80bd7c48-6bb8-4a4b-9c28-8dded3cd60cb",
        "authorId" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "body" : "same reasoning+comment  as for the `GetGpu2DLaunchConfig`",
        "createdAt" : "2019-05-20T15:28:00Z",
        "updatedAt" : "2019-05-20T15:28:01Z",
        "lastEditedBy" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e9beeb2f8dc621ca5951e1ecc99c4b25b638b3c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +144,148 @@}\n#ifndef TENSORFLOW_USE_ROCM\ninline CudaLaunchConfig GetCudaLaunchConfig(int work_element_count,\n                                            const Eigen::GpuDevice& d) {\n  return GetGpuLaunchConfig(work_element_count, d);"
  },
  {
    "id" : "3dd7f861-cbed-4151-8e52-58af7b303b05",
    "prId" : 28568,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28568#pullrequestreview-236060156",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e477d8f3-a4a2-45c8-9739-3404069fa03a",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "I think you should #include \"gpu_cuda_alias.h\"",
        "createdAt" : "2019-05-10T11:45:46Z",
        "updatedAt" : "2019-05-13T15:03:54Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c29bbf38c419f7482e1f7e18831f0465ca6f1e7",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +194,198 @@  return config;\n}\nCREATE_CUDA_HOST_FUNCTION_ALIAS(GetGpuLaunchConfig, GetCudaLaunchConfig);\n\n// Calculate the Cuda launch config we should use for a kernel launch. This"
  },
  {
    "id" : "caf90662-6b08-463a-8eb4-8e221d06315b",
    "prId" : 24293,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/24293#pullrequestreview-220035479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e3e850c-fcfa-416d-a457-289300c3ed14",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Could you add a comment what the '1024' is about?",
        "createdAt" : "2019-03-27T14:06:03Z",
        "updatedAt" : "2019-04-12T19:34:30Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "d0208355-e33a-405e-9dd6-e4703c11df9e",
        "parentId" : "9e3e850c-fcfa-416d-a457-289300c3ed14",
        "authorId" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "body" : "Will do.",
        "createdAt" : "2019-03-27T14:22:47Z",
        "updatedAt" : "2019-04-12T19:34:30Z",
        "lastEditedBy" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "tags" : [
        ]
      },
      {
        "id" : "0b59070d-920c-4d31-af61-918c2814e9d0",
        "parentId" : "9e3e850c-fcfa-416d-a457-289300c3ed14",
        "authorId" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "body" : "Turns out the logic is just a clone of `GetGpuLaunchConfig(int, const Eigen::GpuDevice&)`. Added comments to address that.",
        "createdAt" : "2019-03-28T13:58:01Z",
        "updatedAt" : "2019-04-12T19:34:30Z",
        "lastEditedBy" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "45ed1a8978070a5111d9398ccc3af6c82cc9e0e9",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +176,180 @@  // Assume the kernel be simple enough that it is okay to use 1024 threads\n  // per workgroup.\n  thread_per_block = std::min(1024, d.maxGpuThreadsPerBlock());\n  block_count =\n      std::min(DivUp(physical_thread_count, thread_per_block),"
  },
  {
    "id" : "bb309030-77cb-4125-808e-a754dd6d4237",
    "prId" : 24293,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/24293#pullrequestreview-220034991",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2e62a63-11ce-4d0f-be5e-8fae37d3fff2",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Is the one block per SM limit intentional?",
        "createdAt" : "2019-03-27T14:07:30Z",
        "updatedAt" : "2019-04-12T19:34:30Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "08155e02-6015-445b-92e6-dfa48d4fe960",
        "parentId" : "d2e62a63-11ce-4d0f-be5e-8fae37d3fff2",
        "authorId" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "body" : "I'll need to check with HIP runtime team on this heuristic. It was given by them before we really have `hipOccupancyMaxPotentialBlockSize` implemented in HIP runtime.",
        "createdAt" : "2019-03-27T14:24:32Z",
        "updatedAt" : "2019-04-12T19:34:30Z",
        "lastEditedBy" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "tags" : [
        ]
      },
      {
        "id" : "603d8910-5ad8-41a2-85db-958e436a0219",
        "parentId" : "d2e62a63-11ce-4d0f-be5e-8fae37d3fff2",
        "authorId" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "body" : "Turns out the logic is just a clone of `GetGpuLaunchConfig(int, const Eigen::GpuDevice&)` above which assumes the kernel be simple and memory-bounded.",
        "createdAt" : "2019-03-28T13:56:56Z",
        "updatedAt" : "2019-04-12T19:34:30Z",
        "lastEditedBy" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "tags" : [
        ]
      },
      {
        "id" : "3d06fb60-6f05-46bb-93e9-ae1a2562f7ae",
        "parentId" : "d2e62a63-11ce-4d0f-be5e-8fae37d3fff2",
        "authorId" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "body" : "Added comment to reflect this.",
        "createdAt" : "2019-03-28T13:57:17Z",
        "updatedAt" : "2019-04-12T19:34:30Z",
        "lastEditedBy" : "0d1f2cd3-123c-44ec-b1f1-cbab0f4281ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "45ed1a8978070a5111d9398ccc3af6c82cc9e0e9",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +179,183 @@  block_count =\n      std::min(DivUp(physical_thread_count, thread_per_block),\n               d.getNumGpuMultiProcessors());\n#endif\n"
  }
]