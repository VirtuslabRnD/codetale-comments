[
  {
    "id" : "1d4f7b79-17e7-4ccd-8db2-0245572b8276",
    "prId" : 28571,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28571#pullrequestreview-236177659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "421e3bfb-8c19-4fd7-af9c-7c2e5576c9f8",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "You do start calling the Gpu variant here and in a number of places below. The PR is still manageable, so let's take this shortcut.",
        "createdAt" : "2019-05-10T11:50:38Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "8c09b019-06f0-4965-bba1-64e014bc1d75",
        "parentId" : "421e3bfb-8c19-4fd7-af9c-7c2e5576c9f8",
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "Sounds good, thanks",
        "createdAt" : "2019-05-10T15:43:25Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "1477a5ef71dcef344a565112d44fbe14a87aa478",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +165,169 @@__device__ inline bool GpuValidateShuffleSyncMask(unsigned mask,\n                                                   unsigned src_lane) {\n  unsigned src_dst_mask = 1u << GpuLaneId() | 1u << src_lane;\n#if CUDA_VERSION >= 9000\n  unsigned src_lane_mask = __shfl_sync(mask, mask, src_lane);"
  },
  {
    "id" : "a04b7180-abf2-44f9-a9ac-ba4ba1c34730",
    "prId" : 28571,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28571#pullrequestreview-236273984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98361cd1-ed59-4111-9c82-30cae2a48b71",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Is it valid to add the alias before the template specializations?",
        "createdAt" : "2019-05-10T11:52:00Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "49cce78e-026b-4a7a-9cd2-4194b545f72d",
        "parentId" : "98361cd1-ed59-4111-9c82-30cae2a48b71",
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "I used to have this concern as well. Looking at what the macro does, it does a text replacement to a perfect forwarding signature of the new function name. So the question ended up as whether the perfect forwarding declaration is location relevant. \r\n\r\nI'd say it is not because when compiler does the actual forwarding, it needs to match up the types and number of arguments. The process of matching up the correct function signature should be as the same as if the compiler resolves any other template functions.",
        "createdAt" : "2019-05-10T17:12:21Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      },
      {
        "id" : "74f32994-52f9-458a-90d0-45f4b829e90e",
        "parentId" : "98361cd1-ed59-4111-9c82-30cae2a48b71",
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "I think you are correct, the compiler has seen all template specializations when it instantiates the aliasing template.",
        "createdAt" : "2019-05-10T19:22:14Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1477a5ef71dcef344a565112d44fbe14a87aa478",
    "line" : 480,
    "diffHunk" : "@@ -1,1 +526,530 @@  return old;\n}\nCREATE_CUDA_DEVICE_FUNCTION_ALIAS(GpuAtomicCasHelper, CudaAtomicCasHelper);\n\n// Overload for floating point (using integer comparison to handle NaN"
  },
  {
    "id" : "0a374d62-4e72-40d0-9729-26d022813e0e",
    "prId" : 28571,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28571#pullrequestreview-238887075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f7291db-ea68-43d7-86bd-8425ab36ce74",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Do you want CudaGridRange conditionally, or could you just remove the #if/else? ",
        "createdAt" : "2019-05-15T09:26:37Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "7a410969-3cdd-4f4f-8846-c9a31ae66af9",
        "parentId" : "3f7291db-ea68-43d7-86bd-8425ab36ce74",
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "I want it conditionally, as in most cases of `CREATE_CUDA_*`. Right now the type is also used in `segment_reduction_ops_gpu.cu.cc` and `trdiagonal_solve_op_gpu.cu.cc` kernels. Having it conditionally available in cuda will make those code compile. \r\n\r\nSometime in the future when we send those (ROCm available) kernels up to review we will update the symbol to be using `GpuGridRange` and get rid of the conditional variable here.",
        "createdAt" : "2019-05-15T14:46:53Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      },
      {
        "id" : "24af44e2-84b7-48bc-aea5-3fb8470e8d17",
        "parentId" : "3f7291db-ea68-43d7-86bd-8425ab36ce74",
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "I would rather be able to change call sites independently of adding ROCm support to the kernels. The two things are unrelated and should not merged into single PRs. ",
        "createdAt" : "2019-05-16T12:47:37Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "93c6acfe-f230-4d78-a7d7-5ed86d104338",
        "parentId" : "3f7291db-ea68-43d7-86bd-8425ab36ce74",
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "Good clarification.\r\n\r\n@whchung FYI. Since in our codebase, all call-sites are already renamed. If we start with ROCm support of individual kernels, we will end up doing two things together. Therefore it looks like the right sequence is to:\r\n\r\n- File individual PRs to each symbols from `cuda` to `gpu`, one symbol in a PR at a time.\r\n- File individual PRs to enable one kernel as a time.",
        "createdAt" : "2019-05-16T13:34:13Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      },
      {
        "id" : "fcd6ac72-47a8-4f7e-8df5-4e85b288bb93",
        "parentId" : "3f7291db-ea68-43d7-86bd-8425ab36ce74",
        "authorId" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "body" : "@chsigg, @jerryyin \r\n\r\nI want to make sure we are on the same page here.\r\n\r\nOnce this PR is merged, we will have the \r\n1. Gpu* names visiblle in both ROCm and CUDA builds\r\n2. Cuda* names visible only in CUDA builds\r\n\r\nOnce this PR is merged, we will also start filing PRs for ops that reference these names (in the same 1 PR/op pattern we have established so far). As part of the changes for such a PR, our intent is to update the reference(s) to the Cuda* name(s) to Gpu* name(s).  This implies that the renaming will be spread out across multiple PRs.\r\n\r\nIf it is preferable to have one PR, that does the Cuda* to Gpu* renaming for all references in a single shot (@chsigg, this is what my interpretation of your comment was...please correct if wrong), then we need to file that PR before we file any other PRs to upstream ROCm support. Remember that Cuda* names are not visible in ROCm builds!\r\n",
        "createdAt" : "2019-05-16T14:33:18Z",
        "updatedAt" : "2019-05-16T14:35:48Z",
        "lastEditedBy" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "tags" : [
        ]
      },
      {
        "id" : "15f01e6d-1912-4d8f-a00b-34b8b3740ee8",
        "parentId" : "3f7291db-ea68-43d7-86bd-8425ab36ce74",
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "> Remember that Cuda* names are not visible in ROCm builds!\r\n\r\nRight, I forgot that. So the #ifndef here is consistent with the rest. \r\n\r\nIt will be easier if we change the call sites from Cuda* to Gpu* on our side. We have dependencies that you don't see and infrastructure to help making the change. It will be a series of CLs.\r\n",
        "createdAt" : "2019-05-17T12:04:54Z",
        "updatedAt" : "2019-05-17T12:05:10Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1477a5ef71dcef344a565112d44fbe14a87aa478",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +94,98 @@};\n\n#ifndef TENSORFLOW_USE_ROCM\ntemplate <typename... T>\nusing CudaGridRange = GpuGridRange<T...>;"
  },
  {
    "id" : "ba6bcdc7-b71d-4faa-90c7-08fcce9a0fba",
    "prId" : 24293,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/24293#pullrequestreview-229783889",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9668d95-5a16-43e4-9107-afe248a0c598",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "This doesn't look correct: the static_cast will do a conversion. You probably wanted to use reinterpret_cast, which is technically not legal either.",
        "createdAt" : "2019-04-23T20:27:23Z",
        "updatedAt" : "2019-04-23T20:27:23Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "83dec252-292c-4e1b-92b1-5dcfd197a6bf",
        "parentId" : "d9668d95-5a16-43e4-9107-afe248a0c598",
        "authorId" : "845afb08-ba85-462c-92e8-305877b05a5d",
        "body" : "memcpy is the only legal way of doing type punning in C++.",
        "createdAt" : "2019-04-23T20:39:54Z",
        "updatedAt" : "2019-04-23T20:39:54Z",
        "lastEditedBy" : "845afb08-ba85-462c-92e8-305877b05a5d",
        "tags" : [
        ]
      }
    ],
    "commit" : "45ed1a8978070a5111d9398ccc3af6c82cc9e0e9",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +269,273 @@  return value;\n#elif TENSORFLOW_USE_ROCM\n  uint64_t tmp = static_cast<uint64_t>(value);\n  lo = static_cast<unsigned>(tmp);\n  hi = static_cast<unsigned>(tmp >> 32);"
  },
  {
    "id" : "1688ad2e-7056-4edb-b262-e0651b820b65",
    "prId" : 24293,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/24293#pullrequestreview-229781428",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc73a5b4-a659-4c4c-b11e-009797935744",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Shouldn't this comment be about atomicMin/Max rather than the ordinary min/max?",
        "createdAt" : "2019-04-23T20:34:35Z",
        "updatedAt" : "2019-04-23T20:34:36Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      }
    ],
    "commit" : "45ed1a8978070a5111d9398ccc3af6c82cc9e0e9",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +642,646 @@\n/*\n * CUDA runtime headers have the following defined\n *   __device__  int max(int, int)\n *   __device__  float max(float, float)"
  }
]