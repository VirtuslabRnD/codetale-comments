[
  {
    "id" : "9c9dc300-3d46-4eac-b495-5120cddc379a",
    "prId" : 23567,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/23567#pullrequestreview-174060455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db87c9b5-6afd-448d-bcfc-bec8a188323b",
        "parentId" : null,
        "authorId" : "0ef5de33-9a2c-4376-972d-d9d2d7a02104",
        "body" : "why do we need max_total_size?",
        "createdAt" : "2018-11-09T18:27:14Z",
        "updatedAt" : "2019-01-29T22:33:26Z",
        "lastEditedBy" : "0ef5de33-9a2c-4376-972d-d9d2d7a02104",
        "tags" : [
        ]
      },
      {
        "id" : "b23bc7a3-73ca-4c8b-ba3a-9f42638a2886",
        "parentId" : "db87c9b5-6afd-448d-bcfc-bec8a188323b",
        "authorId" : "cbf01141-48e5-4384-ad99-72f52aeddb60",
        "body" : "max_total_size is the total output size per batch. If use_static_shapes is false, then we always pad to max_total_size.",
        "createdAt" : "2018-11-09T18:45:44Z",
        "updatedAt" : "2019-01-29T22:33:26Z",
        "lastEditedBy" : "cbf01141-48e5-4384-ad99-72f52aeddb60",
        "tags" : [
        ]
      },
      {
        "id" : "17a4a09e-68a2-4b1b-9e99-615f794e1dc6",
        "parentId" : "db87c9b5-6afd-448d-bcfc-bec8a188323b",
        "authorId" : "0ef5de33-9a2c-4376-972d-d9d2d7a02104",
        "body" : "I meant we do use the variable here in the NMSLiteShapeFn",
        "createdAt" : "2018-11-11T20:55:16Z",
        "updatedAt" : "2019-01-29T22:33:26Z",
        "lastEditedBy" : "0ef5de33-9a2c-4376-972d-d9d2d7a02104",
        "tags" : [
        ]
      },
      {
        "id" : "f520d02d-4822-4d6e-a611-322373dce8fc",
        "parentId" : "db87c9b5-6afd-448d-bcfc-bec8a188323b",
        "authorId" : "cbf01141-48e5-4384-ad99-72f52aeddb60",
        "body" : "This statement just checks the rank of the max_total_size input. I'm doing similar checks for other inputs as well. Let me know if that's not the correct way.",
        "createdAt" : "2018-11-12T19:34:21Z",
        "updatedAt" : "2019-01-29T22:33:26Z",
        "lastEditedBy" : "cbf01141-48e5-4384-ad99-72f52aeddb60",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d3fa90c47af1fd0d893a837f6c50d311060cd58",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +142,146 @@  ShapeHandle max_output_size_per_class;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &max_output_size_per_class));\n  ShapeHandle max_total_size;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &max_total_size));\n  ShapeHandle unused_shape;"
  },
  {
    "id" : "4e247b53-860f-4f96-8343-0cdb4c68b85a",
    "prId" : 23567,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/23567#pullrequestreview-182043648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54552dfa-197e-4c8e-b82b-6eea80ad79c8",
        "parentId" : null,
        "authorId" : "0ef5de33-9a2c-4376-972d-d9d2d7a02104",
        "body" : "should we just default to size_per_class * num_classes. a negative value would mean a user does not intended to cap.\r\nwe can check that par_per_class is true.",
        "createdAt" : "2018-12-05T23:45:29Z",
        "updatedAt" : "2019-01-29T22:33:26Z",
        "lastEditedBy" : "0ef5de33-9a2c-4376-972d-d9d2d7a02104",
        "tags" : [
        ]
      },
      {
        "id" : "3b8c1bb6-7a60-4fae-ad31-ea8cf6a035c0",
        "parentId" : "54552dfa-197e-4c8e-b82b-6eea80ad79c8",
        "authorId" : "cbf01141-48e5-4384-ad99-72f52aeddb60",
        "body" : "https://github.com/tensorflow/models/blob/master/research/object_detection/builders/post_processing_builder.py#L78\r\nFails if max_total_size is less than size_per_class. Even if pad_per_class is true, we still need max_total_size as it might be smaller than size_per_class * num_classes. \r\nIf the user does not intend to cap, they can set pad_per_class to true and max_total_size to > size_per_class * num_classes. Does that sound reasonable?",
        "createdAt" : "2018-12-06T00:52:32Z",
        "updatedAt" : "2019-01-29T22:33:26Z",
        "lastEditedBy" : "cbf01141-48e5-4384-ad99-72f52aeddb60",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d3fa90c47af1fd0d893a837f6c50d311060cd58",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +170,174 @@  TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(3, &output_dim));\n  if (c->ValueKnown(output_dim) && c->Value(output_dim) <= 0) {\n    return errors::InvalidArgument(\"max_total_size should be > 0 \");\n  }\n  DimensionHandle size_per_class;"
  },
  {
    "id" : "1c46c5fb-3637-4ed4-aea9-37ecdf42ba10",
    "prId" : 9563,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/9563#pullrequestreview-37900501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26032704-14c9-4bf7-baca-92786a645579",
        "parentId" : null,
        "authorId" : "1836b116-1c54-4d2e-8c7f-d83e825237ef",
        "body" : "Two thoughts here:\r\n\r\n1) uint8 isn't the only possible output: if the bpp is 32 bits, we'd want to be able to produce uint32 tensors, right?  If so, perhaps we can do:\r\n\r\n.Output(\"image: T\")\r\n.Attr(\"T: {uint8, uint16, uint32} = DT_UINT8\")\r\n\r\nI'm not sure what the default should be: are most bmp's only bpp == 8 ?\r\n\r\nYou then would need to templatize the OpKernel by output type, and register different kernels using TypeConstraint on REGISTER_KERNEL; let me know if you'd like some more pointers about how to do this!",
        "createdAt" : "2017-05-12T15:20:19Z",
        "updatedAt" : "2017-05-17T03:26:01Z",
        "lastEditedBy" : "1836b116-1c54-4d2e-8c7f-d83e825237ef",
        "tags" : [
        ]
      },
      {
        "id" : "0fd598ea-00bb-44be-8c42-0fd454bc9279",
        "parentId" : "26032704-14c9-4bf7-baca-92786a645579",
        "authorId" : "15fbfdc4-23b1-4ed3-9c13-a4cc1dffc6de",
        "body" : "` Unrecognized type string 'uint32' from Attr(\"T: {uint8, uint16, uint32} = DT_UINT8\") for Op DecodeBmp`\r\nIt seems uint32 is not a valid type supported by `tensorflow/core/framework/op_def_builder.cc`. Is int32 acceptable? Or you really want to have uint32.",
        "createdAt" : "2017-05-12T17:36:30Z",
        "updatedAt" : "2017-05-17T03:26:01Z",
        "lastEditedBy" : "15fbfdc4-23b1-4ed3-9c13-a4cc1dffc6de",
        "tags" : [
        ]
      },
      {
        "id" : "dcdb6719-c3fe-426e-940d-de3634348383",
        "parentId" : "26032704-14c9-4bf7-baca-92786a645579",
        "authorId" : "1836b116-1c54-4d2e-8c7f-d83e825237ef",
        "body" : "Ah, I just realized, because the bpp value sort of implies 8-bits per channel, it probably doesn't make sense to have anything but uint8 for now, so maybe we keep it as is.  Sorry about this.",
        "createdAt" : "2017-05-12T17:40:32Z",
        "updatedAt" : "2017-05-17T03:26:01Z",
        "lastEditedBy" : "1836b116-1c54-4d2e-8c7f-d83e825237ef",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8bf54f0283438e297b3cb0768f77f47635f65f3",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +567,571 @@REGISTER_OP(\"DecodeBmp\")\n    .Input(\"contents: string\")\n    .Output(\"image: uint8\")\n    .Attr(\"channels: int = 0\")\n    .SetShapeFn(DecodeImageShapeFn)"
  }
]