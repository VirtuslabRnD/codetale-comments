[
  {
    "id" : "9b4b91eb-960c-456d-a789-9f4a00160668",
    "prId" : 46551,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/46551#pullrequestreview-576725856",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73b4fc77-f8c0-4d6f-99d2-a8add301d994",
        "parentId" : null,
        "authorId" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "body" : "Consider adding a static factory function that initializes streams etc and returns a stream_executor::port::StatusOr. That way you can guarantee the object is initialized with non-nullable values",
        "createdAt" : "2021-01-20T19:23:10Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "tags" : [
        ]
      },
      {
        "id" : "35cf2c35-bf56-4180-8f9a-e0992229657b",
        "parentId" : "73b4fc77-f8c0-4d6f-99d2-a8add301d994",
        "authorId" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "body" : "This file use a very limited set of stream_executor funtionality as it would add too much overhead. So I think it make the file more clear to use the CUDA syntax everywhere. This make it more locally stable and to me it looks more readable then mixing 2 differents API.\r\n\r\nIf the creation fail, I launch a FATAL error 2 lines bellow. So there is no chance of having non-initialized object staying around.",
        "createdAt" : "2021-01-20T20:02:54Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "tags" : [
        ]
      },
      {
        "id" : "310eafd4-7d35-44f5-ab57-fd151f273628",
        "parentId" : "73b4fc77-f8c0-4d6f-99d2-a8add301d994",
        "authorId" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "body" : "I don't view this as CUDA vs stream executor syntax as much as general readability/usability improvement. With something like a factory and non-nullable const members, a reader can more easily reason about the possible states that this object takes on. As it is currently written, the reader has to trace through the code to understand the possible states.\r\n\r\nI'm not going to block the PR on this change because this can be easily updated later on.",
        "createdAt" : "2021-01-26T20:17:09Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "tags" : [
        ]
      }
    ],
    "commit" : "72cd28e7c33527013209cd9c32b63c36232822ac",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +70,74 @@  }\n\n  cudaError_t cerr = cudaStreamCreate(&cuda_stream_);\n  if (cerr != cudaSuccess) {\n    LOG(FATAL) << \"could not allocate CUDA stream for context : \""
  },
  {
    "id" : "6ee6b2b5-782a-4aed-8ebf-312c425483e1",
    "prId" : 46551,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/46551#pullrequestreview-572575233",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ee9ef61-68d0-4aca-bd83-037bd4adde30",
        "parentId" : null,
        "authorId" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "body" : "Can you move the ifdef guards outside the class so that you don't need the bogus pointers? Perhaps add a warning in useCudaMallocAsyncAllocator that informs the user that env variable has no effect because they're using the wrong version of CUDA",
        "createdAt" : "2021-01-20T19:33:23Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "tags" : [
        ]
      },
      {
        "id" : "72016687-5646-498c-bf2e-650705b0659f",
        "parentId" : "0ee9ef61-68d0-4aca-bd83-037bd4adde30",
        "authorId" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "body" : "I think it is better to keep all the cudaMallocAsync (including version detection logic) in this file. It help keep all cudaMallocAsync logic at one place.\r\nI tried something to remove the bogus variable here.",
        "createdAt" : "2021-01-20T19:51:35Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "tags" : [
        ]
      }
    ],
    "commit" : "72cd28e7c33527013209cd9c32b63c36232822ac",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +136,140 @@void* GpuCudaMallocAsyncAllocator::AllocateRaw(size_t alignment,\n                                               size_t num_bytes) {\n#if CUDA_VERSION < 11020 || !defined(GOOGLE_CUDA)\n  return nullptr;\n#else"
  },
  {
    "id" : "45840bc6-df14-47a1-9b20-9b9ccc6dc59e",
    "prId" : 46551,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/46551#pullrequestreview-576912973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8f5227d-55d5-430d-b5d8-9e431774e4b3",
        "parentId" : null,
        "authorId" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "body" : "Doesn't this block need to be guarded by the lock_?\r\n\r\nSame for the RequestedSizes and AllocatedSizes functions?",
        "createdAt" : "2021-01-20T19:43:05Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "tags" : [
        ]
      },
      {
        "id" : "28fe87c9-ec9e-4ec3-b866-696169a33cd9",
        "parentId" : "c8f5227d-55d5-430d-b5d8-9e431774e4b3",
        "authorId" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "body" : "The declaration is:\r\n\r\n`absl::flat_hash_map<const void*, size_t> size_map_ GUARDED_BY(lock_);`\r\n\r\nI checked at other places in TF and it seems to guard automatically all access to the variable size_map_. Otherwise, there would be other places that would be buggy.",
        "createdAt" : "2021-01-20T20:11:00Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "tags" : [
        ]
      },
      {
        "id" : "2df07015-6f17-4e24-a6d9-63cfe29c4b71",
        "parentId" : "c8f5227d-55d5-430d-b5d8-9e431774e4b3",
        "authorId" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "body" : "Could you provide a link to the other locations where this is being used? I believe that this is a bug and that you should be using the TF_GUARDED_BY macro. This macro has no effect on behavior, instead it adds thread_annotation which should forbid compilation of code that uses the value being guarded without also acquiring the lock.",
        "createdAt" : "2021-01-26T20:08:51Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "c0867b0b-07b8-440c-8b34-70148720307b",
        "tags" : [
        ]
      },
      {
        "id" : "fef57a8b-cbf0-477c-afa5-2c321ef3c2ae",
        "parentId" : "c8f5227d-55d5-430d-b5d8-9e431774e4b3",
        "authorId" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "body" : "I did a grep an took a random one. \r\nI just redid one and the one I picked activated the lock manually as you expect.\r\nI'm not sure if I badly understood the code when reading or there is an hidden bug. Sadly I have no hope of finding again the exact place that I looked at last time. There is too many places to check.\r\n\r\nIn all cases, I updated the code. In 2 places I didn't take it to read a simple value. In those places, I think it was safe to not take it.",
        "createdAt" : "2021-01-27T01:42:16Z",
        "updatedAt" : "2021-02-08T19:51:03Z",
        "lastEditedBy" : "f5c553cb-d123-4eb4-9f6d-9a715871f6eb",
        "tags" : [
        ]
      }
    ],
    "commit" : "72cd28e7c33527013209cd9c32b63c36232822ac",
    "line" : 170,
    "diffHunk" : "@@ -1,1 +168,172 @@        std::max<std::size_t>(stats_.largest_alloc_size, num_bytes);\n    size_map_[ptr] = num_bytes;\n  }\n  VLOG(10) << Name() << \" Allocated \" << num_bytes << \" at \" << ptr;\n  return ptr;"
  }
]