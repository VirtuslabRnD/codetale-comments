[
  {
    "id" : "4d419042-6430-4e4f-89c3-de22a6dbd936",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65934408-b579-4f2b-819f-070d3e55ab69",
        "parentId" : null,
        "authorId" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "body" : "Can you add a test that uses the new functor, so we can make sure the padding/stride computations match those of the existing implementation?  One way may be to register the kernel in all cases but register it with a different name or a specific label when it's not meant to be the default conv implementation\n",
        "createdAt" : "2016-08-16T15:10:48Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "tags" : [
        ]
      },
      {
        "id" : "62ebbc1c-947e-4afd-a0df-2ce7f581f5a9",
        "parentId" : "65934408-b579-4f2b-819f-070d3e55ab69",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "We've discussed this off-line. The ideal solution is to run all our tests with multiple implementations, but it's not clear how to do that right now. I've manually run the complete set of OS X tests with all the functor combinations, and they pass, so that may be the best approach for now\n",
        "createdAt" : "2016-08-17T20:09:49Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +64,68 @@#define USE_ACCELERATE_GEMM\n#endif  // __APPLE__\n\nnamespace tensorflow {\n"
  },
  {
    "id" : "38077174-8064-4f2d-9abd-1b8b1723513c",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbf41ddc-aa1c-41a8-99c7-1b82353b7012",
        "parentId" : null,
        "authorId" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "body" : "int is used in many places.  Should it be int64, or check the width and the height fit in int32?  (I haven't checked below - maybe there is such a check).\n",
        "createdAt" : "2016-08-16T15:15:23Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "tags" : [
        ]
      },
      {
        "id" : "3e78b08e-1e9a-4c68-8ebc-9076737ee139",
        "parentId" : "dbf41ddc-aa1c-41a8-99c7-1b82353b7012",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "Good point. We do actually make sure that the int64 input parameters fit into int's as part of the common Compute() code below.\n",
        "createdAt" : "2016-08-17T20:14:16Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +85,89 @@class ReferenceConvFunctor {\n public:\n  void operator()(OpKernelContext* context, const T1* input_data,\n                  int input_batches, int input_height, int input_width,\n                  int input_depth, const T2* filter_data, int filter_height,"
  },
  {
    "id" : "f2e3c326-16f6-4e58-9a2f-1f5ac500953c",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d50224d3-868a-4363-adfa-66faf85f371a",
        "parentId" : null,
        "authorId" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "body" : "why T1,T2,T3 instead of a single template parameter T ?\n",
        "createdAt" : "2016-08-16T15:23:39Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "tags" : [
        ]
      },
      {
        "id" : "d47a2491-b779-442e-b66e-56dc98322f0c",
        "parentId" : "d50224d3-868a-4363-adfa-66faf85f371a",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "With lower-precision formats, it's common to need to accumulate into higher-precision variable (e.g. float16 inputs, with a float32 output, or uint8 inputs with a uint32 output). Even though the currently-supported types don't take advantage of this, I have a mild preference for keeping the types specified separately. I've added a comment to make this a bit clearer, if that helps?\n",
        "createdAt" : "2016-08-17T20:18:31Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : null,
    "diffHunk" : "@@ -1,1 +227,231 @@// output types so that the accumulator can potentially be higher-precision than\n// the inputs, even though we don't currently take advantage of this.\ntemplate <class T1, class T2, class T3>\nclass FastGemmFunctor {\n public:"
  },
  {
    "id" : "acca4692-eb05-4372-a619-dac79f93ee2c",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6274e90-0245-4304-bc49-08658f08c5d7",
        "parentId" : null,
        "authorId" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "body" : "make sure patches_per_chunk >= 1 ?\n",
        "createdAt" : "2016-08-16T15:29:14Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "tags" : [
        ]
      },
      {
        "id" : "61d84cc5-4523-4e01-ae2d-f96c630614b3",
        "parentId" : "d6274e90-0245-4304-bc49-08658f08c5d7",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "Done.\n",
        "createdAt" : "2016-08-17T20:43:36Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : null,
    "diffHunk" : "@@ -1,1 +344,348 @@    OP_REQUIRES(context, (filter_value_count * sizeof(T1)) <= max_chunk_size,\n                errors::InvalidArgument(\"Im2Col patch too large for buffer\"));\n    const size_t patches_per_chunk =\n        max_chunk_size / (filter_value_count * sizeof(T1));\n    const size_t im2col_size = patches_per_chunk * filter_value_count;"
  },
  {
    "id" : "9060ac3c-73f7-4429-9924-3ba031e776ef",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73d66972-0ecc-4e76-a71d-c3c12e123422",
        "parentId" : null,
        "authorId" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "body" : "This doesn't validate the filter rows+cols.  Should it?\n",
        "createdAt" : "2016-08-16T15:45:22Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "3c462bc6-aae1-413d-908f-fff1b1ed68ef",
        "tags" : [
        ]
      },
      {
        "id" : "24bce10f-f190-4b2b-b5e6-83be93e82350",
        "parentId" : "73d66972-0ecc-4e76-a71d-c3c12e123422",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "The filter rows and cols get checked above, in a loop over the filter dims.\n",
        "createdAt" : "2016-08-17T21:05:18Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : 539,
    "diffHunk" : "@@ -1,1 +537,541 @@    // The first dimension for filter is rows/height.\n    const int64 input_rows_raw = GetTensorDim(input, data_format_, 'H');\n    OP_REQUIRES(context, FastBoundsCheck(input_rows_raw,\n                                         std::numeric_limits<int>::max()),\n                errors::InvalidArgument(\"Input rows too large\"));"
  },
  {
    "id" : "9b1a05c2-1646-4576-b2e1-60d9b04928be",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "765f5b01-faba-466d-ae4f-db4178194f97",
        "parentId" : null,
        "authorId" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "body" : "There is an Im2Col implementation at the top of conv_grad_ops.cc, that used to be exposed in a header file (could be re-exposed if used elsewere). Any chance that could be re-used here for part of this loop?\n",
        "createdAt" : "2016-08-16T17:37:28Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "tags" : [
        ]
      },
      {
        "id" : "938e7745-296c-4456-ac2b-e84473d114b8",
        "parentId" : "765f5b01-faba-466d-ae4f-db4178194f97",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "That implementation only fills in a single 'pixel' at a time, whereas we've found we get best performance by copying or setting a row of pixels in one call. That does mean some extra complexity in the calculations used by this version though.\n",
        "createdAt" : "2016-08-17T20:53:54Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : 370,
    "diffHunk" : "@@ -1,1 +368,372 @@    T1* im2col_buffer = im2col_buffer_resource->data;\n\n    for (int batch = 0; batch < input_batches; ++batch) {\n      const T1* input_batch_start =\n          input_data + (batch * input_height * input_width * input_depth);"
  },
  {
    "id" : "8fef9eed-8f79-43f1-ab02-c0730ad5aaf0",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6b208a8-94ae-49ab-b391-a32010179b56",
        "parentId" : null,
        "authorId" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "body" : "What would happen if you allocated a using the DeviceContext::allocate_temp? Allocate a temporary tensor once (before the loop), then re-use it in the loop...\n",
        "createdAt" : "2016-08-16T17:48:11Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "tags" : [
        ]
      },
      {
        "id" : "2919d91f-cfe4-49c4-8b57-8e760d5b5372",
        "parentId" : "c6b208a8-94ae-49ab-b391-a32010179b56",
        "authorId" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "body" : "NEVERMIND. I remember our previous conversation, that you needed this memory to live across OpKernel invocations....\n",
        "createdAt" : "2016-08-16T17:50:04Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "tags" : [
        ]
      },
      {
        "id" : "647524da-7184-4c07-a00c-29b7b88ca23d",
        "parentId" : "c6b208a8-94ae-49ab-b391-a32010179b56",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "Yes, that's right, the goal is to allocate once per graph, so that overall memory usage is minimized.\n",
        "createdAt" : "2016-08-17T20:44:28Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : null,
    "diffHunk" : "@@ -1,1 +347,351 @@        max_chunk_size / (filter_value_count * sizeof(T1));\n    const size_t im2col_size = patches_per_chunk * filter_value_count;\n    // Because memory allocation is very expensive on mobile platforms, try to\n    // allocate a persistent buffer that will be kept around between calls. We\n    // use TensorFlow's resource management to ensure that the memory will be"
  },
  {
    "id" : "09b98514-1e77-48de-883c-9aaae6116d42",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3f97d9f-75d4-42e9-955d-4eb47bad0952",
        "parentId" : null,
        "authorId" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "body" : "Holding this lock for the entire op scares me. Can you change the Im2ColBufferResource to do this locking internally (around some simple free list), and return a buffer to the caller? This way, you wont have to hold a lock for the entire op, and concurrent access to the Im2ColBufferResource can be managed internally (i.e. it could just allocate another buffer to a concurrent caller, rather than just blocking).\nWhat kinds of concurrent access patterns do you expect?  You could serialize things with \"intra_op_threads\" flag (if needed).\n",
        "createdAt" : "2016-08-16T17:54:42Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "tags" : [
        ]
      },
      {
        "id" : "52959832-1ecb-430d-ba1b-74ca44f7e11b",
        "parentId" : "a3f97d9f-75d4-42e9-955d-4eb47bad0952",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "For the particular use case I'm trying to address, memory is an extremely scarce resource, and also the expected pattern is to have no inter-op parallelism, only intra-op. This seems to be the best way to get conv performance on the mobile and embedded platforms I'm focused on, and from talking to XQ that seems true even for the server GPU side. My preference would be to stick to this blocking approach for now, unless we run into situations where it's a problem. What do you think?\n",
        "createdAt" : "2016-08-17T20:51:27Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      },
      {
        "id" : "31ae942f-2e64-4867-8c65-f27c3aa355f1",
        "parentId" : "a3f97d9f-75d4-42e9-955d-4eb47bad0952",
        "authorId" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "body" : "You could make the max free list size 1, and have callers block and wait if the free list size is zero.  Then it's clear that callers are serializing on that resource. Anyway, maybe it's a minor nitpick, so up to you...\n",
        "createdAt" : "2016-08-18T22:08:05Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : 366,
    "diffHunk" : "@@ -1,1 +364,368 @@    // aimed at have intra-op parallelism as their focus though, so it shouldn't\n    // be an issue.\n    mutex_lock lock_buffer(im2col_buffer_resource->mu);\n    core::ScopedUnref unref_buffer(im2col_buffer_resource);\n    T1* im2col_buffer = im2col_buffer_resource->data;"
  },
  {
    "id" : "1b3eff55-d388-4594-b097-b403603a8400",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5afb2dd0-a81e-450f-90d3-ed8d17409c64",
        "parentId" : null,
        "authorId" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "body" : "Could pull filter_value_count \\* sizeof(T1) out into a constant and re-use it below.\n",
        "createdAt" : "2016-08-18T22:19:22Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "711b9ad2-1118-41b4-bbcb-e363bd57c690",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : 344,
    "diffHunk" : "@@ -1,1 +342,346 @@    // In this case, we've picked 16 megabytes as a reasonable limit.\n    const size_t max_chunk_size = (16 * 1024 * 1024);\n    OP_REQUIRES(context, (filter_value_count * sizeof(T1)) <= max_chunk_size,\n                errors::InvalidArgument(\"Im2Col patch too large for buffer\"));\n    const size_t patches_per_chunk ="
  },
  {
    "id" : "f0bd682f-1617-4b3c-ac0a-498fa409c67f",
    "prId" : 3778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e50f54a2-6dcb-4b8b-be34-06d9ebac38f7",
        "parentId" : null,
        "authorId" : "0ba8fb18-637a-4ab3-a7c8-cd971823fe45",
        "body" : "Why do we need to include this? The problem with including slow reference implementations, is that they end up being used and are hard to get rid of.\n",
        "createdAt" : "2016-08-22T17:27:11Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "0ba8fb18-637a-4ab3-a7c8-cd971823fe45",
        "tags" : [
        ]
      },
      {
        "id" : "d01dc62d-cd35-43e3-810f-8ea1eb2c5204",
        "parentId" : "e50f54a2-6dcb-4b8b-be34-06d9ebac38f7",
        "authorId" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "body" : "We discussed this offline, but to summarize it's useful for bootstrapping porting to new platforms, though I agree it's a little awkward here.\n",
        "createdAt" : "2016-08-23T01:34:11Z",
        "updatedAt" : "2016-08-23T01:49:01Z",
        "lastEditedBy" : "2424ca1f-2171-47cc-b401-ceb61b6f3fde",
        "tags" : [
        ]
      }
    ],
    "commit" : "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +195,199 @@// enable. Assumes row-major ordering of the values in memory.\ntemplate <class T1, class T2, class T3>\nclass ReferenceGemmFunctor {\n public:\n  void operator()(size_t m, size_t n, size_t k, const T1* a, size_t lda,"
  }
]