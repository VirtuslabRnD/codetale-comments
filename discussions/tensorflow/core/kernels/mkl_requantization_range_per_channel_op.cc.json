[
  {
    "id" : "c23e7e47-28ad-4f27-ac6a-3e8ff947b2bd",
    "prId" : 39893,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/39893#pullrequestreview-426445335",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b66c51f4-a04d-4831-870f-a54eff682528",
        "parentId" : null,
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "Same here. Can we wrap `#ifndef ENABLE_MKLDNN_THREADPOOL` around this line instead? Please also keep the `TODO`.",
        "createdAt" : "2020-06-06T22:10:51Z",
        "updatedAt" : "2020-06-08T17:31:01Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "4770a79b-dbd3-496f-a285-9c123f83d890",
        "parentId" : "b66c51f4-a04d-4831-870f-a54eff682528",
        "authorId" : "23ce0557-a56f-4e03-9da9-4913734266c2",
        "body" : "Done",
        "createdAt" : "2020-06-08T17:09:56Z",
        "updatedAt" : "2020-06-08T17:31:01Z",
        "lastEditedBy" : "23ce0557-a56f-4e03-9da9-4913734266c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b4b07c098170bd891f2426ee9c043249ac41983",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +77,81 @@\n#ifndef ENABLE_MKLDNN_THREADPOOL\n#pragma omp parallel for reduction(max : out_min_max)\n#endif  // !ENABLE_MKLDNN_THREADPOOL\n    // TODO: Add eigen parallel_for"
  },
  {
    "id" : "2f8d91d9-920e-416e-b6e9-1330ea75c0ba",
    "prId" : 25203,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/25203#pullrequestreview-199826741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71ff84b9-62e5-486a-bcb9-9a1be35f90c7",
        "parentId" : null,
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "Why do we need to transpose the input?",
        "createdAt" : "2019-01-28T23:10:25Z",
        "updatedAt" : "2019-02-09T00:28:34Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "d0b2844d-f5f8-4a5f-88db-bdbc1d87a682",
        "parentId" : "71ff84b9-62e5-486a-bcb9-9a1be35f90c7",
        "authorId" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "body" : "For better cache efficiency.",
        "createdAt" : "2019-01-29T21:19:02Z",
        "updatedAt" : "2019-02-09T00:28:34Z",
        "lastEditedBy" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "tags" : [
        ]
      },
      {
        "id" : "7fdd9540-396e-4529-ab32-638e3147d253",
        "parentId" : "71ff84b9-62e5-486a-bcb9-9a1be35f90c7",
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "But matrix transpose is expensive and `transposed_input` is not reused anywhere since this is just element-wise operation. Why not just go through the matrix as-is to find `min` and `max`? That would read the whole matrix just once and not twice.\r\n\r\nI understand that doing the transpose makes it easier to parallelize (since channel is the last dimension). If you prefer to use `shuffle`, please add a TODO in the code that this is for ease of implementation and could be optimized for performance later.",
        "createdAt" : "2019-01-31T22:52:02Z",
        "updatedAt" : "2019-02-09T00:28:34Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "97e95114-55e7-4568-b130-adac6610436c",
        "parentId" : "71ff84b9-62e5-486a-bcb9-9a1be35f90c7",
        "authorId" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "body" : "Point taken! Not sure about the Eigen implementation but the mkl version of transpose was fast. Another point to note is that this op is a calibration step, meaning it will be changed to a const node for the final inference graph.\r\n\r\nnevertheless ToDo and comments are due and provided.",
        "createdAt" : "2019-02-02T03:15:10Z",
        "updatedAt" : "2019-02-09T00:28:34Z",
        "lastEditedBy" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "tags" : [
        ]
      },
      {
        "id" : "e2bf2d5b-8481-4fb6-87d0-9dc1bed8333f",
        "parentId" : "71ff84b9-62e5-486a-bcb9-9a1be35f90c7",
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "Got it. Appreciate the TODO and the note about const node!",
        "createdAt" : "2019-02-04T22:15:22Z",
        "updatedAt" : "2019-02-09T00:28:34Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      }
    ],
    "commit" : "16106ddfd2bdce83f89525577a2999f9d38063cb",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +70,74 @@    // Note that this operation is a calibration step for quantization and will\n    // cease to exist in the final inference graph(will exist as a const node).\n    auto transposed_input = input_matrix.shuffle(shuffling);\n\n    // Find the ranges of each channel in parallel."
  },
  {
    "id" : "fb37f92e-6185-4c08-9a71-c193316cb959",
    "prId" : 25203,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/25203#pullrequestreview-199334781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77fb24f8-b669-44c9-b98c-f151731f6a52",
        "parentId" : null,
        "authorId" : "c02b6806-c89c-4249-b10e-ef3335355570",
        "body" : "Should any of these be consts?```scale``` ```min_per_channel``` ```max_per_channel``` ```abs_max```",
        "createdAt" : "2019-01-29T03:44:13Z",
        "updatedAt" : "2019-02-09T00:28:34Z",
        "lastEditedBy" : "c02b6806-c89c-4249-b10e-ef3335355570",
        "tags" : [
        ]
      },
      {
        "id" : "855b51b6-e2b9-4db7-876d-59df6b54c856",
        "parentId" : "77fb24f8-b669-44c9-b98c-f151731f6a52",
        "authorId" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "body" : "done!",
        "createdAt" : "2019-02-02T04:47:43Z",
        "updatedAt" : "2019-02-09T00:28:34Z",
        "lastEditedBy" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "tags" : [
        ]
      }
    ],
    "commit" : "16106ddfd2bdce83f89525577a2999f9d38063cb",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +84,88 @@      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] ="
  }
]