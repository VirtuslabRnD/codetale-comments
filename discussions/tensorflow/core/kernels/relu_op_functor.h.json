[
  {
    "id" : "d8a255d9-aaaf-4141-a2e4-231250c32cce",
    "prId" : 10818,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/10818#pullrequestreview-47456143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e4fc14b-9928-4538-94b6-68e2f3202b88",
        "parentId" : null,
        "authorId" : "c8f2b008-60f4-4709-b654-47bc9a7795a2",
        "body" : "where do these numbers come from?",
        "createdAt" : "2017-06-26T17:32:28Z",
        "updatedAt" : "2017-07-25T15:03:45Z",
        "lastEditedBy" : "c8f2b008-60f4-4709-b654-47bc9a7795a2",
        "tags" : [
        ]
      },
      {
        "id" : "bfa14672-a190-4928-996c-87a6832d0cf2",
        "parentId" : "6e4fc14b-9928-4538-94b6-68e2f3202b88",
        "authorId" : "d6269140-fc60-4cd0-9330-75e12d55f632",
        "body" : "From the authors' implementation at https://github.com/bioinf-jku/SNNs/blob/master/selu.py#L22",
        "createdAt" : "2017-06-26T17:35:43Z",
        "updatedAt" : "2017-07-25T15:03:45Z",
        "lastEditedBy" : "d6269140-fc60-4cd0-9330-75e12d55f632",
        "tags" : [
        ]
      },
      {
        "id" : "1b0d2f8b-1368-4542-b856-4c7f4a5eadb9",
        "parentId" : "6e4fc14b-9928-4538-94b6-68e2f3202b88",
        "authorId" : "ca61b595-52c0-46c6-ae7c-aed2b20509ce",
        "body" : "It's easier to debug the gradients setting one or both the magic numbers to 1 since [Elu](https://github.com/tensorflow/tensorflow/blob/86f5ab7474825da756838b34e1b4eac93f5fc68a/tensorflow/core/kernels/relu_op_functor.h#L104) is a special case of Selu.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/compiler/tf2xla/kernels/elu_op.cc",
        "createdAt" : "2017-06-27T06:57:04Z",
        "updatedAt" : "2017-07-25T15:03:45Z",
        "lastEditedBy" : "ca61b595-52c0-46c6-ae7c-aed2b20509ce",
        "tags" : [
        ]
      },
      {
        "id" : "f94ff618-e8a8-4e29-92f3-f9c602e309c8",
        "parentId" : "6e4fc14b-9928-4538-94b6-68e2f3202b88",
        "authorId" : "d6269140-fc60-4cd0-9330-75e12d55f632",
        "body" : "If I set both scale and alpha to 1 and the replace the tests for selu with those for elu, the test passes. ",
        "createdAt" : "2017-06-30T17:55:03Z",
        "updatedAt" : "2017-07-25T15:03:45Z",
        "lastEditedBy" : "d6269140-fc60-4cd0-9330-75e12d55f632",
        "tags" : [
        ]
      }
    ],
    "commit" : "97bafa09a7403db67facb78757ec35eaca1215bb",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +136,140 @@                  typename TTypes<T>::Tensor activations) {\n    // features.constant(?)\n    const auto scale = static_cast<T>(1.0507009873554804934193349852946);\n    const auto scale_alpha = static_cast<T>(1.7580993408473768599402175208123);\n    const auto one = static_cast<T>(1);"
  },
  {
    "id" : "9b4488e8-8a66-4dd8-99c4-ddbe454683f8",
    "prId" : 10818,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/10818#pullrequestreview-48562429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9b62e9e-84a0-4c58-87c2-89b4d01ef10c",
        "parentId" : null,
        "authorId" : "a5a9efe3-f44b-4edc-957c-da8e245e1564",
        "body" : "![selu](https://latex.codecogs.com/gif.latex?selu%28x%29%20%3D%20%5Cbegin%7Bcases%7D%5Clambda%20*%20x%2C%20%26%20%5Ctext%7Bif%7D%5C%20x%20%3E%200%20%5C%5C%20%5Clambda%20*%20%5Calpha%20*%20x%20-%20%5Clambda%20*%20%5Calpha%2C%20%26%20%5Ctext%7Bif%7D%5C%20x%20%5Cleq%200%20%5Cend%7Bcases%7D)\r\nThis should be\r\n\r\n```\r\n(activations < static_cast<T>(0)).select(\r\nÂ             gradients * (activations + scale_alpha * scale), gradients * scale);\r\n```\r\n\r\n",
        "createdAt" : "2017-07-07T08:29:09Z",
        "updatedAt" : "2017-07-25T15:03:45Z",
        "lastEditedBy" : "a5a9efe3-f44b-4edc-957c-da8e245e1564",
        "tags" : [
        ]
      },
      {
        "id" : "7335f3b3-4048-49e4-9755-33121899ad28",
        "parentId" : "f9b62e9e-84a0-4c58-87c2-89b4d01ef10c",
        "authorId" : "d6269140-fc60-4cd0-9330-75e12d55f632",
        "body" : "Could you please explain why this should be `scale_alpha * scale` instead of just `scale_alpha` (`scale_alpha = scale * alpha`)",
        "createdAt" : "2017-07-07T09:22:54Z",
        "updatedAt" : "2017-07-25T15:03:45Z",
        "lastEditedBy" : "d6269140-fc60-4cd0-9330-75e12d55f632",
        "tags" : [
        ]
      },
      {
        "id" : "2268f772-52c3-4ee5-b431-0077995bc443",
        "parentId" : "f9b62e9e-84a0-4c58-87c2-89b4d01ef10c",
        "authorId" : "a5a9efe3-f44b-4edc-957c-da8e245e1564",
        "body" : "ohh is it? Sorry for creating the confusion. I thought `scale_alpha` was for `alpha` in ELU. ",
        "createdAt" : "2017-07-07T09:43:27Z",
        "updatedAt" : "2017-07-25T15:03:45Z",
        "lastEditedBy" : "a5a9efe3-f44b-4edc-957c-da8e245e1564",
        "tags" : [
        ]
      }
    ],
    "commit" : "97bafa09a7403db67facb78757ec35eaca1215bb",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +162,166 @@    backprops.device(d) =\n        (activations < static_cast<T>(0)).select(\n            gradients * (activations + scale_alpha), gradients * scale);\n  }\n};"
  }
]