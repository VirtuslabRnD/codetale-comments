[
  {
    "id" : "840395b1-cf97-4144-aac9-f084ff15591d",
    "prId" : 32407,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/32407#pullrequestreview-289506098",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "537adec2-506b-4955-98ef-b638ac391718",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "Can we add a helper method for these input validation logic as well? They're almost the same for v2/v3/v4 op.",
        "createdAt" : "2019-09-17T19:53:59Z",
        "updatedAt" : "2019-09-17T22:13:09Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb6ba10c6cfd1d117e63170b3a3c3eb5bfeb1c7a",
    "line" : 240,
    "diffHunk" : "@@ -1,1 +684,688 @@\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [num_boxes]"
  },
  {
    "id" : "b986f702-fe23-4e5c-80c1-60fbc512fdcd",
    "prId" : 30893,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/30893#pullrequestreview-285675355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d94cbcf-9268-431b-9a36-11cdb5375e81",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "Please don't change the order. Copybara will do some transforms for these headers.",
        "createdAt" : "2019-07-22T16:31:52Z",
        "updatedAt" : "2019-09-09T22:05:02Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      },
      {
        "id" : "d4009194-194f-4e09-aa8a-dab2442955f7",
        "parentId" : "2d94cbcf-9268-431b-9a36-11cdb5375e81",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "Hi @aaroey These were changed by clang-format, If I revert them clang-format check will complain.",
        "createdAt" : "2019-09-09T17:29:08Z",
        "updatedAt" : "2019-09-09T22:05:02Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      },
      {
        "id" : "81e84811-5ed4-45bc-8446-6e00ef1f1cf5",
        "parentId" : "2d94cbcf-9268-431b-9a36-11cdb5375e81",
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "I see, sorry I didn't know that. Feel free to skip.",
        "createdAt" : "2019-09-09T17:47:18Z",
        "updatedAt" : "2019-09-09T22:05:02Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "178ed66d97ae36e7a0cf8d9d4e9626699f193bfb",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +25,29 @@#include \"tensorflow/core/util/gpu_launch_config.h\"\n#include \"tensorflow/stream_executor/stream_executor.h\"\n#include \"third_party/cub/device/device_radix_sort.cuh\"\n#include \"third_party/cub/device/device_segmented_radix_sort.cuh\"\n#include \"third_party/cub/device/device_select.cuh\""
  },
  {
    "id" : "34de9852-32d2-4a1e-b281-86728eecef08",
    "prId" : 30893,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/30893#pullrequestreview-264911173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff4e928b-3ca2-4ac6-9f7b-2dd1cb8eed06",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "This method is almost the same as NonMaxSuppressionV2GPUOp::Compute() except the additional score input handling logic. Please do something like: add a NonMaxSuppressionGPUOpBase class and encapsulate the common code as helper member functions and reuse them in both ops.",
        "createdAt" : "2019-09-04T04:44:23Z",
        "updatedAt" : "2019-09-09T22:05:02Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "178ed66d97ae36e7a0cf8d9d4e9626699f193bfb",
    "line" : 559,
    "diffHunk" : "@@ -1,1 +599,603 @@      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);"
  },
  {
    "id" : "c464685c-3702-4bca-8dec-e51a87e09e4e",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-241674324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78c465ab-b47e-4e4b-9678-2c7025ddf7f9",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Please comment why we need this.",
        "createdAt" : "2019-05-24T11:06:54Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +159,163 @@      int above_threshold = 0;\n      // Make sure that threads are within valid domain.\n      bool valid = false;\n      // Loop over the next kNmsBoxesPerThread boxes and set corresponding bit\n      // if it is overlapping with current box"
  },
  {
    "id" : "abf6f57a-0a24-4dae-a199-be82e82f1811",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-243571118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c46ddf99-4597-46f3-8ff4-713b91dbe13a",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Use stream_executor::Event",
        "createdAt" : "2019-05-24T11:15:04Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      },
      {
        "id" : "1f27706c-4691-4bea-8a3f-199402d7b692",
        "parentId" : "c46ddf99-4597-46f3-8ff4-713b91dbe13a",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "@csigg I couldn't be able to find any examples of using stream_executor::Event in a similar fashion elsewhere in kernels. Even then, I don't think it is possible to implement the logic by stream_executor::Event since there is no mechanism equivalent to cudaEventSynchronize() implemented in the framework. I can try to spin on event::poll but that would be quite inefficient and would probably hinder the rest of the framework as well due to acquired locks. I would have preferred to use ThenExecute() chaining these but it would require all NMS ops to be converted to AsyncOps as well as a proper threadpool on event manager. Currently all events are executed on single thread and doing work there would block the event infrastructure. I can spawn the work on cpu device thread pool on the event callback but I am not sure if this level of complexity is justified.\r\nHow would you propose I would use stream_executor::Event, it is possible that I am missing something obvious.\r\n",
        "createdAt" : "2019-05-29T23:55:34Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 285,
    "diffHunk" : "@@ -1,1 +283,287 @@  int num_to_copy = std::min(kNmsChunkSize, num_boxes);\n  cudaEvent_t copy_done;\n  cudaEventCreate(&copy_done);\n  device.memcpyDeviceToHost(&h_delete_mask[0], &d_delete_mask[0],\n                            num_to_copy * bit_mask_len * sizeof(int));"
  },
  {
    "id" : "7de97df3-84ae-4e97-a415-34f018c38779",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-241674324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a7ee3a0-60db-4ec4-95e7-95a53ac94ccf",
        "parentId" : null,
        "authorId" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "body" : "Add comment what the implementation does. Inline (like, above sections below) is also fine.",
        "createdAt" : "2019-05-24T11:17:29Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1223b96d-efd5-4254-9e4f-f359308f8df2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 343,
    "diffHunk" : "@@ -1,1 +341,345 @@      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);"
  },
  {
    "id" : "5e55b944-0530-4529-8351-99c01937e2aa",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-243628855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad3d0fab-96f4-409f-ab20-e7bef9009528",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "It seems there is no reference to the v2 op: https://github.com/tensorflow/tensorflow/search?q=non_max_suppression_v2&unscoped_q=non_max_suppression_v2. So please implement the V4 op instead.",
        "createdAt" : "2019-05-29T02:00:45Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      },
      {
        "id" : "e140d025-06a0-449c-9fa7-1dfb7cfc77b1",
        "parentId" : "ad3d0fab-96f4-409f-ab20-e7bef9009528",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "That will take time. I will add v3 and v4 in subsequent PRs. Regardless, NMSKernel and NmsGpu are required for #28754 and need to stay as they are. V2 is the closest user of current kernels thus easier to add.",
        "createdAt" : "2019-05-29T18:56:27Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      },
      {
        "id" : "5fb09ad3-8718-4f72-b929-5bc2031544b5",
        "parentId" : "ad3d0fab-96f4-409f-ab20-e7bef9009528",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "@aaroey Also all these https://github.com/tensorflow/models/search?p=1&q=non_max_suppression&unscoped_q=non_max_suppression can be switched to use v2 op on GPU with a simple regex replace (and should IMHO). ",
        "createdAt" : "2019-05-29T20:01:31Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      },
      {
        "id" : "021e237e-6de3-4332-8eb8-082ff2fa1026",
        "parentId" : "ad3d0fab-96f4-409f-ab20-e7bef9009528",
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "They actually all use v3 ops: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L2582. So I would say this implementation probably won't be useful if it's for v2 only.",
        "createdAt" : "2019-05-29T20:23:23Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      },
      {
        "id" : "2b7cb7e0-b493-4484-a72a-ab6e2d7daf6e",
        "parentId" : "ad3d0fab-96f4-409f-ab20-e7bef9009528",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "But isn't V3 with score_threshold=-inf and pad_to_max_output_size=False == V2? I don't see score_threshold and pad_to_max_output being passed around in most places. Just adding _v2 to most of these should give a significant perf boost. Only the one in post_processing.py is making use of V3, the rest can interchangeably use V2 or V3 since V3 with default last 2 arguments is equivalent to v2.",
        "createdAt" : "2019-05-29T20:47:15Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      },
      {
        "id" : "e515b6e5-5b6a-4f1c-8121-abe01f5ed96c",
        "parentId" : "ad3d0fab-96f4-409f-ab20-e7bef9009528",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "As we discussed in person, I am keeping this and will implement other ops in a later PR.",
        "createdAt" : "2019-05-30T05:14:32Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 485,
    "diffHunk" : "@@ -1,1 +483,487 @@\nREGISTER_KERNEL_BUILDER(\n    Name(\"NonMaxSuppressionV2\").TypeConstraint<float>(\"T\").Device(DEVICE_GPU),\n    NonMaxSuppressionV2GPUOp);\n"
  },
  {
    "id" : "41e9ad2f-a82a-4054-8f1c-a302b8057023",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-243006869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62b61e42-29ea-4c62-9582-102d85e29b4d",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "s/thresh/iou_threshold",
        "createdAt" : "2019-05-29T15:41:57Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 223,
    "diffHunk" : "@@ -1,1 +221,225 @@\ntensorflow::Status NmsGpu(const float* d_sorted_boxes_float_ptr,\n                          const int num_boxes, const float thresh,\n                          int* d_selected_indices, int* h_nkeep,\n                          OpKernelContext* context, bool flip_boxes) {"
  },
  {
    "id" : "c45a034e-d841-42af-b238-dd7dc16d208b",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-243006869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96dd908c-5218-4548-a201-c665e53e84d6",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "Will moving this out of the loop speed things up?",
        "createdAt" : "2019-05-29T17:36:11Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 170,
    "diffHunk" : "@@ -1,1 +168,172 @@        valid = true;\n        Box j_box = d_desc_sorted_boxes[j];\n        const Box i_box = shared_i_boxes[threadIdx.x];\n        Flipped<flip_box>(j_box);\n        if (OverThreshold(&i_box, &j_box, shared_i_areas[threadIdx.x],"
  },
  {
    "id" : "506c4dc5-bd3d-4102-bf88-bdb30a74d484",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-243623913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91796b6e-f5d6-46d9-bbd8-fc3c75922c04",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "Is this true only for particular architecture? Shall we make NMS_CHUNK_SIZE architecture specific? Please comment.",
        "createdAt" : "2019-05-29T18:06:08Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      },
      {
        "id" : "c9ca4329-02ec-4c65-97d6-675992414b38",
        "parentId" : "91796b6e-f5d6-46d9-bbd8-fc3c75922c04",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "It is more related to PCI-E mem-copy and CPU processing time. So it is somewhat flat nowadays. I wouldn't attack NMS_CHUNK_SIZE first if I am to optimize this code. This is first reference implementation and there are already better kernels. However, considering that a GPU NMS implementation is missing, this would bring significant speed up already. I would prefer to have a working implementation at 90% efficiency now instead of waiting for a 95% efficient version next year. We can improve the kernels at a later time.",
        "createdAt" : "2019-05-30T04:45:48Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 282,
    "diffHunk" : "@@ -1,1 +280,284 @@  TF_RETURN_IF_CUDA_ERROR(cudaGetLastError());\n  // Overlapping CPU computes and D2H memcpy\n  // both take about the same time\n  int num_to_copy = std::min(kNmsChunkSize, num_boxes);\n  cudaEvent_t copy_done;"
  },
  {
    "id" : "0c21fef6-ea17-4b94-8ba5-9c75c2fd1dd6",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-243929481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d28739d-1524-47f0-b19f-843808d4726b",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "It look like we only need `num_to_copy*2` elements in this buffer, i.e. a circular array. Please either fix it or add comment for the future reference/optimization.",
        "createdAt" : "2019-05-30T16:30:25Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      },
      {
        "id" : "7bec8b09-cfe5-41bc-ba2f-ee764161c783",
        "parentId" : "7d28739d-1524-47f0-b19f-843808d4726b",
        "authorId" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "body" : "I will do that. But very skeptical about any benefits such an optimization would bring. Typical ranges of num_boxes in the wild is around 2k to 6k, that would make that host buffer in the order of 6k x 6k/8=4.5MB. for a chunk size of 2000, circular buffer you suggested is 4kx6k/8=3MB . I believe we can save much more memory through other means, not necessarily in this kernel.",
        "createdAt" : "2019-05-30T17:38:57Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "8f9cf5d6-8597-4d0e-8a0a-4fec1fb0f577",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 254,
    "diffHunk" : "@@ -1,1 +252,256 @@  // using it as a ring buffer. However savings should be a few MB .\n  TF_RETURN_IF_ERROR(context->allocate_temp(DataType::DT_INT32,\n                                            TensorShape({max_nms_mask_size}),\n                                            &h_nms_mask, alloc_attr));\n"
  },
  {
    "id" : "509f17dd-bf73-4a35-ac63-27e223eeb06b",
    "prId" : 28745,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/28745#pullrequestreview-245150548",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d60d0665-2e04-4fc1-9909-05589d01fd23",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "Existing comment is about implementation details. Please add a higher level description like \"Compute a bit mask `d_delete_mask` where bit `i*bit_mask_len*32+j` is set iff box i and box j has IoU score larger than specific threshold.\"",
        "createdAt" : "2019-06-03T22:19:25Z",
        "updatedAt" : "2019-06-04T00:28:06Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f78ad57cc3c87fb4489c726ddb1a0f6fa09f7d1",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +121,125 @@// x1<x2 and y1<y2.\n\n// Starting from highes scoring box, mark any box which has IoU>threshold with\n// given box. Each thread processes a kNmsBoxesPerThread boxes per stride, and\n// each box has bitmask of overlaps of length bit_mask_len."
  }
]