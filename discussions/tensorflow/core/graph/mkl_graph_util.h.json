[
  {
    "id" : "b151a02f-b5de-49ed-9e65-8ab6d5a53f2d",
    "prId" : 40455,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40455#pullrequestreview-430287434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01e5c695-a0ba-43dd-87b4-9e1a52ecbde7",
        "parentId" : null,
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "`isTypeAllowed` is already `false`. So this line is not necessary. Would you like to remove it? (I'm also fine leaving it in to save another round of testing, given that we are close to the branch cut deadline.)",
        "createdAt" : "2020-06-14T20:20:11Z",
        "updatedAt" : "2020-06-14T20:21:14Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "35d6ee5a-f95a-4d5c-8df2-1560116dbc4c",
        "parentId" : "01e5c695-a0ba-43dd-87b4-9e1a52ecbde7",
        "authorId" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "body" : "Yes, was double qualifying just to be sure this time around. :). Let us move on this time in the interest of getting the CI working internally and for the deadline. Will make a note internally for refactoring in future.",
        "createdAt" : "2020-06-15T02:26:26Z",
        "updatedAt" : "2020-06-15T02:26:26Z",
        "lastEditedBy" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "tags" : [
        ]
      },
      {
        "id" : "31c5134c-0278-44dc-97c8-2b269da1db7b",
        "parentId" : "01e5c695-a0ba-43dd-87b4-9e1a52ecbde7",
        "authorId" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "body" : "Thank you for looking into this very soon! @penpornk ",
        "createdAt" : "2020-06-15T02:27:06Z",
        "updatedAt" : "2020-06-15T02:27:06Z",
        "lastEditedBy" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "tags" : [
        ]
      }
    ],
    "commit" : "eabb1453f2efc62a648096c21b2ed993079d8c51",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +233,237 @@          // fall back to Eigen implementation otherwise.\n          BF16UnsupportedWarning();\n          isTypeAllowed = false;\n        }\n      }"
  },
  {
    "id" : "9b637040-7fe7-4b0e-95d5-3ee4930979eb",
    "prId" : 40212,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40212#pullrequestreview-426465304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce59dab4-2740-4343-a774-3dbae36a0747",
        "parentId" : null,
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "If this is specific to `bfloat16` type, why do we have to take the type as an input parameter? It could just be `CheckBfloat16Support()`.\r\n\r\nNit: Also, I think `HasBfloat16Support()` might make it a bit clearer that the function is returning `true` if there is `bfloat16` support.",
        "createdAt" : "2020-06-08T00:22:34Z",
        "updatedAt" : "2020-06-08T18:15:16Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "82a22a8e-bb9b-49b3-be32-d6a9aa23c3af",
        "parentId" : "ce59dab4-2740-4343-a774-3dbae36a0747",
        "authorId" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "body" : "Done changed to HasBfloat16Support(). Thank you for the suggestion.\r\n\r\nAs for as the Type extra check, we wanted the message to be print only when running the BFloat models and not when running the FP32 op models even when the compiler flag is true. (if the model still crashes, the problem is something different). Extra qualifications ensure that message is printed only when intended.",
        "createdAt" : "2020-06-08T15:44:36Z",
        "updatedAt" : "2020-06-08T18:15:16Z",
        "lastEditedBy" : "3cbb7033-4fd4-4c54-9671-d4408e7451f4",
        "tags" : [
        ]
      },
      {
        "id" : "5af5f075-7513-4e48-9ab1-d10c2b8c438c",
        "parentId" : "ce59dab4-2740-4343-a774-3dbae36a0747",
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "I see. This is different from `HasBfloat16Support` that I imagine then (should return true if AVX512F regardless of the datatype we are focusing on, because it's about the machine). Maybe we should keep it `CheckBfloat16Support`. Please see my other comment.",
        "createdAt" : "2020-06-08T17:34:10Z",
        "updatedAt" : "2020-06-08T18:15:16Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      }
    ],
    "commit" : "d50e348ab34abf44c42c1eee67f295f0c635cc7b",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +127,131 @@\n#ifdef ENABLE_INTEL_MKL_BFLOAT16\nstatic inline bool CheckBfloat16Support(DataType T) {\n  static absl::once_flag cpu_bfloat16_warn_once_flag;\n  // Restrict bfloat16 ops to platforms with at least AVX512 support, fall back"
  },
  {
    "id" : "2819211d-84d7-4e07-a5a6-ff496e9fe1c1",
    "prId" : 30401,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/30401#pullrequestreview-269254077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e849a1af-5397-4a9d-8513-3469788f3c19",
        "parentId" : null,
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "Can we add `eager_mode` (or `is_eager`) as an op attribute instead? This way we don't need to rename the op just to pass `eager_mode` through template parameter.",
        "createdAt" : "2019-07-26T16:48:16Z",
        "updatedAt" : "2019-08-02T14:12:12Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "ace46598-1638-49d5-b537-7fc267690d68",
        "parentId" : "e849a1af-5397-4a9d-8513-3469788f3c19",
        "authorId" : "bd0099d1-d419-490b-9eec-ebb5c3552393",
        "body" : "@penpornk I have put a comment right before the declaration of \"_MklEager\", for refactoring purpose in near future (original developer is on a long vacation). I have included your suggestion in the comment. ",
        "createdAt" : "2019-07-26T21:03:47Z",
        "updatedAt" : "2019-08-02T14:12:12Z",
        "lastEditedBy" : "bd0099d1-d419-490b-9eec-ebb5c3552393",
        "tags" : [
        ]
      },
      {
        "id" : "4f52727c-f3c2-43bb-a4c2-2a42c86bdd7b",
        "parentId" : "e849a1af-5397-4a9d-8513-3469788f3c19",
        "authorId" : "bd0099d1-d419-490b-9eec-ebb5c3552393",
        "body" : "I guess that the developer (Mahmoud) wanted to rename the op in order to make sure that the renaming only occurs in pure eager mode  (versus non-eager-mode, all mkl op rewriting is done  thru MKL layout pass (graph/mkl_layout_pass) which is only triggered at the state of\r\nPOST_PARTITIONING (which never reach in pure eager mode!).\r\n\r\nSo, for now, I would recommend that we do refactoring one the original developer is back)\r\nThanks",
        "createdAt" : "2019-07-29T18:59:04Z",
        "updatedAt" : "2019-08-02T14:12:12Z",
        "lastEditedBy" : "bd0099d1-d419-490b-9eec-ebb5c3552393",
        "tags" : [
        ]
      },
      {
        "id" : "82482fbd-d424-4be2-9831-21369d2b4799",
        "parentId" : "e849a1af-5397-4a9d-8513-3469788f3c19",
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "But why exactly do we need to rename the op, though? Making `eager_mode` an attribute is enough to tell the difference, and is much cleaner (no need to spawn new `_MklEager*` ops every time we add a rewrite rule).\r\n",
        "createdAt" : "2019-07-29T20:27:19Z",
        "updatedAt" : "2019-08-02T14:12:12Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "fd2bb403-82c2-4e32-9e12-d90853caeddf",
        "parentId" : "e849a1af-5397-4a9d-8513-3469788f3c19",
        "authorId" : "bd0099d1-d419-490b-9eec-ebb5c3552393",
        "body" : "done with change",
        "createdAt" : "2019-07-31T20:11:10Z",
        "updatedAt" : "2019-08-02T14:12:12Z",
        "lastEditedBy" : "bd0099d1-d419-490b-9eec-ebb5c3552393",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cd322489613257372cb3f20eb029a94f7b7a785",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +109,113 @@// This way we don't need to rename the op just to pass eager_mode\n// through template parameter.\nstatic const char* const kMklEagerOpPrefix = \"_MklEager\";\n\n// Get the name of Mkl op from original TensorFlow op"
  },
  {
    "id" : "3e6496fa-08a1-4e4e-ab0e-fae2413ca460",
    "prId" : 29021,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/29021#pullrequestreview-246233082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "657f3240-ef19-4988-be02-cd8d57f21a48",
        "parentId" : null,
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "It seems `kMklLayoutDependantOpLabel`, `kMklNameChangeOpLabel`, and `kMklQuantizedOpLabel` aren't used. Please remove them.",
        "createdAt" : "2019-05-31T07:15:55Z",
        "updatedAt" : "2019-06-25T19:18:33Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      },
      {
        "id" : "bf2c4bf2-4190-4b89-88e8-d93321a1c26e",
        "parentId" : "657f3240-ef19-4988-be02-cd8d57f21a48",
        "authorId" : "0713c471-eedd-428f-a49d-0dd0d38ea8e2",
        "body" : "no, these three labels are widely used in other files. you can search for it.",
        "createdAt" : "2019-06-04T18:43:03Z",
        "updatedAt" : "2019-06-25T19:18:33Z",
        "lastEditedBy" : "0713c471-eedd-428f-a49d-0dd0d38ea8e2",
        "tags" : [
        ]
      },
      {
        "id" : "1445bb47-3eef-40db-91ba-8a09412d0de9",
        "parentId" : "657f3240-ef19-4988-be02-cd8d57f21a48",
        "authorId" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "body" : "Thank you for clarifying!",
        "createdAt" : "2019-06-05T20:14:53Z",
        "updatedAt" : "2019-06-25T19:18:33Z",
        "lastEditedBy" : "72acf7f0-6473-4b57-a71f-2a44410818c5",
        "tags" : [
        ]
      }
    ],
    "commit" : "8883b4bffa1033c1ce4a66b1913c767d41f2f588",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +101,105 @@static const char* kMklNameChangeOpLabelPattern = \"label='MklNameChangeOp'\";\nstatic const char* kMklQuantizedOpLabel = \"QuantizedMklOp\";\nstatic const char* kMklQuantizedOpLabelPattern = \"label='QuantizedMklOp'\";\n\n// Prefix that we add to Tensorflow op name to construct Mkl op name."
  }
]