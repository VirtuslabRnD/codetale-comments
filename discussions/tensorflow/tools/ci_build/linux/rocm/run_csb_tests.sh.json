[
  {
    "id" : "49f817b7-a52a-4886-b588-8ac9b295c099",
    "prId" : 31454,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/31454#pullrequestreview-272764131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c7896cf-3e29-4ce5-ad64-e4481ee1a3df",
        "parentId" : null,
        "authorId" : "28fabc34-a1ac-4aba-988b-9c9e03232122",
        "body" : "JFYI, this is not testing XLA at all.\r\nin case you also want to test XLA.",
        "createdAt" : "2019-08-08T18:16:24Z",
        "updatedAt" : "2019-08-08T18:16:24Z",
        "lastEditedBy" : "28fabc34-a1ac-4aba-988b-9c9e03232122",
        "tags" : [
        ]
      },
      {
        "id" : "e9983f90-7dd2-4034-acfa-43646908ae64",
        "parentId" : "2c7896cf-3e29-4ce5-ad64-e4481ee1a3df",
        "authorId" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "body" : ">JFYI, this is not testing XLA at all.\r\n\r\nthat is intentional. ROCm support for XLA is still in the process of being upstreamed. \r\n\r\nXLA unit tests will not pass until the upstreaming of ROCm support is complete, and hence leaving those out for now. Will enable them once the upstreaming process is complete.",
        "createdAt" : "2019-08-08T18:38:15Z",
        "updatedAt" : "2019-08-08T18:38:15Z",
        "lastEditedBy" : "80b3505d-f76d-48ab-b8fe-fe7925204b98",
        "tags" : [
        ]
      }
    ],
    "commit" : "a485a743ddce9ca83b1db71c2ccd39d7d3d780a9",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +33,37 @@export TF_GPU_COUNT=${N_GPUS}\n\nyes \"\" | $PYTHON_BIN_PATH configure.py\n\n# Run bazel test command. Double test timeouts to avoid flakes."
  }
]