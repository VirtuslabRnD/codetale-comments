[
  {
    "id" : "e98cc9cd-6d58-4031-b091-c00490690969",
    "prId" : 21212,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/21212#pullrequestreview-141374704",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b231767-ebce-451d-ab1d-44203ca2070f",
        "parentId" : null,
        "authorId" : "af7f6620-5090-4aa8-89df-341e746603ad",
        "body" : "Same here. I think this should remain the same.",
        "createdAt" : "2018-07-29T14:08:06Z",
        "updatedAt" : "2018-07-30T15:27:42Z",
        "lastEditedBy" : "af7f6620-5090-4aa8-89df-341e746603ad",
        "tags" : [
        ]
      },
      {
        "id" : "64f88726-d3bf-4ce3-ab04-24449f0face8",
        "parentId" : "4b231767-ebce-451d-ab1d-44203ca2070f",
        "authorId" : "a6aaebd2-96d2-422a-8146-27055092c22d",
        "body" : "Ya `batch_loss` is the same thing as `loss/int(targ.shape[1])`. I explained it in more detail in the above comment!",
        "createdAt" : "2018-07-30T02:10:19Z",
        "updatedAt" : "2018-07-30T15:27:42Z",
        "lastEditedBy" : "a6aaebd2-96d2-422a-8146-27055092c22d",
        "tags" : [
        ]
      }
    ],
    "commit" : "4312fbefce84436c0ef987ae79d9529d84649b3d",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +692,696 @@        \"            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\\n\",\n        \"                                                         batch,\\n\",\n        \"                                                         batch_loss.numpy()))\\n\",\n        \"    \\n\",\n        \"    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\\n\","
  },
  {
    "id" : "b33b084f-4fbc-4ab2-93e9-00c72805df6e",
    "prId" : 20084,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/20084#pullrequestreview-129714751",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5677e1c7-3b3d-4680-9365-ea7fa97dc4ff",
        "parentId" : null,
        "authorId" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "body" : "I think you also need to worry here about sequences of different lengths, by masking the loss",
        "createdAt" : "2018-06-18T16:17:24Z",
        "updatedAt" : "2018-06-18T19:59:58Z",
        "lastEditedBy" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "tags" : [
        ]
      },
      {
        "id" : "ba510700-594a-4e8f-bbed-36e91da07f71",
        "parentId" : "5677e1c7-3b3d-4680-9365-ea7fa97dc4ff",
        "authorId" : "af7f6620-5090-4aa8-89df-341e746603ad",
        "body" : "Done",
        "createdAt" : "2018-06-18T19:59:06Z",
        "updatedAt" : "2018-06-18T19:59:58Z",
        "lastEditedBy" : "af7f6620-5090-4aa8-89df-341e746603ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce74f7362ee5161976f7c30777b88637be1d02b5",
    "line" : 745,
    "diffHunk" : "@@ -1,1 +743,747 @@        \"                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\\n\",\n        \"                \\n\",\n        \"                loss += loss_function(targ[:, t], predictions)\\n\",\n        \"                \\n\",\n        \"                # using teacher forcing\\n\","
  }
]