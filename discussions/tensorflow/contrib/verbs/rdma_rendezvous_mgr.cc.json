[
  {
    "id" : "883eec2e-a7fd-4dfb-bbb0-c9dbec3f83ca",
    "prId" : 10531,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/10531#pullrequestreview-44461469",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "909c8fdb-cf6d-4e2e-8acb-e8c5468c855b",
        "parentId" : null,
        "authorId" : "1f91e1ca-cefb-4416-8a41-76785bbd0b7f",
        "body" : "Does std::move work properly for GPUTensor?",
        "createdAt" : "2017-06-15T20:31:51Z",
        "updatedAt" : "2017-06-16T03:48:45Z",
        "lastEditedBy" : "1f91e1ca-cefb-4416-8a41-76785bbd0b7f",
        "tags" : [
        ]
      },
      {
        "id" : "efa4b3f3-db5f-453f-9d56-5c05ab6ab692",
        "parentId" : "909c8fdb-cf6d-4e2e-8acb-e8c5468c855b",
        "authorId" : "d212ad43-64a7-4985-ba0b-25a7c8069b62",
        "body" : "std::move should be ok for GPU tensor, see the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.h#L692). Actually the [standard assignment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.h#L161) also just an pointer assignment.",
        "createdAt" : "2017-06-16T02:17:01Z",
        "updatedAt" : "2017-06-16T03:48:45Z",
        "lastEditedBy" : "d212ad43-64a7-4985-ba0b-25a7c8069b62",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ca692ef01038c05b02c87236eaa405159dae6ed",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +117,121 @@                                                dst_dev, &gpu_copy);\n          CHECK(s.ok()) << \"copy tensor to gpu sync\";\n          val = std::move(gpu_copy);\n        } else {\n          AllocatorAttributes host_alloc_attrs;"
  },
  {
    "id" : "90151cec-1dec-4616-baa1-0da6e1e1fc93",
    "prId" : 10531,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/10531#pullrequestreview-44416439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ed7998d-eeb1-4af9-a9b2-118a281b7084",
        "parentId" : null,
        "authorId" : "1f91e1ca-cefb-4416-8a41-76785bbd0b7f",
        "body" : "The chain of copying can be further simplified. Currently, you copy twice here (assuming move works as expected):\r\ninput --> (cpu) copy --> gpu_copy -->(move)--> val,\r\nIf you want to be aggressive, you can do input-->gpu_copy directly, skipping the cpu_copy. In one of our early versions (not released), we had that. But it required low-level functions similar to cudaMemcpy. It was too complex, so in the final public version, we chose the protobuf route for simplicity, but sacrificed performance. Ditto on the sender side (rdma.cc). ",
        "createdAt" : "2017-06-15T20:48:44Z",
        "updatedAt" : "2017-06-16T03:48:45Z",
        "lastEditedBy" : "1f91e1ca-cefb-4416-8a41-76785bbd0b7f",
        "tags" : [
        ]
      },
      {
        "id" : "20345e1f-33d1-47af-91de-e6fac5f91650",
        "parentId" : "0ed7998d-eeb1-4af9-a9b2-118a281b7084",
        "authorId" : "1f91e1ca-cefb-4416-8a41-76785bbd0b7f",
        "body" : "@llhe Thanks for your contribution. The patch looks good except that I am not sure whether std::move works for GPU tensors as expected. Besides speed, can you verify the result is correct? @2sin18 had issues about the convergence while running with GPU. I suspect it has something to do either \"copy\" or \"move\" operations.",
        "createdAt" : "2017-06-15T20:53:27Z",
        "updatedAt" : "2017-06-16T03:48:45Z",
        "lastEditedBy" : "1f91e1ca-cefb-4416-8a41-76785bbd0b7f",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ca692ef01038c05b02c87236eaa405159dae6ed",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +117,121 @@                                                dst_dev, &gpu_copy);\n          CHECK(s.ok()) << \"copy tensor to gpu sync\";\n          val = std::move(gpu_copy);\n        } else {\n          AllocatorAttributes host_alloc_attrs;"
  }
]