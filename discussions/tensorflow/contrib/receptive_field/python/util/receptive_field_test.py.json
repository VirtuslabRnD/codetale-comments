[
  {
    "id" : "940cdfd2-441b-4ac2-8a04-9cb5c1d1fb91",
    "prId" : 13239,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/13239#pullrequestreview-68017647",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0aa461b1-ff41-4a21-a7cf-84f433268201",
        "parentId" : null,
        "authorId" : "e7c664d3-af1d-41d4-8b07-16d0816514da",
        "body" : "In this case, I believe the same behavior could be achieved by adding a new op \"RandomUniform\" to \"_UNCHANGED_RF_LAYER_OPS\". Correct?\r\nThis is just to check my understanding, I am not suggesting you to do this. This is a great test for the stop_propagation option.",
        "createdAt" : "2017-10-03T16:33:43Z",
        "updatedAt" : "2017-10-08T15:58:35Z",
        "lastEditedBy" : "e7c664d3-af1d-41d4-8b07-16d0816514da",
        "tags" : [
        ]
      },
      {
        "id" : "01589500-8e4c-435a-9626-9a6c5da801c6",
        "parentId" : "0aa461b1-ff41-4a21-a7cf-84f433268201",
        "authorId" : "a709924c-9e01-4bdb-b2b5-ef2f20579a3e",
        "body" : "Almost. The implementation of `dropout` uses the shape of the input features to generate uniform random numbers of matching shape. That introduces a dependency of a `RandomUniform` operation on the input features but there is no meaningful way to propagate the receptive field. So the propagation of the receptive field needs to be stopped rather than passed through unchanged.",
        "createdAt" : "2017-10-04T10:03:41Z",
        "updatedAt" : "2017-10-08T15:58:35Z",
        "lastEditedBy" : "a709924c-9e01-4bdb-b2b5-ef2f20579a3e",
        "tags" : [
        ]
      },
      {
        "id" : "4e94d663-f584-47be-b75c-79a083d027c1",
        "parentId" : "0aa461b1-ff41-4a21-a7cf-84f433268201",
        "authorId" : "e7c664d3-af1d-41d4-8b07-16d0816514da",
        "body" : "hmmm... if we added the \"RandomUniform\" op to \"_UNCHANGED_RF_LAYER_OPS\", it would still work I guess? Maybe I am missing something here.\r\nIf I understand correctly, based on what you said and based on my understanding of dropout, the graph with Dropout will have a branch to generate random values which will be used to elementwise multiply the features, deactivating some of them. So if we simply propagated back the RF through this branch without changing it, the computation would still work?",
        "createdAt" : "2017-10-04T15:47:37Z",
        "updatedAt" : "2017-10-08T15:58:35Z",
        "lastEditedBy" : "e7c664d3-af1d-41d4-8b07-16d0816514da",
        "tags" : [
        ]
      },
      {
        "id" : "031f1e9a-0e25-4a33-9cfb-1c510865101d",
        "parentId" : "0aa461b1-ff41-4a21-a7cf-84f433268201",
        "authorId" : "a709924c-9e01-4bdb-b2b5-ef2f20579a3e",
        "body" : "Yes, that's right. I confused myself.",
        "createdAt" : "2017-10-08T15:52:52Z",
        "updatedAt" : "2017-10-08T15:58:35Z",
        "lastEditedBy" : "a709924c-9e01-4bdb-b2b5-ef2f20579a3e",
        "tags" : [
        ]
      },
      {
        "id" : "01f268da-c1ca-43a7-94ce-2f4bd6af3fb0",
        "parentId" : "0aa461b1-ff41-4a21-a7cf-84f433268201",
        "authorId" : "e7c664d3-af1d-41d4-8b07-16d0816514da",
        "body" : "thanks, glad I understand this now.",
        "createdAt" : "2017-10-09T15:15:00Z",
        "updatedAt" : "2017-10-09T15:15:01Z",
        "lastEditedBy" : "e7c664d3-af1d-41d4-8b07-16d0816514da",
        "tags" : [
        ]
      }
    ],
    "commit" : "302207948bbc186a7d99813869313fefbfe8f256",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +257,261 @@         receptive_field.compute_receptive_field_from_graph_def(\n             graph_def, input_node, output_node,\n             ['Dropout/dropout/random_uniform']))\n    self.assertEqual(receptive_field_x, 3)\n    self.assertEqual(receptive_field_y, 3)"
  }
]