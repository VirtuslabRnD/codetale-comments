[
  {
    "id" : "7537cbed-5225-4792-9340-24c3232e5c60",
    "prId" : 24812,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/24812#pullrequestreview-206030521",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac7ae578-8255-49d4-a856-cac4e58c3eb1",
        "parentId" : null,
        "authorId" : "ed1418d4-e022-4541-929d-990e12851735",
        "body" : "nitpick: it's a minor suggestion, up to you.\r\nNo need to create both nodes in the graph\r\n```py\r\ninputs_np = ...\r\nif dynamic_shape_input:\r\n  inputs = get_variable(...)\r\nelse:\r\n  inputs = placeholder(...)\r\n```\r\n\r\nIn fact, I think maybe placeholder alone is good -- if it works for dynamic shape input, it must work for static shape.\r\n",
        "createdAt" : "2019-02-20T21:59:03Z",
        "updatedAt" : "2019-03-07T04:46:22Z",
        "lastEditedBy" : "ed1418d4-e022-4541-929d-990e12851735",
        "tags" : [
        ]
      },
      {
        "id" : "87235076-1998-4471-99fd-e26754ca75d7",
        "parentId" : "ac7ae578-8255-49d4-a856-cac4e58c3eb1",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "I thought this previously. But actually, I think we need to keep both nodes. I tried the dynamic inputs for rnn.dynamic_rnn() but it doesn't work. (I guess they cannot handle the loops dynamically?).\r\n\r\nI can remove the static input tests if that is OK for you.",
        "createdAt" : "2019-02-20T22:17:50Z",
        "updatedAt" : "2019-03-07T04:46:22Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      }
    ],
    "commit" : "40df70df7450a0d83645455d9db7d5d332ea192b",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +89,93 @@  shape = ([time, batch_size, input_size] if time_major else\n           [batch_size, time, input_size])\n  inputs_np = np.random.rand(*shape).astype(dtype.as_numpy_dtype)\n  inputs_static = variable_scope.get_variable(\n      \"inputs\","
  },
  {
    "id" : "af47cb8b-c3fc-4d62-8151-790c00a8f666",
    "prId" : 23588,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/23588#pullrequestreview-181913038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30c28c62-892c-4d8a-955c-0a4dcc71883b",
        "parentId" : null,
        "authorId" : "ed1418d4-e022-4541-929d-990e12851735",
        "body" : "With variable sequence length, we should only compare the \"valid\" portion of outputs, states and gradients between cudnn and tf-rnn.\r\nThe fact this still compares final output, states as if the sequence_length doesn't exist and still passes, worries me.",
        "createdAt" : "2018-12-03T23:32:09Z",
        "updatedAt" : "2018-12-18T03:44:04Z",
        "lastEditedBy" : "ed1418d4-e022-4541-929d-990e12851735",
        "tags" : [
        ]
      },
      {
        "id" : "bdc40922-fd32-4c76-98d2-909df464100f",
        "parentId" : "30c28c62-892c-4d8a-955c-0a4dcc71883b",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "I think the reason it got passed is because both cudnn and tf-rnn zero-outs the portion exceeds the max_seq_length. Can we rely on this?",
        "createdAt" : "2018-12-05T05:58:05Z",
        "updatedAt" : "2018-12-18T03:44:04Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      },
      {
        "id" : "4aa491b5-fb89-4b45-b087-188611513a82",
        "parentId" : "30c28c62-892c-4d8a-955c-0a4dcc71883b",
        "authorId" : "ed1418d4-e022-4541-929d-990e12851735",
        "body" : "Got it. I'd prefer to not rely on it and have more robust tests.",
        "createdAt" : "2018-12-05T18:53:56Z",
        "updatedAt" : "2018-12-18T03:44:04Z",
        "lastEditedBy" : "ed1418d4-e022-4541-929d-990e12851735",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf46fa4c5225e853e602b0e18951884b08729163",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +345,349 @@           variable_seq_lengths=variable_seq_lengths)\n\n      self.assertAllClose(outputs, cu_outputs, rtol=rtol, atol=atol)\n      for s, cu_s in zip(state_tuple, cu_state_tuple):\n        self.assertAllClose(s, cu_s, rtol=rtol, atol=atol)"
  }
]