[
  {
    "id" : "4c94ad03-b697-41c3-946f-9b4f2c0b0073",
    "prId" : 17395,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/17395#pullrequestreview-107876270",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6355602-6591-4f8b-8ac8-18d518a0096c",
        "parentId" : null,
        "authorId" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "body" : "Is this the only difference between adamax and adam? I don't think so, right?",
        "createdAt" : "2018-03-20T16:23:39Z",
        "updatedAt" : "2018-04-14T10:40:04Z",
        "lastEditedBy" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "tags" : [
        ]
      },
      {
        "id" : "b651ebeb-2719-498f-aa83-3a81b977a91c",
        "parentId" : "a6355602-6591-4f8b-8ac8-18d518a0096c",
        "authorId" : "576d94d0-8049-4273-a3d0-f373713532c5",
        "body" : "You're right. Adamax is quite different from Adam. \r\n\r\nAdam:\r\n![image](https://user-images.githubusercontent.com/1112263/38004373-8d5e1fec-326e-11e8-9648-bebb5dab70d8.png)\r\n\r\nAdamax:\r\n![image](https://user-images.githubusercontent.com/1112263/38004389-a279a2b6-326e-11e8-9a34-f512c588aeee.png)",
        "createdAt" : "2018-03-28T01:59:28Z",
        "updatedAt" : "2018-04-14T10:40:04Z",
        "lastEditedBy" : "576d94d0-8049-4273-a3d0-f373713532c5",
        "tags" : [
        ]
      },
      {
        "id" : "fadd88c3-8b81-4389-8218-5eb7ae263f00",
        "parentId" : "a6355602-6591-4f8b-8ac8-18d518a0096c",
        "authorId" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "body" : "So can you add this to the comment?",
        "createdAt" : "2018-03-28T15:56:07Z",
        "updatedAt" : "2018-04-14T10:40:04Z",
        "lastEditedBy" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "tags" : [
        ]
      },
      {
        "id" : "43040933-3528-47e8-9d62-5329fb6b579d",
        "parentId" : "a6355602-6591-4f8b-8ac8-18d518a0096c",
        "authorId" : "576d94d0-8049-4273-a3d0-f373713532c5",
        "body" : "I think the formula of AdaMax has been added in its docstring.",
        "createdAt" : "2018-03-28T22:42:48Z",
        "updatedAt" : "2018-04-14T10:40:04Z",
        "lastEditedBy" : "576d94d0-8049-4273-a3d0-f373713532c5",
        "tags" : [
        ]
      }
    ],
    "commit" : "708e640f67b3f8298aad27e4e106eb8fa9f9dc60",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +64,68 @@    (especially to get rid of division by zero when v_t = 0).\n\n    Contrast to AdamOptimizer, the sparse implementation of this algorithm\n    (used when the gradient is an IndexedSlices object, typically because of\n    `tf.gather` or an embedding lookup in the forward pass) only updates"
  }
]