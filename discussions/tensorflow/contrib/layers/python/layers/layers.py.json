[
  {
    "id" : "df037472-d8e7-408c-91a0-4e3ec45a8884",
    "prId" : 24979,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/24979#pullrequestreview-200708980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e35c9dc3-ed9c-4425-91f0-3d29f9401b4f",
        "parentId" : null,
        "authorId" : "e5d85427-a894-410f-97f7-fa642070a808",
        "body" : "1e-3 seems very large for a fuzzing value. 1e-7 is a standard epsilon value that will work across float16, float32, float64.",
        "createdAt" : "2019-01-29T00:26:22Z",
        "updatedAt" : "2019-01-29T00:27:14Z",
        "lastEditedBy" : "e5d85427-a894-410f-97f7-fa642070a808",
        "tags" : [
        ]
      },
      {
        "id" : "8edcf771-f50f-45a5-96cc-005ecde9573f",
        "parentId" : "e35c9dc3-ed9c-4425-91f0-3d29f9401b4f",
        "authorId" : "fe150b7b-80e3-40ce-88bd-077c51873903",
        "body" : "Choosing the best value here is a bit tricky. Some considerations:\r\n\r\n- 1e-3 is the default for all of the TF Python batchnorm APIs (and the FusedBatchNorm op uses 1e-4).\r\n- 1e-7 is in the subnormal range of values for fp16 (not sure if this is a concern or not).\r\n- If the input array is not normally-distributed, the maximum value can far exceed the stddev, which results in overflow during the division if epsilon is too small.\r\n- The BERT model fails to train (gets NaNs) if this variance_epsilon is set to less than ~3e-5.\r\n\r\nI'm happy to make the value smaller, but I think 3e-5 is as small as we can practically go (1e-4 seems like a safe choice). Let me know what you think.",
        "createdAt" : "2019-01-30T02:06:20Z",
        "updatedAt" : "2019-01-30T02:06:21Z",
        "lastEditedBy" : "fe150b7b-80e3-40ce-88bd-077c51873903",
        "tags" : [
        ]
      },
      {
        "id" : "d965d9a0-976b-4101-8ff4-3bcba68375f2",
        "parentId" : "e35c9dc3-ed9c-4425-91f0-3d29f9401b4f",
        "authorId" : "36d30c80-0806-46a5-9761-90d363a1d4b5",
        "body" : "@fchollet @benbarsdell ping",
        "createdAt" : "2019-02-06T17:36:50Z",
        "updatedAt" : "2019-02-06T17:36:50Z",
        "lastEditedBy" : "36d30c80-0806-46a5-9761-90d363a1d4b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "36a065b4ada1ad5226077ecc57fd1253d6ff8ed9",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2315,2319 @@    # Note that epsilon must be increased for float16 due to the limited\n    # representable range.\n    variance_epsilon = 1e-12 if dtype != dtypes.float16 else 1e-3\n    outputs = nn.batch_normalization(\n        inputs,"
  },
  {
    "id" : "4e38f993-2ad4-4385-9200-8d9edbf424fb",
    "prId" : 20455,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/20455#pullrequestreview-135935052",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6361c094-e968-4a84-bb8a-ba4fa11f519a",
        "parentId" : null,
        "authorId" : "3adc4f6e-8b58-4d7a-bd46-6c24e7f6ade0",
        "body" : "Can we add a test case with `None` value for an axis?",
        "createdAt" : "2018-07-09T16:57:09Z",
        "updatedAt" : "2018-07-09T16:57:58Z",
        "lastEditedBy" : "3adc4f6e-8b58-4d7a-bd46-6c24e7f6ade0",
        "tags" : [
        ]
      },
      {
        "id" : "a02bf414-2171-45c7-98bd-d86bb40b9ba5",
        "parentId" : "6361c094-e968-4a84-bb8a-ba4fa11f519a",
        "authorId" : "25eeacad-bc3c-4a2f-bc4b-5010f7cbeed3",
        "body" : "Like this? I am not sure.\r\n```python\r\ninput_tensor = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"input_data\")\r\nwith tf.variable_scope('conv_layer'):\r\n    conv_out = tf.layers.conv2d(input_tensor, 10, (5,5), padding='same', name=\"conv\")\r\n    maxout_out = tf.contrib.layers.maxout(conv_out, 5, axis=3)\r\n#... some test code.\r\nself.assertEqual(maxout_out.get_shape().as_list()[3], 5, \"shape in maxout function failed\")\r\n```\r\n",
        "createdAt" : "2018-07-10T02:05:33Z",
        "updatedAt" : "2018-07-10T02:53:23Z",
        "lastEditedBy" : "25eeacad-bc3c-4a2f-bc4b-5010f7cbeed3",
        "tags" : [
        ]
      },
      {
        "id" : "fbb1de03-8bd7-464d-af8d-0b6f384144d6",
        "parentId" : "6361c094-e968-4a84-bb8a-ba4fa11f519a",
        "authorId" : "3adc4f6e-8b58-4d7a-bd46-6c24e7f6ade0",
        "body" : "Yes, basically a test case that would have otherwise failed without this change.",
        "createdAt" : "2018-07-10T02:24:07Z",
        "updatedAt" : "2018-07-10T02:24:07Z",
        "lastEditedBy" : "3adc4f6e-8b58-4d7a-bd46-6c24e7f6ade0",
        "tags" : [
        ]
      },
      {
        "id" : "95521215-2b64-475d-b246-e915a7e0f7a7",
        "parentId" : "6361c094-e968-4a84-bb8a-ba4fa11f519a",
        "authorId" : "25eeacad-bc3c-4a2f-bc4b-5010f7cbeed3",
        "body" : "```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Suppose this function belongs to some test class\r\ndef testMaxoutShape(self):\r\n    data = np.random.randn(64, 32, 32, 3).astype(np.float32)\r\n    input_tensor = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"input_data\")\r\n    conv_out = tf.layers.conv2d(input_tensor, 10, (5,5), padding='same', name=\"conv\")\r\n    maxout_out = tf.contrib.layers.maxout(conv_out, 5, axis=3)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        value = sess.run(maxout_out, feed_dict={input_tensor: data})\r\n    self.assertEqual(value.shape[3], 5, \"shape in maxout function failed\")\r\n```",
        "createdAt" : "2018-07-10T03:46:15Z",
        "updatedAt" : "2018-07-10T03:47:12Z",
        "lastEditedBy" : "25eeacad-bc3c-4a2f-bc4b-5010f7cbeed3",
        "tags" : [
        ]
      },
      {
        "id" : "744119c6-36e7-4649-9c90-fdee06ef40db",
        "parentId" : "6361c094-e968-4a84-bb8a-ba4fa11f519a",
        "authorId" : "3adc4f6e-8b58-4d7a-bd46-6c24e7f6ade0",
        "body" : "Would you like to make the change to the test file? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers_test.py#L4172 ",
        "createdAt" : "2018-07-10T17:40:49Z",
        "updatedAt" : "2018-07-10T17:40:49Z",
        "lastEditedBy" : "3adc4f6e-8b58-4d7a-bd46-6c24e7f6ade0",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb6c0cbe460e6deafbe4252abed7e57bf9c4d133",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +3118,3122 @@                       'a multiple of num_units({})'.format(\n                           num_channels, num_units))\n    shape[axis] = num_units\n    shape += [num_channels // num_units]\n"
  },
  {
    "id" : "8709c6ae-ba44-4cc3-b378-ab531acc785d",
    "prId" : 18251,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/18251#pullrequestreview-110312590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8d42c27-f7c8-4e98-b204-f6a464eaf16f",
        "parentId" : null,
        "authorId" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "body" : "Add convolution1d",
        "createdAt" : "2018-04-08T21:54:41Z",
        "updatedAt" : "2018-04-09T01:57:38Z",
        "lastEditedBy" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "tags" : [
        ]
      },
      {
        "id" : "2556dd8e-7295-4f80-9429-c94b9fd576cb",
        "parentId" : "a8d42c27-f7c8-4e98-b204-f6a464eaf16f",
        "authorId" : "04d6768a-662f-42f5-9152-0bb9c7d64855",
        "body" : "@sguada Thanks. Done.",
        "createdAt" : "2018-04-09T01:58:57Z",
        "updatedAt" : "2018-04-09T01:58:57Z",
        "lastEditedBy" : "04d6768a-662f-42f5-9152-0bb9c7d64855",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e59cfdc4ecf179d4d48ab607f74c3f168aa0685",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1069,1073 @@      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n@add_arg_scope\ndef convolution1d(inputs,"
  },
  {
    "id" : "daf86c0f-aedc-4bd3-9004-0f292694ed18",
    "prId" : 13388,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/13388#pullrequestreview-69377676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e94056e8-8c41-4dfc-8224-5befbc48c119",
        "parentId" : null,
        "authorId" : "05c2b6c1-8a55-4dc9-ae76-02172416ea90",
        "body" : "Remove the TODO(reedwm) above. Github doesn't let me comment on a line far away from a change. (Any idea if doing so is possible?)",
        "createdAt" : "2017-10-13T22:09:31Z",
        "updatedAt" : "2017-11-08T17:01:09Z",
        "lastEditedBy" : "05c2b6c1-8a55-4dc9-ae76-02172416ea90",
        "tags" : [
        ]
      },
      {
        "id" : "5e7f430a-ad57-43c1-a3c3-6e14ef88c78b",
        "parentId" : "e94056e8-8c41-4dfc-8224-5befbc48c119",
        "authorId" : "8ff4e099-8cf6-4348-9db7-45b9bc9150c1",
        "body" : "Done",
        "createdAt" : "2017-10-14T00:44:56Z",
        "updatedAt" : "2017-11-08T17:01:09Z",
        "lastEditedBy" : "8ff4e099-8cf6-4348-9db7-45b9bc9150c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "8755a4305af31c18f1a4bc3b0cf4cbb875027156",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +317,321 @@    if not params_shape.is_fully_defined():\n      raise ValueError('Inputs %s has undefined `C` dimension %s.' %\n                       (inputs.name, params_shape))\n\n    # Allocate parameters for the beta and gamma of the normalization."
  },
  {
    "id" : "f3565b6c-b44c-4a09-83a5-25d23d5252b0",
    "prId" : 12273,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/12273#pullrequestreview-75946486",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3352b38-e86e-4c7d-b7d2-465b4106cfaf",
        "parentId" : null,
        "authorId" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "body" : "Can you add a test where this change is tested? i.e. when num_outputs = None but depth_multiplier is Not.",
        "createdAt" : "2017-11-11T23:06:54Z",
        "updatedAt" : "2017-11-11T23:08:20Z",
        "lastEditedBy" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "tags" : [
        ]
      },
      {
        "id" : "204ee854-f9e9-4142-af50-108bac438778",
        "parentId" : "f3352b38-e86e-4c7d-b7d2-465b4106cfaf",
        "authorId" : "23471f8f-8342-4fbf-b567-78fe8b343093",
        "body" : "@sguada As far as I am aware, my test already does this check (`testConvNCHW` in `layers_test.py`), which is a part of this PR. Specifically, the test will assert correct output shape size for NCHW data input twice: Once with `num_outputs = None`, and with `num_outputs=5`, a non-None test value. Both tests are done with `depth_multiplier = 2`, which I believe satisfies the needed tests for this change. Let me know if I have missed something!",
        "createdAt" : "2017-11-12T01:13:12Z",
        "updatedAt" : "2017-11-12T01:13:13Z",
        "lastEditedBy" : "23471f8f-8342-4fbf-b567-78fe8b343093",
        "tags" : [
        ]
      }
    ],
    "commit" : "37eda364fa4681ba5a9908fe4f3b2bc2a36097d2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2564,2568 @@          trainable=trainable,\n          collections=weights_collections)\n      strides = [1, 1, stride_h, stride_w] if data_format.startswith('NC') else [1, stride_h, stride_w, 1]\n\n      outputs = nn.depthwise_conv2d(inputs, depthwise_weights, strides, padding,"
  },
  {
    "id" : "aaec1cc5-0fd9-45b6-96cb-6e341b26da7c",
    "prId" : 10948,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/10948#pullrequestreview-45603456",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba79c4ee-14ba-45dc-bc2b-25a51f76d284",
        "parentId" : null,
        "authorId" : "f5dd8398-4230-4def-9f54-7223465d62b8",
        "body" : "There is no need for the brackets and comma here, since there is only one item.",
        "createdAt" : "2017-06-21T12:52:56Z",
        "updatedAt" : "2017-06-21T12:53:00Z",
        "lastEditedBy" : "f5dd8398-4230-4def-9f54-7223465d62b8",
        "tags" : [
        ]
      },
      {
        "id" : "6f6ae2a2-7aca-42a4-8f6c-e85d6e06c25c",
        "parentId" : "ba79c4ee-14ba-45dc-bc2b-25a51f76d284",
        "authorId" : "2aeb3eaf-db24-4cfd-b600-a38e1bd697b9",
        "body" : "What if num_outputs is a tuple? Then you'd get an obscure TypeError: not\r\nall arguments converted during string formatting. This ValueError is only\r\nraised if the type is wrong in the first place (as I experienced), so in my\r\nopinion it's helpful to be robust to this case.\r\n\r\nWhat do you think?",
        "createdAt" : "2017-06-21T23:15:36Z",
        "updatedAt" : "2017-06-21T23:15:36Z",
        "lastEditedBy" : "2aeb3eaf-db24-4cfd-b600-a38e1bd697b9",
        "tags" : [
        ]
      },
      {
        "id" : "9b0c8163-908e-4769-a86c-a5dd146729e5",
        "parentId" : "ba79c4ee-14ba-45dc-bc2b-25a51f76d284",
        "authorId" : "f5dd8398-4230-4def-9f54-7223465d62b8",
        "body" : "OK. That makes sense.",
        "createdAt" : "2017-06-22T01:37:55Z",
        "updatedAt" : "2017-06-22T01:37:55Z",
        "lastEditedBy" : "f5dd8398-4230-4def-9f54-7223465d62b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "26301bd5582d8591797af350ed1734fe7a21c800",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1447,1451 @@  if not isinstance(num_outputs, six.integer_types):\n    raise ValueError(\n        'num_outputs should be int or long, got %s.' % (num_outputs,))\n\n  layer_variable_getter = _build_variable_getter({'bias': 'biases',"
  },
  {
    "id" : "13924e75-7e59-47ec-b4ed-f16cc4911ca2",
    "prId" : 9773,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/9773#pullrequestreview-39888563",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a690dba-8570-481f-8cce-67efbd3ec326",
        "parentId" : null,
        "authorId" : "e5d85427-a894-410f-97f7-fa642070a808",
        "body" : "Needs a line break after this line",
        "createdAt" : "2017-05-23T22:50:05Z",
        "updatedAt" : "2017-06-23T00:49:48Z",
        "lastEditedBy" : "e5d85427-a894-410f-97f7-fa642070a808",
        "tags" : [
        ]
      }
    ],
    "commit" : "f870631c8c0fdf5f238bdcf2a77f8af20345781b",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +1259,1263 @@    trainable=True,\n    scope=None):\n  \"\"\"Adds a convolution3d_transpose with an optional batch normalization layer.\n  \n  The function creates a variable called `weights`, representing the"
  },
  {
    "id" : "68831f38-796f-4386-831e-241313a42f30",
    "prId" : 6280,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/6280#pullrequestreview-16422482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "043deed9-a578-47ce-861f-fdb7b5953e59",
        "parentId" : null,
        "authorId" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "body" : "Can you propagate the shape info to outputs using set_shape() and get_shape()",
        "createdAt" : "2017-01-12T18:26:22Z",
        "updatedAt" : "2017-01-20T08:27:17Z",
        "lastEditedBy" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a7f701d7b7ea78b4c44b09d8ca7e68dff5a1e03",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1214,1218 @@\n    outputs = array_ops.reshape(inputs, flat_shape)\n\n    # Attempt to propagate shape information, if it is defined.\n    input_shape = inputs.get_shape().as_list()"
  },
  {
    "id" : "b108a0a2-9a34-4e52-bcce-3da5d51683e1",
    "prId" : 6280,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/6280#pullrequestreview-16622209",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59743efd-ebe2-4d54-96a8-7326b11336fc",
        "parentId" : null,
        "authorId" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "body" : "else:\r\n  outputs.set_shape([batch_dim, None])",
        "createdAt" : "2017-01-13T18:27:02Z",
        "updatedAt" : "2017-01-20T08:27:17Z",
        "lastEditedBy" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a7f701d7b7ea78b4c44b09d8ca7e68dff5a1e03",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1223,1227 @@    else:\n      outputs.set_shape([batch_dim, None])\n\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n"
  },
  {
    "id" : "48eecd19-69bf-457d-89a2-21589c273e96",
    "prId" : 3671,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b75562ed-8acc-43d0-ab67-ef39f8d1eccd",
        "parentId" : null,
        "authorId" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "body" : "Is this only true for linear and relu? Any fully connected layer learn the scale, right?\n",
        "createdAt" : "2016-08-12T01:09:28Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "tags" : [
        ]
      },
      {
        "id" : "ea0a1450-80d5-4314-9f50-9bf05c76b0bc",
        "parentId" : "b75562ed-8acc-43d0-ab67-ef39f8d1eccd",
        "authorId" : "11e93e3b-8bc1-440f-924c-b131336deb02",
        "body" : "@martinwicke  I think the general intuition is that normalizing values with a variance of 1.0 before non-linearities such as tanh limits most activations to the linear portion of the function. This causes the network to lose it's expressive power, which the learned gamma and beta terms counteract. I think the idea is if you are applying a \"linear\" activation function, such as the identity transformation or relu, then these terms are not strictly necessary (though still beneficial?) and can be optimized out. Perhaps the default for this parameter should be True?\n",
        "createdAt" : "2016-08-12T04:50:28Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "11e93e3b-8bc1-440f-924c-b131336deb02",
        "tags" : [
        ]
      },
      {
        "id" : "1056799f-950c-4e68-a86f-4063161736fb",
        "parentId" : "b75562ed-8acc-43d0-ab67-ef39f8d1eccd",
        "authorId" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "body" : "I don't quite understand: The data flow goes\n\nlayer -> layer_norm -> activation_fn -> next layer -> activation_fn or another layer_norm or whatnot\n\nSo maybe if the activation_fn given to this is a linear one (None, or relu, or reluX), you may want this to be false?\n\nAnyway, I think a default of True makes more sense -- it seems to me you're not losing much even in terms of performance if it's on.\n",
        "createdAt" : "2016-08-12T05:16:25Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "1424ad7cacea96c4a4213f2a185ba061ac202079",
    "line" : null,
    "diffHunk" : "@@ -1,1 +864,868 @@    center: If True, subtract `beta`. If False, `beta` is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    activation_fn: Optional activation function."
  },
  {
    "id" : "bf600d19-89ea-418a-8d75-ba68c66511c5",
    "prId" : 3671,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b19a0431-0dc9-40b5-b679-88025eba20e6",
        "parentId" : null,
        "authorId" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "body" : "Should this have an is_training boolean like batch_norm?\n",
        "createdAt" : "2016-08-12T01:13:22Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "tags" : [
        ]
      },
      {
        "id" : "bf93ac1d-7235-4db5-a1d0-19a505b3f8d0",
        "parentId" : "b19a0431-0dc9-40b5-b679-88025eba20e6",
        "authorId" : "11e93e3b-8bc1-440f-924c-b131336deb02",
        "body" : "One of the features of layer normalization is that normalization happens across features/neurons in a layer and so each sample in a batch is independent. This means that the mean and variance is calculated the same during both training and inference and we don't need moving averages.\n",
        "createdAt" : "2016-08-12T04:58:22Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "11e93e3b-8bc1-440f-924c-b131336deb02",
        "tags" : [
        ]
      },
      {
        "id" : "1f0f7108-7ccd-469a-8c16-4addfa41de29",
        "parentId" : "b19a0431-0dc9-40b5-b679-88025eba20e6",
        "authorId" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "body" : "You're right, I wrote that before I noticed the absence of update_ops and saw what you were doing, sorry for the noise.\n",
        "createdAt" : "2016-08-12T05:13:21Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "3517ca44-bbf1-44f4-bada-c2b75e1ecd1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "1424ad7cacea96c4a4213f2a185ba061ac202079",
    "line" : null,
    "diffHunk" : "@@ -1,1 +846,850 @@               scale=True,\n               activation_fn=None,\n               reuse=None,\n               variables_collections=None,\n               outputs_collections=None,"
  },
  {
    "id" : "0bab201b-166b-4866-8ead-f38dbe3162da",
    "prId" : 3671,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6139d05-0532-4a71-9d0f-1ac5eb98f2e6",
        "parentId" : null,
        "authorId" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "body" : "an epsilon so small can create huge gradients if the activations are 0.\n",
        "createdAt" : "2016-08-17T20:23:17Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "2eed0e00-6acc-458d-9c8b-34843b8e1b45",
        "tags" : [
        ]
      },
      {
        "id" : "20004440-778a-4c7a-a57e-151981dda5a4",
        "parentId" : "e6139d05-0532-4a71-9d0f-1ac5eb98f2e6",
        "authorId" : "11e93e3b-8bc1-440f-924c-b131336deb02",
        "body" : "@sguada I had originally set this to 0.001 but receive feedback that it is too large. I'm not sure what the correct value to put here is. Do you have a suggestion?\n",
        "createdAt" : "2016-08-17T20:56:50Z",
        "updatedAt" : "2016-08-24T17:57:53Z",
        "lastEditedBy" : "11e93e3b-8bc1-440f-924c-b131336deb02",
        "tags" : [
        ]
      }
    ],
    "commit" : "1424ad7cacea96c4a4213f2a185ba061ac202079",
    "line" : null,
    "diffHunk" : "@@ -1,1 +917,921 @@    mean, variance = nn.moments(inputs, axis, keep_dims=True)\n    # Compute layer normalization using the batch_normalization function.\n    variance_epsilon = 1E-12\n    outputs = nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, variance_epsilon)"
  }
]