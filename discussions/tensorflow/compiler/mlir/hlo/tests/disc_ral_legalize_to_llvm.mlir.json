[
  {
    "id" : "47e81945-4ce3-45e8-a4a8-30d3262a435b",
    "prId" : 50434,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/50434#pullrequestreview-692502275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4798484b-f337-4b80-9953-c8639e3df615",
        "parentId" : null,
        "authorId" : "79b78984-b8e9-4fc7-aa98-92198b35ecca",
        "body" : "It seems like a precondition of this pass is that we already have a `gpu.binary_blob` produced. It seems to me that in such cases the module shouldn't also have an IR content?\r\nNot only they seem redundant, but also they could get out-of-sync.\r\n",
        "createdAt" : "2021-06-24T03:46:01Z",
        "updatedAt" : "2021-06-24T03:59:43Z",
        "lastEditedBy" : "79b78984-b8e9-4fc7-aa98-92198b35ecca",
        "tags" : [
        ]
      },
      {
        "id" : "f4ca8ea6-e34c-4879-a1a7-1322025ecfbc",
        "parentId" : "4798484b-f337-4b80-9953-c8639e3df615",
        "authorId" : "b4bcfac1-e2f9-43bc-ad99-b3b766a1a316",
        "body" : "Thanks! It does not need to contain the actual IR while we still need to keep the function declaration inside the module since the `gpu.launch_op` needs to refer it. Thus, I'll remove the content of the `the_kernel ` while keeping the declaration.",
        "createdAt" : "2021-06-25T06:53:17Z",
        "updatedAt" : "2021-06-25T06:53:17Z",
        "lastEditedBy" : "b4bcfac1-e2f9-43bc-ad99-b3b766a1a316",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9e6891cfdfd317e0e7ec7001287ad87316f987d",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +100,104 @@  gpu.module @kernel_module attributes {gpu.binary_blob = \"BLOB!\"} {\n    llvm.func @the_kernel() attributes {gpu.kernel}\n  }\n\n  // CHECK: llvm.func @test_gpu_launch(%[[CTX:.*]]: !llvm.ptr<i8>"
  }
]