[
  {
    "id" : "4274e1a2-8b78-4d6a-9845-e961b2ec6f06",
    "prId" : 47180,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/47180#pullrequestreview-591727964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40f0fb22-7b27-49ad-a840-39f12a3b826a",
        "parentId" : null,
        "authorId" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "body" : "Can we move these op lowerings under lower_tf.cc or lower_tf.td in the tensorflow mlir directory?",
        "createdAt" : "2021-02-16T08:38:00Z",
        "updatedAt" : "2021-02-17T02:30:59Z",
        "lastEditedBy" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "tags" : [
        ]
      },
      {
        "id" : "3b9cc779-b000-4d73-88ab-9f4d9998b185",
        "parentId" : "40f0fb22-7b27-49ad-a840-39f12a3b826a",
        "authorId" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "body" : "There is an optimization pattern for TensorFlow MLIR dialect:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/transforms/optimize.td",
        "createdAt" : "2021-02-16T08:46:07Z",
        "updatedAt" : "2021-02-17T02:30:59Z",
        "lastEditedBy" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "tags" : [
        ]
      },
      {
        "id" : "f1e2003e-df1e-41fb-830a-96f7a8be6417",
        "parentId" : "40f0fb22-7b27-49ad-a840-39f12a3b826a",
        "authorId" : "46ee1aa2-6cc0-4d18-ba46-b2b3ce5fd5a6",
        "body" : "Thanks for pointing out! My concern is that there are some patterns that converts TFL ops into TFL_Softmax. See `OptimizeToSoftmax`. If we put this pattern into TF dialect, it won't take effect for those optimizations cause the ops have been legalized. Is there any workaround? Though `OptimizeToSoftmax` is not necessary once #47012 gets merged.",
        "createdAt" : "2021-02-16T09:18:39Z",
        "updatedAt" : "2021-02-17T02:30:59Z",
        "lastEditedBy" : "46ee1aa2-6cc0-4d18-ba46-b2b3ce5fd5a6",
        "tags" : [
        ]
      },
      {
        "id" : "770fda42-b6e0-48ac-8b02-ed61ac34a2c7",
        "parentId" : "40f0fb22-7b27-49ad-a840-39f12a3b826a",
        "authorId" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "body" : "Do you think `OptimizeToSoftmax` also can be moved to the optimize.td under the TensorFlow MLIR without causing any impacts on the existing models?",
        "createdAt" : "2021-02-16T11:07:14Z",
        "updatedAt" : "2021-02-17T02:30:59Z",
        "lastEditedBy" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "tags" : [
        ]
      },
      {
        "id" : "be194d81-195f-4223-b939-046ff134ae25",
        "parentId" : "40f0fb22-7b27-49ad-a840-39f12a3b826a",
        "authorId" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "body" : "If not, I think the current approach will be fine.",
        "createdAt" : "2021-02-16T11:09:07Z",
        "updatedAt" : "2021-02-17T02:30:59Z",
        "lastEditedBy" : "dc2d2e0c-b690-4812-96e8-4d9e58f7d2e1",
        "tags" : [
        ]
      },
      {
        "id" : "7fce516b-5e9e-4c61-ba58-13eff0288e6e",
        "parentId" : "40f0fb22-7b27-49ad-a840-39f12a3b826a",
        "authorId" : "46ee1aa2-6cc0-4d18-ba46-b2b3ce5fd5a6",
        "body" : "I'm afraid it's hard to do that without breaking existing models because there are more ops used in `OptimizeToSoftmax`, such as TFL_Sum and TFL_Exp, and those ops are involved in other patterns. That is, we might have to move more patterns than just one. I'd say that most optimization patterns in TFL are suitable in TF as well (regardless XLA); probably a long-term plan is to migrate those compatible patterns into TF. Thank you!",
        "createdAt" : "2021-02-17T00:04:10Z",
        "updatedAt" : "2021-02-17T02:30:59Z",
        "lastEditedBy" : "46ee1aa2-6cc0-4d18-ba46-b2b3ce5fd5a6",
        "tags" : [
        ]
      }
    ],
    "commit" : "07afcc586ce16869ce386fe5bbdb3fe50a65f53e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +717,721 @@}\n\n// Remove (log-)softmax before arg-minmax as (log-)softmax is monotonic.\nforeach ArgMinMaxOp = [TFL_ArgMinOp, TFL_ArgMaxOp] in {\n  def RemoveSoftmaxOpBefore#ArgMinMaxOp : Pat<"
  }
]