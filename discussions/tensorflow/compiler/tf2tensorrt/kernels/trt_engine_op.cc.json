[
  {
    "id" : "80a52861-5c9e-4492-ab91-16d96ec87daa",
    "prId" : 48244,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/48244#pullrequestreview-627133284",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4647ab5b-e004-4dc6-be41-ac6f50dd9124",
        "parentId" : null,
        "authorId" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "body" : "This change doesn't seem to match the description of this PR \"Re-enable INT8 calibration with implicit batch mode\".\r\nIIUC,  the PR description should be \"Enable INT8 calibration for use_implicit_batch=false when there is no dynamic shape inputs\".\r\nAnd the detail description of the PR is \"Previously we disable INT8 calibration when use_implicit_batch=false. This PR partially enable INT8 calibration for use_implicit_batch=false, that is, when the engine has only static input shapes.\"",
        "createdAt" : "2021-04-02T17:18:36Z",
        "updatedAt" : "2021-04-02T17:18:42Z",
        "lastEditedBy" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ca25e3c1b45c6174ef591fca24bc530251b6dc6",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +507,511 @@                errors::InvalidArgument(\n                    \"Dynamic shape mode does not support calibration\"));\n  }\n}\n"
  },
  {
    "id" : "45ae101e-b699-4bc1-bab8-b55d9ca58a08",
    "prId" : 40545,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40545#pullrequestreview-595797585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7906bd77-9b8c-41eb-9442-0ff9192ef12e",
        "parentId" : null,
        "authorId" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "body" : "CollectShapeValues preprares actual_shape_values for AddShape. From this code block, it is possible that AddShape won't be called after CollectShapeValues. It is not straightforward to understand.\r\nCan we combine CollectShapeValues and AddShape and remove actual_shape_values?",
        "createdAt" : "2021-02-12T20:09:48Z",
        "updatedAt" : "2021-03-09T22:29:02Z",
        "lastEditedBy" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "tags" : [
        ]
      },
      {
        "id" : "19160f13-d418-41b3-b105-ada64966ddba",
        "parentId" : "7906bd77-9b8c-41eb-9442-0ff9192ef12e",
        "authorId" : "c5cb8841-0e3d-4435-b088-7d6beb78afd5",
        "body" : "> Can we combine CollectShapeValues and AddShape and remove actual_shape_values? \r\n\r\nNo. `AddShape` is only used for profile generation, which happens before engine construction. `CollecShapeValues` is needed for subsequent inference calls as well: if the engine needs shape value input, then `CollectShapeValues` will copy those values into the actual_shape_values. A have added a note to the declaration of `CollectShapeValues`.",
        "createdAt" : "2021-02-22T23:39:52Z",
        "updatedAt" : "2021-03-09T22:29:02Z",
        "lastEditedBy" : "c5cb8841-0e3d-4435-b088-7d6beb78afd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "47ef8f0dc44b84032bb2cc5401c84a3afe3dd6f2",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +687,691 @@      // Just collect the input shape info and return. The shapes are used to\n      // generate optimization profiles during engine creation.\n      cache_res->profiles_.AddShape(input_concrete_shapes);\n      VLOG(1) << \"Native segment is used during collecting shapes for profiles\";\n      ExecuteNativeSegment(ctx, helper);"
  },
  {
    "id" : "4c56c4cf-2743-4df9-8d75-2b10f4d616a5",
    "prId" : 36439,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/36439#pullrequestreview-355797361",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3a79151-1d07-4487-9b4a-20566ac4949e",
        "parentId" : null,
        "authorId" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "body" : "Can you change the comment to this:\r\nVerifyInputShapes guarantee that the first input is not a scalar. As such we can always use the first input to get the batch size for implicit batch mode. For explicit batch mode, this value is not used. \r\nThen add a TODO for removing the need to batch size to ConvertGraphDefToEngine for explicit batch mode.",
        "createdAt" : "2020-02-07T22:44:02Z",
        "updatedAt" : "2020-02-11T11:21:44Z",
        "lastEditedBy" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "tags" : [
        ]
      },
      {
        "id" : "958e05d7-7abc-4ad9-9a41-96479f83d8e2",
        "parentId" : "a3a79151-1d07-4487-9b4a-20566ac4949e",
        "authorId" : "c5cb8841-0e3d-4435-b088-7d6beb78afd5",
        "body" : "Done. ",
        "createdAt" : "2020-02-10T13:34:19Z",
        "updatedAt" : "2020-02-11T11:21:44Z",
        "lastEditedBy" : "c5cb8841-0e3d-4435-b088-7d6beb78afd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b75a6222b82bb556f63f7a5a04cab45212ed30c6",
    "line" : 236,
    "diffHunk" : "@@ -1,1 +893,897 @@  // the first input to get the batch size for implicit batch mode. For explicit\n  // batch mode, this value is not used.\n  const int batch_size = input_concrete_shapes[0].dim_size(0);\n  // TODO(Tamas): remove the need for batch_size in explicit_batch mode\n  auto& cache = cache_res->cache_;"
  },
  {
    "id" : "b997b9dc-f31d-4a1c-b6bd-3ee8b27bd55c",
    "prId" : 36439,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/36439#pullrequestreview-356262597",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aea54eee-02f0-49c2-89cf-9e3413dd1bc4",
        "parentId" : null,
        "authorId" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "body" : "Can you add a comment to state why it is ok for use_implicit_batch to have empty input_partial_shapes_ and maybe link this to the error message emit fromTRTEngineOp::TRTEngineOp that says this is to support the creation of engine for the TRTEngine op created using an older version of TF-TRT?",
        "createdAt" : "2020-02-10T18:21:06Z",
        "updatedAt" : "2020-02-11T11:21:44Z",
        "lastEditedBy" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "tags" : [
        ]
      },
      {
        "id" : "2bf8f384-26e3-4dec-9c75-e0de5f7736a3",
        "parentId" : "aea54eee-02f0-49c2-89cf-9e3413dd1bc4",
        "authorId" : "c5cb8841-0e3d-4435-b088-7d6beb78afd5",
        "body" : "I have added a comment, let me know if it helps. ",
        "createdAt" : "2020-02-10T21:24:47Z",
        "updatedAt" : "2020-02-11T11:21:47Z",
        "lastEditedBy" : "c5cb8841-0e3d-4435-b088-7d6beb78afd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b75a6222b82bb556f63f7a5a04cab45212ed30c6",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +428,432 @@\n  if (input_partial_shapes_.empty()) {\n    if (!use_implicit_batch_) {\n      return errors::InvalidArgument(\n          \"Explicit batch mode requires input_partial_shapes_ \","
  },
  {
    "id" : "93cb319b-1498-460d-947b-c485e5a96117",
    "prId" : 36439,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/36439#pullrequestreview-356792931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cf462d3-da6c-4758-9ea4-fcb8fc5303de",
        "parentId" : null,
        "authorId" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "body" : "retrieve, missing \"e\"",
        "createdAt" : "2020-02-11T16:18:19Z",
        "updatedAt" : "2020-02-11T17:31:52Z",
        "lastEditedBy" : "90c49754-7917-45e3-8dd2-8e09527f3d4c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b75a6222b82bb556f63f7a5a04cab45212ed30c6",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +174,178 @@  // Array of all input shapes, collected from the input_shapes attribute when\n  // constructing the TRTEngineOp. The input_shapes attribute is set during\n  // graph conversion time. This data is used to retrive which input dimensions\n  // could be unknown. During inference time this information is not available\n  // otherwise (all shapes are known (concrete) shapes when we run inference)."
  },
  {
    "id" : "30610b01-9e99-47cf-adf8-dad409c443ae",
    "prId" : 35047,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/35047#pullrequestreview-333026397",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7619bd08-55ec-47c7-9b4d-af60efd320f4",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "Can we make sure that the TRTEngineOp's setting matches the network name?",
        "createdAt" : "2019-12-12T17:42:20Z",
        "updatedAt" : "2019-12-20T22:22:04Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      },
      {
        "id" : "a2e68faa-d166-4a18-8761-81373da59d14",
        "parentId" : "7619bd08-55ec-47c7-9b4d-af60efd320f4",
        "authorId" : "d8ccd123-f416-49e8-8333-df30e2a1eb20",
        "body" : "I think this is a good idea, but I am concerned if it adds to the latency of inference.\r\nTo do this, we would have to do the following at every inference step for each of the 4 parameters precision_mode, use_calibration, max_batch_size, max_workspace_size:\r\n- Do a pattern matching in the network name to find the parameter value as a substring.\r\n- Convert the TRTEngineOp parameter to a string\r\n- Compare the found pattern with the converted parameter\r\n",
        "createdAt" : "2019-12-16T23:41:56Z",
        "updatedAt" : "2019-12-20T22:22:04Z",
        "lastEditedBy" : "d8ccd123-f416-49e8-8333-df30e2a1eb20",
        "tags" : [
        ]
      },
      {
        "id" : "5ef70de4-c3c1-4d53-9edd-2cc961b4ea41",
        "parentId" : "7619bd08-55ec-47c7-9b4d-af60efd320f4",
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "I think simple string comparisons shouldn't affect inference time that much. But it's also fine to make this a debug-only feature. We can always turn it into a strict check later.",
        "createdAt" : "2019-12-17T04:59:35Z",
        "updatedAt" : "2019-12-20T22:22:04Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc4d6ecc65fc780ddcebd8dee3c8236e6fddd131",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +533,537 @@  if (VLOG_IS_ON(2)) {\n#if IS_TRT_VERSION_GE(6, 0, 0, 0)\n    VLOG(2) << \"  Network name: \" << cuda_engine->getName();\n#endif  // #if IS_TRT_VERSION_GE(6, 0, 0, 0)\n    VLOG(2) << \"  Activation size: \" << cuda_engine->getDeviceMemorySize() << \" bytes\";"
  },
  {
    "id" : "fc712b84-06e3-4146-a0db-356472cf1d4d",
    "prId" : 27764,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/27764#pullrequestreview-225892579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a585fcea-b4d8-4223-bc94-88cf18118719",
        "parentId" : null,
        "authorId" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "body" : "Thanks a lot for fixing this. Can we add a unit test for TRTEngineOp? We can mimic get_serialized_resource_op_test.cc and reuse the same OpTestBase utilities.",
        "createdAt" : "2019-04-12T04:31:56Z",
        "updatedAt" : "2019-04-12T21:25:05Z",
        "lastEditedBy" : "1c0d3996-f30e-4521-a42e-7a5cdc08f5b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "84871f0230ff77cc851adf5b544a900e34ca8b39",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +426,430 @@        break;\n      case nvinfer1::DataType::kHALF:\n        buffers[binding_index] =\n            const_cast<Eigen::half*>(input_tensor.flat<Eigen::half>().data());\n        break;"
  }
]