[
  {
    "id" : "db010d17-3d61-4d67-b8de-92e8988a66cf",
    "prId" : 34882,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/34882#pullrequestreview-330914696",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b303635b-b708-49a2-b2bf-59adbc357be8",
        "parentId" : null,
        "authorId" : "3c76b64e-5fbc-43db-843d-8c0f62dadcab",
        "body" : "Nit:\r\n`}  // namespace`",
        "createdAt" : "2019-12-11T13:37:33Z",
        "updatedAt" : "2020-01-31T23:59:02Z",
        "lastEditedBy" : "3c76b64e-5fbc-43db-843d-8c0f62dadcab",
        "tags" : [
        ]
      },
      {
        "id" : "31067f8c-5cca-4a91-8595-256d316a00d7",
        "parentId" : "b303635b-b708-49a2-b2bf-59adbc357be8",
        "authorId" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "body" : "A bit confusing but the end of the namespace is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/cd68827e01d454937399bafcdb1eb4b9a116678a/tensorflow/compiler/xla/service/gpu/horizontal_fusion.cc#L428",
        "createdAt" : "2019-12-12T00:42:27Z",
        "updatedAt" : "2020-01-31T23:59:02Z",
        "lastEditedBy" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fc98209952114b68b2eaacc542cff87f193aa8b",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +38,42 @@    return root->operands();\n  }\n}\n\n// Returns the number of outputs of the fused computation."
  },
  {
    "id" : "3ab716f1-f365-406a-adb3-e59b7e18af37",
    "prId" : 34882,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/34882#pullrequestreview-330914696",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fd24099-9632-4d9b-8bc3-f963bcb20d9d",
        "parentId" : null,
        "authorId" : "3c76b64e-5fbc-43db-843d-8c0f62dadcab",
        "body" : "Did you see FusionWouldBeTooLarge? Would this be useful here too?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/gpu_fusible.cc#L298",
        "createdAt" : "2019-12-11T14:21:17Z",
        "updatedAt" : "2020-01-31T23:59:02Z",
        "lastEditedBy" : "3c76b64e-5fbc-43db-843d-8c0f62dadcab",
        "tags" : [
        ]
      },
      {
        "id" : "d520ec1c-2bf1-4673-9611-8c8b4a1d7d5b",
        "parentId" : "0fd24099-9632-4d9b-8bc3-f963bcb20d9d",
        "authorId" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "body" : "I have considered it but there are some difficulties I see:\r\n1. The horizontally fused kernel is much larger (e.g., in terms of the number of instructions) than the vertical fusions. So, they need different metrics.\r\n2. We consider fusing many computations at a time, while `FusionWouldBeTooLarge` considers two fusion instructions at a time.",
        "createdAt" : "2019-12-12T00:03:53Z",
        "updatedAt" : "2020-01-31T23:59:02Z",
        "lastEditedBy" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fc98209952114b68b2eaacc542cff87f193aa8b",
    "line" : 274,
    "diffHunk" : "@@ -1,1 +272,276 @@  }\n\n  // Fusing too many computations at a time may not be easily profitable and\n  // may increase compile time due to large kernels. Set a limit to it.\n  constexpr int64 kMaxFusionBatchSize = 32;"
  },
  {
    "id" : "61dc64ee-4f87-4c50-b6d3-1594859f3f5c",
    "prId" : 34882,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/34882#pullrequestreview-335034879",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f59b4e90-7516-4680-9932-4407d43e1851",
        "parentId" : null,
        "authorId" : "3c76b64e-5fbc-43db-843d-8c0f62dadcab",
        "body" : "How specific are these thresholds to a GPU architecture?",
        "createdAt" : "2019-12-19T14:09:44Z",
        "updatedAt" : "2020-01-31T23:59:02Z",
        "lastEditedBy" : "3c76b64e-5fbc-43db-843d-8c0f62dadcab",
        "tags" : [
        ]
      },
      {
        "id" : "b15a412b-b884-486f-a3b8-0702ced904f8",
        "parentId" : "f59b4e90-7516-4680-9932-4407d43e1851",
        "authorId" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "body" : "I'd be surprised if they vary a lot across architectures.\r\n\r\nThere are two parts of launch overhead--the host and device parts. The host part is the time spent on the host CPU to enqueue the job to GPU, and the device part is for the kernel to be executed by SMs (i.e., there is a minimum latency no matter how small the kernel is). The device part is usually ~2-3us and the host is ~5us on my machines (GV100 and Titan V).\r\n\r\nI just searched online and found some nice descriptions about CUDA kernel launch overhead on [stackoverflow](https://stackoverflow.com/questions/27038162/how-bad-is-it-to-launch-many-small-kernels-in-cuda). It is a 2014 post and the cited launch overheads (10us for host and 3-4us for device) are not too far away from my aforementioned numbers. So, I think kernel launch overheads should not vary too much across archs.\r\n\r\nKernel computation latency is another dimension. Theoretically, if we run on a slower GPUs, we may see longer computation latency given the same kernel. However, even if the computation latency is (slightly) longer than the launch latency, it does not mean horizontal fusion of kernels will slow them down since the parallelism is retained/increased and the horizontal fusion does not introduce much overhead to the fused kernel (assuming absence of divergence and memory non-coalescing).",
        "createdAt" : "2019-12-20T01:58:54Z",
        "updatedAt" : "2020-01-31T23:59:02Z",
        "lastEditedBy" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fc98209952114b68b2eaacc542cff87f193aa8b",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +169,173 @@  CHECK(instr.opcode() == HloOpcode::kFusion);\n  constexpr int64 kShapeThreshold = 128 * 2048;\n  constexpr int64 kInstrCountThreshold = 30;\n  auto root = instr.fused_expression_root();\n"
  },
  {
    "id" : "02603b83-f67d-481d-8ae2-b2b9ce40e150",
    "prId" : 34882,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/34882#pullrequestreview-337868608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b8d92ae-e0c3-4fd4-9d6e-a54e69cd4830",
        "parentId" : null,
        "authorId" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "body" : "I assume SPARSE is not really used (for GPU at least). Let me know if my understanding is not accurate.\r\n\r\nIf the format is SPARSE, we should return false too? as the backend cannot really support it?",
        "createdAt" : "2020-01-02T21:32:43Z",
        "updatedAt" : "2020-01-31T23:59:02Z",
        "lastEditedBy" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fc98209952114b68b2eaacc542cff87f193aa8b",
    "line" : 205,
    "diffHunk" : "@@ -1,1 +203,207 @@  auto instrs = fusion_instr.fused_instructions_computation()->instructions();\n  for (auto instr : instrs) {\n    if (instr->shape().layout().format() != DENSE) {\n      continue;\n    }"
  }
]