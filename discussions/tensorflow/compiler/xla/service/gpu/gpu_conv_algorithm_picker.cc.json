[
  {
    "id" : "67a5677f-561d-44f6-9297-ee5d442ef079",
    "prId" : 32364,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/32364#pullrequestreview-286242709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8cef0844-5c7e-4775-9fab-61229432f7c3",
        "parentId" : null,
        "authorId" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "body" : "Is the allocator the only different thing between CUDA and ROCM? Couldn't it be passed as a parameter to share more code between the two?",
        "createdAt" : "2019-09-10T00:49:59Z",
        "updatedAt" : "2019-10-01T18:39:09Z",
        "lastEditedBy" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "tags" : [
        ]
      },
      {
        "id" : "a532e3bb-a30d-42ef-9d59-960ea68ffe51",
        "parentId" : "8cef0844-5c7e-4775-9fab-61229432f7c3",
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "No this allocator is not a only difference between CUDA and ROCM implementations. The logic that only CUDA has:\r\n\r\n- Check for blacklist conv algorithms\r\n- Loop through all potential conv algorithms\r\n- buffer comparator comparisons",
        "createdAt" : "2019-09-10T15:29:05Z",
        "updatedAt" : "2019-10-01T18:39:09Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6171a4eb3d308b1794b75566c5e19cd275c1a2c",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +596,600 @@  std::vector<se::DeviceMemoryBase> operand_buffers;\n\n  ScratchAllocator input_output_allocator(device_ordinal, allocator);\n  const auto initialize_buffer = [stream](DeviceMemoryBase buffer) {\n    // Although we don't have evidence this matters, zero out the buffers"
  },
  {
    "id" : "a017643c-5d80-4420-8104-4293544b21e6",
    "prId" : 32364,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/32364#pullrequestreview-286250370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff055684-449e-4263-a0a8-6596a03f6cc2",
        "parentId" : null,
        "authorId" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "body" : "@timshen91 Any comments?\r\n\r\nTo me it seems if a \"default\" `se::ScratchAllocator` implementation does not exist, maybe a new one should be created in stream executor? (in a different CL?)",
        "createdAt" : "2019-09-10T00:53:06Z",
        "updatedAt" : "2019-10-01T18:39:09Z",
        "lastEditedBy" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "tags" : [
        ]
      },
      {
        "id" : "b8d07e87-d24e-48fe-8b5e-aa1e07cf3da3",
        "parentId" : "ff055684-449e-4263-a0a8-6596a03f6cc2",
        "authorId" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "body" : "FYI this comes from the original implementation of `cudnn_convlution_algorithm_picker.cc` from [here](https://github.com/tensorflow/tensorflow/blame/e7ea87f97e03360719d132a71acc1eb2f93c249f/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc). I am open to the idea of migrating the allocator to stream executor, because it does seems like there isn't a global scoped allocator owning the gpu memory somewhere.",
        "createdAt" : "2019-09-10T15:36:48Z",
        "updatedAt" : "2019-10-01T18:39:09Z",
        "lastEditedBy" : "643592c3-1a8a-4682-9417-fdccde36f4c1",
        "tags" : [
        ]
      },
      {
        "id" : "3099875d-c7e4-406b-bcc4-fcc33b621a30",
        "parentId" : "ff055684-449e-4263-a0a8-6596a03f6cc2",
        "authorId" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "body" : "Oh okay. I would appreciate a PR moving it, but it is optional.",
        "createdAt" : "2019-09-10T15:39:45Z",
        "updatedAt" : "2019-10-01T18:39:09Z",
        "lastEditedBy" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6171a4eb3d308b1794b75566c5e19cd275c1a2c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +48,52 @@using tensorflow::AutotuneResult;\n\nclass ScratchAllocator : public se::ScratchAllocator {\n public:\n  ScratchAllocator(int device_ordinal,"
  }
]