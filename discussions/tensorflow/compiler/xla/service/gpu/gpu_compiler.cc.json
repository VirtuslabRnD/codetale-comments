[
  {
    "id" : "a98d262d-6b5c-4879-bc7e-69852cbd3447",
    "prId" : 50876,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/50876#pullrequestreview-711820525",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2343946-d116-4b99-87f9-c4af88b8c78d",
        "parentId" : null,
        "authorId" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "body" : "Do we need this change?",
        "createdAt" : "2021-07-21T15:09:04Z",
        "updatedAt" : "2021-07-21T15:09:04Z",
        "lastEditedBy" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "tags" : [
        ]
      },
      {
        "id" : "4eeb72a7-bd4e-4b84-9fda-58b679fba7ab",
        "parentId" : "a2343946-d116-4b99-87f9-c4af88b8c78d",
        "authorId" : "1a63ab7a-058a-4b33-8c6a-0498f5437bf8",
        "body" : "The PassFix<> was added to fix AMD HIP issues. If the change in AMDGPUCompiler::OptimizeHloConvolutionCanonicalization eliminates the need for that, I am ok removing the PassFix.",
        "createdAt" : "2021-07-21T15:16:58Z",
        "updatedAt" : "2021-07-21T15:16:58Z",
        "lastEditedBy" : "1a63ab7a-058a-4b33-8c6a-0498f5437bf8",
        "tags" : [
        ]
      }
    ],
    "commit" : "28db1e8e3b2c3b99ef72c90c054a56ae77eba3c4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +396,400 @@    options.set_replace_transpose_with_bitcast(false);\n    options.set_enable_conv_operand_swap(false);\n    collectives_pipeline.AddPass<AlgebraicSimplifier>(options);\n\n    collectives_pipeline.AddPass<AllGatherBroadcastReorder>();"
  },
  {
    "id" : "58e4ece2-2088-49f8-adfa-4b88dfd04832",
    "prId" : 37377,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/37377#pullrequestreview-370095221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5616eff3-f818-49ed-8834-7bf93597aae3",
        "parentId" : null,
        "authorId" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "body" : "Is it too much to ask to extract a common function with `gpu_conv_algorithm_picker` while we're at it?",
        "createdAt" : "2020-03-06T04:50:52Z",
        "updatedAt" : "2020-03-06T04:50:52Z",
        "lastEditedBy" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "tags" : [
        ]
      },
      {
        "id" : "15c1446b-fd22-409a-abd0-d39f6bc8d7e1",
        "parentId" : "5616eff3-f818-49ed-8834-7bf93597aae3",
        "authorId" : "39e3aa8c-a3d1-45a2-8300-9bf553814b31",
        "body" : "They have functionality that is different enough that it might not be worth trying to find the common factor, especially since they're intended to be temporary. There will be different, and more permanent, plumbing in the future.\r\n\r\nIf they were combined and placed in another file, does one that already exists stand out to you as a good candidate, or would a new one make more sense? (I had a quick look and nothing stood out to me.)",
        "createdAt" : "2020-03-06T05:51:21Z",
        "updatedAt" : "2020-03-06T20:23:58Z",
        "lastEditedBy" : "39e3aa8c-a3d1-45a2-8300-9bf553814b31",
        "tags" : [
        ]
      }
    ],
    "commit" : "1aa2e003efc4ed265dad2224ff7dc4fda11aa9e1",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +344,348 @@// right way to share this.\nstatic bool RequireDeterminism() {\n  static bool require_determinism = [] {\n    bool deterministic_ops = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\"TF_DETERMINISTIC_OPS\","
  },
  {
    "id" : "7ed71530-22ca-4e82-9524-e66c52de28d5",
    "prId" : 33583,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/33583#pullrequestreview-316666500",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cfc502c-a612-48df-9d28-62a8593df8c6",
        "parentId" : null,
        "authorId" : "2861a495-42d5-4ac3-b543-23cf14c0a980",
        "body" : "It isn't just a performance consideration right, it looks like this expansion step is also working around some issue in `CudnnBatchNormRewriter`?",
        "createdAt" : "2019-11-14T01:20:51Z",
        "updatedAt" : "2019-12-03T19:59:35Z",
        "lastEditedBy" : "2861a495-42d5-4ac3-b543-23cf14c0a980",
        "tags" : [
        ]
      },
      {
        "id" : "447dadd1-e4f6-40f4-8f8a-19a360a7c58a",
        "parentId" : "0cfc502c-a612-48df-9d28-62a8593df8c6",
        "authorId" : "f5ab3aa4-6485-4b22-bd3b-5e0b0fefe39b",
        "body" : "Technically yes...`CudnnBatchNormRewriter` creates input descs by collapsing all physical dimensions \"major/slower\" than the channel dimension to  dim[0] and creating an `NCHW` descriptor by default. Although that is not incorrect, cudnn has restrictions on dim[0] of input desc for batchnorm inference. This will work around that restriction/unsupported configuration.",
        "createdAt" : "2019-11-14T01:36:37Z",
        "updatedAt" : "2019-12-03T19:59:35Z",
        "lastEditedBy" : "f5ab3aa4-6485-4b22-bd3b-5e0b0fefe39b",
        "tags" : [
        ]
      }
    ],
    "commit" : "25fc64af4437a43714646b8c9b502f25c42895ef",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +166,170 @@      // cudnn, so decompose any remaining batchnorm ops into a soup of HLOs.\n      if (hlo_module->config().debug_options().xla_gpu_use_cudnn_batchnorm()) {\n        // Since BatchNorm inference is essentially pointwise operations, it is\n        // always advantageous to use kernel fusion rather than cudnn.\n        pass.AddPass<BatchNormExpander>("
  }
]