[
  {
    "id" : "3d9638b3-28d8-424d-914d-8a58bc92db58",
    "prId" : 43850,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/43850#pullrequestreview-505934461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca8399a0-370d-4426-baaf-5874f5b4fd87",
        "parentId" : null,
        "authorId" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "body" : "Is it always off by default? Should we retry with this strategy when we OOM?",
        "createdAt" : "2020-10-09T03:14:25Z",
        "updatedAt" : "2020-10-27T01:17:42Z",
        "lastEditedBy" : "46fa9a61-9f6f-4ecb-a276-f19713d630df",
        "tags" : [
        ]
      },
      {
        "id" : "45912a84-1f02-4256-a1aa-37010846ff22",
        "parentId" : "ca8399a0-370d-4426-baaf-5874f5b4fd87",
        "authorId" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "body" : "The multiheap mode is an opt-in (and always off by default now) as in the PR. Our customers somehow are trained to feel fine with adding flags into their training scripts to use XLA (^^\").\r\n",
        "createdAt" : "2020-10-09T19:22:27Z",
        "updatedAt" : "2020-10-27T01:17:42Z",
        "lastEditedBy" : "04e5d7bd-a136-4f0a-9cb0-0e56fd16cec3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b27cbc3e7e05984e23adaf82d74d2df460e79075",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +74,78 @@  opts.set_xla_cpu_enable_xprof_traceme(false);\n  opts.set_xla_gpu_unsafe_fallback_to_driver_on_ptxas_not_found(false);\n  opts.set_xla_multiheap_size_constraint_per_heap(-1);\n\n  return opts;"
  }
]