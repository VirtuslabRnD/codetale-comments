[
  {
    "id" : "80724986-1cbc-4575-8942-e28198dbae72",
    "prId" : 1465,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a5aa515-2043-48f4-a41a-9ac81e93e4a2",
        "parentId" : null,
        "authorId" : "4b7ccdf3-ba05-46b2-b499-9f376f0e9348",
        "body" : "@rmlarsen @vrv The speed problem was coming from the fact that for high dimensional outputs y testing the whole cholesky matrix was prohibitively slow. I fixed this by testing a scalar function of the cholesky matrix for the large test cases. This is a realistic use case in practice (e.g computing the log determinant of a psd matrix).  The whole cholesky test on my desktop now takes 0.7 seconds run directly and a bit slower run from bazel. We still have coverage for the whole matrix case on the smaller test matrices. I've re-squashed this hence why it doesn't appear in the diff. \n",
        "createdAt" : "2016-04-07T11:16:30Z",
        "updatedAt" : "2016-04-07T16:48:06Z",
        "lastEditedBy" : "4b7ccdf3-ba05-46b2-b499-9f376f0e9348",
        "tags" : [
        ]
      }
    ],
    "commit" : "175ba60ec638665b1165b7e9e806c59a4ed5b8d1",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +130,134 @@            e = tf.mul(R, x)\n            K = tf.matmul(e, tf.transpose(e)) / shape[0]  # K is posdef\n            y = tf.reduce_mean(tf.cholesky(K))\n          error = tf.test.compute_gradient_error(x, x._shape_as_list(),\n                                                 y, y._shape_as_list())"
  }
]