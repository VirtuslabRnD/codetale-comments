[
  {
    "id" : "f3e598ba-6c17-4763-9447-11630c40facd",
    "prId" : 27655,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/27655#pullrequestreview-225792444",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8cf3510-fb1e-4770-8493-aa605ff545cc",
        "parentId" : null,
        "authorId" : "05c2b6c1-8a55-4dc9-ae76-02172416ea90",
        "body" : "build() is only called once, so all weights will be in the same graph. So I'm not sure if a LossScale object, or an optimizer with a LossScale object, can be used in multiple graphs.\r\n\r\nIn any case, I think this is fine for now. But add a TODO in loss_scale_test.py or loss_scale_optimizer_test.py to test using different graphs.",
        "createdAt" : "2019-04-11T02:55:59Z",
        "updatedAt" : "2019-04-12T01:03:15Z",
        "lastEditedBy" : "05c2b6c1-8a55-4dc9-ae76-02172416ea90",
        "tags" : [
        ]
      },
      {
        "id" : "0939de4b-da19-4958-9f91-459d4a91ae02",
        "parentId" : "f8cf3510-fb1e-4770-8493-aa605ff545cc",
        "authorId" : "53c3ad32-5f8d-4187-bb81-3faeb47b61f3",
        "body" : "Added a TODO in loss_scale_test.py",
        "createdAt" : "2019-04-11T21:03:14Z",
        "updatedAt" : "2019-04-12T01:03:15Z",
        "lastEditedBy" : "53c3ad32-5f8d-4187-bb81-3faeb47b61f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3d3df0a7c0787d2f8586a5f7cb446aa5c47775",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +142,146 @@\n  @property\n  def _checkpoint_dependencies(self):\n    \"\"\"From Trackable. Gather graph-specific weights to save.\"\"\"\n    if context.executing_eagerly():"
  },
  {
    "id" : "a8417381-162f-44cd-93f9-626144781c44",
    "prId" : 27655,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/27655#pullrequestreview-227382711",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fcc5ea2-8d5d-444e-af04-5dc284329bdc",
        "parentId" : null,
        "authorId" : "60a4517a-466a-4df1-8872-0f6052034ebe",
        "body" : "If g is none in this comprehension then this results in an error.\r\n",
        "createdAt" : "2019-04-16T15:08:29Z",
        "updatedAt" : "2019-04-16T15:08:29Z",
        "lastEditedBy" : "60a4517a-466a-4df1-8872-0f6052034ebe",
        "tags" : [
        ]
      },
      {
        "id" : "f9b7f6e7-35ef-4a92-a036-6b60d3f02a6a",
        "parentId" : "3fcc5ea2-8d5d-444e-af04-5dc284329bdc",
        "authorId" : "53c3ad32-5f8d-4187-bb81-3faeb47b61f3",
        "body" : "Thanks for pointing this out; we have a new PR proposing a patch here: #27898",
        "createdAt" : "2019-04-16T18:53:10Z",
        "updatedAt" : "2019-04-16T18:53:12Z",
        "lastEditedBy" : "53c3ad32-5f8d-4187-bb81-3faeb47b61f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3d3df0a7c0787d2f8586a5f7cb446aa5c47775",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +206,210 @@  \"\"\"Returns a scalar boolean tensor indicating if all gradients are finite.\"\"\"\n  is_finite_per_grad = [math_ops.reduce_all(math_ops.is_finite(g))\n                        for g in grads]\n  return math_ops.reduce_all(is_finite_per_grad)\n"
  }
]