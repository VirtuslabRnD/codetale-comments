[
  {
    "id" : "fc873c1d-7862-4646-9551-be1d6450545b",
    "prId" : 31700,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/31700#pullrequestreview-340175312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee166d10-b06a-43e2-a2ef-251938a5a8e1",
        "parentId" : null,
        "authorId" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "body" : "I think the None wrt grad_grad here just made this code silently return the wrong answer when taking third-order derivatives.\r\n\r\nEither add a prevent_gradient or implement the third-order derivative.",
        "createdAt" : "2020-01-06T19:25:41Z",
        "updatedAt" : "2020-02-07T23:10:25Z",
        "lastEditedBy" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "tags" : [
        ]
      },
      {
        "id" : "b55f6154-d9e3-472f-9368-ef66792bcb8d",
        "parentId" : "ee166d10-b06a-43e2-a2ef-251938a5a8e1",
        "authorId" : "f35c31e7-ba9e-4345-90f1-2284cfa25eff",
        "body" : "This `None` refers to the gradient wrt `labels` passed as the second input into the operation, isn't it? \r\n\r\nBecause we break out of fused implementation when Hessian is computed, I thought this should work properly when higher order derivatives are requested. I tested it locally now by adding `compute_gradient_error` for the result of `tf.hessians` similar to the `testSecondGradient` test case to be added with this PR and got error around 2.12e-8.\r\n",
        "createdAt" : "2020-01-08T22:05:04Z",
        "updatedAt" : "2020-02-07T23:10:25Z",
        "lastEditedBy" : "f35c31e7-ba9e-4345-90f1-2284cfa25eff",
        "tags" : [
        ]
      }
    ],
    "commit" : "24418e0d53f499611f2f7e1fa69ee79235755e03",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +560,564 @@        axis=1)) * softmax)\n\n  return grad, None\n\n"
  },
  {
    "id" : "92a3c4a1-1b1b-46f0-a5d1-86fcc5ac102b",
    "prId" : 21658,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/21658#pullrequestreview-150470856",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28009a91-96b6-4434-bfd4-77013b6d52e6",
        "parentId" : null,
        "authorId" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "body" : "Can you add a gradient checker test for these? Use https://www.tensorflow.org/api_docs/python/tf/test/compute_gradient_error",
        "createdAt" : "2018-08-24T16:03:30Z",
        "updatedAt" : "2018-09-26T05:06:15Z",
        "lastEditedBy" : "0184dacc-e36e-4f02-9c48-fb3acedd41da",
        "tags" : [
        ]
      },
      {
        "id" : "fd36e064-b2a1-45ca-80ed-95257d3f5107",
        "parentId" : "28009a91-96b6-4434-bfd4-77013b6d52e6",
        "authorId" : "1695aed8-fe57-4ec7-8805-7dda84daf237",
        "body" : "Well, I believe that python UTs for GradGrad was already in the original commit.",
        "createdAt" : "2018-08-29T09:14:43Z",
        "updatedAt" : "2018-09-26T05:06:15Z",
        "lastEditedBy" : "1695aed8-fe57-4ec7-8805-7dda84daf237",
        "tags" : [
        ]
      }
    ],
    "commit" : "96eec07af06f4dfc75cee57b74ba4b5347619634",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +398,402 @@\n@ops.RegisterGradient(\"LeakyReluGrad\")\ndef _LeakyReluGradGrad(op, grad):\n  x = op.inputs[1]\n  alpha = op.get_attr(\"alpha\")"
  },
  {
    "id" : "10f93684-5851-467a-b1c9-59fdaf37d051",
    "prId" : 12580,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/12580#pullrequestreview-59629440",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e2699de-baa7-4517-a00b-47d7af8ed064",
        "parentId" : null,
        "authorId" : "6bc97f28-dad4-4ccf-a190-5a69645348d5",
        "body" : "I think here you will need the input 3 and 4 of op FusedBatchNorm instead of FusedBatchNormGrad. Could you forward the pop mean and pop variance to the output 3 and 4 of FusedBatchNorm in the C++ code? This way you can also unify the two branches in the \"_FusedBatchNormGrad(op, *grad)\".",
        "createdAt" : "2017-08-29T20:47:52Z",
        "updatedAt" : "2017-09-20T02:21:36Z",
        "lastEditedBy" : "6bc97f28-dad4-4ccf-a190-5a69645348d5",
        "tags" : [
        ]
      },
      {
        "id" : "b8277adc-183f-4e04-8e8a-9cfab7b32eee",
        "parentId" : "4e2699de-baa7-4517-a00b-47d7af8ed064",
        "authorId" : "2cab004c-cfc3-4ee0-b0ff-8f1d40b1e35d",
        "body" : "`pop_mean` and `pop_var` are inputs to `FusedBatchNormGrad` as well, what's the reason to not use them directly like this? ",
        "createdAt" : "2017-08-30T05:16:52Z",
        "updatedAt" : "2017-09-20T02:21:36Z",
        "lastEditedBy" : "2cab004c-cfc3-4ee0-b0ff-8f1d40b1e35d",
        "tags" : [
        ]
      },
      {
        "id" : "7775a992-312a-4496-bc67-3476d6e95f3e",
        "parentId" : "4e2699de-baa7-4517-a00b-47d7af8ed064",
        "authorId" : "6bc97f28-dad4-4ccf-a190-5a69645348d5",
        "body" : "Note that input 3 and 4 are reserve_space_1 and reserve_space_2, which are not pop mean and pop var, but \r\nreserve_space_1: A 1D Tensor for the computed batch mean, to be reused\r\n                 in the gradient computation.\r\nreserve_space_2: A 1D Tensor for the computed batch variance (inverted variance\r\n                 in the cuDNN case), to be used in the gradient computation.\r\n\r\nREGISTER_OP(\"FusedBatchNormGrad\")\r\n    .Input(\"y_backprop: T\")\r\n    .Input(\"x: T\")\r\n    .Input(\"scale: T\")\r\n    .Input(\"reserve_space_1: T\")\r\n    .Input(\"reserve_space_2: T\")\r\n    .Output(\"x_backprop: T\")\r\n    .Output(\"scale_backprop: T\")\r\n    .Output(\"offset_backprop: T\")\r\n    .Output(\"reserve_space_3: T\")\r\n    .Output(\"reserve_space_4: T\")\r\n    .Attr(\"T: {float}\")\r\n    .Attr(\"epsilon: float = 0.0001\")\r\n    .Attr(\"data_format: string = 'NHWC'\")\r\n    .Attr(\"is_training: bool = true\")\r\n......",
        "createdAt" : "2017-08-30T06:22:26Z",
        "updatedAt" : "2017-09-20T02:21:36Z",
        "lastEditedBy" : "6bc97f28-dad4-4ccf-a190-5a69645348d5",
        "tags" : [
        ]
      },
      {
        "id" : "1c0540cd-9ed1-48d5-be2b-ff4b7db6f8da",
        "parentId" : "4e2699de-baa7-4517-a00b-47d7af8ed064",
        "authorId" : "6bc97f28-dad4-4ccf-a190-5a69645348d5",
        "body" : "Ok, you did the forwarding in _FusedBatchNormGrad, instead of on the C++ side. I think that is fine too. If you go with this way, could you update the comments of reserve_space_1 and reserve_space_2 saying they are pop mean and variance when is_training is False? ",
        "createdAt" : "2017-08-30T06:38:24Z",
        "updatedAt" : "2017-09-20T02:21:36Z",
        "lastEditedBy" : "6bc97f28-dad4-4ccf-a190-5a69645348d5",
        "tags" : [
        ]
      },
      {
        "id" : "2adb17aa-bb02-4224-be11-ad89672ff079",
        "parentId" : "4e2699de-baa7-4517-a00b-47d7af8ed064",
        "authorId" : "2cab004c-cfc3-4ee0-b0ff-8f1d40b1e35d",
        "body" : "Comments were updated.",
        "createdAt" : "2017-08-30T17:14:03Z",
        "updatedAt" : "2017-09-20T02:21:36Z",
        "lastEditedBy" : "2cab004c-cfc3-4ee0-b0ff-8f1d40b1e35d",
        "tags" : [
        ]
      }
    ],
    "commit" : "f33ea380e3615e052929b635e5fa998d27cefa11",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +839,843 @@  x = op.inputs[1]\n  scale = op.inputs[2]\n  pop_mean = op.inputs[3]\n  pop_var = op.inputs[4]\n  grad_grad_x = grad[0]"
  },
  {
    "id" : "753f27b4-1ec3-4488-afdd-bf34522584b2",
    "prId" : 4411,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/4411#pullrequestreview-540019",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5cca23a-f7aa-41d4-af2a-b9e5f75e1332",
        "parentId" : null,
        "authorId" : "17de39de-cc9c-45a3-95c0-7f6d00026c06",
        "body" : "If the shape is not fully defined at this point, this adds an unnecessary dependency at runtime. So it is better to do this when the shape is known, and do your previous bias_add when it is not. For example:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L154\n\nIf possible, please also include a benchmark result to show this is not actually slower for non-trivially large shape. tf.tile may have a few performance problems on GPU, and it is on our roadmap for improvement. \n",
        "createdAt" : "2016-09-17T00:43:50Z",
        "updatedAt" : "2016-09-23T17:41:48Z",
        "lastEditedBy" : "17de39de-cc9c-45a3-95c0-7f6d00026c06",
        "tags" : [
        ]
      },
      {
        "id" : "576c4447-c3d4-4a5f-85b5-3bbefd47d842",
        "parentId" : "c5cca23a-f7aa-41d4-af2a-b9e5f75e1332",
        "authorId" : "c48743e8-dfb4-47a0-b873-6d5270705314",
        "body" : "Some crude benchmarks:\n[benchmark.zip](https://github.com/tensorflow/tensorflow/files/479926/benchmark.zip)\n\nThe benchmarks were done on a workstation with an i7-965 (yes, we laughed too when we got it) and a GTX 1080.\n\nThe results are a bit odd. Using `bias_add` with `zeros_like` seems to give a negligible to moderate performance advantage in most cases, but it is 2-5x slower on the CPU with statically determined shapes (it looks like something is not being optimized properly here). I did notice that on the GPU, using `bias_add` seemed to result in somewhat higher GPU usage overall.\n\nI'm inclined to stay with `tf.tile` for now, since (1) it is a more direct solution (using `bias_add` seems extremely hacky), (2) its performance is more consistent, and (3) you mentioned that the performance of `tf.tile` might be improved in the future.\n\nOf course, the optimal solution would be to write another kernel for this operation (and then we could finally end the chain of gradient ops, since this operation's gradient is identical to `bias_add_grad`), but I'm not sure if that is worthwhile for something that most machine learning people aren't going to need.\n",
        "createdAt" : "2016-09-19T12:37:50Z",
        "updatedAt" : "2016-09-23T17:41:48Z",
        "lastEditedBy" : "c48743e8-dfb4-47a0-b873-6d5270705314",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f7374316e116450ad6c2ba76cb5c55f24c2297f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +224,228 @@  shape = array_ops.shape(op.inputs[0])\n  rank = array_ops.rank(op.inputs[0])\n  bias_shape = array_ops.shape(received_grad)\n  \n  if data_format is \"NCHW\":"
  },
  {
    "id" : "0b8cf652-1e8a-43d0-87e1-bd42690627a3",
    "prId" : 4411,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/4411#pullrequestreview-928428",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1427ca45-d964-4c2e-8f4a-500fa7b388b5",
        "parentId" : null,
        "authorId" : "17de39de-cc9c-45a3-95c0-7f6d00026c06",
        "body" : "Could you assert that rank is 4 for now? For conv3d and 5D tensors from videos, we will have to make adjustment. However, there are plenty of other code that needs to be modified. So just a check would suffice for now. \n",
        "createdAt" : "2016-09-19T18:03:53Z",
        "updatedAt" : "2016-09-23T17:41:48Z",
        "lastEditedBy" : "17de39de-cc9c-45a3-95c0-7f6d00026c06",
        "tags" : [
        ]
      },
      {
        "id" : "3640f0fd-f081-46fe-bbdd-33d702232c9b",
        "parentId" : "1427ca45-d964-4c2e-8f4a-500fa7b388b5",
        "authorId" : "c48743e8-dfb4-47a0-b873-6d5270705314",
        "body" : "`contrib.layers` [uses `bias_add` even for fully-connected layers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L857) (where the tensors are 2-D). I'd be hesitant to mess that up if anyone wanted to do higher-order gradients with that code (that's where I originally noticed this issue, although I'm not using it right now).\n",
        "createdAt" : "2016-09-20T00:36:26Z",
        "updatedAt" : "2016-09-23T17:41:48Z",
        "lastEditedBy" : "c48743e8-dfb4-47a0-b873-6d5270705314",
        "tags" : [
        ]
      },
      {
        "id" : "5bcee7e6-8f26-4e3e-88a2-25b0f80f342f",
        "parentId" : "1427ca45-d964-4c2e-8f4a-500fa7b388b5",
        "authorId" : "17de39de-cc9c-45a3-95c0-7f6d00026c06",
        "body" : "Thanks a lot for getting to that. It is indeed an important use case. Could you add a test case to cover that as well? We need to make sure future changes do not break that.\n",
        "createdAt" : "2016-09-20T05:17:09Z",
        "updatedAt" : "2016-09-23T17:41:48Z",
        "lastEditedBy" : "17de39de-cc9c-45a3-95c0-7f6d00026c06",
        "tags" : [
        ]
      },
      {
        "id" : "540ec120-7aa0-4afe-bfd6-0705693d202d",
        "parentId" : "1427ca45-d964-4c2e-8f4a-500fa7b388b5",
        "authorId" : "c48743e8-dfb4-47a0-b873-6d5270705314",
        "body" : "[`bias_op_test.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/bias_op_test.py) appears to have plenty of tests for ranks 2 and 3 but, curiously, no test for rank-4 tensors (for the bias_add op itself; it has gradient tests for ranks 2 and 4). Is this what you meant?\n",
        "createdAt" : "2016-09-21T11:07:24Z",
        "updatedAt" : "2016-09-23T17:41:48Z",
        "lastEditedBy" : "c48743e8-dfb4-47a0-b873-6d5270705314",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f7374316e116450ad6c2ba76cb5c55f24c2297f",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +223,227 @@  \n  shape = array_ops.shape(op.inputs[0])\n  rank = array_ops.rank(op.inputs[0])\n  bias_shape = array_ops.shape(received_grad)\n  "
  }
]