[
  {
    "id" : "92b3e692-768f-434d-bd9c-f50248bbe1cd",
    "prId" : 46321,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/46321#pullrequestreview-569707877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b203473-e8c8-4664-977a-d015e8c8e1a7",
        "parentId" : null,
        "authorId" : "e5d85427-a894-410f-97f7-fa642070a808",
        "body" : "Need input from @reedwm to see if this is an appropriate way to test mixed precision behavior.\r\n\r\nLGTM otherwise!",
        "createdAt" : "2021-01-14T17:52:05Z",
        "updatedAt" : "2021-01-18T17:23:24Z",
        "lastEditedBy" : "e5d85427-a894-410f-97f7-fa642070a808",
        "tags" : [
        ]
      },
      {
        "id" : "55c1044b-24a6-4383-a2ef-971eb67f2590",
        "parentId" : "9b203473-e8c8-4664-977a-d015e8c8e1a7",
        "authorId" : "05c2b6c1-8a55-4dc9-ae76-02172416ea90",
        "body" : "Can you also add a test to [layer_correctness_test.py](https://github.com/tensorflow/tensorflow/blob/4133fef6917fbf973229a8a8047ba2056cd8b8ee/tensorflow/python/keras/mixed_precision/layer_correctness_test.py#L75) both for Attention and AdditiveAttention? Just need to add a parameter for both classes to the parameterized test. This automatically tests the outputs and gradients are the same between mixed precision and float32.\r\n\r\nAlso better to use the internal-only [policy_scope](https://github.com/tensorflow/tensorflow/blob/7d43911faa7bae5c56e3ffe4bd3c48e75ba37e5c/tensorflow/python/keras/mixed_precision/policy.py#L530) function to set the policy, which is equivalent to the try-finally statement used here",
        "createdAt" : "2021-01-14T19:34:25Z",
        "updatedAt" : "2021-01-18T17:23:24Z",
        "lastEditedBy" : "05c2b6c1-8a55-4dc9-ae76-02172416ea90",
        "tags" : [
        ]
      },
      {
        "id" : "8a1c1be0-ecb5-4213-81b7-5222d8d54e06",
        "parentId" : "9b203473-e8c8-4664-977a-d015e8c8e1a7",
        "authorId" : "e5d85427-a894-410f-97f7-fa642070a808",
        "body" : "@yongtang please update the PR based on this feedback.",
        "createdAt" : "2021-01-15T20:51:16Z",
        "updatedAt" : "2021-01-18T17:23:24Z",
        "lastEditedBy" : "e5d85427-a894-410f-97f7-fa642070a808",
        "tags" : [
        ]
      },
      {
        "id" : "fac0c32c-cab9-46a3-a6ce-ad084d1df9fc",
        "parentId" : "9b203473-e8c8-4664-977a-d015e8c8e1a7",
        "authorId" : "04d6768a-662f-42f5-9152-0bb9c7d64855",
        "body" : "Thanks @fchollet, the PR has been updated with comments addressed.",
        "createdAt" : "2021-01-15T22:41:28Z",
        "updatedAt" : "2021-01-18T17:23:24Z",
        "lastEditedBy" : "04d6768a-662f-42f5-9152-0bb9c7d64855",
        "tags" : [
        ]
      }
    ],
    "commit" : "a265344a534ffdbff4f6ab64289cd5abe4b65eea",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +761,765 @@    self.assertEqual(new_layer.use_scale, True)\n\n  def test_mixed_float16_policy(self):\n    # Test case for GitHub issue:\n    # https://github.com/tensorflow/tensorflow/issues/46064"
  }
]