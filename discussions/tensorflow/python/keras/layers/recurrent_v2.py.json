[
  {
    "id" : "f144765f-3367-4c0c-9342-b020cf2a1a2d",
    "prId" : 35394,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/35394#pullrequestreview-338444237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6577e4cf-e695-4300-8ae4-8f394d1ed946",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "This function is already expecting cudnn build, so we probably shouldn't add this check here, and instead, add this at the caller of this function.",
        "createdAt" : "2020-01-06T02:20:11Z",
        "updatedAt" : "2020-02-10T21:14:37Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8828b16ed35dfdc448975443175829eb599ca3",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +606,610 @@  bias = array_ops.split(K.flatten(bias), 6)\n\n  if build_info.is_cuda_build:\n    # Note that the gate order for CuDNN is different from the canonical format.\n    # canonical format is [z, r, h], whereas CuDNN is [r, z, h]. The swap need to"
  },
  {
    "id" : "22acddff-9461-4fd3-a2dc-3b79ea28940d",
    "prId" : 35394,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/35394#pullrequestreview-338444237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4892e2b6-01bb-4794-b630-6ca589968928",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "I don't have much knowledge about rocm, what's the relationship between cudnn and it? Is it an extra layer above it?",
        "createdAt" : "2020-01-06T02:24:28Z",
        "updatedAt" : "2020-02-10T21:14:37Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8828b16ed35dfdc448975443175829eb599ca3",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +1373,1377 @@  full_bias = array_ops.concat((array_ops.zeros_like(bias), bias), 0)\n\n  if build_info.is_rocm_build:\n    # ROCm MIOpen's weight sequence for LSTM is different from both canonical\n    # and Cudnn format"
  },
  {
    "id" : "04055ebf-179c-483f-b5f4-8e13a680ae68",
    "prId" : 35394,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/35394#pullrequestreview-338444237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dce095ca-443b-4776-bb26-ad26372f7a42",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "Seems that Rocm is a different implementation then all existing setting, then reusing the cudnn_lstm  seems to be a bad practice. Also, how is the Rocm using cudnn_rnn kernel to work?",
        "createdAt" : "2020-01-06T02:27:32Z",
        "updatedAt" : "2020-02-10T21:14:37Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8828b16ed35dfdc448975443175829eb599ca3",
    "line" : 168,
    "diffHunk" : "@@ -1,1 +1375,1379 @@  if build_info.is_rocm_build:\n    # ROCm MIOpen's weight sequence for LSTM is different from both canonical\n    # and Cudnn format\n    # MIOpen: [i, f, o, c] Cudnn/Canonical: [i, f, c, o]\n    # i is input gate weights."
  },
  {
    "id" : "ee8f647a-186b-4882-b540-2883f8801f97",
    "prId" : 31294,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/31294#pullrequestreview-270999500",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "if the kernel supports batch major input, does it mean this transpose should be skipped?",
        "createdAt" : "2019-08-05T14:54:20Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "a22e39d2-651c-4dc4-8422-0fd0c8a8030d",
        "parentId" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "This PR will skip the transpose when sequence_lengths/mask is provided (indicating the v3 API will be used and only v3 supports batch major).",
        "createdAt" : "2019-08-05T17:56:10Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      },
      {
        "id" : "d2d0f640-0cf7-41de-9fbd-5563b56b53b6",
        "parentId" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "Agreed, I wasn't notice this when I look at the start of the function. I can approve the PR as is, but I think it will be nice to have v3 enabled for non-masking case as well.",
        "createdAt" : "2019-08-05T18:00:09Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "379619c9-9ebd-45fa-a6fb-8465c21692f9",
        "parentId" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "Sure. Actually, the cudnn rnn ops in contrib has already done it by preparing the seq_len for users. https://github.com/tensorflow/tensorflow/blob/a232fda68d692da0f409608dd5960d63ac700e5d/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L1133.\r\n\r\nI can do another PR for this feature when this is done.\r\n\r\n",
        "createdAt" : "2019-08-05T18:08:27Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      },
      {
        "id" : "483a58b1-7c4e-467c-a395-49a9c6cc0199",
        "parentId" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "oh, I was wondering if this can be done on the c op level for performance reason. It will be nice If the implementation of cudnnrnn_v3 can take care of the absent of sequence length rather than force user to provide one.",
        "createdAt" : "2019-08-05T20:34:24Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "4f45fffb-c20c-4be5-9cf9-a902bcc38527",
        "parentId" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "I think it could be done. But I don't see the performance issue for letting python prepare the seq_len, since this array is on host memory and will be on host memory even for cudnn calls.",
        "createdAt" : "2019-08-05T20:41:34Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      },
      {
        "id" : "19e1ea1e-b92e-4fe0-afd6-70195565bb67",
        "parentId" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "I guess you have better knowledge for the cudnn kernel, my original idea was that for non-mask input, the generation of seq_len tensor is not needed at all (just process the whole input sequence). ",
        "createdAt" : "2019-08-05T20:46:58Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "0cc40f5c-c3e6-4d4f-a457-76b25aad38b5",
        "parentId" : "6a2edc6b-dc57-4eec-88ff-2523f560cfce",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "Yes, let me check how much work need to be done for generating seq_len inside the op (which might require the API change, because V3 requires seq_len).\r\n\r\nBut for this PR, I think it is ready to go.",
        "createdAt" : "2019-08-05T20:52:58Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b82048bc4ca0925c4aa0fd2728fae15d72032b7",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +488,492 @@  \"\"\"GRU with CuDNN implementation which is only available for GPU.\"\"\"\n  if not time_major and mask is None:\n    inputs = array_ops.transpose(inputs, perm=(1, 0, 2))\n    seq_axis, batch_axis = (0, 1)\n  else:"
  },
  {
    "id" : "00c3b5c1-105b-4718-a0f0-f2a48b6e46d5",
    "prId" : 31294,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/31294#pullrequestreview-270820694",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac508efe-4a4f-41f7-ac6d-bd4ba13cbecf",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "Should we use cudnn_rnnv3 here as well? Do u know what's the performance difference between v1 and v3 if no mask/sequence length is provided? I think the v3 forces user to provide sequence_length, it will be nice that the kernel can allow user to skip it (treat it as full length input), and the python side can just use v3.",
        "createdAt" : "2019-08-05T14:59:16Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b82048bc4ca0925c4aa0fd2728fae15d72032b7",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +544,548 @@    outputs, h, _, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n        inputs, input_h=init_h, input_c=0, params=params, is_training=True,\n        rnn_mode='gru')\n\n  last_output = outputs[-1]"
  },
  {
    "id" : "987dfe7b-d7cd-411c-8b54-8f249eea8092",
    "prId" : 31294,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/31294#pullrequestreview-271011784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aca481f4-f37b-434b-a367-744820fbc28c",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "For the init_h here, it doesn't contain a sequence dim (it only contain one time step), should the expand_dim always apply to 0?",
        "createdAt" : "2019-08-05T21:00:41Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "51bed93d-2b20-4a08-abc5-c01c6e19e410",
        "parentId" : "aca481f4-f37b-434b-a367-744820fbc28c",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "No, it doesn't. The input_h shape is [num_layers, batch, num_units] for time_major, or [batch, num_layers, num_units] for batch_major. The expanded dim is num_layer which has the same position of seq dim.",
        "createdAt" : "2019-08-05T21:06:34Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      },
      {
        "id" : "baa592b0-9451-4265-9f58-e0f97a066093",
        "parentId" : "aca481f4-f37b-434b-a367-744820fbc28c",
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "The input init_h here is always [batch, num_units] (since keras only handles single layer), if the cudnnrnn ops expect the different shape of input for init_h based on the time/batch major, probably add a comment here since the init state has nothing to do with timestep (it is for timestep 0).",
        "createdAt" : "2019-08-05T21:17:30Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "ee1c76b4-4f5d-4c03-b8f8-a45574f18d16",
        "parentId" : "aca481f4-f37b-434b-a367-744820fbc28c",
        "authorId" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "body" : "Yes, that makes sense. I will add some comments on this. Thx.",
        "createdAt" : "2019-08-05T21:20:22Z",
        "updatedAt" : "2019-08-05T21:25:15Z",
        "lastEditedBy" : "8df6b2a7-a5b8-4771-8241-49fd7ff5a2aa",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b82048bc4ca0925c4aa0fd2728fae15d72032b7",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +494,498 @@  # For init_h, cuDNN expects one more dim of num_layers before or after batch\n  # dim for time major or batch major inputs respectively\n  init_h = array_ops.expand_dims(init_h, axis=seq_axis)\n\n  weights = array_ops.split(kernel, 3, axis=1)"
  }
]