[
  {
    "id" : "88eb7645-0741-42b4-9d3e-7d65552b6d89",
    "prId" : 40993,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40993#pullrequestreview-450930399",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fb971f99-35b8-47db-a0d6-26e7e8f9ed4f",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "This doesn't feel right, since timesteps is now a structure, getting the [1] from the flattened list will give u a shape, not a dim.",
        "createdAt" : "2020-07-14T04:24:44Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "89c37836-f714-4181-b323-45f030a101b5",
        "parentId" : "fb971f99-35b8-47db-a0d6-26e7e8f9ed4f",
        "authorId" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "body" : "Since the input shapes are first converted to a nested structure of shape tuples `timesteps = tf_utils.convert_shapes(input_shape)` with the default option `to_tuples=True` (see [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/utils/tf_utils.py#L193)), the subsequent `nest.flatten(timesteps)` command will flatten the nested structure of tuples to a list of shape values. Therefore, `nest.flatten(timesteps)[1]` will return the value of the time dimension of the first input tensor.\r\n\r\nHowever, to make it more clear this could be changed to:\r\n```suggestion\r\n    timesteps = tf_utils.convert_shapes(input_shape, to_tuples=True)\r\n    timesteps = nest.flatten(timesteps)[1]\r\n```",
        "createdAt" : "2020-07-17T20:12:19Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9671160d97ec77c73bbd60e7e0ecd0b40db6abe5",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +192,196 @@                                                 to_tuples=False)\n    timesteps = tf_utils.convert_shapes(input_shape)\n    timesteps = nest.flatten(timesteps)[1]\n    def insert_timesteps(dims):\n      dims = dims.as_list()"
  },
  {
    "id" : "6c0da34b-cd8c-4a80-941a-5d7da63d86ac",
    "prId" : 40993,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40993#pullrequestreview-450930705",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d0981d5-7752-4fed-96bb-1f9110591268",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "Same here, you probably should get the first shape from the flattened list, and then get the first dim.",
        "createdAt" : "2020-07-14T04:25:50Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "f3a23780-da48-48a6-8060-33d570ba5086",
        "parentId" : "2d0981d5-7752-4fed-96bb-1f9110591268",
        "authorId" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "body" : "See comment above.",
        "createdAt" : "2020-07-17T20:12:52Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9671160d97ec77c73bbd60e7e0ecd0b40db6abe5",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +208,212 @@        inputs)\n    batch_size = tf_utils.convert_shapes(input_shape)\n    batch_size = nest.flatten(batch_size)[0]\n    if batch_size and not self._always_use_reshape:\n      inputs, row_lengths = K.convert_inputs_if_ragged(inputs)"
  },
  {
    "id" : "9739f5f8-ab9a-4216-a8bc-56a85284a407",
    "prId" : 40993,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40993#pullrequestreview-450930780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "804baf3e-5446-4bd0-925f-1b439db3bca1",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "same.",
        "createdAt" : "2020-07-14T04:26:19Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "5e5f4220-c439-4d14-a8d9-edaed8a8451b",
        "parentId" : "804baf3e-5446-4bd0-925f-1b439db3bca1",
        "authorId" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "body" : "See comment above.",
        "createdAt" : "2020-07-17T20:13:00Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9671160d97ec77c73bbd60e7e0ecd0b40db6abe5",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +213,217 @@      is_ragged_input = row_lengths is not None\n      input_length = tf_utils.convert_shapes(input_shape)\n      input_length = nest.flatten(input_length)[1]\n\n      # batch size matters, use rnn-based implementation"
  },
  {
    "id" : "f45444ad-e38f-4672-bbf9-230224bb0907",
    "prId" : 40993,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/40993#pullrequestreview-450931667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6452595-558d-4311-a56d-694f4403daa4",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "same here.",
        "createdAt" : "2020-07-14T04:28:50Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "63f73883-949e-45d2-bfa5-363cb22d7390",
        "parentId" : "c6452595-558d-4311-a56d-694f4403daa4",
        "authorId" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "body" : "See comment above.",
        "createdAt" : "2020-07-17T20:14:43Z",
        "updatedAt" : "2020-11-12T13:15:01Z",
        "lastEditedBy" : "502a2592-098c-4c72-ad4b-10f922b1e45e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9671160d97ec77c73bbd60e7e0ecd0b40db6abe5",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +249,253 @@      else:\n        input_length = tf_utils.convert_shapes(input_shape)\n        input_length = nest.flatten(input_length)[1]\n        if not input_length:\n          input_length = nest.map_structure(lambda x: array_ops.shape(x)[1], inputs)"
  },
  {
    "id" : "3150f020-28d9-44f5-b8bf-aab4a86891b1",
    "prId" : 33441,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/33441#pullrequestreview-338455227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acdebbdf-714a-4e45-9b08-03c56f23f41a",
        "parentId" : null,
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "The input might have a different shape since it was reshaped in the call().\r\n\r\n```\r\ninner_input_shape = self._get_shape_tuple((-1,), inputs, 2)\r\n# Shape: (num_samples * timesteps, ...). And track the\t        \r\ninputs = array_ops.reshape(inputs, inner_input_shape)\r\n```",
        "createdAt" : "2019-10-21T16:57:01Z",
        "updatedAt" : "2020-01-09T13:38:03Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "7b8ad846-3ffd-4144-826b-23aef2d261dc",
        "parentId" : "acdebbdf-714a-4e45-9b08-03c56f23f41a",
        "authorId" : "fe74f130-4251-4c4e-b7be-c3be237a8da8",
        "body" : "I see the problem there.\r\nMemory usage would be greatly reduced by storing shape instead of the actual inputs and just reshaping again.  That doesn't actually fix the problem though.  \r\n\r\nDoes compute_mask get called always once, and only once?  It could just remove it from the dict if so, but I can't find anything specific on that.",
        "createdAt" : "2019-10-21T23:14:29Z",
        "updatedAt" : "2020-01-09T13:38:03Z",
        "lastEditedBy" : "fe74f130-4251-4c4e-b7be-c3be237a8da8",
        "tags" : [
        ]
      },
      {
        "id" : "52716153-b19a-471d-844d-866509d5e036",
        "parentId" : "acdebbdf-714a-4e45-9b08-03c56f23f41a",
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "compute_mask is a public API, so it can be called in all different situation, eg two masks can be created if the same layer is called with two different inputs. ",
        "createdAt" : "2019-10-23T21:30:08Z",
        "updatedAt" : "2020-01-09T13:38:04Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "380d7b13-92fd-4c25-a8c7-a86c2bb3fdd8",
        "parentId" : "acdebbdf-714a-4e45-9b08-03c56f23f41a",
        "authorId" : "fe74f130-4251-4c4e-b7be-c3be237a8da8",
        "body" : "All the reshape appears to be the same every time, and I don't see any dependencies on anything but the variable \"input\".\r\n\r\nWhich means we could perform the same reshape in compute mask. Did I miss anything? ",
        "createdAt" : "2019-10-24T15:27:33Z",
        "updatedAt" : "2020-01-09T13:38:04Z",
        "lastEditedBy" : "fe74f130-4251-4c4e-b7be-c3be237a8da8",
        "tags" : [
        ]
      },
      {
        "id" : "b5bd2f15-3c93-43d2-a661-6f2e976831b6",
        "parentId" : "acdebbdf-714a-4e45-9b08-03c56f23f41a",
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "The existing code was trying to get the inner_inputs if it is there, which might be reshaped compare to original inputs.\r\n\r\nIf we want to want cache the inputs, we somehow need to reshape the inputs again, and make sure the layer.compute_mask() is called with same input tensor when layer.call() is invoked.",
        "createdAt" : "2019-10-24T18:53:24Z",
        "updatedAt" : "2020-01-09T13:38:04Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      },
      {
        "id" : "d4502a30-fae9-43ab-a22d-f119292f6996",
        "parentId" : "acdebbdf-714a-4e45-9b08-03c56f23f41a",
        "authorId" : "fe74f130-4251-4c4e-b7be-c3be237a8da8",
        "body" : "Hmm. I think compute_mask() was not working quite right already.  If you'd already called call(), it would have a cached value and use the reshaped input.  If you hadn't, it would just use the original input.\r\n\r\nI'm not sure which is the correct behavior.  I'm would think the reshaped input is correct.",
        "createdAt" : "2019-10-24T22:44:20Z",
        "updatedAt" : "2020-01-09T13:38:04Z",
        "lastEditedBy" : "fe74f130-4251-4c4e-b7be-c3be237a8da8",
        "tags" : [
        ]
      },
      {
        "id" : "13693bbf-ffc0-4cfb-bd23-6a0efe4f2e38",
        "parentId" : "acdebbdf-714a-4e45-9b08-03c56f23f41a",
        "authorId" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "body" : "Sorry for the long wait. Thinking about this more, I think its actually better to remove the unbounded caching here with a small dip for the performance. ",
        "createdAt" : "2020-01-06T03:31:00Z",
        "updatedAt" : "2020-01-09T13:38:04Z",
        "lastEditedBy" : "d9857ae4-1f12-46ba-8f0b-0bc35ebb4bfa",
        "tags" : [
        ]
      }
    ],
    "commit" : "03156a3a295f95fa63384c316e2741b8ec063cc8",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +320,324 @@    inner_input_shape = self._get_shape_tuple((-1,), inputs, 2)\n    inputs = array_ops.reshape(inputs, inner_input_shape)\n    output_mask = self.layer.compute_mask(inputs, inner_mask)\n    if output_mask is None:\n      if mask is None:"
  }
]