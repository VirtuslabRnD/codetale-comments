[
  {
    "id" : "c45530d2-68c0-470e-9f57-f4876b82d4ee",
    "prId" : 1562,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bcaf8744-cdda-4e84-bab2-bd7187fdcbba",
        "parentId" : null,
        "authorId" : "1836b116-1c54-4d2e-8c7f-d83e825237ef",
        "body" : "Probably worth adding documentation in the doc string for this.\n",
        "createdAt" : "2016-03-25T22:14:36Z",
        "updatedAt" : "2016-03-30T00:54:22Z",
        "lastEditedBy" : "1836b116-1c54-4d2e-8c7f-d83e825237ef",
        "tags" : [
        ]
      },
      {
        "id" : "05e2abca-62a5-4dc3-a2b6-90440ccdb5b2",
        "parentId" : "bcaf8744-cdda-4e84-bab2-bd7187fdcbba",
        "authorId" : "6a02641c-f4e4-4bda-8854-330cc24b3b64",
        "body" : "done, thanks for your reminder\n",
        "createdAt" : "2016-03-26T07:40:50Z",
        "updatedAt" : "2016-03-30T00:54:22Z",
        "lastEditedBy" : "6a02641c-f4e4-4bda-8854-330cc24b3b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "19083e1e2f97bb46ecfff9fad35c7f4d10cdc2cb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +242,246 @@\n\ndef prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None):\n  \"\"\"Get WMT data into data_dir, create vocabularies and tokenize data.\n"
  }
]