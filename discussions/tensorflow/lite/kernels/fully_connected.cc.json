[
  {
    "id" : "704ba67b-4e63-411d-8fa9-aee6860cd8e9",
    "prId" : 36131,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/36131#pullrequestreview-347237670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2ba7eb2-e8ab-497e-92ab-85f841a64ed7",
        "parentId" : null,
        "authorId" : "0a35de17-09bf-47a1-9bfa-84c0a8c63acb",
        "body" : "curious why bias needs to be int64?",
        "createdAt" : "2020-01-23T02:13:16Z",
        "updatedAt" : "2020-01-23T02:13:17Z",
        "lastEditedBy" : "0a35de17-09bf-47a1-9bfa-84c0a8c63acb",
        "tags" : [
        ]
      },
      {
        "id" : "cc123553-fd89-49b8-8ffd-9676c1bda263",
        "parentId" : "a2ba7eb2-e8ab-497e-92ab-85f841a64ed7",
        "authorId" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "body" : "To avoid overflow we need 40-bit accumulator, because we have 16-bit activations and 8-bit weights, so we use int64 as an accumulator and, hence, we use bias as int64 not to lose the precision.",
        "createdAt" : "2020-01-23T11:48:22Z",
        "updatedAt" : "2020-01-23T11:48:22Z",
        "lastEditedBy" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "tags" : [
        ]
      }
    ],
    "commit" : "68743b7f730bcffb91f42744cc2518bee00bb612",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +80,84 @@  const bool is_optional_bias_float = !bias || (bias->type == kTfLiteFloat32);\n  const bool is_optional_bias_int =\n      !bias || (bias->type == kTfLiteInt32) || (bias->type == kTfLiteInt64);\n\n  if (is_quantized) {"
  }
]