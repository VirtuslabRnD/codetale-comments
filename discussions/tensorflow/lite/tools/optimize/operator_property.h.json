[
  {
    "id" : "8991aa86-2b42-4b5b-9a7c-afc3682ed2cd",
    "prId" : 36251,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/36251#pullrequestreview-360807849",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "faa1eb8d-3845-4f5d-8f69-ee443bdd3c0e",
        "parentId" : null,
        "authorId" : "14aeda60-1d22-483b-b189-c9d735472826",
        "body" : "I don't fully understand what this flag is exactly. \r\n\r\nDoes this flag force that all the inputs to an operation are 16bit?",
        "createdAt" : "2020-02-06T21:57:29Z",
        "updatedAt" : "2020-06-17T11:26:26Z",
        "lastEditedBy" : "14aeda60-1d22-483b-b189-c9d735472826",
        "tags" : [
        ]
      },
      {
        "id" : "5c6823d8-b015-4b69-9ddf-9f4e5c7224cf",
        "parentId" : "faa1eb8d-3845-4f5d-8f69-ee443bdd3c0e",
        "authorId" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "body" : "This flag is set to be used here:\r\nhttps://github.com/tensorflow/tensorflow/pull/36251/files#diff-33f11d8f6b23513872586da71656e481R459\r\n\r\nI had a problem with one model and the issue was the const input tensor was quantized into 8-bit, but I need a 16-bit quantization for all inputs, when activations are 16-bit. I could not find a better a solution than this flag. It is needed for ADD, MAXIMUM and MINIMUM.",
        "createdAt" : "2020-02-07T09:45:54Z",
        "updatedAt" : "2020-06-17T11:26:26Z",
        "lastEditedBy" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "tags" : [
        ]
      },
      {
        "id" : "c82f4138-60e3-4308-acd4-0daed3f5f83b",
        "parentId" : "faa1eb8d-3845-4f5d-8f69-ee443bdd3c0e",
        "authorId" : "14aeda60-1d22-483b-b189-c9d735472826",
        "body" : "SGTM.",
        "createdAt" : "2020-02-19T02:35:14Z",
        "updatedAt" : "2020-06-17T11:26:26Z",
        "lastEditedBy" : "14aeda60-1d22-483b-b189-c9d735472826",
        "tags" : [
        ]
      }
    ],
    "commit" : "286cd7fc6839bb2fc999fd16fb1801f6b30656b8",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +102,106 @@  // inputs are quantized as weights and this variable indicates\n  // that we want to do quantizations of these tensors as activations.\n  bool quantize_input_as_activations = false;\n};\n"
  }
]