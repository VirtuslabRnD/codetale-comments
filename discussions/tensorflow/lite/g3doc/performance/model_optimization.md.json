[
  {
    "id" : "c13ba9a8-e840-4ae6-aafb-81a2241dfab5",
    "prId" : 42699,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/42699#pullrequestreview-486684331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2caf320b-2951-48ee-a95b-db5c8da3f6eb",
        "parentId" : null,
        "authorId" : "c14be44a-1748-4d93-a075-2bb17587c151",
        "body" : "Could you please mention the expected performance comparison between the 16x8 quantization and the 8x8 quantization? This should give the reader more information for them to make the decision. Thanks.",
        "createdAt" : "2020-09-09T22:21:01Z",
        "updatedAt" : "2020-09-22T19:54:06Z",
        "lastEditedBy" : "c14be44a-1748-4d93-a075-2bb17587c151",
        "tags" : [
        ]
      },
      {
        "id" : "025e89e0-90f6-4c48-b741-3312a04e6502",
        "parentId" : "2caf320b-2951-48ee-a95b-db5c8da3f6eb",
        "authorId" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "body" : "Hi @jianlijianli I added explanation regarding performance. Could you please take a look and re-approve if it is ok? Thanks",
        "createdAt" : "2020-09-11T10:18:25Z",
        "updatedAt" : "2020-09-22T19:54:06Z",
        "lastEditedBy" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6d870d8f3990e98d5453ed641210b41ec1697a4",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +128,132 @@### Full integer quantization with int16 activations and int8 weights\n\n[Quantization with int16 activations](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) is a full integer quantization scheme with activations in int16 and weights in int8. This mode can improve accuracy of the quantized model in comparison to the full integer quantization scheme with both activations and weights in int8 keeping a similar model size. It is recommended when activations are sensitive to the quantization.\n\n<i>NOTE:</i> Currently only non-optimized reference kernel implementations are available in TFLite for this quantization scheme, so by default the performance will be slow compared to int8 kernels. Full advantages of this mode can currently be accessed via specialised hardware, or custom software."
  },
  {
    "id" : "1a6bc891-9914-4c58-9b8b-b3ca55822474",
    "prId" : 42699,
    "prUrl" : "https://github.com/tensorflow/tensorflow/pull/42699#pullrequestreview-487577519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "133b435c-c057-4ca2-aa37-db9fdf199812",
        "parentId" : null,
        "authorId" : "188e576a-21d3-4c6f-bf41-5747a83bdb5b",
        "body" : "Which MobileBERT model are you testing? @renjie-liu as FYI",
        "createdAt" : "2020-09-11T17:07:16Z",
        "updatedAt" : "2020-09-22T19:54:06Z",
        "lastEditedBy" : "188e576a-21d3-4c6f-bf41-5747a83bdb5b",
        "tags" : [
        ]
      },
      {
        "id" : "6c3c4025-3c61-484a-809b-f9fdbe43458b",
        "parentId" : "133b435c-c057-4ca2-aa37-db9fdf199812",
        "authorId" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "body" : "identical to this one: https://github.com/google-research/google-research/tree/master/mobilebert",
        "createdAt" : "2020-09-14T09:29:04Z",
        "updatedAt" : "2020-09-22T19:54:06Z",
        "lastEditedBy" : "44d28fc5-28b1-4fbb-b2f4-273674b18507",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6d870d8f3990e98d5453ed641210b41ec1697a4",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +151,155 @@    <tr><td>MobileNetV2</td><td>Top-1 Accuracy</td><td>0.718</td><td>0.7126</td>\n      <td>0.7137</td></tr>\n    <tr><td>MobileBert</td><td>F1(Exact match)</td><td>88.81(81.23)</td><td>2.08(0)</td>\n      <td>88.73(81.15)</td></tr>\n </table>"
  }
]