[
  {
    "id" : "612e1e96-5a67-478c-99d5-587f1dce9195",
    "prId" : 49398,
    "prUrl" : "https://github.com/ansible/ansible/pull/49398#pullrequestreview-181060175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "parentId" : null,
        "authorId" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "body" : "You don't need start with this code as it will be virtually the same for all the things launched by apply_async in your loop.  You can just use a single scalar local variable to hold the value.  (It also is the end time or max time (and maxtime is really max_timeout), not start).\r\n\r\nAdding that together with the note that we should probably process statvfs information separate from the uuid info:\r\n``` python\r\nresults[mount] = {'info': mount_info,\r\n                            'statvfs': pool.apply_async(get_mount_size, (mount_info['mount'],)),\r\n                            'uuid': uuids.get(mount_info['device']) or pool.apply_async(self._udevadm_uuid, (mount_info['device'],)),}\r\nmax_time = time.time() + max_timeout\r\n```",
        "createdAt" : "2018-12-03T19:17:19Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "tags" : [
        ]
      },
      {
        "id" : "cd7275c6-c059-4034-a1fc-c830630bd5d7",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "body" : "not as single scalar since we are tracking timeout PER child , otherwise we could just leave the general timeout at top of the method",
        "createdAt" : "2018-12-03T19:23:53Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "tags" : [
        ]
      },
      {
        "id" : "5f974a5a-cb5c-4663-a39e-ef8a75298421",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "body" : "You are not tracking timeout per child with this code.  The  time you're recording here is when you ask the library to queue the work.... It's not when the work starts.  Therefore, you might as well save a single general timeout at the top of the method.",
        "createdAt" : "2018-12-03T19:40:00Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "tags" : [
        ]
      },
      {
        "id" : "f76575de-0ae2-457d-ae66-5f347bdecb22",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "body" : "well, yes, queueing per child to be exact, in some of my most extreme tests this was up to 2secs diff, so i opted to keep it per child",
        "createdAt" : "2018-12-03T19:45:26Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "tags" : [
        ]
      },
      {
        "id" : "2513153c-87e1-4dee-98ef-03cf56317e7f",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "body" : "I was looking at time.clock() at the begining of the threaded function as that would give us a more accurate 'execution time'  but we cannot compare it before the `.get` which is the thing we skip on timeout.",
        "createdAt" : "2018-12-03T19:49:42Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "tags" : [
        ]
      },
      {
        "id" : "9d7cd29d-d0a2-446e-833d-09842e1a372a",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "body" : "What was your extreme test?",
        "createdAt" : "2018-12-03T19:59:52Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "tags" : [
        ]
      },
      {
        "id" : "83904323-c09d-449b-9acf-37b4560a98ae",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "body" : "hosts with +300 mount points",
        "createdAt" : "2018-12-03T20:16:21Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "tags" : [
        ]
      },
      {
        "id" : "3834fc45-ce30-48fd-80f0-6cc51217a099",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "body" : "I think you should rerun your test... The cost to invoke apply_async() on 300 elements is .01 seconds here. Adding in the regex, it's still only .05 seconds.  .05 seconds on a 10 second timeout is virtually nothing.",
        "createdAt" : "2018-12-04T00:59:22Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "tags" : [
        ]
      },
      {
        "id" : "8496fb1b-5b29-437c-ae9c-824dfda390eb",
        "parentId" : "b8d668e3-697a-4255-a238-68ac53ddba82",
        "authorId" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "body" : "Close to what i see when its a single target, but i'm testing on 50-70 vms on same host (2cpu) with variable mount points (shell below creates them) in parallel, which is not a fair test. \r\n\r\nI was trying to find a point in which i could check the difference in performance between threaded/non threaded and I ended up with this, which also showed me the queuing diff can get big on busy system. The check themselves need more variety, seems like bind mounts end up caching the info making it a non optimal test, still trying to figure out something i can whip up locally.\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\nADD=$((${RANDOM}%100))\r\nLIM=$((250+${ADD}))\r\n\r\nfor i in $(seq 1 $LIM)\r\ndo\r\n        mkdir /mnt/test_$i\r\n        mount -o bind /var /mnt/test_$i\r\ndone\r\n```\r\n",
        "createdAt" : "2018-12-04T01:40:07Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ca0d08deeb243a6ca57daa2a0b997974129861f",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +491,495 @@                              'extra': pool.apply_async(self.get_mount_info, (mount, device, uuids)),\n                              'timelimit': time.time() + maxtime}\n\n        pool.close()  # done with new workers, start gc\n"
  },
  {
    "id" : "50a3954e-9887-4985-a287-2e8746dd8f1b",
    "prId" : 49398,
    "prUrl" : "https://github.com/ansible/ansible/pull/49398#pullrequestreview-185334907",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ceec38a-f637-41ca-b4da-59949aad22c1",
        "parentId" : null,
        "authorId" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "body" : "@nitzmahone and I talked about this last night and the timeout is an expectation of how long fact gathering as a whole will take, not how long any specific piece of fact gathering will take.  We thought that probably the best place to enforce that sort of fact gathering would be in the action plugin being worked on in a separate PR.  However, for this PR, it would make more sense to at least try to replicate the status quo as much as possible, ie: take a time at the start of this function.  Then every time we check the time to see if it's time to exit due to timeout, we compare time.time() > function_start_time + timeout to see if it's time to exit.",
        "createdAt" : "2018-12-14T22:15:24Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "838eb58c-0fa8-4742-994f-a407c7183e57",
        "tags" : [
        ]
      },
      {
        "id" : "83a7d4c6-f842-4e8e-82ae-5affa8dcbba9",
        "parentId" : "0ceec38a-f637-41ca-b4da-59949aad22c1",
        "authorId" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "body" : "from the docs on setup module\r\n```\r\n- gather_timeout\r\n        Set the default timeout in seconds for individual fact gathering\r\n```",
        "createdAt" : "2018-12-15T01:17:01Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "tags" : [
        ]
      },
      {
        "id" : "c8d9ad6d-21bd-496f-a886-4a2ff8a235c4",
        "parentId" : "0ceec38a-f637-41ca-b4da-59949aad22c1",
        "authorId" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "body" : "as to what 'individual facts' are we never define, so here i was using the timeout per fact, but running them in parallel as much as possible, the total might take a bit more but it should be close enough. We read all ohter facts serially, but we can improve that with this kind of code going forward.",
        "createdAt" : "2018-12-15T01:18:42Z",
        "updatedAt" : "2018-12-19T21:58:24Z",
        "lastEditedBy" : "30fc1801-f8f1-4cc4-9690-e31e203f4f6d",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ca0d08deeb243a6ca57daa2a0b997974129861f",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +517,521 @@                    mounts.append(results[mount]['info'])\n                    del results[mount]\n                    break\n            else:\n                # avoid cpu churn"
  }
]