[
  {
    "id" : "84d12c27-d9e0-4c99-bf05-53cd78d2e24c",
    "prId" : 6363,
    "prUrl" : "https://github.com/apache/kafka/pull/6363#pullrequestreview-229704718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d2b32d9-e039-4dc3-a38c-87645253aaf6",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Doesn't each process only have a single DistributedWorker instance? If so, then would `CONNECT_CLIENT_ID_SEQUENCE.getAndIncrement()` ever return something other than 1?",
        "createdAt" : "2019-03-25T21:02:40Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "1948cd74-8cb1-4774-aeda-30cfff452baf",
        "parentId" : "0d2b32d9-e039-4dc3-a38c-87645253aaf6",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "This is older code which I chose not to change here. \r\n\r\nOne place where you can see the counter making a difference is in integration tests with multiple workers at the moment. ",
        "createdAt" : "2019-04-16T00:13:49Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      },
      {
        "id" : "0a5905e4-a4ce-4842-a160-bf595715b8b0",
        "parentId" : "0d2b32d9-e039-4dc3-a38c-87645253aaf6",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "Given that I've added several integration tests that bring up several workers, I think it makes sense to leave this here. ",
        "createdAt" : "2019-04-25T21:10:18Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb1853d8040f29948c1b9e54f95c0c2400d5b66e",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +192,196 @@\n        String clientIdConfig = config.getString(CommonClientConfigs.CLIENT_ID_CONFIG);\n        String clientId = clientIdConfig.length() <= 0 ? \"connect-\" + CONNECT_CLIENT_ID_SEQUENCE.getAndIncrement() : clientIdConfig;\n        LogContext logContext = new LogContext(\"[Worker clientId=\" + clientId + \", groupId=\" + this.workerGroupId + \"] \");\n        log = logContext.logger(DistributedHerder.class);"
  },
  {
    "id" : "8ff165cd-5541-4238-8d8c-c6d77380a69b",
    "prId" : 6363,
    "prUrl" : "https://github.com/apache/kafka/pull/6363#pullrequestreview-222511189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6d0a70e-53e8-47f6-b015-d07c96202124",
        "parentId" : null,
        "authorId" : "f5ac54e5-ad95-49b9-b49a-3fd091e917ae",
        "body" : "Should we check that `update` is `null`?  It's confusing if `running` is allowed to be `null` but `update` is assumed not to be.",
        "createdAt" : "2019-04-03T20:22:40Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "f5ac54e5-ad95-49b9-b49a-3fd091e917ae",
        "tags" : [
        ]
      },
      {
        "id" : "644a169f-f036-4ee7-b9cb-42566255429f",
        "parentId" : "f6d0a70e-53e8-47f6-b015-d07c96202124",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "This is not meant to be a general method. I removed the `null` check because assignment collections should not be `null`. If we see fit later on we could factor out this logic to a more general diff method in `ConnectAssignment` itself. ",
        "createdAt" : "2019-04-15T22:43:44Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb1853d8040f29948c1b9e54f95c0c2400d5b66e",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +909,913 @@\n    // arguments should assignment collections (connectors or tasks) and should not be null\n    private static <T> Collection<T> assignmentDifference(Collection<T> update, Collection<T> running) {\n        if (running.isEmpty()) {\n            return update;"
  },
  {
    "id" : "623b10b5-d238-44fd-8594-c1a8da985621",
    "prId" : 6363,
    "prUrl" : "https://github.com/apache/kafka/pull/6363#pullrequestreview-231253602",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4487a6a-d624-41b6-9c3e-1d78f44ff963",
        "parentId" : null,
        "authorId" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "body" : "hmm, so this is a potentially interesting change. client.id can be used for some things like quotas. since this is for workers, i'm not sure we're actually changing behavior in any interesting way here, but we should consider what the possible fallout in behavior could be by changing the default client.id behavior here.",
        "createdAt" : "2019-04-24T04:06:36Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "tags" : [
        ]
      },
      {
        "id" : "daf39b2f-144c-4153-9170-4730ba0579cc",
        "parentId" : "e4487a6a-d624-41b6-9c3e-1d78f44ff963",
        "authorId" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "body" : "i think nm on this, pretty sure it was moved from other code. just hard to see without loading the full hidden diffs in GitHub review",
        "createdAt" : "2019-04-26T04:38:52Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "tags" : [
        ]
      },
      {
        "id" : "de332eeb-2ab2-4156-8173-f251176a5d33",
        "parentId" : "e4487a6a-d624-41b6-9c3e-1d78f44ff963",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "Indeed, this code is not an addition. It's moved from `WorkerGroupMember` _as-is_\r\nTo your point, here, every time I reload I search for \"load diff\" and I load the hidden files. Thankfully only 5 or 6 so far :)",
        "createdAt" : "2019-04-26T20:14:45Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb1853d8040f29948c1b9e54f95c0c2400d5b66e",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +192,196 @@\n        String clientIdConfig = config.getString(CommonClientConfigs.CLIENT_ID_CONFIG);\n        String clientId = clientIdConfig.length() <= 0 ? \"connect-\" + CONNECT_CLIENT_ID_SEQUENCE.getAndIncrement() : clientIdConfig;\n        LogContext logContext = new LogContext(\"[Worker clientId=\" + clientId + \", groupId=\" + this.workerGroupId + \"] \");\n        log = logContext.logger(DistributedHerder.class);"
  },
  {
    "id" : "4cbbd064-d144-4ac3-93a7-cef37538f07d",
    "prId" : 6363,
    "prUrl" : "https://github.com/apache/kafka/pull/6363#pullrequestreview-238575681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eb37c56-6176-4a43-8fee-8697b65bd0fb",
        "parentId" : null,
        "authorId" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "body" : "It seems like most of the other logic around the protocol versions is cleanly factored into the protocol classes. Could we refactor this logic as well, e.g. to get this from `assignment`?",
        "createdAt" : "2019-04-24T19:16:38Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "tags" : [
        ]
      },
      {
        "id" : "1085f7e5-6d1d-4c75-b486-d2f992e57a97",
        "parentId" : "6eb37c56-6176-4a43-8fee-8697b65bd0fb",
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "+1",
        "createdAt" : "2019-05-02T05:30:53Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "dc6a4dd2-6ac3-4783-8af9-a4083068863d",
        "parentId" : "6eb37c56-6176-4a43-8fee-8697b65bd0fb",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "This is also candidate for subsequent refactoring. Will create a jira ticket. ",
        "createdAt" : "2019-05-16T18:50:12Z",
        "updatedAt" : "2019-05-17T01:31:31Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb1853d8040f29948c1b9e54f95c0c2400d5b66e",
    "line" : 184,
    "diffHunk" : "@@ -1,1 +901,905 @@        }\n        startAndStop(callables);\n        runningAssignment = member.currentProtocolVersion() == CONNECT_PROTOCOL_V0\n                            ? ExtendedAssignment.empty()\n                            : assignment;"
  },
  {
    "id" : "8e2bda71-71aa-4071-a438-cd37bc0c3456",
    "prId" : 6363,
    "prUrl" : "https://github.com/apache/kafka/pull/6363#pullrequestreview-238575395",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "478bc925-5680-4853-b92d-d0576f926c32",
        "parentId" : null,
        "authorId" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "body" : "why are we clearing these here? i would think the key thing in this class would just be to force the rejoin when tasks are revoked and it doesn't look like we use this anywhere else in this class that this would cause a problem. is there some sharing with code elsewhere that relies on this? (and if so, could we just hand this class a copy instead?)",
        "createdAt" : "2019-04-25T01:29:36Z",
        "updatedAt" : "2019-05-17T01:31:30Z",
        "lastEditedBy" : "f4cc0a00-0225-4972-8b58-0b97edf58337",
        "tags" : [
        ]
      },
      {
        "id" : "bf6b3ea4-3d8a-4d55-a05a-167514c5ef47",
        "parentId" : "478bc925-5680-4853-b92d-d0576f926c32",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "At this point I will clear this if needed in a subsequent cleanup/refactoring after more testing is applied. ",
        "createdAt" : "2019-05-16T18:49:35Z",
        "updatedAt" : "2019-05-17T01:31:31Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb1853d8040f29948c1b9e54f95c0c2400d5b66e",
    "line" : 160,
    "diffHunk" : "@@ -1,1 +846,850 @@\n        if (!assignment.revokedConnectors().isEmpty() || !assignment.revokedTasks().isEmpty()) {\n            assignment.revokedConnectors().clear();\n            assignment.revokedTasks().clear();\n            member.requestRejoin();"
  },
  {
    "id" : "9b3faeed-f695-4339-95b0-6f235fd6e5c4",
    "prId" : 6791,
    "prUrl" : "https://github.com/apache/kafka/pull/6791#pullrequestreview-251736674",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3af54f3c-2278-45b6-92d1-f6c207f52f50",
        "parentId" : null,
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "If the REST API requires authorization, won't this request fail due to lack of headers?",
        "createdAt" : "2019-05-23T23:01:55Z",
        "updatedAt" : "2019-06-03T23:02:10Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      },
      {
        "id" : "533fe7c4-be1c-4c57-86e6-cf4d329cd55f",
        "parentId" : "3af54f3c-2278-45b6-92d1-f6c207f52f50",
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Any answer to this?",
        "createdAt" : "2019-06-03T19:49:52Z",
        "updatedAt" : "2019-06-03T23:02:10Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "292d47a6-219d-41f9-9d14-1f3535047f4f",
        "parentId" : "3af54f3c-2278-45b6-92d1-f6c207f52f50",
        "authorId" : "06b42293-1a51-43e5-9c4c-04cb32562dac",
        "body" : "Hi. Pattern will filter out this endpoint so that authorization will not fail. ",
        "createdAt" : "2019-06-03T20:24:50Z",
        "updatedAt" : "2019-06-03T23:02:10Z",
        "lastEditedBy" : "06b42293-1a51-43e5-9c4c-04cb32562dac",
        "tags" : [
        ]
      },
      {
        "id" : "62ef23fc-2ed0-4215-840a-e8a495fe3246",
        "parentId" : "3af54f3c-2278-45b6-92d1-f6c207f52f50",
        "authorId" : "17dfc3e8-c780-4799-b7e2-f5a14b0a4287",
        "body" : "Hi @haidangdam, you mean there is no Authorization for POST /connectors/xx/tasks, so anyone can change tasks configuration ??",
        "createdAt" : "2019-06-18T09:41:56Z",
        "updatedAt" : "2019-06-18T09:41:56Z",
        "lastEditedBy" : "17dfc3e8-c780-4799-b7e2-f5a14b0a4287",
        "tags" : [
        ]
      },
      {
        "id" : "839c2b83-99d7-42b1-bf58-8c933f975d22",
        "parentId" : "3af54f3c-2278-45b6-92d1-f6c207f52f50",
        "authorId" : "f5ac54e5-ad95-49b9-b49a-3fd091e917ae",
        "body" : "Currently this endpoint should be secured through other means, such as mutual TLS.",
        "createdAt" : "2019-06-19T14:23:03Z",
        "updatedAt" : "2019-06-19T14:23:03Z",
        "lastEditedBy" : "f5ac54e5-ad95-49b9-b49a-3fd091e917ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "d30d6ce4d1cec5c51d9a560dc85c5ee00dcd6f95",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1203,1207 @@                                }\n                                String reconfigUrl = RestServer.urlJoin(leaderUrl, \"/connectors/\" + connName + \"/tasks\");\n                                RestClient.httpRequest(reconfigUrl, \"POST\", null, rawTaskProps, null, config);\n                                cb.onCompletion(null, null);\n                            } catch (ConnectException e) {"
  },
  {
    "id" : "ab357e4f-d4d7-4664-b407-1355bf19bb15",
    "prId" : 6820,
    "prUrl" : "https://github.com/apache/kafka/pull/6820#pullrequestreview-522212974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4bf4f44-1895-45ef-9efe-2c0e2671fe95",
        "parentId" : null,
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "Seems fine to keep this `trace` log in both cases and keep the implementation only in `AbstractHerder`. Wdyt?",
        "createdAt" : "2020-11-03T06:15:18Z",
        "updatedAt" : "2020-11-03T06:15:40Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      },
      {
        "id" : "9de384b4-bcd5-4f48-8930-a3287d933b56",
        "parentId" : "d4bf4f44-1895-45ef-9efe-2c0e2671fe95",
        "authorId" : "8752f2d5-1895-482a-aa14-16113cda1c69",
        "body" : "The trace logging for other operations (`putConnectorConfig`, `taskConfigs`, etc) is done only in `DistributedHerder`, too. For the consistency sake, I'd leave it as is in this PR.",
        "createdAt" : "2020-11-03T06:27:33Z",
        "updatedAt" : "2020-11-03T06:27:34Z",
        "lastEditedBy" : "8752f2d5-1895-482a-aa14-16113cda1c69",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2f678684f9371510f915a8880682228309b02e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +752,756 @@    @Override\n    public void connectorConfig(String connName, final Callback<Map<String, String>> callback) {\n        log.trace(\"Submitting connector config read request {}\", connName);\n        super.connectorConfig(connName, callback);\n    }"
  },
  {
    "id" : "53f214d5-f4fd-4f38-bad8-27bc3e82cac0",
    "prId" : 6850,
    "prUrl" : "https://github.com/apache/kafka/pull/6850#pullrequestreview-244678657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6cfdba58-c35b-46fb-b992-cd27d3f2dfd1",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Minor: it would help to have a comment here to explain why the tasks to restart are removed from the running tasks.",
        "createdAt" : "2019-06-02T16:41:50Z",
        "updatedAt" : "2019-06-03T04:50:03Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "eb9da2e3-4264-44f1-93b5-a0ea4db67e4f",
        "parentId" : "6cfdba58-c35b-46fb-b992-cd27d3f2dfd1",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "Done",
        "createdAt" : "2019-06-03T04:48:26Z",
        "updatedAt" : "2019-06-03T04:50:03Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3383ca76f079af5bffc9fea3992a437b89d4de9",
    "line" : 241,
    "diffHunk" : "@@ -1,1 +1010,1014 @@        // the assignment that was just received after rebalancing.\n        runningAssignment.tasks().removeAll(tasksToRestart);\n        tasksToRestart.clear();\n        for (ConnectorTaskId taskId : assignmentDifference(assignment.tasks(), runningAssignment.tasks())) {\n            callables.add(getTaskStartingCallable(taskId));"
  },
  {
    "id" : "fa00b18e-77fb-42ea-b861-531b2cff45b2",
    "prId" : 6850,
    "prUrl" : "https://github.com/apache/kafka/pull/6850#pullrequestreview-244679030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84fd72f5-c99c-4f9c-af3b-e70948967d1b",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Minor: this log line is in a different location in the similar `updateConfigsWithEager(...)` method. Would it make sense to move this log message to just before the `member.requestRejoin()` call?",
        "createdAt" : "2019-06-02T16:47:06Z",
        "updatedAt" : "2019-06-03T04:50:03Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "98851913-9023-42c9-a224-5d233a6d7b94",
        "parentId" : "84fd72f5-c99c-4f9c-af3b-e70948967d1b",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "Moved it to both before `member.requestRejoin`. I changed it because it was even after we set this variable to false, which made the log message a bit confusing. ",
        "createdAt" : "2019-06-03T04:48:10Z",
        "updatedAt" : "2019-06-03T04:50:03Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      },
      {
        "id" : "be5252ec-2f16-4db8-85b1-4dc022cae76b",
        "parentId" : "84fd72f5-c99c-4f9c-af3b-e70948967d1b",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "(done)",
        "createdAt" : "2019-06-03T04:50:56Z",
        "updatedAt" : "2019-06-03T04:50:57Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3383ca76f079af5bffc9fea3992a437b89d4de9",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +419,423 @@\n            if (needsReconfigRebalance) {\n                log.debug(\"Requesting rebalance due to reconfiguration of tasks (needsReconfigRebalance: {})\",\n                        needsReconfigRebalance);\n                member.requestRejoin();"
  },
  {
    "id" : "b2517f80-ae48-4855-82ff-2f7cef61bbaa",
    "prId" : 7310,
    "prUrl" : "https://github.com/apache/kafka/pull/7310#pullrequestreview-295173069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c03baefd-77f6-4aab-9d99-45e0c021dfa6",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Should this log message be before or after we actually do update the session key?",
        "createdAt" : "2019-09-27T22:46:46Z",
        "updatedAt" : "2019-10-02T21:38:53Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "1ac3eca2-6ea5-437f-bcf1-48062b7ad23c",
        "parentId" : "c03baefd-77f6-4aab-9d99-45e0c021dfa6",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "It follows the precedent set by other methods in this listener; if it's a dealbreaker I can change it.",
        "createdAt" : "2019-09-30T18:39:02Z",
        "updatedAt" : "2019-10-02T21:38:53Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      }
    ],
    "commit" : "5de5daf943cc8540c25e8013175dbf2a86b5f409",
    "line" : 298,
    "diffHunk" : "@@ -1,1 +1436,1440 @@        @Override\n        public void onSessionKeyUpdate(SessionKey sessionKey) {\n            log.info(\"Session key updated\");\n\n            synchronized (DistributedHerder.this) {"
  },
  {
    "id" : "bee0ec67-2778-487e-b58c-ad3f47f9a024",
    "prId" : 7310,
    "prUrl" : "https://github.com/apache/kafka/pull/7310#pullrequestreview-294581302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7958eebd-3901-4360-b005-920de3dd6946",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Why do we only do this for the leader? Maybe add a comment describing when the followers will update their key?",
        "createdAt" : "2019-09-27T22:47:36Z",
        "updatedAt" : "2019-10-02T21:38:53Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5de5daf943cc8540c25e8013175dbf2a86b5f409",
    "line" : 306,
    "diffHunk" : "@@ -1,1 +1444,1448 @@                // tracking expiration and distributing new keys themselves\n                if (isLeader() && keyRotationIntervalMs > 0) {\n                    DistributedHerder.this.keyExpiration = sessionKey.creationTimestamp() + keyRotationIntervalMs;\n                }\n            }"
  },
  {
    "id" : "3d247013-523b-4e18-a126-f208f677f4df",
    "prId" : 7995,
    "prUrl" : "https://github.com/apache/kafka/pull/7995#pullrequestreview-347685665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d4a8d1f-a6df-4c33-8686-7aba3ef1b3d9",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "This change is a good suggestion.",
        "createdAt" : "2020-01-23T23:58:13Z",
        "updatedAt" : "2020-01-24T00:06:39Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      }
    ],
    "commit" : "570109a9da76a1aa26d581f309e278c4d73d1cc0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +700,704 @@                        log.trace(\"Handling connector config request {}\", connName);\n                        if (!isLeader()) {\n                            callback.onCompletion(new NotLeaderException(\"Only the leader can delete connector configs.\", leaderUrl()), null);\n                            return null;\n                        }"
  },
  {
    "id" : "c966433e-4e46-4787-9e0c-97c42bd887fa",
    "prId" : 8069,
    "prUrl" : "https://github.com/apache/kafka/pull/8069#pullrequestreview-427776103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bf3056a-5628-4c72-9f63-e48a4fc42bc4",
        "parentId" : null,
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "I wonder if we should create a jira issue instead",
        "createdAt" : "2020-06-10T07:45:10Z",
        "updatedAt" : "2020-06-11T04:43:53Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfb1bc04832b342e614e891d6da5af1f8ebe30a5",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +344,348 @@\n        // Process any external requests\n        // TODO: Some of these can be performed concurrently or even optimized away entirely.\n        //       For example, if three different connectors are slated to be restarted, it's fine to\n        //       restart all three at the same time instead."
  },
  {
    "id" : "7f6fe8cd-a076-4306-a6c6-b125fb364535",
    "prId" : 8069,
    "prUrl" : "https://github.com/apache/kafka/pull/8069#pullrequestreview-428487610",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43705491-18a2-4790-963a-b504fca7b09b",
        "parentId" : null,
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "do we know how often this message or the one below would be printed?",
        "createdAt" : "2020-06-10T07:50:06Z",
        "updatedAt" : "2020-06-11T04:43:53Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      },
      {
        "id" : "72137c57-1abc-438d-aa63-fb02e2682195",
        "parentId" : "43705491-18a2-4790-963a-b504fca7b09b",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "Not sure how often but I can think of some scenarios that would cause it:\r\n\r\n- Session key is updated\r\n- The number of workers in the cluster changes\r\n\r\nWhy do you ask?",
        "createdAt" : "2020-06-10T17:38:36Z",
        "updatedAt" : "2020-06-11T04:43:53Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      },
      {
        "id" : "814a07df-69d8-4d12-948a-acc89fd5ec0c",
        "parentId" : "43705491-18a2-4790-963a-b504fca7b09b",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "I'm trying to understand if it would overwhelm the logs at the TRACE level",
        "createdAt" : "2020-06-10T21:47:34Z",
        "updatedAt" : "2020-06-11T04:43:54Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      },
      {
        "id" : "65eed887-05d6-47dd-b27f-c40ee4686b81",
        "parentId" : "43705491-18a2-4790-963a-b504fca7b09b",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "Ahh, gotcha. It'll definitely make them a bit noisier, but this line will only ever be reached once per herder tick, which already produces several other messages at a higher level.",
        "createdAt" : "2020-06-10T23:07:48Z",
        "updatedAt" : "2020-06-11T04:43:54Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfb1bc04832b342e614e891d6da5af1f8ebe30a5",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +512,516 @@            }\n        } else {\n            log.trace(\"Skipping config updates with eager rebalancing \"\n                + \"since no config rebalance is required \"\n                + \"and there are no connector config, task config, or target state changes pending\");"
  },
  {
    "id" : "e4c3fba5-6a1e-4edd-8962-23d5d0dd9f37",
    "prId" : 9765,
    "prUrl" : "https://github.com/apache/kafka/pull/9765#pullrequestreview-555704194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4679de2-f049-4d2b-9880-35caa133306c",
        "parentId" : null,
        "authorId" : "4873aa72-3842-44ba-91a8-a59ea9830460",
        "body" : "Maybe add a comment explaining why the additional check is needed.",
        "createdAt" : "2020-12-18T18:56:26Z",
        "updatedAt" : "2020-12-18T18:56:33Z",
        "lastEditedBy" : "4873aa72-3842-44ba-91a8-a59ea9830460",
        "tags" : [
        ]
      }
    ],
    "commit" : "1375f01da6ce979307f7ba52362b453166c2d39e",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1742,1746 @@            // it is still important to have a leader that can write configs, offsets, etc.\n\n            if (rebalanceResolved || currentProtocolVersion >= CONNECT_PROTOCOL_V1) {\n                List<Callable<Void>> callables = new ArrayList<>();\n                for (final String connectorName : connectors) {"
  },
  {
    "id" : "b0d7a989-4aac-45c0-a6b4-21e72bcac19f",
    "prId" : 9780,
    "prUrl" : "https://github.com/apache/kafka/pull/9780#pullrequestreview-584032336",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5de741aa-bf73-40bf-92fb-4d4b3941d7d1",
        "parentId" : null,
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "I think it's better to avoid a variadic argument here. \r\nParameters tend to get added with new features in such constructors. And if a new parameter is required that is also a list, then we'll have a mix of list args with a variadic in the end. \r\n\r\nSince we transform to list I'd suggest using this type here and pass the single argument with `Collections.singletonList` in the caller. ",
        "createdAt" : "2021-02-04T22:21:02Z",
        "updatedAt" : "2021-02-09T07:49:14Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      },
      {
        "id" : "9c302158-701b-465a-bb88-cee7ab5deebd",
        "parentId" : "5de741aa-bf73-40bf-92fb-4d4b3941d7d1",
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "The reason I used a variadic array here was to avoid having to create a new connector when no `AutoCloseable` instances are supplied. If we use a List, then we can change the usage in Connect runtime and in MirrorMaker 2, but anywhere else will break without keeping the old signature. WDYT?",
        "createdAt" : "2021-02-05T00:56:58Z",
        "updatedAt" : "2021-02-09T07:49:14Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "0140d2c5-5e0f-45bb-af46-18b48a06b1c2",
        "parentId" : "5de741aa-bf73-40bf-92fb-4d4b3941d7d1",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "We can always keep a constructor with the old signature along with the new if we wanted not to break classes that use `DistributedHerder`. I'm fine with the change here as a short term workaround. I guess it saves us one constructor but we can use it only once.",
        "createdAt" : "2021-02-05T05:40:44Z",
        "updatedAt" : "2021-02-09T07:49:14Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "54a3f10bc7be899cb47c6500d51ff2cec3e1e572",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +213,217 @@                             String restUrl,\n                             ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                             AutoCloseable... uponShutdown) {\n        this(config, worker, worker.workerId(), kafkaClusterId, statusBackingStore, configBackingStore, null, restUrl, worker.metrics(),\n             time, connectorClientConfigOverridePolicy, uponShutdown);"
  },
  {
    "id" : "db3fa337-a47a-457d-a504-d765226e9c09",
    "prId" : 9780,
    "prUrl" : "https://github.com/apache/kafka/pull/9780#pullrequestreview-583863980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0eed928c-c2fa-4d9b-a27c-fb939702fa89",
        "parentId" : null,
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "see comment above",
        "createdAt" : "2021-02-04T22:21:16Z",
        "updatedAt" : "2021-02-09T07:49:14Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "54a3f10bc7be899cb47c6500d51ff2cec3e1e572",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +231,235 @@                      Time time,\n                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                      AutoCloseable... uponShutdown) {\n        super(worker, workerId, kafkaClusterId, statusBackingStore, configBackingStore, connectorClientConfigOverridePolicy);\n"
  },
  {
    "id" : "a9b1508d-fa78-4707-8fd9-11ceddf50a31",
    "prId" : 10014,
    "prUrl" : "https://github.com/apache/kafka/pull/10014#pullrequestreview-581644860",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c4f7fa4-afb6-466e-9dc6-64d07141e78e",
        "parentId" : null,
        "authorId" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "body" : "I don't think the `keyExpiration < MAX_VALUE` condition is strictly necessary any more, since if we're the leader, we must either have a valid key by this point. It made sense before, as it was basically an implcit `isLeader()` check that explicitly happened much earlier during the callback. Now, we either had a key read before, or added one at the beginning of this loop (and read to the end of the log, executing all callbacks including the one which updates `keyExpiration`). \r\nNeither the `isLeader()` or `internalRequestValidationEnabled()` conditions should change in between the `checkForKeyRotation(now)` call and here, so I don't think there's danger of hitting this condition when the `checkForKeyRotation(now)` didn't ensure that we have a key.\r\n\r\nIf `putSessionKey()` ever changed implementation to not read back the key and ensure that callbacks were executed prior to returning, then this condition would guard in that case, and prevent us from scheduling a `nextRequestTimeoutMs` 50y earlier than the otherwise very reasonable 292 million years in the future. We would \"forget\" to schedule a key expiration because of a race condition while installing a new key, and only schedule a rebalance the next time a rebalance occurred due to some other factor.\r\n\r\nSeems harmless to remove, and only changes behavior for a case where we introduce a (worse) concurrency bug. Removing it will keep someone from wondering \"why is that condition there\" at some point in the future though :)",
        "createdAt" : "2021-02-01T22:09:58Z",
        "updatedAt" : "2021-02-26T16:28:14Z",
        "lastEditedBy" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "tags" : [
        ]
      },
      {
        "id" : "398fc208-6513-4ded-adf2-85d4e6436089",
        "parentId" : "3c4f7fa4-afb6-466e-9dc6-64d07141e78e",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "There's a rationale for leaving the check in here but I'm not sold either way. Let me know what you think:\r\n\r\nThe `Scheduled next key rotation at...` log message should ideally only be emitted if key rotation is actually enabled. Key rotation can be [disabled by setting the ttl to 0](https://github.com/apache/kafka/blob/821d867c1bd01b88aa695827cb57c9aa764857a4/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1579). If someone has key rotation disabled, emitting this log message doesn't make a lot of sense (even if the scheduled rotation time is millions of years in the future).\r\n\r\nThis also mirrors the structure of the scheduled rebalance delay check above, although that check is more necessary because of the tweaking of the `rebalanceResolved` field.",
        "createdAt" : "2021-02-02T14:39:55Z",
        "updatedAt" : "2021-02-26T16:28:14Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      },
      {
        "id" : "6ded1b2b-3330-4826-a532-312197d62db3",
        "parentId" : "3c4f7fa4-afb6-466e-9dc6-64d07141e78e",
        "authorId" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "body" : "Got it, duh. should have checked the other condition before leaving an uninformed comment ðŸ™ƒ ",
        "createdAt" : "2021-02-02T18:23:39Z",
        "updatedAt" : "2021-02-26T16:28:14Z",
        "lastEditedBy" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "tags" : [
        ]
      }
    ],
    "commit" : "37287ea7bb95cf770fe5eccf9414e5f1e928409f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +403,407 @@                    scheduledRebalance, now, nextRequestTimeoutMs);\n        }\n        if (isLeader() && internalRequestValidationEnabled() && keyExpiration < Long.MAX_VALUE) {\n            nextRequestTimeoutMs = Math.min(nextRequestTimeoutMs, Math.max(keyExpiration - now, 0));\n            log.debug(\"Scheduled next key rotation at: {} (now: {} nextRequestTimeoutMs: {}) \","
  },
  {
    "id" : "276e3cdc-7166-4e0b-9fdf-f719a720fa95",
    "prId" : 10014,
    "prUrl" : "https://github.com/apache/kafka/pull/10014#pullrequestreview-615609982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a721f877-8134-4264-8be8-86633ed1b738",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "IIUC, `keyExpiration` is set to `Long.MAX_VALUE` above (line 365) when the key is to be rotated, but then after that is done this herder is the leader and its `keyExpiration` field is modified only through its own `ConfigUpdateListener`. But it's also possible that when all requests to be processed on lines 380-397 take less time than the config store requires to consume its own session key record, then this block is not entered until the next `tick()` call? If so, then isn't it possible that the tick blocks longer than the time until the next key expiration (assuming there are no further config updates or rebalance-inducing conditions)?",
        "createdAt" : "2021-03-09T16:40:41Z",
        "updatedAt" : "2021-03-09T16:51:14Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "e00a36d2-65af-4a28-b0d9-81732dda35ff",
        "parentId" : "a721f877-8134-4264-8be8-86633ed1b738",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "Good question--this shouldn't be possible since `KafkaConfigBackingStore::putSessionKey` does a flush and read-to-end after dispatching the new session key to the producer:\r\n\r\nhttps://github.com/apache/kafka/blob/0e8a84e5d7203765240894873fd2896906dfbc2b/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java#L456-L471",
        "createdAt" : "2021-03-09T16:59:28Z",
        "updatedAt" : "2021-03-09T16:59:28Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      },
      {
        "id" : "f282bdb8-459c-418c-9ef0-c7e44f341a6c",
        "parentId" : "a721f877-8134-4264-8be8-86633ed1b738",
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Actually, the `configLog.readToEnd().get(30000, TimeUnit.MILLISECONDS)` effective call only blocks up to 30 seconds for the consumer to read to the end. So it is *possible* for the `putSessionKey(...)` to return without the consumer having caught up and the session key notification method called. One example scenario is a network event after the producer successfully sends the record, but before the consumer is able to read to the end of the log, at which point the consumer continues retrying with effectively no timeout.\r\n\r\nIt still seems like it's possible in worker with no effective changes for it to block much longer than the key expiration. Obviously, if anything does cause the leader to wake up, it should be recover. But my question is what happens to one of the followers (which maybe isn't partitioned from the Kafka cluster) when they discover the session key is expired? Do they do nothing until they are called (again, assuming no change in membership and no config changes)?",
        "createdAt" : "2021-03-09T18:52:17Z",
        "updatedAt" : "2021-03-09T18:52:17Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "ecafdc5e-c8d5-4857-8e80-86d1b2d0567f",
        "parentId" : "a721f877-8134-4264-8be8-86633ed1b738",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "If the read to end doesn't complete after 30 seconds, the call doesn't return silently, but throws an exception. This in itself may not be great (we could catch exceptions caused by the call to `KafkaConfigBackingStore::putSessionKey` instead of allowing them to kill the entire worker), but it's still impossible for an accidental call to `WorkerGroupMember::poll` with `Long.MAX_VALUE` to happen under those circumstances.",
        "createdAt" : "2021-03-09T18:59:16Z",
        "updatedAt" : "2021-03-09T18:59:41Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      },
      {
        "id" : "9509100f-ed7b-4da0-8bde-78564a0a4a6b",
        "parentId" : "a721f877-8134-4264-8be8-86633ed1b738",
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Sounds good. Thanks for logging [KAFKA-12474](https://issues.apache.org/jira/browse/KAFKA-12474) and [KAFKA-12476](https://issues.apache.org/jira/browse/KAFKA-12476).",
        "createdAt" : "2021-03-18T16:23:19Z",
        "updatedAt" : "2021-03-18T16:23:19Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      }
    ],
    "commit" : "37287ea7bb95cf770fe5eccf9414e5f1e928409f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +403,407 @@                    scheduledRebalance, now, nextRequestTimeoutMs);\n        }\n        if (isLeader() && internalRequestValidationEnabled() && keyExpiration < Long.MAX_VALUE) {\n            nextRequestTimeoutMs = Math.min(nextRequestTimeoutMs, Math.max(keyExpiration - now, 0));\n            log.debug(\"Scheduled next key rotation at: {} (now: {} nextRequestTimeoutMs: {}) \","
  },
  {
    "id" : "6cef9c3a-dd02-4edd-a50a-0093498ab044",
    "prId" : 10822,
    "prUrl" : "https://github.com/apache/kafka/pull/10822#pullrequestreview-687791305",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc910169-0ac6-4bbe-aea0-51cbfaaac7ea",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "This method is where the worker restarts connector and task instances assigned to it. Lines 1153 and 1155, along with the `assignedIdsToRestart` are really the only evidence that this method does that. Can we modify the comments and log messages in this method to make that more evident, and maybe add JavaDoc?",
        "createdAt" : "2021-06-17T15:39:33Z",
        "updatedAt" : "2021-06-17T16:00:15Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "a7ff1c7b-ddae-4381-89f8-1a6569fc0780",
        "parentId" : "dc910169-0ac6-4bbe-aea0-51cbfaaac7ea",
        "authorId" : "094f6954-1557-4cfd-9696-2d85b89e8fea",
        "body" : "added javadoc and cleaned some logs and there are logs in the called methods so didn't added new logs",
        "createdAt" : "2021-06-19T03:32:23Z",
        "updatedAt" : "2021-06-19T03:32:23Z",
        "lastEditedBy" : "094f6954-1557-4cfd-9696-2d85b89e8fea",
        "tags" : [
        ]
      }
    ],
    "commit" : "27100a789ac7b44c8bc13a9e1e65713f340be9e0",
    "line" : 137,
    "diffHunk" : "@@ -1,1 +1138,1142 @@     * @param request the request to restart connector and tasks\n     */\n    protected synchronized void doRestartConnectorAndTasks(RestartRequest request) {\n        String connectorName = request.connectorName();\n        Optional<RestartPlan> maybePlan = buildRestartPlan(request);"
  },
  {
    "id" : "05f70a8d-69ed-47c6-9b8e-a5b1a51bbbe0",
    "prId" : 10822,
    "prUrl" : "https://github.com/apache/kafka/pull/10822#pullrequestreview-688946179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf1d2105-6129-4734-9c22-c050f81e5644",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "The `startConnector(...)` method _should_ handle most of the errors by calling the callback, but there are still a few potential errors that could happen before the worker actually tries starting the connector.\r\n\r\nDo you think this try-catch is needed, though, since the try-catch where this `doRestartConnectorAndTasks(...)` is called will catch and log any unexpected change? I guess keeping this will ensure we proceed with starting the tasks, rather than failing quickly.",
        "createdAt" : "2021-06-21T20:21:36Z",
        "updatedAt" : "2021-06-21T20:42:36Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      },
      {
        "id" : "5e2de7bd-1a0c-4093-8148-68029b6700e0",
        "parentId" : "cf1d2105-6129-4734-9c22-c050f81e5644",
        "authorId" : "094f6954-1557-4cfd-9696-2d85b89e8fea",
        "body" : "@rhauch  I kinda agree with you but I had seen this code in below two links and it had try/catch around both startTask and startConnector .  I added try/catch around startTask so if one tasks fails we can see if others atleasst succeed. The try/catch around startConnector was because I saw the pattern.  But I am torn on this, if you think we should remove the try/catch on connector, I am open to removing the try/catch given its already caught at the higher layer, but IMHO we can keep the startTask try/catch. Wdyt?\r\n\r\nhttps://github.com/apache/kafka/blob/1bd99c809b6546cd6711cc4f2d40785393ca584a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1437\r\nhttps://github.com/apache/kafka/blob/1bd99c809b6546cd6711cc4f2d40785393ca584a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1490",
        "createdAt" : "2021-06-21T22:40:23Z",
        "updatedAt" : "2021-06-21T23:04:27Z",
        "lastEditedBy" : "094f6954-1557-4cfd-9696-2d85b89e8fea",
        "tags" : [
        ]
      },
      {
        "id" : "1ae05e84-ee6f-4111-939b-ca029e0db263",
        "parentId" : "cf1d2105-6129-4734-9c22-c050f81e5644",
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Yeah, I think it's actually fine the way it is: with the try-catrch around `startConnector(...)`, because that helps us catch any problem _at that point_ and allows us to continue starting the tasks.\r\n\r\nIf we were to remove this try-catch around `startConnector(...)`, then any problem would cause us to return immediately from the method.",
        "createdAt" : "2021-06-21T23:42:01Z",
        "updatedAt" : "2021-06-21T23:42:01Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      }
    ],
    "commit" : "27100a789ac7b44c8bc13a9e1e65713f340be9e0",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +1178,1182 @@            } catch (Throwable t) {\n                log.error(\"Connector '{}' restart failed\", connectorName, t);\n            }\n        }\n        if (restartTasks) {"
  }
]