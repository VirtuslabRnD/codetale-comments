[
  {
    "id" : "7dec1b24-9f55-4f38-bedd-fc46cf4681bb",
    "prId" : 6283,
    "prUrl" : "https://github.com/apache/kafka/pull/6283#pullrequestreview-242010053",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9ce8f42-5814-4a54-baf8-11b60d154204",
        "parentId" : null,
        "authorId" : "1462ba0d-5f6b-4517-98de-68943d892c2b",
        "body" : "there are a couple of other places where we call `readToEnd()` and `poll()`  (links below). should we catch this exception in those places also?\r\n\r\nhttps://github.com/apache/kafka/blob/176ea0d0f9b4386cdc0e3b9bab560a11e3d3d516/connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java#L332\r\n\r\nhttps://github.com/apache/kafka/blob/176ea0d0f9b4386cdc0e3b9bab560a11e3d3d516/connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java#L153",
        "createdAt" : "2019-05-24T17:09:49Z",
        "updatedAt" : "2019-05-24T17:10:17Z",
        "lastEditedBy" : "1462ba0d-5f6b-4517-98de-68943d892c2b",
        "tags" : [
        ]
      },
      {
        "id" : "e5e89a55-f1a1-4c83-b1d7-7a80c2d75bbb",
        "parentId" : "f9ce8f42-5814-4a54-baf8-11b60d154204",
        "authorId" : "9af9fedf-5bac-44a0-895d-96aad0032528",
        "body" : "Thanks for the additional eyes @wicknicks !\r\n\r\nThe `readToLogEnd()` call is inside `start()`so my thinking was that failure would happen on startup which seems okay (if not desirable).  If there were broker availability issues it seems reasonable to me for Connect not to start.  If we want to try to be more robust we could come up with some retrying strategy, but I bet that would require changes in a number of other places.\r\n\r\nThe `poll()` method contains its own `catch (KafkaException e)` which is a super class of `TimeoutException` so we ought to be okay there:\r\n\r\nhttps://github.com/apache/kafka/blob/176ea0d0f9b4386cdc0e3b9bab560a11e3d3d516/connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java#L267",
        "createdAt" : "2019-05-25T17:24:49Z",
        "updatedAt" : "2019-05-25T17:25:37Z",
        "lastEditedBy" : "9af9fedf-5bac-44a0-895d-96aad0032528",
        "tags" : [
        ]
      }
    ],
    "commit" : "045131130aec48970581bcbb7353cdb01afffe74",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +314,318 @@                            readToLogEnd();\n                            log.trace(\"Finished read to end log for topic {}\", topic);\n                        } catch (TimeoutException e) {\n                            log.warn(\"Timeout while reading log to end for topic '{}'. Retrying automatically. \" +\n                                \"This may occur when brokers are unavailable or unreachable. Reason: {}\", topic, e.getMessage());"
  },
  {
    "id" : "0d502d3a-3a32-42d3-89a6-b8f33cfa8b65",
    "prId" : 6283,
    "prUrl" : "https://github.com/apache/kafka/pull/6283#pullrequestreview-274125721",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59bcc2b0-91b6-4317-a60d-86277c6f9232",
        "parentId" : null,
        "authorId" : "d840fbbb-c85a-43b7-b9ca-be0779ee4a6e",
        "body" : "I would suggest to backoff the retry, as the timeout probably means the unavailability of the resource (it might be the kafka broker itself, or the network, or something else), an immediate retry likely will not be success.",
        "createdAt" : "2019-08-12T21:22:49Z",
        "updatedAt" : "2019-08-12T21:23:35Z",
        "lastEditedBy" : "d840fbbb-c85a-43b7-b9ca-be0779ee4a6e",
        "tags" : [
        ]
      },
      {
        "id" : "bc1a8cf4-b091-41fb-88a8-039331d268fd",
        "parentId" : "59bcc2b0-91b6-4317-a60d-86277c6f9232",
        "authorId" : "9af9fedf-5bac-44a0-895d-96aad0032528",
        "body" : "In practice, actual retries to the network will still be beholden to the `request.timeout.ms` and `retry.backoff.ms` inside the consumer, so that isn't an issue here.",
        "createdAt" : "2019-08-12T23:47:40Z",
        "updatedAt" : "2019-08-12T23:47:40Z",
        "lastEditedBy" : "9af9fedf-5bac-44a0-895d-96aad0032528",
        "tags" : [
        ]
      },
      {
        "id" : "7845a333-68a7-4c6c-b527-c0e8d96c2a03",
        "parentId" : "59bcc2b0-91b6-4317-a60d-86277c6f9232",
        "authorId" : "d840fbbb-c85a-43b7-b9ca-be0779ee4a6e",
        "body" : "That's true, thanks for refreshing my mind. Then no need to backoff here.",
        "createdAt" : "2019-08-13T08:19:24Z",
        "updatedAt" : "2019-08-13T08:19:24Z",
        "lastEditedBy" : "d840fbbb-c85a-43b7-b9ca-be0779ee4a6e",
        "tags" : [
        ]
      }
    ],
    "commit" : "045131130aec48970581bcbb7353cdb01afffe74",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +317,321 @@                            log.warn(\"Timeout while reading log to end for topic '{}'. Retrying automatically. \" +\n                                \"This may occur when brokers are unavailable or unreachable. Reason: {}\", topic, e.getMessage());\n                            continue;\n                        } catch (WakeupException e) {\n                            // Either received another get() call and need to retry reading to end of log or stop() was"
  },
  {
    "id" : "81d6d2b2-25d3-4cfa-89e8-b1c4c966c52d",
    "prId" : 9347,
    "prUrl" : "https://github.com/apache/kafka/pull/9347#pullrequestreview-499067964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be9f4627-e6c3-4384-a11b-b7a27814e0d9",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Why not using ```partitionsFor(String topic, Duration timeout) ``` (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1944) to replace while loop?",
        "createdAt" : "2020-09-29T17:12:07Z",
        "updatedAt" : "2020-10-05T05:57:57Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "92959993-20ab-4b0d-bde6-d73c616ae14e",
        "parentId" : "be9f4627-e6c3-4384-a11b-b7a27814e0d9",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "I think the semantics needed here is different. The timeout in `partitionsFor` is the max amount of time the api can block waiting for response before it fails with `TimeoutException`. However, the api can return within timeout with empty results as newly created topics data has not been propagated yet. We then have to retry again until `partitionsFor` returns the partition data (upto a max time).",
        "createdAt" : "2020-09-29T19:21:12Z",
        "updatedAt" : "2020-10-05T05:57:57Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      },
      {
        "id" : "b263e6d6-0738-47e0-84eb-36ad74804f24",
        "parentId" : "be9f4627-e6c3-4384-a11b-b7a27814e0d9",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "It seems to me the behavior of ```timeout``` is not consistent in consumer methods. The ```timeout``` used by other methods (for example:  ```position```, ```offsetsForTimes```, ```beginningOffsets``` and ```endOffsets```) is to await the result of specify partitions. It means consumer will send a request again if the timer is not expired and the specify partition has no metadata (i.e topics data has not been propagated yet). Maybe ```partitionsFor``` should be fixed for consistent behavior.",
        "createdAt" : "2020-09-29T20:16:06Z",
        "updatedAt" : "2020-10-05T05:57:57Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "e07c8f67-07d8-4117-9480-43855cb6daa1",
        "parentId" : "be9f4627-e6c3-4384-a11b-b7a27814e0d9",
        "authorId" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "body" : "Not sure about this, no result for \"partitionInfos\" is a valid result. There is no point in automatic retrying. While for other apis that retry automatically, they do it if they get an invalid result back. If `null` was a valid result for them, they shouldn't retry either. ",
        "createdAt" : "2020-09-29T20:40:50Z",
        "updatedAt" : "2020-10-05T05:57:57Z",
        "lastEditedBy" : "e61d770a-e328-41b4-b8c3-6a769370680c",
        "tags" : [
        ]
      },
      {
        "id" : "e0b87842-c8c5-425a-993c-a3d03193f097",
        "parentId" : "be9f4627-e6c3-4384-a11b-b7a27814e0d9",
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "@soondenana Thanks for responses. This patch LGTM. What I want to discuss is unrelated to this patch.\r\n\r\n> they do it if they get an invalid result back. If null was a valid result for them, they shouldn't retry either.\r\n\r\nIf the topic is not exist (or not been propagated yet), ```partitionFor``` can return null. By contrast, ```beginningOffsets``` (and other methods) throws ```TimeoutException```. In order to make consistent behavior, ```beginningOffsets``` (and other methods) should let the topic (or partition) be absent in the returned ```Map```. WDYT?",
        "createdAt" : "2020-09-30T03:57:16Z",
        "updatedAt" : "2020-10-05T05:57:57Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "4054146c69b14a92c53066830f8f732bf446bac4",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +141,145 @@            time.sleep(sleepMs);\n            sleepMs = Math.min(2 * sleepMs, MAX_SLEEP_MS);\n            partitionInfos = consumer.partitionsFor(topic);\n        }\n        if (partitionInfos == null)"
  },
  {
    "id" : "b00f0df1-f6d2-4e25-b674-6a3e418206bf",
    "prId" : 10158,
    "prUrl" : "https://github.com/apache/kafka/pull/10158#pullrequestreview-593849373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "409b1117-0646-4348-a987-12cdeed6ea21",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "`readToLogEnd` is called by `start`. Should we add similar exception handle for that? ",
        "createdAt" : "2021-02-19T03:46:30Z",
        "updatedAt" : "2021-02-19T15:57:59Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "f5062094-846b-4a95-9fc4-faa1f2df18a9",
        "parentId" : "409b1117-0646-4348-a987-12cdeed6ea21",
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "The previous `readToLogEnd()` that used the consumer did not have any special exception handling in the `start()` method. Any problem to start seems like it should propagate up to result in the worker failing. Especially when coupled with your fix in #10152.\r\n\r\nI added this here because retriable exceptions should not stop the KafkaBasedLog's thread.",
        "createdAt" : "2021-02-19T04:26:40Z",
        "updatedAt" : "2021-02-19T15:57:59Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d8f47aa5b71f254cd86f5a4ce15559e74c0df99",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +402,406 @@                                     \"This may occur when brokers are unavailable or unreachable. Reason: {}\", topic, e.getMessage());\n                            continue;\n                        } catch (RetriableException | org.apache.kafka.connect.errors.RetriableException e) {\n                            log.warn(\"Retriable error while reading log to end for topic '{}'. Retrying automatically. \" +\n                                     \"Reason: {}\", topic, e.getMessage());"
  }
]