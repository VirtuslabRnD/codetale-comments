[
  {
    "id" : "76c43726-1777-4006-8a60-24196b485031",
    "prId" : 7771,
    "prUrl" : "https://github.com/apache/kafka/pull/7771#pullrequestreview-327049764",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc742392-b2f5-4598-b62c-f188ce47d212",
        "parentId" : null,
        "authorId" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "body" : "Shouldn't the workers discover that the coordinator is unavailable while it is down?\r\nI'm imagining this test going like this:\r\n\r\n1. steady-state workers are running\r\n2. brokers stop\r\n3. workers discover the coordinator is unavailable \r\n4. workers stop their tasks\r\n5. brokers start\r\n6. workers discover the next coordinator \r\n7. workers start their tasks\r\n8. workers are running unaffected",
        "createdAt" : "2019-12-04T17:26:30Z",
        "updatedAt" : "2019-12-04T18:07:06Z",
        "lastEditedBy" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "tags" : [
        ]
      },
      {
        "id" : "0e9835b6-cea8-4f81-9783-b56db77978c7",
        "parentId" : "fc742392-b2f5-4598-b62c-f188ce47d212",
        "authorId" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "body" : "That was a bit misplaced, because I actually need 3 explicit delays (due to current lack of appropriate handles from the kafka and connect embedded clusters). \r\n1. Bring kafka down, allow workers to discover it's down (heartbeat * 2 + 4 sec)\r\n2. Allow for Kafka to come back up\r\n3. Allow for worker cluster to stabilize after the very last rebalance (delay = 5sec) \r\n\r\nAdded another commit. ",
        "createdAt" : "2019-12-04T18:12:06Z",
        "updatedAt" : "2019-12-04T18:12:06Z",
        "lastEditedBy" : "e77724e7-3db9-47d6-b4d8-a865b1d06edc",
        "tags" : [
        ]
      }
    ],
    "commit" : "a031894a59ce13839b6e5d54bf22152fc5640f88",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +232,236 @@\n        // Allow for the kafka brokers to come back online\n        Thread.sleep(TimeUnit.SECONDS.toMillis(10));\n\n        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),"
  },
  {
    "id" : "df7abf61-87cf-4c8f-b2fd-cdf2ee66061f",
    "prId" : 8118,
    "prUrl" : "https://github.com/apache/kafka/pull/8118#pullrequestreview-367616883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36d061e9-2f65-4993-8116-7309f9405963",
        "parentId" : null,
        "authorId" : "4873aa72-3842-44ba-91a8-a59ea9830460",
        "body" : "Will the DESTROYED tasks show up in REST so that we can add an assertion to make sure they are not UNASSIGNED?",
        "createdAt" : "2020-03-02T23:02:13Z",
        "updatedAt" : "2020-05-21T20:31:11Z",
        "lastEditedBy" : "4873aa72-3842-44ba-91a8-a59ea9830460",
        "tags" : [
        ]
      },
      {
        "id" : "c59bc12f-b4a5-4921-b25a-91b4d5f526e2",
        "parentId" : "36d061e9-2f65-4993-8116-7309f9405963",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "The framework doesn't actually explicitly write the `DESTROYED` state to the status backing store, it just writes a null-valued record: https://github.com/apache/kafka/blob/d3f9cb5cd37486e33cb90a6ec7eb382044ba1e51/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaStatusBackingStore.java#L285\r\n\r\nThat information is surfaced via the REST API by the lack of the presence of that task in the status info for the connector. So verifying exactly the number of tasks should also verify that the expected number of deleted tasks were removed from the status backing store.",
        "createdAt" : "2020-03-02T23:13:13Z",
        "updatedAt" : "2020-05-21T20:31:11Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      },
      {
        "id" : "eff02495-5e90-46d4-bf84-f93341297c49",
        "parentId" : "36d061e9-2f65-4993-8116-7309f9405963",
        "authorId" : "4873aa72-3842-44ba-91a8-a59ea9830460",
        "body" : "Makes sense. And I kind of figured that is likely the reason for lack of explicit assertion there.",
        "createdAt" : "2020-03-03T00:00:15Z",
        "updatedAt" : "2020-05-21T20:31:11Z",
        "lastEditedBy" : "4873aa72-3842-44ba-91a8-a59ea9830460",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bd4bc32249c50608039b95ffe8ab338a9c2126b",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +270,274 @@        connectorProps.put(TASKS_MAX_CONFIG, String.valueOf(decreasedNumTasks));\n        connect.configureConnector(CONNECTOR_NAME, connectorProps);\n        connect.assertions().assertConnectorAndExactlyNumTasksAreRunning(CONNECTOR_NAME, decreasedNumTasks, \"Connector task statuses did not update in time.\");\n    }\n}"
  },
  {
    "id" : "4fff5e03-52bb-44b8-a60a-dc05316d208d",
    "prId" : 10016,
    "prUrl" : "https://github.com/apache/kafka/pull/10016#pullrequestreview-581355573",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d8f2874-91bc-4e8c-8a45-42f333ec39ea",
        "parentId" : null,
        "authorId" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "body" : "Oh, the failure strings are throwing me for a loop. I thought this was encoding the infinite hanging as an intended behavior, which is ridiculous. \r\n\r\nIs this test asserting that without topic auto-creation enabled, the task stops (fails?) in a timely manner? Could you add a comment to that effect? The name of the test doesn't tell me the expected behavior, only the setup conditions.",
        "createdAt" : "2021-02-01T22:51:07Z",
        "updatedAt" : "2021-02-26T16:56:26Z",
        "lastEditedBy" : "12b658fc-2aaf-48e9-ad8b-15fd33dc8321",
        "tags" : [
        ]
      },
      {
        "id" : "5f4aa775-2ca9-4345-981f-33f1d3ce0241",
        "parentId" : "2d8f2874-91bc-4e8c-8a45-42f333ec39ea",
        "authorId" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "body" : "It's asserting that shutdown is not blocked indefinitely when we hit the topic-creation-disabled scenario, but not that the task fails. I'll try to make things clearer.",
        "createdAt" : "2021-02-02T13:45:59Z",
        "updatedAt" : "2021-02-26T16:56:26Z",
        "lastEditedBy" : "f1480a85-4082-46f9-89c6-fe7231733b83",
        "tags" : [
        ]
      }
    ],
    "commit" : "a442a0a2cc5bfe8287edf9b460d5ff68d04a236a",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +320,324 @@        // Then if we delete the connector, it and each of its tasks should be stopped by the framework\n        // even though the producer is blocked because there is no topic\n        StartAndStopLatch stopCounter = connector.expectedStops(1);\n        connect.deleteConnector(CONNECTOR_NAME);\n"
  },
  {
    "id" : "3acabb38-f851-4ced-ab80-d342ab2b395a",
    "prId" : 10016,
    "prUrl" : "https://github.com/apache/kafka/pull/10016#pullrequestreview-599712472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41f0663e-69a4-469b-903d-9b1db23ef864",
        "parentId" : null,
        "authorId" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "body" : "Nit: add a comment with the second important test condition\r\n```suggestion\r\n        // and when the connector is not configured to create topics\r\n        Map<String, String> props = defaultSourceConnectorProps(\"nonexistenttopic\");\r\n```",
        "createdAt" : "2021-02-26T16:26:36Z",
        "updatedAt" : "2021-02-26T16:56:26Z",
        "lastEditedBy" : "145db0de-7396-4643-9a2c-9977e1c6219b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a442a0a2cc5bfe8287edf9b460d5ff68d04a236a",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +306,310 @@\n        // and when the connector is not configured to create topics\n        Map<String, String> props = defaultSourceConnectorProps(\"nonexistenttopic\");\n        props.remove(DEFAULT_TOPIC_CREATION_PREFIX + REPLICATION_FACTOR_CONFIG);\n        props.remove(DEFAULT_TOPIC_CREATION_PREFIX + PARTITIONS_CONFIG);"
  }
]