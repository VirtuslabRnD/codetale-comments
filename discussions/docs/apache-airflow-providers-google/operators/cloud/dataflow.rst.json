[
  {
    "id" : "f993845f-797f-4cbc-9792-59122212a273",
    "prId" : 13461,
    "prUrl" : "https://github.com/apache/airflow/pull/13461#pullrequestreview-565291801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48ffcc99-5c68-4b83-a9e2-a9d3f5572e52",
        "parentId" : null,
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "Personally, I wouldn't consider streaming as another execution model.",
        "createdAt" : "2021-01-07T20:50:58Z",
        "updatedAt" : "2021-01-19T07:38:16Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "185cd343-23f3-4dad-83d2-a7810c05ca3f",
        "parentId" : "48ffcc99-5c68-4b83-a9e2-a9d3f5572e52",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "It is based on the Dataflow documentation:\r\n\r\nhttps://cloud.google.com/dataflow/docs/guides/specifying-exec-params#configuring-pipelineoptions-for-execution-on-the-cloud-dataflow-service",
        "createdAt" : "2021-01-11T12:03:19Z",
        "updatedAt" : "2021-01-19T07:38:16Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3d9d9b04d48ed4b307e8dc918bcae522534ba99",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +131,135 @@\nDataflow has multiple options of executing pipelines. It can be done in the following modes:\nbatch asynchronously (fire and forget), batch blocking (wait until completion), or streaming (run indefinitely).\nIn Airflow it is best practice to use asynchronous batch pipelines or streams and use sensors to listen for expected job state.\n"
  },
  {
    "id" : "d14b7354-2443-4d2e-a64b-8f9ed6c401e4",
    "prId" : 13461,
    "prUrl" : "https://github.com/apache/airflow/pull/13461#pullrequestreview-566195974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e274c18-4897-418e-96b2-325398cd1bf8",
        "parentId" : null,
        "authorId" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "body" : "Would this not require \"gcloud\" as a depedency pre-installed in the airflow worker nodes? (similar to the JRE or python requirements above)",
        "createdAt" : "2021-01-12T01:30:15Z",
        "updatedAt" : "2021-01-19T07:38:16Z",
        "lastEditedBy" : "faa559fa-1122-468b-8ebd-7d04c5c97279",
        "tags" : [
        ]
      },
      {
        "id" : "0c971cbb-36d8-4022-b541-d07a5aa9e5b5",
        "parentId" : "7e274c18-4897-418e-96b2-325398cd1bf8",
        "authorId" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "body" : "It seems to me that only SQL Operator requires Google Cloud CLI to be installed.",
        "createdAt" : "2021-01-12T08:55:36Z",
        "updatedAt" : "2021-01-19T07:38:16Z",
        "lastEditedBy" : "07638d17-cc8b-40a4-abdc-7b39759362ab",
        "tags" : [
        ]
      },
      {
        "id" : "0614d3a0-aa9a-47ed-a6f7-732fe9bab9de",
        "parentId" : "7e274c18-4897-418e-96b2-325398cd1bf8",
        "authorId" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "body" : "@mik-laj  is right. Only `DataflowStartSqlJobOperator` requires `gcloud`.\r\n\r\nI added warning in `DataflowStartSqlJobOperator` section and operator itself about required `gcloud` SDK",
        "createdAt" : "2021-01-12T11:53:51Z",
        "updatedAt" : "2021-01-19T07:38:16Z",
        "lastEditedBy" : "684d7067-ead9-4707-b609-f796c2e1f8ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3d9d9b04d48ed4b307e8dc918bcae522534ba99",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +51,55 @@    files in Cloud Storage, creates a template file (similar to job request),\n    and saves the template file in Cloud Storage. See: :ref:`howto/operator:DataflowTemplatedJobStartOperator`\n  - **Flex Templates**. Developers package the pipeline into a Docker image and then use the ``gcloud``\n    command-line tool to build and save the Flex Template spec file in Cloud Storage. See:\n    :ref:`howto/operator:DataflowStartFlexTemplateOperator`"
  }
]