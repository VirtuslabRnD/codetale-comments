[
  {
    "id" : "946671c7-7366-4404-85b9-99da3faf8e85",
    "prId" : 11367,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "237ced7f-5bd5-4559-b768-34ed0b1fe979",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "insert \"(i.e. etcd's data directory)\"\n",
        "createdAt" : "2015-07-16T19:57:46Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "ca54365f-83fa-4562-b04f-128972f572da",
        "parentId" : "237ced7f-5bd5-4559-b768-34ed0b1fe979",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "done\n",
        "createdAt" : "2015-07-16T21:21:50Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c57c87767097152b5afcf6177f79b9c5f95da673",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +64,68 @@      - unable to stop, update, or start new pods, services, replication controller\n      - existing pods and services should continue to work normally, unless they depend on the Kubernetes API\n  - Apiserver backing storage lost\n    - Results\n      - apiserver should fail to come up"
  },
  {
    "id" : "c0ec6155-bc81-4186-a524-cbf329e2ed25",
    "prId" : 11367,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06a196b3-6fe3-4431-8f6d-85875af1d6b2",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Maybe insert:\n\n```\n  - Network partition\n    - Results\n      - Partition A thinks the nodes in the partition B are down; Partition B thinks the apiserver is down. (Assuming the master VM ends up in partition A.)\n```\n",
        "createdAt" : "2015-07-16T20:03:04Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "edaaf8de-6427-43af-ab4a-2ed5525fbbd5",
        "parentId" : "06a196b3-6fe3-4431-8f6d-85875af1d6b2",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "done\n",
        "createdAt" : "2015-07-16T21:22:31Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c57c87767097152b5afcf6177f79b9c5f95da673",
    "line" : null,
    "diffHunk" : "@@ -1,1 +75,79 @@  - Individual node (VM or physical machine) shuts down\n    - Results\n      - pods on that Node stop running\n  - Network partition\n    - Results"
  },
  {
    "id" : "33aedefe-91b4-4610-852e-cbc3c1736c50",
    "prId" : 11367,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "245f29bc-aeae-4620-a69b-91b840c75750",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Kubelet or docker?\n",
        "createdAt" : "2015-07-16T20:03:35Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "55a6c067-a1e5-4ca0-984f-0c7d3e194d20",
        "parentId" : "245f29bc-aeae-4620-a69b-91b840c75750",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "These are Kubelet, right? Or am I missing something?\n",
        "createdAt" : "2015-07-16T21:22:44Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c57c87767097152b5afcf6177f79b9c5f95da673",
    "line" : null,
    "diffHunk" : "@@ -1,1 +79,83 @@    - Results\n      - partition A thinks the nodes in partition B are down; partition B thinks the apiserver is down. (Assuming the master VM ends up in partition A.)\n  - Kubelet software fault\n    - Results\n      - crashing kubelet cannot start new pods on the node"
  },
  {
    "id" : "98eda913-c480-4cdf-a22e-d5aed2aafb03",
    "prId" : 11367,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9589ad0-ee1d-498c-90a1-333d4dfae7ed",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Isn't this section really super obvious?\n",
        "createdAt" : "2015-07-16T20:04:10Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "f8af074e-348b-4a9c-8c76-0b0bc5602e80",
        "parentId" : "a9589ad0-ee1d-498c-90a1-333d4dfae7ed",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Yes, but it might remind someone to consider whether _they_ are the problem. :)\n",
        "createdAt" : "2015-07-16T21:23:14Z",
        "updatedAt" : "2015-07-16T21:39:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "7fb9baa2-6bf9-4fc2-9bf0-63ba673f2e54",
        "parentId" : "a9589ad0-ee1d-498c-90a1-333d4dfae7ed",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "I originally put this section in because I wanted people to think about how operator error can make a single multi-zone cluster significantly less available than multiple clusters.\n",
        "createdAt" : "2015-07-16T23:53:48Z",
        "updatedAt" : "2015-07-16T23:53:48Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "d878cb22-2432-47cb-a7a9-26590d47fb26",
        "parentId" : "a9589ad0-ee1d-498c-90a1-333d4dfae7ed",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Ah, that makes sense.... a bit less so now that it's moved out of that doc :)\n",
        "createdAt" : "2015-07-16T23:55:23Z",
        "updatedAt" : "2015-07-16T23:55:23Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "c57c87767097152b5afcf6177f79b9c5f95da673",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +85,89 @@      - node marked unhealthy\n      - replication controllers start new pods elsewhere\n  - Cluster operator error\n    - Results\n      - loss of pods, services, etc"
  }
]