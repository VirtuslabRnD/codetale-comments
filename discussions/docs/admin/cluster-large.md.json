[
  {
    "id" : "69a72ee7-618e-4c2c-a5f1-9b18d28060e1",
    "prId" : 21176,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9476783-92c7-4d8b-a932-29fc8f24e97e",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "@dchen1107 Will this still be true after https://github.com/kubernetes/kubernetes/issues/21086?\n",
        "createdAt" : "2016-02-12T23:19:26Z",
        "updatedAt" : "2016-02-16T09:24:29Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "e545eb93-3308-4f5a-9235-4efc653eb85c",
        "parentId" : "a9476783-92c7-4d8b-a932-29fc8f24e97e",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "We did really hardcap CPU since we didn't config the kernel with CONFIG_FAIR_GROUP_SCHED. If we do nothing for addons after #21086, the statement is truely reflect the situation this time. If I decide to make addons burstable at the end, we should change the statement here. But anyway, we are not worse than today. :-P\n",
        "createdAt" : "2016-02-13T00:37:07Z",
        "updatedAt" : "2016-02-16T09:24:29Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a817c7156767f2372486e5ab76f7638745cd773",
    "line" : null,
    "diffHunk" : "@@ -1,1 +81,85 @@### Addon Resources\n\nTo prevent memory leaks or other resource issues in [cluster addons](../../cluster/addons/) from consuming all the resources available on a node, Kubernetes sets resource limits on addon containers to limit the CPU and Memory resources they can consume (See PR [#10653](http://pr.k8s.io/10653/files) and [#10778](http://pr.k8s.io/10778/files)).\n\nFor [example](../../cluster/saltbase/salt/fluentd-gcp/fluentd-gcp.yaml):"
  },
  {
    "id" : "3da2d519-7c2e-4d59-9f28-3f9bf577808c",
    "prId" : 21176,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14c6caeb-0700-4346-a515-b02bcc5093ee",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This paragraph LGTM\n",
        "createdAt" : "2016-02-15T08:13:17Z",
        "updatedAt" : "2016-02-16T09:24:29Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "f26f7598-fca5-40f9-ae8d-0faae28d3917",
        "parentId" : "14c6caeb-0700-4346-a515-b02bcc5093ee",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Actually there are two flags that need to be flipped. `ALLOWED_NOTREADY_NODES` is an obvious one, second is `EXIT_ON_WEAK_ERROR` as a general flag to allow successful startup when something is _probably_ wrong. I can remove/rename it it you think it's not necessary.\n",
        "createdAt" : "2016-02-15T09:08:09Z",
        "updatedAt" : "2016-02-16T09:24:29Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "8ac64fc1-bb48-4122-b9db-d7772fc671e6",
        "parentId" : "14c6caeb-0700-4346-a515-b02bcc5093ee",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Yeah, I think we should default `EXIT_ON_WEAK_ERROR` to false (except in the flaky suite version of the test). As long as we default ALLOWED_NOTREADY_NODES to 0, then people will get the same behavior as they get today without setting any flags, and if they want to allow minor node failure they only have to change ALLOWED_NOTREADY_NODES.\n",
        "createdAt" : "2016-02-15T09:26:42Z",
        "updatedAt" : "2016-02-16T09:24:29Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a817c7156767f2372486e5ab76f7638745cd773",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +124,128 @@with. This will allow `kube-up.sh` to succeed with fewer than `NUM_NODES` coming up. Depending on the\nreason for the failure, those additional nodes may join later or the cluster may remain at a size of\n`NUM_NODES - ALLOWED_NOTREADY_NODES`.\n\n<!-- BEGIN MUNGE: GENERATED_ANALYTICS -->"
  }
]