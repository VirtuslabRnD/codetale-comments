[
  {
    "id" : "0805bdfd-9662-41ea-8fb1-776ec7e94269",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4564520f-1b5f-4243-9846-b7dda80359d8",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Hmm, I think you're placing un-necessary emphasis on the geographic routing aspect. If you simply choose an endpoint based on health check rtt, won't that suffice (or rather, you simply expose whatever loadbalancing algorightms the underlying lb supports and let the user pick one)? \n\nThere are cases where one might really want the _closest_ endpoint (eg: you want to terminate ssl closest to the user because of TLS negotiation and then send http bouncing wherever there is capacity), and if that's what you're prioritizing, please be explicit. \n",
        "createdAt" : "2016-01-09T02:56:00Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "887cf9d4-5b6f-4699-af62-c83326c5d976",
        "parentId" : "4564520f-1b5f-4243-9846-b7dda80359d8",
        "authorId" : null,
        "body" : "Yes, RTT is a good proxy for distance in many cases, and so the implementation might be simply that.  But there are actually multiple reasons why \"close\" is good, beyond pure average latency reasons like the TLS example you cite.  For example, bandwidth charges ($$) within a zone are typically zero, whereas across zones they can be very substantial (etune@ once did a calc to demonstrate how they can easily outweigh CPU charges by several orders of magnitude. Also, it's well-known fact that multi-tier distributed applications suffer disproportionately at the tail end of latency (e.g. a frontend often needs to wait for the slowest of all of it's backends, transitively, so the 99th percentile latency of the backends can easily become the median of the frontend).  Here's a pretty good article describing the phenomenon http://highscalability.com/blog/2012/3/12/google-taming-the-long-latency-tail-when-more-machines-equal.html.  \n",
        "createdAt" : "2016-01-09T17:24:29Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "6ee3fcd5-0647-424d-a0b0-15562ebf669c",
        "parentId" : "4564520f-1b5f-4243-9846-b7dda80359d8",
        "authorId" : null,
        "body" : "@bprashanth ^^\n",
        "createdAt" : "2016-01-09T17:30:11Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "23c44e56-d874-441a-a9f2-5add30c5ff76",
        "parentId" : "4564520f-1b5f-4243-9846-b7dda80359d8",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Are we going to decide all of this on behalf of the user? or just expose it somehow. I don't think we can do the former because only the user is aware of things like write propogation latency/consistency semantics of their app. If we're going to expose it, how (other than just saying here are the good old loadbalancing algorithms gce/aws/nginx supports, pick one)? are you going to do something like profiles (cost effective, maximized availability etc)?\n\nThis feels complicated enough to stay away from till v1 is out and we have feedback. \n",
        "createdAt" : "2016-01-09T21:29:26Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "f20d2247-4cb0-490b-8a25-98a8ef7ee18d",
        "parentId" : "4564520f-1b5f-4243-9846-b7dda80359d8",
        "authorId" : null,
        "body" : "Yes, agreed.  For v1, internal kubernetes clients will find endpoints local to their cluster in preference to those in remote clusters.  The rest can wait until later.\n",
        "createdAt" : "2016-01-12T02:50:05Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "c5854753-c141-4831-b0a8-1002e554dd4c",
        "parentId" : "4564520f-1b5f-4243-9846-b7dda80359d8",
        "authorId" : null,
        "body" : "@bprashanth ^^ (I forgot to \"at\" you).\n",
        "createdAt" : "2016-01-12T02:54:51Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : null,
    "diffHunk" : "@@ -1,1 +55,59 @@      Message Busses (Java, TLS), DNS servers (UDP),\n      SIP servers and databases)\n1. **Find the \"best\" endpoint:** Upon initial discovery and\n   connection, both internal and external clients should ideally find\n   \"the best\" endpoint if multiple eligible endpoints exist.  \"Best\""
  },
  {
    "id" : "b4ab16bc-496a-4c05-848a-82084df062d6",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27465cf1-41e9-4462-b0e7-b75e5c6267ac",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Is the omission of Azure in this doc intentional?\n",
        "createdAt" : "2016-01-09T08:19:10Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "19a902ce-83d4-41e0-bb8d-b06fd3c1a3ee",
        "parentId" : "27465cf1-41e9-4462-b0e7-b75e5c6267ac",
        "authorId" : null,
        "body" : "No, not at all.  I would put it at number 4 on the priority list, below GCE, AWS and basic bare metal.  I don't know too much about Azure, so it was expedient to focus on the first three, about which I do know something :-)\n",
        "createdAt" : "2016-01-09T17:13:50Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +38,42 @@## Requirements\n\n### Discovery, Load-balancing and Failover\n\n1. **Internal discovery and connection**: Pods/containers (running in"
  },
  {
    "id" : "426e2f2e-b648-4d6e-b251-bf68e10c39ee",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd71f9d6-9ef1-4890-b858-7036a0fa4326",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "So you are not going to introduce new federation-specific abstractions, but rather reuse the per-cluster abstractions we already have? I'm not objecting to this, but I saw in the third doc in this PR some mention of MultiHomeXXX so I wasn't sure.\n",
        "createdAt" : "2016-01-09T08:20:52Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "457a6ee6-3323-455f-b2ad-f0af10f56f91",
        "parentId" : "bd71f9d6-9ef1-4890-b858-7036a0fa4326",
        "authorId" : null,
        "body" : "@davidopp You're right. I need to remove the MultiHome stuff from the other doc.  It's out of date.\n",
        "createdAt" : "2016-01-09T17:31:12Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "0652a80c-c30d-4dc5-810d-19c1df1f7e02",
        "parentId" : "bd71f9d6-9ef1-4890-b858-7036a0fa4326",
        "authorId" : null,
        "body" : "Done.\n",
        "createdAt" : "2016-01-12T02:56:33Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +103,107 @@Controller, Service etc) should be able to be successfully deployed\ninto any Kubernetes Cluster or Ubernetes Federation of Clusters,\nwithout modification.  More specifically, a typical configuration\nshould work correctly (although possibly not optimally) across any of\nthe following environments:"
  },
  {
    "id" : "26c1c022-4ca2-4a34-a688-a24f3804a42b",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7b5cb58-50b8-4688-bd94-dce9ed6fbdce",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Where was the previous example running?\n",
        "createdAt" : "2016-01-09T08:21:41Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "75661dbe-5766-482b-a450-60c495af9606",
        "parentId" : "d7b5cb58-50b8-4688-bd94-dce9ed6fbdce",
        "authorId" : null,
        "body" : "@davidopp Anywhere.  Only it did not have a GCE Global Load Balancer, and relied instead on DNS.\n",
        "createdAt" : "2016-01-09T17:38:31Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 395,
    "diffHunk" : "@@ -1,1 +393,397 @@balanced across the pods comprising the service in each cluster.\n\nIn a more sophisticated configuration (e.g. on GCE or GKE), Ubernetes\nautomatically creates a\n[GCE Global L7 Load Balancer](https://cloud.google.com/compute/docs/load-balancing/http/global-forwarding-rules)"
  },
  {
    "id" : "79718fcf-9df1-48f0-964e-cfc76942bca4",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f8ea037-6a2f-4aa6-a217-00d770298536",
        "parentId" : null,
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "I know Kubernetes can't dictate how administrators should set up their DNS, especially in certain enterprise environments, but what would be the canonical way, in a federated world and a mixed one?\n\nRight now, with the SkyDNS addon you get `my-service.my-namespace.svc.cluster.local` or `my-service.my-namespace.svc.my-domain.com`, through a DNS server that is local to the Kubernetes cluster. What if you expand to two clusters and don't federate yet? Just stick with `my-domain.com`, since the DNS servers are still local at this point and an EU pod can't look up names through the US resolver anyway? Or set `cluster-domain` to e.g. `us.my-domain.com` and `eu.my-domain.com` in each, resulting in `my-service.my-namespace.svc.eu.my-domain.com`, perhaps reusing the same plumbing that federation uses to modifiy GCD/Route53 entries?\n\nI'm asking because the migration path(s) to a federated setup might have roadblocks. E.g., in some cases you'd have to prevent someone from creating a federation named \"svc\" or \"pod\", to avoid name clashes with existing SkyDNS names. Not that they're likely choices, but still. :-) And, last but not least, some consistency across federated and non-federated names would help keep everyone sane.\n",
        "createdAt" : "2016-01-14T17:23:07Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      },
      {
        "id" : "38b3e5fb-a356-4f24-a401-edf1a11d4e67",
        "parentId" : "9f8ea037-6a2f-4aa6-a217-00d770298536",
        "authorId" : null,
        "body" : "@therc Yes, agreed, this doc is not at all clear about the details of the various proposed DNS domains, where they're authoritatively served from etc, and requires additional clarity.  I'll probably defer that to a separate PR.\n",
        "createdAt" : "2016-01-19T20:40:03Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 390,
    "diffHunk" : "@@ -1,1 +388,392 @@    my-service.my-namespace.my-federation.my-domain.com 180 IN\tA 104.197.74.77\n    my-service.my-namespace.my-federation.my-domain.com 180 IN\tA 104.197.38.157\n\nEach of the above IP addresses (which are just the external load\nbalancer ingress IP's of each cluster service) is of course load"
  },
  {
    "id" : "359de6db-6635-457c-aa28-faa384b854a7",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a71545b5-8b19-47a2-9919-0ba0b63dfab3",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "What about the use case for clients that are external to the cluster but still inside to the same internal network? e.g. VMs on GCE/AWS or other machines on prem for a bare metal install. Those clients may not need to use https and we want a way to allow them to connect to services without needing to create an endpoint that is visible to the internet. \n",
        "createdAt" : "2016-01-19T18:39:32Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "fd5e033a-de57-4aa3-a92b-dd399c037384",
        "parentId" : "a71545b5-8b19-47a2-9919-0ba0b63dfab3",
        "authorId" : null,
        "body" : "@roberthbailey You make a good point, although \"external\" in this case need not imply an endpoint that is visible to the internet, only that it is accessible from outside the kubernetes cluster. Firewall rules on the load balancer could ensure that it is not visible to the internet.  I'll double-check the feasibility of this and add some detail to clarify here.  \n",
        "createdAt" : "2016-01-19T20:34:22Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +50,54 @@   outside a Kubernetes cluster) must be able to discover and connect\n   to endpoints for Kubernetes services on which they depend.\n   1. **External clients predominantly speak HTTP(S)**: External\n      clients are most often, but not always, web browsers, or at\n      least speak HTTP(S) - notable exceptions include Enterprise"
  },
  {
    "id" : "456d1fe1-bc87-4aaa-b800-695ed632a823",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ab93831-8c56-45b5-b01d-7e257085b28b",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Missing a closing paren after CloudFlare\n",
        "createdAt" : "2016-01-19T18:44:38Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "d15df6ee-89c0-4237-9426-d5720cc0f084",
        "parentId" : "4ab93831-8c56-45b5-b01d-7e257085b28b",
        "authorId" : null,
        "body" : "Done.\n",
        "createdAt" : "2016-02-27T00:24:49Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 167,
    "diffHunk" : "@@ -1,1 +165,169 @@1. [Google Cloud DNS](https://cloud.google.com/dns) (or any other\n   programmable DNS service, like\n   [CloudFlare](http://www.cloudflare.com) can be used to route\n   traffic between regions (and between different cloud providers and\n   on-premise clusters, as it's plain DNS, IP only). Google Cloud DNS"
  }
]