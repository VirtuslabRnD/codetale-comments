[
  {
    "id" : "2aa61305-9cce-4612-8ff3-36f12c81ad24",
    "prId" : 7987,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0a1dd0b-6b15-43b3-934b-fe2fc909b6e3",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "There are two possible admission plugins, right?  Mention both?\n",
        "createdAt" : "2015-05-10T14:56:20Z",
        "updatedAt" : "2015-05-10T14:56:20Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "64300f79-9ff8-4835-9ee9-ac038339dc6d",
        "parentId" : "b0a1dd0b-6b15-43b3-934b-fe2fc909b6e3",
        "authorId" : "fb0525d4-54d0-4bfa-8b41-17f8b31b43fd",
        "body" : "We decided to go with a single admission plugin for now https://github.com/GoogleCloudPlatform/kubernetes/pull/7343#discussion_r29300725\n",
        "createdAt" : "2015-05-10T18:48:12Z",
        "updatedAt" : "2015-05-10T18:48:12Z",
        "lastEditedBy" : "fb0525d4-54d0-4bfa-8b41-17f8b31b43fd",
        "tags" : [
        ]
      },
      {
        "id" : "c64d5087-d8d0-4332-9537-042e79e4d702",
        "parentId" : "b0a1dd0b-6b15-43b3-934b-fe2fc909b6e3",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Lgtm\nOn May 10, 2015 11:48 AM, \"Paul\" notifications@github.com wrote:\n\n> In docs/design/security_context.md\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/7987#discussion_r30004776\n> :\n> \n> > +It is up to an admission plugin to determine if the security context is acceptable or not.  At the\n> \n> We decided to go with a single admission plugin for now #7343 (comment)\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/7343#discussion_r29300725\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/7987/files#r30004776\n> .\n",
        "createdAt" : "2015-05-11T00:41:52Z",
        "updatedAt" : "2015-05-11T00:41:52Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "05e8915b37b91afa3504e5a2c00dd46e22394c0f",
    "line" : 137,
    "diffHunk" : "@@ -1,1 +150,154 @@### Admission\n\nIt is up to an admission plugin to determine if the security context is acceptable or not.  At the\ntime of writing, the admission control plugin for security contexts will only allow a context that\nhas defined capabilities or privileged.  Contexts that attempt to define a UID or SELinux options"
  },
  {
    "id" : "a3cedce3-7e90-4358-9492-88ec56fbb141",
    "prId" : 3910,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa7565de-bb22-48a2-b27e-62c655fef0ee",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "What types of volumes would the MCS labels be used with?  Presumably there aren't files that are sensitive for the container process in the emptydir.  If this for files in the hostDir, or some other type of volume?\n",
        "createdAt" : "2015-01-30T00:49:17Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "05a07860-5710-4b96-9803-faf0af447038",
        "parentId" : "aa7565de-bb22-48a2-b27e-62c655fef0ee",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Everything - the container would be relabeled, the process would have those labels, and any volumes would either be labelled or potentially left as is (in a few cases maybe this is reasonable?).  Common case though is \"you get these labels\".  I believe we have all but the volume support upstream and we carry the relabeling support on RHEL docker.\n\n> On Jan 29, 2015, at 7:49 PM, Eric Tune notifications@github.com wrote:\n> \n> In docs/design/security_context.md:\n> \n> > +A security context is a set of constraints that are applied to a container in order to achieve the following goals (from [security design](security.md)):\n> > +\n> > +1.  Ensure a clear isolation between container and the underlying host it runs on\n> > +2.  Limit the ability of the container to negatively impact the infrastructure or other containers\n> > +\n> > +## Background\n> > +\n> > +The problem of securing containers in Kubernetes has come up [before](https://github.com/GoogleCloudPlatform/kubernetes/issues/398) and the potential problems with container security are [well known](http://opensource.com/business/14/7/docker-security-selinux). Although it is not possible to completely isolate Docker containers from their hosts, new features like [user namespaces](https://github.com/docker/libcontainer/pull/304) make it possible to greatly reduce the attack surface.\n> > +\n> > +## Motivation\n> > +\n> > +### Container isolation\n> > +\n> > +In order to improve container isolation from host and other containers running on the host, containers should only be \n> > +granted the access they need to perform their work. To this end it should be possible to take advantage of Docker \n> > +features such as the ability to [add or remove capabilities](https://docs.docker.com/reference/run/#runtime-privilege-linux-capabilities-and-lxc-configuration) and [assign MCS labels](https://docs.docker.com/reference/run/#security-configuration) \n> > What types of volumes would the MCS labels be used with? Presumably there aren't files that are sensitive for the container process in the emptydir. If this for files in the hostDir, or some other type of volume?\n> \n> —\n> Reply to this email directly or view it on GitHub.\n",
        "createdAt" : "2015-01-30T01:08:22Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "57222eaf-daa6-42a8-9053-1342db534ab6",
        "parentId" : "aa7565de-bb22-48a2-b27e-62c655fef0ee",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Okay, reading further I see you are talking about NFS, and stuff like that.\n",
        "createdAt" : "2015-01-30T01:31:43Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "dcbd2a3c-2e1c-4e14-8543-9b431f258656",
        "parentId" : "aa7565de-bb22-48a2-b27e-62c655fef0ee",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Yeah - actually relabel is bad if arbitrary (I shouldn't be able to relabel existing content because I tricked the master).  It would be better to relabel only new content.\n\n> On Jan 29, 2015, at 8:32 PM, Eric Tune notifications@github.com wrote:\n> \n> In docs/design/security_context.md:\n> \n> > +A security context is a set of constraints that are applied to a container in order to achieve the following goals (from [security design](security.md)):\n> > +\n> > +1.  Ensure a clear isolation between container and the underlying host it runs on\n> > +2.  Limit the ability of the container to negatively impact the infrastructure or other containers\n> > +\n> > +## Background\n> > +\n> > +The problem of securing containers in Kubernetes has come up [before](https://github.com/GoogleCloudPlatform/kubernetes/issues/398) and the potential problems with container security are [well known](http://opensource.com/business/14/7/docker-security-selinux). Although it is not possible to completely isolate Docker containers from their hosts, new features like [user namespaces](https://github.com/docker/libcontainer/pull/304) make it possible to greatly reduce the attack surface.\n> > +\n> > +## Motivation\n> > +\n> > +### Container isolation\n> > +\n> > +In order to improve container isolation from host and other containers running on the host, containers should only be \n> > +granted the access they need to perform their work. To this end it should be possible to take advantage of Docker \n> > +features such as the ability to [add or remove capabilities](https://docs.docker.com/reference/run/#runtime-privilege-linux-capabilities-and-lxc-configuration) and [assign MCS labels](https://docs.docker.com/reference/run/#security-configuration) \n> > Okay, reading further I see you are talking about NFS, and stuff like that.\n> \n> —\n> Reply to this email directly or view it on GitHub.\n",
        "createdAt" : "2015-01-30T01:34:29Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "77fa868c-39e3-44f0-88c7-08c49df76b2e",
        "parentId" : "aa7565de-bb22-48a2-b27e-62c655fef0ee",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "We should define a kubelet default security context as well - ie if nothing is specified this is the context.  The kubelet can just auto assign uids locally for user namespaces and do similar for labels.  At least some defense in depth.\n\n> On Jan 29, 2015, at 8:32 PM, Eric Tune notifications@github.com wrote:\n> \n> In docs/design/security_context.md:\n> \n> > +A security context is a set of constraints that are applied to a container in order to achieve the following goals (from [security design](security.md)):\n> > +\n> > +1.  Ensure a clear isolation between container and the underlying host it runs on\n> > +2.  Limit the ability of the container to negatively impact the infrastructure or other containers\n> > +\n> > +## Background\n> > +\n> > +The problem of securing containers in Kubernetes has come up [before](https://github.com/GoogleCloudPlatform/kubernetes/issues/398) and the potential problems with container security are [well known](http://opensource.com/business/14/7/docker-security-selinux). Although it is not possible to completely isolate Docker containers from their hosts, new features like [user namespaces](https://github.com/docker/libcontainer/pull/304) make it possible to greatly reduce the attack surface.\n> > +\n> > +## Motivation\n> > +\n> > +### Container isolation\n> > +\n> > +In order to improve container isolation from host and other containers running on the host, containers should only be \n> > +granted the access they need to perform their work. To this end it should be possible to take advantage of Docker \n> > +features such as the ability to [add or remove capabilities](https://docs.docker.com/reference/run/#runtime-privilege-linux-capabilities-and-lxc-configuration) and [assign MCS labels](https://docs.docker.com/reference/run/#security-configuration) \n> > Okay, reading further I see you are talking about NFS, and stuff like that.\n> \n> —\n> Reply to this email directly or view it on GitHub.\n",
        "createdAt" : "2015-01-30T01:40:49Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b01746104211797ccd516bae59d996acd33caca",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +16,20 @@In order to improve container isolation from host and other containers running on the host, containers should only be \ngranted the access they need to perform their work. To this end it should be possible to take advantage of Docker \nfeatures such as the ability to [add or remove capabilities](https://docs.docker.com/reference/run/#runtime-privilege-linux-capabilities-and-lxc-configuration) and [assign MCS labels](https://docs.docker.com/reference/run/#security-configuration) \nto the container process.\n"
  },
  {
    "id" : "498cd57c-0988-407b-9075-b53d16c19889",
    "prId" : 3910,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1896d881-b4e1-4daf-a789-9c9b1666c120",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "If pods on different nodes are accessing shared storage, their UIDs need to be be unique across nodes.  So, their uids need to be allocated either statically to nodes at node join time, or dynamically to pods at bind time, by some cluster level thing.  Thoughts?\n",
        "createdAt" : "2015-01-30T15:32:44Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "6035cd3b-0d50-488f-8f2c-a10923c2a553",
        "parentId" : "1896d881-b4e1-4daf-a789-9c9b1666c120",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Correct - I believe that a reasonable default would be that each security context on the master (a service account that is the \"default\" for the namespace?) would get a UID allocated to it that no other security context would get.  An administrator would then later be able to assign complementary UIDs across namespaces if needed.  In the future, there could be additional security contexts that grant access to shared resources.\n\n----- Original Message -----\n\n> > +\n> > +## Proposed Design\n> > +\n> > +### Overview\n> > +A _security context_ consists of a set of constraints that determine how a\n> > container\n> > +is secured before getting created and run. It has a 1:1 correspondence to\n> > a\n> > +[service\n> > account](https://github.com/GoogleCloudPlatform/kubernetes/pull/2297). A\n> > _security context provider_ is passed to the Kubelet so it can have a\n> > chance\n> > +to mutate Docker API calls in order to apply the security context.\n> > +\n> > +It is recommended that this design be implemented in two phases:\n> > +\n> > +1.  Implement the security context provider extension point in the Kubelet\n> > -    so that a default security context can be applied on container run and\n> >   creation.\n> >   +2.  Implement a security context structure that is part of a service\n> >   account. The\n> > -    default context provider can then be used to apply a security context\n> >   based\n> > -    on the service account associated with the pod.\n> \n> If pods on different nodes are accessing shared storage, their UIDs need to\n> be be unique across nodes.  So, their uids need to be allocated either\n> statically to nodes at node join time, or dynamically to pods at bind time,\n> by some cluster level thing.  Thoughts?\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/3910/files#r23849135\n",
        "createdAt" : "2015-01-30T15:53:29Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "71e1ed64-bdbe-49f8-81f8-b360ef928b64",
        "parentId" : "1896d881-b4e1-4daf-a789-9c9b1666c120",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Okay.   So, this could be done manually, or by a namespace creation helper client, or perhaps by a control loop.  SGTM.\n",
        "createdAt" : "2015-01-30T16:20:07Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b01746104211797ccd516bae59d996acd33caca",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +76,80 @@2.  Implement a security context structure that is part of a service account. The\n    default context provider can then be used to apply a security context based\n    on the service account associated with the pod.\n    \n### Security Context Provider"
  },
  {
    "id" : "d26d58b0-8f48-4e1b-aaed-3726cc5cbf2a",
    "prId" : 3910,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9ba929b-519a-4f0d-bfe6-3770e754ce10",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Would it work to have the SecurityContextProvider just modify the api.BoundPod, and not take a docker.Config as an argument.  Reasons:\n- follow the pattern already in the code where we modify objects as they are passed along, including how we add env vars to the pod object in the kubelet.\n- we probably will want a kubelet debug API that lets you see the \"actual Pod started\", including env vars and security context modifications\n- we want to be able to bootstrap using http or file sources.  So, this ensures that any security context information is expressible in the pod.\n- It makes it a tad easier to put in a docker alternative eventually if docker.Config is not used in as many places in the code.\n",
        "createdAt" : "2015-01-30T15:45:01Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "c0d031ac-be18-4805-b204-9e728f3a640d",
        "parentId" : "d9ba929b-519a-4f0d-bfe6-3770e754ce10",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "----- Original Message -----\n\n> > +2.  Implement a security context structure that is part of a service\n> > account. The\n> > -    default context provider can then be used to apply a security context\n> >   based\n> > -    on the service account associated with the pod.\n> >   +\n> >   +### Security Context Provider\n> >   +\n> >   +The Kubelet will have an interface that points to a\n> >   `SecurityContextProvider`. The `SecurityContextProvider` is invoked before\n> >   creating and running a given container:\n> >   +\n> >   +```go\n> >   +type SecurityContextProvider interface {\n> > -    // ModifyContainerConfig is called before the Docker createContainer\n> >   call.\n> > -    // The security context provider can make changes to the Config with\n> >   which\n> > -    // the container is created.\n> > -    // An error is returned if it's not possible to secure the container\n> >   as\n> > -    // requested with a security context.\n> > -   ModifyContainerConfig(pod *api.BoundPod, container *api.Container, config\n> >   *docker.Config) error\n> \n> Would it work to have the SecurityContextProvider just modify the\n> api.BoundPod, and not take a docker.Config as an argument.  Reasons:\n> - follow the pattern already in the code where we modify objects as they are\n>   passed along, including how we add env vars to the pod object in the\n>   kubelet.\n> - we probably will want a kubelet debug API that lets you see the \"actual\n>   Pod started\", including env vars and security context modifications\n> - we want to be able to bootstrap using http or file sources.  So, this\n>   ensures that any security context information is expressible in the pod.\n> - It makes it a tad easier to put in a docker alternative eventually if\n>   docker.Config is not used in as many places in the code.\n\nTrue - I think cesar (correct me if I'm wrong here) had started here because the options to docker may be complex - setting up user namespaces, labels, and default behavior. However, a two step abstraction of making the docker interface we have from the kubelet support the additional options on BoundPods, and then adding bound pods options, seems reasonable.\n\nSome security context stuff might be a finalizer at the master level.  Security context, if applied on the kubelet for final defaults, and on the master for cluster level isolation, seems similar to other finalizer style patterns.\n",
        "createdAt" : "2015-01-30T15:55:38Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "c3d833a4-dff4-4906-9d56-669ad0a2b3f5",
        "parentId" : "d9ba929b-519a-4f0d-bfe6-3770e754ce10",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "----- Original Message -----\n\n> ----- Original Message -----\n> \n> > > +2.  Implement a security context structure that is part of a service\n> > > account. The\n> > > -    default context provider can then be used to apply a security\n> > >   context\n> > >   based\n> > > -    on the service account associated with the pod.\n> > >   +\n> > >   +### Security Context Provider\n> > >   +\n> > >   +The Kubelet will have an interface that points to a\n> > >   `SecurityContextProvider`. The `SecurityContextProvider` is invoked\n> > >   before\n> > >   creating and running a given container:\n> > >   +\n> > >   +```go\n> > >   +type SecurityContextProvider interface {\n> > > -    // ModifyContainerConfig is called before the Docker createContainer\n> > >   call.\n> > > -    // The security context provider can make changes to the Config with\n> > >   which\n> > > -    // the container is created.\n> > > -    // An error is returned if it's not possible to secure the container\n> > >   as\n> > > -    // requested with a security context.\n> > > - ModifyContainerConfig(pod *api.BoundPod, container *api.Container,\n> > >   config\n> > >   *docker.Config) error\n> > \n> > Would it work to have the SecurityContextProvider just modify the\n> > api.BoundPod, and not take a docker.Config as an argument.  Reasons:\n> > - follow the pattern already in the code where we modify objects as they\n> >   are\n> >   passed along, including how we add env vars to the pod object in the\n> >   kubelet.\n> > - we probably will want a kubelet debug API that lets you see the \"actual\n> >   Pod started\", including env vars and security context modifications\n> > - we want to be able to bootstrap using http or file sources.  So, this\n> >   ensures that any security context information is expressible in the pod.\n> > - It makes it a tad easier to put in a docker alternative eventually if\n> >   docker.Config is not used in as many places in the code.\n> \n> True - I think cesar (correct me if I'm wrong here) had started here because\n> the options to docker may be complex - setting up user namespaces, labels,\n> and default behavior. However, a two step abstraction of making the docker\n> interface we have from the kubelet support the additional options on\n> BoundPods, and then adding bound pods options, seems reasonable.\n\nOne thing I did think of - you may need to know (from the image) what user the image is going to run as, and like ENTRYPOINT I think it's frustrating to an end user to have to specify that image up front in the pod definition.  Some level of \"map user X inside the container to Y outside\" happening by default seemed potentially valuable.  However, the two step process (setup security context, then pass to the docker interface) could also handle that.\n\n> Some security context stuff might be a finalizer at the master level.\n> Security context, if applied on the kubelet for final defaults, and on the\n> master for cluster level isolation, seems similar to other finalizer style\n> patterns.\n",
        "createdAt" : "2015-01-30T15:57:14Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "0c4fe5a8-f4bf-4289-b99a-970139b2a834",
        "parentId" : "d9ba929b-519a-4f0d-bfe6-3770e754ce10",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "There exist both \"SCRATCH\" container images (single process, single uid, not sensitive to the choice of uid)  and \"traditional\" images, (which have many entries in their `/etc/passwd` and which may have multiple processes running as multiple uids in them).\n\nShould the  default default  [sic] security context support the rich base image style?  If so how?  Need a range of UIDs right, and don't know how many till you examine the contents of the image.   On the other hand, should we make it easy to also run the scratch style, and encourage it?\n",
        "createdAt" : "2015-01-30T16:30:12Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "82826245-51c1-4139-b35f-d75de65ffda4",
        "parentId" : "d9ba929b-519a-4f0d-bfe6-3770e754ce10",
        "authorId" : "3a60a43e-3e3d-40f9-b82d-24959d7af0d3",
        "body" : "@erictune - Back to the first comment about only modifying the BoundPod ... If we want to only express intent in the pod definition, then we couldn't just mutate it to apply the security context. At some point the intent needs to become implementation. The security context provider which knows how to implement the pod's intent needs to make specific changes to the actual docker calls. Or am I missing something?\n",
        "createdAt" : "2015-01-30T18:27:12Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "3a60a43e-3e3d-40f9-b82d-24959d7af0d3",
        "tags" : [
        ]
      },
      {
        "id" : "5606c747-1a74-499a-8c5e-94fa7fa0b933",
        "parentId" : "d9ba929b-519a-4f0d-bfe6-3770e754ce10",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "If the idea is to support other container formats in the future, seems like we would need a Security Context Provider associated with the underlying container runtime, but agree that ultimately, for the container runtime == docker, you need a docker.Config.\n\n----- Original Message -----\nFrom: \"Cesar Wong\" notifications@github.com\nTo: \"GoogleCloudPlatform/kubernetes\" kubernetes@noreply.github.com\nCc: \"Derek Carr\" decarr@redhat.com\nSent: Friday, January 30, 2015 1:27:40 PM\nSubject: Re: [kubernetes] [Proposal] Security Contexts (#3910)\n\n> +2.  Implement a security context structure that is part of a service account. The\n> -    default context provider can then be used to apply a security context based\n> -    on the service account associated with the pod.\n> -   \n>   +### Security Context Provider\n>   +\n>   +The Kubelet will have an interface that points to a `SecurityContextProvider`. The `SecurityContextProvider` is invoked before creating and running a given container:\n>   +\n>   +```go\n>   +type SecurityContextProvider interface {\n> -    // ModifyContainerConfig is called before the Docker createContainer call.\n> -    // The security context provider can make changes to the Config with which\n> -    // the container is created.\n> -    // An error is returned if it's not possible to secure the container as \n> -    // requested with a security context. \n> - ModifyContainerConfig(pod *api.BoundPod, container *api.Container, config *docker.Config) error\n\n@erictune - Back to the first comment about only modifying the BoundPod ... If we want to only express intent in the pod definition, then we couldn't just mutate it to apply the security context. At some point the intent needs to become implementation. The security context provider which knows how to implement the pod's intent needs to make specific changes to the actual docker calls. Or am I missing something?\n\n---\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/GoogleCloudPlatform/kubernetes/pull/3910/files#r23860957\n",
        "createdAt" : "2015-01-30T18:50:35Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "8540305b-4ff8-4bd7-ba02-d495807ada68",
        "parentId" : "d9ba929b-519a-4f0d-bfe6-3770e754ce10",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Good point.  I withdraw my comments about ModifyContainerConfig.  We can change the code pretty easily later to support things other than docker.  The important thing is to get the API right.\n",
        "createdAt" : "2015-01-30T22:50:59Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b01746104211797ccd516bae59d996acd33caca",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +89,93 @@    // An error is returned if it's not possible to secure the container as \n    // requested with a security context. \n\tModifyContainerConfig(pod *api.BoundPod, container *api.Container, config *docker.Config) error\n\t\n\t// ModifyHostConfig is called before the Docker runContainer call."
  },
  {
    "id" : "fded1d25-2135-4b53-88ba-8608ae71d578",
    "prId" : 3910,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf6664f1-e6bb-4c5e-87ac-6f674b03fc14",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Containers can have multiple processes running as multiple uids.  This may not be the recommended style of container, by my impression is that there are many of them out there.\n",
        "createdAt" : "2015-01-30T16:31:13Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "ee80089e-aacb-452c-8673-687c924dcbb7",
        "parentId" : "cf6664f1-e6bb-4c5e-87ac-6f674b03fc14",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "One option is to declare that we don't support this style of container, but that excludes a lot of existing images, I think.\n\nAnother option is to just set the lead process to have this uid, but allow other processes to have other virtual uids which map back to useless physical uids.\n\nAnother option, which I'm not sure if it works at all, is to map multiple virtual uids to 1 physical uids.\n\n(I'm using virtual to mean \"in-namespace\" and physical to mean \"in the root linux namespace\").\n\nAnother option is to use a per-volume strategy for virtual-to-physical mapping.\nFor NFS, you could use the NFS mount options to map a single local uid to a true remote uid using anonuid and anongid.  Then you would map all the container uids to the local anongid.  Like this:\n`\ncontainer uid 0 maps to rootns uid 12345 using usernamespaces.  rootns uid 12345 maps to NFS server uid 5432 using mount options.\n`\nNote that this does require an nfs mount for every container versus one mount per node, but I think that is the way y'all were planning anyhow.\n\nI think higher level questions is: If there are two uids in a container, should their filesystem writes, at the canonical view of the filesystem, appear as one or two different uids/gids?  I think \"one\" is simpler for users but harder for implementers.\n",
        "createdAt" : "2015-01-30T16:48:23Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "18f804a1-ece0-4ee6-b3da-ae7538d6a407",
        "parentId" : "cf6664f1-e6bb-4c5e-87ac-6f674b03fc14",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "----- Original Message -----\n\n> > -   // An error is returned if it's not possible to secure the container as\n> >   requested\n> > -    // with a security context.\n> > -   ModifyHostConfig(pod *api.BoundPod, container *api.Container, hostConfig\n> >   *docker.HostConfig)\n> >   +}\n> >   +`\n> >   +If the value of the SecurityContextProvider field on the Kubelet is nil,\n> >   the kubelet will create and run the container as it does today.\n> >   +\n> >   +### Security Context\n> >   +\n> >   +A security context has a 1:1 correspondence to a service account and it\n> >   can be included as\n> >   +part of the service account resource. Following is an example of an\n> >   initial implementation:\n> >   +\n> >   +`go\n> >   +type SecurityContext struct {\n> > -    // user is the uid to use when running the container\n> > -   User int\n> \n> One option is to declare that we don't support this style of container, but\n> that excludes a lot of existing images, I think.\n\nSome, but that could also be the old school image (pre-pods) vs the new school (one process / user per container).  We can probably get pretty far on that for Kube users.\n\n> Another option is to just set the lead process to have this uid, but allow\n> other processes to have other virtual uids which map back to useless\n> physical uids.\n> \n> Another option, which I'm not sure if it works at all, is to map multiple\n> virtual uids to 1 physical uids.\n\nI don't think it works.  In docker upstream we had a long discussion on this about ranges - we _think_ we can allocate ranges and have this work, but you have to have large ranges.  If people had to predeclare how many uids they need and had a map or something, we could maybe allocate a set for a namespace (10k was mooted per container before).  I've also wondered whether we could just do two ranges - shared, and unshared.  Shared is allocated by the master and cluster wide.  Unshared is node scoped and each started container gets a set.  I _think_ you can then pass two ranges into the container.  @mrunalp to keep me honest here.\n\n> (I'm using virtual to mean \"in-namespace\" and physical to mean \"in the root\n> linux namespace\").\n> \n> Another option is to use a per-volume strategy for virtual-to-physical\n> mapping.\n> For NFS, you could use the NFS mount options to map a single local uid to a\n> true remote uid using anonuid and anongid.  Then you would map all the\n> container uids to the local anongid.  Like this:\n> `\n> container uid 0 maps to rootns uid 12345 using usernamespaces.  rootns uid\n> 12345 maps to NFS server uid 5432 using mount options.\n> `\n> Note that this does require an nfs mount for every container versus one mount\n> per node, but I think that is the way y'all were planning anyhow.\n> \n> I think higher level questions is: If there are two uids in a container,\n> should their filesystem writes, at the canonical view of the filesystem,\n> appear as one or two different uids/gids?  I think \"one\" is simpler for\n> users but harder for implementers.\n\nYeah, although in practice I suspect 60-80% of containers that people _should_ run will be single uid.  So we can make single uid work well, and have multi uid be not quite as nice.  \n\n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/3910/files#r23854292\n",
        "createdAt" : "2015-01-30T18:00:29Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "f057d127-341e-44e4-9a83-e38ced812a31",
        "parentId" : "cf6664f1-e6bb-4c5e-87ac-6f674b03fc14",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "I agree with \"make single uid work well, and have multi uid be not quite as nice\".  \n",
        "createdAt" : "2015-01-30T18:17:31Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "411d4f9f-13cb-4d23-8d35-4bf8913a6ed1",
        "parentId" : "cf6664f1-e6bb-4c5e-87ac-6f674b03fc14",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Does \"Not quite as nice\" mean:\n1. \"you have to run that old-school container/pod, and maybe your whole namespace or whole cluster, in a more permissive mode than otherwise\". \n2. or \"you have to write a verbose pod spec that includes arcane SecurityContext magic to get it to work at all\"\n\nI hope it means the first one.  First priority is ability to move your existing dockerized workloads onto kubernetes with minimal pod spec writing.  Second priority is to lock down your cluster.\n",
        "createdAt" : "2015-01-30T18:21:43Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b01746104211797ccd516bae59d996acd33caca",
    "line" : null,
    "diffHunk" : "@@ -1,1 +112,116 @@type SecurityContext struct {\n    // user is the uid to use when running the container\n\tUser int\n\t\n\t// AllowPrivileged indicates whether this context allows privileged mode containers"
  },
  {
    "id" : "5e61d269-f5eb-4ff2-8013-176ce37fa8ac",
    "prId" : 3910,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "I agree that we would want to use all the mechanisms mentioned above (capabilities, MCS labels, apparmor profiles) if available.  And that the initial implementation should use these, since RedHat has so much expertise with these. \n\nAt the same time, it is very much tied to a specific implementation.  This makes it harder for users to understand so much detail; harder to drop in alternative implementations should we ever want to do that.  Examples of different implementations:  some company might use grsecurity and Pax already. (I don't but someone might); some hosting provider might write a different implementation that has similar effect (I can see us doing that).\n\nSo, can you think of a way to divide this up into two layers: one that is a core API object that expresses intent, and another, which implements the intent?\n",
        "createdAt" : "2015-01-30T17:48:21Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "55136878-cb22-4024-b0a6-eab55d7afe6c",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "For example, if my intent is: \"This container should not have the same identity as any other container, both for node-local resources and for shared (storage) resources\", then I am pretty sure the system could automatically come up with a User, SELinux.Level, SELinux.Type, and AppArmor.Profile settings.    The question that needs some thought is whether most other intents can similarly be expressed at an abstract level.\n",
        "createdAt" : "2015-01-30T18:00:34Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "c8a1e297-7dce-4be2-ba51-5555e1c2c69e",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "----- Original Message -----\n\n> > +type SELinuxContext struct {\n> > -    // MCS label/SELinux level to run the container under\n> > -    Level string\n> >   +\n> > -    // SELinux type label for container processes\n> > -    Type  string\n> >   +\n> > -    // FUTURE:\n> > -    // LabelVolumeMountsExclusive []Volume\n> > -    // LabelVolumeMountsShared    []Volume\n> >   +}\n> >   +\n> >   +type AppArmorContext struct {\n> > -   // AppArmor profile\n> > -   Profile string\n> >   +}\n> \n> I agree that we would want to use all the mechanisms mentioned above\n> (capabilities, MCS labels, apparmor profiles) if available.  And that the\n> initial implementation should use these, since RedHat has so much expertise\n> with these.\n> \n> At the same time, it is very much tied to a specific implementation.  This\n> makes it harder for users to understand so much detail; harder to drop in\n> alternative implementations should we ever want to do that.  Examples of\n> different implementations:  some company might use grsecurity and Pax\n> already. (I don't but someone might); some hosting provider might write a\n> different implementation that has similar effect (I can see us doing that).\n> \n> So, can you think of a way to divide this up into two layers: one that is a\n> core API object that expresses intent, and another, which implements the\n> intent?\n\nAt a minimum, anything that is not 100% all Linuxes should be an extension plugin (or a default extension).  No disagreement from me.\n\n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/3910/files#r23858436\n",
        "createdAt" : "2015-01-30T18:08:53Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "06315202-9cb6-4491-9e39-616f82d14508",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "----- Original Message -----\n\n> > +type SELinuxContext struct {\n> > -    // MCS label/SELinux level to run the container under\n> > -    Level string\n> >   +\n> > -    // SELinux type label for container processes\n> > -    Type  string\n> >   +\n> > -    // FUTURE:\n> > -    // LabelVolumeMountsExclusive []Volume\n> > -    // LabelVolumeMountsShared    []Volume\n> >   +}\n> >   +\n> >   +type AppArmorContext struct {\n> > -   // AppArmor profile\n> > -   Profile string\n> >   +}\n> \n> For example, if my intent is: \"This container should not have the same\n> identity as any other container, both for node-local resources and for\n> shared (storage) resources\", then I am pretty sure the system could\n> automatically come up with a User, SELinux.Level, SELinux.Type, and\n> AppArmor.Profile settings.    The question that needs some thought is\n> whether most other intents can similarly be expressed at an abstract level.\n\nThis is an interesting question - volumes are very low level (give me EXACTLY this thing).  Security context as modeled is a bit more like volumes.  It means a finalizer goes and turns a generic intent (maybe expressed by the admin or the namespace) onto a specific context on the pod (pod should run as this UID).  Your suggesting the opposite, something that the user can set \"hey, I want this kind of security context\", and then something has to go finalize and specialize it.\n",
        "createdAt" : "2015-01-30T18:12:11Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "1bf5ceec-9e77-48bf-ab61-8c21c8a6a2bd",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "I see developers as:\n- writing pod specs.  \n- making some of their own images, and using some \"community\" docker images\n- knowing which pods need to talk to which other pods\n- decides which pods should be share files with other pods, and which should not.\n- reasons about application level security, such as containing the effects of a local-file-read exploit in a webserver pod.  \n- does not often reason about operating system or organizational security.\n- not necessarily comfortable reasoning about the security properties of a system at the level of detail of Linux Capabilities, SELinux, AppArmor, etc. \n\nI see project admins as:\n- allocating identities and roles and namespaces.\n- reasoning about organizational security: \n  - don't give a developer permissions that are not needed for role.  \n  - protect files on shared storage from unnecessary cross-team access \n- less focused about application security \n\nI see cluster admins as:\n- less focused on application security.  Focused on operation system security.\n- protects the node from bad actors in containers, and properly-configured innocent containers from bad actors in other containers.\n-  comfortable reasoning about the security properties of a system at the level of detail of Linux Capabilities, SELinux, AppArmor, etc. \n- decides who can use which Linux Capabilities, run privileged containers, use hostDir, etc.\n  - e.g. a team that manages Ceph or a mysql server might be trusted to have raw access to storage devices in some organizations, but teams that develop the applications at higher layers would not.\n\nDo you agree that those are reasonable separations of responsibilities for a Kubernetes cluster?\n\nIf so, do you think the current design allows those three groups to work independently of each other and to focus on the information they need?  I'm not sure; I'm trying to think that through.\n",
        "createdAt" : "2015-01-30T18:56:24Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "77e6b531-e1de-49ab-aef1-1e20b5057528",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "----- Original Message -----\n\n> > +type SELinuxContext struct {\n> > -    // MCS label/SELinux level to run the container under\n> > -    Level string\n> >   +\n> > -    // SELinux type label for container processes\n> > -    Type  string\n> >   +\n> > -    // FUTURE:\n> > -    // LabelVolumeMountsExclusive []Volume\n> > -    // LabelVolumeMountsShared    []Volume\n> >   +}\n> >   +\n> >   +type AppArmorContext struct {\n> > -   // AppArmor profile\n> > -   Profile string\n> >   +}\n> \n> I see developers as:\n> - writing pod specs.\n> - making some of their own images, and using some \"community\" docker images\n> - knowing which pods need to talk to which other pods\n> - decides which pods should be share files with other pods, and which\n>   should not.\n> - reasons about application level security, such as containing the effects\n>   of a local-file-read exploit in a webserver pod.\n> - does not often reason about operating system or organizational security.\n> - not necessarily comfortable reasoning about the security properties of a\n>   system at the level of detail of Linux Capabilities, SELinux, AppArmor,\n>   etc.\n> \n> I see project admins as:\n> - allocating identities and roles and namespaces.\n> - reasoning about organizational security:\n>   - don't give a developer permissions that are not needed for role.\n\nAlso: Concerned about how some things running as higher trust (builds can push images to docker repo X) don't get abused by ordinary developers.  Your phrasing is fine, just adding a scenario we think about.\n\n> ```\n> - protect files on shared storage from unnecessary cross-team access\n> ```\n> - less focused about application security\n> \n> I see cluster admins as:\n> - less focused on application security.  Focused on operation system\n>   security.\n> - protects the node from bad actors in containers, and properly-configured\n>   innocent containers from bad actors in other containers.\n> -  comfortable reasoning about the security properties of a system at the\n>   level of detail of Linux Capabilities, SELinux, AppArmor, etc.\n> - decides who can use which Linux Capabilities, run privileged containers,\n>   use hostDir, etc.\n> \n> ```\n> - e.g. a team that manages Ceph or a mysql server might be trusted to\n> have raw access to storage devices in some organizations, but teams that\n> develop the applications at higher layers would not.\n> ```\n> \n> Do you agree that those are reasonable separations of responsibilities for a\n> Kubernetes cluster?\n\nYes, nailed it.  Those distinctions should be in this proposal and in service account (or whatever context it takes)\n\n> If so, do you think the current design allows those three groups to work\n> independently of each other and to focus on the information they need?  I'm\n> not sure; I'm trying to think that through.\n\nI think the proposal doesn't describe the higher level pieces that are in service account and secrets, but assumes they exist.  I would feel like service account is a concept for the developer end of the spectrum and security context is much more about the other end.  The cluster and project admins must allow developers to have capabilities, the developers must understand how they use those capabilities, and in general higher level developer concepts get boiled down into security contexts and execution details.  So this proposal is definitely talking about a part of the overall story.\n\nThe outcome of these proposals / prototypes should at minimum include a document that describes the above and how the pieces provide that spectrum.\n\nI _think_ at this point that I could argue a convincing story about:\n- how namespaces get a service account by default (configured by cluster admins)\n- project admins can tweak both those and authorization policies (what we are working through here https://github.com/openshift/origin/blob/master/docs/proposals/policy.md) to properly isolate those\n- some secrets get added by default or via developers directly adding them to service accounts or their pods\n- how service accounts are converted to a security context down to the kubelet via finalizers / admission controller\n- how secrets from a service account (or other mechanism) could be bind mounted into containers, either a la the docker vault / secrets proposals, or via a more specific volume type\n- how the kubelet could take the info on the security context and turn it into a user namespace / labels / volumes\n- how developers could adapt their images and applications to work within the limitations of the above items\n\nThere is of course a lot of handwaving in between those bits.\n",
        "createdAt" : "2015-01-30T20:16:11Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "78583a45-f62a-450e-bdd7-962415fde39b",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "It would be great to see your above story plus your comments in #2297 in a single overview document.  If you or @csrwng have time to do that, it would be great, since you seem to have the big picture.  Otherwise, I'm willing to make an attempt.\n",
        "createdAt" : "2015-01-30T23:13:20Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "59fac4a9-d61b-4e26-b310-0f26ecc5b9f6",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "We'll try for monday, and then we can collaboratively edit.  Agree an overarching story is a gap - we're designing the bits, but not articulating how they flow from a central point.\n\n----- Original Message -----\n\n> > +type SELinuxContext struct {\n> > -    // MCS label/SELinux level to run the container under\n> > -    Level string\n> >   +\n> > -    // SELinux type label for container processes\n> > -    Type  string\n> >   +\n> > -    // FUTURE:\n> > -    // LabelVolumeMountsExclusive []Volume\n> > -    // LabelVolumeMountsShared    []Volume\n> >   +}\n> >   +\n> >   +type AppArmorContext struct {\n> > -   // AppArmor profile\n> > -   Profile string\n> >   +}\n> \n> It would be great to see your above story plus your comments in #2297 in a\n> single overview document.  If you or @csrwng have time to do that, it would\n> be great, since you seem to have the big picture.  Otherwise, I'm willing to\n> make an attempt.\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/3910/files#r23879770\n",
        "createdAt" : "2015-01-30T23:15:39Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "3277e5ab-0eeb-45f6-8407-4f8056a6abd7",
        "parentId" : "33673317-07a9-40d0-99c1-0c41d15bb45f",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Okay, I read the  https://github.com/openshift/origin/blob/master/docs/proposals/policy.md.  That looks pretty cool and well thought out.  I see now how you separate cluster admin and project admin responsibilities with the master namespace versus other namespaces.  \n",
        "createdAt" : "2015-01-30T23:24:23Z",
        "updatedAt" : "2015-02-09T19:22:26Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b01746104211797ccd516bae59d996acd33caca",
    "line" : null,
    "diffHunk" : "@@ -1,1 +181,185 @@\t// Length is the length of the ID range\n\tLength int\n}\n\n```"
  },
  {
    "id" : "d38f466f-6142-4be6-8d6b-b1333b9e2c79",
    "prId" : 3910,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31f0bd3b-d528-491d-8aac-132e5e78d5af",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Does this mean the in-namespace UID or the root-namespace UID?\n",
        "createdAt" : "2015-02-20T04:59:59Z",
        "updatedAt" : "2015-02-20T04:59:59Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "867666c5-bd01-4646-a7be-ef558e83e6ed",
        "parentId" : "31f0bd3b-d528-491d-8aac-132e5e78d5af",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "For disk, outside.  However, the user to run as inside the namespace may be something a user wants to change.  If namespaces are not present, a mismatch between the two should reject the pod, maybe.  \n\n> On Feb 20, 2015, at 12:00 AM, Tim Hockin notifications@github.com wrote:\n> \n> In docs/design/security_context.md:\n> \n> > +\n> > +## Motivation\n> > +\n> > +### Container isolation\n> > +\n> > +In order to improve container isolation from host and other containers running on the host, containers should only be \n> > +granted the access they need to perform their work. To this end it should be possible to take advantage of Docker \n> > +features such as the ability to [add or remove capabilities](https://docs.docker.com/reference/run/#runtime-privilege-linux-capabilities-and-lxc-configuration) and [assign MCS labels](https://docs.docker.com/reference/run/#security-configuration) \n> > +to the container process.\n> > +\n> > +Support for user namespaces has recently been [merged](https://github.com/docker/libcontainer/pull/304) into Docker's libcontainer project and should soon surface in Docker itself. It will make it possible to assign a range of unprivileged uids and gids from the host to each container, improving the isolation between host and container and between containers.\n> > +\n> > +### External integration with shared storage\n> > +In order to support external integration with shared storage, processes running in a Kubernetes cluster \n> > +should be able to be uniquely identified by their Unix UID, such that a chain of  ownership can be established. \n> > +Processes in pods will need to have consistent UID/GID/SELinux category labels in order to access shared disks.\n> > Does this mean the in-namespace UID or the root-namespace UID?\n> \n> —\n> Reply to this email directly or view it on GitHub.\n",
        "createdAt" : "2015-02-20T05:13:25Z",
        "updatedAt" : "2015-02-20T05:13:25Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b01746104211797ccd516bae59d996acd33caca",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +24,28 @@In order to support external integration with shared storage, processes running in a Kubernetes cluster \nshould be able to be uniquely identified by their Unix UID, such that a chain of  ownership can be established. \nProcesses in pods will need to have consistent UID/GID/SELinux category labels in order to access shared disks.\n\n## Constraints and Assumptions"
  }
]