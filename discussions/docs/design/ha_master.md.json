[
  {
    "id" : "b3b62c16-0164-433c-a1fc-d8e5f02dd0d3",
    "prId" : 29649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b3f4489-ae91-4274-8988-01a05f5cb256",
        "parentId" : null,
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "CoreOS have documented the recommend way to run etcd in a cluster on the cloud here: https://github.com/coreos/etcd/issues/5418 \n",
        "createdAt" : "2016-07-27T04:41:03Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "bca14526-5c58-471c-ae33-ec8a199e9d93",
        "parentId" : "9b3f4489-ae91-4274-8988-01a05f5cb256",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "+1\n",
        "createdAt" : "2016-07-27T13:51:45Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "aae1e6ed-085b-46e2-aee5-fdcb1faf3ff9",
        "parentId" : "9b3f4489-ae91-4274-8988-01a05f5cb256",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "I've read it, but I don't see what exactly do you mean. What is described there assumes that there is a static number of masters, which I want to avoid here and make it dynamic.\n",
        "createdAt" : "2016-07-27T14:02:25Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "644ed7eb-e606-4d31-9457-3dd13d7e79c6",
        "parentId" : "9b3f4489-ae91-4274-8988-01a05f5cb256",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "It bootstraps a cluster with a fixed number of members, and deals with how you operate that cluster even as cluster members are failing, in the scenario where you can remap storage (e.g. GCE PD volumes).  The approach allows for \"no-ops\" when maintaining an etcd cluster.\n\nIt does not yet address dynamically reconfiguring the cluster, that is true.   But I suggest that we need to figure out how to take this recommended approach to running a cluster and extend it to include membership changes.  I believe the static configuration is only used for first cluster boot, so I suspect we will bring up the first cluster (maybe with N=1) following the recommended approach, and then add additional members consistently - i.e. with DNS names that we remap.\n\nThe problem that we risk falling into otherwise is that we're baking in a GCE assumption.  GCE gives you pet-like reliable instances with fixed internal IPs, that will be rebooted and migrated.  AWS \"officially\" supports something similar, but it is really not a good idea in practice - it is much safer on AWS to use auto-scaling groups.  And I think other clouds do _not_ offer you control over IP allocation, for example.\n",
        "createdAt" : "2016-07-27T14:18:09Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "1592b7dc-6c5a-4565-93b5-51eabda8599b",
        "parentId" : "9b3f4489-ae91-4274-8988-01a05f5cb256",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "AFAIU remapping the storage is just an optimisation and is not needed for correctness - new replica will sync with existing quorum. \n\nRegarding DNS names, I don't see what do we need to change. In GCE we have DNS names auto generated based on VM instance names. We'd use it. On other cloud providers we'd have to additionally program DNS.\n\nMaybe it wasn't clear but `INITIAL_ETCD_CLUSTER` does not have IPs, but DNS names for master machines.\n\nDoes that address your concern?\n",
        "createdAt" : "2016-07-27T16:14:27Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "b8da4997-88fc-41de-979f-1b051c5825a0",
        "parentId" : "9b3f4489-ae91-4274-8988-01a05f5cb256",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "I think you should call that out very explicitly, because now we have a DNS dependency in kube-up.  So... given we are using DNS, what else can we use DNS for?  Well, we have to use it for the LB on other clouds - are there advantages to using it on GCE also? I personally prefer to a cluster by name than by IP.  Should we use it internally rather than an internal LB etc.  Is it easier to use DNS to remap virtual names for etcd cluster members, rather than relying on the instance name - for example, will it simplify the case when # apiservers > # etcd nodes.\n",
        "createdAt" : "2016-07-27T16:39:22Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "e2d593cd-3b45-4e16-8faa-c0a0962b183c",
        "parentId" : "9b3f4489-ae91-4274-8988-01a05f5cb256",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "As discussed offline I think that remapping disks to new instances is making them even more pet-set like. As it's not required for correctness I'd not expand this design until we figure out proper solution.\n",
        "createdAt" : "2016-07-29T20:23:40Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : null,
    "diffHunk" : "@@ -1,1 +97,101 @@This will allow us to have exactly the same logic for HA and non-HA master. List of DNS names for VMs\nwith master replicas will be generated in `kube-up.sh` script and passed to as a env variable\n`INITIAL_ETCD_CLUSTER`.\n\n### apiservers"
  },
  {
    "id" : "003f00c1-ad52-47e1-9a28-c41e665ab5bd",
    "prId" : 29649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "parentId" : null,
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "This is actually really painful in practice e.g. how do addons get updated.  We are discussing alternatives in sig-cluster-lifecycle; likely just replacing add-on manager with kubectl (whether automated post-install, user-driven or part of a more comprehensive but external system)\n",
        "createdAt" : "2016-07-27T04:45:15Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "00320f21-6f03-418a-a2e4-22f6a0849aeb",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "I don't understand what's the issue here. Addons will be updated when add-on manager (together with master) will be updated. The only problem is that a single addon might be updated and then reverted as we'll have multiple versions of addon manager competing. It will end once all masters are updated.\n",
        "createdAt" : "2016-07-27T14:08:59Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "3dea5f0c-8dc3-48d2-8090-b5e6ecaf8b6b",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "Let's make it concrete: how does the user update the dashboard in an HA configuration?  My understanding is that operator has to SSH to each master and replace the yaml file.  Is there a trick I'm missing?\n\nThe proposed UX is that instead the operator does `kubectl apply -f https://k8s.io/addons/dashboard` or similar.  There is some work needed to make this happen in all cases, in particular something like https://github.com/kubernetes/kubernetes/issues/19805\n\nSee also https://github.com/kubernetes/features/issues/18\n",
        "createdAt" : "2016-07-27T14:28:09Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "4fbef85e-c2f4-4292-83e1-56fe7b60f05d",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "I see your point. You're right - user would have to SSH to each machine and update YAML file there. But it's exactly the same as it's today, right? It's not any worse.\n",
        "createdAt" : "2016-07-27T15:49:29Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "8f2381e4-c960-44f3-8a18-df05023bd100",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "Well, it is worse with HA, because now you have multiple masters you have to update.  But, sure we can split this out into a separate concern - HA is a big enough issue as it is :-)\n",
        "createdAt" : "2016-07-27T16:06:50Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "c60cbb5b-8f62-4837-a5c8-1d93b7c2d09a",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "In other words I think it's out of scope for this design. It's the same for kops, right?\n",
        "createdAt" : "2016-07-27T16:16:42Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "534df13f-fa01-4587-8436-e81d5f837233",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "Yes, we should split it out into a separate topic.  Once we have an agreed solution from sig-cluster-lifecycle I think that kops, kube-anywhere and I hope kube-up will adopt it!\n",
        "createdAt" : "2016-07-27T16:32:12Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "1c709665-b2f5-4cc1-83c6-8799033d5a8b",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I think that @fgrzadkowski was suggesting that instead of ssh'ing into each master and updating a yaml file, you'd kill all masters, then create N new masters to replace them (with the correct yaml file on disk). So they might fight a bit when they all come up with the new addon definition, but not for very long and they would converge to the same end result. \n\nAn actual addon manager (or kubectl apply) would clearly be better. \n",
        "createdAt" : "2016-07-27T21:20:46Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "6603b4f3-ff87-46e8-8f40-11f93fccf647",
        "parentId" : "f87d7e20-eadc-49e3-90a6-658a0c14ceca",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I'm not super happy with addon manager as it stands.  It is sort of yet-another-API that people may have to understand, which interacts in surprising ways with the \"real\" API.\n",
        "createdAt" : "2016-09-06T15:43:18Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : 195,
    "diffHunk" : "@@ -1,1 +193,197 @@can be updated multiple times in a row after upgrading the master. Long-term we should fix this\nby using a similar mechanisms as controller manager or scheduler. However, currently add-on\nmanager is just a bash script and adding a master election mechanism would not be easy.\n\n## Adding replica"
  },
  {
    "id" : "a8b6a6e5-bdfd-4f86-a90a-927259eba7f9",
    "prId" : 29649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfa4ddc4-c7af-4272-9b72-ecb463854010",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Will they be deployed as part of bootstrapping as introspective services?  \n",
        "createdAt" : "2016-07-27T13:41:02Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "4857ed1a-d6e2-41eb-b84c-99a1821df8e5",
        "parentId" : "bfa4ddc4-c7af-4272-9b72-ecb463854010",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "I don't understand. Again - number of masters equals number of apiservers, etcds, controllers etc. Does that answer your question?\n",
        "createdAt" : "2016-07-27T13:56:53Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "f230fd02-cb9b-4e65-a406-c779b343cdd9",
        "parentId" : "bfa4ddc4-c7af-4272-9b72-ecb463854010",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "> Controller manager, scheduler & cluster autoscaler will use lease mechanism\n\nMeans they are active-passive HA.  Is the supposition that they are deployed as an RC of 2 onto the cluster with only 1 being active at a time. \n",
        "createdAt" : "2016-07-27T14:25:45Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "9908d7aa-e59a-4131-8802-010c9e0bd5eb",
        "parentId" : "bfa4ddc4-c7af-4272-9b72-ecb463854010",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "They will be deployed as they are now - pods created from a manifest file on master machine. As you say, they will use master election to have only a single active master. It's explained in the detailed section.\n",
        "createdAt" : "2016-07-27T15:54:48Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : null,
    "diffHunk" : "@@ -1,1 +57,61 @@* We will introduce provider specific solutions to load balance traffic between master replicas\n  (see section `load balancing`)\n* Controller manager, scheduler & cluster autoscaler will use lease mechanism and\n  only a single instance will be an active master. All other will be waiting in a standby mode.\n* All add-on managers will work independently and each of them will try to keep add-ons in sync"
  },
  {
    "id" : "a3ee1123-d68d-4353-a3b4-f5f5e3a5538a",
    "prId" : 29649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "118ff3d3-14a4-4fec-916d-9a7f275eaebb",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Should just reference etcd documentation honestly. \n",
        "createdAt" : "2016-07-27T13:43:29Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "a5a6fb0a-b176-44a3-a6c3-55892d0837a7",
        "parentId" : "118ff3d3-14a4-4fec-916d-9a7f275eaebb",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Only the first sentence maps to etcd documentation. All other describe configuration choices (e.g. reads will go through leader).\n",
        "createdAt" : "2016-07-27T13:59:05Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : null,
    "diffHunk" : "@@ -1,1 +76,80 @@```\n\nAll etcd instances will be clustered together and one of them will be an elected master.\nIn order to commit any change quorum of the cluster will have to confirm it. Etcd will be\nconfigured in such a way that all writes and reads will go through the master (requests"
  },
  {
    "id" : "2b3552bb-a414-4f83-b3eb-62258f61cb40",
    "prId" : 29649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "003386bb-c2a2-46d3-a826-b99d7dc37214",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Are we still going to allow masters to be added after the initally kube-up run?\n",
        "createdAt" : "2016-09-01T15:49:32Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "dd6a6335-e3aa-4587-a08e-d2dc6d077506",
        "parentId" : "003386bb-c2a2-46d3-a826-b99d7dc37214",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Ok, never mind. I read further below.\n",
        "createdAt" : "2016-09-01T16:09:01Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : null,
    "diffHunk" : "@@ -1,1 +95,99 @@  * `existing` if there are more than one replica, i.e. the list of existing master replicas is non-empty.\n\nThis will allow us to have exactly the same logic for HA and non-HA master. List of DNS names for VMs\nwith master replicas will be generated in `kube-up.sh` script and passed to as a env variable\n`INITIAL_ETCD_CLUSTER`."
  },
  {
    "id" : "6c49b0ed-d5a0-41af-82d3-a1d41458da6d",
    "prId" : 29649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2d68ae4-cdb5-429c-ae77-dc37330403d3",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Addon managers of different versions are going to fight during the rolling upgrade.\n",
        "createdAt" : "2016-09-01T16:11:43Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "b7a038da-7670-46e1-9750-e43c2cc64d07",
        "parentId" : "f2d68ae4-cdb5-429c-ae77-dc37330403d3",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Correct. It's explained in the \"add-on manager\" section above together with the explanation why I suggest to ignore this problem for now.\n",
        "createdAt" : "2016-09-02T13:03:33Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : 255,
    "diffHunk" : "@@ -1,1 +253,257 @@## Upgrades\n\nUpgrading replicated master will be possible by upgrading them one by one using existing tools\n(e.g. upgrade.sh for GCE). This will work out of the box because:\n* Requests from nodes will be correctly served by either new or old master because apiserver is backward compatible."
  },
  {
    "id" : "a257b3e6-210c-4fa4-8693-6601ddbd205f",
    "prId" : 29649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b0c8a20-6937-4de4-bb45-0159c78976a1",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "How does finding API server work for clients coming from outside the cluster?\n",
        "createdAt" : "2016-09-06T08:05:58Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "c41a970f-c4a9-4819-8bf1-48d0ca345074",
        "parentId" : "5b0c8a20-6937-4de4-bb45-0159c78976a1",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "This DNS name will be publicly resolvable. I added this to the text.\n",
        "createdAt" : "2016-09-06T15:00:14Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : null,
    "diffHunk" : "@@ -1,1 +147,151 @@`--apiserver-count` it is not very dynamic and would require restarting all\nmasters to change number of master replicas.\n\nTo allow dynamic changes to the number of apiservers in the cluster, we will\nintroduce a `ConfigMap` in `kube-system` namespace, that will keep an expiration"
  },
  {
    "id" : "6864ddf3-1c69-47eb-bd1a-3db7ba07e6c9",
    "prId" : 29649,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/29649#pullrequestreview-5080036",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5157850c-fd9d-4557-aad7-867cbaac5a91",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Is it worth adding a section here on non-goals? e.g. we don't plan to test that this HA deployment will scale to 2k nodes. \n",
        "createdAt" : "2016-10-19T22:20:03Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "c9148a52-e60d-4846-ac23-826f225051c5",
        "parentId" : "5157850c-fd9d-4557-aad7-867cbaac5a91",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Is it actually a goal or non-goal to this design? I think that kind of orthogonal what are we going to test.\n\nI have a sentence about non-goals in the intro section, so if you believe it's worth I can add it there.\n",
        "createdAt" : "2016-10-20T14:30:15Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +44,48 @@might be very different.\n\n# Overview\n\nIn a cluster with replicated master, we will have N VMs, each running regular master components"
  },
  {
    "id" : "e6410af6-953d-4ab0-af69-ec690aff6083",
    "prId" : 29649,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/29649#pullrequestreview-6211214",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3b5921e-aea0-4bed-addf-632090ec85bd",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I think for some deployments (e.g. gke) we may not want to always make this assumption. Can we parameterize it so that it's the default for kube-up but not the default in the salt/gci scripts?\n",
        "createdAt" : "2016-10-19T22:22:25Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "0c947f13-0b0e-45a7-a9f4-7cc57fe56451",
        "parentId" : "c3b5921e-aea0-4bed-addf-632090ec85bd",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "We can potentially, but why would we? It'd make it more complicated. AFAIU the only real downside is listening on external interface instead of localhost, but I think that if we use SSL by default than it should be safe. It's similar to what we do for apiserver.\n",
        "createdAt" : "2016-10-20T14:36:39Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "b3dba30d-f655-416b-847d-782c957d5315",
        "parentId" : "c3b5921e-aea0-4bed-addf-632090ec85bd",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Does using SSL imply that the client (e.g. apiserver) is authenticated in addition to encrypting the traffic? This implies that the apiserver needs client creds to talk to etcd and must verify the SSL cert provided by the etcd server. \n",
        "createdAt" : "2016-10-27T08:22:49Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "a47b72c4-dc15-426a-ab8e-f1f8ad9cf9ff",
        "parentId" : "c3b5921e-aea0-4bed-addf-632090ec85bd",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Sorry for not being clear here. Etcd uses two interfaces and ports. One for client connections (e.g. apiserver) and other for peer connections (other etcd replicas). Interface for client connections will remain to be localhost, so it will not be using SSL. Interface for peer connections will be using SSL for both encryption and authentication. We will basically follow [this etcd documentation](https://coreos.com/etcd/docs/latest/security.html#example-3-transport-security--client-certificates-in-a-cluster)\n\n@jszczepkowski\n",
        "createdAt" : "2016-10-27T16:28:53Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "de5d9291-2c2a-49ed-bfaf-db7570cc72bd",
        "parentId" : "c3b5921e-aea0-4bed-addf-632090ec85bd",
        "authorId" : "83ddc34c-eac6-462e-98d4-9994e371732f",
        "body" : "This solution excludes deployments using Calico where etcd is shared between Kubernetes and Calico. Localhost listening etcd for client connections on non-master nodes creates a huge security vulnerability. I don't see any reason why we wouldn't want SSL enabled for client connections in such a scenario.\n",
        "createdAt" : "2016-10-28T09:24:48Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "83ddc34c-eac6-462e-98d4-9994e371732f",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +87,91 @@communication we will use SSL (as described [here](https://coreos.com/etcd/docs/latest/security.html)).\n\nWhen generating command line for etcd we will always assume it’s part of a cluster\n(initially of size 1) and list all existing kubernetes master replicas.\nBased on that, we will set the following flags:"
  },
  {
    "id" : "10c0aa7c-547c-416e-8d81-81088bf39ea9",
    "prId" : 29649,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/29649#pullrequestreview-4973461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc014882-fc74-4a11-acc5-b412a161a974",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "We will probably want to think about how we can change this in the future, but I think it's ok for a first iteration. \n",
        "createdAt" : "2016-10-19T22:25:30Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : 170,
    "diffHunk" : "@@ -1,1 +168,172 @@That means that with multiple master replicas and a load balancer in front\nof them, accessing one of the replicas directly (using it's ephemeral public\nIP) will not work on GCE without appropriate flags:\n\n- `kubectl --insecure-skip-tls-verify=true`"
  },
  {
    "id" : "beb8eca7-c86c-4e2a-8925-8ed1c5f90a37",
    "prId" : 29649,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/29649#pullrequestreview-5080036",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27832573-3028-45ba-bbb4-c35919e25dab",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Should we mark this as \"alpha\" or \"beta\" for 1.5 in some way? How do we envision supporting this for 1.6 and future releases?\n",
        "createdAt" : "2016-10-19T22:30:05Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "4a01a19a-64d1-4c5e-9107-86080304adcf",
        "parentId" : "27832573-3028-45ba-bbb4-c35919e25dab",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "I think it'd be a beta feature for 1.5, though I don't think we have any tool/convention how to mark this kind of features as beta. I believe we have the same problem with kubectl. Maybe we should mark it explicitly in documentation/user guide?\n\nWith regard to future, my understanding is that once we have a different default tool to start our clusters it will also implement HA setup for masters and this will become obsolete. Does that answer your answer?\n",
        "createdAt" : "2016-10-20T14:25:04Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +28,32 @@<!-- END MUNGE: UNVERSIONED_WARNING -->\n\n# Automated HA master deployment\n\n**Author:** filipg@, jsz@"
  },
  {
    "id" : "26df0103-f047-468a-85d2-a6540f9f260d",
    "prId" : 29649,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/29649#pullrequestreview-5615055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "162ed538-ecc6-43bf-9bb8-0bd8fe6d5eb0",
        "parentId" : null,
        "authorId" : "e5975294-c47f-41e1-b753-2d15be8ac5bb",
        "body" : "The current `--apiserver-count` is broken anyways as it doesn't take into account failures of apiservers ( https://github.com/kubernetes/kubernetes/issues/22609). Maybe there's also some kind of liveness check/TTL for these IPs needed?\n",
        "createdAt" : "2016-10-20T00:19:50Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "e5975294-c47f-41e1-b753-2d15be8ac5bb",
        "tags" : [
        ]
      },
      {
        "id" : "1f4f14b4-3b15-4147-8546-1974278bf558",
        "parentId" : "162ed538-ecc6-43bf-9bb8-0bd8fe6d5eb0",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "That's a very good point. This made me think that maybe there's a better solution. As you say we should be using a TTL for each IP. What we can do is:\n1. In the Endpoints object annotations we'd keep a TTL for each IP. Each annotation would keep a pair with an IP, that it corresponds to, and a TTL\n2. Each apiserver when updating service `kubernetes` will do two things:\n   1. Add it's own IP if it's not there and add/update TTL for it\n   2. Remove all the IPs with too old TTL\n\nI think it'd be much easier than a `ConfigMap` and would solve the problem of unavailable apiservers. I'll update also the issue you mentioned.\n\n@roberthbailey \n",
        "createdAt" : "2016-10-20T15:19:49Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "d8b86f93-f832-4ca1-9f08-d8fc9e132bac",
        "parentId" : "162ed538-ecc6-43bf-9bb8-0bd8fe6d5eb0",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "We talked about this with @thockin and @jszczepkowski and we believe that a reasonable approach would be to:\n1. Add a ConfigMap that would keep the list of active apiservers, with their expiration times; those would be updated by each apiserver separately\n2. Change EndpointsReconsiler in apiserver to update Endpoints list to match active apiservers from the ConfigMap.\n\nThat way we will have a dynamic configuration and at the same time we will not be updating Endpoints too often, as expiration times will be stored in a dedicated ConfigMap.\n",
        "createdAt" : "2016-10-24T14:10:34Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "045d011d-1214-4898-8524-ee40d44fa1b0",
        "parentId" : "162ed538-ecc6-43bf-9bb8-0bd8fe6d5eb0",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Where will such a ConfigMap live? Which namespace?\n",
        "createdAt" : "2016-10-24T17:08:02Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "e37f6a84-450b-484c-93f4-67f5fa591269",
        "parentId" : "162ed538-ecc6-43bf-9bb8-0bd8fe6d5eb0",
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "I would imagine the \"lock server\" configmap for the api servers would live in `kube-system`.  For communicating the set of IPs that clients should try to connect to, we can lean on the work I'm doing in #30707 along with enhancing kubeconfig to be multi-endpoint aware.  That kubeconfig ConfigMap is proposed to live in a new `kube-public` namespace.\n",
        "createdAt" : "2016-10-24T17:18:51Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      },
      {
        "id" : "9e35bb20-46ff-4377-bf16-5faa4b2321a2",
        "parentId" : "162ed538-ecc6-43bf-9bb8-0bd8fe6d5eb0",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "This ConfigMap would live in `kube-system` and would be used only by apiserver ([here](https://github.com/kubernetes/kubernetes/blob/master/pkg/master/controller.go#L281)) to properly set list of endpoints in `kubernetes` service which lives in `default` namespace.\n",
        "createdAt" : "2016-10-25T09:44:06Z",
        "updatedAt" : "2016-10-28T10:16:15Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e5f27d940ca1395a447170cb5f2b1e365710d12",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +146,150 @@list of IP addresses for all apiservers. As it uses a command line flag\n`--apiserver-count` it is not very dynamic and would require restarting all\nmasters to change number of master replicas.\n\nTo allow dynamic changes to the number of apiservers in the cluster, we will"
  }
]