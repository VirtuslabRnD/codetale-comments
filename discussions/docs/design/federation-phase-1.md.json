[
  {
    "id" : "716ae7e2-3738-40d3-92db-b2c28bc3756e",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89432f67-2c9b-4313-864e-92f0d02d9170",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I have a few comments, though I realize you didn't write this doc.\n",
        "createdAt" : "2016-01-09T08:21:52Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +28,32 @@<!-- END MUNGE: UNVERSIONED_WARNING -->\n\n# Ubernetes Design Spec (phase one)\n\n**Huawei PaaS Team**"
  },
  {
    "id" : "8ca4c0b8-fbb5-4d53-84fd-96b57ac1d4a1",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5eb85eb-691d-493d-afab-ed17f89c3870",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Who is the expected consumer of this information? Just the end-user? Or some kind of higher-level scheduling system?\n",
        "createdAt" : "2016-01-09T08:30:51Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "66e406fa-2402-4635-9cee-05eb46f4f93b",
        "parentId" : "a5eb85eb-691d-493d-afab-ed17f89c3870",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Oh, I guess later you clarify that it is the Ubernetes scheduler.\n",
        "createdAt" : "2016-01-09T08:53:07Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "5866446d-6fbf-4f3a-bfee-700a8e7821a1",
        "parentId" : "a5eb85eb-691d-493d-afab-ed17f89c3870",
        "authorId" : "7856c2cc-1134-49c3-bbc0-d309644c9557",
        "body" : "Yes the Ubernetes scheduler  will consume this information.\n",
        "createdAt" : "2016-01-09T19:43:47Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "7856c2cc-1134-49c3-bbc0-d309644c9557",
        "tags" : [
        ]
      },
      {
        "id" : "57bc64a4-34ef-48e3-8505-0ab16613a2ed",
        "parentId" : "a5eb85eb-691d-493d-afab-ed17f89c3870",
        "authorId" : null,
        "body" : "Done.\n",
        "createdAt" : "2016-01-12T03:29:13Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 297,
    "diffHunk" : "@@ -1,1 +295,299 @@object here**. The cluster resource metrics are stored in cluster\nstatus section, just like what we did to nodes in K8S. In phase one it\nonly contains available CPU resources and memory resources.  The\ncluster controller will periodically poll the underlying cluster API\nServer to get cluster capability. In phase one it gets the metrics by"
  },
  {
    "id" : "af7052c9-be89-457d-aaa4-66693611c356",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f9e2cc9-a8eb-4e38-9ff8-efec9f875864",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Do you need global pod scheduling? Maybe you don't need to support that, and you can just say everything has to run under an RC (or Job or DaemonSet etc..., but no un-controlled pods).\n",
        "createdAt" : "2016-01-09T08:34:13Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "64364e52-981e-4dd5-a7d1-80bb77caea41",
        "parentId" : "0f9e2cc9-a8eb-4e38-9ff8-efec9f875864",
        "authorId" : "7856c2cc-1134-49c3-bbc0-d309644c9557",
        "body" : "In phase one we will only support RC. Whether to support global pod scheduling is still under discussion. One concern is that will it break the API compatibility between Ubernetes and Kubernetes if we don't support pod-level scheduling.\n",
        "createdAt" : "2016-01-09T19:49:08Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "7856c2cc-1134-49c3-bbc0-d309644c9557",
        "tags" : [
        ]
      },
      {
        "id" : "5f94b14e-d0e5-45d1-ad39-b5a08decefe8",
        "parentId" : "0f9e2cc9-a8eb-4e38-9ff8-efec9f875864",
        "authorId" : null,
        "body" : "Done.\n",
        "createdAt" : "2016-01-12T03:39:22Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : null,
    "diffHunk" : "@@ -1,1 +380,384 @@separate design document: [Federated Services](federated-services.md).\n\n## Pod\n\nIn phase one we only support scheduling replication controllers. Pod"
  },
  {
    "id" : "e309a2df-002a-485b-8614-ab9b47decfc1",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29d97cf9-f396-4c8f-9876-171e8c8dd5eb",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Is this the author of the doc or the intended implementor of the proposal? If they are the author, why not submit it in a separate PR so that they can have author attribution (and respond to comments / questions)?\n",
        "createdAt" : "2016-01-19T19:45:36Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "d69e7892-5e1b-4b80-89a9-4e34bcdfd78c",
        "parentId" : "29d97cf9-f396-4c8f-9876-171e8c8dd5eb",
        "authorId" : null,
        "body" : "@roberthbailey Both author and intended implementor.  The reasons for collapsing into a single PR were purely pragmatic.  The Huawei are free to respond to comments, and have in some cases.  We can split this doc into a separate PR if the author attribution issue of real concern to anyone?\n",
        "createdAt" : "2016-01-19T20:45:07Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "b0859f69-aba3-4571-a0e9-42397676bde7",
        "parentId" : "29d97cf9-f396-4c8f-9876-171e8c8dd5eb",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Maybe it's just confusing to me. \n",
        "createdAt" : "2016-01-19T21:15:56Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +30,34 @@# Ubernetes Design Spec (phase one)\n\n**Huawei PaaS Team**\n\n## INTRODUCTION"
  },
  {
    "id" : "713f5e64-0520-4d75-adb6-18d3a177a5b5",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fd2794a-1963-438f-b119-9610e468ab34",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "It may be better to install a pod into each cluster to report back metrics rather than have the cluster controller scrape them. This would make the design more consistent with nodes reporting status to the apiserver. \n",
        "createdAt" : "2016-01-19T19:55:14Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "89ae20c4-58ec-4f18-b0f1-fb5a9050b1f0",
        "parentId" : "9fd2794a-1963-438f-b119-9610e468ab34",
        "authorId" : null,
        "body" : "@roberthbailey I can see pros and cons both ways.  Let me give it some further thought.\n",
        "createdAt" : "2016-01-19T20:49:45Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "df236173-5851-4944-a93f-f89eea156f25",
        "parentId" : "9fd2794a-1963-438f-b119-9610e468ab34",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "If this is an open topic, it might be worth putting both alternatives here and then discussing the pros/cons in a separate doc (with interested parties) or as part of the ubernetes SIG. \n",
        "createdAt" : "2016-01-19T21:17:04Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "5a37ead6-17e2-4b3b-b04f-9981e1f29bcb",
        "parentId" : "9fd2794a-1963-438f-b119-9610e468ab34",
        "authorId" : null,
        "body" : "Agreed, done.\n",
        "createdAt" : "2016-02-27T00:56:19Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +164,168 @@   components, like a sub-RC or a sub-service. And then it creates the\n   corresponding API objects on the underlying K8S clusters.\n1. It periodically retrieves the available resources metrics from the\n   underlying K8S cluster, and updates them as object status of the\n   `cluster` API object.  An alternative design might be to run a pod"
  },
  {
    "id" : "b0a448be-9b18-48fb-9bf1-b5905cfb6ab7",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1b46707-1f7c-45ce-94b1-4614267f1eb2",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Need a new line before this, otherwise it gets captured as part of the previous line (which is confusing)\n",
        "createdAt" : "2016-01-19T19:56:39Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : null,
    "diffHunk" : "@@ -1,1 +198,202 @@1. Other metadata like the version of cluster\n\n$version.clusterSpec\n\n<table style=\"border:1px solid #000000;border-collapse:collapse;\">"
  },
  {
    "id" : "cb8ce3fe-f90b-4053-83a6-d10318988449",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "496ed9e1-e9f6-44bf-867a-102540c1d09a",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Why not use the markdown syntax for the table?\n",
        "createdAt" : "2016-01-19T19:56:49Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "8a21b70e-2a91-4ff9-a3fc-2c8e6f31dff5",
        "parentId" : "496ed9e1-e9f6-44bf-867a-102540c1d09a",
        "authorId" : null,
        "body" : "@roberthbailey Agreed.  Will fix.\n",
        "createdAt" : "2016-01-19T20:50:03Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "2412563d-1bb6-47a9-b44a-c008b6fd1c6d",
        "parentId" : "496ed9e1-e9f6-44bf-867a-102540c1d09a",
        "authorId" : null,
        "body" : "Markdown can't do tables :-(\n",
        "createdAt" : "2016-02-27T00:56:52Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : null,
    "diffHunk" : "@@ -1,1 +200,204 @@$version.clusterSpec\n\n<table style=\"border:1px solid #000000;border-collapse:collapse;\">\n<tbody>\n<tr>"
  },
  {
    "id" : "5839039f-bd6e-4bae-8ea3-e2faa4855ad4",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "605c6772-ad1b-4bee-ae86-d6210cb30ac2",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "What does this mean? And how is it computed? A cluster itself doesn't export (or know) whether it is being created / deleted so how would something scraping the apiserver figure this out? Are there other phases that would need to be captured? Is the intent here to determine when a cluster becomes unavailable?\n\nIt seems like any cluster that is registered should be presumed to be running, but may become temporarily unavailable. Permanent unavailability (e.g. being deleted) should correspond with an admin removing the cluster object from the federated server. \n",
        "createdAt" : "2016-01-19T19:59:34Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "f9091226-a9b1-413b-85fd-b475e74bc767",
        "parentId" : "605c6772-ad1b-4bee-ae86-d6210cb30ac2",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I see this is addressed below. \n",
        "createdAt" : "2016-01-19T20:00:20Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 259,
    "diffHunk" : "@@ -1,1 +257,261 @@</tr>\n<tr>\n<td style=\"padding:5px;\">Phase<br>\n</td>\n<td style=\"padding:5px;\">the recently observed lifecycle phase of the cluster<br>"
  },
  {
    "id" : "d16092a4-d7c5-401a-8e4d-90669ee7b5ba",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f83b079b-3b64-417f-a044-1f0f6be5faec",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "If we want a cluster registered that is unschedulable, we should call it unschedulable, not pending. \n",
        "createdAt" : "2016-01-19T20:00:51Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "063fd9a6-6dde-49af-be49-9b39f377da0f",
        "parentId" : "f83b079b-3b64-417f-a044-1f0f6be5faec",
        "authorId" : null,
        "body" : "Point taken.  I think we can debate the exact naming later in the API review.\n",
        "createdAt" : "2016-02-27T01:05:56Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 305,
    "diffHunk" : "@@ -1,1 +303,307 @@field includes following values:\n\n+ pending: newly registered clusters or clusters suspended by admin\n   for various reasons. They are not eligible for accepting workloads\n+ running: clusters in normal status that can accept workloads"
  },
  {
    "id" : "68bdfdf3-e36e-44f0-9add-7c06d95661ad",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6066b04c-9f2a-4a93-9856-98a9ff243bc9",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "This shouldn't be a valid phase. Once deleted, it shouldn't exist in the object store. \n",
        "createdAt" : "2016-01-19T20:01:13Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "aba38c68-cf92-44f3-9ae2-4ab18926bc19",
        "parentId" : "6066b04c-9f2a-4a93-9856-98a9ff243bc9",
        "authorId" : null,
        "body" : "I'm not sure that I agree.  For rollback, and auditing, amongst others, it's useful to retain records of terminated clusters.\n",
        "createdAt" : "2016-02-27T01:06:53Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +307,311 @@+ running: clusters in normal status that can accept workloads\n+ offline: clusters temporarily down or not reachable\n+ terminated: clusters removed from federation\n\nBelow is the state transition diagram."
  },
  {
    "id" : "8383dd26-54ae-4d51-b43c-8e1a6e06e688",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db1e0e52-1d5b-4afa-a95d-ce0f4e92f6e7",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "What about jobs? Should those be discussed?\n",
        "createdAt" : "2016-01-19T20:26:01Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "adce7d2b-c0f0-4ca1-a5d1-301282368593",
        "parentId" : "db1e0e52-1d5b-4afa-a95d-ce0f4e92f6e7",
        "authorId" : null,
        "body" : "Those will be done after phase 1.\n",
        "createdAt" : "2016-02-27T01:08:29Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : null,
    "diffHunk" : "@@ -1,1 +380,384 @@separate design document: [Federated Services](federated-services.md).\n\n## Pod\n\nIn phase one we only support scheduling replication controllers. Pod"
  },
  {
    "id" : "37f56ef8-87bd-44cb-ba2a-eaaa040272ad",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf474997-bd1a-4a31-a5e3-33b865668035",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "What keeps the RC and sub-RCs in sync? Say that I change my mind about the number of replicas that I want (globally). Something will need to update the sub-RCs to add up to the new number and then update the RCs in each cluster to reflect those changes. \n",
        "createdAt" : "2016-01-19T20:28:46Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "7b7042e1-8a02-48e8-8c46-6c438d04f745",
        "parentId" : "cf474997-bd1a-4a31-a5e3-33b865668035",
        "authorId" : null,
        "body" : "Yes, that's addressed elsewhere in the doc.\n",
        "createdAt" : "2016-02-27T01:09:17Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 398,
    "diffHunk" : "@@ -1,1 +396,400 @@1. Cluster controller periodically polls the latest available resource\n   metrics from the underlying clusters.\n1. Scheduler is watching all pending RCs. It picks up the RC, make\n   policy-driven decisions and split it into different sub RCs.\n1. Each cluster control is watching the sub RCs bound to its"
  },
  {
    "id" : "a42abc24-d810-4ba8-a817-d3e113cb5efc",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d763b28-c711-4e34-b6b5-295f92ad5a7e",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "So the Ubernetes control plane won't try to re-locate into a different cluster if the workload gets stuck in a permanently pending state?\n",
        "createdAt" : "2016-01-19T20:29:33Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "4e69e75e-3cc8-4c3e-aee6-4f7a88f7ada7",
        "parentId" : "0d763b28-c711-4e34-b6b5-295f92ad5a7e",
        "authorId" : null,
        "body" : "Not in phase 1.\n",
        "createdAt" : "2016-02-27T01:10:24Z",
        "updatedAt" : "2016-03-03T23:50:01Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 420,
    "diffHunk" : "@@ -1,1 +418,422 @@the cluster at time _T2_, the cluster may don’t have enough resources\nat that time. We will address this problem in later phases with some\nproposed solutions like resource reservation mechanisms.\n\n![Ubernetes Scheduling](ubernetes-scheduling.png)"
  },
  {
    "id" : "375a9d75-2919-429a-abae-bf9e0b0ce365",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "671191a8-baef-41d3-b9f3-1742f699d05b",
        "parentId" : null,
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "@quinton-hoole just before the merge, I want to raise one query.\nWhen we talk about Ubernetes, it means multiple cluster federation. Ideally we want a single cluster to span all the available machines in a single AZ. But in our data center, we have over 2000 hypervisors in single AZ, considering the scalability of current Kubernetes, we would need to build multiple clusters in single AZ.\nFollowing scheduling algorithm described above, there will be circumstance that if two replicas are defined in single pod, and user does not specify any cluster, then the two replicas will be replaced to two least loaded cluster. If the two least loaded cluster is in single AZ, then even the replicas of the pod were split to multiple clusters, they are still in single AZ, it is not fault-tolerance in AZ level.\nI raised this comment to discuss the approaches that,\n1) Will Ubernetes provide a best practice for deployment mode, like one cluster in single AZ, so we can treat K8S and AZ equally. If this choose this approach how we can support the large scale in single AZ? \n2) If we suggest multiple clusters in single AZ, then in Ubernetes Scheduler, we would might need to consider split the replica into different AZ not only different cluster.\n\n@kevin-wangzefeng @alfred-huangjian \n",
        "createdAt" : "2016-03-04T02:19:48Z",
        "updatedAt" : "2016-03-04T02:21:40Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      },
      {
        "id" : "073781b6-e94d-46ec-96d2-2a0b56a627a7",
        "parentId" : "671191a8-baef-41d3-b9f3-1742f699d05b",
        "authorId" : null,
        "body" : "@mfanjie That's a great question, and some of us have recently been giving exactly that some additional thought.  Here is a summary of my opinion on the matter (and @davidopp @wojtek-t purely FYI regarding scheduling and scalability respectively):\n1. Our scaling goals for Kubernetes v1.3 (targeted around 07/2016) are 2000-5000 nodes per cluster, and Google will be investing considerable engineering effort in getting there in the next few months.  Supporting that scale is explicitly within the top three priority items for v1.3 on our product roadmap.  So your scaling goal of 2,000 nodes in a single cluster and AZ should be achievable without federation by mid 2016 anyway.\n2. Notwithstanding the above, one of the valid use cases of Cluster Federation/Ubernetes is to accommodate very large availability zones (tens of thousands of nodes), not because of any particular scaling limitations of Kubernetes, but rather because, based on our experience at Google, that's the right way to build large, highly available applications. Concretely, a cluster upgrade gone wrong, or a cluster control plane malfunction, when the cluster size is enormous, can be very, very bad.  The solution that we support to address this challenge is building, managing and federating multiple clusters.  So we need to address your questions anyway.\n3. Phase 1 of this project, as outlined in this design doc, is intended to be an absolutely minimum viable product.  So please don't see this as the long-term, complete plan for cluster federation.  It is explicitly a first implementation step.\n4.  There are a few possible solutions to the specific limitation that you mention, at least one of which we plan to implement soon after phase 1.   One solution (as you suggest) would be for the cross-cluster scheduler to explicitly spread across zones as well as clusters (in the same way that for multi-zone \"ubernetes lite\" clusters, our scheduler spreads across nodes as well as zones, making sensible choices between putting too many replicas on one node vs putting too many replicas in one zone, when forced to choose between the two).  I think that this is fairly easy, and a good solution.  Another solution (which is already supported in the above design) would be to specify an appropriate NodeSelector to restrict placement to clusters in non-overlapping zones (although this has the downside of artificially introducing hard placement constraints, which has the potential to strand resources). \n\nI will soon be putting together a detailed design document for review, which will cover a variety of pod placement and movement/rescheduling scenarios and designs, where we can figure out all the details, and the best practises that you mention.  I will be sure to keep you in the loop there.\n",
        "createdAt" : "2016-03-04T17:11:59Z",
        "updatedAt" : "2016-03-04T17:11:59Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "f6c5a3fb-dc43-4d3b-b67b-6efc475271ea",
        "parentId" : "671191a8-baef-41d3-b9f3-1742f699d05b",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "Quinton, thanks for your explanation. That makes sense to me. \nOur k8s cluster is not built on bare metal but on VM, we may deploy 3 or more VMs on single hypervisor as k8s minions. So There will be up to 6000+ minions in single AZ.\nI agree it is right approach to build multiple clusters in single AZ, in addition, from my perspective, federation should have the ability to split the sub RC and replica to different AZ, as when someone defines an ubernetes application, as an end user, he may have no info about underlying  infrastructure info like AZ, and he may not care about which AZ to deploy, high availability is the only need. So Ubernetes scheduler should consider this and make the right decision.\nLooking forward your detailed design doc, thank you!\n",
        "createdAt" : "2016-03-04T23:13:13Z",
        "updatedAt" : "2016-03-04T23:13:13Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      },
      {
        "id" : "c721a07b-f7f4-4308-8d73-aa21b6688fe0",
        "parentId" : "671191a8-baef-41d3-b9f3-1742f699d05b",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Thanks for cc-ing me @quinton-hoole - the above explanation SGTM\n",
        "createdAt" : "2016-03-07T10:42:43Z",
        "updatedAt" : "2016-03-07T10:42:43Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "119e945c-31c8-4a40-b57d-debd0a6c64e5",
        "parentId" : "671191a8-baef-41d3-b9f3-1742f699d05b",
        "authorId" : null,
        "body" : "@mfanjie Agreed - user should only specify HA as a high level requirement, not be required to specify which zones to deploy into.  \n\nOne other thing - why do you run multiple smaller VM's on a single hypervisor/host, rather than a single large VM on that host?  We generally favor the latter as the most scalable way to achieve clusters with large amounts of computing resources, i.e for large clusters, it's better to run smaller numbers of larger nodes, than larger numbers of smaller nodes.\n",
        "createdAt" : "2016-03-11T22:40:39Z",
        "updatedAt" : "2016-03-11T22:40:39Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 411,
    "diffHunk" : "@@ -1,1 +409,413 @@       clusterSelector, all replica will be evenly distributed among\n       these clusters.\n\nThere is a potential race condition here. Say at time _T1_ the control\nplane learns there are _m_ available resources in a K8S cluster. As"
  },
  {
    "id" : "f3ca2246-b0fc-42fb-a408-871f0ffab4cc",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77f881c0-92b6-4f90-b9dd-53fd9045afe4",
        "parentId" : null,
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "Need to raise another concern about global DNS, will the global DNS registration work be pluggable? AWS GEO-DNS is stated in federated-services.md, but in eBay we are using F5 GTM(Global Traffic Manager) as our geography aware DNS, and we would need to watch service registered in Ubernetes Control Plane and write entries in GTM, but here it  says \"performs some global DNS registration\", does it mean AWS GEO-DNS explicitly? I except the DNS client, which is responsible for writting DNS entries, should be pluggable. For example, if we can implement GTM client and plug it into Ubernetes service controller. Could you comment on this? thanks \n",
        "createdAt" : "2016-03-09T06:38:30Z",
        "updatedAt" : "2016-03-09T06:59:56Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      },
      {
        "id" : "69c2e82f-9a09-4158-922f-bce285ca4666",
        "parentId" : "77f881c0-92b6-4f90-b9dd-53fd9045afe4",
        "authorId" : null,
        "body" : "@mfanjie Yes, absolutely.  The internal interface to global DNS will be pluggable.  We will probably focus on delivering implementations for GCE and AWS DNS first, but would very much welcome an F5 GTM contribution from the community.  I will be sure to include you on the interface definition discussions to make sure that the plugin interface works well for your case on F5.\n",
        "createdAt" : "2016-03-11T22:45:41Z",
        "updatedAt" : "2016-03-11T22:45:41Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 180,
    "diffHunk" : "@@ -1,1 +178,182 @@clusters.  Besides interacting with services resources on each\nindividual K8S clusters, the Ubernetes service controller also\nperforms some global DNS registration work.\n\n## API OBJECTS"
  },
  {
    "id" : "182438e7-83f2-4ae9-ae62-de06d4ab919e",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a57103b9-1ea6-469b-9c2b-f7735714c663",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "This section could be made significantly clearer by saying something like \"Whereas the Kubernetes scheduler schedules pending pods to nodes, the Ubernetes scheduler creates per-cluster API objects corresponding to Ubernetes control plane API objects.\"\n",
        "createdAt" : "2016-03-14T05:35:21Z",
        "updatedAt" : "2016-03-14T05:35:21Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +139,143 @@time.\n\n## Ubernetes Scheduler\n\nThe Ubernetes Scheduler schedules resources onto the underlying"
  },
  {
    "id" : "1552b97b-434c-421e-bfe9-70a09e067842",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85849e1f-2a45-41f1-b31d-6dc24986ffe1",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "IIUC this sentence (\"It creates a...\") is wrong. Unless I'm mistaken, the Ubernetes scheduler will contact the API server in each cluster to create API objects, not write directly to their etcd.\n",
        "createdAt" : "2016-03-14T05:36:13Z",
        "updatedAt" : "2016-03-14T05:36:13Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "14c4ef66-abc6-4b74-be96-9d789859d4db",
        "parentId" : "85849e1f-2a45-41f1-b31d-6dc24986ffe1",
        "authorId" : null,
        "body" : "As per our in-person discussion, the document is correct as it stands.  I think you agree now.\n",
        "createdAt" : "2016-03-14T22:46:36Z",
        "updatedAt" : "2016-03-14T22:46:36Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +146,150 @@underlying Kubernetes clusters) and performs the global scheduling\nwork.  For each unscheduled replication controller, it calls policy\nengine to decide how to spit workloads among clusters. It creates a\nKubernetes Replication Controller on one ore more underlying cluster,\nand post them back to `etcd` storage."
  },
  {
    "id" : "439af973-971b-4640-b116-609de9af97f2",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d503445-567f-4a48-989f-6bd235b70a05",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I think I mentioned this previously, but calling them sub-resources is really confusing because we already have a different meaning of \"subresource.\"\n",
        "createdAt" : "2016-03-14T05:37:39Z",
        "updatedAt" : "2016-03-14T05:37:39Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "253f0e83-e282-461c-98d5-c7fca05d3af7",
        "parentId" : "6d503445-567f-4a48-989f-6bd235b70a05",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Also, I didn't understand this. From what I read earlier, I thought the Ubernetes control plane would have a ReplicationController with six replicas, and each cluster's control plane would have a ReplicationController with two replicas. What is the \"sub-resource\"? \n",
        "createdAt" : "2016-03-14T05:39:38Z",
        "updatedAt" : "2016-03-14T05:39:38Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "84bc4394-f818-42ec-81c3-79a57acc3d04",
        "parentId" : "6d503445-567f-4a48-989f-6bd235b70a05",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Never mind, I see the explanation later of sub-resource, but see my comment there about why I don't think it's necessary.\n",
        "createdAt" : "2016-03-14T06:09:45Z",
        "updatedAt" : "2016-03-14T06:09:45Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +161,165 @@performs the following two kinds of work:\n\n1. It watches all the sub-resources that are created by Ubernetes\n   components, like a sub-RC or a sub-service. And then it creates the\n   corresponding API objects on the underlying K8S clusters."
  },
  {
    "id" : "f7dd8f6f-3ea8-4f8d-869c-c5b9e901ce94",
    "prId" : 19313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b31a059-51e1-440d-9362-ce9c0e511294",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I don't understand why you need the sub-RCs, or why you want a controller per cluster instead of a single controller.\n\nIt seems simpler to have a single controller that is a \"reconciler\" whose intended/desired state in reflected by the Ubernetes ReplicationController objects, and whose \"observed state\" is reflected by the ReplicationControllers that it creates it each cluster. To make the \"observed state\" match the \"intended\" state\", it creates RCs in the clusters. This is analogous to a regular ReplicationController whose intended state is a pod replica count and observed state is number of pods that are running. It seems to have fewer moving parts than what is proposed here.\n",
        "createdAt" : "2016-03-14T06:08:14Z",
        "updatedAt" : "2016-03-14T06:08:14Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "2108208c-39ad-4144-9308-3f9894280bf5",
        "parentId" : "9b31a059-51e1-440d-9362-ce9c0e511294",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I discussed this with @quinton-hoole in person today and I think he convinced me that the approach described here is reasonable. The two key arguments were\n\n1) The user who submits an RC (call it X) to the Ubernetes control plane may not have direct access to the underlying clusters where the pods run, but they may want to see the status of the per-cluster RCs, so we need per-cluster RC status reflected in the Ubernetes control plane somehow. We could have X's status concatenate all of the RC statuses, but this would mean X is a different kind of RC (its would have an array of statuses, one per cluster, rather than just one status), so we'd lose the symmetry with regular RCs. \n\n2) In theory you could build the Ubernetes control plane so that except for API objects created by users (like X in (1)) it is stateless/only uses soft state. It would work the way I described in the comment above. But it would be useful to be able to maintain some persistent state rather than being completely stateless. Two specific examples are\n- the Ubernetes control plane might want to record the last time it got a status update from each cluster, so that it can detect cluster failure and move workload to a healthy cluster in response\n- the Ubernetes control plane might want to record the decision made by the component this proposal calls the \"Ubernetes scheduler,\" namely, how to divide the number of replicas requested in X amongst the various clusters, so that if the Ubernetes master fails and recovers, it can just continue executing against the scheduler decision that it was in the middle of carrying out when it failed. If you want to record this kind of state, the easiest way to do it is in objects that look exactly like ReplicationControllers and that live in the Ubernetes control plane. They represent the \"intended state\" that the Ubernetes control plane is driving the real clusters towards (by creating ReplicationControllers in those clusters). This also makes debugging easier, as the intended state is represented by objects rather than just in the memory of (the) controller(s) in the control plane.\n",
        "createdAt" : "2016-03-15T04:39:26Z",
        "updatedAt" : "2016-03-15T04:39:26Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "227710d25b044096fc47c104ad8fd9c3e9f999da",
    "line" : 399,
    "diffHunk" : "@@ -1,1 +397,401 @@   metrics from the underlying clusters.\n1. Scheduler is watching all pending RCs. It picks up the RC, make\n   policy-driven decisions and split it into different sub RCs.\n1. Each cluster control is watching the sub RCs bound to its\n   corresponding cluster. It picks up the newly created sub RC."
  }
]