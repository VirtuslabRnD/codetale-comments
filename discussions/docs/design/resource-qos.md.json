[
  {
    "id" : "30e2ddd3-e14b-44e3-b754-8f196ffb550a",
    "prId" : 14943,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75455f2f-78ad-42cd-a9f2-751662182435",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I think it's important to note what the defaulting logic is so there is no confusion when users specify limits and not requests.  If a limit is specified, but there is no corresponding request, the request defaults to the limit.\n",
        "createdAt" : "2015-10-06T19:42:47Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "ccf4eca2-b35d-4f23-b47e-ce2dfdcb0502",
        "parentId" : "75455f2f-78ad-42cd-a9f2-751662182435",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Done.\n",
        "createdAt" : "2015-10-07T01:17:23Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "a64fe6572abbdfc945952e35b47c36a1b3293ae9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +52,56 @@For each resource, containers can specify a resource request and limit, `0 <= request <= [Node Allocatable](../proposals/node-allocatable.md)` & `request <= limit <= Infinity`.\nIf a pod is successfully scheduled, the container is guaranteed the amount of resources requested.\nScheduling is based on `requests` and not `limits`.\nThe pods and its containers will not be allowed to exceed the specified limit.\nHow the request and limit are enforced depends on whether the resource is [compressible or incompressible](resources.md)."
  },
  {
    "id" : "d94078dc-c667-4693-874c-9884a1b88c40",
    "prId" : 14943,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce231239-b59a-402a-831f-2add860c133c",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Maybe explicitly to note here: the limits default to the node capacity ?\n",
        "createdAt" : "2016-05-18T00:10:34Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "5b6da6be-2648-4567-abe6-56153db84e8f",
        "parentId" : "ce231239-b59a-402a-831f-2add860c133c",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "It is explicitly mentioned a few lines below.\n",
        "createdAt" : "2016-05-18T00:20:38Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "a64fe6572abbdfc945952e35b47c36a1b3293ae9",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +169,173 @@```\n\n- If `requests` and `limits` are not set for all of the resources, across all containers, then the pod is classified as **Best-Effort**.\n\nExamples:"
  },
  {
    "id" : "4f167b24-1a7a-4eac-85b5-a6ba93d285ee",
    "prId" : 14943,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac30ddfa-360f-4a1b-b5a9-be3e5d0f2294",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "I am ok with this change today to applying one oom score for entire pod. \n\nBut in the future, I am thinking distinguishing the containers within a pod with a different oom score? For example two containers in a Pod: A and B, A has both request and limit specified and they are equal, B doesn't have `requests` and `limits` being. With the new policy introduced in this pr, the pod is burstable, but the user might want to oom kill to kill B first before A. I know there are a lot of corner cases to resolve, let's iterate this later. \n",
        "createdAt" : "2016-05-18T00:26:21Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "a1349bb4-d442-4feb-bbc2-04d849d2d76e",
        "parentId" : "ac30ddfa-360f-4a1b-b5a9-be3e5d0f2294",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "In the future, we can hopefully not use OOM scores and have the kernel support priorities :)\nI agree with you @dchen1107 \n",
        "createdAt" : "2016-05-18T00:39:10Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "e324f733-1c71-493b-bd12-665826429185",
        "parentId" : "ac30ddfa-360f-4a1b-b5a9-be3e5d0f2294",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I actually think it will be a little while before users are savvy enough to want differentiated OOM among containers in a pod.  The policy described here is easier to explain which means it's easier for developers to reason against at the moment.\n",
        "createdAt" : "2016-05-18T01:23:38Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "d306679a-6712-4e2d-813f-18bcb8d629d6",
        "parentId" : "ac30ddfa-360f-4a1b-b5a9-be3e5d0f2294",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Agreed now the simplest solution is the best given Kubernetes is already quite complicated.  \n",
        "createdAt" : "2016-05-20T18:45:45Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "a64fe6572abbdfc945952e35b47c36a1b3293ae9",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +195,199 @@### OOM Score configuration at the Nodes\n\nPod OOM score configuration\n- Note that the OOM score of a process is 10 times the % of memory the process consumes, adjusted by OOM_SCORE_ADJ, barring exceptions (e.g. process is launched by root). Processes with higher OOM scores are killed.\n- The base OOM score is between 0 and 1000, so if process A’s OOM_SCORE_ADJ - process B’s OOM_SCORE_ADJ is over a 1000, then process A will always be OOM killed before B."
  }
]