[
  {
    "id" : "0468f47c-d736-471f-b571-f53de4a50e4b",
    "prId" : 18265,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef589fdb-4fde-4c9e-98b0-1d6446a82fba",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Should have omitempty. Nil is default\n",
        "createdAt" : "2016-01-24T19:18:33Z",
        "updatedAt" : "2016-01-24T19:18:33Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "cea5cf42b68205c28bf164cdfb40e0fa6d9d8adc",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +160,164 @@\t// The json tag here is not \"omitempty\" since we need to distinguish nil and empty.\n\t// See https://golang.org/pkg/encoding/json/#Marshal for more details.\n\tNamespaces []api.Namespace  `json:\"namespaces\"`\n\t// empty topology key is interpreted by the scheduler as \"all topologies\"\n\tTopologyKey string `json:\"topologyKey,omitempty\"`"
  },
  {
    "id" : "2d694a8b-5683-440d-a17c-9145e5e50e72",
    "prId" : 18265,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e426ff49-65e0-4d5d-8a63-cf441ffc8c56",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "These type names aren't part of the API, so we'll need to rework these comments in the real implementation.\n",
        "createdAt" : "2016-01-24T19:22:49Z",
        "updatedAt" : "2016-01-24T19:22:49Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "cea5cf42b68205c28bf164cdfb40e0fa6d9d8adc",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +126,130 @@    // system will try to eventually evict the pod from its node.\n\t// When there are multiple elements, the lists of nodes corresponding to each\n\t// PodAffinityTerm are intersected, i.e. all terms must be satisfied.\n\tRequiredDuringSchedulingRequiredDuringExecution []PodAffinityTerm  `json:\"requiredDuringSchedulingRequiredDuringExecution,omitempty\"`\n    // If the anti-affinity requirements specified by this field are not met at"
  },
  {
    "id" : "76ea9fdb-1897-4530-a6ba-dff0bd13770a",
    "prId" : 18265,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66a1b670-3748-4bdf-bc37-c74b194fc500",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "I don't think this is necessary now, and other approaches could mitigate abuse, such as restricting namespaces that could be specified.\n",
        "createdAt" : "2016-01-24T21:41:41Z",
        "updatedAt" : "2016-01-24T21:41:41Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "44a7c0b7-1560-4c9a-be24-67098807a8b9",
        "parentId" : "66a1b670-3748-4bdf-bc37-c74b194fc500",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Yes, that's reasonable. I think what we'd specifically want is: \"prohibit specifying 'all namespaces' with non-'node' TopologyKey for RequiredDuringScheduling anti-affinity? (I think we don't want to prohibit \"exclusive machine\".)\n",
        "createdAt" : "2016-01-24T22:11:08Z",
        "updatedAt" : "2016-01-24T22:11:08Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "cea5cf42b68205c28bf164cdfb40e0fa6d9d8adc",
    "line" : 508,
    "diffHunk" : "@@ -1,1 +506,510 @@affinity and anti-affinity into account. Include a workaround for the issue described at the end of the Affinity section of the Examples section (can't schedule first pod).\n3. Implement a scheduler priority function that takes `PreferredDuringSchedulingIgnoredDuringExecution` affinity and anti-affinity into account\n4. Implement a quota mechanism that charges for the entire topology domain when `RequiredDuringScheduling` anti-affinity is used. Later\nthis should be refined to only apply when it is used to request exclusive access, not when it is used to express conflict with specific pods.\n5. Implement the recommended solution to the \"co-existing with daemons\" issue"
  },
  {
    "id" : "943c293f-2f6a-42ee-8f55-e0b224602382",
    "prId" : 18265,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c37cb100-d81f-414e-830b-f7964de2e09d",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "One could view this similarly to pod anti-affinity. The taints don't need to be actually propagated to nodes in order to be respected.\n",
        "createdAt" : "2016-01-24T21:44:27Z",
        "updatedAt" : "2016-01-24T21:44:27Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "cea5cf42b68205c28bf164cdfb40e0fa6d9d8adc",
    "line" : 580,
    "diffHunk" : "@@ -1,1 +578,582 @@high-level policy about different classes of special machines and the users who belong to the groups\nallowed to access them). Moreover, the concept of nodes \"inheriting\" labels\nfrom pods seems complicated; it seems conceptually simpler to separate rules involving\nrelatively static properties of nodes from rules involving which other pods are running\non the same node or larger topology domain."
  }
]