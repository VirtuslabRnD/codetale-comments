[
  {
    "id" : "3fc3f3e0-92c1-45b4-af9a-b5c11c9dfd37",
    "prId" : 3281,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62b6f84e-5317-44dd-b729-d58491f33bb5",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "does location mean IP address or something else?\n",
        "createdAt" : "2015-01-08T17:24:38Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "9b3b14ea-995b-4a8d-b5be-75ca2b617bc2",
        "parentId" : "62b6f84e-5317-44dd-b729-d58491f33bb5",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Do we want to state here that for now, the master must reach the node via a pathway that the node can reach itself?  Or do we want to bake in the distinction that what node thinks of as its name may not be how the master reaches it?  I would prefer the former, but expect people to ask about the latter.\n",
        "createdAt" : "2015-01-08T18:04:54Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "576e5d53-db03-44c2-a8f2-961697ff4816",
        "parentId" : "62b6f84e-5317-44dd-b729-d58491f33bb5",
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "Not sure what you are meaning by pathway.  Adding some clarification here that we need consistency so that we can verify certificates, at the least.\n\nSuggest some language and I'll include it :)\n",
        "createdAt" : "2015-01-09T17:06:34Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      }
    ],
    "commit" : "3047d2a4bc749952c929b7f67854f8f1dec8a8d9",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +8,12 @@\n1. **Master -> Node**  The master needs to know which nodes can take work and what their current status is wrt capacity.\n  1. **Location** The master knows the name and location of all of the nodes in the cluster.\n\t  * For the purposes of this doc, location and name should be enough information so that the master can open a TCP connection to the Node.  Most probably we will make this either an IP address or a DNS name.  It is going to be important to be consistent here (master must be able to reach kubelet on that DNS name) so that we can verify certificates appropriately.\n  2. **Target AuthN** A way to securely talk to the kubelet on that node.  Currently we call out to the kubelet over HTTP.  This should be over HTTPS and the master should know what CA to trust for that node."
  },
  {
    "id" : "8a968aec-da65-47c5-8072-2e8b143d9c2f",
    "prId" : 3281,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc0d1a1a-321b-4770-9f7d-1135a9f9b08b",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "We should think about how policy is linked to a new Kubelet account.  Several options come to mind:\n1. Some thing automatically creates one or more new policy statements each time a kubelet account is generated.  Will result in a lot of policy statements.  But, a simpler policy language will suffice.\n2. Kubelet accounts are automatically created as members of a Kubelet group.  Policy is written with the group as the pricipal, and individual kubelets accounts are not mentioned.  This would avoid duplication of policy line.  But it means we need to add \"groups\" as a core resource in kubernetes.  And will need to have some way to restrict access of individual kubelet to resources intended for it, such as a policy statement with a condition clause that compares the IP of the kubelet with the IP the pod is bound to, etc. \n3.  Special case handling for kubelet accounts which is different from other principals.\n",
        "createdAt" : "2015-01-08T17:51:01Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "753a96fb-e092-494f-b556-702efd1482fb",
        "parentId" : "dc0d1a1a-321b-4770-9f7d-1135a9f9b08b",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Practically speaking, the need to say \"the kubelet is allowed to do things that are specific to its inherent identity\" is similar to service accounts or other special users.  It would be ideal if we could somehow tie identity to policy in a way that a single policy represents what an identity of form X can do.  In the long run, we expect the kubelet policy, or extension plugins, to have well defined rights and behaviors.  It seems better to make that possible to do in an easily understandable way(in the long run).  Does the kubelet policy really need to be that flexible?  We might want to add to what it can do, but its core policy is likely a fundamental aspect of the system.\n\nPossibly that calls for something like a special policy rule that applies to a kubelet identity with a default checker that can have overrides.\n\n```\nPolicy:\n   any-identity-that-matches-this-regex: kubelet@<host>\n      acts-as-kubelet\n      allow fooresource GET <...>\n```\n\nwhere acts-as-kubelet is an alias for \"we have a special coded checker that has the minimal policy for a kubelet\".  That reduces the need for policy to be totally generic to the level of checking request parameters and such.\n",
        "createdAt" : "2015-01-08T18:13:20Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "91aa87ed-0648-4146-b7c1-0cd7468895fa",
        "parentId" : "dc0d1a1a-321b-4770-9f7d-1135a9f9b08b",
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "I was thinking something more along the lines of what @smarterclayton is suggesting to start with.  We'd name the kubelet accounts with something we could glob on (kubelet@host or kubelet:host or whatever) and then apply policy there. We could hard code it to start.\n",
        "createdAt" : "2015-01-09T01:30:00Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      }
    ],
    "commit" : "3047d2a4bc749952c929b7f67854f8f1dec8a8d9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +42,46 @@* [optional] **API driven CA** Optionally, we will run a CA in the master that will mint certificates for the nodes/kubelets.  There will be pluggable policies that will automatically approve certificate requests here as appropriate.\n  * **CA approval policy**  This is a pluggable policy object that can automatically approve CA signing requests.  Stock policies will include `always-reject`, `queue` and `insecure-always-approve`.  With `queue` there would be an API for evaluating and accepting/rejecting requests.  Cloud providers could implement a policy here that verifies other out of band information and automatically approves/rejects based on other external factors.\n* **Scoped Kubelet Accounts** These accounts are per-minion and (optionally) give a minion permission to register itself.\n\t* To start with, we'd have the kubelets generate a cert/account in the form of `kubelet:<host>`.  To start we would then hard code policy such that we give that particular account appropriate permissions.  Over time, we can make the policy engine more generic.\n* [optional] **Bootstrap API endpoint** This is a helper service hosted outside of the Kubernetes cluster that helps with initial discovery of the master."
  },
  {
    "id" : "4680e394-3c21-4236-b65d-833eae10f380",
    "prId" : 3281,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "815b0d28-5b36-4074-9991-3b6012cef2cc",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Should clarify here that this is currently \"master\" but could be \"masters\" and eventually will change over time (maybe as \"not-considered-yet-but-known-issues\" in a later section)\n",
        "createdAt" : "2015-01-08T18:07:29Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "d346be08-6597-41c0-8c87-955d75e9fe84",
        "parentId" : "815b0d28-5b36-4074-9991-3b6012cef2cc",
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "Adding a note.\n",
        "createdAt" : "2015-01-09T17:08:20Z",
        "updatedAt" : "2015-01-09T17:09:09Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      }
    ],
    "commit" : "3047d2a4bc749952c929b7f67854f8f1dec8a8d9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +13,17 @@  3. **Caller AuthN/Z** This would be the master verifying itself (and permissions) when calling the node.  Currently, this is only used to collect statistics as authorization isn't critical.  This may change in the future though.\n2. **Node -> Master**  The nodes currently talk to the master to know which pods have been assigned to them and to publish events.\n  1. **Location** The nodes must know where the master is at.\n  2. **Target AuthN** Since the master is assigning work to the nodes, it is critical that they verify whom they are talking to.\n  3. **Caller AuthN/Z** The nodes publish events and so must be authenticated to the master. Ideally this authentication is specific to each node so that authorization can be narrowly scoped.  The details of the work to run (including things like environment variables) might be considered sensitive and should be locked down also."
  }
]