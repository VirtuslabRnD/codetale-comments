[
  {
    "id" : "e16e2337-baeb-410f-a93b-b619d820c87c",
    "prId" : 4519,
    "prUrl" : "https://github.com/apache/kafka/pull/4519#pullrequestreview-102903512",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27e4459b-bcda-494e-97ee-dd14a33dea62",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "Note users can also explicitly override the default assigned timestamp of the generated results when sending to the downstream processor via <code>#forward()</code>",
        "createdAt" : "2018-03-11T19:16:55Z",
        "updatedAt" : "2018-03-16T00:26:51Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      },
      {
        "id" : "184ff9f3-8d88-42c5-ac8c-5a56fc24266b",
        "parentId" : "27e4459b-bcda-494e-97ee-dd14a33dea62",
        "authorId" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "body" : "Yes. It seems to be covered already in L125.",
        "createdAt" : "2018-03-12T00:34:13Z",
        "updatedAt" : "2018-03-16T00:26:51Z",
        "lastEditedBy" : "9baadc38-cd41-4762-be0c-791258ced78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a24643caadd76196cc325e747476f3514af8a381",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +129,133 @@\n    <p>\n\tNote, that the describe default behavior can be changed in the Processor API by assigning timestamps to output records explicitly when calling <code>#forward()</code>.\n    </p>\n"
  },
  {
    "id" : "eaa24db9-d43f-4973-a5ef-4f5c87e412f9",
    "prId" : 4876,
    "prUrl" : "https://github.com/apache/kafka/pull/4876#pullrequestreview-112472683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ea6766e-66da-4ff2-81ff-e93e039da7fd",
        "parentId" : null,
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "We can make it `<a id=\"#streams_processor_node\" href=\"#streams_processor_node\">` as we did for other refs.",
        "createdAt" : "2018-04-16T15:39:47Z",
        "updatedAt" : "2018-04-16T16:49:32Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc8f9de42d1c1d577bf859028b67208a5f33f2ee",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +64,68 @@        <li>A <b>stream</b> is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set. A stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a <b>data record</b> is defined as a key-value pair.</li>\n        <li>A <b>stream processing application</b> is any program that makes use of the Kafka Streams library. It defines its computational logic through one or more <b>processor topologies</b>, where a processor topology is a graph of stream processors (nodes) that are connected by streams (edges).</li>\n        <li>A <b><a id=\"streams_processor_node\" href=\"#streams_processor_node\">stream processor</a></b> is a node in the processor topology; it represents a processing step to transform data in streams by receiving one input record at a time from its upstream processors in the topology, applying its operation to it, and may subsequently produce one or more output records to its downstream processors. </li>\n    </ul>\n"
  },
  {
    "id" : "490be9c9-4662-4fea-a3b4-d4e98337fd36",
    "prId" : 5458,
    "prUrl" : "https://github.com/apache/kafka/pull/5458#pullrequestreview-154002660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cac25f60-c5b7-4efd-991f-e3e0e9a3d40f",
        "parentId" : null,
        "authorId" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "body" : "minor typo: `many stream processing applications` instead of `many stream processing application`",
        "createdAt" : "2018-09-07T04:22:37Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "403b8bdd-d152-4255-86a7-0bdb3d2b40a5",
        "tags" : [
        ]
      },
      {
        "id" : "55350236-86ba-46e5-be5f-448de597e1d3",
        "parentId" : "cac25f60-c5b7-4efd-991f-e3e0e9a3d40f",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "ack, thanks @satishd !",
        "createdAt" : "2018-09-10T23:28:10Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b4a48c9ba077b62b4160aad95036574f52d677d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +176,180 @@\n    <p>\n        Besides the guarantee that each record will be processed exactly-once, another issue that many stream processing application will face is how to\n        handle <a href=\"tbd\">out-of-order data</a> that may impact their business logic. In Kafka Streams, there are two causes that could potentially\n        result in out-of-order data arrivals with respect to their timestamps:"
  },
  {
    "id" : "a9fa32dc-5cf8-4386-85e3-63a1a3cbc4ab",
    "prId" : 5458,
    "prUrl" : "https://github.com/apache/kafka/pull/5458#pullrequestreview-154434875",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d4c0af0-81d2-459d-94e1-8a5a3894ffa7",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "nit: `handle` -> `handling`",
        "createdAt" : "2018-09-11T20:51:32Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "0c215f1a-9da7-413b-9d06-c9c5d835bd70",
        "parentId" : "6d4c0af0-81d2-459d-94e1-8a5a3894ffa7",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "hmm.. I think `... is how to handle` is correct?",
        "createdAt" : "2018-09-11T23:22:16Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b4a48c9ba077b62b4160aad95036574f52d677d",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +177,181 @@    <p>\n        Besides the guarantee that each record will be processed exactly-once, another issue that many stream processing application will face is how to\n        handle <a href=\"tbd\">out-of-order data</a> that may impact their business logic. In Kafka Streams, there are two causes that could potentially\n        result in out-of-order data arrivals with respect to their timestamps:\n    </p>"
  },
  {
    "id" : "ec7005c5-3b58-422d-972a-f877f4698832",
    "prId" : 5458,
    "prUrl" : "https://github.com/apache/kafka/pull/5458#pullrequestreview-154435159",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be84e167-81f2-455d-9f9a-b0ae537d4ab5",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "Is it worth mentioning that we wish to fix this in the future?",
        "createdAt" : "2018-09-11T20:58:56Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "54f6d31a-2f96-49ed-8b05-243b3dd478a8",
        "parentId" : "be84e167-81f2-455d-9f9a-b0ae537d4ab5",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "I think in web docs we would not need to mention future work plans, since it is usually \"time dependent\" but the web docs do not usually illustrate when the content was written, and hence cannot tell when does \"future\" really means.",
        "createdAt" : "2018-09-11T23:23:38Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b4a48c9ba077b62b4160aad95036574f52d677d",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +200,204 @@    <ul>\n        <li> For Stream-Stream joins, all three types (inner, outer, left) handle out-of-order records correctly, but the resulted stream may contain unnecessary leftRecord-null for left joins, and leftRecord-null or null-rightRecord for outer joins. </li>\n        <li> For Stream-Table joins, out-of-order records are not handled (i.e., Streams applications don't check for out-of-order records and just process all records in offset order), and hence it may produce unpredictable results. </li>\n        <li> For Table-Table joins, out-of-order records are not handled (i.e., Streams applications don't check for out-of-order records and just process all records in offset order). However, the join result is a changelog stream and hence will be eventually consistent. </li>\n    </ul>"
  },
  {
    "id" : "ba806c3f-1b73-47e2-a2e0-daeeede24a85",
    "prId" : 5458,
    "prUrl" : "https://github.com/apache/kafka/pull/5458#pullrequestreview-154435485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee3f4130-1dfb-4c51-8def-21e57d70623e",
        "parentId" : null,
        "authorId" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "body" : "This seems to be true only for windowed agg/joins. For non-windowed aggregations, there's nothing you can do right now; we always process in order of offset (for now). Does this deserve a separate call-out?",
        "createdAt" : "2018-09-11T21:02:12Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "f84c555e-0e5d-4773-b994-4121b6b8dada",
        "tags" : [
        ]
      },
      {
        "id" : "fa5734b2-457f-48c0-be6e-532a1c7d999b",
        "parentId" : "ee3f4130-1dfb-4c51-8def-21e57d70623e",
        "authorId" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "body" : "That's a good point! Will incorporate.",
        "createdAt" : "2018-09-11T23:25:14Z",
        "updatedAt" : "2018-09-11T23:28:15Z",
        "lastEditedBy" : "eba565e8-e1d5-4749-9c9a-5d117de6c96c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b4a48c9ba077b62b4160aad95036574f52d677d",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +193,197 @@        For stateless operations, out-of-order data will not impact processing logic since only one record is considered at a time, without looking into the history of past processed records;\n        for stateful operations such as aggregations and joins, however, out-of-order data could cause the processing logic to be incorrect. If users want to handle such out-of-order data, generally they need to allow their applications\n        to wait for longer time while bookkeeping their states during the wait time, i.e. making trade-off decisions between latency, cost, and correctness.\n        In Kafka Streams specifically, users can configure their window operators for windowed aggregations to achieve such trade-offs (details can be found in <a href=\"/{{version}}/documentation/streams/developer-guide\"><b>Developer Guide</b></a>).\n        As for Joins, users have to be aware that some of the out-of-order data cannot be handled by increasing on latency and cost in Streams yet:"
  }
]