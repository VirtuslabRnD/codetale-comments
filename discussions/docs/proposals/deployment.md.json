[
  {
    "id" : "7bf233c7-f67c-453d-9cb5-3c3d197d8180",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7a93acc-064e-43ff-8550-c4e7823948a8",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Should default to 1, like ReplicationController.\n",
        "createdAt" : "2015-08-05T17:23:18Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "ad22a839-1387-4305-a72f-a80d6b1afd7b",
        "parentId" : "a7a93acc-064e-43ff-8550-c4e7823948a8",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Added that to comment\n",
        "createdAt" : "2015-08-05T23:43:58Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +65,69 @@\ntype DeploymentSpec struct {\n  // Number of desired pods. This is a pointer to distinguish between explicit\n  // zero and not specified. Defaults to 1.\n  Replicas *int"
  },
  {
    "id" : "f00675ef-5e10-4f38-b343-759451df7b85",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0917bed3-8170-4fe4-aed5-871327c35049",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "We also need the equivalent of --deployment-label-key:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubectl/cmd/rollingupdate.go#L78\n",
        "createdAt" : "2015-08-06T00:31:32Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "229d0056-3c0b-4472-96c8-35d98630d2e4",
        "parentId" : "0917bed3-8170-4fe4-aed5-871327c35049",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Added `uniqueLabel *string` as per https://github.com/GoogleCloudPlatform/kubernetes/issues/12298#issuecomment-128636518\n",
        "createdAt" : "2015-08-11T00:06:05Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : null,
    "diffHunk" : "@@ -1,1 +72,76 @@  // selected by this will be scaled down.\n  Selector map[string]string\n\n  // Describes the pods that will be created.\n  Template *PodTemplateSpec"
  },
  {
    "id" : "b0751561-6b43-478b-81ff-cb9cc4708ead",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f3c905c-b323-4234-8a79-a71f4ba65da4",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "You're thinking this is a discriminated union (aka OneOf)?\n",
        "createdAt" : "2015-08-06T00:32:08Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "c7d24a93-b445-4dde-949d-70eef61f42fc",
        "parentId" : "0f3c905c-b323-4234-8a79-a71f4ba65da4",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "DeploymentType will always be there (user specified or defaulted).\nOther fields will be oneOf based on the DeploymentType.\nRight now, there is just RollingUpdateSpec. Will add more specs as we add more DeploymentType's.\n",
        "createdAt" : "2015-08-06T01:18:04Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "79576fe7-a6b2-45bc-9049-9502e1e52385",
        "parentId" : "0f3c905c-b323-4234-8a79-a71f4ba65da4",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Hrm, we took type out of Volume.  Shouldn't this follow the same pattern? \n",
        "createdAt" : "2015-08-11T23:41:49Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "7ca43cc9-3ba2-4dbe-82fa-a46b21a456c1",
        "parentId" : "0f3c905c-b323-4234-8a79-a71f4ba65da4",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Union/OneOf is something we should add to the API conventions. \n\nWe have feedback that undiscriminated unions are problematic: #6979.\n\nWe could just put a TODO here to follow the convention, whatever we decide it should be.\n",
        "createdAt" : "2015-08-12T02:28:48Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "14f340c7-6320-4610-931a-709459615283",
        "parentId" : "0f3c905c-b323-4234-8a79-a71f4ba65da4",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "On Aug 11, 2015, at 10:29 PM, Brian Grant notifications@github.com wrote:\n\nIn docs/proposals/deployment.md\nhttps://github.com/GoogleCloudPlatform/kubernetes/pull/12236#discussion_r36821874\n:\n\n> -  Replicas *int\n>   +\n> -  // Label selector for pods. Existing ReplicationControllers which have all these\n> -  // label selectors will be scaled down.\n> -  Selector map[string][string]\n>   +\n> -  // Describes the pods that will be created.\n> -  Template *PodTemplateSpec\n>   +\n> -  // The deployment strategy to use to replace existing pods with new ones.\n> -  Strategy DeploymentStrategy\n>   +}\n>   +\n>   +type DeploymentStrategy struct {\n> -  // Type of deployment. Can be \"Recreate\" or \"RollingUpdate\".\n> -  Type DeploymentType\n\nUnion/OneOf is something we should add to the API conventions.\n\nWe have feedback that undiscriminated unions are problematic: #6979\nhttps://github.com/GoogleCloudPlatform/kubernetes/issues/6979.\n\nOur UI team has complained because of that as well, also, the Type field\nbeing CamelCase isn't much better because they can't use the value as a\nfield index for the type, but instead have to use a switch or change the\ncase.  Ie \"strategy[strategy.type]\" in JS doesn't return\n\"strategy.rollingUpdate\" because of case.\n\nWe could just put a TODO here to follow the convention, whatever we decide\nit should be.\n\n窶能nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/kubernetes/pull/12236/files#r36821874\n.\n",
        "createdAt" : "2015-08-12T02:32:54Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "5d6b524c-c770-455a-b2fd-00a6c3baeb75",
        "parentId" : "0f3c905c-b323-4234-8a79-a71f4ba65da4",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Adding a TODO\n",
        "createdAt" : "2015-08-12T20:56:03Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +91,95 @@type DeploymentStrategy struct {\n  // Type of deployment. Can be \"Recreate\" or \"RollingUpdate\".\n  Type DeploymentType\n\n  // TODO: Update this to follow our convention for oneOf, whatever we decide it"
  },
  {
    "id" : "f6bfa76e-7938-47fd-b5c6-58d40c7ac6b0",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ffa1a04-d33b-4147-b31d-0ea0d0c0003e",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Needs more comment -- explain what these mean.\n",
        "createdAt" : "2015-08-12T04:22:14Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "3c1bd673-b097-428b-bbfb-d5c54d6793e7",
        "parentId" : "1ffa1a04-d33b-4147-b31d-0ea0d0c0003e",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Oh, I see you wrote the comment with the const definition. I guess that's OK.\n",
        "createdAt" : "2015-08-12T04:22:49Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : null,
    "diffHunk" : "@@ -1,1 +90,94 @@\ntype DeploymentStrategy struct {\n  // Type of deployment. Can be \"Recreate\" or \"RollingUpdate\".\n  Type DeploymentType\n"
  },
  {
    "id" : "cb8a64b7-d0dc-487c-b898-175f0a648e44",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65a1f4b9-2a27-45bd-911b-ef4b6d350da2",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "See my earlier comment -- you mean all RCs whose labels match DeploymentSpec.Selector, right?\n\nIf you actually mean all RCs whose Selector matches DeploymentSpec.Selector, then I'm confused. I can see that would have the effect of having the Deployment find all RCs that manage Pods that match the DeploymentSpec.Selector, but it wasn't clear to me why you want to do that instead of matching RCs. Either way, I strongly suggest explaining the motivation, since I can't be the only person who would be confused.\n",
        "createdAt" : "2015-08-12T05:11:38Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "d84b6fab-0fe9-406a-9b0e-62651cc05cd6",
        "parentId" : "65a1f4b9-2a27-45bd-911b-ef4b6d350da2",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "No, this is a pod selector. If we decide to (optionally) generate a Service from Deployment, it would use the exact same selector.\n\nThe RC selectors would be derived from it, with the uniquifying label injected.\n\nIf it were a selector of RCs, we'd need a RC template rather than a pod template.\n",
        "createdAt" : "2015-08-12T06:09:59Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +161,165 @@For each pending deployment, it will:\n\n1. Find all RCs whose label selector is a superset of DeploymentSpec.Selector.\n   - For now, we will do this in the client - list all RCs and then filter the\n     ones we want. Eventually, we want to expose this in the API."
  },
  {
    "id" : "4c564431-053e-440e-8365-1f214c26ce25",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4096c27b-250d-4a4d-86ca-34ee186d167d",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "We need to specify rounding. I propose that we round up for both MaxUnavailable and MaxSurge.\n",
        "createdAt" : "2015-08-13T00:03:35Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "e2423ec0-257c-4d04-afe4-2fee9b16840a",
        "parentId" : "4096c27b-250d-4a4d-86ca-34ee186d167d",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "But I could be persuaded that rounding down is safer, if anyone wants to take that position. :-)\n",
        "createdAt" : "2015-08-13T00:04:50Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "6ece890e-0357-426f-9af7-391205f48de9",
        "parentId" : "4096c27b-250d-4a4d-86ca-34ee186d167d",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Added a comment saying we round up.\n",
        "createdAt" : "2015-08-13T01:20:29Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : null,
    "diffHunk" : "@@ -1,1 +117,121 @@  // This can not be 0 if MaxSurge is 0.\n  // By default, a fixed value of 1 is used.\n  // Example: when this is set to 30%, the old RC can be scaled down by 30%\n  // immediately when the rolling update starts. Once new pods are ready, old RC\n  // can be scaled down further, followed by scaling up the new RC, ensuring"
  },
  {
    "id" : "079f6637-9d30-4562-b378-51f9dca512bc",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4f0558d-6d75-4eb7-93bb-1f090e1e566f",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Detail: Not valid for both MaxUnavailable and MaxSurge to be 0 or 0%\n",
        "createdAt" : "2015-08-13T00:05:30Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "4260e386-3f3a-4377-bb4e-aac5a26aaee6",
        "parentId" : "c4f0558d-6d75-4eb7-93bb-1f090e1e566f",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Added a comment\n",
        "createdAt" : "2015-08-13T01:20:15Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +133,137 @@  // immediately when the rolling update starts. Once old pods have been killed,\n  // new RC can be scaled up further, ensuring that total number of pods running\n  // at any time during the update is atmost 130% of original pods.\n  MaxSurge IntOrString\n"
  },
  {
    "id" : "d4baa816-55a1-42c1-9265-e7cc1dbde4c4",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "parentId" : null,
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "It seems to me that there needs to be a way to track the status of a deployment process to make these decisions. For example, `DeploymentStatus` could have a `[]Condition` which has types representing in-progress states like \"pending\" or \"running\", and terminal states like \"completed\" or \"failed\". The controller could then correctly respond to hash updates while a deployment is in progress (e.g. cancel current, await completion). The condition also would provide a reference to the new RC.\n",
        "createdAt" : "2015-08-13T18:00:14Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "29c1a3a9-ecb9-4beb-8c0a-a1f5a9a7dae3",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "I agree that []Condition in DeploymentStatus can be helpful. Will add that.\nHow does it provide a reference to the new RC?\n",
        "createdAt" : "2015-08-13T19:35:49Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "36f8fcc8-3a14-4d4c-84e3-45f0ec203152",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "I'm still thinking through the right way to describe this per #7856 (naturally, my first inclination was to add a `Phase` field, which is no good) :grin:\n",
        "createdAt" : "2015-08-13T19:46:00Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "f28952f2-b01a-4459-bca7-d6ed84d72fba",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "I discussed this with @bgrant0607 and it doesnt look like we need conditions here.\nDeployment will be \"always running\" like ReplicationController. It will not succeed or fail, but will always keep trying. Incase of errors, we can add exponential backoff and generate events, but it should keep trying.\n\nDeploymentStatus will have Replicas (total old+new pods) and UpdatedReplicas(new pods) fields which can be used to track its progress.\n",
        "createdAt" : "2015-08-13T20:43:13Z",
        "updatedAt" : "2015-08-13T20:57:49Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "c5dc5d84-1e8e-4b25-8932-c953361ef7b0",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Hrm - when we discussed in the hangout I thought we talked about \"permfail\"\nin a particular template if it it violates some implicit condition (like no\npods ever become ready) and we restore the original 100% availability?\n\nOn Thu, Aug 13, 2015 at 4:43 PM, Nikhil Jindal notifications@github.com\nwrote:\n\n> In docs/proposals/deployment.md\n> https://github.com/kubernetes/kubernetes/pull/12236#discussion_r37022839\n> :\n> \n> > -     - We will first add this to RC.PodTemplateSpec.Metadata.Labels for all RCs to\n> > -       ensure that all new pods that they create will have this label.\n> > -     - Then we will add this label to their existing pods and then add this as a selector\n> > -       to that RC.\n> >   +3. Find if there exists an RC for which value of \"deployment.kubernetes.io/podTemplateHash\" label\n> > -   is same as hash of DeploymentSpec.PodTemplateSpec. If it exists already, then\n> > -   this is the RC that will be ramped up. If there is no such RC, then we create\n> > -   a new one using DeploymentSpec and then add a \"deployment.kubernetes.io/podTemplateHash\" label\n> > -   to it. RCSpec.replicas = 0 for a newly created RC.\n> >   +4. Scale up the new RC and scale down the olds ones as per the DeploymentStrategy.\n> > -   - Raise an event if we detect an error, like new pods failing to come up.\n> >   +5. Go back to step 1 unless the new RC has been ramped up to desired replicas\n> > -   and the old RCs have been ramped down to 0.\n> >   +6. Cleanup.\n> >   +\n> >   +DeploymentController is stateless so that it can recover incase it crashes during a deployment.\n> \n> I discussed this with @bgrant0607 https://github.com/bgrant0607 and it\n> doesnt look like we need conditions here.\n> Deployment will be \"always running\" like ReplicationController. It will\n> not succeed or fail, but will always keep trying. Incase of errors, we can\n> add exponential backoff and generate events, but it should not keep trying.\n> \n> Status will have Replicas (total old+new pods) and UpdatedReplicas(new\n> pods) fields which can be used to track its progress.\n> \n> 窶能n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/12236/files#r37022839.\n\n## \n\nClayton Coleman | Lead Engineer, OpenShift\n",
        "createdAt" : "2015-08-18T01:02:19Z",
        "updatedAt" : "2015-08-18T01:02:19Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "c72d6f4a-f420-4987-95c4-4a22f86063fe",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "No. My understanding is that we will never permfail.\nEven if pods, dont become ready, we will keep trying. User can fix the problem in the meantime (for ex: if image didnt exist, user can add the image) and then pods will start becoming ready.\n",
        "createdAt" : "2015-08-18T20:55:07Z",
        "updatedAt" : "2015-08-18T20:55:07Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "7f7ce52a-afd3-4001-bf69-eccb50fbafa5",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "This was \"automatic rollback on error\", which is listed under future improvements, below.\n",
        "createdAt" : "2015-08-18T23:12:02Z",
        "updatedAt" : "2015-08-18T23:12:02Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "27b49241-5b86-4565-aa3e-c55dba3c9733",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Isn't rollback on error perm fail?  If we fail once within a user defined timeout, isn't that the definition of perm fail?\n",
        "createdAt" : "2015-08-18T23:27:50Z",
        "updatedAt" : "2015-08-18T23:27:50Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "4b3c8275-37b4-4167-adff-1cc108038c9e",
        "parentId" : "232b7a52-4805-46cf-af73-b8e8c09f1bb7",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Yes. Sorry I wasnt clear.\nAs per this proposal (which is what we are aiming for in 1.1), we wont have permfail. Deployment will just keep trying. We do not have a \"Failed\" condition (or any condition).\n\nIn future, though, we do want to support that. We will add conditions, so that deployment can be permfailed and go in a Failed condition. We will then support things like \"automatic rollback on error\".\n",
        "createdAt" : "2015-09-03T07:39:28Z",
        "updatedAt" : "2015-09-03T07:39:28Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 189,
    "diffHunk" : "@@ -1,1 +187,191 @@6. Cleanup.\n\nDeploymentController is stateless so that it can recover incase it crashes during a deployment.\n\n### MinReadySeconds"
  },
  {
    "id" : "ea2f1832-ddbb-40db-b303-30d2424fc3f0",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e09c2b85-ef55-4a80-92e1-66af421790f1",
        "parentId" : null,
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "I don't know that any approach would affect the external API, and let me know if this is out of scope of discussion in this PR, but how could deployments with the OOTB strategies be implemented at nontrivial scale without each strategy being executed in a pod somewhere? We can't tie up a server thread for every strategy execution (which, even if they took almost no CPU, have deadlines measured in hours). This is why strategy code runs in a pod in OpenShift's deployment system.\n",
        "createdAt" : "2015-08-13T18:28:02Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "b8d8262f-853f-48c4-bcdd-561fe8981ecb",
        "parentId" : "e09c2b85-ef55-4a80-92e1-66af421790f1",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Yes we will need to support running the deployment in a pod to support custom deployment strategies. We can support some hooks without it, tough.\nI guess we will add this soon after the basic stuff.\n",
        "createdAt" : "2015-08-13T19:46:45Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "af466055-fce2-4391-8736-a2591c6bf01f",
        "parentId" : "e09c2b85-ef55-4a80-92e1-66af421790f1",
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "I meant not just for custom strategies. How will a rolling update for a given deployment be performed in the server process? The rolling updater code can't run in the server process: it might take several minutes or even hours.\n",
        "createdAt" : "2015-08-13T19:59:10Z",
        "updatedAt" : "2015-08-13T20:39:17Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "1d429c19-705a-4cc9-aecf-acb0709f03f4",
        "parentId" : "e09c2b85-ef55-4a80-92e1-66af421790f1",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "DeploymentController will be an always running process like ReplicationController.\n",
        "createdAt" : "2015-08-13T20:44:05Z",
        "updatedAt" : "2015-08-13T20:56:18Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "6697b4be-c9c4-4ae1-995d-816c2e5f5bcc",
        "parentId" : "e09c2b85-ef55-4a80-92e1-66af421790f1",
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "I might not be coming across clearly. Ultimately, some process is going to have to execute the code in `rolling_updater.go` to make a given deployment happen. That code will run almost arbitrarily long (OpenShift sets the deadline at 6 hours). If that code executes in a k8s server process (like the master), it will consume a server thread for up to 6 hours. If you queue processing and execute them sequentially, deployments won't scale at all. Parallelization will be limited to threading in-process, which also won't scale.\n\nHow can the deployment strategy code execute in a timely fashion if not in a separate physical process somewhere (e.g. a Pod container)?\n",
        "createdAt" : "2015-08-13T21:14:08Z",
        "updatedAt" : "2015-08-13T21:14:08Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "6b1aaa0c-f9c3-4219-b769-3367b96c188b",
        "parentId" : "e09c2b85-ef55-4a80-92e1-66af421790f1",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "As @bgrant0607 clarified, Deployment is more like a ReplicationController rather than a Job. It is a continuously running process, trying to bring the system state towards the desired state. It wont fail (can exponentially back off) or succeed.\n\nIn light of this info, DeploymentController wont be able to use rolling_updater as is. \nWe will need some refactoring to share common code. For ex: Deployment can use the when to scale up and scale down logic in the for loop in rolling_updater.\n",
        "createdAt" : "2015-08-13T23:12:18Z",
        "updatedAt" : "2015-08-13T23:12:18Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 256,
    "diffHunk" : "@@ -1,1 +254,258 @@\nApart from the above, we want to add support for the following:\n* Running the deployment process in a pod: In future, we can run the deployment process in a pod. Then users can define their own custom deployments and we can run it using the image name.\n* More DeploymentTypes: https://github.com/openshift/origin/blob/master/examples/deployment/README.md#deployment-types lists most commonly used ones.\n* Triggers: Deployment will have a trigger field to identify what triggered the deployment. Options are: Manual/UserTriggered, Autoscaler, NewImage."
  },
  {
    "id" : "1941db35-ff26-46d4-aaee-52a40dcb635b",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9398e0ee-dd5e-493a-8cbb-df40049929c1",
        "parentId" : null,
        "authorId" : "009c48f4-b253-43a2-91a7-3f470d3c1311",
        "body" : "I find maxUnavailable and maxSurge to be somewhat confusing. Doesn't this mean that, for a given update that doesn't change the size, the number of pods running at any given time can vary by (maxUnavailable + maxSurge), entirely depending on the updater implementation?\n\nIs there a demand to have separate knobs for both of these bounds? It seems like they could probably both be controlled by a single number for simplicity. Or, given a more well-defined updater, you might only ever need a single one of these knobs. For example, if the updater guarantees that the new pod is running before the old pod is taken down, you'd only need maxSurge.\n",
        "createdAt" : "2015-08-14T22:42:42Z",
        "updatedAt" : "2015-08-14T22:42:42Z",
        "lastEditedBy" : "009c48f4-b253-43a2-91a7-3f470d3c1311",
        "tags" : [
        ]
      },
      {
        "id" : "499e3818-0c35-4c89-90fc-4da183b529df",
        "parentId" : "9398e0ee-dd5e-493a-8cbb-df40049929c1",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "On Aug 14, 2015, at 6:43 PM, James Eady notifications@github.com wrote:\n\nIn docs/proposals/deployment.md\nhttps://github.com/kubernetes/kubernetes/pull/12236#discussion_r37126196:\n\n> -  // can be scaled down further, followed by scaling up the new RC, ensuring\n> -  // that at least 70% of original number of pods are available at all times\n> -  // during the update.\n> -  MaxUnavailable IntOrString\n>   +\n> -  // The maximum number of pods that can be scheduled above the original number of\n> -  // pods.\n> -  // Value can be an absolute number (ex: 5) or a percentage of total pods at\n> -  // the start of the update (ex: 10%). This can not be 0 if MaxUnavailable is 0.\n> -  // Absolute number is calculated from percentage by rounding up.\n> -  // By default, a value of 1 is used.\n> -  // Example: when this is set to 30%, the new RC can be scaled up by 30%\n> -  // immediately when the rolling update starts. Once old pods have been killed,\n> -  // new RC can be scaled up further, ensuring that total number of pods running\n> -  // at any time during the update is atmost 130% of original pods.\n> -  MaxSurge IntOrString\n\nI find maxUnavailable and maxSurge to be somewhat confusing. Doesn't this\nmean that, for a given update that doesn't change the size, the number of\npods running at any given time can vary by (maxUnavailable + maxSurge),\nentirely depending on the updater implementation?\n\nIs there a demand to have separate knobs for both of these bounds? It seems\nlike they could probably both be controlled by a single number for\nsimplicity. Or, given a more well-defined updater, you might only ever need\na single one of these knobs. For example, if the updater guarantees that\nthe new pod is running before the old pod is taken down, you'd only need\nmaxSurge.\n\nTo have both sides, yes.  We need to support deployments where the existing\ndeployment is consuming all physical resources, which requires the caller\nto specify a lower bound.  See some of the upthread but we decided to keep\nthem split so as to better reflect their impact, as well as to describe the\npolicy more clearly to users.  The previous was a negative or positive\npercentage that was also confusing.\n\n窶能nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/12236/files#r37126196.\n",
        "createdAt" : "2015-08-14T23:03:32Z",
        "updatedAt" : "2015-08-14T23:03:32Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +134,138 @@  // new RC can be scaled up further, ensuring that total number of pods running\n  // at any time during the update is atmost 130% of original pods.\n  MaxSurge IntOrString\n\n  // Minimum number of seconds for which a newly created pod should be ready"
  },
  {
    "id" : "16f49355-de4e-4735-bca1-5ff2cac3995f",
    "prId" : 12236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d055f6f-3b9c-439e-ac84-54167779c301",
        "parentId" : null,
        "authorId" : "8f990fa6-5669-45e5-9c97-9436c3eac127",
        "body" : "Will the pod template spec be canonicalised? The order of labels, for example, shouldn't matter.\n",
        "createdAt" : "2015-09-03T02:46:36Z",
        "updatedAt" : "2015-09-03T02:46:36Z",
        "lastEditedBy" : "8f990fa6-5669-45e5-9c97-9436c3eac127",
        "tags" : [
        ]
      },
      {
        "id" : "b5bd9b0c-2822-4b2e-801d-a7fdc19f9784",
        "parentId" : "2d055f6f-3b9c-439e-ac84-54167779c301",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Yes thats a good point. We can sort the labels as per their key names (and remove duplicates) before computing the hash.\n",
        "createdAt" : "2015-09-03T07:31:40Z",
        "updatedAt" : "2015-09-03T07:31:40Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "336a2e01-6173-4aa5-a7a8-636fab19d66c",
        "parentId" : "2d055f6f-3b9c-439e-ac84-54167779c301",
        "authorId" : "8f990fa6-5669-45e5-9c97-9436c3eac127",
        "body" : "Will this be hashing the textual representation (JSON?) or the `PodTemplateSpec` object? In the former case, I suggest using something like [tent/canonical-json](https://github.com/tent/canonical-json-go) which is used by the [TUF](http://theupdateframework.com/) implementation in [docker/notary](https://github.com/docker/notary).\n",
        "createdAt" : "2015-09-03T13:05:36Z",
        "updatedAt" : "2015-09-03T13:05:36Z",
        "lastEditedBy" : "8f990fa6-5669-45e5-9c97-9436c3eac127",
        "tags" : [
        ]
      },
      {
        "id" : "ee2aa8e4-9f6c-47e0-8cb1-d0a5200a7090",
        "parentId" : "2d055f6f-3b9c-439e-ac84-54167779c301",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I think we use Adler32 to do the limited hash today.\n\nOn Sep 3, 2015, at 9:05 AM, Kamal Marhubi notifications@github.com wrote:\n\nIn docs/proposals/deployment.md\nhttps://github.com/kubernetes/kubernetes/pull/12236#discussion_r38642754:\n\n> +#### Deployment Controller\n> +\n> +The DeploymentController will make Deployments happen.\n> +It will watch Deployment objects in etcd.\n> +For each pending deployment, it will:\n> +\n> +1. Find all RCs whose label selector is a superset of DeploymentSpec.Selector.\n> -   - For now, we will do this in the client - list all RCs and then filter the\n> -     ones we want. Eventually, we want to expose this in the API.\n>   +2. The new RC can have the same selector as the old RC and hence we add a unique\n> -   selector to all these RCs (and the corresponding label to their pods) to ensure\n> -   that they do not select the newly created pods (or old pods get selected by\n> -   new RC).\n> -   - The label key will be \"deployment.kubernetes.io/podTemplateHash\".\n> -   - The label value will be hash of the podTemplateSpec for that RC without\n> -     this label. This value will be unique for all RCs, since PodTemplateSpec should be unique.\n\nWill this be hashing the textual representation (JSON?) or the\nPodTemplateSpec object? In the former case, I suggest using something like\ntent/canonical-json https://github.com/tent/canonical-json-go which is\nused by the TUF http://theupdateframework.com/ implementation in\ndocker/notary https://github.com/docker/notary.\n\n窶能nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/12236/files#r38642754.\n",
        "createdAt" : "2015-09-03T19:41:43Z",
        "updatedAt" : "2015-09-03T19:41:43Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "3aa9f563-3562-4fe1-b613-cd3bb31e722c",
        "parentId" : "2d055f6f-3b9c-439e-ac84-54167779c301",
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "> I think we use Adler32 to do the limited hash today.\n\nWe switched to `Semantic.DeepEqual`.\n",
        "createdAt" : "2015-09-03T19:47:36Z",
        "updatedAt" : "2015-09-03T19:47:36Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "e1d182b4-08b0-469f-9206-ac3ef8de35bb",
        "parentId" : "2d055f6f-3b9c-439e-ac84-54167779c301",
        "authorId" : "8f990fa6-5669-45e5-9c97-9436c3eac127",
        "body" : "It looks like a deep object hasher with Adler32. This still doesn't make it clear what should happen if only order in a repeated element changes. At the moment this is used for hashing comparing running containers to their spec: https://github.com/kubernetes/kubernetes/blob/8e33cbfa281b7e464100a052c9408a1947ec29d5/pkg/kubelet/container/helpers.go#L88\n\nI just verified that, eg, changing order of elements in the `ports` section of a container spec will result in the pod being recreated. The same issue would occur here for deployments, but at a larger scale. I don't know if that is a bug or not, but I filed one to get input on the container case: https://github.com/kubernetes/kubernetes/issues/13572\n",
        "createdAt" : "2015-09-04T00:46:42Z",
        "updatedAt" : "2015-09-04T00:46:42Z",
        "lastEditedBy" : "8f990fa6-5669-45e5-9c97-9436c3eac127",
        "tags" : [
        ]
      },
      {
        "id" : "bebedb96-7517-4a0f-866a-f1c43ec99a9f",
        "parentId" : "2d055f6f-3b9c-439e-ac84-54167779c301",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "I just tried `api.Semantic.DeepEqual(rc.Spec.Template, deployment.Spec.Template)` and it returns true even when the order of labels is different in `rc.Spec.Template` and `deployment.Spec.Template`. So just using api.Semantic.DeepEqual does what we want.\n",
        "createdAt" : "2015-09-11T23:51:16Z",
        "updatedAt" : "2015-09-11T23:51:16Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "be6342db1dd1f832031331664fd674c754ceaece",
    "line" : 172,
    "diffHunk" : "@@ -1,1 +170,174 @@   - The label key will be \"deployment.kubernetes.io/podTemplateHash\".\n   - The label value will be hash of the podTemplateSpec for that RC without\n     this label. This value will be unique for all RCs, since PodTemplateSpec should be unique.\n   - If the RCs and pods dont already have this label and selector:\n     - We will first add this to RC.PodTemplateSpec.Metadata.Labels for all RCs to"
  }
]