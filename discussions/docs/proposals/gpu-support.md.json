[
  {
    "id" : "2a1451f5-22c2-41fe-a26b-0645dd83ca20",
    "prId" : 24071,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7518d946-8373-472a-8ea2-71b7fb1dc3e4",
        "parentId" : null,
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "What does this command do?\n",
        "createdAt" : "2016-04-09T16:40:02Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      },
      {
        "id" : "c0a276e7-8c14-4c03-a8cf-a9a912e7721f",
        "parentId" : "7518d946-8373-472a-8ea2-71b7fb1dc3e4",
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "The plugin runs as a standalone daemon, listening to HTTP requests. One of the endpoints returns flags to add to Docker command lines. By default, it assumes you want to run on device 0, but you can change the URL to provide a list of device numbers instead.\n",
        "createdAt" : "2016-04-09T16:48:47Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      }
    ],
    "commit" : "82e1949170b7194a0aa062873f60fbc052f05907",
    "line" : null,
    "diffHunk" : "@@ -1,1 +208,212 @@readability:\n\n    $ curl -s localhost:3476/docker/cli\n    --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia0\n    --volume-driver=nvidia-docker"
  },
  {
    "id" : "9ae6fcfa-4af6-419b-ab17-fb72c6ba5eb7",
    "prId" : 24071,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6beb8045-d1d4-4713-a3f0-ad071226c748",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "The resources are non-fungible, history has taught us there needs to be: \n1. GPU Card Count \n2. Card Version (May be better abstracted with newer versions of CUDA)\n3. Driver+Library Version \n\nIn a heterogeneous env. all three can vary and your software can depend on those three variables. \n",
        "createdAt" : "2016-04-12T21:02:38Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "c3fec2cb-7168-41d4-96eb-cf069d7b655e",
        "parentId" : "6beb8045-d1d4-4713-a3f0-ad071226c748",
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "Right. We should put it explicitly in the docs.\nFYI, in Monday's meeting. We talked about the same thing.\n",
        "createdAt" : "2016-04-12T21:05:59Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      },
      {
        "id" : "953be6cd-354a-4cad-a9c0-58762a78104f",
        "parentId" : "6beb8045-d1d4-4713-a3f0-ad071226c748",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "@davidopp in another thread suggested to describe this kind non-fungible resource using a different approach called opaque counted resource at: https://github.com/kubernetes/kubernetes/issues/19049#issuecomment-170292193\n\nI think that approach can cover the three attributes listed above by @timothysc. So that Kubelet can expose something like:\n\nGPU-Version+Driver-Version: count through NodeCapacity / NodeAllocatable. \n\nMeanwhile, we also talked about this in today's sig-node meeting on the initial support (v0) with more restriction: \n- Only allow one GPU per node\n- Introducing an experimental flag which includes GPU version info, and hardcode Kubelet to report GPU as NodeCapacity & NodeAllocatable if the flag is on. Also report this information through node label.\n- Do we need to have a flag to config host device associating with GPU flag? I forgot our decision on this one.\n- In this approach, required libraries have to be included in image itself. A given pod requesting a GPU, Kubelet based on the configuration to setup the runtime environment. \n\ncc/ @erictune Did I miss anything above? \n\nOnce initContainer or VolumeContainer work is done, we can use that feature to allow preconfiged nvidia-docker volume driver, and Kubelet can discover those, and expose those using opaque counted resource approach above. \n",
        "createdAt" : "2016-04-13T00:10:54Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "ff848ccb-8a27-4a6a-8934-bf18042ec699",
        "parentId" : "6beb8045-d1d4-4713-a3f0-ad071226c748",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "> Only allow one GPU per node\n\nCan you explain the reason for this restriction? From the scheduler's perspective, it is no harder to deal with a resource with capacity 1 than a resource with capacity N.\n\n> hardcode Kubelet to report GPU as NodeCapacity & NodeAllocatable if the flag is on.\n\nHow much code would you have to add to Kubelet to have Kubelet auto-detect the GPU(s) today, without init container or volume container?\n",
        "createdAt" : "2016-04-13T05:19:02Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "4da1d6c7-1cc3-4a6e-b28f-7c0c527e5c14",
        "parentId" : "6beb8045-d1d4-4713-a3f0-ad071226c748",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "@dchen1107 re: @davidopp 's  #19049 (comment) - totally agree. \n\nWe used to call this arbitrary machine local limits, which other systems copied the basic design:  ( xref https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=2905 ) \n\nStep 1: on startup there would be an option to kick a \"plugin|discovery_script\" whose output would be name value pairs of capabilities to be used for scheduling.    \n\nStep 2: On allocate/match there would be another accompanying plugin/script to provision/assign said resource to a job, in this case would be pod and the machines status would be refreshed to ensure we decremented the resource count so the scheduler did the appropriate thing. \n\nIn being generic you can apply arbitrary resource labeling.  (gpus/fpgas/infiniband cards/booster rockets/ion cannons/etc.) ;-) \n",
        "createdAt" : "2016-04-14T14:14:26Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "82e1949170b7194a0aa062873f60fbc052f05907",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +65,69 @@\nUsers should be able to request GPU resources for their workloads, as easily as\nfor CPU or memory. Kubernetes should keep an inventory of machines with GPU\nhardware, schedule containers on appropriate nodes and set up the container\nenvironment with all that's necessary to access the GPU. All of this should"
  },
  {
    "id" : "abc04954-2f9d-4150-8611-9a7574771ff5",
    "prId" : 24071,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d1fae4f-9e84-4f35-a1d6-ea7d986ef8a0",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "I've never seen a cluster manager being able to isolate on anything other then card boundaries. \n",
        "createdAt" : "2016-04-12T21:05:52Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "ab7fa8d5-74da-4934-bd4c-69a4ef3b6087",
        "parentId" : "9d1fae4f-9e84-4f35-a1d6-ea7d986ef8a0",
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "It doesn't qualify as a cluster manager, but our legacy environment packs together multiple processes on the same card through static allocation. At some point I'd like to do something similar with Kubernetes. We can't keep using the legacy pipeline because we're fed up with VMs. :-)  Even assuming exclusive use of the card, some might want to declare \"I want to run this on a card with at least x GB\". We have some workloads that fit the beefier hardware in our colocated machines, but not on AWS setups. They really only care about the memory.\n",
        "createdAt" : "2016-04-12T21:46:45Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      },
      {
        "id" : "6e0dd66a-ff09-4669-a513-f74566b86075",
        "parentId" : "9d1fae4f-9e84-4f35-a1d6-ea7d986ef8a0",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Yeah, but iirc there is no memory protection, so an errant process could corrupt *.  In the case of multiple processes sharing the same card, you could simply leverage as a POD, but there should be no cross POD splicing of a GPU.   \n\nWe strictly need to prohibit that kind of abuse, b/c there is no memory protection. \n",
        "createdAt" : "2016-04-13T15:15:37Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "ae07d0cc-de07-4e73-90d6-2454e4f58a2f",
        "parentId" : "9d1fae4f-9e84-4f35-a1d6-ea7d986ef8a0",
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "Yeah, there are caveats, but it's done in MPI land: https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf\n\nI agree with you on disallowing sharing for arbitrary pods, but within the same namespace/user (and perhaps additional selectors, such as a common \"build version\" label), the user should be able to opt in to device sharing. You can easily aggregate multiple containers into the same pod if the makeup of the processes is consistent or predictable, but that's not always the case. Wouldn't pod affinity help here?\n",
        "createdAt" : "2016-04-13T15:57:17Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      },
      {
        "id" : "54ed93bd-36da-4758-adc9-ac7cbb0e0aef",
        "parentId" : "9d1fae4f-9e84-4f35-a1d6-ea7d986ef8a0",
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "> Wouldn't pod affinity help here?\n\nI think let's not make it complicated at first step, i.e. coupling with unnecessary things.\n\nI agree that we could try giving user the capability to opt in to device sharing. This sounds like real use case, although conservation of resources sharing should be done.\n",
        "createdAt" : "2016-04-13T16:33:21Z",
        "updatedAt" : "2016-04-25T17:04:48Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      }
    ],
    "commit" : "82e1949170b7194a0aa062873f60fbc052f05907",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +85,89 @@- some vendors require fairly tight coupling between the kernel driver\ncontrolling the GPU and the libraries/applications that access the hardware\n- it adds more resource types (whole GPUs, GPU cores, GPU memory)\n- it can introduce new security pitfalls\n- for systems with multiple GPUs, affinity matters, similarly to NUMA"
  }
]