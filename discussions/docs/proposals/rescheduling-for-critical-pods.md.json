[
  {
    "id" : "a0167e16-8ba0-4623-b3bc-29a38458ca82",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a167691-506d-47e1-a499-ed267f04d053",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "what are those various reasons?\n",
        "createdAt" : "2016-07-19T17:10:42Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "5be6dbef-e1cf-4a39-adfd-f0df69c325e5",
        "parentId" : "1a167691-506d-47e1-a499-ed267f04d053",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Security issues: Heapster talking to Kubelets directly, DNS serving on open port.\n\n@roberthbailey and @thockin can probably elaborate, since I don't know the details.\n",
        "createdAt" : "2016-07-19T17:48:23Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "dd0e1120-c005-444e-a8b3-7f1ef0d19398",
        "parentId" : "1a167691-506d-47e1-a499-ed267f04d053",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Yeah, we are moving the addons to the master that we can (like gclb) but we don't have a good way to run all of them on the master at this time. \n",
        "createdAt" : "2016-07-19T23:11:31Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "0a6a3794-7892-4c3a-8bb1-a9ed0d9a0d68",
        "parentId" : "1a167691-506d-47e1-a499-ed267f04d053",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "There's a pretty fundamental difference between controllers which watch API server and enact other changes (ingress controller) and apps which serve something to the cluster (dns).  Maybe worth calling this out.\n",
        "createdAt" : "2016-07-24T16:37:26Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "83020718-242e-4b3f-8d92-8380c41987ad",
        "parentId" : "1a167691-506d-47e1-a499-ed267f04d053",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "@thockin I agree there's a fundamental difference, but do you think it affects whether they are critical to keep scheduled?\n",
        "createdAt" : "2016-07-24T19:16:37Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "b438b3b2-3040-4605-9658-d28fdc1ae1e6",
        "parentId" : "1a167691-506d-47e1-a499-ed267f04d053",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "No, my point was to clarify why some run on master and some can't\n\nOn Jul 24, 2016 12:17 PM, \"David Oppenheimer\" notifications@github.com\nwrote:\n\n> In docs/proposals/rescheduling-for-critical-pods.md\n> https://github.com/kubernetes/kubernetes/pull/29195#discussion_r71993699\n> :\n> \n> > +\n> > +Documentation for other releases can be found at\n> > +[releases.k8s.io](http://releases.k8s.io).\n> > +</strong>\n> > +--\n> > +\n> > +<!-- END STRIP_FOR_RELEASE -->\n> > +\n> > +<!-- END MUNGE: UNVERSIONED_WARNING -->\n> > +\n> > +# Rescheduler: guaranteed scheduling of critical addons\n> > +\n> > +## Motivation\n> > +\n> > +In addition to Kubernetes core components like api-server, scheduler, controller-manager running on a master machine\n> > +there is a bunch of addons which due to various reasons have to run on a regular cluster node, not the master.\n> \n> @thockin https://github.com/thockin I agree there's a fundamental\n> difference, but do you think it affects whether they are critical to keep\n> scheduled?\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/29195/files/a135c597401e8d9e12e19845600b8061ffdb4b7c#r71993699,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFVgVNQ7Lniq9ALktA7Rr5zQyU9uXbhCks5qY7pDgaJpZM4JPqoM\n> .\n",
        "createdAt" : "2016-07-24T21:55:17Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +33,37 @@\nIn addition to Kubernetes core components like api-server, scheduler, controller-manager running on a master machine\nthere is a bunch of addons which due to various reasons have to run on a regular cluster node, not the master.\nSome of them are critical to have fully functional cluster: Heapster, DNS, UI. Users can break their cluster\nby evicting a critical addon (either manually or as a side effect of an other operation like upgrade)"
  },
  {
    "id" : "0b8815de-86cd-4424-8525-7b28e36e2f0f",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a3086e2-c992-4f3c-8903-02e692508d41",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "@davidopp Is this the intended plan?  Seems to be disconnect from what you wrote the other day. \n",
        "createdAt" : "2016-07-20T14:35:20Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "b5d42efd-410c-4684-8723-cc741dd00a1a",
        "parentId" : "4a3086e2-c992-4f3c-8903-02e692508d41",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "@timothysc Are you talking about the email I sent to kubernetes-sig-scheduling? I said\n\n> nobody has started working on the \"full\" rescheduler or priority/preemption scheme yet; however there will be a simple version of preemption to allow prioritized scheduling of critical cluster addon pods in 1.4; see #29023 for details (design proposal coming very very soon)\n\nThis is the design proposal. I was referring to. As to whether this should be called \"the rescheduler\" I can see arguments both ways... It might be better to call this the \"critical pod rescheduler\" since the design does not explicitly consider the other types of rescheduling we have discussed elsewhere (e.g. in https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/rescheduling.md) but I don't think it's a big deal. This is also not intended as a general priority/preemption mechanism.\n",
        "createdAt" : "2016-07-24T19:17:04Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "dd711001-2193-42a2-b02c-51fa54f5623c",
        "parentId" : "4a3086e2-c992-4f3c-8903-02e692508d41",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "@jcantrill @jeremyeder @dgoodwin FYI.  This will affect upgrade path for addons on our end, as well as new master component. \n",
        "createdAt" : "2016-07-25T20:49:38Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "848745ef-a817-481f-bcce-15c5fdc54a06",
        "parentId" : "4a3086e2-c992-4f3c-8903-02e692508d41",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Yeah. It's called rescheduler because it reschedules some pods to make sure critical addons are scheduled, but it doesn't implement any other policies discussed before. In the future it can evolve into generic rescheduler, though it'd require redesign of the component.\n",
        "createdAt" : "2016-07-27T16:09:26Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +43,47 @@## Design\n\nRescheduler will ensure that critical addons are always scheduled.\nIn the first version it will implement only this policy, but later we may want to introduce other policies.\nIt will be a standalone component running on master machine similarly to scheduler."
  },
  {
    "id" : "0edfdace-b4bd-4cb4-b818-2f5732ef52d1",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53294439-4cdc-4b4f-9e87-8c5cefe47303",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "s/evicting/deleting/ ?  Or are we defining \"eviction\" at this time?\n\nWill the scheduler always give these pods priority?  What prevents the deleted pods from being scheduled first?\n",
        "createdAt" : "2016-07-24T16:54:17Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "ebe1be72-0c8c-4078-b44c-95ad794d6121",
        "parentId" : "53294439-4cdc-4b4f-9e87-8c5cefe47303",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "How does this interact with multiple schedulers?  Can it delete pods from a non-default scheduler? Will that scheduler know not to immediately re-schedule in that same place?\n",
        "createdAt" : "2016-07-24T16:55:22Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "e2e1ddb5-0805-4e4d-a51c-d4add2368ae1",
        "parentId" : "53294439-4cdc-4b4f-9e87-8c5cefe47303",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "> s/evicting/deleting/ ? Or are we defining \"eviction\" at this time?\n\nYeah, probably the introduction should say they evict and delete are synonymous. I don't think there's any meaningful distinction between them (e.g. there's no implication that something that is evicted will be rescheduled if it's not managed by a controller).\n\n> Will the scheduler always give these pods priority? What prevents the deleted pods from being scheduled first?\n\nSee the section below that starts \"To avoid situation when Scheduler will schedule another pod into the space prepared for the critical addon\"\n\n> How does this interact with multiple schedulers? Can it delete pods from a non-default scheduler? Will that scheduler know not to immediately re-schedule in that same place?\n\nI think the mechanism described here works with multiple schedulers. Multiple schedulers makes it hard to know whether an evicted pod will be able to reschedule, but not such determination needs to be made in this scheme.\n",
        "createdAt" : "2016-07-24T19:17:53Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "9e037bc1-c08f-4783-9118-53ef46d224e0",
        "parentId" : "53294439-4cdc-4b4f-9e87-8c5cefe47303",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "ACk I didn't read ahead\n\nOn Jul 24, 2016 12:18 PM, \"David Oppenheimer\" notifications@github.com\nwrote:\n\n> In docs/proposals/rescheduling-for-critical-pods.md\n> https://github.com/kubernetes/kubernetes/pull/29195#discussion_r71993725\n> :\n> \n> > +critical addons are scheduled assuming the cluster is big enough.\n> > +This possibly may affect other pods (including production user’s applications).\n> > +\n> > +## Design\n> > +\n> > +Rescheduler will ensure that critical addons are always scheduled.\n> > +In the first version it will implement only this policy, but later we may want to introduce other policies.\n> > +It will be a standalone component running on master machine similarly to scheduler.\n> > +Those components will share common logic (initially rescheduler will in fact import some of scheduler packages).\n> > +\n> > +### Guaranteed scheduling of critical addons\n> > +\n> > +Rescheduler will observe critical addons (running in `kube-system` namespace\n> > +with annotation `alpha.kubernetes.io/service-type` http://alpha.kubernetes.io/service-type set to `CRITICAL`).\n> > +If one of them is marked by scheduler as unschedulable (pod condition `PodScheduled` set to `false`, the reason set to `Unschedulable`)\n> > +the component will try to find a space for the addon by evicting some pods and then the scheduler will schedule the addon.\n> \n> s/evicting/deleting/ ? Or are we defining \"eviction\" at this time?\n> \n> Yeah, probably the introduction should say they evict and delete are\n> synonymous. I don't think there's any meaningful distinction between them\n> (e.g. there's no implication that something that is evicted will be\n> rescheduled if it's not managed by a controller).\n> \n> Will the scheduler always give these pods priority? What prevents the\n> deleted pods from being scheduled first?\n> \n> See the section below that starts \"To avoid situation when Scheduler will\n> schedule another pod into the space prepared for the critical addon\"\n> \n> How does this interact with multiple schedulers? Can it delete pods from a\n> non-default scheduler? Will that scheduler know not to immediately\n> re-schedule in that same place?\n> \n> I think the mechanism described here works with multiple schedulers.\n> Multiple schedulers makes it hard to know whether an evicted pod will be\n> able to reschedule, but not such determination needs to be made in this\n> scheme.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/29195/files/a135c597401e8d9e12e19845600b8061ffdb4b7c#r71993725,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFVgVIRPk3FOx_gOgZY0_eh2CB0vS133ks5qY7qQgaJpZM4JPqoM\n> .\n",
        "createdAt" : "2016-07-24T21:57:12Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "349ebf69-135f-4e95-af3f-a70eb7db064f",
        "parentId" : "53294439-4cdc-4b4f-9e87-8c5cefe47303",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@davidopp If that plan is to `delete` pods, then it would affect https://github.com/kubernetes/kubernetes/issues/29033\n",
        "createdAt" : "2016-07-25T04:12:18Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "6e3a0a05-1904-4836-bad4-0e0a060907dd",
        "parentId" : "53294439-4cdc-4b4f-9e87-8c5cefe47303",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "@vishh not sure if I understand the problem. Could you please elaborate?\n",
        "createdAt" : "2016-08-10T14:02:57Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : null,
    "diffHunk" : "@@ -1,1 +53,57 @@(with annotation `scheduler.alpha.kubernetes.io/critical-pod`).\nIf one of them is marked by scheduler as unschedulable (pod condition `PodScheduled` set to `false`, the reason set to `Unschedulable`)\nthe component will try to find a space for the addon by evicting some pods and then the scheduler will schedule the addon.\n\n#### Scoring nodes"
  },
  {
    "id" : "a25b7eb2-b0b3-4f76-b6ae-9edb8230124a",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3ce9345-da26-44fd-9671-3b63ad38f1f6",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "is this toleration automatic, or do pods have to specify it?\n",
        "createdAt" : "2016-07-24T17:00:22Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "a2eab1d7-e5ab-4ee8-a237-0fee793627e1",
        "parentId" : "f3ce9345-da26-44fd-9671-3b63ad38f1f6",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Pods have to specify it, but it's essentially just an extra line in the PodSpec. That does raise an issue around how we will roll out this feature--we need to update all the pod templates for the critical pods before we enable this feature. (We can leave the critical pods that are already running as-is, because the toleration isn't needed we need to schedule a new instance on a node with a taint.)\n",
        "createdAt" : "2016-07-24T19:20:15Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "a5a34cff-6544-49d0-92d9-8b70796d52da",
        "parentId" : "f3ce9345-da26-44fd-9671-3b63ad38f1f6",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "We could make toleration an updatable field, perhaps, as long as kubelet\ndoesn't hash on it.\n\nOn Jul 24, 2016 12:21 PM, \"David Oppenheimer\" notifications@github.com\nwrote:\n\n> In docs/proposals/rescheduling-for-critical-pods.md\n> https://github.com/kubernetes/kubernetes/pull/29195#discussion_r71993754\n> :\n> \n> > +since there is a chance that it would block this operation for longer period of time.\n> > +We will also try to respect Termination Grace Period, though without any guarantee.\n> > +In case we have to remove a pod with termination grace period longer than 10s it will be shortened to 10s.\n> > +\n> > +The proposed order while choosing a node to schedule a critical addon and pods to remove:\n> > +1. a node where the critical addon pod can fit after evicting only pods satisfying both\n> > +(1) their disruption budget will not be violated by such eviction and (2) they have grace period <= 10 seconds\n> > +1. a node where the critical addon pod can fit after evicting only pods whose disruption budget will not be violated by such eviction\n> > +1. any node where the critical addon pod can fit after evicting some pods\n> > +\n> > +### Interaction with Scheduler\n> > +\n> > +To avoid situation when Scheduler will schedule another pod into the space prepared for the critical addon,\n> > +the chosen node has to be temporarily excluded from a list of nodes considered by Scheduler while making decisions.\n> > +For this purpose the node will get a temporary taint “CriticalAddonsOnly”\n> > +and each critical addon has to have defined toleration for this taint.\n> \n> Pods have to specify it, but it's essentially just an extra line in the\n> PodSpec. That does raise an issue around how we will roll out this\n> feature--we need to update all the pod templates for the critical pods\n> before we enable this feature. (We can leave the critical pods that are\n> already running as-is, because the toleration isn't needed we need to\n> schedule a new instance on a node with a taint.)\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/29195/files/a135c597401e8d9e12e19845600b8061ffdb4b7c#r71993754,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFVgVGrZHF5mBLsoOY65Z2iZcSac7v38ks5qY7segaJpZM4JPqoM\n> .\n",
        "createdAt" : "2016-07-24T21:58:41Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "7b4d235a-2d62-45f7-a3ea-f5858e9957ea",
        "parentId" : "f3ce9345-da26-44fd-9671-3b63ad38f1f6",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Just to be clear, for this feature it doesn't need to be updatable on a live pod, only in the pod template.\n",
        "createdAt" : "2016-07-24T22:22:45Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "b80da059-dff6-4aac-9b41-531a226ccf12",
        "parentId" : "f3ce9345-da26-44fd-9671-3b63ad38f1f6",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "This will be specified in addons yaml files in https://github.com/kubernetes/kubernetes/tree/master/cluster/addons\n",
        "createdAt" : "2016-08-10T14:15:40Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : null,
    "diffHunk" : "@@ -1,1 +86,90 @@For this purpose the node will get a temporary\n[Taint](../../docs/design/taint-toleration-dedicated.md) “CriticalAddonsOnly”\nand each critical addon has to have defined toleration for this taint.\nAfter Rescheduler has no more work to do: all critical addons are scheduled or cluster is too small for them,\nall taints will be removed."
  },
  {
    "id" : "4350cbe8-ea68-46f1-8c72-157b90444816",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8437a4d1-15cb-4dfc-a5e3-51fdee7ef708",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Is pod qos taken into account when deciding eviction at all?\n",
        "createdAt" : "2016-07-25T02:59:34Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "127ae55a-2f38-425e-b2f1-c0d3be08af1b",
        "parentId" : "8437a4d1-15cb-4dfc-a5e3-51fdee7ef708",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "+1. What is the interaction with QoS here?\n",
        "createdAt" : "2016-07-25T04:13:29Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "548d5a87-29a9-42b4-830a-8dd70e3db79b",
        "parentId" : "8437a4d1-15cb-4dfc-a5e3-51fdee7ef708",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I think it's fine to leave that as TBD, as the doc here says (\"choose a random node\")\n\nThe assumption is that this feature will only be used to schedule pods whose absence basically breaks your cluster, so even a \"bad\" choice of nodes is better than the state we have today (where the pod may stay pending forever).\n\nIn other words I think these are good questions but we should not feel like we need to figure out the answers now.\n",
        "createdAt" : "2016-07-25T05:48:17Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "28d2eddd-ad27-4905-986b-256d65f61439",
        "parentId" : "8437a4d1-15cb-4dfc-a5e3-51fdee7ef708",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "> In other words I think these are good questions but we should not feel like we need to figure out the answers now.\n\nagreed.\n",
        "createdAt" : "2016-07-25T20:54:32Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "311dc037-519d-4598-8ae5-cc481fbc0fc5",
        "parentId" : "8437a4d1-15cb-4dfc-a5e3-51fdee7ef708",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Also this is a temporary solution until the full priority mechanism is being implemented in Kubernetes, so let's do not care about QOS at this point. \n",
        "createdAt" : "2016-08-10T15:11:59Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +60,64 @@(chosen as described in [Evicting pods](rescheduling-for-critical-pods.md#evicting-pods)) to schedule given addons.\nLater we may want to introduce some heuristic:\n* minimize number of evicted pods with violation of disruption budget or shortened termination grace period\n* minimize number of affected pods by choosing a node on which we have to evict less pods\n* increase probability of scheduling of evicted pods by preferring a set of pods with the smallest total sum of requests"
  },
  {
    "id" : "a213e768-c6fc-438c-9a4c-a66a46bd17b2",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c602afd-3e7c-4217-bd50-071185629ba2",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "what about pods backed by a daemon set?  Will they have lower priority?  How about knowing if we evict fewest possible pods or will we evict based on QOS?  I could see us choosing one big pod or a number of smaller.\n",
        "createdAt" : "2016-07-25T03:02:16Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "60a96724-e966-4a98-b488-47f740ca4007",
        "parentId" : "0c602afd-3e7c-4217-bd50-071185629ba2",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "See my previous response.\n",
        "createdAt" : "2016-07-25T05:49:05Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "ae21198e-508c-4781-8981-d4cbe12e0111",
        "parentId" : "0c602afd-3e7c-4217-bd50-071185629ba2",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "We treat daemon set pods as a part of node configuration and because of that they have higher priority. We won't care about QOS. Regarding optimization it's not clear what is better so let's do not give any guarantees at this point.\n",
        "createdAt" : "2016-08-10T15:21:59Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +76,80 @@The proposed order while choosing a node to schedule a critical addon and pods to remove:\n1. a node where the critical addon pod can fit after evicting only pods satisfying both\n(1) their disruption budget will not be violated by such eviction and (2) they have grace period <= 10 seconds\n1. a node where the critical addon pod can fit after evicting only pods whose disruption budget will not be violated by such eviction\n1. any node where the critical addon pod can fit after evicting some pods"
  },
  {
    "id" : "2da1332a-4232-4d01-9968-3a0186289111",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e78e3493-c779-42c5-9ad3-b2e40596984a",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "FYI: On the kubelet side, we provide a `max-grace-period` configuration flag.\n",
        "createdAt" : "2016-07-25T04:17:47Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "49ac74d7-9387-41fa-8974-128611601bd0",
        "parentId" : "e78e3493-c779-42c5-9ad3-b2e40596984a",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "This implies we will have yet another set of policy knobs. \n",
        "createdAt" : "2016-07-25T21:02:22Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +72,76 @@since there is a chance that it would block this operation for longer period of time.\nWe will also try to respect Termination Grace Period, though without any guarantee.\nIn case we have to remove a pod with termination grace period longer than 10s it will be shortened to 10s.\n\nThe proposed order while choosing a node to schedule a critical addon and pods to remove:"
  },
  {
    "id" : "6478446c-e3ee-493c-9524-e1193ac2a083",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Would it make sense to let the kubelet handle evictions and instruct the kubelet that a given pod is `Critical`? Kubelet already handles evictions. \n",
        "createdAt" : "2016-07-25T04:23:14Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "2d9e5121-859e-4482-bbc3-6dab583c5de0",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Can you explain what you mean? The component that is described here needs to make the decision about which node to do the eviction on. Are you suggesting that the Kubelet should decide which pods to evict once the choice of nodes is made? I think from a complexity standpoint it is easier to put that logic in an external component (i.e. the rescheduler component described here), at least initially.\n",
        "createdAt" : "2016-07-25T05:51:43Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "98545fd6-471c-4dfd-96a5-83860e6617c4",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Yeah. I was wondering if the scheduler can choose an appropriate node based on disruption budget, etc., and let kubelet deal with evicting existing pods to accommodate a `Critical` pod? \nThe rationale behind the suggestion is that kubelet is already evicting pods and it respects QoS. It should probably respect disruption budget too on a best effort basis. So in terms of complexity, letting the kubelet deal with evictions to me seems simpler. WDYT?\n",
        "createdAt" : "2016-07-25T18:03:30Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "448ae292-425d-4f71-8b3d-20ba0fef8a8e",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "I don't know about that, the scheduler can just mark and not care.  I'm already worried that the kubelet is becoming obtuse.  We could eventual weight the score based on QoS of the running pods which the scheduler should already be aware of.  \n",
        "createdAt" : "2016-07-25T20:57:34Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "4c5e7e66-109b-4742-bd81-4e4adf7cff93",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I think we should do the evictions from the rescheduler for the first version, and then re-discuss.\n\nI think the behavior this proposal gives should be replaced by \"proper\" scheduler-driven priority/preemption soon-ish, which in theory could have the kubelet choose the victims, as you suggested, though I think it requires more thought before we decide.\n\nOne thing I wanted to mention is that there are some rescheduling scenarios where you'd want the rescheduler to make the eviction decision, i.e. for example when it wants to limit itself to choosing victims that it knows can reschedule elsewhere. That's not a criteria the kubelet can apply without global information. That's not really the same use case as preemption, though.\n",
        "createdAt" : "2016-07-25T23:58:36Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "81f0d967-8225-404e-a1a6-a49e39c96ae1",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "> I think the behavior this proposal gives should be replaced by \"proper\" scheduler-driven priority/preemption soon-ish, which in theory could have the kubelet choose the victims, as you suggested, though I think it requires more >thought before we decide.\n\nThis is exactly what would be beneficial in the longer term and tackle wider community needs. @davidopp Is there a hurry for getting this feature as proposed in to solve something specific  or just to get feedback from the community and collect more requirements before we zero in on the design of the more generic priority based pre-emption ? \n",
        "createdAt" : "2016-07-26T06:16:30Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "123f71d5-6033-4087-b2fe-1a40205a509d",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "> Is there a hurry for getting this feature as proposed in to solve something specific or just to get feedback from the community and collect more requirements before we zero in on the design of the more generic priority based pre-emption ?\n\nYes, unfortunately there is a hurry -- some users have had DNS or Heapster or both (I can't remember) fail to schedule after being evicted (I forget the reason for the eviction) because their cluster was full of regular pods. As you can imagine, bad things happen when DNS fails to schedule. Heapster (and the UI) are marginally less critical, but it's still pretty bad when you lose the ability to monitor the cluster. So this is really intended as a stopgap measure to prevent one source of outages.\n\nMy hope is that post-1.4 we can implement proper preemption in the scheduler (or some combination of the scheduler and kubelet, as @vishh proposed), and remove the logic described in this proposal while keeping the core intact as the framework for implementing the things we had been planning to do with rescheduler (e.g. move pods from heavily-utilized to under-utilized nodes). We should have a discussion on the right way to do proper priorities and preemptions before implementing it. (Some of the issues are described [here](https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/rescheduling.md) but it's definitely not a full design and deserves a full design doc.)\n",
        "createdAt" : "2016-07-26T06:33:29Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "20196018-52b9-4e62-a696-569ec118169d",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "@davidopp thanks for the pointer to the rescheduling proposal\n",
        "createdAt" : "2016-07-26T06:41:39Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "eb6ebf5b-aeb8-4c27-a075-bc848015b4fa",
        "parentId" : "7d751a95-0fc8-43d9-962e-979769651d7a",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Yeah, implementing full blown priorities mechanism is out of the scope of this proposal.\n",
        "createdAt" : "2016-08-10T15:40:40Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +61,65 @@Later we may want to introduce some heuristic:\n* minimize number of evicted pods with violation of disruption budget or shortened termination grace period\n* minimize number of affected pods by choosing a node on which we have to evict less pods\n* increase probability of scheduling of evicted pods by preferring a set of pods with the smallest total sum of requests\n* avoid nodes which are ‘non-drainable’ (according to drain logic), for example on which there is a pod which doesn’t belong to any RC/RS/Deployment"
  },
  {
    "id" : "84644641-b255-45fd-83d9-a14dd9637014",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55de3628-5760-46ec-b71c-7a48887680f9",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Are we going to keep this in the main repository? or separate?  Is there a reason this can't be a self-hosted introspective component?  \n",
        "createdAt" : "2016-07-25T20:50:55Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "5d0c4754-ed19-40e4-a4b6-bdfc6a7944f4",
        "parentId" : "55de3628-5760-46ec-b71c-7a48887680f9",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "AFAIK @piosz is planning to do this not in the main repo. I'm not sure it's good to run it as an addon pod, since it is in charge of making sure critical addon pods can schedule. Running it on the master as proposed here seems safer.\n",
        "createdAt" : "2016-07-26T00:01:07Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "a4e5f8e5-1b05-49cb-9896-7346160208f5",
        "parentId" : "55de3628-5760-46ec-b71c-7a48887680f9",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@davidopp - I think that by addon here you mean something running on nodes, but there are also addons that run on master machine (cluster autoscaler). So we are currently leaning towards running it as \"addon on master machine\".\n",
        "createdAt" : "2016-07-28T15:43:25Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "c11f6e56-eab3-4db9-9e74-c4d2514fdc47",
        "parentId" : "55de3628-5760-46ec-b71c-7a48887680f9",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "The link to the repo https://github.com/kubernetes/contrib/tree/master/rescheduler\n",
        "createdAt" : "2016-08-10T15:42:49Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "7832b695-e3be-4757-a33e-db593a2e267e",
        "parentId" : "55de3628-5760-46ec-b71c-7a48887680f9",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Did you just put that in contrib?  Did @bgrant0607 have a stroke?  \n",
        "createdAt" : "2016-08-18T00:56:15Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "9a35f623-c748-4496-aec2-05083f39225f",
        "parentId" : "55de3628-5760-46ec-b71c-7a48887680f9",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "@timothysc I'll just ask @piosz to move it when we're ready to move forward with gutting contrib.\n\nThat said, the bot is capable of running on arbitrary repos now, so I'm not aware of a compelling reason to have put this in contrib.\n",
        "createdAt" : "2016-08-18T06:51:50Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +46,50 @@In the first version it will implement only this policy, but later we may want to introduce other policies.\nIt will be a standalone component running on master machine similarly to scheduler.\nThose components will share common logic (initially rescheduler will in fact import some of scheduler packages).\n\n### Guaranteed scheduling of critical addons"
  },
  {
    "id" : "4f3fa6c7-2448-410e-8d82-44b601d76ac0",
    "prId" : 29195,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58968be4-7a3b-457d-8741-253a57c6b525",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I guess pods backed by a PDB will be prefferred over pods that don't?\n",
        "createdAt" : "2016-08-18T17:08:31Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "40682c77-f6c5-4fe6-b317-28cc19615674",
        "parentId" : "58968be4-7a3b-457d-8741-253a57c6b525",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "What is PDB?\n",
        "createdAt" : "2016-08-25T12:23:40Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "9e4cd796-4774-430a-8499-e2666057f964",
        "parentId" : "58968be4-7a3b-457d-8741-253a57c6b525",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "PodDisruptionBudget\n",
        "createdAt" : "2016-08-25T12:47:30Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "2e343b9c-41ca-4d72-912e-02343f7ef3a9",
        "parentId" : "58968be4-7a3b-457d-8741-253a57c6b525",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "SGTM\n",
        "createdAt" : "2016-08-25T16:14:13Z",
        "updatedAt" : "2016-10-10T09:42:01Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "4346e7b31290eb495a33cae2251c667b059c64aa",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +75,79 @@\nThe proposed order while choosing a node to schedule a critical addon and pods to remove:\n1. a node where the critical addon pod can fit after evicting only pods satisfying both\n(1) their disruption budget will not be violated by such eviction and (2) they have grace period <= 10 seconds\n1. a node where the critical addon pod can fit after evicting only pods whose disruption budget will not be violated by such eviction"
  }
]