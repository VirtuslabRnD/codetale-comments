[
  {
    "id" : "44100f4d-c543-4469-91af-ca6ae7f52d1b",
    "prId" : 11713,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "931168dc-91d4-4f77-aabb-fffe954a3121",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Why not -1000?\n",
        "createdAt" : "2015-08-07T19:00:53Z",
        "updatedAt" : "2015-08-07T19:00:53Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "b21f4bbe-a03b-49cd-873e-aed5c3b5196f",
        "parentId" : "931168dc-91d4-4f77-aabb-fffe954a3121",
        "authorId" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "body" : "Personally, I thought it should be -1000 until we have better protection for Kubelet (and -1000 was what was in my initial proposal), but @vmarmol had great reasons for why it should be -999. If it's -1000 we run the risk of system panics, because it tells the OOM killer the process can't be killed. This would be an issue if Docker, for example, uses too much memory.\n\nIt depends on the behavior we want: if we think there's no point keeping up the node if Docker dies - then we should set it to -1000. But cluster administrators might want the node running even if Docker dies.\n",
        "createdAt" : "2015-08-07T19:34:08Z",
        "updatedAt" : "2015-08-07T19:37:50Z",
        "lastEditedBy" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "tags" : [
        ]
      },
      {
        "id" : "318ff69b-a9a6-42f9-b61b-80068b0dfb65",
        "parentId" : "931168dc-91d4-4f77-aabb-fffe954a3121",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Thanks for the explanation. Maybe mention this in the doc? People might wonder.\n",
        "createdAt" : "2015-08-07T19:38:51Z",
        "updatedAt" : "2015-08-07T19:38:51Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "03490af6a4fa68c9cc9a113de04aa48fe026aae4",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +143,147 @@    - If burstable containers with multiple processes conflict, then the formula for OOM scores is a heuristic, it will not ensure \"Request and Limit\" guarantees. This is one reason why control loops will be added in subsequent iterations.\n- Pod infrastructure container\n  - OOM_SCORE_ADJ: -999\n- Kubelet, Docker, Kube-Proxy\n  - OOM_SCORE_ADJ: -999 (won’t be OOM killed)"
  },
  {
    "id" : "683c8302-837a-4e3e-bf3d-e450a35662dd",
    "prId" : 11713,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01fa0839-9d73-448f-989b-2547a088ae8c",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Why not -1000?\n",
        "createdAt" : "2015-08-07T19:00:59Z",
        "updatedAt" : "2015-08-07T19:00:59Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "03490af6a4fa68c9cc9a113de04aa48fe026aae4",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +145,149 @@  - OOM_SCORE_ADJ: -999\n- Kubelet, Docker, Kube-Proxy\n  - OOM_SCORE_ADJ: -999 (won’t be OOM killed)\n  - Hack, because these critical tasks might die if they conflict with guaranteed containers. in the future, we should place all user-pods into a separate cgroup, and set a limit on the memory they can consume.\n"
  },
  {
    "id" : "ba6493bd-5446-43ba-9474-33e1aad44f52",
    "prId" : 11713,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cc94bd1-8014-489e-94ca-e2200582e5db",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "This is a little confusing. It implies you are not using CPU limits, but earlier you said \"Containers will be throttled if they exceed their limit. If limit is unspecified, then the containers can use excess CPU when available.\"\n",
        "createdAt" : "2015-08-07T19:10:19Z",
        "updatedAt" : "2015-08-07T19:10:26Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "03490af6a4fa68c9cc9a113de04aa48fe026aae4",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +174,178 @@- **CPU-sharing Issues** Suppose that a node is running 2 container: a container A requesting for 50% of CPU (but without a CPU limit), and a container B not requesting for resoruces. Suppose that both pods try to use as much CPU as possible. After the proposal is implemented, A will get 100% of the CPU, and B will get around 0% of the CPU. However, a fairer scheme would give the Burstable container 75% of the CPU and the Best-Effort container 25% of the CPU (since resources past the Burstable container’s request are not guaranteed). TODO: think about whether this issue to be solved, implement a solution.\n- **CPU kills**: System tasks or daemons like the Kubelet could consume more CPU, and we won't be able to guarantee containers the CPU amount they requested. If the situation persists, we might want to kill the container. TODO: Draft a policy for CPU usage killing and implement it.\n- **CPU limits**: Enabling CPU limits can be problematic, because processes might be hard capped and might stall for a while. TODO: Enable CPU limits intelligently using CPU quota and core allocation.\n\nDocumentation:"
  }
]