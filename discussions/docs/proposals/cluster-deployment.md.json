[
  {
    "id" : "8429f932-4e78-4dbd-a368-70faaea90d96",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d863983-4da7-4d2e-923c-5f66276ba5d1",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "You took away two of the requirements from the original doc here. \n",
        "createdAt" : "2015-12-07T18:26:54Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "0eb8428a-2131-4b50-980e-34b8f65e187b",
        "parentId" : "2d863983-4da7-4d2e-923c-5f66276ba5d1",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "I removed `nsenter` because with the proposed way to run kubelet we don't need it anymore.\n\nI removed requirement for kubelet binary to be installed because for setups where we run kubelet in docker container it'd be awkward to first install it and then run few steps later. For deployments that use images which already have it installed (like GCI) it will be just easier to run it in the next step.\n",
        "createdAt" : "2015-12-08T07:49:45Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : null,
    "diffHunk" : "@@ -1,1 +60,64 @@- Basic connectivity between nodes (i.e. nodes can all ping each other)\n- Docker installed (and in production setups should be monitored to be always running)\n- One of the supported OS\n\nWe will provide a node specification conformance test that will verify if provisioning has been successful."
  },
  {
    "id" : "7323a8b1-23d2-4497-93d6-b33f14f737ae",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69e15ee8-e65f-482d-a1a7-09600bd6e0e4",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "You need to expand this to say that we can ping a container running on the docker bridge.  Since we pretty much are guaranteed to be able to ping the machines.  It's pinging the containers that run under Docker that we care about.\n",
        "createdAt" : "2015-12-07T21:00:54Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "695ace33-e6af-4a5c-a325-bd71a0885255",
        "parentId" : "69e15ee8-e65f-482d-a1a7-09600bd6e0e4",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "But that would mean that we already have \"kubernetes network\" (i.e. we have forwarding rules for subnetworks assigned for each node). According to @bprashanth we can do it only once we have master (and in the future kubelet) running.\n",
        "createdAt" : "2015-12-08T08:32:44Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "162db4fb-43f4-463f-bfa1-d35a3e719201",
        "parentId" : "69e15ee8-e65f-482d-a1a7-09600bd6e0e4",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Given the current architecture we'd need the kubelet to create the node, the nodecontroller to allocated cidrs and the route controller to create routes. With flannel we'd need each flannel daemon to request a cidr from the flannel master. I think it's easier to take this as a first pass. \n",
        "createdAt" : "2016-01-05T03:20:23Z",
        "updatedAt" : "2016-01-05T03:20:23Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +84,88 @@\n1. Can ```ssh``` to all machines and run a test docker image\n2. Can ```ssh``` to master and nodes and ping other machines\n\n## Step 2: Generate certificates"
  },
  {
    "id" : "8cf68b55-5550-4e19-8a55-39b0d4110179",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5d04c2a-d6ff-4f85-8aa9-d01d11cf2691",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Can you layout the specific certificates that need to be generated?\n",
        "createdAt" : "2015-12-07T21:01:33Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "838b1403-3d49-42f4-a3d8-4955081a5497",
        "parentId" : "c5d04c2a-d6ff-4f85-8aa9-d01d11cf2691",
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "What about certs for securing masters to etcd?\n",
        "createdAt" : "2015-12-11T08:31:11Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "00bb0af6-ed64-422a-8a03-9e4da8ea6bc6",
        "parentId" : "c5d04c2a-d6ff-4f85-8aa9-d01d11cf2691",
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "I'd also like to see a standardized method of generating these certs that can be productionized.\n\nFor openshift-ansible we are currently using a combination if straight openssl and an openshift CA tool to generate all the certs we need. \n\nContrib/ansible uses a modified version of easyrsa3 that is pulled down from a web address on each run.\n\nThe ideal solution will either be a integrated tool (like the openshift CA) or a packaged version of easyrsa3 that supports both the SAN certs as well as combined client/server certs (needed for etcd peer certs)\n",
        "createdAt" : "2015-12-11T08:37:18Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "6cf164ac-7706-40a7-b7de-01c98dbdbc8d",
        "parentId" : "c5d04c2a-d6ff-4f85-8aa9-d01d11cf2691",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "@detiber Regarding etcd communication in default setup we communicate using localhost without SSL. Regarding cert generations, this PR will not go into details like certs generation method.\n\n@brendandburns Can I leave a TODO for now? I'll send a followup PR for this, if that's ok with you.\n",
        "createdAt" : "2015-12-11T15:04:37Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "d9202950-f61b-4cc8-80bd-6d1b9ae37a9b",
        "parentId" : "c5d04c2a-d6ff-4f85-8aa9-d01d11cf2691",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "@detiber For the simple setup, cert generation should be automatable (also see https://github.com/kubernetes/kubernetes/issues/18112). For more boutique deployments, we also need to support static cert distribution. \n",
        "createdAt" : "2015-12-16T22:20:19Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : null,
    "diffHunk" : "@@ -1,1 +88,92 @@## Step 2: Generate certificates\n\n**Objective**: Generate security certificates used to configure secure communication between client, master and nodes\n\nTODO: Enumerate ceritificates which have to be generated."
  },
  {
    "id" : "ebd3dda3-5366-4346-82ea-c92698bf6417",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af6ddd40-a31c-4b31-a232-c5ae06e6bc32",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Can we list out the complete set?\n",
        "createdAt" : "2015-12-07T21:05:04Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "52179764-96c0-4e0b-ae21-b45d6860476d",
        "parentId" : "af6ddd40-a31c-4b31-a232-c5ae06e6bc32",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "It's listed in substeps. Currently it's just `kube-proxy`.\n",
        "createdAt" : "2015-12-08T09:15:32Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : null,
    "diffHunk" : "@@ -1,1 +169,173 @@## Step 5: Add daemons\n\n**Objective:** Start all system daemons (e.g. kube-proxy)\n\n**Substeps:**:"
  },
  {
    "id" : "e43b17f8-d0c5-46ce-92fc-e9532a40d67b",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Any chance we might consider cloud-init instead? It's even simpler. I'm not positive we want a central-server style deployment.  This makes it harder to add nodes in a one-off manner.\n",
        "createdAt" : "2015-12-07T21:06:37Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "2ba1d26d-6d62-47ea-bfd6-e77fe15f944d",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "I agree with @brendandburns. I fail to see the value of ansible over cloud-init or even salt here. Whatever we run here should not require central-server style deployment.\n\n> It has low requirements on the cluster machines\n\nSo does salt running in a docker container.\n",
        "createdAt" : "2015-12-08T08:12:58Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "58ade088-1a34-4304-8872-c7f77e7610e7",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "We can definitely consider :)\n\nHowever I don't see a problem with adding new nodes using some ansible commands. I guess you can just apply some roles just to the new server and it should configure new machine properly. Am I missing something?\n\nAFAIU for cloudinit you need to install it on the node. For ansible you don't have to do it.\n",
        "createdAt" : "2015-12-08T08:20:32Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "499cbde4-525e-4140-abcf-d264bab1917f",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "@mikedanese I think we agree that salt is just too complicated for our use-case. Let me take a closer look at cloud-init.\n",
        "createdAt" : "2015-12-08T09:18:35Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "d4f50e0f-529f-451f-be3b-60fbf1d8e290",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "cloud-init targets local initialization of cloud instances which is exactly what we want to do. CoreOS and trusty deployments already use cloud-init. I think we should generate cloud-init templates with jinja. I don't see ansible as any less complicated than salt.\n",
        "createdAt" : "2015-12-10T20:04:47Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "5cc15fcb-db60-4fbc-95e0-ebedd3a3a3ad",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Ansible doesn't require a central server.  It requires SSH access to all machines.  It requires no agent or install on any host.\n",
        "createdAt" : "2015-12-10T23:01:14Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "bf0a13d4-181d-49b1-bad2-ddc5de9ba7e2",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "cloud-init is only available in IaaS environments.\n",
        "createdAt" : "2015-12-10T23:01:43Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "674b63b5-9224-4d9d-b25e-c853d305ab01",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "but where do you run ssh when you resize an autoscaling group?\n",
        "createdAt" : "2015-12-10T23:03:20Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "33d59a40-09e7-468b-b3dc-cffdbd9ae47e",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "> cloud-init is only available in IaaS environments.\n\nnot true, there are certain variables that can only be expanded by talking to a IaaS metadata stub like $external_ipv4 but those can be largely avoided. You can also fill them in with a non IaaS [datasource](https://cloudinit.readthedocs.org/en/latest/topics/datasources.html). Here's a blog post on how to use cloud-init to configure a bare metal atomic cluster http://www.projectatomic.io/blog/2015/06/creating-a-simple-bare-metal-atomic-host-cluster/. In this example the configure a meta-data file as a datasource rather than a IaaS metadata server.\n",
        "createdAt" : "2015-12-10T23:07:10Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "a1db9336-23a1-46ae-8fd6-479efde05752",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "The percentage of deployments of Kube without autoscaling groups would\nsurprise you. :)\n\nOn Dec 10, 2015, at 6:03 PM, Mike Danese notifications@github.com wrote:\n\nIn docs/proposals/cluster-deployment.md\nhttps://github.com/kubernetes/kubernetes/pull/18287#discussion_r47300326:\n\n> +\n> +**Substeps:**:\n> +\n> +1. Create daemonset for kube-proxy\n> +\n> +### Step 7: Add add-ons\n> +\n> +**Objective**: Add default add-ons (e.g. dns, logging)\n> +\n> +**Substeps:**:\n> +\n> +1. Create Deployments (and daemonsets if needed) for all add-ons\n> +\n> +## Deployment technology\n> +\n> +We will use Ansible as the default technology for deployment orchestration. It has low requirements on the cluster machines\n\nbut where do you run ssh when you resize an autoscaling group?\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/18287/files#r47300326.\n",
        "createdAt" : "2015-12-11T00:14:51Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "a16eede2-1c6d-48df-8c70-e4f311034518",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Colin might weigh in here but this is extremely uncommon.\n\nOn Dec 10, 2015, at 6:07 PM, Mike Danese notifications@github.com wrote:\n\nIn docs/proposals/cluster-deployment.md\nhttps://github.com/kubernetes/kubernetes/pull/18287#discussion_r47300699:\n\n> +\n> +**Substeps:**:\n> +\n> +1. Create daemonset for kube-proxy\n> +\n> +### Step 7: Add add-ons\n> +\n> +**Objective**: Add default add-ons (e.g. dns, logging)\n> +\n> +**Substeps:**:\n> +\n> +1. Create Deployments (and daemonsets if needed) for all add-ons\n> +\n> +## Deployment technology\n> +\n> +We will use Ansible as the default technology for deployment orchestration. It has low requirements on the cluster machines\n\ncloud-init is only available in IaaS environments.\n\nnot true, there are certain variables that can only be expanded by talking\nto a IaaS metadata stub like $external_ipv4 but those can be largely\navoided. Here's a blog post on how to use cloud-init to configure a bare\nmetal atomic cluster\nhttp://www.projectatomic.io/blog/2015/06/creating-a-simple-bare-metal-atomic-host-cluster/\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/18287/files#r47300699.\n",
        "createdAt" : "2015-12-11T00:15:36Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "fd3882ac-4c6f-4a35-a9f8-afd37f6ce933",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "And why not both?  The idea of being able to scale out nodes with just cloud-init makes native cloud scaling easy, and being able to use ansible is easier and more intuitive for non-cloud environments.\n\nI've toyed with the idea of encapsulating the logic in a python script that could be called as either a cloud-init script or an ansible module.\n",
        "createdAt" : "2015-12-11T08:17:06Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "b9ced3a9-6261-49b7-8966-bcfc29773420",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "To add a bit, using Ansible for building out at least the first master and etcd instance(s) is preferable to me than plain cloud-init, since otherwise there would still need to be some type of orchestration of building out the configs as well as generating and distributing the certs.\n\nAfter that initial bootstrapping is done, I feel like we should be able to provision additional masters or nodes using a minimal cloud-init (or ansible module) that provides just enough config to be able to connect to the existing cluster and join.\n\nIdeally, I'd like to see this minimal config consist of just the certs and addresses needed to access the master (if provisioning a node) or etcd (if provisioning a master)\n",
        "createdAt" : "2015-12-11T08:27:35Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "76d76419-475a-4758-8996-ecf1c4a68c35",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Something that can run locally is a hard requirement here. We need something that is compatible with Autoscaling group/MIG deployment style. It looks like ansible supports something like this. http://docs.ansible.com/ansible/playbooks_delegation.html#local-playbooks\n\nI still don't buy that ansible is less complex then masterless salt though but maybe less complexity is not the main reason for switching.\n\n> why not both?\n\nMore lines of code and more complexity. If we can avoid this, we should.\n\n> there would still need to be some type of orchestration of building out the configs as well as generating and distributing the certs\n\nYes we'd probably generate cloud-config from jinja templates and a short script so we could reuse a lot of the salt. Right now, we use salt as a config file template and not much else.\n",
        "createdAt" : "2015-12-11T20:04:17Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "123e2249-a5b1-42e9-b7e1-28aa5d4d1f74",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "The main reasons why I think Ansible might be better are:\n- it's more portable (it's just ssh to the node) and it's easier conceptually (it' just running commands over ssh)\n- it's already somewhat accepted by the community (e.g RedHat and Samsung are using it).\n\nI think that for cloud deployments where we want to support cluster autoscaling (which is very rare, and currently works only for GCE/GKE and is not integrated with kubernetes almost at all) we can just have slightly different approach. If we follow this proposal than for GKE, where we are using GCE routes for kubernetes networking, we would just have to copy certificates (using metadata server most likely) and kubelet config, and run two docker containers. Doesn't sound too complicated. I imagine it'd be configured somewhere during \"provisioning\" step.\nFor other deployments adding additional nodes would be running \"provision\", \"deploy node\" and \"setup kubernetes networking\" steps. That would be a single command on the user-side.\n",
        "createdAt" : "2015-12-14T10:37:35Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "f3ae8fb6-35f8-40ed-838c-68f057ffb075",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "> Ideally, I'd like to see this minimal config consist of just the certs and addresses needed to access the master (if provisioning a node) or etcd (if provisioning a master)\n\n@detiber: This is the goal for the end-state. We also need to figure out how to get from where we are today to that end-state. \n",
        "createdAt" : "2015-12-16T22:31:56Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "8564e9a0-7dcc-4822-8358-af53fe264448",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "We had a long discussion with @brendandburns, @roberthbailey and @mikedanese about technology and what we really care about. I'll try summarize our reasoning here.\n\nFirst of all we don't really care about the technology itself as long as it provides a way to use MIG-like deployments (both Ansible and cloud-init meet this requirement). What is important however is that we structure scripts/deployment in a way that each node can be deployed separately from other nodes. This will allow us to avoid cross-node dependencies and will make it easy to add additional nodes. We hope that structuring script properly will help us to avoid breaking theses assumptions in the future.\n\nAs it seems that Ansible is more adopted among our community we suggest to use it as the technology to implement deployment machinery.\n\n@brendandburns, @roberthbailey @mikedanese - Did I miss something? \n",
        "createdAt" : "2015-12-17T14:36:18Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "70169ac9-e92e-4d58-b1e0-4e964b144431",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "> I've toyed with the idea of encapsulating the logic in a python script that could be called as either a cloud-init script or an ansible module.\n\n@detiber We've had a lot of push-back on current python scripts in the repository because some OS distributions don't have python and other scripts only work with certain python versions (2.7 but not 3.4 or vice versa). I think we should avoid python if at all possible to make the deployment more cross platform. \n",
        "createdAt" : "2015-12-17T18:35:26Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "b280b594-144e-44cb-98a5-2ba8662507c0",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "> The idea of being able to scale out nodes with just cloud-init makes native cloud scaling easy...\n\nWhat @mikedanese mentioned earlier is that we already have scaling out with masterless salt and using local ansible playbooks can achieve the same purpose while (theoretically) sharing code/configuration for the non-cloud case. \n",
        "createdAt" : "2015-12-17T18:36:53Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "57064192-2f67-4016-bf24-75a82d10d54d",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "@roberthbailey You've brought up an important issue being requirements on the node and controlling machine (from where we run kube-up). It seems that both cloud-init and Ansible requires python installed on the node.\n",
        "createdAt" : "2015-12-18T13:29:52Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "a39f992e-f388-4838-b37e-df0c647287a0",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "That was certainly inadvertent. :smile: \n\nSome relevant issues: https://github.com/kubernetes/kubernetes/issues/18805 and https://github.com/kubernetes/kubernetes/issues/9849\n",
        "createdAt" : "2015-12-18T18:29:28Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "029157f6-5593-4bb9-ab66-4401044b0b94",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Apparently ansible requires that the target host have python installed. http://docs.ansible.com/ansible/intro_installation.html#managed-node-requirements\n\nI wish there was something that templated and placed files at certain filepaths and registered ONE systemd unit for the kubelet. We are going to have trouble getting this simpler deployment running on CoreOS and others\n",
        "createdAt" : "2016-02-22T23:52:27Z",
        "updatedAt" : "2016-02-22T23:53:30Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "d48bc890-2299-43df-aabd-526f4513eb11",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "> Ansible doesn't require a central server. It requires SSH access to all machines. It requires no agent or install on any host.\n\nSeems to require python and some pip packages.\n",
        "createdAt" : "2016-02-23T00:12:41Z",
        "updatedAt" : "2016-02-23T00:12:41Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "6bcb4566-2b58-4fd4-870f-b3b14986b75d",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Is there any way we could convince coreos to include python 2.X in their base image or is that a non-starter? @yifan-gu @aaronlevy\n",
        "createdAt" : "2016-02-23T00:12:54Z",
        "updatedAt" : "2016-02-23T00:12:54Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "3cc36245-d655-4c66-ab1b-1a8e665d1281",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "body" : "Including python in the image would be a non-starter.\n",
        "createdAt" : "2016-02-23T00:48:24Z",
        "updatedAt" : "2016-02-23T00:48:24Z",
        "lastEditedBy" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "tags" : [
        ]
      },
      {
        "id" : "a70388dd-dec4-4b48-b754-a3e9b013844a",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "@aaronlevy Could you please explain why? \n",
        "createdAt" : "2016-02-23T00:55:01Z",
        "updatedAt" : "2016-02-23T00:55:01Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "a3561e40-ebb6-46ba-9888-9e50eaf22ddc",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "bf95bd39-e6f3-44fb-a035-236e724b169e",
        "body" : "CoreOS is designed to be a base system that has just enough in it to bring up a system and is safe to automatically upgrade. Allowing extra software that we don't ship in the base image to link against libraries in base or shipping entirely new runtime and libraries specifically for that purpose increases the interface between the OS and user applications dramatically, far beyond what we can promise will be stable from release to release. Automatic upgrades need that stability promise in order to work.\n",
        "createdAt" : "2016-02-23T01:16:51Z",
        "updatedAt" : "2016-02-23T01:16:51Z",
        "lastEditedBy" : "bf95bd39-e6f3-44fb-a035-236e724b169e",
        "tags" : [
        ]
      },
      {
        "id" : "770b5f26-e214-48cc-a0a9-5d439f814650",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Seems pretty reasonable.\n",
        "createdAt" : "2016-02-23T02:31:25Z",
        "updatedAt" : "2016-02-23T02:31:25Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "5f5ce11b-9393-4dce-9b44-2e09e763ad37",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "With ansible you can always drop down to `raw` or `script` for CoreOS and others (atomic comes with python installed), in which case this is just something abstracted behind the role (or not even abstracted, just templated, copied, and exploded).\n",
        "createdAt" : "2016-02-23T04:44:44Z",
        "updatedAt" : "2016-02-23T04:44:44Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "5e2242df-9ed1-4043-8b49-cc2082d7368b",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "I was able to work around the python requirement running ansible in a container on the node, using the ansible chroot connector, and bind mounting /usr /bin /lib and /lib64 into the chroot.\n",
        "createdAt" : "2016-02-23T05:51:26Z",
        "updatedAt" : "2016-02-23T05:51:26Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "87a0c570-e340-4371-b8a4-db6d03ab32e5",
        "parentId" : "94b2d65e-a619-49f3-87e0-ece288a8175e",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "> You've brought up an important issue being requirements on the node and controlling machine (from where we run kube-up). It seems that both cloud-init and Ansible requires python installed on the node.\n\nThere is a golang implementation of cloud-init. https://github.com/coreos/coreos-cloudinit\n",
        "createdAt" : "2016-03-18T03:21:27Z",
        "updatedAt" : "2016-03-18T03:21:27Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : 191,
    "diffHunk" : "@@ -1,1 +189,193 @@## Deployment technology\n\nWe will use Ansible as the default technology for deployment orchestration. It has low requirements on the cluster machines\nand seems to be popular in kubernetes community which will help us to maintain it.\n"
  },
  {
    "id" : "8aaab5ef-36fb-48c6-859f-633714aafe68",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1e30a1a-7ba8-4423-96c8-bde258082c09",
        "parentId" : null,
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "Personally, I'd like to see any kubelet config be limited to the bare minimum needed to contact the api server. Managing config files has been a major pain point for my work on openshift-ansible. \n",
        "createdAt" : "2015-12-11T08:01:45Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "9b6d5d08-ce3a-4adf-8b50-02631db23ade",
        "parentId" : "e1e30a1a-7ba8-4423-96c8-bde258082c09",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Once we have kubelet config in apiserver ( @mikedanese is working on this) this will be the case. For now we don't have it.\n",
        "createdAt" : "2015-12-11T15:05:14Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +110,114 @@kubelet binary and will run it using ```nsenter``` to workaround problem with mount propagation\n1. kubelet config file - we will read kubelet configuration file from disk instead of apiserver; it will\nbe generated locally and copied to all nodes.\n\n**Exit criteria**:"
  },
  {
    "id" : "e65bd17d-88d5-479e-94b5-8283fda8a21c",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb43fbd7-9449-4270-a7c9-4f0820cf3fcb",
        "parentId" : null,
        "authorId" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "body" : "Why limit to just master and nodes? Shouldn't this also extend to externally hosted etcd hosts as well?\n",
        "createdAt" : "2015-12-11T08:29:11Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "6746e0cc-0c52-4344-889b-945a672c83e3",
        "tags" : [
        ]
      },
      {
        "id" : "8d182dd1-cea8-4460-bfb8-80ea39662dd1",
        "parentId" : "eb43fbd7-9449-4270-a7c9-4f0820cf3fcb",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Because for this is limited to the basic setup where etcd is running on the same node as master. This might be a future extension, but not for first version.\n",
        "createdAt" : "2015-12-11T14:58:59Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : null,
    "diffHunk" : "@@ -1,1 +56,60 @@\n**Objective**: Create a set of machines (master + nodes) where we will deploy Kubernetes.\n\nFor this phase to be completed successfully, the following requirements must be completed for all nodes:\n- Basic connectivity between nodes (i.e. nodes can all ping each other)"
  },
  {
    "id" : "f83f4b6b-968f-4aba-bf4b-88f47153d28f",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40455925-d5dc-4a40-be36-d78a608b4f1d",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I think this is backwards. Right now the kubelet gets configuration from a bunch of flags. We are moving towards having it in a config file instead, and then eventually towards having it in a config object in the apiserver. We don't want to go from config in the apiserver to decentralized config files on each node. \n",
        "createdAt" : "2015-12-16T22:23:41Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "1ed27f27-36c0-4639-bc65-3471d4a63467",
        "parentId" : "40455925-d5dc-4a40-be36-d78a608b4f1d",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Maybe I just read it backwards. By v1.2 here you mean the intermediate step we expect to achieve by the 1.2 release of kubernetes, not the second iteration of this specific design. \n",
        "createdAt" : "2015-12-16T22:26:02Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "809279b8-414e-417d-8469-5029a7d6b938",
        "parentId" : "40455925-d5dc-4a40-be36-d78a608b4f1d",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Yes - `v1.2 simplifications` describes how this might look for v1.2 release, which is a intermediate step.\n",
        "createdAt" : "2015-12-17T14:11:24Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +109,113 @@1. kubelet-runner.sh - we will provide a custom docker image to run kubelet; it will contain\nkubelet binary and will run it using ```nsenter``` to workaround problem with mount propagation\n1. kubelet config file - we will read kubelet configuration file from disk instead of apiserver; it will\nbe generated locally and copied to all nodes.\n"
  },
  {
    "id" : "ded6106a-075d-493e-a18d-1378529ce6a1",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b63e9860-576f-4285-95ec-d1ea0944acdc",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Can kubelet be run as a standalone binary instead of being run inside docker container?\n",
        "createdAt" : "2015-12-18T23:15:09Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "fefbdd0f-1757-4d95-8432-77710b27410b",
        "parentId" : "b63e9860-576f-4285-95ec-d1ea0944acdc",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "You mean as part of the simplification? I think it'd make the deployment harder as we'd have to copy binary, configure something like supervisord (or equivalent) etc.\n",
        "createdAt" : "2015-12-22T10:25:18Z",
        "updatedAt" : "2015-12-22T10:30:31Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "a4914451-e872-43a1-9ae1-5db64011bbf2",
        "parentId" : "b63e9860-576f-4285-95ec-d1ea0944acdc",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "My concern is mainly about separating the lifecycle of kubelet from that of docker.\nIs running supervisord inside the docker container and managing a kubelet running in the host through that an option?\n",
        "createdAt" : "2015-12-28T19:51:21Z",
        "updatedAt" : "2015-12-28T19:51:21Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "744c3ad9-8086-4ebe-bb31-9e8cf0344997",
        "parentId" : "b63e9860-576f-4285-95ec-d1ea0944acdc",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "I'm not sure I follow. What would be the advantage of such setup?\n",
        "createdAt" : "2015-12-29T10:38:25Z",
        "updatedAt" : "2015-12-29T10:38:25Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +107,111 @@**v1.2 simplifications**:\n\n1. kubelet-runner.sh - we will provide a custom docker image to run kubelet; it will contain\nkubelet binary and will run it using ```nsenter``` to workaround problem with mount propagation\n1. kubelet config file - we will read kubelet configuration file from disk instead of apiserver; it will"
  },
  {
    "id" : "6f2e995d-f3a6-4b2c-bdb8-72d32ff177d2",
    "prId" : 18287,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cae76ee-d170-437b-a2ff-541da98ec9ce",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Currently flannel acquires it's own cidr and the nodecontroller is told to back off from cidr allocation when --networking=flannel. In general I'd like to support both modes, advantages of allowing third party cidr allocation include eg: being able to use non-continguous cidr ranges. no reason all that logic should live in nodecontroller, which will just do the simple 'take this /16 and carve out contiguous /24s' thing. \n\nNot sure how this will get implemented down the line, so best to be vague about flannel acquiring/using the allocated cidr, if possible.\n",
        "createdAt" : "2016-01-05T03:24:40Z",
        "updatedAt" : "2016-01-05T03:24:40Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "8f089a1e-25bd-4329-8b46-49bf5919068c",
        "parentId" : "4cae76ee-d170-437b-a2ff-541da98ec9ce",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Oh sorry, you have this in the simplications section. Leaving comment in for context anyway.\n",
        "createdAt" : "2016-01-05T03:25:18Z",
        "updatedAt" : "2016-01-05T03:25:18Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9398ade21925ef7d487f9e7d0fd91b9409c38e79",
    "line" : 155,
    "diffHunk" : "@@ -1,1 +153,157 @@\n1. copy manifest for flannel server to master machine\n2. create a daemonset with flannel daemon (it will read assigned CIDR and configure network appropriately).\n\n**v1.2 simplifications**:"
  }
]