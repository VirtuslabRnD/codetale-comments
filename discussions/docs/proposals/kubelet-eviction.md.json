[
  {
    "id" : "dd2eb1c6-9b2d-4718-bd23-9853608498ca",
    "prId" : 25237,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d842e6b6-c1a0-4f27-b0a0-7f16c45d3458",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "+1 - keeps Kubelet policy consistent across memory and disk by framing it as a best effort resource level decision.\n",
        "createdAt" : "2016-05-06T01:37:21Z",
        "updatedAt" : "2016-05-20T22:46:52Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "34ebb7e384c6a14e74c44c9b9e711687ed707b92",
    "line" : 225,
    "diffHunk" : "@@ -1,1 +421,425 @@\nThe rationale for failing **all** pods instead of just best effort is because disk is currently\na best effort resource for all QoS classes.\n\nKubelet will apply the same policy even if there is a dedicated `image` filesystem."
  },
  {
    "id" : "0c979d91-7fd9-4068-be94-a3f2e6927b31",
    "prId" : 25237,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b65fbfcd-0a2f-4c91-88ce-aba89efe8a7e",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Is this required to be specified for each signal?\n",
        "createdAt" : "2016-05-17T01:57:08Z",
        "updatedAt" : "2016-05-20T22:46:52Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "3c2e791d-9c6a-4691-b5bf-25defeee9a91",
        "parentId" : "b65fbfcd-0a2f-4c91-88ce-aba89efe8a7e",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "`0` is the default. Specified that explicitly in the patch.\n",
        "createdAt" : "2016-05-17T18:38:05Z",
        "updatedAt" : "2016-05-20T22:46:52Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "34ebb7e384c6a14e74c44c9b9e711687ed707b92",
    "line" : 175,
    "diffHunk" : "@@ -1,1 +360,364 @@Following are the flags through which `minimum-thresholds` can be configured for each evictable resource:\n\n`--minimum-eviction-thresholds=\"memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi\"`\n\nThe default `minimum-eviction-threshold` is `0` for all resources."
  },
  {
    "id" : "2de8241d-1c19-48cc-a760-94a95f8e8df7",
    "prId" : 25237,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af39eb9a-6a2b-4cfc-9040-c9a71acab1d2",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I think this is fine.  If folks want to get fancier they can write a pod and run a DaemonSet.\n",
        "createdAt" : "2016-05-17T01:58:06Z",
        "updatedAt" : "2016-05-20T22:46:52Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "96714777-7e00-4564-b650-1bd336706936",
        "parentId" : "af39eb9a-6a2b-4cfc-9040-c9a71acab1d2",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Acknowledged.\n",
        "createdAt" : "2016-05-17T18:38:18Z",
        "updatedAt" : "2016-05-20T22:46:52Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "34ebb7e384c6a14e74c44c9b9e711687ed707b92",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +326,330 @@Images will be deleted based on eviction thresholds. If kubelet can delete logs and keep disk space availability\nabove eviction thresholds, then kubelet will not delete any images.\nIf `kubelet` decides to delete unused images, it will delete *all* unused images.\n\n### Evict pods"
  },
  {
    "id" : "04537fe1-6aa5-41f8-877e-6ea4c1302185",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66cdaaf5-c183-44ec-92e7-a3854a79c423",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "...and preventing out of resource situations.  ?\n",
        "createdAt" : "2015-12-15T21:26:50Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "31e708ba-7101-431f-9f09-39c975b889d8",
        "parentId" : "66cdaaf5-c183-44ec-92e7-a3854a79c423",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "That's taints. \n",
        "createdAt" : "2016-01-06T16:31:33Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +57,61 @@## Scope of proposal\n\nThis proposal defines a pod eviction policy for reclaiming compute resources.\n\nIn the first iteration, it focuses on memory; later iterations are expected to cover"
  },
  {
    "id" : "44129287-882b-41f8-8e4f-9b2517057657",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af3993c5-ed2a-4361-820d-ccfae66ba5bb",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Are the mechanics intented to be portable to future resources?\n",
        "createdAt" : "2015-12-15T21:27:34Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "214d2db3-4c50-4a2e-ba22-07d01d762349",
        "parentId" : "af3993c5-ed2a-4361-820d-ccfae66ba5bb",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Nvm, I continued reading\n",
        "createdAt" : "2015-12-15T21:28:00Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +61,65 @@In the first iteration, it focuses on memory; later iterations are expected to cover\nother resources like disk.  The proposal focuses on a simple default eviction strategy\nintended to cover the broadest class of user workloads.\n\n## Eviction Signals"
  },
  {
    "id" : "aa538b19-ec3f-421f-b855-9c32f0d0f260",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "575a17d1-a947-47b4-8724-51d6b22b29bb",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Can we clarify the higher level requirements or goals explicitly before proposing solutions?\n",
        "createdAt" : "2015-12-16T02:21:51Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "2908a388-f7fc-4559-9ccb-e973f7f5fb0d",
        "parentId" : "575a17d1-a947-47b4-8724-51d6b22b29bb",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Sure, will add a section on goals.  \n",
        "createdAt" : "2015-12-16T03:18:50Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +62,66 @@other resources like disk.  The proposal focuses on a simple default eviction strategy\nintended to cover the broadest class of user workloads.\n\n## Eviction Signals\n"
  },
  {
    "id" : "4fd030d1-7a81-4fc6-a8c7-8124d822f0a4",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdaf5b63-9f3b-4dc2-988d-08e27954f6fc",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Specifically within `BestEffort` I'd like to choose the most-recent jobs first. \n",
        "createdAt" : "2015-12-16T02:32:18Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "25e51768-1f6a-441c-9e55-06cb63086067",
        "parentId" : "cdaf5b63-9f3b-4dc2-988d-08e27954f6fc",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I went back and forth on that and thought keeping greedy consistent made sense to me across all qos tiers since it seemed most in line with our existing OOM killer process.  \n",
        "createdAt" : "2015-12-16T03:29:30Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +204,208 @@quality of service tier in the following order.\n\n* `BestEffort` pods that consume the most of the starved resource are failed\nfirst.\n* `Burstable` pods that consume the greatest amount of the starved resource"
  },
  {
    "id" : "7e3cb9d3-b855-4adf-92c0-cac7e636d3de",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Why would we ever have to evict Guaranteed pods? The scheduler essentially failed if we have to evict `Guaranteed` pods. Can we start with not Evicting `Guaranteed` pods at all?\n",
        "createdAt" : "2015-12-16T02:33:37Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "d4f7722f-54ba-46ea-bf8a-8bfbd611aa16",
        "parentId" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "See comment above on burstable.\n",
        "createdAt" : "2015-12-16T03:34:03Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "db131160-ebcf-4d98-a880-2c3328e313f3",
        "parentId" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I don't think it makes sense to ever evict `Guaranteed` pods. \n",
        "createdAt" : "2016-04-12T00:24:51Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "b885e3ae-8a5b-4985-932c-1641b43de318",
        "parentId" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "> I don't think it makes sense to ever evict Guaranteed pods.\n\n+1\n",
        "createdAt" : "2016-04-18T21:08:20Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "031d99b9-c426-44b7-aba8-13a69bc00a37",
        "parentId" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@derekwaynecarr I still think that it does not make sense to evict Guaranteed pods and even Burstable pods if their usage is < request. It violates QoS.\n",
        "createdAt" : "2016-04-25T23:06:17Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "74d8171d-9b6c-4e78-8f61-69a80e8d990f",
        "parentId" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Ping @derekwaynecarr \n",
        "createdAt" : "2016-04-27T21:30:44Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "23517e35-3dcb-441f-a1f7-5774360362e6",
        "parentId" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "see text below\n",
        "createdAt" : "2016-04-27T21:47:12Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "78730249-3981-45ed-9882-c1dc8ecefa51",
        "parentId" : "396d6000-63c9-4e71-8107-41b79df70b14",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "QoS is only as good as the foundation its built upon.  A guaranteed pod is guaranteed to never be evicted because of another pod.  If the node is on fire because the docker daemon or the kubelet or some other system daemon is exceeding their expected compute budgets, the guarantee is worth nothing.  If two guaranteed pods are on the same node, do you want to kill the experience for both pods by not evicting one of them and having the entire system OOM?\n",
        "createdAt" : "2016-04-28T19:23:44Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : 212,
    "diffHunk" : "@@ -1,1 +210,214 @@has exceeded its request, the strategy targets the largest consumer of the\nstarved resource.\n* `Guaranteed` pods that consume the greatest amount of the starved resource\nrelative to their request are killed first.  If no pod has exceeded its request,\nthe strategy targets the largest consumer of the starved resource."
  },
  {
    "id" : "fb8967f5-9ccf-4840-a33f-5064aa3bc613",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fc75dec-61ca-452b-bd2c-7da0e787f3ee",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "If no pods in the `Burstable` class are exceeding their request, why will we have to evict any pods?\nIs it because the node overhead became too high?\nEviction of pods with resource requests isn't ideal.\n",
        "createdAt" : "2015-12-16T02:34:44Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "96c57758-3489-46d4-866b-ca6bf2cb2068",
        "parentId" : "0fc75dec-61ca-452b-bd2c-7da0e787f3ee",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "If we are not enforcing system reserved and kube reserved than either of those groups could induce memory pressure and cause a need to evict Burstable.\n\nIf we had memory soft limits in place we could hope to avoid this a little better, but I don't think we can say it will never ever happen until we make a more explicit statement on enforcing the limits on system and kube daemons.  That said, I think their enforcement if done should be an operator decision.  Operator may choose for example to let docker memory growth grow and not kill that to avoid all containers from restarting.  At least that was my thought process.\n",
        "createdAt" : "2015-12-16T03:33:06Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +206,210 @@* `BestEffort` pods that consume the most of the starved resource are failed\nfirst.\n* `Burstable` pods that consume the greatest amount of the starved resource\nrelative to their request for that resource are killed first.  If no pod\nhas exceeded its request, the strategy targets the largest consumer of the"
  },
  {
    "id" : "f305f1b6-a32f-458f-8288-fa35eb4faf2a",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccb51e30-d2ed-4d92-b02d-16abcecfaf1e",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "The node needs a policy mechanism...  \n\nwhere policy could be plugable, etc. \n",
        "createdAt" : "2016-01-06T16:31:04Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "13f3b328-842b-44ad-b094-7120ba7fdd95",
        "parentId" : "ccb51e30-d2ed-4d92-b02d-16abcecfaf1e",
        "authorId" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "body" : "+1 for plugable policy\n",
        "createdAt" : "2016-01-12T18:14:36Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "aefa6d0d-0ad0-4062-966c-ddc56e70652b",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +38,42 @@## Goals\n\nThe node needs a mechanism to preserve stability when available compute resources are low.\n\nThis is especially important when dealing with incompressible compute resources such"
  },
  {
    "id" : "ed5e19f8-67e8-4902-90ca-21dcff05e5f6",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1e6ac86-4f63-4b88-8795-b965eb6ff048",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Is the intention to keep the kubelet UX/CLI common across all resources?\n",
        "createdAt" : "2016-04-12T00:15:31Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "6f4244e9-2192-42a0-acc3-d9200540f467",
        "parentId" : "a1e6ac86-4f63-4b88-8795-b965eb6ff048",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Yes, I think I should be able to set an eviction threshold in the same manner for disk as I would memory.  I think the syntax allows for that.\n",
        "createdAt" : "2016-04-12T17:22:33Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +59,63 @@This proposal defines a pod eviction policy for reclaiming compute resources.\n\nIn the first iteration, it focuses on memory; later iterations are expected to cover\nother resources like disk.  The proposal focuses on a simple default eviction strategy\nintended to cover the broadest class of user workloads."
  },
  {
    "id" : "390a71d5-9707-4532-8779-d43552debb82",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc4480ed-6438-47bd-8356-c4d56b03fbea",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "In case the scheduler messed up, the kubelet might have to reject `Guaranteed` pods as well.\n",
        "createdAt" : "2016-04-12T00:27:43Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +227,231 @@### Feasibility checks during kubelet admission\n\nThe `kubelet` will reject `BestEffort` pods if any of its associated\neviction thresholds have been exceeded independent of the configured\ngrace period."
  },
  {
    "id" : "16fd83cf-70b5-4c1c-8362-e46a4adf6228",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "237fe28e-257e-477b-8ce2-4756eb375291",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Are we going to support `>` as well? IIRC, your recent PR did have support for `>`.\n",
        "createdAt" : "2016-04-25T22:59:27Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "fd0ad78a-857b-4105-a036-9c634ea32d3f",
        "parentId" : "237fe28e-257e-477b-8ce2-4756eb375291",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "The PR had it because I was experimenting with cpu usage awhile back and forgot to take it out.  I can remove support for that operator in that PR.\n",
        "createdAt" : "2016-04-26T19:17:34Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +80,84 @@\n* valid `eviction-signal` tokens as defined above.\n* valid `operator` tokens are `<`\n* valid `quantity` tokens must match the quantity representation used by Kubernetes\n"
  },
  {
    "id" : "832af7b3-21a7-4c29-934c-0424dbf3120f",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af0af5ec-ec3e-4e6d-a190-51f7397fe433",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "The scheduler will soon support usage based scheduling. Whatever logic that kubelet follows should be shared with the scheduler as well ideally. \nIf the node is partitioned statically, this section effectively reduces the amount of `Available` resources right?\n",
        "createdAt" : "2016-04-25T23:12:46Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "e51c24bf-7a6c-4d8e-b097-7b5df979726c",
        "parentId" : "af0af5ec-ec3e-4e6d-a190-51f7397fe433",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Answering myself: Node Conditions will be used to make the nodes under memory pressure least preferable.\n",
        "createdAt" : "2016-04-27T21:33:15Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "9a30cc98-b910-4cc3-91e2-e73c310568c8",
        "parentId" : "af0af5ec-ec3e-4e6d-a190-51f7397fe433",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I can add a blurb in this section that the scheduler should use `MemoryPressure` to dissuade placing additional best effort pods on the node.\n",
        "createdAt" : "2016-04-27T21:52:05Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : 240,
    "diffHunk" : "@@ -1,1 +238,242 @@```\n\nIf the `kubelet` sees that it has less than `256Mi` of memory available\non the node, but the `kubelet` has not yet initiated eviction since the\ngrace period criteria has not yet been met, the `kubelet` will still immediately"
  },
  {
    "id" : "a4629ee8-0ba9-4a12-89b2-3c94dcf88d34",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af40618b-a51e-40bb-bb84-df0f08fd4483",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Would it make sense to support %age here? How do you envision admins setting this flag? Is a homogeneous cluster assumed here?\n",
        "createdAt" : "2016-04-27T21:26:24Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "423de52d-87fa-49e4-8907-de3e77ac1778",
        "parentId" : "af40618b-a51e-40bb-bb84-df0f08fd4483",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I think %age is confusing to some degree to express in a flag as it makes the syntax more complex, and I am not sure how it helps in a heterogenous cluster versus homogenous cluster.  We don't specify system-reserved or kube-reserved in relative percentages, so I would prefer to be consistent here.\n",
        "createdAt" : "2016-04-27T21:42:20Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "7707d0b5-0d77-47a2-a826-c8cb155e806f",
        "parentId" : "af40618b-a51e-40bb-bb84-df0f08fd4483",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "> How do you envision admins setting this flag?\n\nBased on workload and experience.  Average number of pods per node, etc.  Many clusters will have utilization metrics that they have seen and will have experienced spikes in utilization that induced OOM.  If utilization is a top priority, then I suspect the flag would be set uniformly across the cluster with a value that satisfies their target balanced with their willingness to accept OOM, speed to respond, and ability to procure new nodes.\n\nMany operators are not pushing utilization to huge numbers and are interested in node stability and just the consistent control plane afforded by Kubernetes/OpenShift.  In those cases, I suspect they would set eviction thresholds to happen earlier since they probably have slack on other nodes in the cluster and are comfortable or desire lower utilization for their workload.\n\nFor the vast majority though, I would imagine folks will follow this pattern:\n- operators are not seeing OOMs, so they set nothing\n- cluster gets popular, users do more, start to experience OOM on nodes regularly\n- operator files a bug with us and buys more nodes temporarily\n- operators notice users are not doing the right thing with their requests, or using best effort pods to run dbs, etc.\n- operators decide to experiment on a month-by-month basis to set eviction flags until their incidence of oom observed reaches satisfactory outcomes.\n- users continue to do unexpected things like run dbs in besteffort pods\n\n> Is a homogeneous cluster assumed here?\n\nNo.  Tailoring values based on node profiles is probably something I would expect Ansible to do today probably similar to how we can specify node labels.\n",
        "createdAt" : "2016-04-28T19:12:03Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +121,125 @@\n```\n--eviction-hard=\"\": A set of eviction thresholds (e.g. memory.available<1Gi) that if met would trigger a pod eviction.\n```\n"
  },
  {
    "id" : "fb074b0c-2e50-4dee-96e9-ae3fad73dd4a",
    "prId" : 18724,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0c34a0c-0dde-4999-ad7f-c08e11b5711b",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "At what point will kubelet give up on a pod termination? Is it a timeout or will it look at the reason for termination failure?\n",
        "createdAt" : "2016-04-27T21:29:59Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "95ea81f8-5f5b-4149-b4cb-63ec541613b6",
        "parentId" : "c0c34a0c-0dde-4999-ad7f-c08e11b5711b",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Ideally, we will call `KillPod` directly using the grace period override specified here in #24843 which is a blocking call.\n\nIf it returns an error, I think its in our interest to pick another rather than get fancier.\n",
        "createdAt" : "2016-04-27T21:46:35Z",
        "updatedAt" : "2016-04-29T14:25:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "542668cc7998fe0acb315a43731e1f45ecdcc85b",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +190,194 @@\nIf a pod is not terminated because a container does not happen to die\n(i.e. processes stuck in disk IO for example), the `kubelet` may select\nan additional pod to fail instead.  The `kubelet` will invoke the `KillPod`\noperation exposed on the runtime interface.  If an error is returned,"
  }
]