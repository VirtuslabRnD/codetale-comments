[
  {
    "id" : "7f4011f8-4ee7-4572-b05a-e31da4944076",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d840b7a-66dc-48c6-a5f6-257762905fb6",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I think there is the image-metadata volume caveat.  Address it here?\n",
        "createdAt" : "2015-11-07T07:13:03Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "17bb4422-bde6-46ac-9f1b-026af49d2ec3",
        "parentId" : "2d840b7a-66dc-48c6-a5f6-257762905fb6",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@pmorie: What is `image-metadata`?\n",
        "createdAt" : "2015-11-10T19:39:18Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "ca5c75a6-912a-4652-99bd-f4e24e242e2d",
        "parentId" : "2d840b7a-66dc-48c6-a5f6-257762905fb6",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "@vish: a volume specified in the docker image metadata which doesn't have an equivalent in the pod that uses it.  We have some code in origin that detects these and creates volumes for them in pods, but it's possible we can wind up in this scenario in kubernetes.  To go into detail, say a user refers to an image with this in its Dockerfile:\n\n```\nVOLUME /some/path\n```\n\nIf you refer to this image and don't mount a volume into that path, docker will create an area on disk that corresponds to this path and bind-mount it in for you automatically.\n",
        "createdAt" : "2015-11-17T00:16:35Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "6101b35c-c2fc-423a-a6a5-fb6b72ce0bf5",
        "parentId" : "2d840b7a-66dc-48c6-a5f6-257762905fb6",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@pmorie: I wonder why kubernetes isn't mimicking origin's behavior in this case.\n",
        "createdAt" : "2015-12-09T23:43:26Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "0e7e8959-786f-4b18-a5f5-1b6c1dd9559e",
        "parentId" : "2d840b7a-66dc-48c6-a5f6-257762905fb6",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I added a note. Is there an issue for this topic?\n",
        "createdAt" : "2015-12-09T23:44:14Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "97d2ea20-a20a-411b-b666-d2133f5e6745",
        "parentId" : "2d840b7a-66dc-48c6-a5f6-257762905fb6",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "@vishh The specific flow I named is in `oc new-app`, which is a macro that we have.  It doesn't apply to all pods, though a feature that does that has been discussed.\n\ncc @abhgupta\n",
        "createdAt" : "2015-12-10T18:28:33Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +122,126 @@The default location of the docker root directory is `/var/lib/docker`.\n\nVolumes are handled by kubernetes.\n*Caveat: Volumes specified as part of Docker images are not handled by Kubernetes currently.*\n"
  },
  {
    "id" : "295db58c-c7ee-42c0-bdcf-fe41c603bcf7",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd69c463-8601-49ab-a346-377f6f56f1ba",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "This section on aufs, devicemapper, and overlayfs is awesome to have in this doc, :100: \n",
        "createdAt" : "2015-11-07T07:13:55Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "495f1df4-efda-4206-bb9e-adce5a817ef7",
        "parentId" : "cd69c463-8601-49ab-a346-377f6f56f1ba",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : ":)\n",
        "createdAt" : "2015-11-10T19:39:26Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 131,
    "diffHunk" : "@@ -1,1 +129,133 @@*Note: Image layer IDs can be obtained by running `docker history -q --no-trunc <imagename>`*\n\n##### Aufs\n\nImage layers and writable layers are stored under `/var/lib/docker/aufs/diff/<id>`."
  },
  {
    "id" : "d455be20-dbaa-439e-8773-0b73f7300381",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09566cd7-dbb5-4d0e-8ca5-fddc11193fa8",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Maybe note that we will probably add read-only containers, and let that be controlled by pod security policy or another aspect, which means container writes _can_ be limited in a fashion.\n",
        "createdAt" : "2015-11-08T04:30:20Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "c7bba6a8-1a69-4f43-9c25-0fa1f0e7be25",
        "parentId" : "09566cd7-dbb5-4d0e-8ca5-fddc11193fa8",
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "Is it important to figure out the size of writable layer separately? Can we look at the size of total container (including image size).\n",
        "createdAt" : "2015-11-10T15:27:50Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "d72caa47-f631-4f39-9893-20cf6366664a",
        "parentId" : "09566cd7-dbb5-4d0e-8ca5-fddc11193fa8",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@smarterclayton: Will do. \n@rhvgoyal: Since image layers are shared, image usage needs to be accounted as infrastructure overhead. The writable layer needs to be limited to enforce a total limit on the number of bytes a pod and a container can occupy on local disk.\n",
        "createdAt" : "2015-11-10T18:03:27Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "88ac609a-41c1-40a8-907f-7e32500b6945",
        "parentId" : "09566cd7-dbb5-4d0e-8ca5-fddc11193fa8",
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "@vishh what happens if one starts pulling its images and pull in tons of those and does not actually write much data so writable layer will be small. In that case none of the image size will be accounted to the user who did it?\n\nI am wondering what's wrong accounting image size also towards container size. This is like using a system which is doing de-duplication and top layer does not know about it. \n",
        "createdAt" : "2015-11-12T20:13:41Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "13299050-51da-4964-983d-899f45368bce",
        "parentId" : "09566cd7-dbb5-4d0e-8ca5-fddc11193fa8",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Consider the following scenario:\n1. Pod `foo` belonging to user `Alice` pulls a few layers and starts running. The system charges disk usage of those layers to `Alice`.\n2. Then Pod `bar` belonging to user `Bob` re-uses some of the layers pulled by Pod `foo`. User `Bob` does not get charged for disk usage since the layers are being re-used.\n3. Pod `foo` exits and `Alice` would expect disk usage to drop. For that to happen, the usage needs to be transferred to someone else. Now the system has to keep track of layer usage and then transfer costs accordingly.\nThis is pretty complex. Since the layers are shared across all users and kubernetes namespaces, I'd prefer classifying image layer usage as infrastructure cost.\nTo prevent blowing up machines, we can perform feasibility checks before pulling images, once `dry-runs` are available for pulls. \n\nIn the future, we can consider not sharing image layers across kubernetes namespaces, for the purposes of accounting. I'm not sure if that is required today though. This will also require changes to docker or images to make them namespace scoped.\n",
        "createdAt" : "2015-11-12T20:22:51Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +91,95 @@#### Writable Layer\n\nDocker creates a writable layer for every container on the host. Depending on the storage driver, the location and the underlying filesystem of this layer will change.\n\nAny files that the container creates or updates (assuming there are no volumes) will be considered as writable layer usage."
  },
  {
    "id" : "f83d2a69-5b75-4728-afd3-a840919e08c5",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ca4eb6c-4d3e-46a2-a9ec-67bccc9f73af",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Or we can potentially offer admins a way to offer certain host paths in a controlled fashion (between host path and empty dir) - not sure what though, so nothing to add yet.\n",
        "createdAt" : "2015-11-08T04:32:31Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "84d168d3-76df-41df-a96c-3e24bf881162",
        "parentId" : "9ca4eb6c-4d3e-46a2-a9ec-67bccc9f73af",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "HostPath is the current backdoor for access to docker, secondary disks, etc, right? If we provide alternate means to access those resources, we can try making HostPath read-only by default.\n",
        "createdAt" : "2015-11-10T18:05:22Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +110,114 @@‘HostPath’ volumes map in existing directories in the host filesystem into a pod. Kubernetes manages only the mapping. It does not manage the source on the host filesystem.\n\nIn addition to this, the changes introduced by a pod on the source of a hostPath volume is not cleaned by kubernetes once the pod exits. Due to these limitations, we will have to account hostPath volumes to system overhead. We should explicitly discourage use of HostPath in read-write mode.\n\n`EmptyDir`, `GitRepo` and other local storage volumes map to a directory on the host root filesystem, that is managed by Kubernetes (kubelet). Their contents are erased as soon as the pod exits. Tracking and potentially restricting usage for volumes is possible."
  },
  {
    "id" : "59eee24e-ca48-4b1f-9266-66e8816dd0c1",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab35fe12-a2e7-4582-a4cd-69c25001491f",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "We're looking in the near term to move docker logs directly to the journal (still on disk).  I'll make sure we follow up on the implications of that change for quota (I don't know whether journald today can stripe based on container)\n",
        "createdAt" : "2015-11-08T04:42:50Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "7005cb35-6c55-465f-b92b-209254b0e057",
        "parentId" : "ab35fe12-a2e7-4582-a4cd-69c25001491f",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Ok. Will the kubelet be updated to serve logs via journal then? \n",
        "createdAt" : "2015-11-10T21:47:39Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "3990a11d-a6f8-4ecb-960d-e36724a84dd3",
        "parentId" : "ab35fe12-a2e7-4582-a4cd-69c25001491f",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Yes\n\nOn Nov 10, 2015, at 4:47 PM, Vish Kannan notifications@github.com wrote:\n\nIn docs/proposals/disk-accounting.md\nhttps://github.com/kubernetes/kubernetes/pull/16889#discussion_r44470125:\n\n> +Block size: 4096       Fundamental block size: 4096\n> +\n> +Blocks: Total: 25770312   Free: 25754754   Available: 24439938\n> +\n> +Inodes: Total: 6553600    Free: 6553556\n> +\n> +```\n> +\n> +*Note: The container’s writable layer is mounted only when the container is running.*\n> +\n> +To enforce`limits`the volume created for the container’s writable layer filesystem can be dynamically [resized](https://jpetazzo.github.io/2014/01/29/docker-device-mapper-resize/), to not use more than`limit`.`request`will have to be enforced by the kubelet.\n> +\n> +\n> +#### Container logs\n> +\n> +Container logs are not storage driver specific. We can use either`du`or`quota`to track log usage per container. Log files are stored under`/var/lib/docker/containers/<container-id>`.\n\nOk. Will the kubelet be updated to serve logs via journal then?\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/16889/files#r44470125.\n",
        "createdAt" : "2015-11-12T22:29:54Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "486ad64d-41e5-4beb-813f-fec7d5c8e571",
        "parentId" : "ab35fe12-a2e7-4582-a4cd-69c25001491f",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Does journald perform log rotation? Is the kubelet expected to do anything with logs, if it were to be running out of disk space?\nI was thinking of performing explicit log retention and rotation in kubelet. May be we can make that functionality plugin based? This is probably a topic for a separate issue though.\n",
        "createdAt" : "2015-11-13T01:28:36Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "0e18145d-14c7-4fb8-a534-2f55531b870b",
        "parentId" : "ab35fe12-a2e7-4582-a4cd-69c25001491f",
        "authorId" : "7db9cb1e-f5f2-40b7-8976-c2f85c0a6288",
        "body" : "Yes journald has it's own configurable log rotation mechanisms.\n",
        "createdAt" : "2016-02-15T05:41:17Z",
        "updatedAt" : "2016-02-15T05:41:17Z",
        "lastEditedBy" : "7db9cb1e-f5f2-40b7-8976-c2f85c0a6288",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 305,
    "diffHunk" : "@@ -1,1 +303,307 @@#### Container logs\n\nContainer logs are not storage driver specific. We can use either `du` or `quota` to track log usage per container. Log files are stored under `/var/lib/docker/containers/<container-id>`.\n\nIn the case of quota, we can create a separate gid for tracking log usage. This will let users track log usage and writable layer’s usage individually."
  },
  {
    "id" : "a705f3ab-ef29-4b8b-ab1f-0761bcc9f718",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "995824a8-bff0-4dce-b6f4-e7279e07ae40",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "I am wondering whether some smarter scheduler can detect `OutOfDisk` kubelets and stop send Pods to those kubelets. So image eviction (or other maintenance procedures) doesn't cause denial of service to Pods. Once those kubelets restore disk space, they are back to service again and.\n",
        "createdAt" : "2015-11-09T15:54:44Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "c42f1f85-976d-4017-bbcb-4c9c20a7a537",
        "parentId" : "995824a8-bff0-4dce-b6f4-e7279e07ae40",
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "Wasn't that #16179?\n",
        "createdAt" : "2015-11-09T15:57:12Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      },
      {
        "id" : "5c2f72c0-62cc-4944-a7df-b5b2aa23f238",
        "parentId" : "995824a8-bff0-4dce-b6f4-e7279e07ae40",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "@ncdc nice!\n",
        "createdAt" : "2015-11-09T16:17:03Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +44,48 @@Large images and rapid logging can lead to temporary downtime on the nodes. The node has to free up disk space by deleting images and containers. During this cleanup, existing pods can fail and new pods cannot be started. The node will also transition into an `OutOfDisk` condition, preventing more pods from being scheduled to the node.\n\nAutomated eviction of pods that are hogging the local disk is not possible since proper accounting isn’t available.\n\nSince local disk is a non-compressible resource, users need means to restrict usage of local disk by pods and containers. Proper disk accounting is a prerequisite. As of today, a misconfigured low QoS class pod can end up bringing down the entire cluster by taking up all the available disk space (misconfigured logging for example)"
  },
  {
    "id" : "c845af9f-0704-4a50-8b27-e4d8fd28fa5b",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f665ce25-8541-4964-a29f-f98ec075643a",
        "parentId" : null,
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "We strongly discourage usage of loopback devices for thin pool creation. User should be setting up an lvm thin pool from real block devices and pass that pool to docker. I want to make sure that we don't get too hung up on loopback or promote it as supported model.\n\nFollowing is link to docker-storage-setup utility which an setup an lvm thin pool relatively easily as long as one can point this utility to a volume group with free space or pass in a disk to use and utility will create a volume group and create an lvm thin pool out of it.\n\nhttps://github.com/projectatomic/docker-storage-setup\n",
        "createdAt" : "2015-11-10T15:41:09Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "586391e4-9815-494f-9454-9fd7d8af4f6c",
        "parentId" : "f665ce25-8541-4964-a29f-f98ec075643a",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Acknowledged. @rhvgoyal: Can you help clarify the language around devicemapper in this doc? Feel to send a PR against my branch or wait till this doc is merged. \nIn general, I'd appreciate help from you guys for supporting devicemapper.\n",
        "createdAt" : "2015-11-10T19:44:35Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "4fabc5aa-cd62-4c71-b463-24ba68b978fb",
        "parentId" : "f665ce25-8541-4964-a29f-f98ec075643a",
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "@vishh once doc is merged, then I can generate a PR for fixing devicemapper stuff to fix if something needs fixing.\n",
        "createdAt" : "2015-11-12T20:15:55Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "4d0b4d60-7285-4be7-a4e0-f5bb05047b72",
        "parentId" : "f665ce25-8541-4964-a29f-f98ec075643a",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Much appreciated!\n",
        "createdAt" : "2015-11-12T20:23:40Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 269,
    "diffHunk" : "@@ -1,1 +267,271 @@\nDevicemapper storage driver will setup two volumes, metadata and data, that will be used to store image layers and container writable layer. The volumes can be real devices or loopback. A Pool device is created which uses the underlying volume for real storage.\n\nA new thinly-provisioned volume, based on the pool, will be created for running container’s.\n"
  },
  {
    "id" : "4e4c4c01-d097-4489-9e18-e4bb05e8bcea",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "parentId" : null,
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "Images and containers both are stored in thin pool. Images are not stored separately on rootfs. So I don't understand that how do you come up with image\nusage.\n",
        "createdAt" : "2015-11-10T15:43:49Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "7bd1ac89-5e3a-43eb-b0e4-8d1d68619b2a",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@rhvgoyal: What would a `stat -f /` inside the container give me? Will it be usage of the writable layer or virtual size?\n",
        "createdAt" : "2015-11-10T19:45:37Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "5c9c8e01-cae9-4fc0-b076-ed70b3512d9d",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "@vishh this will give you the file system usage of container rootfs. That would include the size of all underlying images + container writable layer.\n",
        "createdAt" : "2015-11-12T20:17:26Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "526232ee-2040-41e8-a6eb-c71b33036e0f",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Hmm. So if multiple container's use `ubuntu` for example, each and every one of them will show up as using the image layers independently.\nThis will be an issue. We need absolute limits and usage, not virtual ones. The system has to partition the available disk storage among all the pods on the node.\nIs there some means to extract the usage of just the `writable` layer @rhvgoyal ?\n",
        "createdAt" : "2015-11-12T20:26:36Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "bf863c95-6442-4642-9840-c103ee118492",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "5328b1c0-0dbd-4fd8-869d-e914880959c2",
        "body" : "We could subtract the image usage from the container usage. @rhvgoyal That should be possible, right?\n",
        "createdAt" : "2015-11-12T20:29:33Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5328b1c0-0dbd-4fd8-869d-e914880959c2",
        "tags" : [
        ]
      },
      {
        "id" : "5d98afed-3f20-4f2b-8e68-3cea00c837d1",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "@mrunalp you could try subtracting image size from container size but there are corner cases which don't fit well. For example, what if some container deletes some of the files. Then usage will go negative. Or what if some container deletes some files and replaces with its own data. Then total usage will seem like zero but in thin pool we have consumed lot of space. \n\nSo once I had suggested this idea to @smarterclayton internally but then thought that corner cases will make this method problematic.\n",
        "createdAt" : "2015-11-12T20:35:50Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "ab229bb5-ae7e-4be7-954e-3f8a5f596bb7",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@rhvgoyal: In Kubernetes, container's do not share read-write layers. How can a container deletion affect another container? Can you elaborate a bit more?\n",
        "createdAt" : "2015-11-12T20:44:16Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "8724cdd6-f142-4a3e-9168-6c91be50bdfa",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "@vishh I was thinking of following scenario. Say a container is started with image 'foo' which container a file 'bar' of say 1G. container now overwrites this\nfile 'bar' with 1G of data, then COW will happen in thin pool, and fresh 1G of blocks will be allocated. But if we do 'stat -f /' then container total usage will not vary and subtracting image size will show size of writable layer as 0.\n\nIOW, container wrote 1G of data but still we calculated writable layer size as 0. And that's what I was trying to point to if we go with the method of subtracting image size from container size.\n",
        "createdAt" : "2015-11-12T21:36:03Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "86437aae-b7d6-46be-851e-d3267cc9553c",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Ah! Got it. Is it possible to get the usage of just the write layer? Isn't\nthe writable layer backed by a dedicated volume?\n\nOn Thu, Nov 12, 2015 at 1:36 PM, Vivek Goyal notifications@github.com\nwrote:\n\n> In docs/proposals/disk-accounting.md\n> https://github.com/kubernetes/kubernetes/pull/16889#discussion_r44718149\n> :\n> \n> > +\n> > +\\* Requires serialized image pulls.\n> > +\n> > +\\* Metadata files are not tracked.\n> > +\n> > +\n> > +##### Devicemapper\n> > +\n> > +Devicemapper storage driver will setup two volumes, metadata and data, that will be used to store image layers and container writable layer. The volumes can be real devices or loopback. A Pool device is created which uses the underlying volume for real storage.\n> > +\n> > +A new thinly-provisioned volume, based on the pool, will be created for running container’s.\n> > +\n> > +The kernel tracks the usage of the pool device at the block device layer. The usage here includes image layers and container’s writable layers.\n> > +\n> > +Since the kubelet has to track the writable layer usage anyways, we can subtract the aggregated root filesystem usage from the overall pool device usage to get the image layer’s disk usage.\n> > +\n> \n> @vishh https://github.com/vishh I was thinking of following scenario.\n> Say a container is started with image 'foo' which container a file 'bar' of\n> say 1G. container now overwrites this\n> file 'bar' with 1G of data, then COW will happen in thin pool, and fresh\n> 1G of blocks will be allocated. But if we do 'stat -f /' then container\n> total usage will not vary and subtracting image size will show size of\n> writable layer as 0.\n> \n> IOW, container wrote 1G of data but still we calculated writable layer\n> size as 0. And that's what I was trying to point to if we go with the\n> method of subtracting image size from container size.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/16889/files#r44718149.\n",
        "createdAt" : "2015-11-12T21:52:18Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "c1bb5941-b3de-462e-93ba-5e2d2c5a171b",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "In case of devicemapper I can't think of any easy way to get the size of just writable layer. Yes every container rootfs is backed by a separate volume to begin with that volume is a snapshot of image volume. That volume is not empty. So you can't figure out easily how many blocks of volume are shared and how many are not shared.\n",
        "createdAt" : "2015-11-12T21:56:40Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "21d78e5b-fd50-47ef-857e-a544bd200021",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Hmm. This means that in the case of devicemapper, Kubernetes cannot provide accounting for the writable layer. This will be accounted as infrastructure cost. \nSince the rootfs usage includes the image layers, users will have to set higher limits for disks in the case of devicemapper, even-though the image layers are shared. This will affect portability of pod configurations.\n",
        "createdAt" : "2015-11-13T00:49:28Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "7324060f-5904-4c66-a24b-8be5b9ce80bb",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Can we enforce a model in case of devicemapper, the root filesystem is readonly? We track volume and logging separately anyway. \n",
        "createdAt" : "2015-12-16T23:02:43Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "3de41b0c-24f5-4803-86b1-a6eb2e53d346",
        "parentId" : "112b6852-612f-45ff-aee6-9df33e2d998d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Once this proposal is merged, @rhvgoyal has volunteered to fix the device mapper bits in this doc.\n",
        "createdAt" : "2015-12-16T23:23:37Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 275,
    "diffHunk" : "@@ -1,1 +273,277 @@\nSince the kubelet has to track the writable layer usage anyways, we can subtract the aggregated root filesystem usage from the overall pool device usage to get the image layer’s disk usage.\n\nLinux quota and `du` will not work with device mapper.\n"
  },
  {
    "id" : "d6c16faa-e8ae-48b7-9dee-5ffcad8f73f8",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "944cfda0-7fc1-41c0-910d-0d959e2fc445",
        "parentId" : null,
        "authorId" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "body" : "As container rootfs is a block device, container size is limited by size of block device. Right now docker default is 100G per container but this is tunable with a docker option dm.basesize=X during first time start of docker. So that should allow you to come up with a common size limit per container.\n\nWe have been thinking of implementing something where one could specify bigger size for a freshly created container. So say default is 10G then one could say that create a container with size 50G. But this is all theory right now. Once patches are implemented, we will know if upstream likes these or not.\n\nThe article you have linked is all the magic behind dokcer's back. I would rather propose proper API/functionality in docker so that one could do what's needed instead of going behind docker's back and start playing with its metadata files or thin pool  or thin devices etc.\n",
        "createdAt" : "2015-11-10T16:07:00Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "5303e6e3-a559-4d64-a083-6bb654b7a6a2",
        "tags" : [
        ]
      },
      {
        "id" : "fda1d589-a25a-43e9-bbec-86bd29efa791",
        "parentId" : "944cfda0-7fc1-41c0-910d-0d959e2fc445",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I would prefer a major portion of this proposal to be implemented in docker.  The reality is that users need accounting today and this proposal tries to provide a solution for that. Once docker provides better primitives, we can avoid reverse engineering docker.\n",
        "createdAt" : "2015-11-10T21:39:33Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +299,303 @@\nTo enforce `limits` the volume created for the container’s writable layer filesystem can be dynamically [resized](https://jpetazzo.github.io/2014/01/29/docker-device-mapper-resize/), to not use more than `limit`. `request` will have to be enforced by the kubelet.\n\n\n#### Container logs"
  },
  {
    "id" : "98759597-efa8-4a8a-b2e5-940b8144878e",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0547aa05-1712-48d0-9cc3-309d51b4a00f",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I prefer eviction when availability drops below a threshold.\n\nPer the QoS tiers, is disk request/limit now required to get Burstable/Guaranteed qos tiers?  Or will adoption of this feature and past pods that did not make a request become BestEffort?\n",
        "createdAt" : "2015-11-12T23:00:46Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "62ce7313-4d49-49ab-a06d-2acab2e59904",
        "parentId" : "0547aa05-1712-48d0-9cc3-309d51b4a00f",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "`Disk` is not a schedulable resource as of now. I don't expect that to change until we have solid accounting. Is there a need for disk scheduling from users now? Is this something that is affecting cluster reliability?\n",
        "createdAt" : "2015-11-13T00:57:53Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "08908c5e-a1f6-4c1b-8de0-e5da6218b5d1",
        "parentId" : "0547aa05-1712-48d0-9cc3-309d51b4a00f",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Eviction to ensure node stability SGTM. Eviction policy is beyond the scope of this document. \n",
        "createdAt" : "2015-12-09T23:51:53Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 294,
    "diffHunk" : "@@ -1,1 +292,296 @@\nIf local disk becomes a schedulable resource, `linux quota` can be used to impose `request` and `limits` on the container writable layer.\n`limits` can be enforced using hard limits. Enforcing `request` will be tricky. One option is to enforce `requests` only when the disk availability drops below a threshold (10%). Kubelet can at this point evict pods that are exceeding their requested space. Other options include using `soft limits` with grace periods, but this option is complex.\n\n###### Devicemapper"
  },
  {
    "id" : "0c0e75ba-d0de-4d3c-a4f6-bb056af0030d",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed59e3d6-aaac-47ec-99f2-76ee1b1e5b5b",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "make clear that what you described below is 1.1 release. \n",
        "createdAt" : "2015-11-16T22:40:32Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "b7295cc9-c417-4215-855a-73cb08e7dc80",
        "parentId" : "ed59e3d6-aaac-47ec-99f2-76ee1b1e5b5b",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Done\n",
        "createdAt" : "2015-12-09T23:53:35Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +36,40 @@This proposal is an attempt to come up with a means for accounting disk usage in Kubernetes clusters that are running docker as the container runtime. Some of the principles here might apply for other runtimes too.\n\n### Why is disk accounting necessary?\n\nAs of kubernetes v1.1 clusters become unusable over time due to the local disk becoming full. The kubelets on the node attempt to perform garbage collection of old containers and images, but that doesn’t prevent running pods from using up all the available disk space."
  },
  {
    "id" : "330ef7bb-95bc-4466-a852-801d8c092b53",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39f6bb5e-a0da-4c54-8f9e-c04ad8cab607",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "docker history <imagename> show each layer's size, right? \n\n```\n# docker history mysql\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n51aa388e5cbe        6 days ago          /bin/sh -c #(nop) CMD [\"mysqld\"]                0 B                 \ne9e2648c8f5f        6 days ago          /bin/sh -c #(nop) EXPOSE 3306/tcp               0 B                 \n7b07ecfaec34        6 days ago          /bin/sh -c #(nop) ENTRYPOINT &{[\"/entrypoint.   0 B                 \ne0bd003f9986        6 days ago          /bin/sh -c #(nop) COPY file:174697bdc9dfe5f1b   2.667 kB            \nfa882ed5d445        6 days ago          /bin/sh -c #(nop) VOLUME [/var/lib/mysql]       0 B                 \n4a1423420bbd        6 days ago          /bin/sh -c sed -Ei 's/^(bind-address|log)/#&/   1.771 kB            \na6c42d36b3d9        6 days ago          /bin/sh -c {                                                        echo mysql-community-server my   201.5 MB            \n3c9993ae1919        6 days ago          /bin/sh -c echo \"deb http://repo.mysql.com/ap   55 B                \n523d9e7d5efc        6 days ago          /bin/sh -c #(nop) ENV MYSQL_VERSION=5.7.9-1de   0 B                 \na019b92678d9        6 days ago          /bin/sh -c #(nop) ENV MYSQL_MAJOR=5.7           0 B                 \n7b27836f1fd6        6 days ago          /bin/sh -c apt-key adv --keyserver ha.pool.sk   20.52 kB            \n33dcf29c1ddf        6 days ago          /bin/sh -c apt-get update && apt-get install    32.83 MB            \n45534ee3011a        6 days ago          /bin/sh -c mkdir /docker-entrypoint-initdb.d    0 B                 \nf45838e43804        6 days ago          /bin/sh -c groupadd -r mysql && useradd -r -g   330.4 kB            \n1d6f63d023f5        7 days ago          /bin/sh -c #(nop) CMD [\"/bin/bash\"]             0 B                 \nef2704e74ecc        7 days ago          /bin/sh -c #(nop) ADD file:3037fa9e903e9ae533   125.1 MB            \n```\n",
        "createdAt" : "2015-11-17T00:59:32Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "0132ff7a-b916-426d-8730-d9e0e25c380a",
        "parentId" : "39f6bb5e-a0da-4c54-8f9e-c04ad8cab607",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Yup. Added a section on this. Thanks for the tip @dchen1107 \n",
        "createdAt" : "2015-12-10T00:20:21Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +127,131 @@Container images and writable layers are managed by docker and their location will change depending on the storage driver. Each image layer and writable layer is referred to by an ID. The image layers are read-only. Once saved, existing writable layers can be frozen. Saving feature is not of importance to kubernetes since it works only on immutable images.\n\n*Note: Image layer IDs can be obtained by running `docker history -q --no-trunc <imagename>`*\n\n##### Aufs"
  },
  {
    "id" : "0ed73b46-d58f-4051-9863-37b8658b84b0",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4850914-41c2-4d2a-98fa-f367aded7a68",
        "parentId" : null,
        "authorId" : "95b7051f-0751-4088-8141-bc45a44ac5ca",
        "body" : "s/let's/let\n",
        "createdAt" : "2015-11-20T02:48:01Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "95b7051f-0751-4088-8141-bc45a44ac5ca",
        "tags" : [
        ]
      },
      {
        "id" : "41fcf8ca-c65a-491b-a128-accd68cc0354",
        "parentId" : "d4850914-41c2-4d2a-98fa-f367aded7a68",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Fixed\n",
        "createdAt" : "2015-12-10T00:22:25Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +233,237 @@\n[Disk quota](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Storage_Administration_Guide/ch-disk-quotas.html) feature provided by the linux kernel can be used to track the usage of image layers. Ideally, we need `project` support for disk quota, which lets us track usage of directory hierarchies using `project ids`. Unfortunately, that feature is only available for zfs filesystems. Since most of our distributions use `ext4` by default, we will have to use either `uid` or `gid` based quota tracking.\n\nBoth `uids` and `gids` are meant for security. Overloading that concept for disk tracking is painful and ugly. But, that is what we have today.\n"
  },
  {
    "id" : "1324e6ae-cc0e-413e-bc83-015f631f532e",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d566ff97-b695-4c59-820f-256ca31ec567",
        "parentId" : null,
        "authorId" : "95b7051f-0751-4088-8141-bc45a44ac5ca",
        "body" : "dup `implementation`?\n",
        "createdAt" : "2015-11-20T02:53:16Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "95b7051f-0751-4088-8141-bc45a44ac5ca",
        "tags" : [
        ]
      },
      {
        "id" : "2e90eeba-ee3c-4cc4-9efc-0c0f0ffe239a",
        "parentId" : "d566ff97-b695-4c59-820f-256ca31ec567",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Removed the dup. Thanks @dalanlan \n",
        "createdAt" : "2015-12-10T00:22:45Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +250,254 @@\n* Requires updates to default ownership on docker’s internal storage driver directories. We will have to deal with storage driver implementation details in any approach that is not docker native.\n\n* Requires additional node configuration - quota subsystem needs to be setup on the node. This can either be automated or made a requirement for the node.\n"
  },
  {
    "id" : "bed9d8eb-012c-4d2f-80e9-ad019e09a634",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bc0ac4a-6eff-4adb-a564-6d0bf66c365a",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "This should be ok since the volumes are pod level resources right?\n",
        "createdAt" : "2016-01-08T21:54:16Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "6585a9ab-cb33-43eb-a139-6a22f04c22d6",
        "parentId" : "8bc0ac4a-6eff-4adb-a564-6d0bf66c365a",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Yeah. Limits will be per-volume.\n",
        "createdAt" : "2016-01-08T23:03:32Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 318,
    "diffHunk" : "@@ -1,1 +316,320 @@The local disk based volumes map to a directory on the disk. We can use `du` or `quota` to track the usage of volumes.\n\nThere exists a concept called `FsGroup` today in kubernetes, which lets users specify a gid for all volumes in a pod. If that is set, we can use the `FsGroup` gid for quota purposes. This requires `limits` for volumes to be a pod level resource though.\n\n"
  },
  {
    "id" : "199def7e-6053-48f6-94e7-793fc6d7c74e",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03df54b0-8d9c-4f0e-956b-01fac4bd7ed3",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Should we add something about garbage collecting uid's and gid's?\n",
        "createdAt" : "2016-01-08T22:01:45Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "b5342364-7ef0-43be-8b0c-5e2ea7889e60",
        "parentId" : "03df54b0-8d9c-4f0e-956b-01fac4bd7ed3",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I'm hoping we'd avoid this approach in favor of \"project IDs\". \n",
        "createdAt" : "2016-01-08T23:04:05Z",
        "updatedAt" : "2016-01-08T23:08:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +377,381 @@\n* Update disk manager in kubelet to use quota when configured.\n\n\n#### Milestone 4 - Users manage local disks"
  },
  {
    "id" : "3f76ef88-d039-4343-a062-c2eccbdca694",
    "prId" : 16889,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b98b44b-3fc6-4bca-9c5b-36c66441e3ba",
        "parentId" : null,
        "authorId" : "7db9cb1e-f5f2-40b7-8976-c2f85c0a6288",
        "body" : "s/titbits/tidbits\n",
        "createdAt" : "2016-02-15T05:40:28Z",
        "updatedAt" : "2016-02-15T05:40:28Z",
        "lastEditedBy" : "7db9cb1e-f5f2-40b7-8976-c2f85c0a6288",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b430379d86ffb6230f85138b29cd8a9d8c22fd0",
    "line" : 632,
    "diffHunk" : "@@ -1,1 +630,634 @@    * `chown -R :9000 /var/lib/docker/**container**/b8cc9fae3851f9bcefe922952b7bca0eb33aa31e68e9203ce0639fc9d3f3c61b/*`\n\n##### Testing titbits\n\n* Ubuntu 15.10 doesn’t ship with the quota module on virtual machines. [Install ‘linux-image-extra-virtual’](http://askubuntu.com/questions/109585/quota-format-not-supported-in-kernel) package to get quota to work."
  }
]