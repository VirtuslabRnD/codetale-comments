[
  {
    "id" : "86a8de5b-204c-4d46-944b-bd60aa232913",
    "prId" : 12530,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0ab8ab8-ef27-4498-ae44-23bf821a3f7e",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Are you going to say in this doc which approach you'll use?\n",
        "createdAt" : "2015-09-04T10:45:42Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "fd21f8cd-a97c-4f42-b874-8b46ffaf9750",
        "parentId" : "c0ab8ab8-ef27-4498-ae44-23bf821a3f7e",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "It's hard to say, before playing with network setup - probably both.\n",
        "createdAt" : "2015-09-04T13:47:59Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca30de0bde294bf60f22ce51b366e0e5a291c112",
    "line" : null,
    "diffHunk" : "@@ -1,1 +84,88 @@### Option 2\n\nAs a second (equivalent) option we will run Kubemark on top of 'real' Kubernetes cluster, where both Master and Hollow Nodes will be Pods.\nIn this option we'll be able to use Kubernetes mechanisms to streamline setup, e.g. by using Kubernetes networking to ensure unique IPs for\nHollow Nodes, or using Secrets to distribute Kubelet credentials. The downside of this configuration is that it's likely that some noise"
  },
  {
    "id" : "3a7b9db8-7977-492f-bfd9-80d6b1bc81af",
    "prId" : 12530,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "849b73fa-d651-4a44-a3f7-f6a495764339",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Add: for option #2, figure out how to get Hollow nodes and master components of the Hollow cluster to run on Kuberenetes.\n",
        "createdAt" : "2015-09-04T10:45:51Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "d9bf8d53-df09-41e3-83c1-ccee57a4fd56",
        "parentId" : "849b73fa-d651-4a44-a3f7-f6a495764339",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2015-09-04T14:09:57Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca30de0bde294bf60f22ce51b366e0e5a291c112",
    "line" : null,
    "diffHunk" : "@@ -1,1 +145,149 @@very well isolated from the rest, as it is about sending requests to the real API server. Because of that we can discuss requirements\nseparately.\n\n## Concerns\n"
  },
  {
    "id" : "d871feb1-5c25-4df9-bbfe-7fa8005d08b0",
    "prId" : 12530,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99b90199-a262-4a06-8459-0700d170b3c4",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "It's a little odd that you have separate sections for \"Necessary work\" and \"Work plan\" but it's not a big deal.\n",
        "createdAt" : "2015-09-04T10:45:53Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "2611e9df-0908-45d0-9a40-04315c778cff",
        "parentId" : "99b90199-a262-4a06-8459-0700d170b3c4",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "It made more sense when there was no \"Concerns\" in between:/ I don't have a good idea on what to do with those.\n",
        "createdAt" : "2015-09-04T14:11:02Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca30de0bde294bf60f22ce51b366e0e5a291c112",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +161,165 @@don't need to solve this problem now.\n\n## Work plan\n\n- Teach/make sure that Master can talk to multiple Kubelets on the same Machine [option 1](#option-1):"
  },
  {
    "id" : "813ed76c-e9b3-4bb7-ba1e-cbd739e18ea2",
    "prId" : 12530,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a116499a-d93c-42f6-b424-2ba7f432235b",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "For option #2, need to get the Hollow nodes and master components of the Hollow cluster to run on Kuberenetes.\n",
        "createdAt" : "2015-09-04T10:45:57Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "f4bfe53c-ded1-4114-b2f7-a2b71c3c45fa",
        "parentId" : "a116499a-d93c-42f6-b424-2ba7f432235b",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2015-09-04T14:12:38Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca30de0bde294bf60f22ce51b366e0e5a291c112",
    "line" : null,
    "diffHunk" : "@@ -1,1 +172,176 @@  - make sure that Master can talk to two HollowKubelets running on the same machine\n- Make sure that we can run Hollow cluster on top of Kubernetes [option 2](#option-2)\n- Write a player that will automatically put some predefined load on Master, <- this is the moment when itâ€™s possible to play with it and is useful by itself for\nscalability tests. Alternatively we can just use current density/load tests,\n- Benchmark our machines - see how many Watch clients we can have before everything explodes,"
  },
  {
    "id" : "3ce8a78a-5597-44fb-aa6d-c4052c4f327d",
    "prId" : 12530,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c94b2c55-4550-425a-b62e-d5c1def07567",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Later on you say \"we don't think that running Kubemark on Kubernetes virtualized cluster networking will cause noticable performance impact\"\n",
        "createdAt" : "2015-09-07T20:21:44Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "21fd6aa0-924d-48b9-9134-777ca2fb22f4",
        "parentId" : "c94b2c55-4550-425a-b62e-d5c1def07567",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yes, but even in that sentence I have 'but it requires testing'. I.e. I don't think it'll cause problems, but I won't bet $10 on it.\n",
        "createdAt" : "2015-09-08T07:32:01Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca30de0bde294bf60f22ce51b366e0e5a291c112",
    "line" : null,
    "diffHunk" : "@@ -1,1 +89,93 @@will appear in Kubemark results from either CPU/Memory pressure from other things running on Nodes (e.g. FluentD, or Kubelet) or running\ncluster over an overlay network. We believe that it'll be possible to turn off cluster monitoring for Kubemark runs, so that the impact\nof real Node daemons will be minimized, but we don't know what will be the impact of using higher level networking stack. Running a\ncomparison will be an interesting test in itself.\n"
  },
  {
    "id" : "25cd4272-5da7-4615-9f33-168398df0c84",
    "prId" : 12530,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6837e39c-7133-47ed-a661-4629f7ec64a4",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "HollowKubelets\n",
        "createdAt" : "2015-09-07T20:21:57Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "f1343892-3a4b-4884-b929-b72938ee1aad",
        "parentId" : "6837e39c-7133-47ed-a661-4629f7ec64a4",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I might be mistaken, but I think the meaning is: normally Heapster scrapes cAdvisor data from Kubelets (ordinary ones), we mock this functionality.\n",
        "createdAt" : "2015-09-08T07:38:51Z",
        "updatedAt" : "2015-09-10T07:54:52Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca30de0bde294bf60f22ce51b366e0e5a291c112",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +124,128 @@In addition to system components we run Heapster as a part of cluster monitoring setup. Heapster currently watches Events, Pods and Nodes\nthrough the API server. In the test setup we can use real heapster for watching API server, with mocked out piece that scrapes cAdvisor\ndata from Kubelets.\n\n### Elasticsearch and Fluentd"
  }
]