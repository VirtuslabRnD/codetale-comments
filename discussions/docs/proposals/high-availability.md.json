[
  {
    "id" : "476ab553-2ae8-467a-9562-cde77dec754e",
    "prId" : 6995,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fe7506a-832e-41a5-aa40-3f4eebc4848d",
        "parentId" : null,
        "authorId" : null,
        "body" : "I would strongly discourage designing a leader election algorithm from scratch here.  There are many devils in the details, and much prior art.  A good starting point would be to stand on the shoulders of etcd and zookeeper, e.g.:\n\nhttp://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection\nhttps://groups.google.com/forum/#!topic/etcd-dev/EbAa4fjypb4\n",
        "createdAt" : "2015-04-17T19:32:30Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "4e861235-217a-49b1-80b9-1fb8556b9ad9",
        "parentId" : "9fe7506a-832e-41a5-aa40-3f4eebc4848d",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Totally agree, we were intending to follow well established patterns.  The example only serves, as well.. an example. \n",
        "createdAt" : "2015-04-17T20:47:48Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "5ef8534b-173e-4e86-9530-d895699e30c8",
        "parentId" : "9fe7506a-832e-41a5-aa40-3f4eebc4848d",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "@wojtek-t wrote one for Omega and would probably be good to consult with.\n",
        "createdAt" : "2015-04-17T23:50:01Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "0beb72729e39b490396e7cdabe0639c2bf1ee69a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +11,15 @@3. Active-Active (Load Balanced): Clients can simply load-balance across any number of servers that are currently running.  Their general availability can be continuously updated, or published, such that load balancing only occurs across active participants.  This aspect of HA is outside of the scope of *this* proposal because there is already a partial implementation in the apiserver.\n\n## Design Discussion Notes on Leader Election\nImplementation References:\n* [zookeeper](http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection)"
  },
  {
    "id" : "97d6d007-c2ed-468a-91e7-1627b613ba77",
    "prId" : 6995,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db6cf8d0-393c-4c03-9c41-895d4bf3b27b",
        "parentId" : null,
        "authorId" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "body" : "Since we are now proposing a generic HA master election API (open to users and not restricted to infrastructure components), I believe we need to ensure that an unrelated client/component cannot take over the lease when the elected master fails. One way to go about this is to use namespaces for writing these keys. Then the administrator can write its keys for the infrastructure components into its own namespace (default ?) and clients can do so in their own namespaces.\n\nVery crudely, something on the lines of (don't remember what our current pattern is):\n\n```\n.../<namespace>/leader-keys/<component-type1>\n.../<namespace>/leader-keys/<component-type2>\n```\n\nThis still assumes that multiple \"types\" of components within a namespace have to play nicely with each other, but that is still a much more manageable problem.\n",
        "createdAt" : "2015-04-22T21:18:58Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "tags" : [
        ]
      },
      {
        "id" : "8aa862a0-3737-4459-9f5c-abab43a109c6",
        "parentId" : "db6cf8d0-393c-4c03-9c41-895d4bf3b27b",
        "authorId" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "body" : "We will look at implementing a namespace in the api calls, but that is an implementation detail so will be left of out the proposal.\n",
        "createdAt" : "2015-04-24T18:52:55Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "aee8926e-0646-4183-b0d7-65633cf782b0",
        "tags" : [
        ]
      },
      {
        "id" : "d9a3259b-96f7-4b79-9be6-b3228c2df265",
        "parentId" : "db6cf8d0-393c-4c03-9c41-895d4bf3b27b",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "If you're feeling particularly pedantic, you could remove all references to \"leader\" and \"master\" and just describe this in terms of acquire/renew/release the lease. (This relates to Brian's comment \"I'd think about the API as a lock/lease mechanism rather than necessarily just for master election.\")\n",
        "createdAt" : "2015-04-26T06:05:09Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "0cbcf137-480b-4770-81cd-b0b4300e3b48",
        "parentId" : "db6cf8d0-393c-4c03-9c41-895d4bf3b27b",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Actually never mind, I think it's useful to describe how you use the mechanism to perform leader election. So I wouldn't change what you already have.\n",
        "createdAt" : "2015-04-26T06:11:14Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "0beb72729e39b490396e7cdabe0639c2bf1ee69a",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +17,21 @@* [initialPOC](https://github.com/rrati/etcd-ha)\n\nIn HA, the apiserver will provide an api for sets of replicated clients to do master election: acquire the lease, renew the lease, and release the lease.  This api is component agnostic, so a client will need to provide the component type and the lease duration when attemping to become master.  The lease duration should be tuned per component.  The apiserver will attempt to create a key in etcd based on the component type that contains the client's hostname/ip and port information. This key will be created with a ttl from the lease duration provided in the request.  Failure to create this key means there is already a master of that component type, and the error from etcd will propigate to the client.  Successfully creating the key means the client making the request is the master.  Only the current master can renew the lease.  When renewing the lease, the apiserver will update the existing key with a new ttl.  The location in etcd for the HA keys is TBD.\n\nThe first component to request leadership will become the master.  All other components of that type will fail until the current leader releases the lease, or fails to renew the lease within the expiration time.  On startup, all components should attempt to become master.  The component that succeeds becomes the master, and should perform all functions of that component.  The components that fail to become the master should not perform any tasks and sleep for their lease duration and then attempt to become the master again. A clean shutdown of the leader will cause a release of the lease and a new master will be elected."
  },
  {
    "id" : "1dd6bc4c-1780-44dc-9199-f1b944b853f0",
    "prId" : 6995,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb925be6-b4c6-4a46-9d19-e8a192549628",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "You didn't really say why you're not choosing this approach. For something like a scheduler it would probably be easy to use this approach (each of N simultaneously-running scheduler replicas is responsible for scheduling every Nth pod, or they compete to be the first to schedule each pod, or whatever). The scheduler case might be more complicated; I haven't really thought about it. But I don't think \"there is already a partial implementation in the apiserver\" is the reason why you're not advocating this approach.\n",
        "createdAt" : "2015-04-26T06:00:34Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "6e36bdcf-7cc1-48b1-a4f8-86f62642856d",
        "parentId" : "cb925be6-b4c6-4a46-9d19-e8a192549628",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Sorry I mean \"the replication controller case might be more complicated\"\n",
        "createdAt" : "2015-04-26T06:03:14Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "b7d7cdbe-0378-4c30-8756-b7de5ef9c54c",
        "parentId" : "cb925be6-b4c6-4a46-9d19-e8a192549628",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "> On Apr 26, 2015, at 2:00 AM, David Oppenheimer notifications@github.com wrote:\n> \n> In docs/proposals/high-availability.md:\n> \n> > @@ -0,0 +1,40 @@\n> > +# High Availability of Scheduling and Controller Components in Kubernetes\n> > +This document serves as a proposal for high availability of the scheduler and controller components in kubernetes.  This proposal is intended to provide a simple High Availability api for kubernetes components with the potential to extend to services running on kubernetes.  Those services would be subject to their own constraints.\n> > +\n> > +## Design Options\n> > +For complete reference see [this](https://www.ibm.com/developerworks/community/blogs/RohitShetty/entry/high_availability_cold_warm_hot?lang=en)\n> > +\n> > +1. Hot Standby: In this scenario, data and state are shared between the two components such that an immediate failure in one component causes the the standby deamon to take over exactly where the failed component had left off.  This would be an ideal solution for kubernetes, however it poses a series of challenges in the case of controllers where component-state is cached locally and not persisted in a transactional way to a storage facility.  This would also introduce additional load on the apiserver, which is not desireable.  As a result, we are **NOT** planning on this approach at this time. \n> > +\n> > +2. **Warm Standby**: In this scenario there is only one active component acting as the master and additional components running but not providing service or responding to requests.  Data and state are not shared between the active and standby components.  When a failure occurs, the standby component that becomes the master must determine the current state of the system before resuming functionality.  This is the apprach that this proposal will leverage.\n> > +\n> > +3. Active-Active (Load Balanced): Clients can simply load-balance across any number of servers that are currently running.  Their general availability can be continuously updated, or published, such that load balancing only occurs across active participants.  This aspect of HA is outside of the scope of _this_ proposal because there is already a partial implementation in the apiserver.\n> > You didn't really say why you're not choosing this approach. For something like a scheduler it would probably be easy to use this approach (each of N simultaneously-running scheduler replicas is responsible for scheduling every Nth pod, or they compete to be the first to schedule each pod, or whatever).\n> \n> For truly level driven data flows the competitive model (with optimization to reduce contention) seems like the long term ideal.  I don't think any of our scheduler decisions should be perfect (just not bad), and done right at volume a few bad scheduling decisions based on inadequate info is no different than a flaky machine or bad network condition.\n> The scheduler case might be more complicated; I haven't really thought about it. But I don't think \"there is already a partial implementation in the apiserver\" is the reason why you're not advocating this approach.\n> \n> â€”\n> Reply to this email directly or view it on GitHub.\n",
        "createdAt" : "2015-04-26T15:45:04Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "93de4763-2181-47e1-8c92-d2220c3acad3",
        "parentId" : "cb925be6-b4c6-4a46-9d19-e8a192549628",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yes, you could run multiple copies of our current scheduler and be fine-- there will probably be a few double-scheduling attempts (esp when there's no queue of pods to schedule) but those are handled correctly by the rest of the system already.\n",
        "createdAt" : "2015-04-27T18:27:46Z",
        "updatedAt" : "2015-05-01T14:03:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "0beb72729e39b490396e7cdabe0639c2bf1ee69a",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +9,13 @@2. **Warm Standby**: In this scenario there is only one active component acting as the master and additional components running but not providing service or responding to requests.  Data and state are not shared between the active and standby components.  When a failure occurs, the standby component that becomes the master must determine the current state of the system before resuming functionality.  This is the apprach that this proposal will leverage.\n\n3. Active-Active (Load Balanced): Clients can simply load-balance across any number of servers that are currently running.  Their general availability can be continuously updated, or published, such that load balancing only occurs across active participants.  This aspect of HA is outside of the scope of *this* proposal because there is already a partial implementation in the apiserver.\n\n## Design Discussion Notes on Leader Election"
  }
]