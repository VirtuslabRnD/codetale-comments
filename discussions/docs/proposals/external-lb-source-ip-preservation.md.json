[
  {
    "id" : "f6eb4eca-89cd-4a47-8029-4ffe5ffd68fc",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69ed6418-f8c8-4f35-9168-9673aac07964",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Additionally, for NumServicePods <= NumNodes, the pods can use a HostPort to force a maximum of one replica per node.\n",
        "createdAt" : "2016-08-05T05:28:40Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "b360237f-67df-4a84-aeb2-ab9b6510fec6",
        "parentId" : "69ed6418-f8c8-4f35-9168-9673aac07964",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "I will add this comment.\nA single user may coordinate their HostPorts, but harder to coordinate across multiple users, so hostPorts have multi-tenant usability issues though.\n",
        "createdAt" : "2016-08-05T17:33:26Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "b4c0528d-1d46-4f62-a7ca-8289e1f149f9",
        "parentId" : "69ed6418-f8c8-4f35-9168-9673aac07964",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "We do have selector spreading that means we are unlikely to concentrate too many pods on a node (though there are scenarios of course, as it is just a weighting)\n",
        "createdAt" : "2016-08-20T22:00:41Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 178,
    "diffHunk" : "@@ -1,1 +176,180 @@\nWe can, however, state that for NumServicePods << NumNodes or NumServicePods >> NumNodes, a fairly close-to-equal\ndistribution will be seen, even without weights.\n\nOnce the external load balancers provide weights, this functionality can be added to the LB programming path."
  },
  {
    "id" : "d8175e6a-61fa-42a0-ac18-bd3861e866f2",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bb9e6dc-f82e-4b16-9e0f-bce7f4802a4f",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "I'm pretty worried about this, there are a bunch of single threaded frontend apps (node.js I'm looking at you) where it makes sense to deploy many pods per node.\n\nwe're going to create some hard to debug situations where one node gets one pod and another node gets three pods and the single pod node is 3x slower than the 3 pod node.\n\nAt the very least we need to add lots of warnings about this in the docs.\n",
        "createdAt" : "2016-08-05T15:33:49Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "72fe2574-e100-425b-9918-20ded36f91f5",
        "parentId" : "4bb9e6dc-f82e-4b16-9e0f-bce7f4802a4f",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "This is optional for a reason.  Without weights we can only trade one flaw for another.  You can either have client IP or balance, but not both.  Hopefully we'll see evolution of the LBs over time.\n",
        "createdAt" : "2016-08-05T16:23:33Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "161038b2-8bf1-467a-9318-d552f362ac8f",
        "parentId" : "4bb9e6dc-f82e-4b16-9e0f-bce7f4802a4f",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Maybe we want to bump up the spreading priority for pods behind loadbalanced services. \nHonestly anything on Kube behind a cloud lb is already pretty hard to debug. \n",
        "createdAt" : "2016-08-05T16:44:45Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "ac302ce2-a5b8-47ba-86d3-5515f35db2ee",
        "parentId" : "4bb9e6dc-f82e-4b16-9e0f-bce7f4802a4f",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "I guess my meta-point is I'm going to see weird CPU behavior in my monitoring, and unless I'm paying _super_ close attention to the details of how LB works, I'm going to be confused.\n\nNot saying we shouldn't do this, I think it's good for users to have the option, I just think we need some facility for warnings in contexts like these.\n",
        "createdAt" : "2016-08-07T03:37:43Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "ed88d158-ec43-4a94-8638-27371d28dabe",
        "parentId" : "4bb9e6dc-f82e-4b16-9e0f-bce7f4802a4f",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "I think I'm missing something. If we're doing DNAT (but not SNAT) at the LBs, why _can't_ we have a nodePort per pod and hence loadbalance across dissimilar nodes correctly?\n\nIf we're _not_ doing DNAT at the LBs, then I feel like a lot of the rest of this doc needs further explanation, because we haven't talked about the node requirements for L2 or L3 forwarding anywhere.  Overall \"life of a packet\" description needed somewhere above...\n\n(much later edit: ack, I see from comments elsewhere that we're assuming no NAT at all (specifically no DNAT) with this solution.  Mentioning that in the doc would be good...)\n",
        "createdAt" : "2016-08-08T03:38:16Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "5802cfc0-6e9e-4ff6-978a-26e653f093b3",
        "parentId" : "4bb9e6dc-f82e-4b16-9e0f-bce7f4802a4f",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "@anguslees This would require allocation of nodePorts per pod for a Service, and even then, I am not sure if the LB could be programmed with more than one entry per destination VM. I know GCCE doesn't allow this, and I _think_ AWS behaves the same.\n\nBTW, The GCE LB does not perform any DNAT, rather the packets arrive with their destination IP == VIP.\n",
        "createdAt" : "2016-08-09T02:31:02Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "9d814068-35dd-4647-939e-3d02f425cafb",
        "parentId" : "4bb9e6dc-f82e-4b16-9e0f-bce7f4802a4f",
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "AWS supports setting up multiple listeners that hit different NodePorts, but their traffic needs to hit different ports on the load balancer's external IP, so, no, it's not going to work.\n",
        "createdAt" : "2016-08-09T18:14:57Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 172,
    "diffHunk" : "@@ -1,1 +170,174 @@kube-proxy rules which would correctly balance across all endpoints.\n\nWith the new functionality, the external traffic will not be equally load balanced across pods, but rather\nequally balanced at the node level (because GCE/AWS and other external LB implementations do not have the ability\nfor specifying the weight per node, they balance equally across all target nodes, disregarding the number of"
  },
  {
    "id" : "6c428f2e-3c82-46a1-af93-9175c8a00d2b",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87442283-9201-4e8d-9f9b-fff8e561ce7d",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Are we actually enforcing this (if so how)? \n",
        "createdAt" : "2016-08-05T16:44:40Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "c8de0120-1755-4bef-85b7-567cccd6a6bb",
        "parentId" : "87442283-9201-4e8d-9f9b-fff8e561ce7d",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "@bprashanth You mean enforcing the 'read-only' part ? I didn't find an option in the API definition, we probably need to handle in code, I will have to find another example that implements this.\n",
        "createdAt" : "2016-08-05T17:35:58Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "dcd4917c-a8d7-4182-a7a9-08714f7b2599",
        "parentId" : "87442283-9201-4e8d-9f9b-fff8e561ce7d",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "How about just checking that `EndpointAddress.IP` is one of the local addresses?\n",
        "createdAt" : "2016-08-08T03:19:40Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "0fd6ab13-4396-45dc-8a85-3a9a5f7a490b",
        "parentId" : "87442283-9201-4e8d-9f9b-fff8e561ce7d",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "> How about just checking that EndpointAddress.IP is one of the local\n> addresses?\n\nHarder than it sounds.  IPAM is essentially pluggable, so it would\nrequire a new interface into network plugins, cross-component comms,\netc.  We already have a channel for the info, we should at least start\nwith that :)\n",
        "createdAt" : "2016-08-08T04:48:31Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "b665dfeb-1fd6-4ef6-b388-0efcd2a4fb91",
        "parentId" : "87442283-9201-4e8d-9f9b-fff8e561ce7d",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "Yes, coordination with IPAM is quite difficult - servicecontroller directly copying the Node name from the Pod to the EndpointsAddress eliminates all the isMemberOfLocalPodCIDRsSet checks and helps avoid a lot of complexity.\n",
        "createdAt" : "2016-08-09T02:26:17Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "4b6bb869-6219-47f4-9d94-75c623d6ef68",
        "parentId" : "87442283-9201-4e8d-9f9b-fff8e561ce7d",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Yep cool.  I was just assuming it was easy to check against `net.InterfaceAddrs()`, but I can see there might be cases where an effectively local endpoint address isn't so easily discoverable.\n",
        "createdAt" : "2016-08-09T04:59:41Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "eb3c749f-f858-43a3-9d54-4487405a8a2c",
        "parentId" : "87442283-9201-4e8d-9f9b-fff8e561ce7d",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "If you're putting the new field in endpoints.Status, and endpoints has a /status subresource, then that's good enough I think. We're still not ensuring that _only_ the endpoints controller sets it, but we'll reject updates to stats through the normal resource that things like kubectl edit hit.\n",
        "createdAt" : "2016-08-09T21:23:49Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : null,
    "diffHunk" : "@@ -1,1 +133,137 @@To allow kube-proxy to recognize if an endpoint is local requires that the EndpointAddress struct\nshould also contain the NodeName it resides on. This new string field will be read-only and\npopulated *only* by the Endpoints Controller.\n\n### Service Annotation to opt-in for new behaviour"
  },
  {
    "id" : "b9f01434-608e-415f-b691-561764cbaf8c",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "efe78bc6-bc85-40e2-82c9-ae8d0a6867ab",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Is there a way we can test the node port behavior needed by AWS on a GCE e2e? \n",
        "createdAt" : "2016-08-05T16:48:00Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "b309e406-2081-4453-8431-c4375a2d930e",
        "parentId" : "efe78bc6-bc85-40e2-82c9-ae8d0a6867ab",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "Is there an existing example of AWS preparedeness testing using a GCE e2e test ?\nI suppose we could make the test client mimic the AWS healthcheck behaviour and/or write a dummy ELB that mimics AWS behaviour in GCE, if we had enough time.\n",
        "createdAt" : "2016-08-05T20:28:48Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "504d5e48-be71-4f54-b590-dfdf9dcf9b94",
        "parentId" : "efe78bc6-bc85-40e2-82c9-ae8d0a6867ab",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "I think @therc volunteered, unless I misunderstood. I think we would benefit from the aws lb test not really needing an aws lb cluster fwiw, because all our SQ testing is currently on a GCE/GKE cluster. \n",
        "createdAt" : "2016-08-09T21:32:33Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "accf6557-2e1d-42db-be29-7c16c93fff6f",
        "parentId" : "efe78bc6-bc85-40e2-82c9-ae8d0a6867ab",
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "Can one cheat by setting up e.g. nginx as a mock ELB? I can help only with the small stuff for 1.4, as I already have a lot of stuff in flight. Also, in the meantime the AWS cloud provider already has some of this through annotations that enable L7 proxying (with X-Forwarded-For:) and the Proxy protocol.\n",
        "createdAt" : "2016-08-09T22:09:41Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 238,
    "diffHunk" : "@@ -1,1 +236,240 @@time for Cloud LB to declare node healthy and vice versa) to endpoint changes.\n\n2. Inter-Operability Tests\n\nValidate that internal cluster communications are still possible from nodes without local endpoints. This change"
  },
  {
    "id" : "0de40ca5-0a1a-4f8e-addb-956f3cfdc852",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c0daf9f-f409-4d72-8048-97bba855b58c",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "If we're going to do this, we should probably also just make it do health checks of the pods on the node in general.  e.g we should report health based not only on whether the endpoint is present, but also if it is locally passing it's readiness/health check.\n",
        "createdAt" : "2016-08-05T17:38:44Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "ea7e754f-e0b8-4f2b-a48a-924b56e91209",
        "parentId" : "7c0daf9f-f409-4d72-8048-97bba855b58c",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "I need to test to see the relation between the endpoint update to kube-proxy and the pod failing/passing its readiness/health checks - I believe the endpoints watch will update kube-proxy state when the pod status changes pretty much immediately i.e. endpoints are added/withdrawn for services based on the pod readiness/health by servicecontroller.\nWe were discussing the latency of getting the result of the kubelet health check to kube-proxy via the API server (since there is no horizontal communication path on the node). Even in the GKE case, where the master runs in a different project but in the same zone, the latency should be very small.\n",
        "createdAt" : "2016-08-05T20:22:47Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "5d9e5e71-4f5c-48b0-9ccc-9e51b0f383a9",
        "parentId" : "7c0daf9f-f409-4d72-8048-97bba855b58c",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "(architectural side note: It's useful if the health status can go through a point with cluster-wide health knowledge, so you can add logic like \"never report < n% of the cluster as unhealthy\" and prevent cascading failures on overload)\n",
        "createdAt" : "2016-08-07T23:54:16Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "e1e903a5-0faf-4a87-b3f3-90a1e562787e",
        "parentId" : "7c0daf9f-f409-4d72-8048-97bba855b58c",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "> If we're going to do this, we should probably also just make it do health checks of the pods on the node in general. e.g we should report health based not only on whether the endpoint is present, but also if it is locally passing it's readiness/health check.\n\nThat's already handled by natire of the endpoint not being in the\nEndpoints set if it is not Ready.\n",
        "createdAt" : "2016-08-08T04:43:29Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "6ebbaa11-ac7c-40c9-9a47-0e4808811c21",
        "parentId" : "7c0daf9f-f409-4d72-8048-97bba855b58c",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "@anguslees Could you elaborate more on that - I wasn't planning any cluster-wide coordination in the individual kube-proxy health check responses because I am not aware of any piece of our infrastructure that could help coordinate this.\n",
        "createdAt" : "2016-08-09T02:21:09Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "06499eb4-3e21-40ff-9836-627c238fb14d",
        "parentId" : "7c0daf9f-f409-4d72-8048-97bba855b58c",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "@girishkalele: The danger with doing health checking directly from the LB is that you might \"overreact\" during overload.  Two examples:\n- An intermittent failure affects a significant portion (eg: ~half) of your servers.  The LB considers them unhealthy, and then dumps all the resulting load on the few remaining servers.  The additional load then causes a few more of them to fail, dumping _more_ load on the _fewer_ remaining servers, etc.  You've turned what could have been a partial outage into a complete outage.\n- When your servers try to come back from a cluster-wide catastrophe like the above, the first server the LB considers \"healthy\" will immediately get the full cluster load dumped on it.  Assuming that drives it into the ground, then the next server to appear \"healthy\" will get the same treatment, etc guaranteeing that no server is able to stay up.  You've turned what could have been a recovery into an indefinite outage.\n\nOne possible automated solution to both of these is to be able to say \"no fewer than N% should be considered unhealthy\", or some similar policy.  That means that if there _are_ fewer than this many healthy, then you assume you're in some catastrophic situation, and you (effectively) blackhole some portion of queries at the LB rather than overload the remaining servers that _are_ healthy.\n\nNot a big deal, and I fully expect you to forget about this comment until after you suffer your first DoS outage ;)  My note was only that it is useful to have an intermediate point to insert arbitrary business logic between the real health check and what the LB does.\n",
        "createdAt" : "2016-08-09T04:49:41Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "38db6ba6-6583-4c0d-8628-132e59e54617",
        "parentId" : "7c0daf9f-f409-4d72-8048-97bba855b58c",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "> hat means that if there are fewer than this many healthy, then you assume you're in some catastrophic situation, and you (effectively) blackhole some portion of queries at the LB rather than overload the remaining servers that are healthy.\n\nSuch policy is pretty hard to implement on cloud, suggest spinning off to another issue so we and flesh out the idea?\n\n> Not a big deal, and I fully expect you to forget about this comment until after you suffer your first DoS outage ;) My note was only that it is useful to have an intermediate point to insert arbitrary business logic between the real health check and what the LB does.\n\nYou can enforce dos policy through Ingress. Most attacks are layer 7 anyway, and some cloud lbs will protect you from the most basic l4 attacks for free. If you're interested in participating in DoS design: https://github.com/kubernetes/kubernetes/issues/30088\n",
        "createdAt" : "2016-08-09T21:15:29Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : null,
    "diffHunk" : "@@ -1,1 +111,115 @@will be allocated for these health check for this purpose.\nkube-proxy already watches for Service and Endpoint changes, it will maintain an in-memory lookup\ntable indicating the number of local endpoints for each service.\nFor a value of zero local endpoints, it responds with a health check failure (503 Service Unavailable),\nand success (200 OK) for non-zero values."
  },
  {
    "id" : "e5bf1424-2509-464e-af6a-3e1047c4dae9",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ac14fe2-4afe-472c-b0d1-2ca9c76a9fb8",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "\"Must\" here makes it sound like directly delivering packets is the only way to preserve source IPs, which is not true.  Any method that delivers the packet to the destination host with the source unmodified will achieve the desired goal.\n\nEg: the packet could be encapsulated and then passed around between nodes any number of times, and still be able to be (eventually) delivered to the correct endpoint in a way that preserved the original source information.\n\nI guess what I'm saying is \"avoiding double hops\" is one optimisation goal, and \"preserving source IP\" is a _separate_ correctness goal.  This proposal claims to want the latter, but actually focuses on implementing the former.  I think we should call out one or the other.\n",
        "createdAt" : "2016-08-08T00:06:59Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "b6228436-5dcb-460c-a663-15d840d15fd9",
        "parentId" : "1ac14fe2-4afe-472c-b0d1-2ca9c76a9fb8",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Elsewhere it has been clarified that the architecture expected in this proposal involves the original user packet arriving at a node _unmodified_ (no NAT of any sort).\n\nThis makes it easy to invent schemes where we pass packets around between nodes for eventual delivery to the destination pod during complicated cases like uneven load balancing, [pod migration](https://github.com/kubernetes/kubernetes/issues/3949), other types of pod affinity, less capable underlying cloud providers, etc, etc.  So I think that makes it even more important to say the goal here is to preserve source IP, and not assume the only way to do that is by avoiding double hops.\n\nIn particular, we could add all sorts of alternatives to the below, like \"traffic steering by arbitrary node-level go code\" if we desired (and performance/overhead was acceptable) ;)\n",
        "createdAt" : "2016-08-09T05:12:02Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +88,92 @@## Overview\n\nThe double hop must be prevented by programming the external load balancer to direct traffic\nonly to nodes that have local pods for the service. This can be accomplished in two ways, either\nby API calls to add/delete nodes from the LB node pool or by adding health checking to the LB and"
  },
  {
    "id" : "a94606cb-25c1-49fd-8a58-c65b2d4f473e",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7283f7d-2cd6-4fd0-8af1-7622dc38ce04",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "I gather the rest of this doc has a specific architecture in mind - and it would be good to explicitly detail that here somewhere.  I _think_ the assumed architecture here is:\n\n```\nLB does host-specific DNAT -> (l3 forward over network) -> node does pod-specific DNAT\n      -> (l3 forward on node) -> pod\n```\n\nIn particular, I think this doc has significant implications for the LB->node hop.  This bit needs to be explicitly defined since it affects cloud-portability.  (eg: layer2 or layer3 forwarding LB->node packets without DNAT at this hop are other possible interpretations that fit what is currently described in this doc, but require very different node-level implementations).\n",
        "createdAt" : "2016-08-08T00:37:30Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "3c5a23b3-67b9-4370-9120-1cf6a94c637d",
        "parentId" : "c7283f7d-2cd6-4fd0-8af1-7622dc38ce04",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "Yes, we can only go down a certain path since the packet paths are already implemented.\nUnfortunately, we don't have a detailed \"life-of-a-packet\" overview for the existing path and fixups, but for this change, I will try to convert some slides into SVGs - its hard to embed images into Markdown docs that render correctly everywhere in Github and locally.\n\nIn GCE, there is no DNAT performed, the packets arrive on the GCE VMs with the Virtual IP still intact.\n",
        "createdAt" : "2016-08-09T02:18:37Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "c1a4b747-441c-49e8-8eee-68560b5641fe",
        "parentId" : "c7283f7d-2cd6-4fd0-8af1-7622dc38ce04",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : ".. Aha, so something (presumably iptables rules) on the actual node needs to have the public address+port configured, and DNAT to the pod IP+port.\n\nThis sort of detail _definitely_ needs to be part of the proposal, since it is highly likely other clouds won't have implemented the exact same packet behaviour you're expecting here.\n\nI don't think we necessarily need SVG diagrams here, but an actual description of how the packets are assumed to arrive/leave the node, and what this demands for local network routing, firewalling and kernel configuration is important.\n",
        "createdAt" : "2016-08-09T04:35:08Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "e403e08b-b9fa-4fe6-80bc-8939b27b3a40",
        "parentId" : "c7283f7d-2cd6-4fd0-8af1-7622dc38ce04",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "> .. Aha, so something (presumably iptables rules) on the actual node needs to have the public address+port configured, and DNAT to the pod IP+port.\n\nThis is how it already works, we're just proposing a twist that DNATs to local pods\n",
        "createdAt" : "2016-08-09T21:08:56Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "3dab65ee-18df-4ec8-b7d9-8ebfbb41e462",
        "parentId" : "c7283f7d-2cd6-4fd0-8af1-7622dc38ce04",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Specifically we support 2 primary modes of packet receipt for a load-balancer:\n\n1) VIP style.  Packet arrives with dstip and dstport set to the VIP:port of the external LB for a particular Service\n2) Proxy style.  Packet arrives with dstip set to the VM, on a node-port for a particular Service\n\nI think we should treat both of those as \"external\" and handle them the same in all ways, including firewalls (@freehan) and local-affinity (this doc).\n\nWhat this doc covers is fundamentally not that complex.  Upon receipt of a packet that qualifies as \"external\" (as per above) do we bounce to a random backend from the set of ALL available backends or do we choose a backend on the same node (if any, otherwise define the behavior). \n",
        "createdAt" : "2016-08-15T17:06:57Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "539bdc78-7811-43ab-8d9e-f3cb431cee5f",
        "parentId" : "c7283f7d-2cd6-4fd0-8af1-7622dc38ce04",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "I understand, but only thanks to the discussion (here and elsewhere) in these comments.  Can we actually add something to the doc capturing these network details please?\n",
        "createdAt" : "2016-08-16T00:37:05Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +92,96 @@by API calls to add/delete nodes from the LB node pool or by adding health checking to the LB and\nfailing/passing health checks depending on the presence of local pods.\n\n## Traffic Steering using LB programming\n"
  },
  {
    "id" : "19156d8c-5dd2-4b26-bbd9-bc484ef7dfb1",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bebd9817-04e6-4664-9d53-990a39818d71",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "I couldn't quickly find it in the k8s docs and I'm surprised I don't already know the answer:  Does k8s do \"lame duck\" controlled shutdowns of pods?  ie: where you start failing health checks for some period before actually killing the pod.  If this is in place, and can be made to last 30-60s, then I think it's still reasonable to use this approach.\n",
        "createdAt" : "2016-08-08T01:01:10Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "e7f8f9b0-4915-4dc1-9291-5d951d8f900c",
        "parentId" : "bebd9817-04e6-4664-9d53-990a39818d71",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "> I couldn't quickly find it in the k8s docs and I'm surprised I don't already\n> know the answer: Does k8s do \"lame duck\" controlled shutdowns of pods? ie:\n> where you start failing health checks for some period before actually\n> killing the pod. If this is in place, and can be made to last 30-60s, then I\n> think it's still reasonable to use this approach.\n\n\"graceful termination\" is what you want to look for ;)\n",
        "createdAt" : "2016-08-08T04:46:54Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "9f2f0aad-54f8-4365-905b-326fa6e137a2",
        "parentId" : "bebd9817-04e6-4664-9d53-990a39818d71",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Aha, and the docs even talk explicitly about interaction with the loadbalancers.  Excellent.  So obviously we'd have to ensure that the grace period was longer than the time taken to notice and reprogram the LB.  That seems easy enough.\n",
        "createdAt" : "2016-08-08T05:20:01Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "fce9b453-7d52-4d85-948d-ab45cb117dc3",
        "parentId" : "bebd9817-04e6-4664-9d53-990a39818d71",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Except in the case of node failure.  We want the LB to reprogram ASAP\n",
        "createdAt" : "2016-08-15T17:29:03Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : null,
    "diffHunk" : "@@ -1,1 +102,106 @@If the API endpoint is temporarily unavailable, the datapath will be misprogrammed till the\nreprogramming is successful and the API->datapath tables are updated by the cloud provider backend.\n\n## Traffic Steering using Health Checks\n"
  },
  {
    "id" : "7665147a-85e1-44ba-8455-51d253fdb128",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a6ef213-910c-469c-9efa-8d07d9b0a3ee",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Need to call out here if we're using the \"liveness\" or \"readiness\" \"health\".  In particular, you want to be able to say \"I'm not suitable for serving, but I also shouldn't be killed just now\" - this is used currently for a startup grace period but also needs to be used to tell the LB to cleanly drain traffic away from a pod that is _about_ to be killed.\n",
        "createdAt" : "2016-08-08T03:14:19Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "2e85169c-f709-41e8-83e4-91712a0ed998",
        "parentId" : "0a6ef213-910c-469c-9efa-8d07d9b0a3ee",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "This is a loadbalancer health check api, we don't really control the action a loadbalancer takes when a check fails. Nothing in Kubernetes should be reacting to this failure, for now at least. The loadbalancer shoudln't be deleting pods, but we can't enforce that eg: an cloud autoscaling group doesn't just take out instances that report unhealthy.\n",
        "createdAt" : "2016-08-09T21:20:19Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "0c9bfeb0-e308-44b1-afa9-d7bdc59c33cc",
        "parentId" : "0a6ef213-910c-469c-9efa-8d07d9b0a3ee",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "@anguslees I am not sure when we get the endpoint withdrawn from the service endpoints list - assuming it is withdrawn when the pod processes are sent the termination notification, we also need the applications in the pod not to terminate immediately but keep serving longer than the time taken for the LB to drain traffic away.\n",
        "createdAt" : "2016-08-22T18:00:10Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +120,124 @@This will allow much faster transition times on the order of 1-5 seconds, and involve no\nAPI calls to the cloud provider (and hence reduce the impact of API unreliability), keeping the\ntime window where traffic might get directed to nodes with no local endpoints to a minimum.\n\n## Choice of traffic steering approaches by individual Cloud Provider implementations"
  },
  {
    "id" : "6dc24347-ea49-42a8-bfa8-43a366507ecf",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84332866-b628-4936-8f3a-59bc5ce12f39",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "See my comment above about ignoring being harmful.  If we expect services to actually use the source IP for something, then I think we have to return an error when we know the source IP is going to be a lie.\n",
        "createdAt" : "2016-08-08T03:41:04Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +185,189 @@This feature is added as an opt-in annotation.\nDefault behaviour of LoadBalancer type services will be unchanged for all Cloud providers.\nThe annotation will be ignored by existing cloud provider libraries until they add support.\n\n### GCE 1.4"
  },
  {
    "id" : "d31387b3-5512-4556-872f-767f507e0d3c",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19629aa5-0992-45c0-be2e-565c08eb25a6",
        "parentId" : null,
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "It's mentioned later in the API changes, but it would be nice to state explicitly here, too, that the health checks are performed on an additional NodePort. My first reaction was \"Wait a second...\"\n",
        "createdAt" : "2016-08-09T18:03:54Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      },
      {
        "id" : "fc9cecc4-acbc-40ad-a097-67fc18b720cc",
        "parentId" : "19629aa5-0992-45c0-be2e-565c08eb25a6",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "I will add a mention here.\n",
        "createdAt" : "2016-08-09T22:22:42Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +107,111 @@This approach requires that all worker nodes in the cluster be programmed into the LB target pool.\nTo steer traffic only onto nodes that have endpoints for the service, we program the LB to perform\nnode healthchecks. The kube-proxy daemons running on each node will be responsible for responding\nto these healthcheck requests (URL `/healthz`) from the cloud provider LB healthchecker. An additional nodePort\nwill be allocated for these health check for this purpose."
  },
  {
    "id" : "669fecba-b550-40d9-ab0f-aaa8d57fe511",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "789c9e1b-0068-482d-9f50-09184c53823a",
        "parentId" : null,
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "I can help with AWS testing. Summoning @justinsb \n",
        "createdAt" : "2016-08-09T18:05:03Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : null,
    "diffHunk" : "@@ -1,1 +205,209 @@### AWS TBD\n\nTBD *discuss timelines and feasibility with Kubernetes sig-aws team members*\n\n### Openstack TBD"
  },
  {
    "id" : "5334825f-603a-4111-81f0-2278fb7689d4",
    "prId" : 30105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75dcf85d-2d28-4978-bc48-a872a596bb26",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "This is a little contentious.  Do we blackhole traffic or just fall back on the global behavior?  During that window, users will experience a real outage.  Is that better than getting the wrong source IP?\n\nAdditionally, if we treat nodeports equivalently to VIPs, this is a big change in nodeport behavior.\n\nAdvice wanted.  Don't block on this but DO revisit, please.\n",
        "createdAt" : "2016-08-15T17:42:37Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "72c72b0b-e7ef-465a-99ee-ac5a332230b5",
        "parentId" : "75dcf85d-2d28-4978-bc48-a872a596bb26",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "> During that window, users will experience a real outage. Is that better than getting the wrong source IP?\n\nI'd say yes for the same reason discussed below, it's easier to engineer around blackholing either with client retries or by just labeling nodes with `role=lb` and loading it up with replicas. If a client ip is blacklisted it shoudn't be allowed through, no matter what, IMO.\n",
        "createdAt" : "2016-08-15T17:51:29Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "694e3e25-21ec-40de-b6db-cc1b0ce0a509",
        "parentId" : "75dcf85d-2d28-4978-bc48-a872a596bb26",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I can buy this, but we need to be VERY clear that this is a departure from\nexisting nodeports.  Or we need another thing that is like nodeports but\nwith the new semantic.\n\nOn Mon, Aug 15, 2016 at 10:52 AM, Prashanth B notifications@github.com\nwrote:\n\n> In docs/proposals/external-lb-source-ip-preservation.md\n> https://github.com/kubernetes/kubernetes/pull/30105#discussion_r74802541\n> :\n> \n> > +annotate the Service to request the new ESIPP behaviour as documented in the section below.\n> > +\n> > +### NodePort allocation for HealthChecks\n> > +\n> > +An additional nodePort allocation will be necessary for services that are of type LoadBalancer and\n> > +have the new annotation specified. This additional nodePort is necessary for kube-proxy to listen for\n> > +healthcheck requests on all nodes.\n> > +This NodePort will be added as an annotation to the Service after allocation (in the alpha release).\n> > +\n> > +## Behavior Changes expected\n> > +\n> > +### External Traffic Blackholed on nodes with no local endpoints\n> > +\n> > +When the last endpoint on the node has gone away and the LB has not marked the node as unhealthy,\n> > +worst-case window size = (N+1) \\* HCP, where N = minimum failed healthchecks and HCP = Health Check Period,\n> > +external traffic will still be steered to the node. This traffic will be blackholed and not forwarded\n> \n> During that window, users will experience a real outage. Is that better\n> than getting the wrong source IP?\n> \n> I'd say yes for the same reason discussed below, it's easier to engineer\n> around blackholing either with client retries or by just labeling nodes\n> with role=lb and loading it up with replicas. If a client ip is\n> blacklisted it shoudn't be allowed through, no matter what, IMO.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/30105/files/ac8a27036cf9811c9de760c578958948370629f1#r74802541,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFVgVN_ncL-7QkYA5CQN5XroCJyDxBv-ks5qgKdWgaJpZM4JdJyL\n> .\n",
        "createdAt" : "2016-08-15T17:56:26Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "a92fe77b-55ef-4b65-813b-93c4aa3553d5",
        "parentId" : "75dcf85d-2d28-4978-bc48-a872a596bb26",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "Currently, the new \"local-endpoints-only\" load balancing chain is reachable only for packets arriving with the VIP address.\nFor packets arriving on the VM IP and nodePort, the old behaviour (balance across all endpoints) will be preserved.\n",
        "createdAt" : "2016-08-15T23:11:47Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "59d1f695-3efc-41e2-987b-cc9cc6b0810a",
        "parentId" : "75dcf85d-2d28-4978-bc48-a872a596bb26",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Let's get this merged and revisit what to do on NodePort.  I think we need\nto do SOMETHING, but we can solicit feedback on which thing that is.\n\nOn Mon, Aug 15, 2016 at 4:12 PM, Girish Kalele notifications@github.com\nwrote:\n\n> In docs/proposals/external-lb-source-ip-preservation.md\n> https://github.com/kubernetes/kubernetes/pull/30105#discussion_r74852320\n> :\n> \n> > +annotate the Service to request the new ESIPP behaviour as documented in the section below.\n> > +\n> > +### NodePort allocation for HealthChecks\n> > +\n> > +An additional nodePort allocation will be necessary for services that are of type LoadBalancer and\n> > +have the new annotation specified. This additional nodePort is necessary for kube-proxy to listen for\n> > +healthcheck requests on all nodes.\n> > +This NodePort will be added as an annotation to the Service after allocation (in the alpha release).\n> > +\n> > +## Behavior Changes expected\n> > +\n> > +### External Traffic Blackholed on nodes with no local endpoints\n> > +\n> > +When the last endpoint on the node has gone away and the LB has not marked the node as unhealthy,\n> > +worst-case window size = (N+1) \\* HCP, where N = minimum failed healthchecks and HCP = Health Check Period,\n> > +external traffic will still be steered to the node. This traffic will be blackholed and not forwarded\n> \n> Currently, the new \"local-endpoints-only\" load balancing chain is\n> reachable only for packets arriving with the VIP address.\n> For packets arriving on the VM IP and nodePort, the old behaviour (balance\n> across all endpoints) will be preserved.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/30105/files/ac8a27036cf9811c9de760c578958948370629f1#r74852320,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFVgVBmz8PbVtjcRgjrKVVizKtm1r2rCks5qgPJmgaJpZM4JdJyL\n> .\n",
        "createdAt" : "2016-08-15T23:18:28Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "9a81c75d-26c5-4361-bae4-19766bcdd634",
        "parentId" : "75dcf85d-2d28-4978-bc48-a872a596bb26",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "To add some clarity, we currently blackhole traffic arriving with the VIP:svcPort if there are no local endpoints.\n",
        "createdAt" : "2016-08-22T18:01:14Z",
        "updatedAt" : "2016-09-06T21:15:04Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "30e7c055d42fe4d6632632f908b7d46bb1e350a7",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +160,164 @@When the last endpoint on the node has gone away and the LB has not marked the node as unhealthy,\nworst-case window size = (N+1) * HCP, where N = minimum failed healthchecks and HCP = Health Check Period,\nexternal traffic will still be steered to the node. This traffic will be blackholed and not forwarded\nto other endpoints elsewhere in the cluster.\n"
  }
]