[
  {
    "id" : "06bc73be-6e1a-42c3-b490-b0a9a112bafb",
    "prId" : 23343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7c3be27-cc97-4a84-96ed-75f066cde4e7",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "nit: This could also be handled by the DaemonSet upgrade policy.\n",
        "createdAt" : "2016-04-12T20:52:46Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "8566dc87-5ac6-4b5d-a344-824648442578",
        "parentId" : "e7c3be27-cc97-4a84-96ed-75f066cde4e7",
        "authorId" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "body" : "What is the status on this? Closest I could find was: https://github.com/kubernetes/kubernetes/pull/19627\n",
        "createdAt" : "2016-04-13T00:45:32Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7f4402e41555db052170f2a3c039ec184b7c70f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +101,105 @@1. Cluster administrator introduces \"v2\" kubelet daemonset\n1. \"v1\" kubelet pulls down and starts \"v2\"\n1. Cluster administrator removes \"v1\" kubelet daemonset\n1. \"v1\" kubelet is killed\n1. Both \"bootstrap\" and \"v2\" kubelets race for file lock"
  },
  {
    "id" : "19df40ab-f66f-4d77-8d95-a87489cb04b4",
    "prId" : 23343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41172b39-179d-40df-946b-eb1209b9f34f",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Can you add a few lines to clarify the configuration mismatch issues we discussed in the past and some options to mitigate it? \nhttps://github.com/kubernetes/kubernetes/pull/23343#issuecomment-205399017\n",
        "createdAt" : "2016-04-12T20:56:28Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "5ea75bfc-9a67-4847-8aac-67e7a4e5e6d0",
        "parentId" : "41172b39-179d-40df-946b-eb1209b9f34f",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Could you list the problems that are NOT handled in this proposal (if there is any) and potential workaround for them? E.g., what's required to make this production ready.\n",
        "createdAt" : "2016-04-12T21:04:01Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7f4402e41555db052170f2a3c039ec184b7c70f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +138,142 @@  of scope for this proposal. It is assumed that either the cluster\n  administrator or the daemonset upgrade policy will handle this.\n\n## Other discussion\n"
  },
  {
    "id" : "c2a4876e-c9c3-445b-a603-16b58771c15f",
    "prId" : 23343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a7f7421-9234-4553-89f8-4695fbadd1e1",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "nit: breaking up a paragraph with new lines will make commenting easier. \n",
        "createdAt" : "2016-04-12T20:58:32Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7f4402e41555db052170f2a3c039ec184b7c70f",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +31,35 @@\n## Abstract\n\nIn a self-hosted Kubernetes deployment (see (this\ncomment)[https://github.com/kubernetes/kubernetes/issues/246#issuecomment-64533959]"
  },
  {
    "id" : "a83b61e0-255b-461e-afbd-16ec62f456c1",
    "prId" : 23343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "56448c88-7799-4c02-9ba6-4f5ccf17b6de",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Current Kubelet upgrade procedure requires a node drain to evacuate all pods in kube 1.2.  I suspect it may continue to be the case in kube 1.3 and will definitely be when we add things like pod level cgroups.  Can you add where you see draining happening with a damson set drive deployment model like this?\n",
        "createdAt" : "2016-04-19T20:27:59Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "dce9d55c-5a1c-4608-8f93-1001a7a91b36",
        "parentId" : "56448c88-7799-4c02-9ba6-4f5ccf17b6de",
        "authorId" : "625aa63f-7185-453c-9db6-330c20486b28",
        "body" : "Good point, updated the proposal to address this: https://github.com/kubernetes/kubernetes/pull/23343/files#diff-952062ba896c0f541a728990c7525b1fR131.\n\nI'm putting this in the \"out of scope\" section for now, as this is an important consideration for production readiness, but out of scope for this specific proposal which simply aims to cover initial bootstrap of kubelet.\n",
        "createdAt" : "2016-04-21T00:14:47Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "625aa63f-7185-453c-9db6-330c20486b28",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7f4402e41555db052170f2a3c039ec184b7c70f",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +100,104 @@\n1. Cluster administrator introduces \"v2\" kubelet daemonset\n1. \"v1\" kubelet pulls down and starts \"v2\"\n1. Cluster administrator removes \"v1\" kubelet daemonset\n1. \"v1\" kubelet is killed"
  },
  {
    "id" : "446ec899-d0f1-41df-81c7-2abd479f0857",
    "prId" : 23343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14b252b3-4439-4802-bb8d-daf266969f88",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "In 1.2 we ran into issues other than cgroup stuff so draining of a node was still needed.  I am not sure how that impacts this design if at all.\n",
        "createdAt" : "2016-04-19T20:29:50Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7f4402e41555db052170f2a3c039ec184b7c70f",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +126,130 @@* Deterministically pulling and running kubelet pod: we would prefer not to have\n  to loop until we finally get a kubelet pod.\n* It is possible that the bootstrap kubelet version is incompatible with the\n  newer versions that were run in the node. For example, the cgroup\n  configurations might be incompatible. In the beginning, we will require"
  },
  {
    "id" : "05fc438a-2cb6-4d5b-b9f5-df5157d77613",
    "prId" : 23343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ad6c4b1-70a4-4de2-9a25-8d8a663d1745",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "What about bootstrapping the master nodes? Why is that not tackled here? How is it different from bootstrapping kubelets on other nodes?\n",
        "createdAt" : "2016-04-27T21:48:57Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "e660e021-d4a1-4cd7-9bcc-6d89aeb245cb",
        "parentId" : "2ad6c4b1-70a4-4de2-9a25-8d8a663d1745",
        "authorId" : "625aa63f-7185-453c-9db6-330c20486b28",
        "body" : "This line is meant to say you must have at least a temporary control plane available somewhere in order to bootstrap _any_ node, be it master or minion. In our POC we have a \"control-plane-in-a-single-binary\" type solution for standing up a temporary control plane.\n\nI will try and reword this, but what I was trying to get across is that a control plane must exist for the bootstrap kubelet to contact so it can pull down the kubelet daemonset and pivot. This can be a temporary control plane that lives only during the bootstrap process.\n",
        "createdAt" : "2016-04-27T21:53:58Z",
        "updatedAt" : "2016-05-02T19:10:16Z",
        "lastEditedBy" : "625aa63f-7185-453c-9db6-330c20486b28",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7f4402e41555db052170f2a3c039ec184b7c70f",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +42,46 @@\nThis proposal presents a solution to the kubelet bootstrap, and assumes a\nfunctioning control plane (e.g. an apiserver, controller-manager, scheduler, and\netcd cluster), and a kubelet that can securely contact the API server. This\nfunctioning control plane can be temporary, and not necessarily the \"production\""
  }
]