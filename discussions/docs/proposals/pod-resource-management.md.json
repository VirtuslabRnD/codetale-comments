[
  {
    "id" : "3cc2bf92-9b98-451a-be39-97c80bd7265c",
    "prId" : 29583,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b94cff1-7be0-4356-9575-3f9a3d363dcb",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "detail on how it helps?  i have no objection on adding Pod UID, just curious.\n",
        "createdAt" : "2016-08-04T21:48:51Z",
        "updatedAt" : "2016-08-04T22:15:09Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "6880f448a1be7a905483a2150eab58ef750dd60f",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +422,426 @@\nOther smaller work items that we would be good to have before the release of this feature.\n- [ ] Add Pod UID to the downward api which will help simplify the e2e testing logic.\n- [ ] Check if parent cgroup exist and error out if they don’t.\n- [ ] Set top level cgroup limit to resource allocatable until we support QoS level cgroup updates. If cgroup root is not `/` then set node resource allocatable as the cgroup resource limits on cgroup root."
  },
  {
    "id" : "ec3592c0-800c-4291-9535-3bd41c897b25",
    "prId" : 26751,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec133b1c-ebcc-47a7-9830-b60d0c4a54c3",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "This is not clarified, but does this mean if I have a pod with UID: `5f9b19c9-3a30-11e6-8eea-28d2444e470d`\n\nwhich cgroup path do I get:\n1. /5f9b19c9-3a30-11e6-8eea-28d2444e470d/cpu.quota\n2. /pod5f9b19c9-3a30-11e6-8eea-28d2444e470d/cpu.quota\n3. /pod-5f9b19c93a3011e6-8eea28d2444e470d/cpu.quota\n\nI am hoping 3 so I can inspect /sys/fs/cgroup and know this is a pod...\n",
        "createdAt" : "2016-06-24T17:28:03Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "74c3bacb-838e-41b9-9d6c-2044c587bb45",
        "parentId" : "ec133b1c-ebcc-47a7-9830-b60d0c4a54c3",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "This is more a question for how the implementation will allow me to clearly see the cgroupfs that this cgroup aligns to a pod, etc. and not just getting opaque ids.\n",
        "createdAt" : "2016-06-24T17:33:08Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "4255744e-5447-4a2d-afb0-6b1949a631a8",
        "parentId" : "ec133b1c-ebcc-47a7-9830-b60d0c4a54c3",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "If we can just state what PodUID expands to in one spot, that would be good, and we can then use the shorthand throughout.  same with the qos level cgroup.\n",
        "createdAt" : "2016-06-24T17:33:47Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "e425c6bbda08dfe4f963192e9aad3d581479411b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +132,136 @@\n```\n/PodUID/cpu.quota = cpu limit of Pod  \n/PodUID/cpu.shares = cpu request of Pod  \n/PodUID/memory.limit_in_bytes = memory limit of Pod"
  },
  {
    "id" : "3db34089-23ad-49e6-99b9-e02828bf3419",
    "prId" : 26751,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6a92796-bf35-4b69-8242-a77f86f4a050",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I know this is explanatory, but on a systemd system, I would like to be able to type:\n\n`systemctl status burstable.slice`\n\nand get aggregate usage of that tier, so just a long winded way of saying I would not want to abbrev this in implementation.\n",
        "createdAt" : "2016-06-24T17:31:49Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "f96aaee2-c206-4450-9811-1131aba7f12b",
        "parentId" : "b6a92796-bf35-4b69-8242-a77f86f4a050",
        "authorId" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "body" : "Really sorry but I am not sure if i understand what you mean by \" I would not want to abbrev this in implementation.\"\n",
        "createdAt" : "2016-06-27T01:52:04Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "tags" : [
        ]
      },
      {
        "id" : "60542398-dbc1-4792-9468-48cd9a389fbc",
        "parentId" : "b6a92796-bf35-4b69-8242-a77f86f4a050",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I mean I do not want a slice on my machine called \"Bu\", so when we implement this, it should have human-readable fully qualified names like \"burstable.slice\"\n",
        "createdAt" : "2016-06-27T14:56:17Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "e0d668d2-7ee6-4210-be34-16df786939d9",
        "parentId" : "b6a92796-bf35-4b69-8242-a77f86f4a050",
        "authorId" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "body" : "Yes thats exactly how I planned to have it.\n",
        "createdAt" : "2016-06-27T23:06:02Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "tags" : [
        ]
      }
    ],
    "commit" : "e425c6bbda08dfe4f963192e9aad3d581479411b",
    "line" : 189,
    "diffHunk" : "@@ -1,1 +187,191 @@\n```\n/Bu/cpu.shares = summation of cpu requests of all Bu pods  \n/Bu/PodUID/cpu.quota = Pod Cpu Limit  \n/Bu/PodUID/cpu.shares = Pod Cpu Request   "
  },
  {
    "id" : "385a0408-a21b-4181-8c02-fefc3b98aa67",
    "prId" : 26751,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f944978c-3a25-4d14-9bd8-ea710ea2573b",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "docker here is valid too.\n",
        "createdAt" : "2016-06-24T17:38:20Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "e425c6bbda08dfe4f963192e9aad3d581479411b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +348,352 @@  |   +- system\n  |   +- docker (optional)\n  |   +- ...\n  |\n  +- Kube-reserved "
  },
  {
    "id" : "f8ab5cf5-06c5-4995-b4c5-cf2406df77c4",
    "prId" : 26751,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f58a0d3d-59ec-4ce7-97e7-46ff26e43c09",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "will this happen before or after image pulling?\n\n/cc @vishh - I would like to be able to in the future charge image pulling to the pod cgroup.\n",
        "createdAt" : "2016-06-24T17:45:06Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "ce3bee44-a37f-48ca-97f2-d1c03f5a68d7",
        "parentId" : "f58a0d3d-59ec-4ce7-97e7-46ff26e43c09",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "This will happen before kubelet performs any activity for a pod. Ideally,\nwe can charge all pod related activities including image pulling, volumes\ncreation, network setup, etc., to the pod level cgroups.\n\nOn Fri, Jun 24, 2016 at 10:45 AM, Derek Carr notifications@github.com\nwrote:\n\n> In docs/proposals/pod-resource-management.md\n> https://github.com/kubernetes/kubernetes/pull/26751#discussion_r68434206\n> :\n> \n> > +- Also we don't set any quota on the cpu resources as the containers on the `BE` pods can use any amount of free resources on the node.\n> > +- Having memory limit of `BE` cgroup as (Allocatable - summation of memory limits of Guaranteed and Burstable pods) would result in `BE` pods becoming more susceptible to being OOM killed. As more `G` and `Bu` pods are scheduled kubelet will more likely kill `BE` pods, even if the `G` and `Bu` pods are using less than their request since we will be dynamically reducing the size of `BE` m.limit_in_bytes. But this allows for better memory guarantees to the `G` and `Bu` pods.\n> > +\n> > +## Implementation Plan\n> > +\n> > +The implementation plan is outlined in the next sections.\n> > +We will have a 'enable-qos-cgroups' flag to specify if the user wants to use the QoS based cgroup hierarchy. The flag would be set to false by default at least in v1.4.\n> > +\n> > +#### Top level Cgroups for QoS tiers\n> > +\n> > +Two top level cgroups for Burstable and Best effort Qos classes are created when Kubelet starts to run on a node. All guaranteed pods cgroups are by default nested under the `Root`. So we dont create a top level cgroup for the Guaranteed class. For raw cgroup systems we would use libcontainers cgroups manager for general cgroup management(cgroup creation/destruction). But for systemd we don't have equivalent support for slice management in libcontainer yet. So we will be adding support for the same in the Kubelet. These cgroups are only created once on Kubelet initialization as a part of node setup.\n> > +\n> > +#### Pod level Cgroup creation and deletion (Docker runtime)\n> > +\n> > +- When a new pod is brought up, its QoS class is firstly determined.\n> > +- We add an interface to Kubelet’s ContainerManager to create and delete pod level cgroups under the cgroup that matches the pod’s QoS class.\n> \n> will this happen before or after image pulling?\n> \n> /cc @vishh https://github.com/vishh - I would like to be able to in the\n> future charge image pulling to the pod cgroup.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/26751/files/86f33723eebe28602e420e4118a1a13e986b6c79#r68434206,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AGvIKCpbqYXhEBU8FuM31nc8RF73t0Lmks5qPBfKgaJpZM4ItEyP\n> .\n",
        "createdAt" : "2016-06-24T17:47:27Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "e425c6bbda08dfe4f963192e9aad3d581479411b",
    "line" : 381,
    "diffHunk" : "@@ -1,1 +379,383 @@\n- When a new pod is brought up, its QoS class is firstly determined.\n- We add an interface to Kubelet’s ContainerManager to create and delete pod level cgroups under the cgroup that matches the pod’s QoS class.\n- This interface will be pluggable. Kubelet will support both systemd and raw cgroups based __cgroup__ drivers. We will be using the --cgroup-driver flag proposed in the [Systemd Node Spec](kubelet-systemd.md) to specify the cgroup driver.\n- We inject creation and deletion of pod level cgroups into the pod workers."
  },
  {
    "id" : "36e1da29-3ef5-4c32-8219-16f8c1b0113f",
    "prId" : 26751,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4909b93-6f28-4749-b1b5-8b51ccbf82a3",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Will we update the `kubelet` summary stats provider to return pod level usage as well.  Right now, its just containers.\n",
        "createdAt" : "2016-06-24T17:52:04Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "24ee3bd3-9f48-4f92-bfff-f0ac519a3307",
        "parentId" : "c4909b93-6f28-4749-b1b5-8b51ccbf82a3",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "(this would let me simplify some code in eviction)\n",
        "createdAt" : "2016-06-24T17:52:26Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "ad8c0d4d-5520-42e6-b2ab-5b514ac31368",
        "parentId" : "c4909b93-6f28-4749-b1b5-8b51ccbf82a3",
        "authorId" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "body" : "I believe thats our plan. \ncc @vishh \n",
        "createdAt" : "2016-06-27T01:55:30Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "tags" : [
        ]
      }
    ],
    "commit" : "e425c6bbda08dfe4f963192e9aad3d581479411b",
    "line" : 396,
    "diffHunk" : "@@ -1,1 +394,398 @@#### Add Pod level metrics to Kubelet's metrics provider\n\nUpdate Kubelet’s metrics provider to include Pod level metrics. Use cAdvisor's cgroup subsystem information to determine various Pod level usage metrics.\n\n`Note: Changes to cAdvisor might be necessary.`"
  },
  {
    "id" : "af5c1610-79b6-4ddb-9a6f-62184d38b361",
    "prId" : 26751,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ca9006a-6b45-49af-8506-7e5d58780c0c",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "To clarify, opt-in does not mean alpha?\n",
        "createdAt" : "2016-06-24T17:52:55Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "9c4d1666-eb29-4f29-9293-166bf24a4a80",
        "parentId" : "5ca9006a-6b45-49af-8506-7e5d58780c0c",
        "authorId" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "body" : "cc @vishh \n",
        "createdAt" : "2016-06-27T02:01:59Z",
        "updatedAt" : "2016-07-25T21:28:17Z",
        "lastEditedBy" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "tags" : [
        ]
      }
    ],
    "commit" : "e425c6bbda08dfe4f963192e9aad3d581479411b",
    "line" : 402,
    "diffHunk" : "@@ -1,1 +400,404 @@## Rollout Plan\n\nThis feature will be opt-in in v1.4 and an opt-out in v1.5. We recommend users to drain their nodes and opt-in, before switching to v1.5, which will result in a no-op when v1.5 kubelet is rolled out.\n\n"
  }
]