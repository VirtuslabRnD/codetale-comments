[
  {
    "id" : "cd8362a0-727d-42a7-afad-7d37f60dba45",
    "prId" : 14943,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ed8b25f-53b1-43f3-907b-ba64d9190696",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "This is already implemented as of 1.1. Unfortunately there doesn't seem to be any user or admin documentation yet, just design docs in proposals/. I've asked the autoscaling team to write something, and then we can link to it here.\n",
        "createdAt" : "2015-10-05T03:46:35Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "c0dca17f-47b1-4a5a-bbc1-3b4c8b4ad9fa",
        "parentId" : "2ed8b25f-53b1-43f3-907b-ba64d9190696",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Acknowledged. cc @piosz \n",
        "createdAt" : "2015-10-05T18:53:23Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "537b7f0b-9ab2-4df7-b293-22bb61466cae",
        "parentId" : "2ed8b25f-53b1-43f3-907b-ba64d9190696",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "I'll try to do it tomorrow.\n",
        "createdAt" : "2015-10-05T18:59:50Z",
        "updatedAt" : "2016-05-20T22:03:35Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "a64fe6572abbdfc945952e35b47c36a1b3293ae9",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +43,47 @@On the other hand having Resources filled is critical for scheduling decisions.\nCurrent solution to set up Resources to hardcoded value has obvious drawbacks.\nWe need to implement a component which will set initial Resources to a reasonable value.\n\n## Design"
  },
  {
    "id" : "e1b99123-1a26-4415-815a-b0eb2434bf0e",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28fa8c0b-3ed1-4deb-9c71-c08073c44e0a",
        "parentId" : null,
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "In order to have the same logic exposed via API or to some UI it would make sense to have a service that would  return some predictions for a given pod template. It would be used by admission plugin and potentially some UIs.\n",
        "createdAt" : "2015-08-11T15:15:31Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "c94a99e9-606f-4e2e-a5fb-3537fa72c2f4",
        "parentId" : "28fa8c0b-3ed1-4deb-9c71-c08073c44e0a",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "I agree but let's make it in next iteration.\n",
        "createdAt" : "2015-08-11T15:38:31Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +45,49 @@which will set initial Resources to a reasonable value.\n\n## Design\n\nInitialResources component will be implemented as an [admission plugin](../../plugin/pkg/admission/) and invoked right before"
  },
  {
    "id" : "6208b4c5-f06f-4582-b2f1-8bd3fbb2c690",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47c962f4-38b2-4b45-a639-45f7c4fce2d9",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "We could add recommendations as annotations for those containers that already set resources.\n",
        "createdAt" : "2015-08-11T18:01:34Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "9add7671-3f42-4a2a-9667-1ac3848f729a",
        "parentId" : "47c962f4-38b2-4b45-a639-45f7c4fce2d9",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "I'll add this to the list of next steps.\n",
        "createdAt" : "2015-08-12T11:48:40Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : null,
    "diffHunk" : "@@ -1,1 +102,106 @@* add other features to the model like *namespace*\n* remember predefined values for the most popular images like *mysql*, *nginx*, *redis*, etc.\n* dry mode, which allows to ask system for resource recommendation for a container without running it\n* add estimation as annotations for those containers that already has resources set\n* support for other data sources like [Hawkular](http://www.hawkular.org/)"
  },
  {
    "id" : "b847dcce-af67-4e8b-8c07-808a2b58fd36",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "parentId" : null,
        "authorId" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "body" : "What happens if a container is assigned a certain memory request, but is killed (e.g. system OOM killed, or killed by control loops that might be added in the future)?\n\nFor example, suppose there's a container X that needs 500mb of memory to run. The limit ranger's default request is 10mb. X is assigned a 10mb request and scheduled onto a node with < 500mb memory available. X will probably get killed. If the user tries to launch another copy of X, we probably want to use something higher than 10mb (e.g. increase by 5%).\n\nAdditionally, if containers are killed, how would they be accounted for when we take the 90th percentile? For the first iteration, maybe we could ignore containers that get killed in computations (e.g. when computing 90th percentile). \n",
        "createdAt" : "2015-08-12T15:11:18Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "tags" : [
        ]
      },
      {
        "id" : "ccb720fd-78e8-4464-acb3-ed24b7787963",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "body" : "Oh I see you mentioned \"1 hour\" in the design doc. Do you mean that you would only count containers that have been running for an hour? That should resolve the 2nd issue (accounting for killed containers when we compute the 90th percentile) :)\n",
        "createdAt" : "2015-08-12T15:14:56Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "tags" : [
        ]
      },
      {
        "id" : "a493db3d-b899-41e7-97ad-8290a306edac",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "According to your example there is a plan to have reactive vertical autoscaler (see #10782) which will handle such situation. Also there is a possibility to add a feature to Initial Resources which will handle OOM in the way described by you (probably by increasing 2x rather than by 5%).\n\nAll containers will be considered as historical data. Those that have been killed also provide useful data. For example extending you example I can imagine situation when container needs 500mb, limit ranger assigned default to 10mb, the container was killed after some time and then was restarted but with request assigned to 500mb by initial resources.\n\n\"1 hour\" means total uptime of all containers running the same image (so 60 containers running for 1 minute are fine).\n",
        "createdAt" : "2015-08-12T19:17:25Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "050208d2-b52b-412c-846a-06e0b7b13b3a",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@AnanyaKumar: If pods get evicted and not restarted on OOMs, it will be difficult to take into account the usage of old incarnations of a pod while creating a new pod. If we consider supporting replication controllers, we can maybe consider using the historical usage of the pods in an RC while creating a new pod in the RC. @piosz: WDYT?\n",
        "createdAt" : "2015-08-12T19:48:18Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "a8a3e9c4-f188-4b4d-ade3-8996a44b5fa8",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "body" : "@piosz I think I didn't communicate what I was saying clearly!\n\nImagine that a Flask container needs 500mb to run properly. We launch 100 of these containers. 95 of them are scheduled onto machines with less than 100mb memory available. These 95 containers use 100mb, and are then killed by the system, (and in the future eventually evicted).\n\nSo we have 95 containers that used 100mb, and 5 containers that used 500mb. The 90th percentile is 100mb. So we will re-launch flask containers with a 100mb request. The flask containers might return to machines with only 100mb available, get killed, and the whole cycle could continue.\n\nIn the worst case, we might never converge on the memory requirements of a container. More practically, converging on the memory requirements of a container could take very long.\n\nInstead, I think we should not take into account the usage of containers that were out of resource killed. These containers were killed before they got the resources they need, so their usage does not accurately represent their requirements. What do you think?\n\n@vishh Yup, I think we should not take into account containers that were OOMed (or old incarnations of containers).\n",
        "createdAt" : "2015-08-13T01:26:08Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "3c734967-9d67-42aa-a243-d401c1524cb6",
        "tags" : [
        ]
      },
      {
        "id" : "d6f9e2a6-3e83-4712-a6da-37baa3e45a4c",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "@AnanyaKumar the expected solution of the problem described by you is not by manipulating data samples and percentages (imagine the situation when neither of the containers won't be scheduled on a machine with 500mb memory available so that we will never converge) but rather by observing OOM and the react to it. In your example containers with OOM will be relaunch with request set to 200mb, then 400mb, and finally 800mb.\n\n@vishh I'm not sure what do you mean. If the pod is evicted because it uses too much memory we should track its reincarnation and increase memory request as described above.\n",
        "createdAt" : "2015-08-13T18:27:44Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "d59e8860-2d26-4061-ba4b-2740dca0c59c",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@piosz: You pretty much summarized what I mentioned earlier! As long as using a pod or RC's recent historical usage for the purposes of initial limits is in the roadmap, I am a happy camper. \n",
        "createdAt" : "2015-08-13T21:44:32Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "124dfb2e-fc1d-41ca-a6f0-f248ae1a9d25",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "@piosz: It's not clear to me that the system is useful without the feature you described (increasing prediction when InitialResources notices OOM). Right now, the doc says you just use 90th percentile usage as the prediction. I would suggest that you consider including the feature in the first version, and updating the doc to reflect this.\n",
        "createdAt" : "2015-08-19T22:53:31Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "3f541a9f-9322-45c1-9496-5c9e1265e9eb",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "@davidopp I don't agree with you that feature without handling OOM in the way that is described above is useless. The prediction based on historical data is usually better than the default one. It doesn't solve OOM problem but the problem exists without the feature as well.\n\nOn the other hand I agree that handling OOM is very important feature and it will be done in the next iteration (especially for v1.1). Let's finish review of this PR and then I will add a new PR modifying this document with proposal how to implement reaction for OOM. WDYT?\n\nAlso I'm adding the mentioned feature to Next Steps paragraph since I haven't done it yet.\n",
        "createdAt" : "2015-08-20T12:42:47Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "339b3115-4326-4291-80f3-b6c3547485f5",
        "parentId" : "567451ef-915e-441c-b26f-5028d6ad92df",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "That's fine but I think you may be under-estimating the importance of the adjustment. We are planning to implement \"put best-effort pods on least-loaded machines according to usage\" once resource API is implemented. So pod that requests no resources might actually have a smaller chance of OOM than a pod that gets a prediction and gets stuck in an OOM loop because of using fixed 90th percentile.\n",
        "createdAt" : "2015-08-21T23:15:49Z",
        "updatedAt" : "2015-08-21T23:15:49Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +74,78 @@* 30 days same image, assuming there is at least 1 sample\n\nIf there is still no data the default value will be set by LimitRanger. Same parameters will be configurable with appropriate flags.\n\n#### Example"
  },
  {
    "id" : "44f6c199-6dac-4bac-bfb2-eb9a01d57179",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c56ece1-7bfa-4a58-acf3-518d0304860a",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "What if we don't have data for the last 7 days? Will we go with whatever data that we have?\n",
        "createdAt" : "2015-08-12T19:43:07Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "27489217-d5b3-424b-9cfb-c7d863b1a86c",
        "parentId" : "6c56ece1-7bfa-4a58-acf3-518d0304860a",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Then we will take a look into data from last 30 days (as described below). If there is still no data the default value will be set by Limit Ranger. The same note as above: the model definition is a subject of improvement. The one described here seems to be a good choice for the first version.\n",
        "createdAt" : "2015-08-13T18:10:34Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : null,
    "diffHunk" : "@@ -1,1 +70,74 @@InitialResources will set Request for both cpu/mem as the 90th percentile of the first (in the following order) set of samples defined in the following way:\n\n* 7 days same image:tag, assuming there is at least 60 samples (1 hour)\n* 30 days same image:tag, assuming there is at least 60 samples (1 hour)\n* 30 days same image, assuming there is at least 1 sample"
  },
  {
    "id" : "9834a86e-b95d-4ef3-82e4-48fb328d2ff2",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1497e453-68f6-487b-b20d-3a8553e3ca1d",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Is the abstraction layer heapster or a different component that interacts with the underlying storage?\n",
        "createdAt" : "2015-08-12T19:58:11Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "0490890a-05ff-4d49-b29c-c1a37f163334",
        "parentId" : "1497e453-68f6-487b-b20d-3a8553e3ca1d",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "As discussed f2f there will be an interface inside of the component with a bunch of possible implementation: GCM, Influxdb and possibly Heapster as well once it will store historical data. The interface should be simple in the first version like:\n\n```\nGetUsagePercentile(resourceType, perc, image, timeInterval)\n```\n",
        "createdAt" : "2015-08-13T18:04:15Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "018560f7-64fc-4047-a5ab-4edbfa5378ab",
        "parentId" : "1497e453-68f6-487b-b20d-3a8553e3ca1d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I talked about this offline with @piosz. For the initial prototyping phase, the plan is to have a custom backend in the new plugin. But very soon, we intend to switch over to heapster APIs that will abstract out the underlying storage.\n",
        "createdAt" : "2015-08-13T21:43:08Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "61090e09-2193-4692-a1e0-fe3bed2fafc0",
        "parentId" : "1497e453-68f6-487b-b20d-3a8553e3ca1d",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Added note about it\n",
        "createdAt" : "2015-08-17T13:17:36Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +90,94 @@* [GCM](../../docs/user-guide/monitoring.md#google-cloud-monitoring) - since GCM is not as powerful as InfluxDB some aggregation will be made on the client side\n\nBoth will be hidden under an abstraction layer, so it would be easy to add another option.\nThe code will be a part of Initial Resources component to not block development, however in the future it should be a part of Heapster.\n"
  },
  {
    "id" : "5b7e2a80-0ba7-4a6c-a5d3-60b7eb476410",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cde9b316-5f3f-4103-acae-12acbd5173a9",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "The way this is written is a little confusing. Maybe adding an example would help. For example (assuming this is correct): \"For example, if we have at least 60 samples from image:tag over the past 7 days, we will use the 90th percentile of all of the samples of image:tag over the past 7 days. Otherwise, if we have at least 60 samples from image:tag over the past 30 days, we will use the 90th percentile of all of the samples over of image:tag the past 30 days. Otherwise, if we have at least 1 sample from image over the past 30 days, we will use that the 90th percentile of all of the samples of image over the past 30 days.\"\n",
        "createdAt" : "2015-08-19T22:40:03Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "59b01288-8f92-412a-9076-9e17402cada1",
        "parentId" : "cde9b316-5f3f-4103-acae-12acbd5173a9",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Added Example paragraph\n",
        "createdAt" : "2015-08-20T12:48:41Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +68,72 @@CPU/memory usage of each container is exported periodically (by default with 1 minute resolution) to the backend (see more in [Monitoring pipeline](#monitoring-pipeline)).\n\nInitialResources will set Request for both cpu/mem as the 90th percentile of the first (in the following order) set of samples defined in the following way:\n\n* 7 days same image:tag, assuming there is at least 60 samples (1 hour)"
  },
  {
    "id" : "d69d31b9-bc5f-4542-8abf-e87ea6173a91",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21d55671-fce0-4a68-adf6-5a49a2d53dc3",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Is there any way to say \"I want the default specified by LimitRanger to be used, not a prediction from InitialResources component\"? \n",
        "createdAt" : "2015-08-19T22:41:56Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "122a0745-835b-4b10-9804-0be9d2dfd41a",
        "parentId" : "21d55671-fce0-4a68-adf6-5a49a2d53dc3",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "I think everyone agrees that predicting should be a default option. So that the option you mentioned may be introduced easily later. Since the change affects to our API I'd prefer to not adding it in the first version and later if there would be any case where default option is better than one predicted by InitialResources or anyone that complains about lack of the option we will do it. Especially there is simple workaround to just set defaults manually.\n",
        "createdAt" : "2015-08-20T12:33:05Z",
        "updatedAt" : "2015-08-20T13:18:49Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "90668b54-5bb2-4ed2-a6c0-ca514611cdfc",
        "parentId" : "21d55671-fce0-4a68-adf6-5a49a2d53dc3",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : ">  Especially there is simple workaround to just set defaults manually.\n\nSure, but by that reasoning there's no reason to have default in LimitRanger at all, since you can always just set it manually.\n",
        "createdAt" : "2015-08-21T23:18:15Z",
        "updatedAt" : "2015-08-21T23:18:15Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +49,53 @@InitialResources component will be implemented as an [admission plugin](../../plugin/pkg/admission/) and invoked right before\n[LimitRanger](https://github.com/GoogleCloudPlatform/kubernetes/blob/7c9bbef96ed7f2a192a1318aa312919b861aee00/cluster/gce/config-default.sh#L91).\nFor every container without Resources specified it will try to predict amount of resources that should be sufficient for it.\nSo that a pod without specified resources will be treated as\n[Burstable](https://github.com/GoogleCloudPlatform/kubernetes/blob/be5e224a0f1c928d49c48aa6a6539d22c47f9238/docs/proposals/resource-qos.md#qos-classes)."
  },
  {
    "id" : "ff5afc66-4531-4d72-aaaf-d00c2f7a9469",
    "prId" : 12472,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d17173f-0940-47ec-b7a3-d00f367eb8cf",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "These should all have been relative links. Please fix them.\n",
        "createdAt" : "2015-08-27T20:19:37Z",
        "updatedAt" : "2015-08-27T20:19:37Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "1d00a22a-694f-4904-9889-14d961559370",
        "parentId" : "2d17173f-0940-47ec-b7a3-d00f367eb8cf",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "The reason why I don't use relative link in lines 42, 50, 55 is that the links point to specific lines in the code in other documents. The content of those lines will probably change many times so that it's hard to guarantee that they will point to what I originally wanted.\n\nI'll change this line because it points to paragraph in other doc which should be rather stable.\n",
        "createdAt" : "2015-08-28T09:22:03Z",
        "updatedAt" : "2015-08-28T09:22:03Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "83ff34221fbb3667cabb0e6e0f2762c71a0745bf",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +51,55 @@For every container without Resources specified it will try to predict amount of resources that should be sufficient for it.\nSo that a pod without specified resources will be treated as\n[Burstable](https://github.com/GoogleCloudPlatform/kubernetes/blob/be5e224a0f1c928d49c48aa6a6539d22c47f9238/docs/proposals/resource-qos.md#qos-classes).\n\nInitialResources will set only [Request](https://github.com/GoogleCloudPlatform/kubernetes/blob/3d2d99c6fd920386eea4ec050164839ec6db38f0/pkg/api/v1/types.go#L665)"
  }
]