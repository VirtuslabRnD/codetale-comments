[
  {
    "id" : "2f9a7cda-5c23-403c-8277-af77bfd85174",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-592528032",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3eb1851f-74d2-4eab-8377-90cd65677c6b",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "This can throw StaleBrokerEpochException. It would be useful for KafkaEventQueue.run() to log the event associated with the exception. ",
        "createdAt" : "2021-02-11T01:33:46Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "463816ad-498d-4e71-b065-6e8bb5f483a2",
        "parentId" : "3eb1851f-74d2-4eab-8377-90cd65677c6b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "`handleEventException` handles logging exceptions thrown by events.",
        "createdAt" : "2021-02-16T20:48:46Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "f8f24323-dc9a-4149-8101-8ad7411f860d",
        "parentId" : "3eb1851f-74d2-4eab-8377-90cd65677c6b",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It seems that we are logging at the debug level. I am wondering if we should log at WARN as before in ZK based appoach.\r\n\r\n```\r\n        if (exception instanceof ApiException) {\r\n            log.debug(\"{}: failed with {} in {} us\", name,\r\n                exception.getClass().getSimpleName(), deltaUs);\r\n            return exception;\r\n        }\r\n\r\n```",
        "createdAt" : "2021-02-17T01:31:42Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "76cad274-2996-4865-9a73-d6b58ebe2d58",
        "parentId" : "3eb1851f-74d2-4eab-8377-90cd65677c6b",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "We can get here just because the user made an invalid RPC, so I don't know if WARN is appropriate.  I'll change it to INFO for now.  ",
        "createdAt" : "2021-02-17T19:01:51Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 559,
    "diffHunk" : "@@ -1,1 +557,561 @@\n    ControllerResult<AlterIsrResponseData> alterIsr(AlterIsrRequestData request) {\n        clusterControl.checkBrokerEpoch(request.brokerId(), request.brokerEpoch());\n        AlterIsrResponseData response = new AlterIsrResponseData();\n        List<ApiMessageAndVersion> records = new ArrayList<>();"
  },
  {
    "id" : "a137dab8-4b31-4d19-98cb-15382d4ffeb8",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-591621664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a486b87b-ca40-45e2-8fc4-72d826e55d4e",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "In the ZK case, we use the ZK version to do conditional updates. In Raft, could we associated each partitionState with the offset in the Raft log and use that as partitionEpoch for conditional updates? This way, we don't need to explicitly maintain a separate partitionEpoch field and the epoch is automatically bumped up for any change to the partition record, not just for leader and isr.",
        "createdAt" : "2021-02-11T01:39:11Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "3d0b07e9-c84c-46d7-91ad-584eede94ee7",
        "parentId" : "a486b87b-ca40-45e2-8fc4-72d826e55d4e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Yes, I think that could work for partition epoch.  Let's do that once we have the initial code in, though...",
        "createdAt" : "2021-02-16T21:02:02Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 590,
    "diffHunk" : "@@ -1,1 +588,592 @@                    continue;\n                }\n                if (partitionData.currentIsrVersion() != partition.partitionEpoch) {\n                    responseTopicData.partitions().add(new AlterIsrResponseData.PartitionData().\n                        setPartitionIndex(partitionData.partitionIndex())."
  },
  {
    "id" : "bc604eda-fb41-4615-8be9-b5bf57f56f16",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-589716082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e31c7fc3-b571-44f2-9e53-2c31000bd462",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add some comment for this class?",
        "createdAt" : "2021-02-12T22:20:43Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +80,84 @@ * of each partition, as well as administrative tasks like creating or deleting topics.\n */\npublic class ReplicationControlManager {\n    /**\n     * A special value used to represent the leader for a partition with no leader. "
  },
  {
    "id" : "d4ed0dcd-c2e4-4ecf-8bf7-3e0b2e38b424",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-592609398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99e12181-198c-46ab-a75a-0f3954834c91",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Are we deprecating the state-change log and the controller log that we had before?",
        "createdAt" : "2021-02-12T22:28:11Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "2bd6b1d2-6ef2-4bcd-a83a-cdc107fabf50",
        "parentId" : "99e12181-198c-46ab-a75a-0f3954834c91",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I don't think the state change log can scale to the number of partitions we need.  It gets too verbose.  Also, this information is available in the metadata log itself.",
        "createdAt" : "2021-02-17T20:41:57Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 315,
    "diffHunk" : "@@ -1,1 +313,317 @@        PartitionControlInfo prevPartInfo = topicInfo.parts.get(record.partitionId());\n        if (prevPartInfo == null) {\n            log.info(\"Created partition {}:{} with {}.\", record.topicId(),\n                record.partitionId(), newPartInfo.toString());\n            topicInfo.parts.put(record.partitionId(), newPartInfo);"
  },
  {
    "id" : "03b10b33-9ac5-4a64-bbdf-53a057a9406b",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-594683396",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26ed7eb1-b544-4fee-aad7-6217ea9dbf34",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Could we add a TODO for handling the preferred leader election?",
        "createdAt" : "2021-02-17T23:52:30Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "87a6a725-fcf2-433c-b673-d532166cbcdf",
        "parentId" : "26ed7eb1-b544-4fee-aad7-6217ea9dbf34",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Hmm, I thought this already handles preferred leader election (there are only two options, PREFERRED and UNCLEAN, so far...)",
        "createdAt" : "2021-02-18T17:52:32Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "d480d8a9-5feb-4cea-b263-59b73d01f731",
        "parentId" : "26ed7eb1-b544-4fee-aad7-6217ea9dbf34",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2.",
        "createdAt" : "2021-02-18T18:55:42Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "54a62653-2925-45ec-9878-3998bc3e178b",
        "parentId" : "26ed7eb1-b544-4fee-aad7-6217ea9dbf34",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2.\r\n\r\n\r\n> \r\n> Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred?\r\n\r\nWell, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance.",
        "createdAt" : "2021-02-19T02:14:23Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "7eb25f0e-d924-4ca8-939d-84a411c83144",
        "parentId" : "26ed7eb1-b544-4fee-aad7-6217ea9dbf34",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "ack.  I fixed this",
        "createdAt" : "2021-02-20T01:21:30Z",
        "updatedAt" : "2021-02-20T01:21:30Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 755,
    "diffHunk" : "@@ -1,1 +753,757 @@\n    ControllerResult<ElectLeadersResponseData> electLeaders(ElectLeadersRequestData request) {\n        boolean unclean = electionIsUnclean(request.electionType());\n        List<ApiMessageAndVersion> records = new ArrayList<>();\n        ElectLeadersResponseData response = new ElectLeadersResponseData();"
  },
  {
    "id" : "e23736b0-3286-4263-ac92-5354bc594e31",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-593495905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f003ae64-651f-4719-951e-ab46d5581887",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Is this check already implied since we are iterating `brokersToIsrs`?",
        "createdAt" : "2021-02-18T00:51:31Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "8305a096-08c9-48e7-9829-3320f15573e7",
        "parentId" : "f003ae64-651f-4719-951e-ab46d5581887",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "We're iterating over the partitions with no leader, which may or may not have the newly activated broker in their ISR.",
        "createdAt" : "2021-02-18T17:55:39Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 745,
    "diffHunk" : "@@ -1,1 +743,747 @@            // TODO: if this partition is configured for unclean leader election,\n            // check the replica set rather than the ISR.\n            if (Replicas.contains(partition.isr, brokerId)) {\n                records.add(new ApiMessageAndVersion(new PartitionChangeRecord().\n                    setPartitionId(topicPartition.partitionId())."
  },
  {
    "id" : "842038ce-15c4-43b6-a296-ffcb7937baca",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-594435064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd67d1a5-63d9-4e73-9869-7f4d5e7d7465",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "> It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline). If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1. Maybe we can tune this a bit later?\r\n\r\nIt's fine to revisit that later. The tradeoff is that if we wait, it slightly increases the probability of availability since another replica could join isr.",
        "createdAt" : "2021-02-18T18:51:14Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "7eec415b-ff28-43e2-80e6-2257158e3583",
        "parentId" : "dd67d1a5-63d9-4e73-9869-7f4d5e7d7465",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "This is resolved in the latest version of the code, where we disable metadata updates on the shutting down broker before starting controlled shutdown, and bump the leader epoch of all partitions.",
        "createdAt" : "2021-02-19T18:04:52Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 850,
    "diffHunk" : "@@ -1,1 +848,852 @@                    // shutting down broker is in here.  This prevents the broker from\n                    // getting re-added to the ISR later.\n                    handleNodeDeactivated(brokerId, records);\n                    break;\n                case SHUTDOWN_NOW:"
  },
  {
    "id" : "75225e4b-d734-48fb-b117-de0014373308",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-594439509",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d063767e-8f85-4a6d-a39e-c23be6842c59",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "As Jason pointed out, in ZK based approach, the controller bumps up the leader epoch for removing replica from ISR too.\r\n\r\nAlso, since the broker is no longer receiving the leaderAndIsr requests, we need some logic for the broker to ignore the new partition record (for follower fetching) once it starts the controlled shutdown process.",
        "createdAt" : "2021-02-19T02:17:34Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "4d500a3f-4067-4b9d-a318-c7b0934ff982",
        "parentId" : "d063767e-8f85-4a6d-a39e-c23be6842c59",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I changed this so that the leader epoch is bumped if and only if there is a leader present in the PartitionChange record.  (It is possible to bump the epoch without changing the leader by including the same leader again in the record.)\r\n\r\nWe now use this during controlled shutdown to unconditionally bump the leader epochs.  Otherwise, we only bump the leader epochs if the leader changed.",
        "createdAt" : "2021-02-19T18:10:42Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 682,
    "diffHunk" : "@@ -1,1 +680,684 @@                setPartitionId(topicPartition.partitionId()).\n                setTopicId(topic.id);\n            int[] newIsr = Replicas.copyWithout(partition.isr, brokerId);\n            if (newIsr.length == 0) {\n                // We don't want to shrink the ISR to size 0. So, leave the node in the ISR."
  },
  {
    "id" : "87882cbc-d686-45df-a34e-12ff62e9dd1d",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-594661287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ecd5e61-56cb-4b3f-bd62-137f41fa3a4e",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Currently, for leader initiated AlterIsr request, the controller doesn't bump up the leader epoch. If we change that, it will slightly increase unavailability since all clients have to refresh the metadata in this case.",
        "createdAt" : "2021-02-19T19:03:59Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "dc3ea9bb-4f9a-4b9e-8f21-df6039c59ce9",
        "parentId" : "6ecd5e61-56cb-4b3f-bd62-137f41fa3a4e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Hmm... ReplicationControlManager should not allow this to happen during an alter isr request.  There is some code that checks if the alter isr request is attempting to remove the current leader from the isr, and returns an error if so.  So the leader should not be changed by an alter isr request and therefore the leader epoch will not be.\r\n\r\n```\r\n                if (!Replicas.contains(newIsr, partition.leader)) {\r\n                    // An alterIsr request can't remove the current leader.\r\n                    responseTopicData.partitions().add(new AlterIsrResponseData.PartitionData().\r\n                        setPartitionIndex(partitionData.partitionIndex()).\r\n                        setErrorCode(Errors.INVALID_REQUEST.code()));\r\n                    continue;\r\n                }\r\n```",
        "createdAt" : "2021-02-19T20:41:39Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "12889461-47ab-4b70-8ae9-55e7419f17f1",
        "parentId" : "6ecd5e61-56cb-4b3f-bd62-137f41fa3a4e",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Yes, the alterIsr doesn't change leader, but generates a PartitionChangeRecord. On replaying this record, the code following code bumps on leaderEpoch?\r\n\r\n`        PartitionControlInfo newPartitionInfo = prevPartitionInfo.merge(record);`",
        "createdAt" : "2021-02-19T21:52:36Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "251911e5-b388-4f26-a822-054ca7f1f6b0",
        "parentId" : "6ecd5e61-56cb-4b3f-bd62-137f41fa3a4e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "merge only bumps the epoch if the leader was set.\r\n\r\n```\r\n        PartitionControlInfo merge(PartitionChangeRecord record) {\r\n            int[] newIsr = (record.isr() == null) ? isr : Replicas.toArray(record.isr());\r\n            int newLeader;\r\n            int newLeaderEpoch;\r\n            if (record.leader() == NO_LEADER_CHANGE) {\r\n                newLeader = leader;\r\n                newLeaderEpoch = leaderEpoch;\r\n            } else {\r\n                newLeader = record.leader();\r\n                newLeaderEpoch = leaderEpoch + 1;\r\n            }\r\n            return new PartitionControlInfo(replicas,\r\n                newIsr,\r\n                removingReplicas,\r\n                addingReplicas,\r\n                newLeader,\r\n                newLeaderEpoch,\r\n                partitionEpoch + 1);\r\n        }\r\n```",
        "createdAt" : "2021-02-20T00:10:42Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +142,146 @@            } else {\n                newLeader = record.leader();\n                newLeaderEpoch = leaderEpoch + 1;\n            }\n            return new PartitionControlInfo(replicas,"
  },
  {
    "id" : "b2d6175a-c2a8-495d-b610-297d04f4af7b",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-594633387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f57d6a3f-7f05-4806-bcb7-8fbe3085536e",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Hmm, merge bumps up the leaderEpoch. It seems that this needs to be persisted in the metadata log?",
        "createdAt" : "2021-02-19T19:54:01Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "86b46337-3eb3-4e2f-8bf1-01fbcedf4798",
        "parentId" : "f57d6a3f-7f05-4806-bcb7-8fbe3085536e",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "The leader epoch is managed implicitly -- every time a PartitionChangeRecord appears, the epoch is bumped if the leader is not NO_LEADER_CHANGE.",
        "createdAt" : "2021-02-19T22:48:43Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 344,
    "diffHunk" : "@@ -1,1 +342,346 @@                \":\" + record.partitionId() + \", but no partition with that id was found.\");\n        }\n        PartitionControlInfo newPartitionInfo = prevPartitionInfo.merge(record);\n        topicInfo.parts.put(record.partitionId(), newPartitionInfo);\n        brokersToIsrs.update(record.topicId(), record.partitionId(),"
  },
  {
    "id" : "8b7e04d6-6f17-4df2-9df0-e3ab3999d4cb",
    "prId" : 10070,
    "prUrl" : "https://github.com/apache/kafka/pull/10070#pullrequestreview-594667179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eea8648-8a45-4bef-ad77-6a31c9f38f85",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Hmm, we should set the leader to NO_LEADER_CHANGE, right?",
        "createdAt" : "2021-02-19T23:28:15Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "a545efbc-4110-4960-87d5-5217c7c8ac4f",
        "parentId" : "6eea8648-8a45-4bef-ad77-6a31c9f38f85",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "That is the default so we don't need to set it unless we're changing it",
        "createdAt" : "2021-02-20T00:07:59Z",
        "updatedAt" : "2021-02-20T00:12:14Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "b3cf6350-e7a0-4c08-9b13-cb624007e057",
        "parentId" : "6eea8648-8a45-4bef-ad77-6a31c9f38f85",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "We have NO_LEADER_CHANGE as the default for the serialized data. However, the active controller replays the PartitionChangeRecord created in memory, which defaults leader to NO_LEADER_CHANGE, right?",
        "createdAt" : "2021-02-20T00:35:14Z",
        "updatedAt" : "2021-02-20T00:37:20Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bcdd2ec50b7fbacffcb3c5810a37ae8161a64c3",
    "line" : 610,
    "diffHunk" : "@@ -1,1 +608,612 @@                    continue;\n                }\n                records.add(new ApiMessageAndVersion(new PartitionChangeRecord().\n                    setPartitionId(partitionData.partitionIndex()).\n                    setTopicId(topic.id)."
  },
  {
    "id" : "ae7c2cc8-9ca9-42e3-9de9-b6b6689f538e",
    "prId" : 10184,
    "prUrl" : "https://github.com/apache/kafka/pull/10184#pullrequestreview-599923433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b489e6ed-3f85-480f-bf41-c0940b4bb9c2",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Should we remove deleted topic name from `topicsByName` also?",
        "createdAt" : "2021-02-26T08:08:04Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "f5f1810f-c7ee-4165-8846-29c37355f6fb",
        "parentId" : "b489e6ed-3f85-480f-bf41-c0940b4bb9c2",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Good catch.  Fixed.",
        "createdAt" : "2021-02-26T20:36:21Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "6aec8bdb9327d2f72b67f7acc8e158d737491711",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +360,364 @@    }\n\n    public void replay(RemoveTopicRecord record) {\n        // Remove this topic from the topics map and the topicsByName map.\n        TopicControlInfo topic = topics.remove(record.topicId());"
  },
  {
    "id" : "b3fcf739-6037-4f2c-9f63-69fb80b83fe3",
    "prId" : 10184,
    "prUrl" : "https://github.com/apache/kafka/pull/10184#pullrequestreview-602314452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a59a0e5-cd34-44b0-8395-dd4b04f11927",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Should we update brokersToIsrs too?",
        "createdAt" : "2021-02-26T23:36:34Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "016825e9-d0b2-4e79-b626-a1db0d618989",
        "parentId" : "7a59a0e5-cd34-44b0-8395-dd4b04f11927",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "The ISRs should already have been updated by `BrokerChangeRecords` that were previously replayed.",
        "createdAt" : "2021-02-27T01:03:31Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "f2c2858e-f46e-49f6-8ed5-2195838b2d73",
        "parentId" : "7a59a0e5-cd34-44b0-8395-dd4b04f11927",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Hmm, you mean PartitionChangeRecord? I don't see PartitionChangeRecord being generated from the topicDeletion request.",
        "createdAt" : "2021-02-27T02:04:46Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "82b6ee8e-72e9-4b42-85a3-b85b9bb1c6d6",
        "parentId" : "7a59a0e5-cd34-44b0-8395-dd4b04f11927",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Sorry, you're right: we need to remove this from `brokersToIsrs`.  Fixed.",
        "createdAt" : "2021-03-02T22:03:34Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "6aec8bdb9327d2f72b67f7acc8e158d737491711",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +367,371 @@                \" to remove.\");\n        }\n        topicsByName.remove(topic.name);\n\n        // Delete the configurations associated with this topic."
  },
  {
    "id" : "1c883fdd-9947-4acd-9953-7271c466cb75",
    "prId" : 10184,
    "prUrl" : "https://github.com/apache/kafka/pull/10184#pullrequestreview-600054750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc3d11cb-b99a-4df6-9118-c21ae2f77061",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "I guess we haven't hooked up the logic to trigger the deletion of the replicas of the deleted topic in the broker?",
        "createdAt" : "2021-02-26T23:42:30Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "7ffbcba3-2cba-4bea-9ef7-d65737961bbf",
        "parentId" : "bc3d11cb-b99a-4df6-9118-c21ae2f77061",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "We haven't hooked that up yet, correct.  But that logic is in `BrokerMetadataListener`. It would probably be better to have a separate PR for that.",
        "createdAt" : "2021-02-27T01:08:52Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "6aec8bdb9327d2f72b67f7acc8e158d737491711",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +381,385 @@\n        log.info(\"Removed topic {} with ID {}.\", topic.name, record.topicId());\n    }\n\n    ControllerResult<CreateTopicsResponseData>"
  },
  {
    "id" : "61fbc456-cc41-4fb2-b59c-c6c8e2cfc27a",
    "prId" : 10184,
    "prUrl" : "https://github.com/apache/kafka/pull/10184#pullrequestreview-603712651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "beb67357-177d-4893-9dd5-ffa04c7112f1",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Hmm, why do we need to remove for -1 broker? It doesn't seem that brokersToIsrs tracks that.\r\n",
        "createdAt" : "2021-03-03T23:46:16Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "0b96ad1f-0e3b-435f-98c4-5b3426c82f90",
        "parentId" : "beb67357-177d-4893-9dd5-ffa04c7112f1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "The test case `BrokersToIsrsTest.testNoLeader` suggests that it is a possible case. It looks like the path through `ReplicationControlManager.handleNodeDeactivated` could result in a `PartitionChangeRecord` which has leaderId set to -1. ",
        "createdAt" : "2021-03-04T02:09:05Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "e87b0375-691c-4c34-a327-64517df7a73d",
        "parentId" : "beb67357-177d-4893-9dd5-ffa04c7112f1",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It's true and a partition could have isr and no leader. However, in that case, `isrMembers` in brokersToIsrs will still be updated with key from replicaId in isr and isr will never have -1 in its list. The noLeader info is only stored in the value of `isrMembers`.",
        "createdAt" : "2021-03-04T02:22:47Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "20e9d94f-26d2-443f-9750-72e8bd744195",
        "parentId" : "beb67357-177d-4893-9dd5-ffa04c7112f1",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I stepped through `testNoLeader` and it seems that -1 can indeed be a key in `isrMembers`. The `noLeaderIterator` makes the expectation explicit. ",
        "createdAt" : "2021-03-04T02:38:02Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "d83e6bbd-5105-4117-8879-2551767949cc",
        "parentId" : "beb67357-177d-4893-9dd5-ffa04c7112f1",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Got it. The following comment confirmed this.\r\n\r\n```\r\n    /**\r\n     * A map of broker IDs to the partitions that the broker is in the ISR for.\r\n     * Partitions with no isr members appear in this map under id NO_LEADER.\r\n     */\r\n    private final TimelineHashMap<Integer, TimelineHashMap<Uuid, int[]>> isrMembers;\r\n\r\n```",
        "createdAt" : "2021-03-04T05:20:50Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6aec8bdb9327d2f72b67f7acc8e158d737491711",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +378,382 @@            }\n        }\n        brokersToIsrs.removeTopicEntryForBroker(topic.id, NO_LEADER);\n\n        log.info(\"Removed topic {} with ID {}.\", topic.name, record.topicId());"
  },
  {
    "id" : "5648b089-8323-423e-94e5-fb8a47a6e9ea",
    "prId" : 10184,
    "prUrl" : "https://github.com/apache/kafka/pull/10184#pullrequestreview-603683234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf49562f-813d-43b2-8a23-55b80fa16d85",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "Should it check `ZERO_UUID` (use `INVALID_REQUEST` instead of `UNKNOWN_TOPIC_ID`)?",
        "createdAt" : "2021-03-04T04:31:08Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "6aec8bdb9327d2f72b67f7acc8e158d737491711",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +595,599 @@    Map<Uuid, ResultOrError<String>> findTopicNames(long offset, Collection<Uuid> ids) {\n        Map<Uuid, ResultOrError<String>> results = new HashMap<>(ids.size());\n        for (Uuid id : ids) {\n            if (id == null || id.equals(Uuid.ZERO_UUID)) {\n                results.put(id, new ResultOrError<>(new ApiError(INVALID_REQUEST,"
  },
  {
    "id" : "c78beabb-009e-481f-b512-ee5c904b5387",
    "prId" : 10184,
    "prUrl" : "https://github.com/apache/kafka/pull/10184#pullrequestreview-603683234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33bc6266-fc2b-4d20-9d00-508511063dfa",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "At this static member is imported, we can replace all `Errors.INVALID_REQUEST` by `INVALID_REQUEST` in this class.",
        "createdAt" : "2021-03-04T04:37:22Z",
        "updatedAt" : "2021-03-04T17:23:46Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      }
    ],
    "commit" : "6aec8bdb9327d2f72b67f7acc8e158d737491711",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +77,81 @@import static org.apache.kafka.clients.admin.AlterConfigOp.OpType.SET;\nimport static org.apache.kafka.common.config.ConfigResource.Type.TOPIC;\nimport static org.apache.kafka.common.protocol.Errors.INVALID_REQUEST;\nimport static org.apache.kafka.common.protocol.Errors.UNKNOWN_TOPIC_ID;\nimport static org.apache.kafka.common.protocol.Errors.UNKNOWN_TOPIC_OR_PARTITION;"
  },
  {
    "id" : "f9c08a85-fd96-4ccc-a7d0-9683b50ebd35",
    "prId" : 10252,
    "prUrl" : "https://github.com/apache/kafka/pull/10252#pullrequestreview-604370743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7a3930c-49a9-4893-9449-2dc26fa11686",
        "parentId" : null,
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Note that we have switched from a regular `Random` to a `SecureRandom`. Probably a good idea, but it could have perf implications.",
        "createdAt" : "2021-03-04T05:16:46Z",
        "updatedAt" : "2021-03-08T17:16:27Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      },
      {
        "id" : "a509c82d-54b9-4697-8f9f-9b0104b4b4f0",
        "parentId" : "b7a3930c-49a9-4893-9449-2dc26fa11686",
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "Yeah, I'm aware. But it felt like something that had to be done. The rate of topic creation is typically not high anyway.",
        "createdAt" : "2021-03-04T17:29:40Z",
        "updatedAt" : "2021-03-08T17:16:27Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "a1a4e517-2ba6-453b-83b2-31e415dbf550",
        "parentId" : "b7a3930c-49a9-4893-9449-2dc26fa11686",
        "authorId" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "body" : "Can we note this in the PR description?",
        "createdAt" : "2021-03-04T17:35:09Z",
        "updatedAt" : "2021-03-08T17:16:27Z",
        "lastEditedBy" : "d8c7cf80-a55a-474c-a4f4-f60a9efda52c",
        "tags" : [
        ]
      }
    ],
    "commit" : "85c26fab544a58ad390d69130b5635c2ca7ef56e",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +509,513 @@            }\n        }\n        Uuid topicId = Uuid.randomUuid();\n        successes.put(topic.name(), new CreatableTopicResult().\n            setName(topic.name())."
  },
  {
    "id" : "3521b025-a37c-44d1-90e0-620909151fe7",
    "prId" : 10343,
    "prUrl" : "https://github.com/apache/kafka/pull/10343#pullrequestreview-630676439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "666667e3-c330-4e55-8633-656d43a8d3e0",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "nit: it would improve readability to factor out some functions for some of the work here. Here we can have a separate function with a nice name for building the assignments",
        "createdAt" : "2021-04-07T21:28:28Z",
        "updatedAt" : "2021-04-13T17:58:03Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "b6ec1506-7274-4412-9f18-c1211f403014",
        "parentId" : "666667e3-c330-4e55-8633-656d43a8d3e0",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Yes, let's factor out the validation logic into a separate function.",
        "createdAt" : "2021-04-07T21:59:49Z",
        "updatedAt" : "2021-04-13T17:58:03Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "c55b26a421a7e5b1b79d77f69d6802d8a961bbf7",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +1064,1068 @@        if (topic.assignments() != null) {\n            placements = new ArrayList<>();\n            for (CreatePartitionsAssignment assignment : topic.assignments()) {\n                validateManualPartitionAssignment(assignment.brokerIds(),\n                    OptionalInt.of(replicationFactor));"
  },
  {
    "id" : "c12121d7-aa6d-49aa-9809-ffaa2d947223",
    "prId" : 10343,
    "prUrl" : "https://github.com/apache/kafka/pull/10343#pullrequestreview-634869257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e909e763-b2b2-4364-b026-24a10651c9d0",
        "parentId" : null,
        "authorId" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "body" : "I guess this logic is consistent with the current implementation. It might have been nice to make this an idempotent operation.",
        "createdAt" : "2021-04-09T01:27:34Z",
        "updatedAt" : "2021-04-13T17:58:03Z",
        "lastEditedBy" : "5c21df64-97d8-46ab-9722-a7e9ba1d7c49",
        "tags" : [
        ]
      },
      {
        "id" : "da71fed1-fa41-4527-a1fd-35b723550526",
        "parentId" : "e909e763-b2b2-4364-b026-24a10651c9d0",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Yeah, for now I'm just being consistent with the current implementation.  I think eventually we will want a retransmission cache to solve all these problems at once.",
        "createdAt" : "2021-04-13T17:46:49Z",
        "updatedAt" : "2021-04-13T17:58:03Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "c55b26a421a7e5b1b79d77f69d6802d8a961bbf7",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +1031,1035 @@            throw new UnknownTopicOrPartitionException();\n        }\n        if (topic.count() == topicInfo.parts.size()) {\n            throw new InvalidPartitionsException(\"Topic already has \" +\n                topicInfo.parts.size() + \" partition(s).\");"
  },
  {
    "id" : "773f1788-87c7-43f8-ad98-39342350a6a0",
    "prId" : 10564,
    "prUrl" : "https://github.com/apache/kafka/pull/10564#pullrequestreview-641031504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "810b969c-ad53-4fa5-b303-fddf1445ec1a",
        "parentId" : null,
        "authorId" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "body" : "The `newLeader` is `NO_LEADER` (due to empty `newIsr`) so we could set `NO_LEADER` to `record`. Does it make sense?",
        "createdAt" : "2021-04-21T08:44:14Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "0f776378-bb23-4193-9bb0-1db5973f3781",
        "tags" : [
        ]
      },
      {
        "id" : "d93a5bc3-4561-4567-8eb1-afc51499f916",
        "parentId" : "810b969c-ad53-4fa5-b303-fddf1445ec1a",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I agree that if `newIsr.length` is 0 on line 1080, then `newLeader` must be `NO_LEADER`.  However, it's not necessary to add any special logic for this since `bestLeader` already handles this case.",
        "createdAt" : "2021-04-21T12:56:52Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "39ed3f8abae2035a27c321e08f9cfce71974514c",
    "line" : 429,
    "diffHunk" : "@@ -1,1 +1105,1109 @@                newIsr = new int[] {newLeader};\n            } else if (newIsr.length == 0) {\n                // We never want to shrink the ISR to size 0.\n                newIsr = partition.isr;\n            }"
  },
  {
    "id" : "eeb6ba0b-2bd2-46bd-bbf9-f870b18ff872",
    "prId" : 10564,
    "prUrl" : "https://github.com/apache/kafka/pull/10564#pullrequestreview-641842571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "951805c1-a6a1-4fe0-a8d7-f62f1cbde140",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "We use to log all leader and isr changes in info even for clean leader election. ",
        "createdAt" : "2021-04-21T20:34:54Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "ae595546-e8ed-4f89-ae75-d57ea9f6aaae",
        "parentId" : "951805c1-a6a1-4fe0-a8d7-f62f1cbde140",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I don't think this will be practical as the number of partitions grows.  A 10-node cluster with a million partitions could have tens of thousands of partition changes when a node goes away or comes back.",
        "createdAt" : "2021-04-22T07:04:06Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "39ed3f8abae2035a27c321e08f9cfce71974514c",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +219,223 @@                log.info(\"UNCLEAN partition change for {}: {}\", description, diff(prev));\n            } else if (log.isDebugEnabled()) {\n                log.debug(\"partition change for {}: {}\", description, diff(prev));\n            }\n        }"
  },
  {
    "id" : "1eface3c-9331-421d-84be-819b86c3c5c3",
    "prId" : 10564,
    "prUrl" : "https://github.com/apache/kafka/pull/10564#pullrequestreview-646401219",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e8e79d6-97ff-4914-b0cb-f4a01ad1baf5",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Instead of relying upon NO_LEADER, could we make brokerToRemove and brokerToAdd optional int to make them clearer?",
        "createdAt" : "2021-04-21T20:42:55Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "a912daad-2125-4e06-b693-67f866cd6998",
        "parentId" : "2e8e79d6-97ff-4914-b0cb-f4a01ad1baf5",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "What about this place? It seems that Instead of relying upon NO_LEADER, it might be clearer to make brokerToRemove and brokerToAdd optional int?",
        "createdAt" : "2021-04-22T21:51:33Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "ba9ff429-4410-42ea-ac68-7c8eede59b03",
        "parentId" : "2e8e79d6-97ff-4914-b0cb-f4a01ad1baf5",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "I think it's easier to use NO_LEADER since then I don't have to special-case the comparisons (I can just compare with NO_LEADER and it will fail when examining ISR members, for example).",
        "createdAt" : "2021-04-27T22:09:13Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "39ed3f8abae2035a27c321e08f9cfce71974514c",
    "line" : 388,
    "diffHunk" : "@@ -1,1 +1064,1068 @@     *\n     * @param context           A human-readable context string used in log4j logging.\n     * @param brokerToRemove    NO_LEADER if no broker is being removed; the ID of the\n     *                          broker to remove from the ISR and leadership, otherwise.\n     * @param brokerToAdd       NO_LEADER if no broker is being added; the ID of the"
  },
  {
    "id" : "6f3fe4fd-708b-4e0c-a322-805f0fea5a66",
    "prId" : 10564,
    "prUrl" : "https://github.com/apache/kafka/pull/10564#pullrequestreview-641840225",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65df6dca-2158-4bf0-9faa-f8aba72be6b3",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Would it be better to return optional int here?",
        "createdAt" : "2021-04-21T20:53:06Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "331318a1-dde9-40e6-be63-6d1c451e3e4f",
        "parentId" : "65df6dca-2158-4bf0-9faa-f8aba72be6b3",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "We use NO_LEADER in a lot of different places in the code, so I think it's reasonable to use it here.  it is actually a valid value for the leader of the partition, so translating to and from OptionalInt would be awkward, I think.",
        "createdAt" : "2021-04-22T07:01:04Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "39ed3f8abae2035a27c321e08f9cfce71974514c",
    "line" : 350,
    "diffHunk" : "@@ -1,1 +896,900 @@    }\n\n    static int bestLeader(int[] replicas, int[] isr, boolean uncleanOk,\n                          Function<Integer, Boolean> isAcceptableLeader) {\n        int bestUnclean = NO_LEADER;"
  },
  {
    "id" : "6cd50dbb-40d9-4701-9746-8bd192054121",
    "prId" : 10564,
    "prUrl" : "https://github.com/apache/kafka/pull/10564#pullrequestreview-646420127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28607176-977c-4f66-8b44-4c355bc15cf9",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.",
        "createdAt" : "2021-04-21T21:13:47Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "ec1a20fb-4e37-4531-b60f-422a5f19b585",
        "parentId" : "28607176-977c-4f66-8b44-4c355bc15cf9",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "That will block the controlled shutdown from finishing in many cases.  Is the intention to allow for a clean leader election later on if the other replicas catch up?",
        "createdAt" : "2021-04-22T07:11:55Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "68ff24ec-eca1-4d92-816b-962904bdfc7d",
        "parentId" : "28607176-977c-4f66-8b44-4c355bc15cf9",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "Yes, it probably doesn't make a big difference and it happens rarely. So, we could just keep the logic in this PR.",
        "createdAt" : "2021-04-22T21:52:40Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "d2abc9dc-0447-48f2-a2c3-aee606644dda",
        "parentId" : "28607176-977c-4f66-8b44-4c355bc15cf9",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "One thing I've been considering for controlled shutdown is that perhaps we could just keep the shutting down replicas in the ISR but move the leaders.  I think that would be a bit more graceful than what we have now, but we'd have to do some extra work to get there.\r\n\r\nAnyway, let's consider this later, as you suggested...",
        "createdAt" : "2021-04-27T22:10:55Z",
        "updatedAt" : "2021-04-27T22:21:40Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      },
      {
        "id" : "75cfe5cd-6d8f-456c-88fa-073513391d92",
        "parentId" : "28607176-977c-4f66-8b44-4c355bc15cf9",
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "The reason that we want to remove the shutting down replicas from ISR is to optimize for latency. If we don't do that, when the broker actually shuts down, it can block the producer for replica.max.ms before the replica can be taken out of ISR. So, I think this optimization is still useful.",
        "createdAt" : "2021-04-27T22:26:21Z",
        "updatedAt" : "2021-04-27T22:26:21Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "39ed3f8abae2035a27c321e08f9cfce71974514c",
    "line" : 423,
    "diffHunk" : "@@ -1,1 +1099,1103 @@                // Choose a new leader.\n                boolean uncleanOk = configurationControl.uncleanLeaderElectionEnabledForTopic(topic.name);\n                newLeader = bestLeader(partition.replicas, newIsr, uncleanOk, isAcceptableLeader);\n            }\n            if (!electionWasClean(newLeader, newIsr)) {"
  },
  {
    "id" : "cfb795f8-4cf2-4348-b44d-ec3ed7709023",
    "prId" : 10564,
    "prUrl" : "https://github.com/apache/kafka/pull/10564#pullrequestreview-648515521",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d95f7234-064b-4907-a7e3-d6496c591ebb",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "In the old controller, we bump up the leader epoch if the ISR is changed during the controlled shutdown. This helps prevent the shutting down broker from being added to ISR again. In the raft controller, we bump up the partitionEpoch when the ISR is changed. Do we plan to fence a fetch request with unmatched partitionEpoch to achieve the same logic? If so, do we have a jira to track that?",
        "createdAt" : "2021-04-27T22:40:40Z",
        "updatedAt" : "2021-04-27T22:41:56Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      },
      {
        "id" : "5a7d79d8-d300-4199-b3c5-54ed491682a8",
        "parentId" : "d95f7234-064b-4907-a7e3-d6496c591ebb",
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Filed https://issues.apache.org/jira/browse/KAFKA-12733 for this",
        "createdAt" : "2021-04-29T18:19:10Z",
        "updatedAt" : "2021-04-29T18:19:10Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "39ed3f8abae2035a27c321e08f9cfce71974514c",
    "line" : 146,
    "diffHunk" : "@@ -1,1 +368,372 @@            prevPartitionInfo.isr, newPartitionInfo.isr, prevPartitionInfo.leader,\n            newPartitionInfo.leader);\n        String topicPart = topicInfo.name + \"-\" + record.partitionId() + \" with topic ID \" +\n            record.topicId();\n        newPartitionInfo.maybeLogPartitionChange(log, topicPart, prevPartitionInfo);"
  },
  {
    "id" : "5ec8eaa8-4202-48cc-9faa-7ec4cad07706",
    "prId" : 10679,
    "prUrl" : "https://github.com/apache/kafka/pull/10679#pullrequestreview-658897394",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5d67321-b818-49b3-a7a9-1c0ed16aaf3a",
        "parentId" : null,
        "authorId" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "body" : "Can you add a JavaDoc description here?",
        "createdAt" : "2021-05-13T13:24:11Z",
        "updatedAt" : "2021-05-13T13:24:12Z",
        "lastEditedBy" : "514c0afb-8649-4fd9-a6ea-ee9e1b695274",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e2513f77bfdbd3d5f3a25c5c13d758d2ce948f8",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +283,287 @@     * A count of the total number of partitions in the cluster.\n     */\n    private int globalPartitionCount;\n\n    /**"
  },
  {
    "id" : "73c86821-de44-410a-a3e6-378cf5c1facc",
    "prId" : 10753,
    "prUrl" : "https://github.com/apache/kafka/pull/10753#pullrequestreview-704613472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4d97337b-0379-4a11-ba74-eb20783e828c",
        "parentId" : null,
        "authorId" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "body" : "It would be useful to add an info level logging when an AlterIsr triggers the completion of the partition reassignment.",
        "createdAt" : "2021-07-13T20:55:24Z",
        "updatedAt" : "2021-07-13T22:00:43Z",
        "lastEditedBy" : "442b5138-0781-4001-8ac5-0593f2136d1c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac0f9935b6955513a172869345f5bdb44a3abe67",
    "line" : 277,
    "diffHunk" : "@@ -1,1 +714,718 @@                            \"the ongoing partition reassignment.\", request.brokerId(),\n                            topic.name, partitionId);\n                    }\n                }\n                responseTopicData.partitions().add(new AlterIsrResponseData.PartitionData()."
  }
]