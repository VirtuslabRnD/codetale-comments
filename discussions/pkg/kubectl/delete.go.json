[
  {
    "id" : "2a1d4868-326c-4812-8019-be2405bd7fd7",
    "prId" : 61790,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61790#pullrequestreview-107483901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f1a6812-bb3a-43a4-8998-69e946770630",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "I think this PR should focus on fixing the flake instead of refactoring Scaler. \r\n\r\nWould you keep the Scaler as-is, and check these returned error the way `DeploymentReaper` does, to see if it fixes the flake?\r\n```go\r\niscaleGetErr, ok := err.(ScaleError)\r\nif errors.IsNotFound(err) || (ok && errors.IsNotFound(scaleGetErr.ActualError)) {\r\n// ...\r\n```\r\n\r\nMaking the scale client to return the actual API error can be done in another PR. ",
        "createdAt" : "2018-03-27T21:06:50Z",
        "updatedAt" : "2018-03-28T12:25:27Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "955b50dc-80fa-4963-a369-4da1d501d432",
        "parentId" : "2f1a6812-bb3a-43a4-8998-69e946770630",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> Would you keep the Scaler as-is, and check these returned error the way DeploymentReaper does, to see if it fixes the flake?\r\n\r\nIt's actually pretty hard to reason about which places get \"normal\" errors and which don't.  I ended up doing this after messed up the change a couple times.\r\n\r\n\r\n\r\n> Did #60455 cause the flake? If so, why?\r\n\r\nI suspect it originated at the scale client errors instead of the non-scale client errors.",
        "createdAt" : "2018-03-27T21:33:24Z",
        "updatedAt" : "2018-03-28T12:25:27Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "6cad9199-475b-484a-8bac-a5a1e10b90a4",
        "parentId" : "2f1a6812-bb3a-43a4-8998-69e946770630",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "> It's actually pretty hard to reason about which places get \"normal\" errors and which don't. I ended up doing this after messed up the change a couple times.\r\n\r\nDid you mean the returned error could be a 404, but is neither `errors.IsNotFound` nor `ScaleError`?",
        "createdAt" : "2018-03-27T21:46:27Z",
        "updatedAt" : "2018-03-28T12:25:27Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "bcf0ba92-c3cd-4ade-8588-2940a0cae725",
        "parentId" : "2f1a6812-bb3a-43a4-8998-69e946770630",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> Did you mean the returned error could be a 404, but is neither errors.IsNotFound nor ScaleError?\r\n\r\nYeah.  It was when I started and I started peeling back layers until I found the scale client.",
        "createdAt" : "2018-03-27T22:00:36Z",
        "updatedAt" : "2018-03-28T12:25:27Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "1272fda2990b4bafdb72bdbf4acdc400fe1addd1",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +369,373 @@\tretry := NewRetryParams(reaper.pollInterval, reaper.timeout)\n\twaitForJobs := NewRetryParams(reaper.pollInterval, timeout)\n\tif err = scaler.Scale(namespace, name, 0, nil, retry, waitForJobs); err != nil && !errors.IsNotFound(err) {\n\t\treturn err\n\t}"
  },
  {
    "id" : "41fde288-5104-476e-b95e-4286ee66dd60",
    "prId" : 59851,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59851#pullrequestreview-120637079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a44d172-7e08-4ff0-984d-cc02389c5d59",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "scaling down doesn't mean all dependents are deleted, just pods. why not foreground here as well? same question applies to other reapers",
        "createdAt" : "2018-05-03T14:26:07Z",
        "updatedAt" : "2018-05-23T07:01:03Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "5f27e8c7-ec3f-4daf-8d32-54e0eb0edf65",
        "parentId" : "1a44d172-7e08-4ff0-984d-cc02389c5d59",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "actually, can we remove all the reapers and rely on server-side GC instead at this point?",
        "createdAt" : "2018-05-03T14:29:12Z",
        "updatedAt" : "2018-05-23T07:01:03Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "e78890e5-642e-460d-9996-7708eafc8e84",
        "parentId" : "1a44d172-7e08-4ff0-984d-cc02389c5d59",
        "authorId" : "fbb90de5-459e-43f3-969b-110ed8a91f5c",
        "body" : "> can we remove all the reapers and rely on server-side GC instead\r\n\r\n@liggitt I think the answer is \"yes\", but I would prefer this change to be a separate PR.\r\n@caesarxuchao what do you think?",
        "createdAt" : "2018-05-09T23:43:35Z",
        "updatedAt" : "2018-05-23T07:01:03Z",
        "lastEditedBy" : "fbb90de5-459e-43f3-969b-110ed8a91f5c",
        "tags" : [
        ]
      },
      {
        "id" : "acda44a4-761b-4945-a81a-3b38db37fd0e",
        "parentId" : "1a44d172-7e08-4ff0-984d-cc02389c5d59",
        "authorId" : "fbb90de5-459e-43f3-969b-110ed8a91f5c",
        "body" : "> why not foreground here as well?\r\n\r\n@liggitt I would also prefer to have foreground everywhere, but @caesarxuchao previously suggested to use background for all reapers, see https://github.com/kubernetes/kubernetes/pull/59851#discussion_r171072408",
        "createdAt" : "2018-05-10T06:11:59Z",
        "updatedAt" : "2018-05-23T07:01:03Z",
        "lastEditedBy" : "fbb90de5-459e-43f3-969b-110ed8a91f5c",
        "tags" : [
        ]
      },
      {
        "id" : "d6c30fe6-1b72-4d59-938e-98bb0b625b8f",
        "parentId" : "1a44d172-7e08-4ff0-984d-cc02389c5d59",
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "I'll tackle the followup to drop the reapers. @nilebox can you update this PR asap so we can get it in?",
        "createdAt" : "2018-05-16T13:25:03Z",
        "updatedAt" : "2018-05-23T07:01:03Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b5afd8809c93c31d1787347160f947f68f1fe03",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +213,217 @@\t}\n\t// Using a background deletion policy because the replication controller\n\t// has already been scaled down.\n\tpolicy := metav1.DeletePropagationBackground\n\tdeleteOptions := &metav1.DeleteOptions{PropagationPolicy: &policy}"
  }
]