[
  {
    "id" : "0a677eaf-9e43-48a5-b35b-298e00744754",
    "prId" : 100183,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100183#pullrequestreview-679420509",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ab3f425-07b7-4c4e-b99c-dc30796c45aa",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "so if volume was uncertain, this would result in volume being deleted from DSOW. Previously we did exact opposite:\r\n\r\n> if volume mount was uncertain we should keep trying to mount the volume\r\n\r\nwhich means if volume-mgr started mounting the volume and say first mount failed and volume entered uncertain state, then we do not remove the volume from DSOW until volume is correctly mounted.  Now we would basically abort potentially in-flight mount request and issue umount requests.  I am not sure which one is more correct. \r\n",
        "createdAt" : "2021-06-08T19:07:23Z",
        "updatedAt" : "2021-06-08T19:08:10Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "287162a3-bc3e-40ac-a3fa-f40c1d5550ad",
        "parentId" : "0ab3f425-07b7-4c4e-b99c-dc30796c45aa",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I think it is fine. ",
        "createdAt" : "2021-06-08T19:15:54Z",
        "updatedAt" : "2021-06-08T19:15:54Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "cac592f6-53aa-4803-9be8-ec6a67e57d25",
        "parentId" : "0ab3f425-07b7-4c4e-b99c-dc30796c45aa",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "> if volume mount was uncertain we should keep trying to mount the volume\r\n\r\nWe want to keep this behavior when a pod needs the volume. However, the change above is for deleted pods, where we want to \"keep trying to **unmount** the volume\". The mount may never succeed, because of timeouts or whatnot.",
        "createdAt" : "2021-06-09T09:26:24Z",
        "updatedAt" : "2021-06-09T09:26:24Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5da73032f7882afe180af77a5fcc8f441d36807",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +279,283 @@\t\t\tcontinue\n\t\t}\n\t\tvar volumeToMountSpecName string\n\t\tif volumeToMount.VolumeSpec != nil {\n\t\t\tvolumeToMountSpecName = volumeToMount.VolumeSpec.Name()"
  },
  {
    "id" : "b1cb16f2-dd06-4134-a980-b96e63558030",
    "prId" : 100183,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100183#pullrequestreview-679585463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bb54664-ac1a-4b2c-b891-dbcf354bf70f",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "This is kind subtle behaviour change btw. Do we have tests for it? I think existing uncertain device and mount tests only cover setup or mount failures. Having said that - it might be tricky to test it, but it might be a good idea to have some coverage, ",
        "createdAt" : "2021-06-08T19:36:17Z",
        "updatedAt" : "2021-06-08T19:36:17Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "d14f2dfd-69da-46fa-9e2a-1fb835f91e42",
        "parentId" : "5bb54664-ac1a-4b2c-b891-dbcf354bf70f",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Added an unit test. I had to refactor `findAndRemoveDeletedPods` tests a bit - there were two tests with identical initialization, I created a common function with it and added a third test.",
        "createdAt" : "2021-06-09T12:28:23Z",
        "updatedAt" : "2021-06-09T12:28:23Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5da73032f7882afe180af77a5fcc8f441d36807",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +285,289 @@\t\tremoved := dswp.actualStateOfWorld.PodRemovedFromVolume(volumeToMount.PodName, volumeToMount.VolumeName)\n\t\tif removed && podExists {\n\t\t\tklog.V(4).InfoS(\"Actual state does not yet have volume mount information and pod still exists in pod manager, skip removing volume from desired state\", \"pod\", klog.KObj(volumeToMount.Pod), \"podUID\", volumeToMount.Pod.UID, \"volumeName\", volumeToMountSpecName)\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "135b61f4-65ff-4384-887f-5c2860fb3960",
    "prId" : 98850,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98850#pullrequestreview-602557319",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a5677d6-4fbc-4b9d-a02d-ef859a6527c9",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "Is this needed? Does klog have issues with a nil value?",
        "createdAt" : "2021-03-02T22:57:26Z",
        "updatedAt" : "2021-03-17T00:59:37Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "22719c3c-b3fa-4043-b2b3-c096c32abcca",
        "parentId" : "3a5677d6-4fbc-4b9d-a02d-ef859a6527c9",
        "authorId" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "body" : "I have fixed it,thanks.",
        "createdAt" : "2021-03-03T06:34:35Z",
        "updatedAt" : "2021-03-17T00:59:37Z",
        "lastEditedBy" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "tags" : [
        ]
      }
    ],
    "commit" : "01a4e4facebda8a643383625d3cece724efe2a66",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +278,282 @@\t\tvar volumeToMountSpecName string\n\t\tif volumeToMount.VolumeSpec != nil {\n\t\t\tvolumeToMountSpecName = volumeToMount.VolumeSpec.Name()\n\t\t}\n\t\tif !exists && podExists {"
  },
  {
    "id" : "18dbddaf-a7dd-4e27-95aa-5fbf29707d04",
    "prId" : 98850,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98850#pullrequestreview-605345669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e15c227-8792-4cf3-90a9-d0bd83ba413c",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "```suggestion\r\n\t\tklog.V(5).InfoS(\"Insufficient time has passed since last attempt, skipping findAndRemoveDeletedPods()\", \"nextRetryTime\", dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration), \"retryDuration\", dswp.getPodStatusRetryDuration)\r\n```",
        "createdAt" : "2021-03-02T23:13:06Z",
        "updatedAt" : "2021-03-17T00:59:37Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "d4466a0f-aad0-4cd3-8d13-a3c314370875",
        "parentId" : "4e15c227-8792-4cf3-90a9-d0bd83ba413c",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "Why resolve?",
        "createdAt" : "2021-03-05T15:36:15Z",
        "updatedAt" : "2021-03-17T00:59:37Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      }
    ],
    "commit" : "01a4e4facebda8a643383625d3cece724efe2a66",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +171,175 @@\t// populator loop.\n\tif time.Since(dswp.timeOfLastGetPodStatus) < dswp.getPodStatusRetryDuration {\n\t\tklog.V(5).InfoS(\"Skipping findAndRemoveDeletedPods(). \", \"nextRetryTime\", dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration), \"retryDuration\", dswp.getPodStatusRetryDuration)\n\t\treturn\n\t}"
  },
  {
    "id" : "c05f6a1a-b3fb-450d-8da5-b86f357d812f",
    "prId" : 93710,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93710#pullrequestreview-473931864",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a42ae14-3380-4997-9f0d-e70e415e230c",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I like this much better and we are no longer deleting volumes from DSOW. thank you.",
        "createdAt" : "2020-08-24T21:50:23Z",
        "updatedAt" : "2020-08-25T00:15:34Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "ba9e2473-ea24-4e5d-ab7d-4080368ee09c",
        "parentId" : "0a42ae14-3380-4997-9f0d-e70e415e230c",
        "authorId" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "body" : "Yeah, after some discussions this seems like a much easier and safe approach. ",
        "createdAt" : "2020-08-24T22:35:06Z",
        "updatedAt" : "2020-08-25T00:15:34Z",
        "lastEditedBy" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6d8e6c5c2e5e25a0dd6a003497519a226128161",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +234,238 @@\t\t\t\t\t// It is not possible right now for a CSI plugin to be both attachable and non-deviceMountable\n\t\t\t\t\t// So the uniqueVolumeName should remain the same after the attachability change\n\t\t\t\t\tdswp.desiredStateOfWorld.MarkVolumeAttachability(volumeToMount.VolumeName, false)\n\t\t\t\t\tklog.Infof(\"Volume %v changes from attachable to non-attachable.\", volumeToMount.VolumeName)\n\t\t\t\t\tcontinue"
  },
  {
    "id" : "9e56a401-f204-40ff-8a7b-af7c085aeec6",
    "prId" : 88141,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88141#pullrequestreview-369734893",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "140d16bb-f9bc-45e5-b0d6-c0beec5be033",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "The comment on this function is out of date now.",
        "createdAt" : "2020-03-05T04:05:48Z",
        "updatedAt" : "2020-03-06T00:46:03Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "4f1d3d9b-5a99-48d4-8b56-773d2160cec5",
        "parentId" : "140d16bb-f9bc-45e5-b0d6-c0beec5be033",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "How was this fixed. The comment still reads:\r\n\r\n> // ReprocessPod removes the specified pod from the list of processedPods\r\n\t// (if it exists) forcing it to be reprocessed. This is required to enable\r\n\t// remounting volumes on pod updates (volumes like Downward API volumes\r\n\t// depend on this behavior to ensure volume content is updated).\r\n",
        "createdAt" : "2020-03-05T16:38:44Z",
        "updatedAt" : "2020-03-06T00:46:03Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "f7bb4ae7-b331-40e3-ac55-17ef91dd3652",
        "parentId" : "140d16bb-f9bc-45e5-b0d6-c0beec5be033",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Updated the comment.",
        "createdAt" : "2020-03-05T16:45:39Z",
        "updatedAt" : "2020-03-06T00:46:03Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "723761aa8874d03138535e132ade6f2b54d6ddf4",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +149,153 @@}\n\nfunc (dswp *desiredStateOfWorldPopulator) ReprocessPod(\n\tpodName volumetypes.UniquePodName) {\n\tdswp.markPodProcessingFailed(podName)"
  },
  {
    "id" : "0d5d4904-d724-4bd6-97da-f23584e31121",
    "prId" : 88141,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88141#pullrequestreview-370020446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a87210ee-72dc-4458-a6b6-cc77dacdcfd5",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "```suggestion\r\n\tdswp.pods.RLock()\r\n\tdefer dswp.pods.RUnlock()\r\n\t_, exist := dswp.pods.processedPods[podName]\r\n```",
        "createdAt" : "2020-03-06T00:37:01Z",
        "updatedAt" : "2020-03-06T00:46:03Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "94005ebf-44d8-49b7-b076-87a5b061c1c9",
        "parentId" : "a87210ee-72dc-4458-a6b6-cc77dacdcfd5",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Since there is no chance of panic in accessing the map, I choose the (slightly) more efficient way of releasing the lock.\r\nI prefer to keep this form.",
        "createdAt" : "2020-03-06T00:46:51Z",
        "updatedAt" : "2020-03-06T00:46:51Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "e7f6c1fc-984b-4375-af1c-25a1ea8717a4",
        "parentId" : "a87210ee-72dc-4458-a6b6-cc77dacdcfd5",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "I would prefer to lean towards safety rather then performance in this tradeoff. But will defer to you as PR author.",
        "createdAt" : "2020-03-06T00:58:55Z",
        "updatedAt" : "2020-03-06T00:58:55Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "723761aa8874d03138535e132ade6f2b54d6ddf4",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +465,469 @@\tdswp.pods.RLock()\n\t_, exist := dswp.pods.processedPods[podName]\n\tdswp.pods.RUnlock()\n\treturn exist\n}"
  },
  {
    "id" : "fada622c-158d-4569-8a5d-131c008ee79f",
    "prId" : 81429,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81429#pullrequestreview-276780810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c039d4b-f3de-486b-b9ef-0660708ef193",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Just to be on safe side, for in-tree drivers I think we should still skip calling fs resize on the node if volume is of type raw block. \r\n\r\nThe other thing is - we are going to need some e2e tests for this. ",
        "createdAt" : "2019-08-14T19:02:25Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "8b351057-5ee8-49ad-b838-112eeb4ef138",
        "parentId" : "2c039d4b-f3de-486b-b9ef-0660708ef193",
        "authorId" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "body" : "What if migration is enabled?\r\n\r\nin-tree: no Pending condition on PVC if mode Block\r\nCSI: always Pending condition on PVC\r\nmigrated CSI: ?",
        "createdAt" : "2019-08-15T20:09:43Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "tags" : [
        ]
      },
      {
        "id" : "b9516ce0-d82c-46be-86b2-c99f1518c072",
        "parentId" : "2c039d4b-f3de-486b-b9ef-0660708ef193",
        "authorId" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "body" : "Our e2e test expects that Block volumes don't get the condition\r\nhttps://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/testsuites/volume_expand.go#L190\r\nI'm running the tests for aws migration.\r\nSo the test needs to know if the plugin is CSI or in-tree or migrated CSI which will be ugly.",
        "createdAt" : "2019-08-15T20:24:07Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "tags" : [
        ]
      },
      {
        "id" : "64ab1099-cbf2-4d44-ba15-f1ca61abd725",
        "parentId" : "2c039d4b-f3de-486b-b9ef-0660708ef193",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "That e2e is broken for CSI raw block volumes, no matter what we do here right? Plugin already knows if it is CSI or in-tree. I guess in-tree migrated CSI is tricky. But I think this ugliness will go away once we merge - https://github.com/container-storage-interface/spec/pull/381 \r\n\r\nSo our options are:\r\n1. Keep in-tree drivers behaviour same as CSI, but since in CSI too we are moving towards skipping `NodeExpandVolume` for raw block devices, this seems like a step backward and will cause unnecessary churn.\r\n2. Live with ugliness in e2e until we https://github.com/container-storage-interface/spec/pull/381 is merged and fix the in-tree <-> csi drivers.\r\n\r\nI am leaning towards #2. And yeah - I think we should not skip calling fs resize if driver is in-tree but migration is enabled for it. \r\n\r\n",
        "createdAt" : "2019-08-15T20:52:41Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "3fcae6be-0dc9-4aba-ac29-3afe4752d4c4",
        "parentId" : "2c039d4b-f3de-486b-b9ef-0660708ef193",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I have opened https://github.com/kubernetes/kubernetes/pull/81611 to fix this problem. I think the condition checking there added very little value.",
        "createdAt" : "2019-08-19T19:57:32Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dbe0b3ad83fae61b631501a5636f56120a405e8",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +384,388 @@\t\treturn\n\t}\n\tif processedVolumesForFSResize.Has(string(uniqueVolumeName)) {\n\t\t// File system resize operation is a global operation for volume,\n\t\t// so we only need to check it once if more than one pod use it."
  },
  {
    "id" : "c1ab4e33-4a3e-466b-ba07-0d2e790137d0",
    "prId" : 81429,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81429#pullrequestreview-278772597",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Should we pass readonly down to the plugin and let the plugin decide if resize should be skipped?  Or actually let it fail?  What does it mean to resize a read only volume?",
        "createdAt" : "2019-08-22T20:30:35Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "874eed8c-8f96-4b9e-a141-c484839fe817",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "yeah that is a good idea and ties into discussion we had this yesterday in CSI spec meeting.  Currently `NodeExpandVolumeRequest` can't carry over that information and @bswartz wanted readonly volumes to be expandable on nodes too. In his case - all the plugin would do is, rescan iscsi lun. \r\n\r\nI think, we should not be making that change right now in this PR. ",
        "createdAt" : "2019-08-22T22:12:25Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "e5d943a3-df68-4dfb-8a8d-661e083ef8cd",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I'm wondering if volumeSpec.ReadOnly is sufficient enough, or do we also need to be checking container readonly too?",
        "createdAt" : "2019-08-22T22:18:37Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "7ef67247-efec-4260-8f40-7d49debc5e00",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "The other question I have is more philosophical.  If a user requests expansion on a read-only volume, should we actually ignore it and return success?  We didn't actually expand the filesystem to what they requested.",
        "createdAt" : "2019-08-22T22:19:54Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "aa71a432-9dcf-47fa-b9f7-c1b0f42a836a",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "For CSI - in the long run we want the decision to be taken by the plugin(SP) and not the CO. For in-tree implementations we currently support, doing resizing of readonly volumes with filesystem on it - could result in filesystem corruption.\r\n\r\n> I'm wondering if volumeSpec.ReadOnly is sufficient enough, or do we also need to be checking container readonly too?\r\n\r\nThe container readonly is handled by CRI and is an option for bind mount. I think checking `volumeSpec.Readonly` should be sufficient for our use case.",
        "createdAt" : "2019-08-22T22:34:50Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "069efd84-b51e-4212-8b6d-fbf5fa1fd196",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Shouldn't the resizing attempt fail if the volume is attached/mounted as read only?",
        "createdAt" : "2019-08-22T22:42:33Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "217da731-4c22-4ab3-a369-e75432b9bb8d",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "In many cases - expanding the filesystem does not care how volume is mounted.  Expanding(resize2fs/xfsgrow) the volume sometimes just takes device path (not the mount path). ",
        "createdAt" : "2019-08-23T01:16:54Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "1ae3a59c-c6ef-49f0-8bc5-b95dfaa90cb5",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "So the PV.readOnly field is only used for determining if volumes should be attached readonly: https://github.com/kubernetes/kubernetes/issues/70505\r\n\r\nIs that the only readOnly setting you want to skip here?\r\n",
        "createdAt" : "2019-08-23T01:28:33Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "96f5e917-8ad8-454e-b6d4-a7548d30f946",
        "parentId" : "71f16ab0-8322-4e84-9c17-1f16ea02ad0f",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Yes - so attached as `ReadOnly` and mounted(NodePublished) as `ReadOnly` are two different things. Reading the linked issue, it seems like we are not setting `PersistentVolume.spec.CSIPersistentVolumeSource.readOnly` anywhere but once we do - this code should be updated to respect that and *not* resize a readonly attached volume.\r\n\r\nAgain this is with the caveat that - once we have option to pass `ReadOnly` to `NodeExpandRequest` - we will have to update the call to pass it as a flag and let SP decide, how best to handle it.\r\n",
        "createdAt" : "2019-08-23T02:36:33Z",
        "updatedAt" : "2019-08-23T02:49:08Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dbe0b3ad83fae61b631501a5636f56120a405e8",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +392,396 @@\t// This is the same flag that determines filesystem resizing behaviour for offline resizing and hence\n\t// we should use it here. This value comes from Pod.spec.volumes.persistentVolumeClaim.readOnly.\n\tif volumeSpec.ReadOnly {\n\t\t// This volume is used as read only by this pod, we don't perform resize for read only volumes.\n\t\tklog.V(5).Infof(\"Skip file system resize check for volume %s in pod %s/%s \"+"
  },
  {
    "id" : "61cd65a7-6bd7-46a6-845a-fe7fbb9f36f6",
    "prId" : 80369,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80369#pullrequestreview-264669332",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f55ebe84-0b39-454f-9924-9a6fb422d7d5",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "If for some reason if these errors were transient and volume eventually did get added to DSOW.. should we remove the errors from pod if `allVolumesAdded` is true in lin#343?\r\n",
        "createdAt" : "2019-07-19T18:37:36Z",
        "updatedAt" : "2019-08-05T08:08:51Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "492b1bdc-84e9-46ee-b9b7-9a82de9915fe",
        "parentId" : "f55ebe84-0b39-454f-9924-9a6fb422d7d5",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Good idea, I removed errors if `allVolumesAdded`.",
        "createdAt" : "2019-07-22T08:35:14Z",
        "updatedAt" : "2019-08-05T08:08:51Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "c744385804a7e51fdb15cc86894b30ff69bdcd70",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +324,328 @@\t\t\t\tuniquePodName,\n\t\t\t\terr)\n\t\t\tdswp.desiredStateOfWorld.AddErrorToPod(uniquePodName, err.Error())\n\t\t\tallVolumesAdded = false\n\t\t}"
  },
  {
    "id" : "32b408c2-17ea-4f26-b833-f8cd49b2ba28",
    "prId" : 75458,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75458#pullrequestreview-216338423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9eb09885-8fb2-4884-b113-5eb25069ba79",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I have been thinking about how this interacts with reconstruction process. Reconstruction process runs after reconciliation loop has run and all pod sources has been synced. It considers volumes that are not mounted in ASOW and not present in DSOW. ",
        "createdAt" : "2019-03-19T02:33:15Z",
        "updatedAt" : "2019-03-19T02:33:15Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "43df2db6-56e1-48f2-afca-2f9378454395",
        "parentId" : "9eb09885-8fb2-4884-b113-5eb25069ba79",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "So my main point is - I am thinking reconstruction process ideally, should have added back the volume to ASOW as mounted and that could have caused proper cleanup.  @jingxu97 do you think that is accurate?",
        "createdAt" : "2019-03-19T02:35:22Z",
        "updatedAt" : "2019-03-19T02:35:22Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "495a3bd9-d508-4d07-8cc9-11ba1d1c2a3e",
        "parentId" : "9eb09885-8fb2-4884-b113-5eb25069ba79",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "the full reconstruction process works as follows\r\n1. wait for all sources are ready and desired state has synced once (that means all the pods that already exist before kubelet restarts should be added into the desired state after this stage)\r\n2. reconciler starts reconstruction process once. It scans the directory and tries to get information about the volume. It first uses the volume name information to check whether desired state already has it or not. If desired state already has the volume information, skip reconstruction process so that this volume can be handled by reconciler later through desired and actual state.\r\n3. after reconstruction process finishes, the reconciler starts its loops, and volume should be added into actual state through normal volume operations. (the mount operations should pass since mount already exists)\r\n\r\nThe problem is if pod is deleted immediately during or after kubelet restarts, desired state has volume information for a moment, but could be deleted very soon before actual state has the mount information. The previous check in this code is just checking whether volume exist in the actual state. This is not enough because volume could exist in actual state after it passes verify attach check, but it might not go through the mount operation yet. So the problem now is if actual state does not has mount information, desired state deletes the volume, unmount will not be triggered. \r\n\r\nThis PR changes to check whether the volume mount information already exists in the actual state. If not, it will prevent volume being deleted from the desired state so that it gives more time for actual state to update. ",
        "createdAt" : "2019-03-19T17:47:18Z",
        "updatedAt" : "2019-03-19T17:47:18Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "098135b9-3d94-4c26-92d0-2d52bc2005cb",
        "parentId" : "9eb09885-8fb2-4884-b113-5eb25069ba79",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "yeah, I understand what we are fixing. I am just thinking, we may have a potential race between regular reconciliation and reconstruction process.  \r\n\r\n1. If regular reconciliation has volume in DSW but not mounted in ASOW, unless reconstruction and removal of volume from DSW race with each other, reconstruction process should have fixed the volume.\r\n2. Even if regular reconciliation process runs and removes the volume from DSW and volume is not mounted in ASW, the reconstruction process should have _at least_ cleaned up the mount point.\r\n\r\n",
        "createdAt" : "2019-03-19T18:17:32Z",
        "updatedAt" : "2019-03-19T18:17:32Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cb5df672813326952171be6c9ab52d821b85c1a",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +263,267 @@\t\t\t\tvolumeToMount.GenerateMsgDetailed(fmt.Sprintf(\"Actual state has not yet has this volume mounted information and pod (%q) still exists in pod manager, skip removing volume from desired state\",\n\t\t\t\t\tformat.Pod(volumeToMount.Pod)), \"\"))\n\t\t\tcontinue\n\t\t}\n\t\tklog.V(4).Infof(volumeToMount.GenerateMsgDetailed(\"Removing volume from desired state\", \"\"))"
  },
  {
    "id" : "baa94c1d-4cf6-41f9-b314-89963c73afdc",
    "prId" : 62460,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/62460#pullrequestreview-122555533",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e90d759-0ccb-4cf7-8fee-e10ef6de5285",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I do not think we really enforce this ReadOnly flag anywhere",
        "createdAt" : "2018-05-22T15:00:08Z",
        "updatedAt" : "2018-05-31T09:12:23Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "8e451237-63b1-4ee1-904f-896a0aaccb7d",
        "parentId" : "1e90d759-0ccb-4cf7-8fee-e10ef6de5285",
        "authorId" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "body" : "I'm not sure if I understand what you mean. Currently `PersistentVolumeClaimVolumeSource.ReadOnly` is copied to `volume.Spec.ReadOnly`, and many volume plugins use this field to attach/mount volumes, so I add this check here.",
        "createdAt" : "2018-05-23T12:17:08Z",
        "updatedAt" : "2018-05-31T09:12:23Z",
        "lastEditedBy" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca12c733239e8de8a5d20c0588d87b420075c614",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +394,398 @@\nfunc mountedReadOnlyByPod(podVolume v1.Volume, pod *v1.Pod) bool {\n\tif podVolume.PersistentVolumeClaim.ReadOnly {\n\t\treturn true\n\t}"
  },
  {
    "id" : "0b17c011-1ab5-4c56-aeef-64674a0a5ead",
    "prId" : 61071,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61071#pullrequestreview-103613176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75bcb663-c9e2-42dc-a230-9c19f5437892",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "What happens when we quickly add a pod and then delete it. The code will wait for volume be attached before it can be mounted and hence it will not be present in ASW. But the entry will not be removed from DSW even when pod is deleted because of this check right? ",
        "createdAt" : "2018-03-13T20:16:27Z",
        "updatedAt" : "2018-03-15T22:35:49Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "4bcdd476-3736-4c35-885e-5d3c94cbd34a",
        "parentId" : "75bcb663-c9e2-42dc-a230-9c19f5437892",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "because we have an extra check on podExists here, so the entry should be removed. When pod is added and quickly deleted before reconciler sets up mount for it, kubelet should be able to delete it without problem, and podManager will not have this pod anymore, so podExists will become false.",
        "createdAt" : "2018-03-13T21:07:33Z",
        "updatedAt" : "2018-03-15T22:35:49Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bd006de40b670c6e4fb444fd042e9bf0ef916dd",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +246,250 @@\t\t}\n\n\t\tif !dswp.actualStateOfWorld.VolumeExists(volumeToMount.VolumeName) && podExists {\n\t\t\tglog.V(4).Infof(volumeToMount.GenerateMsgDetailed(\"Actual state has not yet has this information skip removing volume from desired state\", \"\"))\n\t\t\tcontinue"
  },
  {
    "id" : "63dce612-2ee2-41b1-8aa2-82483509f6a3",
    "prId" : 59873,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59873#pullrequestreview-103811268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb06906e-c95c-4c7c-9178-ca4fcbd3f8df",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this commit showed up in a bisect of scalability test regression of pod startup time (https://github.com/kubernetes/kubernetes/issues/60589#issuecomment-372485732), and this change looks odd... does this force double mount setup for all pods?",
        "createdAt" : "2018-03-12T23:20:06Z",
        "updatedAt" : "2018-03-12T23:20:06Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "d51ae40c-da21-427f-87ad-54fac17f7f74",
        "parentId" : "cb06906e-c95c-4c7c-9178-ca4fcbd3f8df",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "The flow that leads to `MarkRemountRequired` call is:\r\n\r\n- `kubelet.syncPod` is called (on pod update or every 90s)\r\n  -  `volumeManager.WaitForAttachAndMount` is called and marks the pod for reprocessing\r\n\r\n- `desiredStateOfWorldPopulator.findAndAddNewPods` is called periodically\r\n    - `desiredStateOfWorldPopulator.processPodVolumes` is called (only when the pod was marked for reprocessing in `syncPod`)\r\n        - `actualStateOfWorld.MarkRemountRequired` is called\r\n            - reconciler re-mounts the volumes (updates secrets, downward API, ...). Update of DownwardAPI is the reason why `syncPod` is involved.\r\n\r\nNotice that before this PR, `MarkRemountRequired` was called directly by `kubelet.syncPod`, so the frequency of the calls did not change. With this PR, they are called only in the right order. I don't think it could affect pod startup time.\r\n\r\nOn the other hand, `syncPod` is called with every pod update and pod changes quite often during startup. It is possible that I missed something there.",
        "createdAt" : "2018-03-14T13:16:47Z",
        "updatedAt" : "2018-03-14T13:16:47Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "c232a0165ab7835e9b29509889ab8b1ac7d109c5",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +308,312 @@\t\t// New pod has been synced. Re-mount all volumes that need it\n\t\t// (e.g. DownwardAPI)\n\t\tdswp.actualStateOfWorld.MarkRemountRequired(uniquePodName)\n\t}\n"
  },
  {
    "id" : "71d8fbd5-7ace-4fdf-909a-9dfb31c07e11",
    "prId" : 39493,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39493#pullrequestreview-15408880",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7599a488-aed3-4b42-8c17-021ce5c3a8b5",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Do we still need this check below? Since if `Volume` field is `nil` - it would automatically mean we are dealing with temporal storage?\r\n\r\nAlso - it may be good idea to cover this with some test? Not sure how easy that will be though.",
        "createdAt" : "2017-01-05T22:26:03Z",
        "updatedAt" : "2017-01-05T22:26:03Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "c557a387-0919-4ec4-bab6-b57ea68b28ea",
        "parentId" : "7599a488-aed3-4b42-8c17-021ce5c3a8b5",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "We still need to check if the temporal storage is disk- or memory-backed.  The idea here was that we wanted to skip removing disk-backed temporal storage until the pod is removed from the API server, but remove memory-backed volumes as soon as the pod is in a terminal state.\r\n\r\nI have a follow-on PR https://github.com/kubernetes/kubernetes/pull/37228 for 1.6 that removes this check entirely.",
        "createdAt" : "2017-01-05T22:31:12Z",
        "updatedAt" : "2017-01-05T22:31:12Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "e9049987-e96b-42bf-9457-f0221968ae8a",
        "parentId" : "7599a488-aed3-4b42-8c17-021ce5c3a8b5",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "> Do we still need this check below? Since if Volume field is nil - it would automatically mean we are dealing with temporal storage?\r\n\r\nNo, `VolumeSource` volumes could include both.\r\n\r\n> Also - it may be good idea to cover this with some test? Not sure how easy that will be though.\r\n\r\nAlways a good idea.",
        "createdAt" : "2017-01-05T22:33:46Z",
        "updatedAt" : "2017-01-05T22:33:46Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "df8605a8-4ec1-4193-92f7-533d3794d908",
        "parentId" : "7599a488-aed3-4b42-8c17-021ce5c3a8b5",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "> Also - it may be good idea to cover this with some test? Not sure how easy that will be though.\r\n\r\nI'll try to write a test as part of my follow-on PR.",
        "createdAt" : "2017-01-05T22:39:29Z",
        "updatedAt" : "2017-01-05T22:39:29Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4e67252360b623a8d27b65d6ea6011f119b331c",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +166,170 @@\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif (volume.EmptyDir == nil || volume.EmptyDir.Medium != v1.StorageMediumMemory) &&\n\t\t\t\tvolume.ConfigMap == nil && volume.Secret == nil {\n\t\t\t\tcontinue"
  },
  {
    "id" : "96530c51-894d-4ce1-a76b-8ce6391a8e75",
    "prId" : 36779,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36779#pullrequestreview-9167211",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cac9716c-f242-484c-bf15-7878c33fb140",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Unless \"non-memory backed volume\"?\n",
        "createdAt" : "2016-11-15T05:48:16Z",
        "updatedAt" : "2016-11-16T16:17:35Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "c8486d51-8c56-4dd5-9acf-389f0cf6af03",
        "parentId" : "cac9716c-f242-484c-bf15-7878c33fb140",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "I thought about this but I couldn't think of a good reason to add a volume back to a node for a pod that is terminated.  The only reason I'm filtering the ones we remove is to not make waves for the release.  In general, I can't think of a reason we would want to leave volumes attached to a node for a terminated pods except for debugging.\n",
        "createdAt" : "2016-11-15T14:47:39Z",
        "updatedAt" : "2016-11-16T16:17:35Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "d7767eab-01fa-4969-a6f8-93a71589de19",
        "parentId" : "cac9716c-f242-484c-bf15-7878c33fb140",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "> I thought about this but I couldn't think of a good reason to add a volume back to a node for a pod that is terminated. The only reason I'm filtering the ones we remove is to not make waves for the release. In general, I can't think of a reason we would want to leave volumes attached to a node for a terminated pods except for debugging.\n\nNeither can I, but in the interest of minimizing potentially disruptive changes, it makes sense to limit the scope of the change as much as possible. Thoughts?\n",
        "createdAt" : "2016-11-16T21:39:33Z",
        "updatedAt" : "2016-11-16T21:39:33Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "d4d361cc-766c-483d-9115-8dfe7b7e4ee7",
        "parentId" : "cac9716c-f242-484c-bf15-7878c33fb140",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "@saad-ali the change required to check that actually makes the change more invasive because I need to get the volume.  In `findAndRemoveDeletedPods()`, that is already available via `volumeToMount`.  In `findAndAddNewPods()`, the logic is inverted; for each pod, process volumes.  I'm not sure if it is safe to call `dswp.desiredStateOfWorld.GetVolumesToMount()` in `findAndAddNewPods()`.\n\nI can extend my e2e tests to include a check that ensures non-memory backed volumes are untouched by this change?\n",
        "createdAt" : "2016-11-16T22:39:37Z",
        "updatedAt" : "2016-11-16T22:39:37Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "44ef59fa-b19b-4a18-a753-48217fed966b",
        "parentId" : "cac9716c-f242-484c-bf15-7878c33fb140",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Ack. That's fine. I'm a little nervous about it, but let's just get it in and give it time to bake and keep an eye out for strange volume behaviors.\n",
        "createdAt" : "2016-11-18T04:22:04Z",
        "updatedAt" : "2016-11-18T04:22:04Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "b80bea4a62629437b938f080db732eea1b962cc8",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +139,143 @@\tfor _, pod := range dswp.podManager.GetPods() {\n\t\tif isPodTerminated(pod) {\n\t\t\t// Do not (re)add volumes for terminated pods\n\t\t\tcontinue\n\t\t}"
  }
]