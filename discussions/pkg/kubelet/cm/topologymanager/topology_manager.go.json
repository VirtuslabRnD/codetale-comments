[
  {
    "id" : "dad7732f-e4e6-4444-b6b3-11fc2188ee10",
    "prId" : 92967,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92967#pullrequestreview-521866901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39966aee-8bb4-40b5-a67d-d823c720f0f1",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "this is a nice cleanup!",
        "createdAt" : "2020-11-02T17:53:30Z",
        "updatedAt" : "2020-11-12T11:26:46Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7714918db923523d98e6dd834f53b57a407acd0",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +56,60 @@type manager struct {\n\t//Topology Manager Scope\n\tscope Scope\n}\n"
  },
  {
    "id" : "fcf50b8d-3eb9-429a-8af0-2523388bf60c",
    "prId" : 88876,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88876#pullrequestreview-370033081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5959fb5-3240-431f-8572-4d88e14878d0",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Why do I have to call allocate with none?\r\n\r\nIs it cpu manager on but topology manager off?",
        "createdAt" : "2020-03-06T01:34:32Z",
        "updatedAt" : "2020-03-06T01:34:52Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "2175b26e-e47e-46c8-a67a-9d3f3bea4f26",
        "parentId" : "e5959fb5-3240-431f-8572-4d88e14878d0",
        "authorId" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "body" : "Yes, cpu manager static policy and topology manager none policy",
        "createdAt" : "2020-03-06T01:38:12Z",
        "updatedAt" : "2020-03-06T01:38:12Z",
        "lastEditedBy" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "tags" : [
        ]
      },
      {
        "id" : "5245ae65-1e63-46ba-96fd-e2493f9a7f1d",
        "parentId" : "e5959fb5-3240-431f-8572-4d88e14878d0",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "thanks for clarification , fix makes sense as a result ",
        "createdAt" : "2020-03-06T01:38:57Z",
        "updatedAt" : "2020-03-06T01:38:58Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "0551d408ace162540c66bc14182100337230a3ca",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +229,233 @@\tfor _, container := range append(pod.Spec.InitContainers, pod.Spec.Containers...) {\n\t\tif m.policy.Name() == PolicyNone {\n\t\t\terr := m.allocateAlignedResources(pod, &container)\n\t\t\tif err != nil {\n\t\t\t\treturn lifecycle.PodAdmitResult{"
  },
  {
    "id" : "fdc3620b-d1e8-4b38-87b3-c90667c3ffa8",
    "prId" : 87759,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87759#pullrequestreview-367963034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "471d5e14-43f5-4de7-b7a7-b0836a6fc839",
        "parentId" : null,
        "authorId" : "27be8452-a4b7-4000-aec2-c3647ed683bb",
        "body" : "It seems that topology manager callout resource managers to pre-allocate compute resources here.\r\nBut I cannot find a logic to free pre-allocated resources when one of containers got error from `m.calculateAffinity()`. in this PR.\r\nI think we should free pre-reserved resources in that case.\r\n(Maybe it is handled by some logic... like `RemoveContainer`)\r\n\r\nSo, I'm curious that what will happen on the below situation.\r\n-4 containers in a Pod.\r\n-The first contaienr is admitted and resources are allocated(by `allocateAlignedResources`).\r\n-The Second container got error from `m.calculateAffinity` and admission is rejected.",
        "createdAt" : "2020-03-03T07:49:55Z",
        "updatedAt" : "2020-03-03T07:50:08Z",
        "lastEditedBy" : "27be8452-a4b7-4000-aec2-c3647ed683bb",
        "tags" : [
        ]
      },
      {
        "id" : "92fa8e28-7b84-41a4-9715-0dc629106eef",
        "parentId" : "471d5e14-43f5-4de7-b7a7-b0836a6fc839",
        "authorId" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "body" : "It is not really correct to say \"the first container is admitted\". \r\n\r\nThe split of the original `AddContainer` into two parts: `Allocate` and `AddContainer` means that in reality the \"pre-allocation\" is only acted upon when the **pod** is admitted. \r\n\r\nIn your scenario, the pod will not be admitted due to the second container's `m.calculateAffinity` error.\r\nTherefore the resources which were \"pre-allocated\" to the first container will not be consumed and those resources remain free.",
        "createdAt" : "2020-03-03T11:34:07Z",
        "updatedAt" : "2020-03-03T11:36:46Z",
        "lastEditedBy" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "tags" : [
        ]
      },
      {
        "id" : "286563a9-1830-46e4-8ae6-2c22b5bf28b9",
        "parentId" : "471d5e14-43f5-4de7-b7a7-b0836a6fc839",
        "authorId" : "27be8452-a4b7-4000-aec2-c3647ed683bb",
        "body" : "`It is not really correct to say \"the first container is admitted\".`\r\n`The split of the original AddContainer into two parts`\r\n=> I understood these, points.\r\n\r\n`in reality the \"pre-allocation\" is only acted upon when the pod is admitted.`\r\n=> But I didnt understand this point, because it seems CPU Manager(static policy) allocates cpus for container on `Allocate` part right after hint calculation of a container, so I thought cpus will be allocated even Pod is not admitted.\r\n\r\nWhen the returned value of admit in below code is ture, Topology Manager calls out `Allocate` funtion of resource managers by `allocateAlignedResources()`.\r\n`result, admit := m.calculateAffinity(pod, &container)`\r\n\r\nHere, in the case of CPU Manager, CPUManager calls out `staticPolicy.Allocate(..)`.\r\n(I looked only CPU Manager.)\r\n\r\nThen It seems static policy : \r\n1) allocates cpus based on stored affinity for container by `staticPolicy.allocateCPUs() and takeByTopology() internally`\r\n2) update `state` of cpus to exclude allocated cpus by the below code.\r\n    `s.SetDefaultCPUSet(s.GetDefaultCPUSet().Difference(result))`",
        "createdAt" : "2020-03-03T12:53:30Z",
        "updatedAt" : "2020-03-03T12:53:30Z",
        "lastEditedBy" : "27be8452-a4b7-4000-aec2-c3647ed683bb",
        "tags" : [
        ]
      },
      {
        "id" : "15774659-83fd-43bf-bcfc-a49611aa3fdb",
        "parentId" : "471d5e14-43f5-4de7-b7a7-b0836a6fc839",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "It's possible that CPUs may get allocated even if the pod is ultimately not admitted. However, any cpus allocated to non-admitted pods (as well as any pods that may have terminated for some other reason unbeknownst to the kubelet) are lazily reclaimed at the top of `GetTopologyHints()` here: https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/cpumanager/cpu_manager.go#L303",
        "createdAt" : "2020-03-03T12:59:36Z",
        "updatedAt" : "2020-03-03T12:59:37Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "a044e737-48e2-4301-aa42-1ee7584c16c8",
        "parentId" : "471d5e14-43f5-4de7-b7a7-b0836a6fc839",
        "authorId" : "27be8452-a4b7-4000-aec2-c3647ed683bb",
        "body" : "Oh, I missed that garbage collector, I will take a look it.\r\nThankyou @nolancon @klueska",
        "createdAt" : "2020-03-03T13:06:53Z",
        "updatedAt" : "2020-03-03T13:06:53Z",
        "lastEditedBy" : "27be8452-a4b7-4000-aec2-c3647ed683bb",
        "tags" : [
        ]
      },
      {
        "id" : "16af1a93-3bf3-4379-afe7-6618d3272d31",
        "parentId" : "471d5e14-43f5-4de7-b7a7-b0836a6fc839",
        "authorId" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "body" : "I've tested out your scenario locally @bg-chun and it works as expected in @klueska's description.",
        "createdAt" : "2020-03-03T13:10:07Z",
        "updatedAt" : "2020-03-03T13:10:07Z",
        "lastEditedBy" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "tags" : [
        ]
      }
    ],
    "commit" : "2327934a8602d2f0dad369a86c5475d27fc1b062",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +248,252 @@\t\tm.podTopologyHints[string(pod.UID)][container.Name] = result\n\n\t\terr := m.allocateAlignedResources(pod, &container)\n\t\tif err != nil {\n\t\t\treturn lifecycle.PodAdmitResult{"
  },
  {
    "id" : "55f77e42-bcac-4f09-9431-a6cd2418e899",
    "prId" : 81722,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81722#pullrequestreview-281062714",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22680eb7-a73e-4f0c-98bb-78182bec3c6a",
        "parentId" : null,
        "authorId" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "body" : "we should change the name of socketmask eventually (can be done later)",
        "createdAt" : "2019-08-28T16:19:50Z",
        "updatedAt" : "2019-08-28T16:26:03Z",
        "lastEditedBy" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "tags" : [
        ]
      },
      {
        "id" : "a85b8dc4-2c12-4679-8a7f-5c6f51890074",
        "parentId" : "22680eb7-a73e-4f0c-98bb-78182bec3c6a",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Yes. I think it should just become BitMask",
        "createdAt" : "2019-08-28T19:49:09Z",
        "updatedAt" : "2019-08-28T19:49:09Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "df1b54fc09519548fe37c458ab99f960ac1960bb",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +177,181 @@\t// Set the default affinity as an any-numa affinity containing the list\n\t// of NUMA Nodes available on this machine.\n\tdefaultAffinity, _ := socketmask.NewSocketMask(m.numaNodes...)\n\n\t// Loop through all hint providers and save an accumulated list of the"
  },
  {
    "id" : "c5c98a5a-464d-4af2-8277-3701fe21e176",
    "prId" : 80569,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80569#pullrequestreview-275357964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db9338fa-a12a-4e15-a617-8debc57abca2",
        "parentId" : null,
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "In what case we will get to a situation that we don't any hint for resource? isn't that will blocked on the scheduler level?",
        "createdAt" : "2019-07-28T07:46:13Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "56107792-867c-4e1f-a84e-a751858cd10a",
        "parentId" : "db9338fa-a12a-4e15-a617-8debc57abca2",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "The semantics we agreed to enforce are:\r\n1) nil == don't care --> (1,1: true)\r\n1) {} == care but has no alignment --> (1,1: false)",
        "createdAt" : "2019-07-28T10:05:53Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "9359b0ab-cf19-42b0-95d4-f04e5ff3aea3",
        "parentId" : "db9338fa-a12a-4e15-a617-8debc57abca2",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I don't want to hardcode the (1,1: true) or the (1,1: false) inside the hintprovider, because different policies might want to encode the \"don't care\" and \"care but has no alignment\" differently.\r\n\r\nThe current semantics are admittedly \"best-effort\" specific and I'd imagine they would be pushed down into the logic of your `Merge()` abstraction.",
        "createdAt" : "2019-07-28T10:20:24Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "349d9860-202b-4f62-8f30-336632e49644",
        "parentId" : "db9338fa-a12a-4e15-a617-8debc57abca2",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "Agree, this logic should be pushed down to the merge abstraction. \r\nI am investigating what is the best way to do that.  So for \"nil == don't care\" we can just remove the resource from the allproviderhints  (better name would be allresourceshints). and for   \"{} == care has no alignment\" I don't understand in what case we will have  a resource with empty hints.  Can you elaborate on this? ",
        "createdAt" : "2019-07-28T18:28:27Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "889a2512-175c-4c3b-a42a-09b95ef49f07",
        "parentId" : "db9338fa-a12a-4e15-a617-8debc57abca2",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "It sounds to me that the hints shouldn't be nil if the plugin cares about Topology as there has to be some hint even if it is not preferred.\r\n\r\nA nil hint would indicate to me that there were not enough resources to calculate the hint and there should have been an error.",
        "createdAt" : "2019-08-15T10:22:05Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fdd52b058c60aa851f31efbff1a72153f123ecb",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +184,188 @@\t\t\t}\n\n\t\t\tif len(hints[resource]) == 0 {\n\t\t\t\tklog.Infof(\"[topologymanager] Hint Provider has no possible socket affinities for resource '%s'\", resource)\n\t\t\t\taffinity, _ := socketmask.NewSocketMask()"
  },
  {
    "id" : "31d28f05-d4f5-4660-9e0b-7f4f387af347",
    "prId" : 80569,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80569#pullrequestreview-275717343",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09b62e4b-41a2-4312-abe1-6417aa0a5568",
        "parentId" : null,
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "So you going over all the resources just to change them from a map[string][]TopologyHint to  [][]TopologyHint. Why not just keeping it as a map[string][]TopologyHint. In my merge abstraction I was expecting the permutation to be map[string]TopologyHint. I was thinking that it might be useful in the future that we will know what is the resource this hint is belong too. One case for that is GPU direct. I will be able to write a policy that align GPU PCI  address with NIC PCI address which are have on the same NUMA with the cpus. ",
        "createdAt" : "2019-07-28T07:52:43Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "165a37db-55ec-412e-9724-146c2bef1be0",
        "parentId" : "09b62e4b-41a2-4312-abe1-6417aa0a5568",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I am fine keeping it as a map if that makes your `Merge()` abstraction better. I was just tring to change the code from its current state as little as possible.",
        "createdAt" : "2019-07-28T10:06:46Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "0e60d7d3-8188-486d-8230-cdae4c5f7ab1",
        "parentId" : "09b62e4b-41a2-4312-abe1-6417aa0a5568",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "Is this due time constraints for the release? not sure when kubernetes 1.16 is release, and I doubt the merge abstraction will be merged into this release. (there are other patches that should be merged first). If that the case we can keep the code as is, but we will need to revisit this later. ",
        "createdAt" : "2019-07-28T18:16:55Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "dab28548-e6f0-4fb5-9c0d-4f041db6140e",
        "parentId" : "09b62e4b-41a2-4312-abe1-6417aa0a5568",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "It's more due to ease of reviewability. The smaller the change, the easier it is to review and verify its correctness relative to the existing code.",
        "createdAt" : "2019-07-29T11:26:39Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "257b8982-6cca-4046-a069-4754c1b53038",
        "parentId" : "09b62e4b-41a2-4312-abe1-6417aa0a5568",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "@moshe010 You could introduce the change in Topology Manager as part of your PR and explain the reasoning",
        "createdAt" : "2019-08-15T11:31:27Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      },
      {
        "id" : "c9dda416-e638-4289-89fd-3f755bd172a7",
        "parentId" : "09b62e4b-41a2-4312-abe1-6417aa0a5568",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "yes, sure I will do the changes in my PR",
        "createdAt" : "2019-08-15T11:45:51Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "bd9fbda7-3652-4cbc-be58-a7f05c27205b",
        "parentId" : "09b62e4b-41a2-4312-abe1-6417aa0a5568",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "on second thought, for the current policies (best effort and strict) I don't need merge  signature to be  map only for more advance policies in the future.  Because we are in time constraints for 1.16 I will make the merge signature to be slice and we will revisit it late.",
        "createdAt" : "2019-08-15T23:27:29Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fdd52b058c60aa851f31efbff1a72153f123ecb",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +192,196 @@\t\t\t}\n\n\t\t\tallProviderHints = append(allProviderHints, hints[resource])\n\t\t}\n\t}"
  },
  {
    "id" : "ab2f6de8-6790-4d48-bcd7-9ff826104ebb",
    "prId" : 80569,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80569#pullrequestreview-277466810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "parentId" : null,
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "Why this is need? alternative will be to not include it in allProviderHints.\r\nFor example if I have cpu provider and device plugin provider and only cpu provider  has hint, I will only add them in the allProviderHints.  also in Line 177 when you are going per resource if that resource don't have hint is should not  be included in the allProviderHints. This is also related to my comment below that it better to have allProviderHints as map[string][] TopologyHint, so we will keep a map of resources that have hints.",
        "createdAt" : "2019-07-28T09:50:20Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "47c52aa9-a0c7-49a7-96d7-06d635d754dc",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "If we keep it as a map below, then it's probably fine to remove this. As of now, having a {1,1:true} is the same as not having a hint at all (at least for the best effort policy). I can see how it might cause problems though when integrating your `Merge()` abstraction. At what level in this logic do you pass things down to the `Merge()`?",
        "createdAt" : "2019-07-28T10:15:02Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "1960aa9f-5e71-4ae8-aebe-76f8e8b16ef6",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I pictured you passing the entire hints map into merge and having it pop out the single `TopologyHint`, with the current logic becoming the implementation of the `best-effort` merge policy.",
        "createdAt" : "2019-07-28T10:24:20Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "b71be629-7989-4ad4-8864-a96e9a22aa89",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "So my intention was like in the POC code [1]. you see that I am passing a permutation of map[string]TopologyHint and from permutation I create the merged hint.\r\nI assumed that I will have only resources with hints, but  like you mention above: \r\nnil - as don't care\r\n{} - as care but not align.\r\n So I wonder in what situation in the best-effort policy we will have resource with empty hints?\r\n\r\n[1] https://github.com/moshe010/kubernetes/blob/numa_toplogy_new/pkg/kubelet/cm/topologymanager/topology_manager.go#L182-L183",
        "createdAt" : "2019-07-28T18:12:57Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "684ecebb-9c3f-4de0-854f-511dd1575fe0",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Right now I have this happening in the device manager when the set of available devices is less than the set of requested devices for a specific resource type:\r\nhttps://github.com/kubernetes/kubernetes/pull/80570/commits/1a2b03b0d88cc52b1e3f783a0ae89798bd6dac4e#diff-02911c3198a5635aba854ec2d6844cfdR47\r\n\r\nThis can happen if some devices becomes unhealthy after the scheduler has admitted the pod, but before the device manager has done its allocation (which happens after the call to \r\n`GetTopologyHints()`).\r\n\r\nThinking about his more though, this really this should return an `error` and not an empty list. Maybe we need to modify the interface again to allow us to return errors here.\r\n\r\nWith that said, in the future, I see the need for `{}` becoming important once we introduce the `GetPreferredAllocations()` call discussed in https://github.com/kubernetes/enhancements/pull/1121\r\n\r\nOnce that is in place, we will need a way of encoding that there are no preferred allocations, and that we'd prefer to block admission of the pod until a preferred allocation can be calculated.",
        "createdAt" : "2019-07-30T11:27:05Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "fe75121a-76c7-4ae2-89eb-a91d7ee38db5",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "regarding the case of devices becomes unhealthy after the scheduler has admitted the pod I agree with you it should be error and not empty list. We should probably need to change the interface.\r\n\r\nRegrading the GetPreferredAllocations(), so I understand you want to  use  {} to indicate that there are no preferred allocations. So instead of returning {} I think we should generate hints of all allocation and mark the good ones as preferred. (if there are not preferred allocation all the hints will be preferred =false) It up to the policy  to decide if to admit or not. So we can use best-effort policy top pick not preferred one. we can use  strict policy to failed. We can even add another policy that strict the GPU preferred allocation and best effort other resources like cpu and nic. (it like best-effort but with checking the GPU hint is preferred) This can be done once we pass the all resources hints as a map and implement  merge abstraction\r\nWhat do you think? ",
        "createdAt" : "2019-07-30T19:16:33Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "cdcc1608-48ab-40bf-9069-fda90adcc674",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Yes, of course. You are right. That is actually how it is designed and exactly what I outlined in the proposal / feedback document. Not sure what I was thinking when I wrote this this morning.\r\n\r\nSo yeah, not sure exactly when we would have a „care but no hints“. \r\n\r\nProbably need to rethink this.",
        "createdAt" : "2019-07-30T19:30:46Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "7594f733-92c9-4570-b7c7-100694c4929b",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "Yeah I agree here that we should always return hints, and look at the case where the resources have become unavailable and we no longer have enough to satisfy the request. \r\n\r\nShould the topology manager error and fail the pod? Or should it delegate that to the relevant providers, ie. when device manager comes to do actual allocation it will fail at this stage?",
        "createdAt" : "2019-08-15T11:07:11Z",
        "updatedAt" : "2019-08-16T06:06:27Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      },
      {
        "id" : "0f267fca-aff7-4c65-92ee-f8cd56c908df",
        "parentId" : "b6659697-98d2-4636-9ddf-a527e3931263",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "I have create PR to extend GetTopologyHints to return error see https://github.com/kubernetes/kubernetes/pull/81687",
        "createdAt" : "2019-08-20T22:05:26Z",
        "updatedAt" : "2019-08-20T22:05:27Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fdd52b058c60aa851f31efbff1a72153f123ecb",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +165,169 @@\t\thints := provider.GetTopologyHints(pod, container)\n\n\t\t// If hints is nil, insert a single, preferred any-socket hint into allProviderHints.\n\t\tif hints == nil || len(hints) == 0 {\n\t\t\tklog.Infof(\"[topologymanager] Hint Provider has no preference for socket affinity with any resource\")"
  },
  {
    "id" : "85a4a6a5-2eb2-40a7-b0fb-c6a1fe3a6989",
    "prId" : 73580,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73580#pullrequestreview-242604653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f3fd89e-a6fe-49c4-aaf7-975039fc8481",
        "parentId" : null,
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Why call this `RemovePod()` instead of `RemoveContainer()` since it only removes one containers info from the pod?\r\n\r\nAlso, when it is called in your follow-on PR in the container lifecycle code:\r\nhttps://github.com/kubernetes/kubernetes/pull/74357/files#diff-81cef540f416b0451c18470420adc42dR71\r\n\r\nit sits next to the CPUManagers `RemoveContainer()` call.",
        "createdAt" : "2019-04-26T10:36:23Z",
        "updatedAt" : "2019-07-17T08:30:02Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "f583d6aa-a9cf-4735-aee9-6c41316732c5",
        "parentId" : "3f3fd89e-a6fe-49c4-aaf7-975039fc8481",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Looks like this has been resolved in the newest revision.",
        "createdAt" : "2019-05-28T11:26:07Z",
        "updatedAt" : "2019-07-17T08:30:02Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d7e31e66e0f1b2c86a05c74d480f39a6e17af21",
    "line" : 220,
    "diffHunk" : "@@ -1,1 +257,261 @@\tklog.Infof(\"[topologymanager] RemoveContainer - Container ID: %v podTopologyHints: %v\", containerID, m.podTopologyHints)\n\treturn nil\n}\n\nfunc (m *manager) Admit(attrs *lifecycle.PodAdmitAttributes) lifecycle.PodAdmitResult {"
  },
  {
    "id" : "c21fccf2-37e2-4fb3-bfe7-3cc28f6a249b",
    "prId" : 73580,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73580#pullrequestreview-243851098",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b826661-1e25-43e0-9312-8b765e7d15ce",
        "parentId" : null,
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "You probably shouldn't remove the entire set of hints for a pod from `m.podTopologyHints` anytime one of the containers in the pod gets removed. Something more like the following is probably more appropriate:\r\n\r\n```\r\ndelete(m.podTopologyHints[podUIDString], containerID)\r\nif len(m.podTopologyHints[podUIDString]) == 0 {\r\n        delete(m.podTopologyHints, podUIDString)\r\n}\r\n```",
        "createdAt" : "2019-05-28T11:25:26Z",
        "updatedAt" : "2019-07-17T08:30:02Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "246f0555-d328-419c-9ca5-9fc4cd60fabd",
        "parentId" : "8b826661-1e25-43e0-9312-8b765e7d15ce",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Hmm, this won't quite work because `m.podTopologyHints[podUIDString]` isn't indexed by `containerID`, but rather by `containerName`. Not sure immediately how to address this...\r\n\r\nYou may need to loop through the `pod.ContainerStatus` and `pod.InitContainerStatus` slices looking for a match.",
        "createdAt" : "2019-05-28T19:00:25Z",
        "updatedAt" : "2019-07-17T08:30:02Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "abe520ff-c061-42fe-80e8-77b2a823111c",
        "parentId" : "8b826661-1e25-43e0-9312-8b765e7d15ce",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "Going to postpone a solution for this for now",
        "createdAt" : "2019-05-30T15:09:28Z",
        "updatedAt" : "2019-07-17T08:30:02Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d7e31e66e0f1b2c86a05c74d480f39a6e17af21",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +253,257 @@func (m *manager) RemoveContainer(containerID string) error {\n\tpodUIDString := m.podMap[containerID]\n\tdelete(m.podTopologyHints, podUIDString)\n\tdelete(m.podMap, containerID)\n\tklog.Infof(\"[topologymanager] RemoveContainer - Container ID: %v podTopologyHints: %v\", containerID, m.podTopologyHints)"
  },
  {
    "id" : "3a1a5732-c0d1-45cd-9e68-2893094c204a",
    "prId" : 73580,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73580#pullrequestreview-262886035",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef9b34ea-5d70-4f39-bf94-4caf5abf71d2",
        "parentId" : null,
        "authorId" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "body" : "Why is that? I can have 4 devices on the socket 0 and another 4 on socket 1. Devices can be linked as in example of Nvidia with NVlink. Pod can request 2 devices and it is expected that those two will be allocated from same socket, regardless of other resources that affects PodQOS* classification.",
        "createdAt" : "2019-05-31T15:37:35Z",
        "updatedAt" : "2019-07-17T08:30:02Z",
        "lastEditedBy" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "tags" : [
        ]
      },
      {
        "id" : "f6c91ae7-54cb-477b-90af-5c0a8ac89a0f",
        "parentId" : "ef9b34ea-5d70-4f39-bf94-4caf5abf71d2",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I agree. This probably shouldn't be a constraint in the `TopologyManager`, and rather each `HintProvider` should decide this for themselves. I think it is OK, for the initial, `alpha` implementation though. We can revisit it before this feature goes beta.",
        "createdAt" : "2019-06-01T16:05:59Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "df8a6f76-7fd9-41cf-8fb9-066f0bdb6e81",
        "parentId" : "ef9b34ea-5d70-4f39-bf94-4caf5abf71d2",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Actually. I take it back. In order for the `CPUManager` to allocate CPUs in an exclusive manner, the pod has to be in the Guaranteed QoS tier. The hints are useless if the `CPUManager` can't actually act on them, and it can't if the Pod is not Guaranteed.",
        "createdAt" : "2019-06-01T17:07:04Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "c9adbdf8-3726-4e62-97c2-bb3523e169a6",
        "parentId" : "ef9b34ea-5d70-4f39-bf94-4caf5abf71d2",
        "authorId" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "body" : "@klueska this is only in case if the CPU manager policy is `static`. If it is `none` - there is no need of Pod to be Guaranteed. If static policy gets improved in next years, it is also not going to be a problem. There is no big issues to dynamically adjust cpuset for pods to be aligned to the socket even in shared pool, it is just limitation on currently implemented `static` policy.\r\n",
        "createdAt" : "2019-06-01T18:27:31Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "659c7c1f-39ba-41a7-8331-fcc6b3b5f2fb",
        "tags" : [
        ]
      },
      {
        "id" : "0f18450a-375f-4442-b9c0-7c0897a3c644",
        "parentId" : "ef9b34ea-5d70-4f39-bf94-4caf5abf71d2",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "At least right now if it is `none`, then the hints don't provide any useful information. Nothing can be acted on by the hints. I do get your point though. Other `CPUManager` policies could be created in the future that _could_ act on the hints even if the QoS tier isn't Guaranteed, so we shouldn't artificially limit the `TopologyManager` to accept hints in this case alone. I still think it's fine for this `alpha`, but I agree we should revisit.",
        "createdAt" : "2019-06-01T19:03:57Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "80ef3444-669c-40b0-84d5-761abd0c405a",
        "parentId" : "ef9b34ea-5d70-4f39-bf94-4caf5abf71d2",
        "authorId" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "body" : "@klueska or @kad, mind making a ticket about this?",
        "createdAt" : "2019-06-27T16:12:47Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "tags" : [
        ]
      },
      {
        "id" : "f89aa23a-2439-429e-87b9-f4f202083395",
        "parentId" : "ef9b34ea-5d70-4f39-bf94-4caf5abf71d2",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "I am going to go ahead and resolve this conversation, and we can revisit this discussion after alpha. Thanks!",
        "createdAt" : "2019-07-17T08:51:14Z",
        "updatedAt" : "2019-07-17T08:51:14Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d7e31e66e0f1b2c86a05c74d480f39a6e17af21",
    "line" : 247,
    "diffHunk" : "@@ -1,1 +284,288 @@\n\t} else {\n\t\tklog.Infof(\"[topologymanager] Topology Manager only affinitises Guaranteed pods.\")\n\t}\n"
  },
  {
    "id" : "acf26690-e151-44ea-820d-1824eba0a8f1",
    "prId" : 73580,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73580#pullrequestreview-244594267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0533f7a-b11d-4fb5-bdda-fe5f8d330170",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "It seems TopologyHint construction can be moved outside the loop.\r\nThe assignment on line 165 can just use the TopologyHint instance.",
        "createdAt" : "2019-06-01T15:50:34Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "d4550197-e222-4430-b4d8-5f11357b7c91",
        "parentId" : "d0533f7a-b11d-4fb5-bdda-fe5f8d330170",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "This feels like a premature optimization (in practice there are only a small number of hint providers), and could decrease the readability of the code for little added benefit.",
        "createdAt" : "2019-06-01T17:00:03Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d7e31e66e0f1b2c86a05c74d480f39a6e17af21",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +167,171 @@\t\tif hints == nil || len(hints) == 0 {\n\t\t\tklog.Infof(\"[topologymanager] Hint Provider has no preference for socket affinity\")\n\t\t\taffinity, _ := socketmask.NewSocketMask()\n\t\t\taffinity.Fill()\n\t\t\thints = []TopologyHint{{affinity, true}}"
  },
  {
    "id" : "8c259974-3778-4926-bc4f-ce3ee4f42a9e",
    "prId" : 73580,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73580#pullrequestreview-259675463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d0238e0-a71e-49f1-ac8f-fd1c4f4408eb",
        "parentId" : null,
        "authorId" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "body" : "nit: this test could be negated, moving the assignment into the body, eliminating the previous test and the unconditional assignment below. (optional)",
        "createdAt" : "2019-06-27T16:11:50Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "tags" : [
        ]
      },
      {
        "id" : "d60c0c2c-c8a4-4059-b4af-94223005ca12",
        "parentId" : "9d0238e0-a71e-49f1-ac8f-fd1c4f4408eb",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Yeah, I actually thought about this when I wrote this, but I liked having a series of conditional checks with an unconditional setting of the best hint if they weren't satisfied.\r\n\r\nThis actually breaks down with the assignment being done on line 214 above though, so I'd be OK with whatever makes this most clear.",
        "createdAt" : "2019-07-09T18:13:54Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d7e31e66e0f1b2c86a05c74d480f39a6e17af21",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +229,233 @@\t\t// mergedHints that have a narrower SocketAffinity than the\n\t\t// SocketAffinity in the current bestHint.\n\t\tif !mergedHint.SocketAffinity.IsNarrowerThan(bestHint.SocketAffinity) {\n\t\t\treturn\n\t\t}"
  },
  {
    "id" : "26e6c930-fb94-4fee-86c8-d56002de3ffb",
    "prId" : 73580,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73580#pullrequestreview-262885444",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a5ce0cf-27e6-4012-81a5-450d09fa9aae",
        "parentId" : null,
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : "Can you explain why to do And and not OR? So I think you want  to create mergedAffinity which has the socketAffinities of all the hints in the  permutation. Then in Line 228 you will take the Narrower socketAffint hint anyway.\r\nSo if the device  manager hinst is [(1,1 prefered= true)]  and the cpu manager is [(0,1 prefered= true), (1,0 prefered= true), (1,1 prefered= true)] the topology manger hint will be (1,1 prefered= true).",
        "createdAt" : "2019-07-03T21:40:37Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "7377a20e-c30b-41fc-82c9-99c88757a12f",
        "parentId" : "2a5ce0cf-27e6-4012-81a5-450d09fa9aae",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Hmm. I'm a bit confused by your example, because I would actually expect the correct output of the TopologyManager after the merge to be either (0,1 preferred= true) or (1,0 preferred= true), but never (1,1 preferred= true).\r\n\r\nIn one sense this is a contrived example, however, because the CPUManager should never return hints of [(0,1 preferred= true), (1,0 preferred= true), (1,1 preferred= true)]. The (1,1) case will never be a preferred allocation if it is possible to allocate all requested CPUs from either (0,1) or (1,0).\r\n\r\nThat is to say, the only time you should ever set (1,1 preferred= true) for any hint provider is if there is no other NUMA alignment possible (i.e. you only set (1,1 preferred= true) if the allocation request can only be satisfied by allocating devices from both sockets).\r\n\r\nAs such, the merge algorithm is currently designed to search for the narrowest socket mask that satisfies alignment on (at least) one of the sockets from (at least) one of the preferred socket mask hints from all providers. I.e. it treats the mask as an actual _mask_, where setting a bit in the mask indicates that is is OK to allocate a device with affinity to that socket (but it is not necessarily required).\r\n\r\nIn your example above (adjusted to account for the proper setting of the preferred field for the last hing in the CPUManager), this would generate the following merged hints:\r\n```\r\n[(1,1, preferred=true), (0,1, preferred=true)] = (0,1, preferred=true)\r\n[(1,1, preferred=true), (1,0, preferred=true)] = (1,0, preferred=true)\r\n[(1,1, preferred=true), (1,1, preferred=false)] = (1,1, preferred=false)\r\n```\r\nWith (0,1, preferred=true) winning out in the end.\r\n\r\nIt sounds like you are promoting a different algorithm, however. One that guarantees allocation of devices according to exact matches in socket affinities from all hint providers. I.e one that would result in the following (I drop the preferred flag here because its existence would be meaningless in this setup):\r\n```\r\n[(1,1), (0,1)] = (0,0)\r\n[(1,1), (1,0)] = (0,0)\r\n[(1,1), (1,1)] = (1,1)\r\n```\r\nWith (1,1) winning out, and the devicemanager and CPUManager now being told to explicitly allocate devices from *both* sockets (rather than interpreting the socket mask as an \"OK\" to allocate from one, the other, or both).",
        "createdAt" : "2019-07-09T18:11:28Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "73dc64c0-201a-4427-abf6-f40dddea46bb",
        "parentId" : "2a5ce0cf-27e6-4012-81a5-450d09fa9aae",
        "authorId" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "body" : " yes you are absolutely right, that was my interpretation of strict mode. (and why I didn't understand the need for preferred flag:) )",
        "createdAt" : "2019-07-09T20:21:21Z",
        "updatedAt" : "2019-07-17T08:30:03Z",
        "lastEditedBy" : "2ed5c6ef-f119-4350-84bd-11a7582f6d4f",
        "tags" : [
        ]
      },
      {
        "id" : "93407687-089a-4b4c-a568-3f6fafbf4850",
        "parentId" : "2a5ce0cf-27e6-4012-81a5-450d09fa9aae",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "I think the original comment was resolved, and the discussion on the strict poilcy interpretation can be moved to Kevin's google doc. So I am going to resolve this. Thanks for the discussion!",
        "createdAt" : "2019-07-17T08:50:12Z",
        "updatedAt" : "2019-07-17T08:50:12Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d7e31e66e0f1b2c86a05c74d480f39a6e17af21",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +200,204 @@\t\tmergedAffinity, _ := socketmask.NewSocketMask()\n\t\tmergedAffinity.Fill()\n\t\tmergedAffinity.And(socketAffinities...)\n\n\t\t// Build a mergedHintfrom the merged affinity mask, indicating if an"
  }
]