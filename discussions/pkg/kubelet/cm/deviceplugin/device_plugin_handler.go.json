[
  {
    "id" : "400da5f5-d4dc-4c54-bfd5-85b0eb53fd96",
    "prId" : 54488,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54488#pullrequestreview-73522731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "525a7c1f-f728-47a7-95fd-d65c6ca41f31",
        "parentId" : null,
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Node Allocatable isn't supposed to be updated.\r\n[Allocatable != Resources available](https://github.com/kubernetes/community/blob/aab1ceb21501beef8b692ac265dfc65e8150e354/contributors/design-proposals/node/node-allocatable.md) :)\r\n\r\nResource accounting is done at the scheduler level.",
        "createdAt" : "2017-10-28T22:06:23Z",
        "updatedAt" : "2017-11-02T01:20:42Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "ff46f241-1a12-4a41-b71e-2fd564b15734",
        "parentId" : "525a7c1f-f728-47a7-95fd-d65c6ca41f31",
        "authorId" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "body" : "Well, it's mainly to solve the problem that when kubelet restarts, pods that are already scheduled on the node wouldn't fail to restart upon device plugin failure.  This case, kubelet will remove its registered resource from its capacity, and we need to update Node Allocatable to ensure it passes corresponding predicates in admission if allocation succeed.",
        "createdAt" : "2017-10-29T03:55:28Z",
        "updatedAt" : "2017-11-02T01:20:42Z",
        "lastEditedBy" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "tags" : [
        ]
      },
      {
        "id" : "0bc73576-7c38-43fe-8a6e-ebf49fbe4e06",
        "parentId" : "525a7c1f-f728-47a7-95fd-d65c6ca41f31",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "This doesn't change the actual node allocatable but just changes the value used in the general predicator.",
        "createdAt" : "2017-10-29T05:07:23Z",
        "updatedAt" : "2017-11-02T01:20:42Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      },
      {
        "id" : "fda2bb3a-bef4-40eb-9c18-66f37edf14e8",
        "parentId" : "525a7c1f-f728-47a7-95fd-d65c6ca41f31",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Cool :) Thanks for clarifying this",
        "createdAt" : "2017-11-01T15:58:37Z",
        "updatedAt" : "2017-11-02T01:20:42Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "06308963833f12e087f7db4ced5289e3c39f4159",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +295,299 @@\t}\n\tif newAllocatableResource != nil {\n\t\tnode.SetAllocatableResource(newAllocatableResource)\n\t}\n}"
  },
  {
    "id" : "0ea855dd-cb93-40ed-9740-cc3e7b0caad4",
    "prId" : 54019,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54019#pullrequestreview-71360290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0da0a5e4-b395-4290-b4ff-1fddb04107ff",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "This can result in a race - Imagine a pod getting rejected by the kubelet because it temporarily stranded resources that it has previously allocated to a pod it had rejected because Allocation failed. ",
        "createdAt" : "2017-10-23T22:40:32Z",
        "updatedAt" : "2017-10-25T00:49:13Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "a8b62a32-2d7a-4a44-9a5f-b24af0048a8e",
        "parentId" : "0da0a5e4-b395-4290-b4ff-1fddb04107ff",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "May be run an update before every Allocation and ensure that the update is cheap?",
        "createdAt" : "2017-10-23T22:40:55Z",
        "updatedAt" : "2017-10-25T00:49:13Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "c2e6aa4a-73fd-4c81-82c5-91d920ff7f4b",
        "parentId" : "0da0a5e4-b395-4290-b4ff-1fddb04107ff",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "We are currently running h.updateAllocatedDevices() at the beginning of Allocate. I wouldn't say the update is super cheap because it is bound by the no. of activePods. It is a good point that we would like to make it cheap, at least for common cases of normal pod/container allocation. I updated the code to only call h.updateAllocatedDevices() when the pod requires device plugin resources.",
        "createdAt" : "2017-10-23T23:53:23Z",
        "updatedAt" : "2017-10-25T00:49:13Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e501f01d8593a451ba7c30b00d738d58cecad0b4",
    "line" : 205,
    "diffHunk" : "@@ -1,1 +203,207 @@\t\t// and container2 both require X resource A and Y resource B. Both allocation\n\t\t// requests may fail if we serve them in mixed order.\n\t\t// TODO: may revisit this part later if we see inefficient resource allocation\n\t\t// in real use as the result of this. Should also consider to parallize device\n\t\t// plugin Allocate grpc calls if it becomes common that a container may require"
  },
  {
    "id" : "848dce5f-ab79-486b-9c5a-a0da744181c2",
    "prId" : 54019,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54019#pullrequestreview-71712682",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "924824b9-15c1-4730-a161-3b877cc7b268",
        "parentId" : null,
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "I'm a bit surprised by the usage of the Mutex here. Usually you would want to keep critical section to a minimum.\r\nWe are more in a big lock situation here.\r\n",
        "createdAt" : "2017-10-24T22:44:15Z",
        "updatedAt" : "2017-10-25T00:49:13Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "2a7ac85e-534c-451c-9277-920cc30c8bb3",
        "parentId" : "924824b9-15c1-4730-a161-3b877cc7b268",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "I think this is needed to synchronize read/write access on h.podDevices. Also this part shouldn't be expensive.",
        "createdAt" : "2017-10-25T00:26:10Z",
        "updatedAt" : "2017-10-25T00:49:13Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e501f01d8593a451ba7c30b00d738d58cecad0b4",
    "line" : 233,
    "diffHunk" : "@@ -1,1 +231,235 @@// for the found one. An empty struct is returned in case no cached state is found.\nfunc (h *HandlerImpl) GetDeviceRunContainerOptions(pod *v1.Pod, container *v1.Container) *DeviceRunContainerOptions {\n\th.Lock()\n\tdefer h.Unlock()\n\treturn h.podDevices.deviceRunContainerOptions(string(pod.UID), container.Name)"
  }
]