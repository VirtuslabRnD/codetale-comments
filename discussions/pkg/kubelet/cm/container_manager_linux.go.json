[
  {
    "id" : "d4e0f9a1-c2ca-4c20-a7a3-6c1e7f14b2d8",
    "prId" : 97888,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97888#pullrequestreview-574721249",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "089cfbdc-1c7d-4ebd-9cd4-c5b705771568",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "Maybe add a comment here that this is the best effort lookup as pid file may be overridden: https://docs.docker.com/engine/reference/commandline/dockerd/#daemon:~:text=%2Dp%2C%20%2D%2Dpidfile%20string,for%20daemon%20PID%20file%20(default%20%22%2Fvar%2Frun%2Fdocker.pid%22)",
        "createdAt" : "2021-01-23T01:38:46Z",
        "updatedAt" : "2021-01-23T02:55:52Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      }
    ],
    "commit" : "89c42bd3d5782a002bd48c6a3b89d1c5135a9252",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +72,76 @@\tdockerProcessName = \"dockerd\"\n\t// dockerd option --pidfile can specify path to use for daemon PID file, pid file path is default \"/var/run/docker.pid\"\n\tdockerPidFile         = \"/var/run/docker.pid\"\n\tcontainerdProcessName = \"containerd\"\n\tmaxPidFileLength      = 1 << 10 // 1KB"
  },
  {
    "id" : "c5d59db5-8102-47de-94c5-6165e7f5b8ef",
    "prId" : 93931,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93931#pullrequestreview-466713106",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b80f1003-1b0f-4376-af0c-80c9283b1249",
        "parentId" : null,
        "authorId" : "b7c04289-77b0-4d05-90b7-3bde23e2b0bf",
        "body" : "You could maybe make this a constant and move it to the top of the file, but that is up to you.",
        "createdAt" : "2020-08-12T16:54:37Z",
        "updatedAt" : "2020-08-12T16:55:21Z",
        "lastEditedBy" : "b7c04289-77b0-4d05-90b7-3bde23e2b0bf",
        "tags" : [
        ]
      },
      {
        "id" : "355e463e-b306-4405-b8ab-e31ff5970386",
        "parentId" : "b80f1003-1b0f-4376-af0c-80c9283b1249",
        "authorId" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "body" : "Thanks @odinuge !  Your suggestion is good! \r\nBut since it is only used in this function, so I prefer to keep it that way for now.",
        "createdAt" : "2020-08-13T12:28:54Z",
        "updatedAt" : "2020-08-13T12:28:54Z",
        "lastEditedBy" : "e7b8fd7e-f93b-44b6-b6d0-4331207d901c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad1739f8bc9b77f989d525dc248ca13edcafe526",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +217,221 @@\tif failSwapOn {\n\t\t// Check whether swap is enabled. The Kubelet does not support running with swap enabled.\n\t\tswapFile := \"/proc/swaps\"\n\t\tswapData, err := ioutil.ReadFile(swapFile)\n\t\tif err != nil {"
  },
  {
    "id" : "2a34f525-8441-4dd8-ad15-74e2b28263ae",
    "prId" : 92863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92863#pullrequestreview-690995745",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6b0e603-acc7-49a7-8d00-1e0295574339",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "this case should add error to the errors list\r\n```\r\nerrList = append(errList, err)\r\n```\r\n\r\n\r\n?\r\n\r\n/hold",
        "createdAt" : "2021-06-21T16:44:23Z",
        "updatedAt" : "2021-06-21T16:44:23Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "bfd187e4-66c3-4e07-a4b8-8133fd52cdd5",
        "parentId" : "c6b0e603-acc7-49a7-8d00-1e0295574339",
        "authorId" : "7ff04a68-17e8-419a-84b6-e644739df26f",
        "body" : "Yes, `errList = append(errList, err)` is called in L465\r\n",
        "createdAt" : "2021-06-22T10:12:07Z",
        "updatedAt" : "2021-06-22T10:12:08Z",
        "lastEditedBy" : "7ff04a68-17e8-419a-84b6-e644739df26f",
        "tags" : [
        ]
      },
      {
        "id" : "839b4f79-84fb-45ec-bd16-15b2af6c0fd6",
        "parentId" : "c6b0e603-acc7-49a7-8d00-1e0295574339",
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "I see, I was confused by continue `above`, sorry",
        "createdAt" : "2021-06-23T18:08:42Z",
        "updatedAt" : "2021-06-23T18:08:42Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      }
    ],
    "commit" : "26e83ac4d4398ed94ed5391e4faed54824ed9a4d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +462,466 @@\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tklog.ErrorS(err, \"Updating kernel flag failed (Hint: enable KubeletInUserNamespace feature flag to ignore the error)\", \"flag\", flag)\n\t\t\t\t}\n\t\t\t\terrList = append(errList, err)"
  },
  {
    "id" : "a9ad590a-dca8-4f25-8a4f-ab514703684a",
    "prId" : 81086,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81086#pullrequestreview-275677773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9c31895-29cd-44a8-950f-d2fea7851dad",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Q: If it is a situation in which (for example) the file name is too long (i.e. an `ENAMETOOLONG` error), do we want to return that it isn't a kernel pid or do we want to return some other form of error message?",
        "createdAt" : "2019-08-08T18:53:47Z",
        "updatedAt" : "2019-08-08T18:54:13Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "982029a0-76f2-4f57-8a30-d982bccb0ea6",
        "parentId" : "f9c31895-29cd-44a8-950f-d2fea7851dad",
        "authorId" : "5ab6d693-9905-4525-856e-7bdf0d85ce64",
        "body" : "It's hard to imagine a kernel PID would belong to a process spawned in a too long path so imho not returning an error is fine here.",
        "createdAt" : "2019-08-15T21:23:18Z",
        "updatedAt" : "2019-08-15T21:23:27Z",
        "lastEditedBy" : "5ab6d693-9905-4525-856e-7bdf0d85ce64",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd925d6611ef6cae9ffef98a5d062eae0bfb07ff",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +889,893 @@\t// Kernel threads have no associated executable.\n\t_, err := os.Readlink(fmt.Sprintf(\"/proc/%d/exe\", pid))\n\treturn err != nil && os.IsNotExist(err)\n}\n"
  },
  {
    "id" : "3ffa3aff-c95e-4791-a19f-733b228fbfe8",
    "prId" : 79671,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79671#pullrequestreview-263814894",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d320b49d-52eb-45ae-93de-8baf8a26dffa",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I see how this is simpler than the alternative, and runc is making the variable global and writable for better or worse.  Since a proper fix was put in master, and there is no other known writer to that var I could find, this is probably reasonable and small.",
        "createdAt" : "2019-07-18T17:53:11Z",
        "updatedAt" : "2019-07-18T17:53:11Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "820a717bce3ef92f9280a4870d449c1e903255f2",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +207,211 @@\t// for more info.\n\tfor i, pageSize := range fs.HugePageSizes {\n\t\tfs.HugePageSizes[i] = strings.ReplaceAll(pageSize, \"kB\", \"KB\")\n\t}\n"
  },
  {
    "id" : "f253c591-fd93-4231-a393-89b9641a6038",
    "prId" : 77429,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77429#pullrequestreview-233953901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b62a832f-3a56-47b4-b032-80b23f2ee9d2",
        "parentId" : null,
        "authorId" : "05637862-b60e-403e-8519-09d1b3f0c9c2",
        "body" : "IMO, that's wrong\r\n`errs = []error{}` in line#841 has the same function",
        "createdAt" : "2019-05-06T08:17:27Z",
        "updatedAt" : "2019-05-30T00:26:09Z",
        "lastEditedBy" : "05637862-b60e-403e-8519-09d1b3f0c9c2",
        "tags" : [
        ]
      },
      {
        "id" : "5367ca64-3c53-4d01-94ae-f7f21b5fd521",
        "parentId" : "b62a832f-3a56-47b4-b032-80b23f2ee9d2",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "If attemptsRemaining >= 0, the loop would continue, leading to code on line 841.\r\n\r\nSee also Matt's comment.",
        "createdAt" : "2019-05-06T12:32:56Z",
        "updatedAt" : "2019-05-30T00:26:09Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "c46ec66a1fd88365220596ed03322cedfe16a761",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +841,845 @@\t\tif err != nil {\n\t\t\tfinalErr = fmt.Errorf(\"failed to list PIDs for root: %v\", err)\n\t\t\tcontinue\n\t\t}\n"
  },
  {
    "id" : "322351d9-e1f7-42e2-91fb-f5867a518bf2",
    "prId" : 77429,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77429#pullrequestreview-260897957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f1293ad-c17b-4d22-b458-9f6ad9dc3899",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "My understanding is we attempt to find and move non-kernel thread multiple times just so that we don't miss any transient pids. This change makes this loop a single pass at best, and so this is a change in existing logic, bot a simple bug fix. @dashpole is this change an informed decision?",
        "createdAt" : "2019-07-11T18:02:14Z",
        "updatedAt" : "2019-07-11T18:02:14Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "ede51216-6581-4c29-970e-d2e30e4d691a",
        "parentId" : "9f1293ad-c17b-4d22-b458-9f6ad9dc3899",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "> This change makes this loop a single pass at best, and so this is a change in existing logic, not a simple bug fix.\r\n\r\nThis is incorrect.  This case is only hit when there are no non-kernel pids to move.  In the previous implementation, the `break` breaks out of the loop to the return at the end of the function.  In this case, we just return directly rather than break and return.",
        "createdAt" : "2019-07-11T18:24:38Z",
        "updatedAt" : "2019-07-11T18:24:38Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "73ce37b7-fbc9-4e08-ab66-fcdf25c46897",
        "parentId" : "9f1293ad-c17b-4d22-b458-9f6ad9dc3899",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Ah gotcha. Makes sense",
        "createdAt" : "2019-07-11T19:05:34Z",
        "updatedAt" : "2019-07-11T19:05:34Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "c46ec66a1fd88365220596ed03322cedfe16a761",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +857,861 @@\t\t// Check if we have moved all the non-kernel PIDs.\n\t\tif len(pids) == 0 {\n\t\t\treturn nil\n\t\t}\n"
  },
  {
    "id" : "71932c72-400d-4979-a346-86e27491e410",
    "prId" : 74357,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74357#pullrequestreview-264854591",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5c4f4e8-ba2b-4b48-bf0d-17e21f9e721a",
        "parentId" : null,
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "In the other OSes, we return a nil `topologyManager`.  I don't think we want to use a fake manager outside of test code right?",
        "createdAt" : "2019-07-17T14:36:35Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "762d9b93-7ede-4837-b62e-8fb551e6d3dc",
        "parentId" : "c5c4f4e8-ba2b-4b48-bf0d-17e21f9e721a",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "+1",
        "createdAt" : "2019-07-17T14:46:01Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "dfbe98c0-a67d-4d81-adf4-526c8f69fbb3",
        "parentId" : "c5c4f4e8-ba2b-4b48-bf0d-17e21f9e721a",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "From what I can see, the other OSes don't have TopologyManager(or device/cpu manager) as part of the containerManagerImpl so there is no reference, so we don't return anything?\r\n\r\n I was following the convention used by DeviceManager which returns a ManagerStub below. There is also a reference to the Fake CPU Manager in the other OSes when returning the internalContainerLifecycleImpl.\r\n\r\nIf I return nil, the code errors as cm.TopologyManager functions are called below. So I could wrap those calls in a check for the feature gate but looks like the fake/stub managers are used outside of test code.",
        "createdAt" : "2019-07-17T16:23:53Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      },
      {
        "id" : "40947d5d-7a09-4e5c-9825-47fbbb42cd55",
        "parentId" : "c5c4f4e8-ba2b-4b48-bf0d-17e21f9e721a",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I actually think the only call you would need to wrap is this one (which probably makes sense to wrap anyway):\r\n\r\n```\r\npkg/kubelet/kubelet.go:\r\nklet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetTopologyPodAdmitHandler())\r\n```\r\n\r\nLooking through the code, I don't see anywhere else that `cm.topologyManager` is referenced (other than https://github.com/kubernetes/kubernetes/pull/74357/files#diff-06aa1b61a9bc4a50aa8d8b0a69d95bb2R347), whose internal references to the `topologyManager` are already protected by a check on the feature gate:\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/74357/files#diff-81cef540f416b0451c18470420adc42dR40",
        "createdAt" : "2019-07-18T11:49:14Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "274e3019-b334-4f79-bbd2-df5932f28a6d",
        "parentId" : "c5c4f4e8-ba2b-4b48-bf0d-17e21f9e721a",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "Ah yes I see the code I was referring to is actually in the CPU and Device Manager PRs.\r\nhttps://github.com/kubernetes/kubernetes/pull/73920 & https://github.com/kubernetes/kubernetes/pull/74345 have the references to topology manager to register them as Hint Providers ",
        "createdAt" : "2019-07-18T15:24:28Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      },
      {
        "id" : "23da7c6f-7e4b-4492-bab3-a977afbe070b",
        "parentId" : "c5c4f4e8-ba2b-4b48-bf0d-17e21f9e721a",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "It still feels more natural to me to remove the `FakeManager()` here and protect all of the accesses to `cm.topologymanager` with a check on the feature gate, but I will leave it to @sjennings for the final call.",
        "createdAt" : "2019-07-19T10:45:04Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "930526a1-5e97-4196-9427-425fd7eea6e8",
        "parentId" : "c5c4f4e8-ba2b-4b48-bf0d-17e21f9e721a",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "we can clean it up in a follow on.  there are a number of things that can be cleaned up.  i'm making a list :)",
        "createdAt" : "2019-07-22T14:29:29Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f0081cc36991f9099a51129b6f24a35ed82b965",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +299,303 @@\t\tklog.Infof(\"[topologymanager] Initilizing Topology Manager with %s policy\", nodeConfig.ExperimentalTopologyManagerPolicy)\n\t} else {\n\t\tcm.topologyManager = topologymanager.NewFakeManager()\n\t}\n"
  },
  {
    "id" : "3b1fa55e-6b11-4754-bfd4-8f226940eebe",
    "prId" : 74357,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74357#pullrequestreview-263594612",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae4199fa-d408-4cfa-b3db-6077355ed41c",
        "parentId" : null,
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "this message could conflict if the policy name in the flag is invalid and `NewManager` falls back to the default policy",
        "createdAt" : "2019-07-17T14:39:45Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "ef679486-fbd9-470c-9bc6-fc2181574e35",
        "parentId" : "ae4199fa-d408-4cfa-b3db-6077355ed41c",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I would argue that falling back to the default policy on an invalid policy name in the flag is bad behaviour. It should instead return an error, and fail fast in this case.",
        "createdAt" : "2019-07-17T14:51:24Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "a91dd703-cca8-47fc-a34c-5846e8038c92",
        "parentId" : "ae4199fa-d408-4cfa-b3db-6077355ed41c",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "i agree, this should be fatal to the kubelet, `NewManager` should return a `(Manager, err)`",
        "createdAt" : "2019-07-17T14:54:08Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "2f01838b-4644-4a3e-86aa-09a318a5c3a4",
        "parentId" : "ae4199fa-d408-4cfa-b3db-6077355ed41c",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I think this was borrowed from the semantics of the CPUManager, which currently does a similar fallback to the None policy if an incorrect name is passed in the flag.\r\n\r\nIt's probably not worth blocking this PR until this is in place, since (at the moment) it's just potential logging error.\r\n\r\nI will open a followup PR once this is merged to address this (in both the TopologyManager and the CPUManager).",
        "createdAt" : "2019-07-17T15:06:47Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "634e78ef-872a-4271-aa32-72891f2bd323",
        "parentId" : "ae4199fa-d408-4cfa-b3db-6077355ed41c",
        "authorId" : "575530a6-8847-4729-9bb3-71927fc83799",
        "body" : "Going to resolve this conversation as there is a follow on PR planned. Thanks!",
        "createdAt" : "2019-07-18T08:30:47Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "575530a6-8847-4729-9bb3-71927fc83799",
        "tags" : [
        ]
      },
      {
        "id" : "64d24b9c-8e3f-4698-96d1-ffaa86fdb7e2",
        "parentId" : "ae4199fa-d408-4cfa-b3db-6077355ed41c",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Here is the PR: https://github.com/kubernetes/kubernetes/pull/80294\r\n\r\nDepending on which one lands first (either this PR or that one), one of them will need to be updated to make the change that handles `NewManager()` returning an `error` at this point in the code.",
        "createdAt" : "2019-07-18T11:36:46Z",
        "updatedAt" : "2019-07-24T14:04:37Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f0081cc36991f9099a51129b6f24a35ed82b965",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +297,301 @@\t\t}\n\n\t\tklog.Infof(\"[topologymanager] Initilizing Topology Manager with %s policy\", nodeConfig.ExperimentalTopologyManagerPolicy)\n\t} else {\n\t\tcm.topologyManager = topologymanager.NewFakeManager()"
  },
  {
    "id" : "806d355a-06a1-4ec4-a0b9-d8e8ff4ec707",
    "prId" : 74357,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74357#pullrequestreview-269756076",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e21d3df6-ad95-4da6-ac0e-5e90966060fc",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "nit: \"Initializing\"",
        "createdAt" : "2019-08-01T17:04:26Z",
        "updatedAt" : "2019-08-01T17:05:38Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f0081cc36991f9099a51129b6f24a35ed82b965",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +297,301 @@\t\t}\n\n\t\tklog.Infof(\"[topologymanager] Initilizing Topology Manager with %s policy\", nodeConfig.ExperimentalTopologyManagerPolicy)\n\t} else {\n\t\tcm.topologyManager = topologymanager.NewFakeManager()"
  },
  {
    "id" : "5669f00b-e823-4a1b-b128-e78fd69c791d",
    "prId" : 73651,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73651#pullrequestreview-203545201",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b91f3f1e-bb5e-4285-8a3f-4b26f77ebfd1",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "It seems internalCapacity only needs to be allocated when the above if condition is true.\r\nOtherwise capacity can be used.",
        "createdAt" : "2019-02-14T02:30:23Z",
        "updatedAt" : "2019-02-14T02:30:23Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "2597a1d97ef4d8f54b1ca661453e32794b756909",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +237,241 @@\tpidlimits, err := pidlimit.Stats()\n\tif err == nil && pidlimits != nil && pidlimits.MaxPID != nil {\n\t\tinternalCapacity[pidlimit.PIDs] = *resource.NewQuantity(\n\t\t\tint64(*pidlimits.MaxPID),\n\t\t\tresource.DecimalSI)"
  },
  {
    "id" : "b1877cda-d7e0-41bb-aeff-da10dfa10e73",
    "prId" : 62541,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/62541#pullrequestreview-114989411",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c84734f1-257c-4008-9306-7a22b3d61f77",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "is this comment still accurate?  its still the internal canonical name, no?\r\n\r\nthis pr is a nice a cleanup, but the usage of string meant absolute name, and CgroupName had meant internal canonical name.",
        "createdAt" : "2018-04-15T20:57:16Z",
        "updatedAt" : "2018-05-01T15:29:18Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "d3c695d4-4968-4c06-85be-10d80798d3ba",
        "parentId" : "c84734f1-257c-4008-9306-7a22b3d61f77",
        "authorId" : "fd5eb1fb-f30a-4735-908c-944e87d99845",
        "body" : "Yes, I think the comment is still accurate, since CgroupName is still an absolute path (even though it's encoded as a slice of strings rather than a single string.) If you'd like me to make that more explicit, I can do that, but I think it's fine for now...",
        "createdAt" : "2018-04-24T22:36:48Z",
        "updatedAt" : "2018-05-01T15:29:18Z",
        "lastEditedBy" : "fd5eb1fb-f30a-4735-908c-944e87d99845",
        "tags" : [
        ]
      }
    ],
    "commit" : "b230fb8ac4070c3c8b5f0388599e7b9c240b3191",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +124,128 @@\t// Absolute cgroupfs path to a cgroup that Kubelet needs to place all pods under.\n\t// This path include a top level container for enforcing Node Allocatable.\n\tcgroupRoot CgroupName\n\t// Event recorder interface.\n\trecorder record.EventRecorder"
  },
  {
    "id" : "08245ff5-66a8-4b39-8e50-9f5eb0842452",
    "prId" : 59769,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59769#pullrequestreview-105048999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "106b428d-4909-4c31-ba25-9af73508d835",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "shouldn't this be gated by the feature gate?",
        "createdAt" : "2018-03-19T15:04:55Z",
        "updatedAt" : "2018-03-19T15:04:55Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "f06bcd35-4579-4351-a819-92aef0d0a403",
        "parentId" : "106b428d-4909-4c31-ba25-9af73508d835",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "The feature gate which prevents adding Local Storage to node Capacity and Allocatable is in `kubelet_node_status.go`.  This PR doesn't change that.  Although I agree that having the capacity in the container manager be different from the capacity on the node status is confusing and bug-prone.  I can add another feature gate here if you think that is a good idea.",
        "createdAt" : "2018-03-19T16:37:53Z",
        "updatedAt" : "2018-03-19T16:37:53Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "02f43c08-fafc-4d23-8b8e-f385ac662820",
        "parentId" : "106b428d-4909-4c31-ba25-9af73508d835",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "@dashpole the problem we are tracking is a regression in podstartup time - https://github.com/kubernetes/kubernetes/issues/60589#issuecomment-374236943 looks like this PR could be contributing to it right?",
        "createdAt" : "2018-03-19T16:40:07Z",
        "updatedAt" : "2018-03-19T16:40:07Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      },
      {
        "id" : "6e570e46-9840-4ff8-af5b-1029ae364ae8",
        "parentId" : "106b428d-4909-4c31-ba25-9af73508d835",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "@dims this may contribute to node startup, but pod startup would seem quite odd.",
        "createdAt" : "2018-03-19T16:51:17Z",
        "updatedAt" : "2018-03-19T16:51:17Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "1fe59ce4-ceaa-4c34-b34a-648b6dc394fe",
        "parentId" : "106b428d-4909-4c31-ba25-9af73508d835",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "But the scalability tests use a mock cadvisor, so this shouldn't contribute to node startup latency in those tests either.",
        "createdAt" : "2018-03-19T16:52:23Z",
        "updatedAt" : "2018-03-19T16:52:23Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "8e2cd145-f0f4-44d6-908d-8a33ef53ef27",
        "parentId" : "106b428d-4909-4c31-ba25-9af73508d835",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "Ack. thanks @dashpole will keep looking",
        "createdAt" : "2018-03-19T16:55:45Z",
        "updatedAt" : "2018-03-19T16:55:45Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "b259543985b10875f4a010ed0285ac43e335c8e0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +538,542 @@\t\treturn fmt.Errorf(\"failed to get rootfs info: %v\", err)\n\t}\n\tfor rName, rCap := range cadvisor.EphemeralStorageCapacityFromFsInfo(rootfs) {\n\t\tcm.capacity[rName] = rCap\n\t}"
  },
  {
    "id" : "dcf40dd3-db4e-4487-880f-aacdc362dbdb",
    "prId" : 51357,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51357#pullrequestreview-59393220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "240ed05b-87d2-48d4-a9b7-4293f9206ffe",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "@ConnorDoyle -- explained why this is off ContainerManager and not PodContainerManager in person review, and this seemed fine for me... major concern was that its not off the top-level kubelet.",
        "createdAt" : "2017-08-29T21:54:49Z",
        "updatedAt" : "2017-08-31T06:42:48Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "50674ec614a67cd7c6a4fbc7a524e8eda0cf3d4f",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +304,308 @@}\n\nfunc (cm *containerManagerImpl) InternalContainerLifecycle() InternalContainerLifecycle {\n\treturn &internalContainerLifecycleImpl{cm.cpuManager}\n}"
  },
  {
    "id" : "8bbd5110-e1d1-46c3-8ce4-3fe6352e1c72",
    "prId" : 51357,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51357#pullrequestreview-59393450",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "012435cd-f76e-44cb-a792-e8dd0cd4e807",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "all accesses to cpu manager are behind this gate, so this will be safe to avoid nils",
        "createdAt" : "2017-08-29T21:55:54Z",
        "updatedAt" : "2017-08-31T06:42:48Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "50674ec614a67cd7c6a4fbc7a524e8eda0cf3d4f",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +546,550 @@\n\t// Initialize CPU manager\n\tif utilfeature.DefaultFeatureGate.Enabled(kubefeatures.CPUManager) {\n\t\tcm.cpuManager.Start(cpumanager.ActivePodsFunc(activePods), podStatusProvider, runtimeService)\n\t}"
  },
  {
    "id" : "4fa1d516-55e0-45c2-b572-a5c27459f79e",
    "prId" : 51209,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51209#pullrequestreview-58523131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dca1ff08-72fc-47ae-af14-30488c198dcd",
        "parentId" : null,
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "Should log level be used?",
        "createdAt" : "2017-08-24T07:21:48Z",
        "updatedAt" : "2017-09-01T18:56:49Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "84855a64-df54-4236-a5ab-29eb7882116d",
        "parentId" : "dca1ff08-72fc-47ae-af14-30488c198dcd",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "This is a one-time log, so probably ok to stay as info.",
        "createdAt" : "2017-08-24T22:00:14Z",
        "updatedAt" : "2017-09-01T18:56:49Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      }
    ],
    "commit" : "02001af752cb6068ecab42ec5fd6a2792df38945",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +278,282 @@\t}\n\n\tglog.Infof(\"Creating device plugin handler: %t\", devicePluginEnabled)\n\tif devicePluginEnabled {\n\t\tcm.devicePluginHandler, err = NewDevicePluginHandlerImpl(updateDeviceCapacityFunc)"
  },
  {
    "id" : "0900d88e-468a-4a63-9780-2efc3c51573c",
    "prId" : 51209,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51209#pullrequestreview-60006498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "465f493e-33ee-4a61-bb5c-8bfdca30013c",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Ideally, allocation of resources would happen as part of pod creation so as to fail pods during admission whose extended resource request cannot be satisfied by the kubelet due to any reason.\r\nFor example, a device may have just turned unhealthy and the scheduler may not have noticed it. Note there can be multiple entities scheduling pods on to nodes.\r\nNode level admission control is the last line of defence against state propagation issues in k8s. \r\nIf not for allocation during pod admission, I'd at-least like to see an admission handler (we have quite a few of them), that validates that devices are available to satisfy a pod's request. Essentially a dry run of the logic here.",
        "createdAt" : "2017-08-28T17:42:35Z",
        "updatedAt" : "2017-09-01T18:56:49Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "334cb4fc-1d70-462b-b095-3b6fe95ac8a6",
        "parentId" : "465f493e-33ee-4a61-bb5c-8bfdca30013c",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "Agree that sounds a more ideal handling, which probably will be addressed after 1.8 when we try to move the GetResources logic to PodContainerManager.",
        "createdAt" : "2017-08-28T23:04:31Z",
        "updatedAt" : "2017-09-01T18:56:49Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      },
      {
        "id" : "0b222d32-003d-4c5f-af43-82ac58cb1f67",
        "parentId" : "465f493e-33ee-4a61-bb5c-8bfdca30013c",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Can you file an issue with the background information just that it doesn't get lost? You can assign the issue to yourself since you plan on fixing it. ",
        "createdAt" : "2017-08-29T23:07:54Z",
        "updatedAt" : "2017-09-01T18:56:49Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "43007b85-1fda-4fca-9000-4efc7de3918b",
        "parentId" : "465f493e-33ee-4a61-bb5c-8bfdca30013c",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "Filed https://github.com/kubernetes/kubernetes/issues/51592\r\nLeave it unassigned for now as I don't have enough background information on this. It probably needs more discussions with sig-node.",
        "createdAt" : "2017-08-30T05:22:21Z",
        "updatedAt" : "2017-09-01T18:56:49Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      },
      {
        "id" : "9bd64567-8842-41c4-b1cf-3e8d8579e473",
        "parentId" : "465f493e-33ee-4a61-bb5c-8bfdca30013c",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "+1 on what @vishh said.",
        "createdAt" : "2017-08-31T22:21:25Z",
        "updatedAt" : "2017-09-01T18:56:49Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "02001af752cb6068ecab42ec5fd6a2792df38945",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +602,606 @@\tmountsMap := make(map[string]string)\n\tenvsMap := make(map[string]string)\n\tallocResps, err := cm.devicePluginHandler.Allocate(pod, container, activePods)\n\tif err != nil {\n\t\treturn opts, err"
  },
  {
    "id" : "0a43a6ce-f436-46c4-9fc5-215747c2ee96",
    "prId" : 48123,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48123#pullrequestreview-46692951",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d103fe8f-45fb-4966-a57f-c2f51bafdec8",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I found it a bit strange that with this feature, the image fs will longer be *dedicated* even though we call it that....not really pertinent to this PR though.",
        "createdAt" : "2017-06-27T21:58:33Z",
        "updatedAt" : "2017-06-28T01:45:26Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "82f782006604f3eec8c59204df5c54178037f38a",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +234,238 @@\t}\n\n\tif hasDedicatedImageFs, _ := cadvisorInterface.HasDedicatedImageFs(); hasDedicatedImageFs {\n\t\timagesfs, err := cadvisorInterface.ImagesFsInfo()\n\t\tif err != nil {"
  },
  {
    "id" : "74aa0fb0-4d95-4c9b-bfcf-c3ada3b43471",
    "prId" : 31546,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/31546#pullrequestreview-4535224",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0dc51ec2-5c1f-4d92-b3d9-148ba1cb27d2",
        "parentId" : null,
        "authorId" : "2fa5a4c6-e0a7-4dbe-a80e-7770a5dc92a1",
        "body" : "I didn't understand the reason not to create a step in the hierarchy there for consistency.\n\nSure, I get that we aren't applying limits, but just having a slice for it seems like it'd make things like cgls look more consistent and could potentially make other cgroup parsing code a little simpler.\n\nDid I miss a discussion somewhere? Got a link handy?\n",
        "createdAt" : "2016-10-11T20:51:14Z",
        "updatedAt" : "2016-11-02T12:07:19Z",
        "lastEditedBy" : "2fa5a4c6-e0a7-4dbe-a80e-7770a5dc92a1",
        "tags" : [
        ]
      },
      {
        "id" : "b08ad48e-6dd8-41cf-bd87-acf8cf8ded96",
        "parentId" : "0dc51ec2-5c1f-4d92-b3d9-148ba1cb27d2",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "see: https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/pod-resource-management.md#proposed-cgroup-hierarchy\n\ncopy/pasted pertinent highlights:\n1. This hierarchy highly prioritizes resource guarantees to the G over Bu and BE pods.\n2. By not having a separate cgroup for the G class, the hierarchy allows the G pods to burst and utilize all of Node's Allocatable capacity.\n3. The BE and Bu pods are strictly restricted from bursting and hogging resources and thus G Pods are guaranteed resource isolation.\n",
        "createdAt" : "2016-10-17T18:52:12Z",
        "updatedAt" : "2016-11-02T12:07:19Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "42289c27580ab7d7f5b8593600c86c7eb0845b34",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +237,241 @@// We create top level QoS containers for only Burstable and Best Effort\n// and not Guaranteed QoS class. All guaranteed pods are nested under the\n// RootContainer by default. InitQOS is called only once during kubelet bootstrapping.\nfunc InitQOS(cgroupDriver, rootContainer string, subsystems *CgroupSubsystems) (QOSContainersInfo, error) {\n\tcm := NewCgroupManager(subsystems, cgroupDriver)"
  },
  {
    "id" : "2825b7b3-cdad-41bb-97cc-66ff0e618cb0",
    "prId" : 27853,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "688a4465-54ba-46c6-a148-d4feffd56eec",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Don't we have to allow access to all devices here or is that the default setup for `Create`?\n",
        "createdAt" : "2016-07-01T21:09:31Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "470c33b4-3f4e-4f78-9ff5-2ca4684e3a23",
        "parentId" : "688a4465-54ba-46c6-a148-d4feffd56eec",
        "authorId" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "body" : "Yes by default create sets allowalldevices : true. Should I change that behaviour?\n",
        "createdAt" : "2016-07-01T21:15:57Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "tags" : [
        ]
      }
    ],
    "commit" : "5000e74664b58e278bfbfa28098dabc03fb7ae9b",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +228,232 @@\t\tcontainerConfig := &CgroupConfig{\n\t\t\tName:               absoluteContainerName,\n\t\t\tResourceParameters: &ResourceConfig{},\n\t\t}\n\t\t// TODO(@dubstack) Add support on systemd cgroups driver"
  },
  {
    "id" : "290f91b9-32c6-46de-8c82-e94d1cc9ae40",
    "prId" : 27853,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f27f4817-15f0-4504-a887-0b2464be09f1",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "We should ensure that `rootContainer` exists in reality. What if I pass a random path like `foo13#$$` for `--cgroup-root`?\n",
        "createdAt" : "2016-07-01T21:10:35Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "93f88aba-51c3-4721-b16e-9abf131a5c7d",
        "parentId" : "f27f4817-15f0-4504-a887-0b2464be09f1",
        "authorId" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "body" : "Yes I thought it would be a good idea to have that check over here: https://github.com/kubernetes/kubernetes/blob/master/cmd/kubelet/app/server.go#L360\n\nWDYT?? \n",
        "createdAt" : "2016-07-01T21:19:53Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "3793cc73-3064-4ef2-a6f3-54e97eed898e",
        "tags" : [
        ]
      }
    ],
    "commit" : "5000e74664b58e278bfbfa28098dabc03fb7ae9b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +224,228 @@\tfor _, qosClass := range qosClasses {\n\t\t// get the container's absolute name\n\t\tabsoluteContainerName := path.Join(rootContainer, string(qosClass))\n\t\t// containerConfig object stores the cgroup specifications\n\t\tcontainerConfig := &CgroupConfig{"
  },
  {
    "id" : "efc21de8-d1b9-4e28-8ea4-6ed947b9c850",
    "prId" : 27853,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "523c8bc2-a380-4bc4-b5c0-e32f230cee07",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Add a // TODO add support for cgroupRoot?\n",
        "createdAt" : "2016-07-08T21:29:35Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "5000e74664b58e278bfbfa28098dabc03fb7ae9b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +215,219 @@// TODO(@dubstack) Add support for cgroup-root to work on both systemd and cgroupfs\n// drivers. Currently we only support systems running cgroupfs driver\nfunc InitQOS(rootContainer string, subsystems *cgroupSubsystems) (QOSContainersInfo, error) {\n\tcm := NewCgroupManager(subsystems)\n\t// Top level for Qos containers are created only for Burstable"
  },
  {
    "id" : "d7999e98-3a0f-4db6-92ae-2c8fcf4470e6",
    "prId" : 27853,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4bf61a6-34a4-446a-be79-08d0cf05a3e2",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "maybe a note or TODO that this is not yet supported on systemd ;-)\n",
        "createdAt" : "2016-07-08T21:31:40Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "5000e74664b58e278bfbfa28098dabc03fb7ae9b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +231,235 @@\t\t}\n\t\t// TODO(@dubstack) Add support on systemd cgroups driver\n\t\tif err := cm.Create(containerConfig); err != nil {\n\t\t\treturn QOSContainersInfo{}, fmt.Errorf(\"failed to create top level %v QOS cgroup : %v\", qosClass, err)\n\t\t}"
  },
  {
    "id" : "1d2838e5-f20e-4057-ab87-4194013d8568",
    "prId" : 27853,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db3df7a7-3aba-421d-b503-5ca3cd02353e",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "who ensures they are not deleted after the fact?\n",
        "createdAt" : "2016-07-08T21:33:25Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "07d35fc9-0484-4864-8788-ae5d4cd64168",
        "parentId" : "db3df7a7-3aba-421d-b503-5ca3cd02353e",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "for your own future sanity, i think you will want to ensure that these are never deleted by a rogue init system.  its fine to handle that in a future todo if @vishh agrees.\n",
        "createdAt" : "2016-07-08T21:37:33Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "e8d3f6c7-49ab-4758-ba15-217aa1065b5a",
        "parentId" : "db3df7a7-3aba-421d-b503-5ca3cd02353e",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "We talked about ensuring that qos cgroups continues to exist whenever a pod\nlevel cgroup is being created. IIRC, @dubstack is going to add that\nfunctionality in a future PR.\n\nOn Fri, Jul 8, 2016 at 2:38 PM, Derek Carr notifications@github.com wrote:\n\n> In pkg/kubelet/cm/container_manager_linux.go\n> https://github.com/kubernetes/kubernetes/pull/27853#discussion_r70144806\n> :\n> \n> > @@ -240,6 +290,15 @@ func (cm *containerManagerImpl) setupNode() error {\n> >         return err\n> >     }\n> > -   // Setup top level qos containers only if CgroupsPerQOS flag is specified as true\n> > -   if cm.NodeConfig.CgroupsPerQOS {\n> > -       qosContainersInfo, err := InitQOS(cm.NodeConfig.CgroupRoot, cm.subsystems)\n> \n> for your own future sanity, i think you will want to ensure that these are\n> never deleted by a rogue init system. its fine to handle that in a future\n> todo if @vishh https://github.com/vishh agrees.\n> \n> â€”\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/27853/files/8134b9de63922520d329b09348c13f5cc110507e#r70144806,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AGvIKM8pg3kWyFLjOpd4zq99F5RKmqdpks5qTsNBgaJpZM4I7o6F\n> .\n",
        "createdAt" : "2016-07-08T22:17:14Z",
        "updatedAt" : "2016-07-15T17:02:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5000e74664b58e278bfbfa28098dabc03fb7ae9b",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +296,300 @@\t// Setup top level qos containers only if CgroupsPerQOS flag is specified as true\n\tif cm.NodeConfig.CgroupsPerQOS {\n\t\tqosContainersInfo, err := InitQOS(cm.NodeConfig.CgroupRoot, cm.subsystems)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to initialise top level QOS containers: %v\", err)"
  },
  {
    "id" : "395d1524-2c9c-4198-bab4-2cb2899ab203",
    "prId" : 26391,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16566347-30a2-4b35-a492-013ecd42feb6",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Was this `/var/run` or `/run`? Just checking..\n",
        "createdAt" : "2016-05-27T00:25:35Z",
        "updatedAt" : "2016-05-27T00:27:12Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "5f7db6db-a70d-4bf9-ac2a-dacd1093b540",
        "parentId" : "16566347-30a2-4b35-a492-013ecd42feb6",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "`/run` is right (it has more docker information than `/var/run`)\n",
        "createdAt" : "2016-05-27T00:28:29Z",
        "updatedAt" : "2016-05-27T00:28:29Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "89e8db87-17c1-428f-80d7-a57eea89a02a",
        "parentId" : "16566347-30a2-4b35-a492-013ecd42feb6",
        "authorId" : "b56795d9-0816-4d28-b233-275e1ee4baf1",
        "body" : "Is using this pid file documented and stable? We should probably ask the Docker team to do that if not.\n",
        "createdAt" : "2016-05-27T18:25:38Z",
        "updatedAt" : "2016-05-27T18:25:38Z",
        "lastEditedBy" : "b56795d9-0816-4d28-b233-275e1ee4baf1",
        "tags" : [
        ]
      },
      {
        "id" : "e9187ceb-523a-466d-8cb9-5c72c70e748d",
        "parentId" : "16566347-30a2-4b35-a492-013ecd42feb6",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "I can't find any documentation around this path, but we do have a fallback (`pidof`) if the file isn't found. I'll follow up with a docker issue to check the stability.\n",
        "createdAt" : "2016-05-27T19:19:49Z",
        "updatedAt" : "2016-05-27T19:19:49Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4d8dea0d707a673f45eda6502d5958e3e6195d3",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +59,63 @@\tdockerPidFile         = \"/var/run/docker.pid\"\n\tcontainerdProcessName = \"docker-containerd\"\n\tcontainerdPidFile     = \"/run/docker/libcontainerd/docker-containerd.pid\"\n)\n"
  },
  {
    "id" : "1a48585a-f302-46d5-b902-f7b0eaa3ab0f",
    "prId" : 25982,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1add0c98-43fa-4a32-958d-b6f43e4a87a9",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Should we require cpu and memory accounting to be enabled for kube daemons by default?\n",
        "createdAt" : "2016-05-20T21:52:37Z",
        "updatedAt" : "2016-05-23T18:22:59Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5a8851d4364944ac322f0d57a5438b477baa92cc",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +546,550 @@\tif systemd, found := cgs[\"name=systemd\"]; found {\n\t\tif systemd != cpu {\n\t\t\tglog.Warningf(\"CPUAccounting not enabled for pid: %d\", pid)\n\t\t}\n\t\tif systemd != memory {"
  },
  {
    "id" : "c13caa9a-cee5-4762-ac26-70ba40f6e85d",
    "prId" : 21337,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "135a3520-47f9-4f8b-80af-96269cad4663",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Do we want these scheduled in serial, or is it better to schedule them in separate go routines?\n",
        "createdAt" : "2016-02-16T23:41:27Z",
        "updatedAt" : "2016-02-17T00:39:57Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "63fc1e38-ff9c-400f-8e1b-739a5c6b6c5c",
        "parentId" : "135a3520-47f9-4f8b-80af-96269cad4663",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Serial is fine. Honestly, I don't expect it to every change once we detect it. But running them once in awhile ensures correctness.\n",
        "createdAt" : "2016-02-16T23:47:17Z",
        "updatedAt" : "2016-02-17T00:39:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "8cd3afc1-9760-4cf0-b323-8316347baa00",
        "parentId" : "135a3520-47f9-4f8b-80af-96269cad4663",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I'm moving these to a separate go routine to run it less often.\n",
        "createdAt" : "2016-02-16T23:48:32Z",
        "updatedAt" : "2016-02-17T00:39:57Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "7de6a253839fe0c46bea71905b1e59d7ea5935ce",
    "line" : null,
    "diffHunk" : "@@ -1,1 +341,345 @@\n\tgo wait.Until(func() {\n\t\tfor _, task := range cm.periodicTasks {\n\t\t\tif task != nil {\n\t\t\t\ttask()"
  },
  {
    "id" : "d235eaca-cc94-4bf1-a10c-e798572792fb",
    "prId" : 20687,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/20687#pullrequestreview-244190517",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41523345-6858-439b-afe1-d500e5af285c",
        "parentId" : null,
        "authorId" : "a097dedf-39b9-4150-82ce-b4beb3b8d606",
        "body" : "Why create a new  `fs.Manager` instead of the default `cont.manager`?",
        "createdAt" : "2019-05-31T08:30:04Z",
        "updatedAt" : "2019-05-31T08:30:04Z",
        "lastEditedBy" : "a097dedf-39b9-4150-82ce-b4beb3b8d606",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3b5d5774c499c77fdf68a0709c0a2c2cdfe3df8",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +264,268 @@\t\t}\n\t\tcont.ensureStateFunc = func(_ *fs.Manager) error {\n\t\t\treturn manager.Apply(os.Getpid())\n\t\t}\n\t\tsystemContainers = append(systemContainers, cont)"
  }
]