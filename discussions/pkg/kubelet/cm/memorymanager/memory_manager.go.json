[
  {
    "id" : "4139aadd-7ebb-4c70-bf84-e36abeef31e8",
    "prId" : 101030,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/101030#pullrequestreview-689308924",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "052dfeb3-8a2f-41ce-9a50-13feb4132ad7",
        "parentId" : null,
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Is it OK for this to be cached like this? I guess the assumption is that _if_ the amount of allocatable memory was ever changed, then the kubelet would be restarted and this value recalculated.",
        "createdAt" : "2021-06-22T09:00:45Z",
        "updatedAt" : "2021-06-22T09:03:08Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "9f334e8f-6c50-430d-a24e-c0269a683e6f",
        "parentId" : "052dfeb3-8a2f-41ce-9a50-13feb4132ad7",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "Exactly, to change the allocatable you should update the `reservedMemory` parameter and it will require the kubelet restart.",
        "createdAt" : "2021-06-22T10:04:49Z",
        "updatedAt" : "2021-06-22T10:09:53Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      }
    ],
    "commit" : "681905706d43c91403bace725a600d35e02d59c1",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +422,426 @@// GetAllocatableMemory returns the amount of allocatable memory for each NUMA node\nfunc (m *manager) GetAllocatableMemory() []state.Block {\n\treturn m.allocatableMemory\n}\n"
  },
  {
    "id" : "421614ef-a06d-4f26-a6e5-e8f17368754c",
    "prId" : 98924,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98924#pullrequestreview-601873566",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e616afa9-0b65-4212-9d7b-42f0c77a2ecd",
        "parentId" : null,
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "nit: `s/nodes that used/nodes that are used/`\r\n\r\n",
        "createdAt" : "2021-03-02T13:08:26Z",
        "updatedAt" : "2021-03-02T15:02:06Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "72f41fd2-0499-4cfa-809b-64509b12f0d9",
        "parentId" : "e616afa9-0b65-4212-9d7b-42f0c77a2ecd",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "done",
        "createdAt" : "2021-03-02T14:26:44Z",
        "updatedAt" : "2021-03-02T15:02:06Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      }
    ],
    "commit" : "95b2777204404d21aba42b6b21acf260becc29c8",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +185,189 @@}\n\n// GetMemory provides NUMA nodes that used to allocate the container memory\nfunc (m *manager) GetMemoryNUMANodes(pod *v1.Pod, container *v1.Container) sets.Int {\n\t// Get NUMA node affinity of blocks assigned to the container during Allocate()"
  },
  {
    "id" : "50098db0-3749-48b2-b6f3-a59691c3cbcf",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-528189897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3256e6c-28e2-4c02-a7fd-0de9cf69ff70",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "we are mixing terminology here.\r\n\r\ni thought static was removed above for singleNUMA?",
        "createdAt" : "2020-11-03T17:14:24Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "28eb1b83-1b00-4979-92b9-ab7bc6019774",
        "parentId" : "a3256e6c-28e2-4c02-a7fd-0de9cf69ff70",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "We agreed to use a static policy, similar to the CPU manager because it does not really always provide single NUMA node allocation(when the container memory requests bigger than the amount of the allocatable memory on the single NUMA node), the KEP also is using static policy, see - https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-manager-policy-flag",
        "createdAt" : "2020-11-04T09:55:09Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "f6a9e7dd-f033-436b-833b-d966ffa110ec",
        "parentId" : "a3256e6c-28e2-4c02-a7fd-0de9cf69ff70",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I had made a similar comment during the KEP review: \r\nhttps://github.com/kubernetes/enhancements/pull/1203#discussion_r470652108\r\n\r\nAlso related:\r\nhttps://github.com/kubernetes/enhancements/pull/1203#discussion_r435160319\r\n\r\nI was ultimately convinced that something like `static` makes sense because`multi-numa` vs. `single-numa` is not actually a policy decision. Similar to the `CPUManager` the policy is actually about whether a container can have the exclusive resources assigned to it changed after the container has started running.\r\n\r\nJust as the `CPUManager`'s `static` policy disallows one from changing which CPUs are assigned to a container after it has started, so does the `MemoryManager`'s `static'  policy disallow the reassignment of exclusive memory.\r\n\r\nWhether `multi-numa` or `single-numa` is employed is independent of the `MemoryManager`'s policy (it will happily do either depending on how much memory is requested).\r\n\r\nThis is instead a property of its role as a `HintProvider` when used in conjunction with the `TopologyManager`.  If someone has specified a `TopologyManagerPolicy` of `single-numa-node`, then and only-then will the `MemoryManager` be forced to restrict allocations to a single numa node (though it will likely do so in many cases even without the `TopologyManager` as it attempts to allocate memory efficiently).",
        "createdAt" : "2020-11-11T15:56:45Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +128,132 @@\t\tpolicy = NewPolicyNone()\n\n\tcase policyTypeStatic:\n\t\tsystemReserved, err := getSystemReservedMemory(machineInfo, nodeAllocatableReservation, reservedMemory)\n\t\tif err != nil {"
  },
  {
    "id" : "143734c1-901e-4d61-ae5e-b13e585efa93",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-528189897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3414cf04-db96-48b6-ace5-fc163bd2e5b3",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "i dont have a great solution here in my head, but i find it confusing that we use a patch like syntax for updating container resources across cpu and memory managers.",
        "createdAt" : "2020-11-03T17:18:29Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "ea05364a-3f92-49fc-9f71-7f1f0749550a",
        "parentId" : "3414cf04-db96-48b6-ace5-fc163bd2e5b3",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "Can you please clarify, why does it confusing?",
        "createdAt" : "2020-11-04T10:00:10Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "9c41b866-8594-40bb-a80e-c2568095ee8a",
        "parentId" : "3414cf04-db96-48b6-ace5-fc163bd2e5b3",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "ideally, we would have a way to roll-up our desired state and make a single call, but like i said earlier, its not clear how to do that in the current factoring.  not blocking on that, just more a comment.",
        "createdAt" : "2020-11-09T19:49:56Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "3b290289-9d08-4021-a210-b6e04bb95c18",
        "parentId" : "3414cf04-db96-48b6-ace5-fc163bd2e5b3",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "Thanks for the clarification, as part of our future work I will take a look at the whole flow again, and will try to figure out how can we to improve it.",
        "createdAt" : "2020-11-10T09:53:46Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "b3139515-956d-44a3-a5f3-c407ecbf8395",
        "parentId" : "3414cf04-db96-48b6-ace5-fc163bd2e5b3",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "With the introduction of the `Allocate()` abstraction in `v1.18` it would actually be possible to do this without the `Update()` call now.\r\n\r\nin the past we didn't know which resources we were going to give the container until we were inside the body of this `AddContainer()` call, so we had to use `Update()` since the container was already started.\r\n\r\nWith the introduction of the `Allocate()` abstraction, we now know which resources will be allocated to each container at pod admission time before the container ever starts.\r\n\r\nWe would need to add some plumbing to propagate this information up to the initial `ContainerCreate()` call, but the information is already available at the right time. ",
        "createdAt" : "2020-11-11T16:02:39Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 199,
    "diffHunk" : "@@ -1,1 +197,201 @@\taffinity := strings.Join(nodes, \",\")\n\tklog.Infof(\"[memorymanager] Set container %q cpuset.mems to %q\", containerID, affinity)\n\terr := m.containerRuntime.UpdateContainerResources(containerID, &runtimeapi.LinuxContainerResources{CpusetMems: affinity})\n\tif err != nil {\n\t\tklog.Errorf(\"[memorymanager] AddContainer error: error updating cpuset.mems for container (pod: %s, container: %s, container id: %s, err: %v)\", pod.Name, container.Name, containerID, err)"
  },
  {
    "id" : "c3b08043-3ecf-4b9d-8cf7-3df3df441486",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-528189897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c01074ba-45b6-44b3-b848-d1d63d02c774",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "just noting how a lot of the logic maps to what we have in cpu manager.  we may want to find a factoring in future that allows us to move the sources ready stuff up a level to meta manager of managers.  for now, this is fine, but the less duplication we have in long run the better.",
        "createdAt" : "2020-11-03T17:25:29Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "dda8fba8-f0ae-418b-b90c-73fa9b3841f1",
        "parentId" : "c01074ba-45b6-44b3-b848-d1d63d02c774",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "You totally right, I will add it to my TODO list",
        "createdAt" : "2020-11-04T10:05:46Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "65cd83ba-de02-464c-82d0-017b300fddb4",
        "parentId" : "c01074ba-45b6-44b3-b848-d1d63d02c774",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "+1\r\n\r\nThough I don't want to simply merge the two into a single Manager. If we can find a good way to abstract out the common bits, but still have distinct managers for each, that would be ideal.",
        "createdAt" : "2020-11-11T16:03:13Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 272,
    "diffHunk" : "@@ -1,1 +270,274 @@\n// TODO: move the method to the upper level, to re-use it under the CPU and memory managers\nfunc (m *manager) removeStaleState() {\n\t// Only once all sources are ready do we attempt to remove any stale state.\n\t// This ensures that the call to `m.activePods()` below will succeed with"
  },
  {
    "id" : "8c8fb709-8a4f-4a55-9227-279e5f29b5ae",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-528189897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68f89f82-d7fa-4e3b-8aed-95dad7b3ff47",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "can we leave a placeholder in this interface for ephemeral containers?  just something that notes its needed, or maybe error if ephemeral containers are enabled?",
        "createdAt" : "2020-11-03T17:26:56Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "6a7a5461-5353-45c2-89b7-81dee5153d3a",
        "parentId" : "68f89f82-d7fa-4e3b-8aed-95dad7b3ff47",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "Can you please clarify what should be done here regarding the ephemeral containers? From the documentation, I can see that\r\n```\r\nEphemeral containers differ from other containers in that they lack guarantees for resources or execution, and they will never be automatically restarted, so they are not appropriate for building applications.\r\n```\r\nso in general we do not allocate any memory to it during the allocation phase and we also do not need to remove it from the state during call to the `removeStaleState`",
        "createdAt" : "2020-11-04T10:28:16Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "76df2e63-c2e1-4ed2-b0a5-5d8ff80c9e98",
        "parentId" : "68f89f82-d7fa-4e3b-8aed-95dad7b3ff47",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "@derekwaynecarr \r\nThis function is responsible for garbage collecting any stale state in the `MemoryManager` for containers that have exited but not been cleaned up yet.\r\n\r\nThere should never be a case where ephemeral containers have an entry in this state. The `MemoryManager` only allocates memory to containers from pods in the `GuaranteedQoS` class and only if they have `requested==limits`. Ephemeral containers should never fall into this case.",
        "createdAt" : "2020-11-11T16:31:47Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 294,
    "diffHunk" : "@@ -1,1 +292,296 @@\tfor _, pod := range activePods {\n\t\tactiveContainers[string(pod.UID)] = make(map[string]struct{})\n\t\tfor _, container := range append(pod.Spec.InitContainers, pod.Spec.Containers...) {\n\t\t\tactiveContainers[string(pod.UID)][container.Name] = struct{}{}\n\t\t}"
  },
  {
    "id" : "db70eb09-79e8-45c8-a67f-2d52153ba225",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-528853762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b06534a-dd18-4755-8732-47aa66e22991",
        "parentId" : null,
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Looks like it's missing an implementation of `GetPodTopologyHints()`.",
        "createdAt" : "2020-11-11T13:53:18Z",
        "updatedAt" : "2021-02-08T23:22:49Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "58b25658-b45c-486c-a5b7-f60ee12be029",
        "parentId" : "8b06534a-dd18-4755-8732-47aa66e22991",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "It implemented under the separate commit, I wanted to save commit history because it was implemented by @pablitoergosum and I wanted to save this contribution.",
        "createdAt" : "2020-11-12T08:33:37Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 415,
    "diffHunk" : "@@ -1,1 +413,417 @@\n\treturn reservedMemoryConverted, nil\n}"
  },
  {
    "id" : "0e380e6c-03f2-47fe-90e0-8bdbc2429d9e",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-529089665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45713507-4f5c-49b4-8bbe-ca9f7ebfc1cf",
        "parentId" : null,
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "You shouldn't need this `initialContainers` list since you are not migrating from an old state format to a new one.",
        "createdAt" : "2020-11-12T13:00:30Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "bb559d73-2218-429b-ab2d-180c809fb99d",
        "parentId" : "45713507-4f5c-49b4-8bbe-ca9f7ebfc1cf",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "It should also be removed from the state implementation.",
        "createdAt" : "2020-11-12T13:03:50Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "72ab8cf3-558d-4528-8691-c5d802aec66e",
        "parentId" : "45713507-4f5c-49b4-8bbe-ca9f7ebfc1cf",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "nice catch, done",
        "createdAt" : "2020-11-12T13:34:52Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 160,
    "diffHunk" : "@@ -1,1 +158,162 @@\tm.podStatusProvider = podStatusProvider\n\tm.containerRuntime = containerRuntime\n\tm.containerMap = initialContainers\n\n\tstateImpl, err := state.NewCheckpointState(m.stateFileDirectory, memoryManagerStateFileName, m.policy.Name())"
  },
  {
    "id" : "05051c0f-1fbe-4323-9eda-1109b63750c8",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-529186889",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "15223ad3-85b2-43ad-8c26-fd8fa067ecb8",
        "parentId" : null,
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I think you can just call `m.policyRemoveContainerById()` here if you want.\r\nUnless you explicitly wanted to print out that warning above.\r\n\r\nIf so, I would still do this instead (though I don't think it's necessary):\r\n```\r\n\t// if error appears it means container entry already does not exist under the container map\r\n\t_, _, err := m.containerMap.GetContainerRef(containerID)\r\n\tif err != nil {\r\n\t\tklog.Warningf(\"[memorymanager] Failed to get container %s from container map error: %v\", containerID, err)\r\n\t\treturn nil\r\n\t}\r\n\r\n\terr = m.policyRemoveContainerByRef(containerId)\r\n\tif err != nil {\r\n\t\tklog.Errorf(\"[memorymanager] RemoveContainer error: %v\", err)\r\n\t\treturn err\r\n\t}\r\n```",
        "createdAt" : "2020-11-12T13:09:40Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "d529912a-9fb1-4d2e-9060-9b244b6799cb",
        "parentId" : "15223ad3-85b2-43ad-8c26-fd8fa067ecb8",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "Hm, I do not have such a method under the manager, I can add it, but I unsure if it really needed.",
        "createdAt" : "2020-11-12T14:12:28Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "55fb21fa-a66a-45e2-9601-52dbeb56eeab",
        "parentId" : "15223ad3-85b2-43ad-8c26-fd8fa067ecb8",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I see. Yeah, there is one in the `CPUManager` that basically does the same thing you are doing here (but without the log message). I would probably add it for symmetry with the `CPUManager`, but it's up to you since we plan to reabstract this together with the `CPUManager` later.",
        "createdAt" : "2020-11-12T15:24:00Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 245,
    "diffHunk" : "@@ -1,1 +243,247 @@\t\tklog.Errorf(\"[memorymanager] RemoveContainer error: %v\", err)\n\t\treturn err\n\t}\n\n\treturn nil"
  },
  {
    "id" : "d9eb6257-8acb-474c-9148-645f53c4be46",
    "prId" : 95479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95479#pullrequestreview-575598449",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I don't understand what this check is doing, can you explain why nodeAllocatable must equal reserved?",
        "createdAt" : "2021-01-13T14:45:28Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "0db7bc32-12f0-47c8-83fb-ac307821f99a",
        "parentId" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "The`nodeAllocatable` feature can give you a way to configure some amount of memory to be reserved for the system, kube, or eviction procedure, but it does not provide you a way to granulate the reserved memory among NUMA nodes. To inform the memory manager how much memory should be reserved from each NUMA node we introduced the `--reserved-memory` flag, but this flag does not makes any sense out of the memory manager and should be aligned together with the node allocatable flags because the scheduler makes in account theses flags during allocatable memory under the node.",
        "createdAt" : "2021-01-14T14:14:10Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "a348512e-8ba0-4f4a-a12d-8401501e6ece",
        "parentId" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Oh, so --reserved-memory is subdividing another flag.  Hrm.  I kind of expected that to be done in kubelet configuration validation (both flags are available there) not in the memory manager.",
        "createdAt" : "2021-01-20T20:46:40Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "d83898c1-814b-4082-8a67-6d440d62a179",
        "parentId" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I has a similar comment early on in the review process: https://github.com/kubernetes/kubernetes/pull/95479#discussion_r522102019",
        "createdAt" : "2021-01-20T22:36:48Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "a7941a3b-92fe-4366-b9ba-a1bf083cec68",
        "parentId" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "The main problem that all relevant `nodeAllocatable` parameters parsed after the `ValidateKubeletConfiguration` called, you can see https://github.com/kubernetes/kubernetes/blob/0e2248f753d0b99cb3ac12bfa48c692c0ebbe70e/cmd/kubelet/app/server.go#L690 and some parameters also depends on the value under kubelet flags(see - https://github.com/kubernetes/kubernetes/blob/0e2248f753d0b99cb3ac12bfa48c692c0ebbe70e/cmd/kubelet/app/server.go#L700)\r\nProbably I can refactor it but I think it should be done under the separate PR.\r\nFor the current PR, I can:\r\n- move the validation logic https://github.com/kubernetes/kubernetes/pull/95479/files#diff-501be18894cc46e590062eb7d2a32948635de2de8bcc8baf4a44cf3916dea084R377 under the line https://github.com/kubernetes/kubernetes/blob/0e2248f753d0b99cb3ac12bfa48c692c0ebbe70e/cmd/kubelet/app/server.go#L711\r\n- leave it as it now, and move it, once the refactor PR will be merged\r\n\r\n@klueska @smarterclayton WDT?",
        "createdAt" : "2021-01-24T14:06:40Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "280fd24e-370f-4df7-8802-e2995fe392fd",
        "parentId" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "An additional option, I can move this validation to https://github.com/kubernetes/kubernetes/blob/0e2248f753d0b99cb3ac12bfa48c692c0ebbe70e/cmd/kubelet/app/options/options.go#L290",
        "createdAt" : "2021-01-24T14:29:02Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "73a27798-6dc0-4d68-a7a2-3fac703ab6d3",
        "parentId" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "authorId" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "body" : "After additional digging, the eviction threshold hardly coupled with the capacity of the machine(the kubelet get the capacity from the cadvisor and the cadvisor initialized after the kubeletConfig validation and also after the server validation)",
        "createdAt" : "2021-01-24T14:41:03Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "b15151c4-81af-487d-8c8f-a4f39690bd34",
        "tags" : [
        ]
      },
      {
        "id" : "b0c78a31-bdae-41b1-aa11-627580309e6a",
        "parentId" : "784f8aa3-1db9-4531-bfda-13f96f1ce5c3",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Validation is always hermetic (depends on nothing except feature gates and the fields in a given config), but any validation that CAN be done in validation should be.  For instance, if a feature gate makes a calculation more complex, the parts of the calculation that can be done with the gate off could still be in validation.  (so if the config had partial data, we could validate with that partially, then rely on a deeper validation).\r\n\r\nThat said, this is fine for now.\r\n",
        "createdAt" : "2021-01-25T16:53:38Z",
        "updatedAt" : "2021-02-08T23:22:50Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "102124464a994946e151c976775cf751423b14f7",
    "line" : 378,
    "diffHunk" : "@@ -1,1 +376,380 @@\n\t\tif !(*nodeAllocatableMemory).Equal(*reservedMemory) {\n\t\t\treturn fmt.Errorf(\"the total amount %q of type %q is not equal to the value %q determined by Node Allocatable feature\", reservedMemory.String(), resourceType, nodeAllocatableMemory.String())\n\t\t}\n\t}"
  }
]