[
  {
    "id" : "e8e560c6-5e2e-4f66-9f73-ab5826af926c",
    "prId" : 102344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102344#pullrequestreview-678952513",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be47e90c-e291-4288-be0f-ed1dc7f47d22",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Do you intend to keep this log? If yes, make 15s a constant.",
        "createdAt" : "2021-06-10T16:30:32Z",
        "updatedAt" : "2021-06-11T07:11:56Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eadd1a9ead7a009a9abfbd603a5efd0560473cc",
    "line" : 439,
    "diffHunk" : "@@ -1,1 +2125,2129 @@\t\t\t\tklog.ErrorS(err, \"Failed cleaning pods\")\n\t\t\t}\n\t\t\tduration := time.Since(start)\n\t\t\tif duration > housekeepingWarningDuration {\n\t\t\t\tklog.ErrorS(fmt.Errorf(\"housekeeping took too long\"), \"Housekeeping took longer than 15s\", \"seconds\", duration.Seconds())"
  },
  {
    "id" : "e585c701-dad6-4bcd-8ae6-862689a08007",
    "prId" : 102344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102344#pullrequestreview-678952513",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b6d7f40-dafb-4014-9126-29790aaa06aa",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Can you add a comment to explain what condition volume manager checks before doing unmount, to make sure there is no deadlock?",
        "createdAt" : "2021-06-10T16:35:37Z",
        "updatedAt" : "2021-06-11T07:11:56Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eadd1a9ead7a009a9abfbd603a5efd0560473cc",
    "line" : 358,
    "diffHunk" : "@@ -1,1 +1820,1824 @@\t// volumes are unmounted after the pod worker reports ShouldPodRuntimeBeRemoved (which is satisfied\n\t// before syncTerminatedPod is invoked)\n\tif err := kl.volumeManager.WaitForUnmount(pod); err != nil {\n\t\treturn err\n\t}"
  },
  {
    "id" : "c53da826-6d0d-4933-8a42-7cd4ed62de1d",
    "prId" : 102344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102344#pullrequestreview-689782849",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a14500d3-7df0-4ef9-975d-77f146075712",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Just thinking out loud.\r\n\r\nIf a pod hasn't taken any resources and being deleted, no new resources will be taken in this state, thus safe to skip admission.\r\n\r\nIf a pod has taken resources and being deleted, it means that it was admitted before, and admit here because the pod has taken resources in reality anyway, and finish the termination.\r\n\r\nThe behavior change makes sense to me, but probably need some comments to explain.",
        "createdAt" : "2021-06-10T16:53:10Z",
        "updatedAt" : "2021-06-11T07:11:56Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "be44425b-1c4c-4558-b8fd-bc94dd98a7e7",
        "parentId" : "a14500d3-7df0-4ef9-975d-77f146075712",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "agreed, will add a comment",
        "createdAt" : "2021-06-22T17:07:43Z",
        "updatedAt" : "2021-06-22T17:07:43Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eadd1a9ead7a009a9abfbd603a5efd0560473cc",
    "line" : 495,
    "diffHunk" : "@@ -1,1 +2197,2201 @@\t\t// the pod worker is invoked it will also avoid setting up the pod, so\n\t\t// we simply avoid doing any work.\n\t\tif !kl.podWorkers.IsPodTerminationRequested(pod.UID) {\n\t\t\t// We failed pods that we rejected, so activePods include all admitted\n\t\t\t// pods that are alive."
  },
  {
    "id" : "ee7be3ef-76e8-45d3-ba06-a3ad806eced7",
    "prId" : 102344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102344#pullrequestreview-690972868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b63155d-e174-43e2-ba84-30091b1bf400",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "We were using `ShouldPodContainersBeTerminating` in the volume manager, but `IsPodTerminationRequested` here.\r\n\r\nIt is really hard to reason about, and make me worry about unexpected deadlocks.",
        "createdAt" : "2021-06-10T17:44:24Z",
        "updatedAt" : "2021-06-11T07:11:56Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "6ccf00c2-4c49-453b-b5de-c7b9a2ef909c",
        "parentId" : "1b63155d-e174-43e2-ba84-30091b1bf400",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "We are in a sync* method here, which means we are running in the pod worker and we know for certain the pod is known to the kubelet.  This method is checking whether in the time since syncPod has started, has someone asked us to stop.  That is the equivalent to the previous check \r\n\r\n> for someone running in the pod worker, should this pod keep attempting to create containers?\r\n\r\nThe `ShouldPodContainersBeTerminating` method is different.  It is running *outside* a pod worker, and can't return true until `syncPod` exits.  So the syncPod gets to know early (because it's the one creating containers) and all other workers have to wait until syncPod exits.\r\n\r\nThis is the core race the PR addresses - that other loops need to know FOR SURE that no new containers can be started, which only pod worker knows.  The pod worker itself needs to know when syncPod has been requested to stop early, but syncPod at this line of code also knows that containers could still get started in the next loop.  In the future, when context cancellation happens, instead of continuing on to sync pod, we would exit earlier from syncPod, which means that the volume manager sees ShouldPodContainersBeTerminating earlier.  At that point we would remove IsPodTerminationRequested.\r\n\r\nI can change this to simply be an exit early, or I can have it check for context cancellation, but that's bring more scope in.  The correct version of this code looks like:\r\n\r\n```\r\nfunc syncPod(ctx context.Context) { \r\n   ...\r\n   if err := kl.volumeManager.WaitForAttachAndMount(ctx,` pod); err != nil {\r\n     if err == context.Canceled {\r\n       return err\r\n     }\r\n     ... // existing debug logic\r\n  }\r\n  result := kl.containerRuntime.SyncPod(ctx, ...)\r\n  if result is cancelled {\r\n    return context.Canceled\r\n  }\r\n  ... // err checking\r\n  return nil\r\n}\r\n```\r\n\r\nThere is no deadlock possible that I'm aware of because SyncPod will fail when mounts aren't ready at container create or container start (which is why we wait here in normal paths).",
        "createdAt" : "2021-06-22T15:11:10Z",
        "updatedAt" : "2021-06-22T15:11:11Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "f585034a-db29-479b-84c6-ee122b6cd434",
        "parentId" : "1b63155d-e174-43e2-ba84-30091b1bf400",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "So:\r\n* `IsPodTerminationRequested`: Termination request is sent to the pod worker (but may be queued);\r\n* `ShouldPodContainersBeTerminating`: Termination actually starts in the pod worker.\r\n\r\nI see the difference now.\r\n\r\nSomehow it is not clear enough from reading the comments of those 2 functions. Can we clarify it a bit more in the function comment?",
        "createdAt" : "2021-06-28T16:47:03Z",
        "updatedAt" : "2021-06-28T18:08:09Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eadd1a9ead7a009a9abfbd603a5efd0560473cc",
    "line" : 257,
    "diffHunk" : "@@ -1,1 +1696,1700 @@\t// Volume manager will not mount volumes for terminating pods\n\t// TODO: once context cancellation is added this check can be removed\n\tif !kl.podWorkers.IsPodTerminationRequested(pod.UID) {\n\t\t// Wait for volumes to attach/mount\n\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {"
  },
  {
    "id" : "1c450e42-e69a-427e-a693-960d599e9e05",
    "prId" : 102344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102344#pullrequestreview-689652575",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eb0ac6a-03b4-4b6a-95f2-067d0e2e7190",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "We may want to keep this check for safety, doesn't seem to be debug only.",
        "createdAt" : "2021-06-10T18:25:11Z",
        "updatedAt" : "2021-06-11T07:11:56Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "7cbdb19f-ea7b-47da-8cfe-9e30b5a4c9eb",
        "parentId" : "6eb0ac6a-03b4-4b6a-95f2-067d0e2e7190",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "This is definitely an assertion - if someone calls KillPod with stale cache then you can get here still have containers running, so I expect the assertion to remain and the logging to be at V(4) or above.",
        "createdAt" : "2021-06-22T15:18:38Z",
        "updatedAt" : "2021-06-22T15:18:38Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eadd1a9ead7a009a9abfbd603a5efd0560473cc",
    "line" : 333,
    "diffHunk" : "@@ -1,1 +1795,1799 @@\t\tklog.InfoS(\"Post-termination container state\", \"pod\", klog.KObj(pod), \"podUID\", pod.UID, \"containers\", strings.Join(containers, \" \"))\n\t}\n\tif len(runningContainers) > 0 {\n\t\treturn fmt.Errorf(\"detected running containers after a successful KillPod, CRI violation: %v\", runningContainers)\n\t}"
  },
  {
    "id" : "35877615-d84c-4f28-a071-0f4faa2e20e1",
    "prId" : 102344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/102344#pullrequestreview-689628294",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc5ae23e-08fa-4712-b784-5067a9bf9141",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "This duplicates https://github.com/kubernetes/kubernetes/blob/ce8b7afc8af35a08adbb88509cd8bac4ea6e0348/pkg/kubelet/kubelet_pods.go#L1532-L1540\r\n\r\nHowever, I don't see that code being changed in this PR?",
        "createdAt" : "2021-06-21T21:56:39Z",
        "updatedAt" : "2021-06-21T22:22:44Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "d301742d-02b3-4af3-9536-c6e124415a84",
        "parentId" : "dc5ae23e-08fa-4712-b784-5067a9bf9141",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "It does not duplicate that?  The check condition is \"if the pod is in a phase other than a terminal phase, move the phase to pending\".  The check you describe as duplicate is \"if the phase is terminal, block moving back to a non-terminal\".",
        "createdAt" : "2021-06-22T14:59:43Z",
        "updatedAt" : "2021-06-22T14:59:43Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "3eadd1a9ead7a009a9abfbd603a5efd0560473cc",
    "line" : 179,
    "diffHunk" : "@@ -1,1 +1557,1561 @@\t\tif apiPodStatus.Phase != v1.PodFailed && apiPodStatus.Phase != v1.PodSucceeded {\n\t\t\tapiPodStatus.Phase = v1.PodPending\n\t\t}\n\t\tapiPodStatus.Reason = runnable.Reason\n\t\tapiPodStatus.Message = runnable.Message"
  },
  {
    "id" : "54fd286a-b5b9-4f07-b654-440d1d7ff58e",
    "prId" : 101712,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/101712#pullrequestreview-654936846",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44067ac7-8eb3-459c-9fb8-917838ebee40",
        "parentId" : null,
        "authorId" : "db4d847e-0006-4342-9243-2f3f71f190b9",
        "body" : "just curious, why are we only doing this for the CRI stats provider (and not also the regular [dockershim] stats provider)?",
        "createdAt" : "2021-05-04T01:50:26Z",
        "updatedAt" : "2021-05-04T01:57:25Z",
        "lastEditedBy" : "db4d847e-0006-4342-9243-2f3f71f190b9",
        "tags" : [
        ]
      },
      {
        "id" : "2ea5c793-2d40-4c8e-b4d2-276344749662",
        "parentId" : "44067ac7-8eb3-459c-9fb8-917838ebee40",
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "regular stats provider is disabled by MetricSet. It doesn't do any transformations",
        "createdAt" : "2021-05-08T00:46:12Z",
        "updatedAt" : "2021-05-08T00:46:12Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8ae653c1d55a0aa5ff5acabeed0214f8f3fd176",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +681,685 @@\t\t\tkubeDeps.RemoteImageService,\n\t\t\thostStatsProvider,\n\t\t\tutilfeature.DefaultFeatureGate.Enabled(features.DisableAcceleratorUsageMetrics))\n\t}\n"
  },
  {
    "id" : "172b6b9f-3dd9-4d41-903c-288ba598f6fb",
    "prId" : 99861,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99861#pullrequestreview-605565497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8db798a-929c-4f71-98de-b51d3ee9b233",
        "parentId" : null,
        "authorId" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "body" : "using `nil` for err here",
        "createdAt" : "2021-03-05T20:15:41Z",
        "updatedAt" : "2021-03-17T09:10:35Z",
        "lastEditedBy" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "tags" : [
        ]
      }
    ],
    "commit" : "be91ea5bd160620a1428c59b5af210ec4c7421e5",
    "line" : 304,
    "diffHunk" : "@@ -1,1 +1931,1935 @@\t\t// callback.\n\t\tif !open {\n\t\t\tklog.ErrorS(nil, \"Update channel is closed, exiting the sync loop\")\n\t\t\treturn false\n\t\t}"
  },
  {
    "id" : "30e14cc0-f36d-45e6-95e1-d64d58b3513a",
    "prId" : 99861,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99861#pullrequestreview-605565497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "715de2e0-1fa4-40df-b97d-4e41a86809dc",
        "parentId" : null,
        "authorId" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "body" : "using `nil` for err here",
        "createdAt" : "2021-03-05T20:16:00Z",
        "updatedAt" : "2021-03-17T09:10:35Z",
        "lastEditedBy" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "tags" : [
        ]
      }
    ],
    "commit" : "be91ea5bd160620a1428c59b5af210ec4c7421e5",
    "line" : 313,
    "diffHunk" : "@@ -1,1 +1958,1962 @@\t\tcase kubetypes.SET:\n\t\t\t// TODO: Do we want to support this?\n\t\t\tklog.ErrorS(nil, \"Kubelet does not support snapshot update\")\n\t\tdefault:\n\t\t\tklog.ErrorS(nil, \"Invalid operation type received\", \"operation\", u.Op)"
  },
  {
    "id" : "93ba3c52-57bb-4503-917e-fd263467027c",
    "prId" : 99861,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99861#pullrequestreview-605565497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a648ec79-d582-4252-abad-a34befb22aed",
        "parentId" : null,
        "authorId" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "body" : "using `nil` for err here",
        "createdAt" : "2021-03-05T20:16:16Z",
        "updatedAt" : "2021-03-17T09:10:35Z",
        "lastEditedBy" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "tags" : [
        ]
      }
    ],
    "commit" : "be91ea5bd160620a1428c59b5af210ec4c7421e5",
    "line" : 377,
    "diffHunk" : "@@ -1,1 +2213,2217 @@\t}\n\tif s == nil {\n\t\tklog.ErrorS(nil, \"Container runtime status is nil\")\n\t\treturn\n\t}"
  },
  {
    "id" : "57d5b3a5-e66a-4dc4-9276-492b87ff80d6",
    "prId" : 99861,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99861#pullrequestreview-605565497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "157bea4c-2a73-4a23-8bb6-bcc45b2a981a",
        "parentId" : null,
        "authorId" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "body" : "had added key as `staticPodPath` earlier, but changed to `path`",
        "createdAt" : "2021-03-05T20:20:55Z",
        "updatedAt" : "2021-03-17T09:10:35Z",
        "lastEditedBy" : "c4fff1f4-0021-4f1f-a538-7ada97206ed5",
        "tags" : [
        ]
      }
    ],
    "commit" : "be91ea5bd160620a1428c59b5af210ec4c7421e5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +273,277 @@\t// define file config source\n\tif kubeCfg.StaticPodPath != \"\" {\n\t\tklog.InfoS(\"Adding static pod path\", \"path\", kubeCfg.StaticPodPath)\n\t\tconfig.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(kubetypes.FileSource))\n\t}"
  },
  {
    "id" : "dd321502-f07b-4f53-987f-33638df6da1a",
    "prId" : 99861,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99861#pullrequestreview-605662684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed09810f-f20e-4cdf-b22a-2a230c820659",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : ":+1: ",
        "createdAt" : "2021-03-05T22:35:31Z",
        "updatedAt" : "2021-03-17T09:10:35Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      }
    ],
    "commit" : "be91ea5bd160620a1428c59b5af210ec4c7421e5",
    "line" : 330,
    "diffHunk" : "@@ -1,1 +1979,1983 @@\t\t\t} else {\n\t\t\t\t// If the pod no longer exists, ignore the event.\n\t\t\t\tklog.V(4).InfoS(\"SyncLoop (PLEG): pod does not exist, ignore irrelevant event\", \"event\", e)\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "efdd814e-0c08-41cc-a3eb-1706856323a5",
    "prId" : 99336,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99336#pullrequestreview-602257511",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe58db68-5657-4ff5-84a1-37d843b6c125",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "Since this code block got moved up, the comment on old L427/new L459 is now after the first instance of this pattern, might wanna move that up.\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/3c514ae588fdd2867eb9cced1c43c9ee2276a7ce/pkg/kubelet/kubelet.go#L426-L427",
        "createdAt" : "2021-03-02T20:24:12Z",
        "updatedAt" : "2021-04-21T19:57:53Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "0f65ee74-9467-4971-b1c3-bdd52bb0a51f",
        "parentId" : "fe58db68-5657-4ff5-84a1-37d843b6c125",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "makes sense, will update.",
        "createdAt" : "2021-03-02T20:47:40Z",
        "updatedAt" : "2021-04-21T19:57:53Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "7deac5e6970aa98fbcfc1d4f021c98cca3797051",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +393,397 @@\t// If kubeClient == nil, we are running in standalone mode (i.e. no API servers)\n\t// If not nil, we are running as part of a cluster and should sync w/API\n\tif kubeDeps.KubeClient != nil {\n\t\tkubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0, informers.WithTweakListOptions(func(options *metav1.ListOptions) {\n\t\t\toptions.FieldSelector = fields.Set{metav1.ObjectNameField: string(nodeName)}.String()"
  },
  {
    "id" : "5c25c0a3-b37b-4778-b1ea-955329a93303",
    "prId" : 98848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98848#pullrequestreview-589073813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17faf867-eeb8-48cb-8a72-83111130edd3",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "Please remove this log line. It causes logspam (see #98137).",
        "createdAt" : "2021-02-11T23:25:40Z",
        "updatedAt" : "2021-02-11T23:27:11Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      }
    ],
    "commit" : "853cc1cba8497fddf9364f08fe576b87f51b3ea7",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +534,538 @@\t\tnodeHasSynced = func() bool {\n\t\t\tif kubeInformers.Core().V1().Nodes().Informer().HasSynced() {\n\t\t\t\tklog.Infof(\"kubelet nodes sync\")\n\t\t\t\treturn true\n\t\t\t}"
  },
  {
    "id" : "e811f439-7745-4067-a258-ef3ea0573583",
    "prId" : 98742,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98742#pullrequestreview-584439995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d042195-6ab2-4bcd-85d4-4531b4c9b977",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "Could we get a unit test update to check for these cases?",
        "createdAt" : "2021-02-05T00:44:28Z",
        "updatedAt" : "2021-02-06T05:07:17Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "08be5e70-b87a-4e09-a6f8-8e9cade2eef9",
        "parentId" : "6d042195-6ab2-4bcd-85d4-4531b4c9b977",
        "authorId" : "d201d1ca-4170-481e-8e0a-c6e9991f353a",
        "body" : "Test is updated!",
        "createdAt" : "2021-02-05T15:23:31Z",
        "updatedAt" : "2021-02-06T05:07:17Z",
        "lastEditedBy" : "d201d1ca-4170-481e-8e0a-c6e9991f353a",
        "tags" : [
        ]
      }
    ],
    "commit" : "321ca8af526e6ae255c6d6d11b01d060b00eaa11",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2022,2026 @@\t// optimization: avoid invoking the pod worker if no further changes are possible to the pod definition\n\t// (i.e. the pod has completed and its containers have been terminated)\n\tif podWorkerTerminal && containersTerminal {\n\t\tklog.V(4).InfoS(\"Pod has completed and its containers have been terminated, ignoring remaining sync work\", \"pod\", klog.KObj(pod), \"syncType\", syncType)\n\t\treturn"
  },
  {
    "id" : "69571225-32fc-4f7e-951b-b936eebcdac7",
    "prId" : 98424,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98424#pullrequestreview-582879806",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10a9657c-6d92-403e-8bb7-ad302862af22",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "We need to update the comment above referring to mirror pods",
        "createdAt" : "2021-02-01T21:56:11Z",
        "updatedAt" : "2021-02-04T17:46:01Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "5fac59a9-7cd1-4ef8-8b86-c87696956e83",
        "parentId" : "10a9657c-6d92-403e-8bb7-ad302862af22",
        "authorId" : "3eccedfc-5c53-4555-94cb-69f2b56e485c",
        "body" : "Will do.",
        "createdAt" : "2021-02-03T22:58:57Z",
        "updatedAt" : "2021-02-04T17:46:01Z",
        "lastEditedBy" : "3eccedfc-5c53-4555-94cb-69f2b56e485c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f918e11e3aaef25fee0525f9552e36d23b6b86a6",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1510,1514 @@\t// the runtime cleans up.\n\tpodFullName := kubecontainer.GetPodFullName(pod)\n\tif kl.podKiller.IsPodPendingTerminationByPodName(podFullName) {\n\t\treturn fmt.Errorf(\"pod %q is pending termination\", podFullName)\n\t}"
  },
  {
    "id" : "931f69ad-3426-4d69-b7e7-069319742bdd",
    "prId" : 98376,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98376#pullrequestreview-583236160",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4679566-ea1d-4b8c-83d0-2681219d2e79",
        "parentId" : null,
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "Can you explain why it is advantageous to handle these all this way in this sync loop? Why were readinessManager and startupManager not in here before? Would it make sense to have the livenessManager behave like them rather than the other way around? (I wish this was documented in a comment from before!)",
        "createdAt" : "2021-02-03T23:58:18Z",
        "updatedAt" : "2021-03-06T13:43:53Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "cbf22537-5734-4ebd-a49f-586439ea9cbc",
        "parentId" : "c4679566-ea1d-4b8c-83d0-2681219d2e79",
        "authorId" : "4da24861-03eb-421f-b365-cf63f3c423b3",
        "body" : "The main benefit, apart from being easier to present/understand, is to allow me to call the `HandlePodSyncs` in a consistent manner after each *Manager update.",
        "createdAt" : "2021-02-04T10:18:27Z",
        "updatedAt" : "2021-03-06T13:43:53Z",
        "lastEditedBy" : "4da24861-03eb-421f-b365-cf63f3c423b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b203fb0565607f37f907cc89e2d257e93ae4e9b8",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +1988,1992 @@\t\tkl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready)\n\t\thandleProbeSync(kl, update, handler, \"readiness\", map[bool]string{true: \"ready\", false: \"\"}[ready])\n\tcase update := <-kl.startupManager.Updates():\n\t\tstarted := update.Result == proberesults.Success\n\t\tkl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started)"
  },
  {
    "id" : "db5ed543-fff2-4e12-888e-e5878c665de8",
    "prId" : 98103,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/98103#pullrequestreview-569421587",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdeb0d2b-7841-4466-bb9c-fa7c3f031fbd",
        "parentId" : null,
        "authorId" : "d201d1ca-4170-481e-8e0a-c6e9991f353a",
        "body" : "For reviewers: ensure podKiller.KillPod to be called",
        "createdAt" : "2021-01-15T16:41:57Z",
        "updatedAt" : "2021-01-18T23:57:06Z",
        "lastEditedBy" : "d201d1ca-4170-481e-8e0a-c6e9991f353a",
        "tags" : [
        ]
      }
    ],
    "commit" : "1563fb68e673bd854470e8c280ab0c5a92a062cd",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1775,1779 @@\t\tkl.podKiller.MarkMirrorPodPendingTermination(pod)\n\t}\n\tkl.podKiller.KillPod(&podPair)\n\n\t// We leave the volume/directory cleanup to the periodic cleanup routine."
  },
  {
    "id" : "b545462c-38dd-4dfa-bda1-c57be5adbe02",
    "prId" : 97932,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97932#pullrequestreview-565807127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b29f88b-7524-4a20-91b1-94a1a60d1844",
        "parentId" : null,
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "good one :)",
        "createdAt" : "2021-01-11T23:01:51Z",
        "updatedAt" : "2021-01-11T23:02:00Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3073b739f8827843b53faf954b9e279dc2261a3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +421,425 @@\tvar serviceLister corelisters.ServiceLister\n\tvar serviceHasSynced cache.InformerSynced\n\t// If kubeClient == nil, we are running in standalone mode (i.e. no API servers)\n\t// If not nil, we are running as part of a cluster and should sync w/API\n\tif kubeDeps.KubeClient != nil {"
  },
  {
    "id" : "7b9749da-e06b-4a78-b1e3-5cd06f20d620",
    "prId" : 97006,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97006#pullrequestreview-544605760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da15c9bc-df48-46fa-b815-638e10a6fce2",
        "parentId" : null,
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "Just to refresh my memory, does this cause timestamps not to be set for metrics, or does it set the timestamp at 0?",
        "createdAt" : "2020-12-03T17:06:35Z",
        "updatedAt" : "2020-12-04T02:08:34Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "12fab956-cc64-4db9-aae3-06aafa0e3544",
        "parentId" : "da15c9bc-df48-46fa-b815-638e10a6fce2",
        "authorId" : "fe910004-4ae6-4d2f-9675-6996649ae5b8",
        "body" : "The collector will not include a timestamp because it is set to zero: https://github.com/google/cadvisor/blob/65fa5b44d3840d465a5c004a0f6722e0fd540137/metrics/prometheus_machine.go#L240",
        "createdAt" : "2020-12-03T17:11:40Z",
        "updatedAt" : "2020-12-04T02:08:34Z",
        "lastEditedBy" : "fe910004-4ae6-4d2f-9675-6996649ae5b8",
        "tags" : [
        ]
      },
      {
        "id" : "00ecd8c4-e331-4e67-afde-9f8c78fdf1b9",
        "parentId" : "da15c9bc-df48-46fa-b815-638e10a6fce2",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "makes sense.  A comment would be helpful for future readers.",
        "createdAt" : "2020-12-03T17:12:43Z",
        "updatedAt" : "2020-12-04T02:08:34Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "4be90bc8-9f95-405f-a05d-f04c1afaa83a",
        "parentId" : "da15c9bc-df48-46fa-b815-638e10a6fce2",
        "authorId" : "59920a9a-4a3d-4e8e-92f0-b4cd6a6d0c8e",
        "body" : "Added. Leaving this unresolved so others can find this.",
        "createdAt" : "2020-12-04T02:07:02Z",
        "updatedAt" : "2020-12-04T02:08:34Z",
        "lastEditedBy" : "59920a9a-4a3d-4e8e-92f0-b4cd6a6d0c8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4037d26849a1111aece7c4a479e5853d25edf9b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +554,558 @@\t// Avoid collector collects it as a timestamped metric\n\t// See PR #95210 and #97006 for more details.\n\tmachineInfo.Timestamp = time.Time{}\n\tklet.setCachedMachineInfo(machineInfo)\n"
  },
  {
    "id" : "20a86e65-92a3-4e6b-9c00-9499c44c0c27",
    "prId" : 94109,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94109#pullrequestreview-472058842",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93b85c5c-39ca-4024-9a9d-0bff841db81c",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "note: this is in sync with cadvisor housekeeping, so no action is required.",
        "createdAt" : "2020-08-19T20:44:26Z",
        "updatedAt" : "2020-08-19T20:44:26Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "66fa4956-fee7-4d3e-adb0-c54c62d4744c",
        "parentId" : "93b85c5c-39ca-4024-9a9d-0bff841db81c",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "actually, we now do eviction with on-demand metrics, so it doesn't matter how it relates to the housekeeping interval.",
        "createdAt" : "2020-08-20T23:54:37Z",
        "updatedAt" : "2020-08-20T23:54:37Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "02daa3ec23d9d2da3340ac1c02e203c2bf2d018d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +139,143 @@\n\t// Period for performing eviction monitoring.\n\t// ensure this is kept in sync with internal cadvisor housekeeping.\n\tevictionMonitoringPeriod = time.Second * 10\n"
  },
  {
    "id" : "e30fa275-c241-4d9b-b2f9-f7e84194e702",
    "prId" : 94109,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94109#pullrequestreview-470922058",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b273310-9505-4326-843c-55048c1982e5",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "this code is so old, it speculated that a node would have a namespace ;-)",
        "createdAt" : "2020-08-19T20:44:54Z",
        "updatedAt" : "2020-08-19T20:44:54Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "02daa3ec23d9d2da3340ac1c02e203c2bf2d018d",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +441,445 @@\tnodeLister := corelisters.NewNodeLister(nodeIndexer)\n\n\t// construct a node reference used for events\n\tnodeRef := &v1.ObjectReference{\n\t\tKind:      \"Node\","
  },
  {
    "id" : "36ea4722-e0a0-4aad-9b24-5e7c1066af19",
    "prId" : 94109,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94109#pullrequestreview-471196185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7247f4ce-d4b8-45bd-83cd-43b579dffc7f",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "remove this line as well?\r\n\r\n",
        "createdAt" : "2020-08-20T04:28:39Z",
        "updatedAt" : "2020-08-20T04:28:39Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      }
    ],
    "commit" : "02daa3ec23d9d2da3340ac1c02e203c2bf2d018d",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +2085,2089 @@\t}\n\t// Periodically log the whole runtime status for debugging.\n\t// condition is unmet.\n\tklog.V(4).Infof(\"Container runtime status: %v\", s)\n\tnetworkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady)"
  },
  {
    "id" : "26581321-3b10-43fd-aed1-0f335a3b683e",
    "prId" : 94109,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94109#pullrequestreview-471936661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a139d25-a250-44a8-926c-2c45a1e3803f",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "```suggestion\r\n\tklog.V(4).Infof(\"Container runtime status: %v\", s)\r\n```",
        "createdAt" : "2020-08-20T19:49:30Z",
        "updatedAt" : "2020-08-20T19:49:30Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      }
    ],
    "commit" : "02daa3ec23d9d2da3340ac1c02e203c2bf2d018d",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +2086,2090 @@\t// Periodically log the whole runtime status for debugging.\n\t// condition is unmet.\n\tklog.V(4).Infof(\"Container runtime status: %v\", s)\n\tnetworkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady)\n\tif networkReady == nil || !networkReady.Status {"
  },
  {
    "id" : "8e403180-67f6-4e4b-ac1b-cc084454df58",
    "prId" : 94087,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94087#pullrequestreview-596927119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9d77145-87eb-48aa-92ac-36ce33c5931f",
        "parentId" : null,
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "this message is posted *a lot* in the logs, i think it can be omitted.\r\n",
        "createdAt" : "2021-02-21T02:32:26Z",
        "updatedAt" : "2021-02-21T02:32:26Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      },
      {
        "id" : "ca48077b-c45e-4f74-a849-8f76f74083a0",
        "parentId" : "c9d77145-87eb-48aa-92ac-36ce33c5931f",
        "authorId" : "5e225159-999d-430a-8b58-d5220dc1429d",
        "body" : "Line 447 `klog.Infof(\"kubelet nodes sync\")` is removed in #98137\r\n",
        "createdAt" : "2021-02-23T05:53:50Z",
        "updatedAt" : "2021-02-23T05:53:50Z",
        "lastEditedBy" : "5e225159-999d-430a-8b58-d5220dc1429d",
        "tags" : [
        ]
      },
      {
        "id" : "b29e63c0-3bbc-4cbc-b066-d24f14e9a7b3",
        "parentId" : "c9d77145-87eb-48aa-92ac-36ce33c5931f",
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "it's still present at HEAD:\r\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet.go#L452\r\n",
        "createdAt" : "2021-02-24T00:24:08Z",
        "updatedAt" : "2021-02-24T00:24:08Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "752135242e49dc571159eb51328bcfc7734e6033",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +448,452 @@\t\t\t\treturn true\n\t\t\t}\n\t\t\tklog.Infof(\"kubelet nodes not sync\")\n\t\t\treturn false\n\t\t}"
  },
  {
    "id" : "1624b376-531c-49ce-953f-a73c01dd8e1e",
    "prId" : 92863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92863#pullrequestreview-688046164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ee10ead-cdfe-4f7f-ae91-2025692da1a4",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "hint is misleading as the error is printed outside the check `libcontainersystem.RunningInUserNS()`.",
        "createdAt" : "2021-06-18T22:22:00Z",
        "updatedAt" : "2021-06-18T22:22:00Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "ad060160-bd6d-4cc6-a6ec-2f05413ee7a3",
        "parentId" : "2ee10ead-cdfe-4f7f-ae91-2025692da1a4",
        "authorId" : "7ff04a68-17e8-419a-84b6-e644739df26f",
        "body" : "updated",
        "createdAt" : "2021-06-21T05:43:58Z",
        "updatedAt" : "2021-06-21T05:43:58Z",
        "lastEditedBy" : "7ff04a68-17e8-419a-84b6-e644739df26f",
        "tags" : [
        ]
      }
    ],
    "commit" : "26e83ac4d4398ed94ed5391e4faed54824ed9a4d",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +490,494 @@\t\t\t\toomWatcher = nil\n\t\t\t} else {\n\t\t\t\tklog.ErrorS(err, \"Failed to create an oomWatcher (running in UserNS, Hint: enable KubeletInUserNamespace feature flag to ignore the error)\")\n\t\t\t\treturn nil, err\n\t\t\t}"
  },
  {
    "id" : "437b7192-f541-4234-b6b4-a581c70846c5",
    "prId" : 92817,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92817#pullrequestreview-443512034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b616000-de6c-4d4b-934f-52c344f69a7f",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "same `deleter` vs `deletor`. Let's make consistent one way or another",
        "createdAt" : "2020-07-06T20:04:53Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "e3108950-6832-4d6c-a4cb-6af63f7876f2",
        "parentId" : "3b616000-de6c-4d4b-934f-52c344f69a7f",
        "authorId" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "body" : "spellchecker suggests to use `deleter` and hence used it. Is it fine to change the existing one to `deleter` ?",
        "createdAt" : "2020-07-06T23:18:27Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "tags" : [
        ]
      },
      {
        "id" : "a2075525-46b0-498c-bf2d-f09f7d77d117",
        "parentId" : "3b616000-de6c-4d4b-934f-52c344f69a7f",
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "I like deleter over deletor too. I wouldn't rename as part of this PR, perhaps a follow up clean up.",
        "createdAt" : "2020-07-07T00:12:32Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "aeeb41cf-1152-4387-9450-fc6213643e3f",
        "parentId" : "3b616000-de6c-4d4b-934f-52c344f69a7f",
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "My biggest concern is when code does not look consistent",
        "createdAt" : "2020-07-07T00:13:08Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "46f1d8f6-a99c-4f0a-80d3-81a8bd67837f",
        "parentId" : "3b616000-de6c-4d4b-934f-52c344f69a7f",
        "authorId" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "body" : "Sure, will create a follow up PR ",
        "createdAt" : "2020-07-07T02:00:11Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "tags" : [
        ]
      }
    ],
    "commit" : "acac15c20e5effb079502f8cb047be5871853f80",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +655,659 @@\tklet.containerGC = containerGC\n\tklet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, integer.IntMax(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod))\n\tklet.sandboxDeleter = newPodSandboxDeleter(klet.containerRuntime)\n\n\t// setup imageManager"
  },
  {
    "id" : "6d5b70fb-5243-406e-9e67-4b34d018fa4f",
    "prId" : 92817,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92817#pullrequestreview-444403098",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b943fac2-e9be-45c7-b1d3-e115f339ff53",
        "parentId" : null,
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "is this triggered on any container removal?  Should we check somewhere to make sure all containers are removed before we remove the sandbox?",
        "createdAt" : "2020-07-06T20:24:51Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "ae35a430-2a99-4854-9cca-6b3d13b0f547",
        "parentId" : "b943fac2-e9be-45c7-b1d3-e115f339ff53",
        "authorId" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "body" : "deletePodSandbox internally removes sandboxes only if all the containers are removed.",
        "createdAt" : "2020-07-06T23:21:33Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "tags" : [
        ]
      },
      {
        "id" : "682910a5-c63c-4c1a-8658-50f7a7532f1f",
        "parentId" : "b943fac2-e9be-45c7-b1d3-e115f339ff53",
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "Maybe it can be called `deletePodSandboxIfEmpty` or something since its not clear from this logic",
        "createdAt" : "2020-07-08T04:23:40Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      }
    ],
    "commit" : "acac15c20e5effb079502f8cb047be5871853f80",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +1872,1876 @@\t\t}\n\t\tif e.Type == pleg.ContainerRemoved {\n\t\t\tkl.deletePodSandbox(e.ID)\n\t\t}\n"
  },
  {
    "id" : "0c0921f2-cb20-4de6-8b1a-909ba59be705",
    "prId" : 92817,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92817#pullrequestreview-444068803",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f463ee8-bbbe-4545-89ae-86b6bcc42425",
        "parentId" : null,
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "do we need to do pruning here?   Or should we simplify everything so we just handle the case where we are deleting everything?",
        "createdAt" : "2020-07-07T00:25:12Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "9ee2ee74-3068-4f4e-97fc-33003b0adb8f",
        "parentId" : "4f463ee8-bbbe-4545-89ae-86b6bcc42425",
        "authorId" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "body" : "i didn't understand the comment exactly but the logic here is from kuberuntime gc https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/kuberuntime_gc.go#L314 ",
        "createdAt" : "2020-07-07T01:58:37Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "af81f9c4-a75e-4ffc-8796-b6a575aa6a95",
        "tags" : [
        ]
      },
      {
        "id" : "793721d1-fa7d-4762-b2c1-f43a24844552",
        "parentId" : "4f463ee8-bbbe-4545-89ae-86b6bcc42425",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "Basically, if the pod is still running, we clean up all sandboxes except the one that is running.  If it isn't, we remove them all.",
        "createdAt" : "2020-07-07T16:41:31Z",
        "updatedAt" : "2020-07-22T21:12:49Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "acac15c20e5effb079502f8cb047be5871853f80",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +2203,2207 @@func (kl *Kubelet) deletePodSandbox(podID types.UID) {\n\tif podStatus, err := kl.podCache.Get(podID); err == nil {\n\t\ttoKeep := 1\n\t\tif kl.IsPodDeleted(podID) {\n\t\t\ttoKeep = 0"
  },
  {
    "id" : "15445de8-dd95-4bba-8ae1-e36717730eee",
    "prId" : 92442,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92442#pullrequestreview-444992574",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d1af4c4-ac3c-4e50-afd5-6c64a4f82012",
        "parentId" : null,
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "mirror pods never get to this point due to the `continue` on 2036",
        "createdAt" : "2020-07-08T17:38:41Z",
        "updatedAt" : "2020-07-08T20:38:41Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "a39d3b85-4d8c-41fa-82c5-a2eaa5607a22",
        "parentId" : "5d1af4c4-ac3c-4e50-afd5-6c64a4f82012",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "This call is to check for pod which has mirror pod (static pod, e.g.).\r\nThat's why I place it here after the kubetypes.IsMirrorPod() call.",
        "createdAt" : "2020-07-08T17:42:21Z",
        "updatedAt" : "2020-07-08T20:38:41Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "a76a959294ba13bfe931a0c2df568450f71148e2",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +2037,2041 @@\t\t\tcontinue\n\t\t}\n\t\tif _, ok := kl.podManager.GetMirrorPodByPod(pod); ok {\n\t\t\tkl.podKiller.MarkMirrorPodPendingTermination(pod)\n\t\t}"
  },
  {
    "id" : "0a0fa169-d09c-4008-b1dd-5f2c32bc4c24",
    "prId" : 91577,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91577#pullrequestreview-436743426",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7be697b-6eee-40e9-8cae-1790db2281ca",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "should there be a default case that at least logs receiving unhandled op types?",
        "createdAt" : "2020-06-01T13:57:29Z",
        "updatedAt" : "2020-06-09T19:27:17Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "319f7030-4f9d-4134-9213-fe182a6d2341",
        "parentId" : "c7be697b-6eee-40e9-8cae-1790db2281ca",
        "authorId" : "4ff6be29-02e8-4f7f-8b4d-a207c13aeecd",
        "body" : "Is it worth to test this code branch, mocking klog maybe?\r\n```\r\nfunc TestInvalidEventSyncLoop(t *testing.T) {\r\n       testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)\r\n       defer testKubelet.Cleanup()\r\n\r\n       kubelet := testKubelet.kubelet\r\n      kubelet.runtimeState.setRuntimeSync(time.Now())\r\n       kubelet.resyncInterval = time.Second * 30\r\n\r\n       ch := make(chan kubetypes.PodUpdate)\r\n       // add an invalid operation\r\n       go func() { ch <- kubetypes.PodUpdate{Op: 1000, Source: kubetypes.ApiserverSource} }()\r\n       defer close(ch)\r\n\r\n       kubelet.syncLoopIteration(ch, kubelet, make(chan time.Time), make(chan time.Time), make(chan *pleg.PodLifecycleEvent, 1))\r\n       // TODO - mocked klog call assertion\r\n}\r\n```",
        "createdAt" : "2020-06-03T13:34:06Z",
        "updatedAt" : "2020-06-09T19:27:17Z",
        "lastEditedBy" : "4ff6be29-02e8-4f7f-8b4d-a207c13aeecd",
        "tags" : [
        ]
      },
      {
        "id" : "a8c81783-fb8f-4675-9e69-79b4a5e3b66a",
        "parentId" : "c7be697b-6eee-40e9-8cae-1790db2281ca",
        "authorId" : "4ff6be29-02e8-4f7f-8b4d-a207c13aeecd",
        "body" : "@liggitt any thoughts on this?",
        "createdAt" : "2020-06-24T10:45:41Z",
        "updatedAt" : "2020-06-24T10:45:41Z",
        "lastEditedBy" : "4ff6be29-02e8-4f7f-8b4d-a207c13aeecd",
        "tags" : [
        ]
      },
      {
        "id" : "c2f1d021-8989-40e9-9979-ad7488f047fd",
        "parentId" : "c7be697b-6eee-40e9-8cae-1790db2281ca",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "mocking klog seems like overkill here... I mostly just care about not silently dropping unexpected cases",
        "createdAt" : "2020-06-24T15:09:12Z",
        "updatedAt" : "2020-06-24T15:09:12Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ed41c3f1036785c6c86dd35d20412c8387cf382",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +1837,1841 @@\t\tdefault:\n\t\t\tklog.Errorf(\"Invalid event type received: %d.\", u.Op)\n\t\t}\n\n\t\tkl.sourcesReady.AddSource(u.Source)"
  },
  {
    "id" : "4cbfe2e6-25a6-4465-b496-27fa8a14176e",
    "prId" : 90284,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90284#pullrequestreview-399255042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d53f788-394e-44b1-bfd0-1666a6e5e6fd",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "I'm curious how/why this duplication arose - are there calls to `NewMainKubelet` that aren't in `cmd/...` which could be relying on the internal calculation of the `nodeName`?",
        "createdAt" : "2020-04-20T21:35:06Z",
        "updatedAt" : "2020-04-23T23:53:08Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "cf13b169-d3fc-4b08-a6a7-7dc57f205168",
        "parentId" : "9d53f788-394e-44b1-bfd0-1666a6e5e6fd",
        "authorId" : "c666776f-f3a4-4854-9679-2638da5217ce",
        "body" : "Just confirmed. `NewMainKubelet` is only used in `cmd/kubelet/server.go`\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/master/cmd/kubelet/app/server.go#L1178\r\n\r\n--\r\nLooks like it was introduced in a previous kubelet refactoring @mtaufen https://github.com/kubernetes/kubernetes/commit/f277205f4fde9f6a540e196b12d9832569b07d8f\r\nI look forward to hearing your suggestions",
        "createdAt" : "2020-04-23T14:39:19Z",
        "updatedAt" : "2020-04-23T23:53:08Z",
        "lastEditedBy" : "c666776f-f3a4-4854-9679-2638da5217ce",
        "tags" : [
        ]
      },
      {
        "id" : "fb8bc8c9-d16d-4843-a31e-9aa35ecccbb5",
        "parentId" : "9d53f788-394e-44b1-bfd0-1666a6e5e6fd",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Kubemark `hollow_kubelet.go` calls directly into `RunKubelet`, so that would skip the initial hostname calculation IIRC.",
        "createdAt" : "2020-04-23T15:58:45Z",
        "updatedAt" : "2020-04-23T23:53:08Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "b5078c8c-cef5-4e1f-b648-809105ce9774",
        "parentId" : "9d53f788-394e-44b1-bfd0-1666a6e5e6fd",
        "authorId" : "c666776f-f3a4-4854-9679-2638da5217ce",
        "body" : "Right, `hollow_kubelet.go/server.go` --> `RunKubelet` (calculation IIRC)->`createAndInitKubelet`->`NewMainKubelet` (calculation IIRC again)\r\n\r\n--\r\nIt seems that NewMainKubelet does the calculation again IIRC is not necessary",
        "createdAt" : "2020-04-23T16:09:04Z",
        "updatedAt" : "2020-04-23T23:53:08Z",
        "lastEditedBy" : "c666776f-f3a4-4854-9679-2638da5217ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "40da2c521ac5a3e4708b3b90c5711f83344c8ae8",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +375,379 @@\thostname string,\n\thostnameOverridden bool,\n\tnodeName types.NodeName,\n\tnodeIP string,\n\tproviderID string,"
  },
  {
    "id" : "5ccfa105-8658-4bdf-8ef7-6dffa7fa5763",
    "prId" : 87759,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87759#pullrequestreview-357424664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcd2a088-b278-4e76-8956-abc833652e2c",
        "parentId" : null,
        "authorId" : "931a9d14-ddc7-40c6-a3a5-d7d7c5157663",
        "body" : "Is it guaranteed that the AdmitHandler is called on every occasion, for example on a Kubelet restart? I'm thinking if it's ever possible to have no allocated resources when a container is added.",
        "createdAt" : "2020-02-05T09:52:29Z",
        "updatedAt" : "2020-02-27T16:10:17Z",
        "lastEditedBy" : "931a9d14-ddc7-40c6-a3a5-d7d7c5157663",
        "tags" : [
        ]
      },
      {
        "id" : "a047a923-2629-4291-87f4-1e0747821ad3",
        "parentId" : "dcd2a088-b278-4e76-8956-abc833652e2c",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "Yes. Admit handler is always called -- even across a kubelet restart.",
        "createdAt" : "2020-02-05T15:28:23Z",
        "updatedAt" : "2020-02-27T16:10:17Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "a792b6dd-d6cb-4009-b816-72f3833dc564",
        "parentId" : "dcd2a088-b278-4e76-8956-abc833652e2c",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "I agree though, we will need to make sure (especially in the CPUManager), that we handle the fact that Allocate() can be called multiple times for the same container.",
        "createdAt" : "2020-02-05T15:40:40Z",
        "updatedAt" : "2020-02-27T16:10:17Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "0d71efee-25ea-47f8-b4c0-5211fbcc6b2c",
        "parentId" : "dcd2a088-b278-4e76-8956-abc833652e2c",
        "authorId" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "body" : "@nolancon ^^",
        "createdAt" : "2020-02-05T15:40:58Z",
        "updatedAt" : "2020-02-27T16:10:17Z",
        "lastEditedBy" : "8eff55b7-2f52-4dd6-b8a6-b75a1c427179",
        "tags" : [
        ]
      },
      {
        "id" : "036b56e0-2a53-47f5-b62d-443a7dc9f53b",
        "parentId" : "dcd2a088-b278-4e76-8956-abc833652e2c",
        "authorId" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "body" : "This seems to be already handled:  https://github.com/kubernetes/kubernetes/blob/aa964c905576d3e873994de156999f609ae8be2e/pkg/kubelet/cm/cpumanager/policy_static.go#L196-L199",
        "createdAt" : "2020-02-12T12:49:45Z",
        "updatedAt" : "2020-02-27T16:10:17Z",
        "lastEditedBy" : "9b4e4d81-187d-4943-a9be-08f439915f8f",
        "tags" : [
        ]
      }
    ],
    "commit" : "2327934a8602d2f0dad369a86c5475d27fc1b062",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +869,873 @@\tklet.AddPodSyncHandler(activeDeadlineHandler)\n\n\tklet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetAllocateResourcesPodAdmitHandler())\n\n\tcriticalPodAdmissionHandler := preemption.NewCriticalPodAdmissionHandler(klet.GetActivePods, killPodNow(klet.podWorkers, kubeDeps.Recorder), kubeDeps.Recorder)"
  },
  {
    "id" : "6aa34f22-704f-48dd-9e3f-73a5de9312be",
    "prId" : 85879,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85879#pullrequestreview-333728225",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ece8575-ac51-438e-8645-f3079051aecf",
        "parentId" : null,
        "authorId" : "ffdbc0a5-19fd-4509-a56e-a4979ac0c1d7",
        "body" : "Sorry for omitting this sentence, fix tests by only adding it @wojtek-t @dashpole .\r\nBy the way, I wonder why tests passed before.",
        "createdAt" : "2019-12-18T04:51:17Z",
        "updatedAt" : "2019-12-18T04:51:17Z",
        "lastEditedBy" : "ffdbc0a5-19fd-4509-a56e-a4979ac0c1d7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8e1cc9ee01afa5ff4d45d6a7317fd90c8dc2058",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +672,676 @@\tklet.dockerLegacyService = kubeDeps.dockerLegacyService\n\tklet.criHandler = kubeDeps.criHandler\n\tklet.runtimeService = kubeDeps.RemoteRuntimeService\n\n\tif utilfeature.DefaultFeatureGate.Enabled(features.RuntimeClass) && kubeDeps.KubeClient != nil {"
  },
  {
    "id" : "85dc28b5-766a-4501-9c40-0dd18ff40a5c",
    "prId" : 84253,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84253#pullrequestreview-306249678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "249ebad0-f727-42e9-ad54-c5ed2ad96ab6",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Worth making it clear in the comment that since we are calling this in `syncLoop`, we don't need to call it anywhere else?\r\n\r\n",
        "createdAt" : "2019-10-23T23:35:19Z",
        "updatedAt" : "2019-10-24T00:16:44Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae9e93d7845e1f5656c4ec6a87ff64b2ee3320a7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1834,1838 @@\t)\n\tduration := base\n\t// Responsible for checking limits in resolv.conf\n\t// The limits do not have anything to do with individual pods\n\t// Since this is called in syncLoop, we don't need to call it anywhere else"
  },
  {
    "id" : "3cc0b79d-8e33-44a2-b2db-4b6068fc3559",
    "prId" : 84253,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84253#pullrequestreview-306260469",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f6632ac-fddf-49bc-b5d0-11eb6f8ec530",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Nit: We could extract this into a function with a descriptive name (i.e. `performOnlyNecessaryResolvConfLimitsCheck`). That might be overengineering though... up to you :)",
        "createdAt" : "2019-10-23T23:36:14Z",
        "updatedAt" : "2019-10-24T00:16:44Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "ed89332a-3445-40ef-883f-d99262536672",
        "parentId" : "6f6632ac-fddf-49bc-b5d0-11eb6f8ec530",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Since there are only a few lines and it is called once, probably we don't need to extract it.",
        "createdAt" : "2019-10-24T00:17:55Z",
        "updatedAt" : "2019-10-24T00:17:55Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae9e93d7845e1f5656c4ec6a87ff64b2ee3320a7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1837,1841 @@\t// The limits do not have anything to do with individual pods\n\t// Since this is called in syncLoop, we don't need to call it anywhere else\n\tif kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" {\n\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()\n\t}"
  },
  {
    "id" : "e7e9398f-e93d-43e8-b09c-e0136235cf6c",
    "prId" : 83730,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83730#pullrequestreview-454989809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d765d9df-3333-42d8-8b48-8dab15ade990",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "is the drive/path to the windows installation a constant? I thought both were changeable",
        "createdAt" : "2019-11-16T03:52:34Z",
        "updatedAt" : "2021-02-02T12:15:22Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "51c8b6b2-13a9-4b75-b5e2-6bfa532b991f",
        "parentId" : "d765d9df-3333-42d8-8b48-8dab15ade990",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "should probably use https://godoc.org/golang.org/x/sys/windows#GetSystemDirectory",
        "createdAt" : "2019-11-16T03:54:24Z",
        "updatedAt" : "2021-02-02T12:15:22Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "f2e3e69e-6b6f-4955-a5fb-8071a3ea0a81",
        "parentId" : "d765d9df-3333-42d8-8b48-8dab15ade990",
        "authorId" : "9be482a0-90db-482a-8b45-44658adcfdf1",
        "body" : "Inside a Windows container, they're constant.",
        "createdAt" : "2020-07-24T15:39:27Z",
        "updatedAt" : "2021-02-02T12:15:22Z",
        "lastEditedBy" : "9be482a0-90db-482a-8b45-44658adcfdf1",
        "tags" : [
        ]
      }
    ],
    "commit" : "de4602995b0fc44037b0cc826c2bde61b4bb21a2",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +146,150 @@\t// The path in containers' filesystems where the hosts file is mounted.\n\tlinuxEtcHostsPath   = \"/etc/hosts\"\n\twindowsEtcHostsPath = \"C:\\\\Windows\\\\System32\\\\drivers\\\\etc\\\\hosts\"\n\n\t// Capacity of the channel for receiving pod lifecycle events. This number"
  },
  {
    "id" : "af30ff09-a29e-44b6-8ff0-5f61bc3748db",
    "prId" : 80369,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80369#pullrequestreview-270368328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f67c57a5-0d48-466e-b658-55c52628a9d7",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "if this recurs multiple times, we will just fall back to event spam protection, so i think this is fine.",
        "createdAt" : "2019-08-02T19:57:26Z",
        "updatedAt" : "2019-08-05T08:08:51Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "c744385804a7e51fdb15cc86894b30ff69bdcd70",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1674,1678 @@\t\t// Wait for volumes to attach/mount\n\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {\n\t\t\tkl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, \"Unable to attach or mount volumes: %v\", err)\n\t\t\tklog.Errorf(\"Unable to attach or mount volumes for pod %q: %v; skipping pod\", format.Pod(pod), err)\n\t\t\treturn err"
  },
  {
    "id" : "0c938459-a32b-4a50-b2b6-34518b96c799",
    "prId" : 79873,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79873#pullrequestreview-273406748",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00cad2c0-8c5e-4d91-b6b8-6d9d35a74ecb",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "In order for the propagation to occur, do we need to configure some entity to actually read the value of `runtimeError`?",
        "createdAt" : "2019-07-08T15:34:58Z",
        "updatedAt" : "2019-08-10T01:53:03Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "5fb4b08d-8294-442e-94aa-3cdc4a497d29",
        "parentId" : "00cad2c0-8c5e-4d91-b6b8-6d9d35a74ecb",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "+1",
        "createdAt" : "2019-08-09T23:52:35Z",
        "updatedAt" : "2019-08-10T01:53:03Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "daa92e8c-eb9d-4472-bd63-9e0a483d3f91",
        "parentId" : "00cad2c0-8c5e-4d91-b6b8-6d9d35a74ecb",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Updated the PR.\r\nruntimeError is considered by runtimeState#runtimeErrors",
        "createdAt" : "2019-08-10T01:54:09Z",
        "updatedAt" : "2019-08-10T01:54:09Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "3865e2b956a3e3ccf1131a3aa1b682aede5d3d4e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2183,2187 @@\t\tkl.runtimeState.setNetworkState(nil)\n\t}\n\t// information in RuntimeReady condition will be propagated to NodeReady condition.\n\truntimeReady := s.GetRuntimeCondition(kubecontainer.RuntimeReady)\n\t// If RuntimeReady is not set or is false, report an error."
  },
  {
    "id" : "5c6af4bc-dfc7-4f91-822f-197356738590",
    "prId" : 78414,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78414#pullrequestreview-242722420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a0e060b-113d-4c88-965b-5f732ca2d9c8",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Nit: Can you make it clear that these limits do not have anything to do with individual pods/the existence of an individual pod has no impact on these limits?",
        "createdAt" : "2019-05-28T14:48:56Z",
        "updatedAt" : "2019-05-30T20:52:01Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      }
    ],
    "commit" : "4979686a8175b6cad694ae2e7c0683384841af15",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2026,2030 @@\tstart := kl.clock.Now()\n\tsort.Sort(sliceutils.PodsByCreationTime(pods))\n\t// Responsible for checking limits in resolv.conf\n\t// The limits do not have anything to do with individual pods\n\tif kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" {"
  },
  {
    "id" : "4f8178a4-606d-48dc-a5da-b8c74661b924",
    "prId" : 78414,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78414#pullrequestreview-244020949",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f52968da-1f51-4e1f-b811-b8421f071fd1",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I'm not sure when the check should be performed. I'm not even sure why we placed the checks in pod add/update events. @MrHohn, do you have any idea?",
        "createdAt" : "2019-05-30T20:25:39Z",
        "updatedAt" : "2019-05-30T20:52:01Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "c8747887-1e04-4fac-a8bd-1c58f0b7a092",
        "parentId" : "f52968da-1f51-4e1f-b811-b8421f071fd1",
        "authorId" : "26bd3fa6-ac8d-4cb0-b746-a30dc1dc2931",
        "body" : "Good catch.. As this has nothing to do with pod, why didn't we call it just once upon kubelet startup? Maybe this was for preventing the resolv.conf file becomes invalid after kubelet startup (e.g. file edited after kubelet runs)? It seems like we read config from this file and generate pod dns config upon pod creation:\r\nhttps://github.com/kubernetes/kubernetes/blob/a44ef3079dbb188979c7de7f2d15e9c39800c869/pkg/kubelet/network/dns/dns.go#L323-L328",
        "createdAt" : "2019-05-30T20:41:47Z",
        "updatedAt" : "2019-05-30T20:52:01Z",
        "lastEditedBy" : "26bd3fa6-ac8d-4cb0-b746-a30dc1dc2931",
        "tags" : [
        ]
      },
      {
        "id" : "99ed5cad-6a3e-4890-bff3-cdbe63e1fd03",
        "parentId" : "f52968da-1f51-4e1f-b811-b8421f071fd1",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Since HandlePodAdditions accepts several Pods, it seems we can afford the cost of resolv.conf validation outside the loop.\r\n\r\nHandlePodUpdates has similar issue.\r\n\r\nWill push an update.",
        "createdAt" : "2019-05-30T20:46:28Z",
        "updatedAt" : "2019-05-30T20:52:01Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "4979686a8175b6cad694ae2e7c0683384841af15",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2028,2032 @@\t// Responsible for checking limits in resolv.conf\n\t// The limits do not have anything to do with individual pods\n\tif kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" {\n\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()\n\t}"
  },
  {
    "id" : "48a4e606-30ac-4fd6-96ba-e8e01c9f2052",
    "prId" : 73891,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73891#pullrequestreview-213695693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fd5fdfe-522a-47c3-b95f-a3f281d08c7d",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "1. Let's move the construction of `NewWatcher()`, `NewDesiredStateOfWorld()`, and `NewActualStateOfWorld()` inside the `NewPluginManager(...)` constructor method instead of passed in here as parameters -- these are implementation details of the PluginManager that are not relevant to the kubelet.\r\n2. Just have the kubelet pass in `sockDir`, `deprecatedSockDir`. And maybe `recorder` (Double check if PluginManger should be constructing its own new recorder or using this one. I don't remember off the top of my head).",
        "createdAt" : "2019-03-13T00:53:45Z",
        "updatedAt" : "2019-05-30T23:01:15Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e185544421d5504b0fe2ae32c8ebfb3000a1224",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +787,791 @@\t}\n\tif klet.enablePluginsWatcher {\n\t\tklet.pluginManager = pluginmanager.NewPluginManager(\n\t\t\tklet.getPluginsRegistrationDir(), /* sockDir */\n\t\t\tklet.getPluginsDir(),             /* deprecatedSockDir */"
  },
  {
    "id" : "e2ce5e98-08e3-4e1a-8762-0137b6b9914f",
    "prId" : 72910,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72910#pullrequestreview-193794137",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49431f62-837d-4b0a-a278-5ba33fd07c1b",
        "parentId" : null,
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "can a node name be changed without a process restart? /cc @derekwaynecarr ",
        "createdAt" : "2019-01-15T08:57:12Z",
        "updatedAt" : "2019-01-15T10:01:31Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "2868d304-b9d6-4048-9aa9-a52dbb42a631",
        "parentId" : "49431f62-837d-4b0a-a278-5ba33fd07c1b",
        "authorId" : "a472aa4a-c4f7-4692-bc21-708257a1dacc",
        "body" : "I do not find the place to change node name in code. But not sure.",
        "createdAt" : "2019-01-15T09:03:25Z",
        "updatedAt" : "2019-01-15T10:01:31Z",
        "lastEditedBy" : "a472aa4a-c4f7-4692-bc21-708257a1dacc",
        "tags" : [
        ]
      },
      {
        "id" : "4593fd63-353c-4d7e-ada3-5e20e205eee6",
        "parentId" : "49431f62-837d-4b0a-a278-5ba33fd07c1b",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "@brancz no.",
        "createdAt" : "2019-01-17T17:20:50Z",
        "updatedAt" : "2019-01-17T17:20:51Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "64c74c12-87aa-43e3-86d9-6e09e2a1e37c",
        "parentId" : "49431f62-837d-4b0a-a278-5ba33fd07c1b",
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "Thanks for the confirmation!",
        "createdAt" : "2019-01-17T19:12:57Z",
        "updatedAt" : "2019-01-17T19:12:57Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d73c7daed42fff44e2d49b47285b35adafe011c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1304,1308 @@\t\tcollectors.NewLogMetricsCollector(kl.StatsProvider.ListPodStats),\n\t)\n\tmetrics.SetNodeName(kl.nodeName)\n\n\t// Setup filesystem directories."
  },
  {
    "id" : "c8e9633b-abf2-4f56-873c-65aedb277481",
    "prId" : 70508,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70508#pullrequestreview-174101493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "526a3e29-752c-4a1e-9cc2-127112dcd741",
        "parentId" : null,
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "This should probably part of an `Init` function in the server rather than being at the top level ",
        "createdAt" : "2018-11-10T20:50:28Z",
        "updatedAt" : "2018-11-15T17:45:19Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "4aa40faa-3ff5-47ac-a1a6-2416d2a459b8",
        "parentId" : "526a3e29-752c-4a1e-9cc2-127112dcd741",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "I think I prefer keeping it with the creation of all of the other directories the kubelet makes.",
        "createdAt" : "2018-11-12T21:30:02Z",
        "updatedAt" : "2018-11-15T17:45:19Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "630cb53f82c19d0acc3116f7d48196f329ab6d10",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +1261,1265 @@\t\treturn fmt.Errorf(\"error creating plugins directory: %v\", err)\n\t}\n\tif err := os.MkdirAll(kl.getPodResourcesDir(), 0750); err != nil {\n\t\treturn fmt.Errorf(\"error creating podresources directory: %v\", err)\n\t}"
  },
  {
    "id" : "893dc2c8-0cfa-4a76-8d4b-a8d3da357e0a",
    "prId" : 67031,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67031#pullrequestreview-148386727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3e1aea1-8dd3-4a39-98dc-e4380a6b9855",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Please add the function that uses this lock in the comment, and state that the lock must not be used outside of the function.",
        "createdAt" : "2018-08-21T22:24:27Z",
        "updatedAt" : "2018-08-22T08:48:19Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "3d8efd62-8387-43b1-9391-a1c3349b9223",
        "parentId" : "e3e1aea1-8dd3-4a39-98dc-e4380a6b9855",
        "authorId" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "body" : "done",
        "createdAt" : "2018-08-22T08:49:49Z",
        "updatedAt" : "2018-08-22T08:49:49Z",
        "lastEditedBy" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ffa4e17e01e8abbadb8c25af2b8c9e73630256b",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1014,1018 @@\tnodeStatusUpdateFrequency time.Duration\n\n\t// syncNodeStatusMux is a lock on updating the node status, because this path is not thread-safe.\n\t// This lock is used by Kublet.syncNodeStatus function and shouldn't be used anywhere else.\n\tsyncNodeStatusMux sync.Mutex"
  },
  {
    "id" : "3de3719f-070a-41b7-b9dc-3cd99826fe63",
    "prId" : 65226,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65226#pullrequestreview-135452111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ae45317-22d4-4508-ad27-268d2d8c8202",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "it would be good to have a broader discussion on if we want all interaction with `cloud` to go through this interface in the future, or if we want to change the cloud provider interface to accept a context w/ timeout on operations in the future so each caller can decide how to handle it across the code base.  for now, this cleans up the existing member vars on kubelet so is a nice incremental improvement.",
        "createdAt" : "2018-07-09T14:58:19Z",
        "updatedAt" : "2018-07-09T15:06:02Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d9fb4de2929bdeba897c9cd79aea616a75c9521",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1006,1010 @@\tcloud cloudprovider.Interface\n\t// Handles requests to cloud provider with timeout\n\tcloudResourceSyncManager *cloudResourceSyncManager\n\n\t// Indicates that the node initialization happens in an external cloud controller"
  },
  {
    "id" : "8f96abdf-0f86-4500-bd48-5e4346628e76",
    "prId" : 64560,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64560#pullrequestreview-125795641",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e62bddf4-1bcb-4cea-a811-488d36b753f1",
        "parentId" : null,
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "This could be used from API instead of harcoding https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/pluginregistration/v1alpha1/constants.go#L20",
        "createdAt" : "2018-06-05T01:12:04Z",
        "updatedAt" : "2018-06-05T01:12:04Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "b7d65a59-9c6f-4304-986b-b98069956abe",
        "parentId" : "e62bddf4-1bcb-4cea-a811-488d36b753f1",
        "authorId" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "body" : "yep, will do in the follow up PR.",
        "createdAt" : "2018-06-05T01:43:22Z",
        "updatedAt" : "2018-06-05T01:43:22Z",
        "lastEditedBy" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea474cd99cb4eb769bf8d80885edd817e2fe739f",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1293,1297 @@\tif kl.enablePluginsWatcher {\n\t\t// Adding Registration Callback function for CSI Driver\n\t\tkl.pluginWatcher.AddHandler(\"CSIPlugin\", csi.RegistrationCallback)\n\n\t\t// Start the plugin watcher"
  },
  {
    "id" : "2e156670-5898-483c-ab6b-5c33d2f5eaa7",
    "prId" : 64006,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64006#pullrequestreview-125348356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6718eb0-2e2a-4558-9f43-e2a193523c81",
        "parentId" : null,
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "Can this use a unix socket instead of localhost? (more secure)",
        "createdAt" : "2018-06-01T01:00:59Z",
        "updatedAt" : "2018-06-01T01:24:56Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "7d018e06-e1ca-471e-868e-bd2561d6db96",
        "parentId" : "d6718eb0-2e2a-4558-9f43-e2a193523c81",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "I can add a TODO.\r\n\r\nI decided to go with localhost first because it is simpler. For unix socket, we need to parse URL returned by container runtime to figure out whether it is unix socket (container runtime could still return regular URL), and handle unix socket differently for NewUpgradeAwareHandler.",
        "createdAt" : "2018-06-01T08:28:15Z",
        "updatedAt" : "2018-06-01T08:28:15Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "7f72f56c-69d1-4a8a-a9e5-9a7a90dd706a",
        "parentId" : "d6718eb0-2e2a-4558-9f43-e2a193523c81",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "Follow up is fine, but I'm just curious - did you try this? I don't see anything in NewUpgradeAwareHandler that looks like it would have a problem with a unix URL?",
        "createdAt" : "2018-06-02T00:37:33Z",
        "updatedAt" : "2018-06-02T00:37:34Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      }
    ],
    "commit" : "746c32db4cf47a0245ef1c1aa053df49fcf51c13",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +2146,2150 @@\t}\n\tif !crOptions.RedirectContainerStreaming {\n\t\tconfig.Addr = net.JoinHostPort(\"localhost\", \"0\")\n\t} else {\n\t\t// Use a relative redirect (no scheme or host)."
  },
  {
    "id" : "a692d558-81bb-4ace-999b-20d959c005cf",
    "prId" : 63170,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63170#pullrequestreview-116974361",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32344553-c83e-41b8-8c7e-7c91dd61cb15",
        "parentId" : null,
        "authorId" : "ddc05ecb-1a86-4393-8e22-f1bb528c2d50",
        "body" : "I think this is a pretty clean way of allowing validateNodeIP() to be mocked for testing.  The alternative I can think of is to create a kubelet wrapper for `net.InterfaceAddrs()` to allow you test get testing coverage for the `validateNodeIP()` function itself.",
        "createdAt" : "2018-05-02T16:35:05Z",
        "updatedAt" : "2018-05-02T16:54:29Z",
        "lastEditedBy" : "ddc05ecb-1a86-4393-8e22-f1bb528c2d50",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a218aaee2102f9f2708366ebc1934d8602e7d2d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +522,526 @@\t\tcontainerRuntimeName: containerRuntime,\n\t\tnodeIP:               parsedNodeIP,\n\t\tnodeIPValidator:      validateNodeIP,\n\t\tclock:                clock.RealClock{},\n\t\tenableControllerAttachDetach:            kubeCfg.EnableControllerAttachDetach,"
  },
  {
    "id" : "74ae1434-60f6-4fc1-bac7-5c6d50ce6769",
    "prId" : 61869,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61869#pullrequestreview-133009863",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "435ec886-5987-4251-9a23-455962195258",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this doesn't seem to make sense... isn't hostnameOverride an \"explicit configuration\" as well? why would --node-ip trump the cloud provider, and not --hostname-override? I could see making the cloud provider completely authoritative, but the ordering here doesn't make sense to me\r\n\r\nedit: retracing this, this address isn't `--node-ip`, it's `--address`. that makes even less sense to have it override the cloud-provider-specified addresses.\r\n\r\nkubelet node status jumps through a ton of hoops to derive an IP address to report:\r\n\r\n```\r\n\t\t// 1) Use nodeIP if set\r\n\t\t// 2) If the user has specified an IP to HostnameOverride, use it\r\n\t\t// 3) Lookup the IP from node name by DNS and use the first valid IPv4 address.\r\n\t\t//    If the node does not have a valid IPv4 address, use the first valid IPv6 address.\r\n\t\t// 4) Try to get the IP from the network interface used as default gateway\r\n\t\tif kl.nodeIP != nil {\r\n\t\t\tipAddr = kl.nodeIP\r\n\t\t} else if addr := net.ParseIP(kl.hostname); addr != nil {\r\n\t\t\tipAddr = addr\r\n\t\t} else {\r\n\t\t\tvar addrs []net.IP\r\n\t\t\taddrs, _ = net.LookupIP(node.Name)\r\n\t\t\tfor _, addr := range addrs {\r\n\t\t\t\tif err = kl.nodeIPValidator(addr); err == nil {\r\n\t\t\t\t\tif addr.To4() != nil {\r\n\t\t\t\t\t\tipAddr = addr\r\n\t\t\t\t\t\tbreak\r\n\t\t\t\t\t}\r\n\t\t\t\t\tif addr.To16() != nil && ipAddr == nil {\r\n\t\t\t\t\t\tipAddr = addr\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\tif ipAddr == nil {\r\n\t\t\t\tipAddr, err = utilnet.ChooseHostInterface()\r\n\t\t\t}\r\n\t\t}\r\n```\r\n\r\nunfortunately, we have to match that here when requesting certs",
        "createdAt" : "2018-06-28T15:16:43Z",
        "updatedAt" : "2018-06-28T15:50:19Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "236d38aa-10b8-41a3-83a9-8b0162a2249a",
        "parentId" : "435ec886-5987-4251-9a23-455962195258",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Updated https://github.com/kubernetes/kubernetes/pull/65585 to make this match as well",
        "createdAt" : "2018-06-28T15:38:02Z",
        "updatedAt" : "2018-06-28T15:38:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ef7a6b20-28db-4679-88d3-7a32c50e0517",
        "parentId" : "435ec886-5987-4251-9a23-455962195258",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "long-term (hopefully in 1.12), we want to switch the serving cert request to react to changes in the node status, so that the node always has a valid cert (or a pending csr) for the addresses in the node status. that lets us contain the crazy address-computing code to one place, and also drive node addresses via an external cloud provider",
        "createdAt" : "2018-06-28T16:04:18Z",
        "updatedAt" : "2018-06-28T16:04:18Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "47c39cbc-95ef-4942-87aa-4fc6dea0aaec",
        "parentId" : "435ec886-5987-4251-9a23-455962195258",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "> long-term (hopefully in 1.12)\r\n\r\nturns out it was easier than expected. see https://github.com/kubernetes/kubernetes/pull/65594",
        "createdAt" : "2018-06-28T20:35:31Z",
        "updatedAt" : "2018-06-28T20:35:31Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "7354bbe5ac838af560ca5dbabcd7d05e28ef8f59",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +745,749 @@\t\t// If the address was explicitly configured, use that. Otherwise, try to\n\t\t// discover addresses from the cloudprovider. Otherwise, make a best guess.\n\t\tif cfgAddress := net.ParseIP(kubeCfg.Address); cfgAddress != nil && !cfgAddress.IsUnspecified() {\n\t\t\tips = []net.IP{cfgAddress}\n\t\t\tnames = []string{klet.GetHostname(), hostnameOverride}"
  },
  {
    "id" : "57dfd472-e727-4460-86bd-8061198f6e3d",
    "prId" : 60246,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60246#pullrequestreview-99523460",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90565213-4ee8-4dbc-8c72-c294d011a797",
        "parentId" : null,
        "authorId" : "547bbedc-a8ad-4ffb-9ddf-e4f35eb761b9",
        "body" : "Could [wait.ExponentialBackoff](https://godoc.org/k8s.io/apimachinery/pkg/util/wait#ExponentialBackoff) be used here?",
        "createdAt" : "2018-02-22T22:20:03Z",
        "updatedAt" : "2018-02-22T22:20:03Z",
        "lastEditedBy" : "547bbedc-a8ad-4ffb-9ddf-e4f35eb761b9",
        "tags" : [
        ]
      },
      {
        "id" : "a35bc3be-ac08-445a-9902-7aa381dfcdda",
        "parentId" : "90565213-4ee8-4dbc-8c72-c294d011a797",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Unfortunately wait.ExponentialBackoff exits after a fixed number of steps, while we want to loop forever if necessary.",
        "createdAt" : "2018-02-26T16:49:40Z",
        "updatedAt" : "2018-02-26T16:49:40Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "f4749db7-69ee-4cfc-ab3d-4007b9196f45",
        "parentId" : "90565213-4ee8-4dbc-8c72-c294d011a797",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "I'm not sure about this. Even though you check aggressively here, the other part which updates `runtimeErrors` still works at its original rate, e.g. `updateRuntimeUp` is still called every 5 seconds...\r\n\r\nI can imagine this may help in some way, but are we sure this is the right place to put the backoff?",
        "createdAt" : "2018-02-27T00:27:42Z",
        "updatedAt" : "2018-02-27T00:27:45Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "ab169c18-e650-48d8-ac93-90d651caad06",
        "parentId" : "90565213-4ee8-4dbc-8c72-c294d011a797",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "This helps the generic case of runtimeErrors. We can certainly add more specific backoffs and/or implement a channel-signaling mechanism in the future to further improve startup performance.",
        "createdAt" : "2018-02-27T00:45:17Z",
        "updatedAt" : "2018-02-27T00:45:17Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "7290313dfdda4554e19d06e14d8a554e9e884615",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1770,1774 @@\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\tglog.Infof(\"skipping pod synchronization - %v\", rs)\n\t\t\t// exponential backoff\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))"
  },
  {
    "id" : "ed028ce9-a073-4a1f-8350-3e0d681f9780",
    "prId" : 58755,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58755#pullrequestreview-134511636",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8345ce18-1e2d-40a2-9ec7-22bcaaed7783",
        "parentId" : null,
        "authorId" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "body" : "probably make sense to replace the \"CSIPlugin\" with pluginwatcherapi.CSIPlugin in this change, how do you think",
        "createdAt" : "2018-07-05T05:31:12Z",
        "updatedAt" : "2018-07-17T08:03:57Z",
        "lastEditedBy" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "tags" : [
        ]
      }
    ],
    "commit" : "a5842503eb6932f72c3e14f6bf47c6bc65a0a87f",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1353,1357 @@\tif kl.enablePluginsWatcher {\n\t\t// Adding Registration Callback function for CSI Driver\n\t\tkl.pluginWatcher.AddHandler(\"CSIPlugin\", csi.RegistrationCallback)\n\t\t// Adding Registration Callback function for Device Manager\n\t\tkl.pluginWatcher.AddHandler(pluginwatcherapi.DevicePlugin, kl.containerManager.GetPluginRegistrationHandlerCallback())"
  },
  {
    "id" : "1aedb8f4-8e6d-4e71-9d93-c085843449a5",
    "prId" : 51645,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51645#pullrequestreview-60252257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25074319-c008-4f76-b379-c48fbda3385c",
        "parentId" : null,
        "authorId" : "26bd3fa6-ac8d-4cb0-b746-a30dc1dc2931",
        "body" : "Could we update the comment to explain what it does here?",
        "createdAt" : "2017-09-01T22:13:01Z",
        "updatedAt" : "2017-09-05T18:40:42Z",
        "lastEditedBy" : "26bd3fa6-ac8d-4cb0-b746-a30dc1dc2931",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d4bc931d34389d4c33ed941c6cd8c22400e92db",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +760,764 @@\n\t// If the experimentalMounterPathFlag is set, we do not want to\n\t// check node capabilities since the mount path is not the default\n\tif len(kubeCfg.ExperimentalMounterPath) != 0 {\n\t\tkubeCfg.ExperimentalCheckNodeCapabilitiesBeforeMount = false"
  },
  {
    "id" : "693d1ffe-cac3-4b7e-8fca-346443d33945",
    "prId" : 51423,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51423#pullrequestreview-99128016",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "929d6fc7-2550-4b39-aff7-27ae3f80a724",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "why not use wait.NeverStop?  i am not sure i understand this part of the change.",
        "createdAt" : "2018-02-24T20:46:31Z",
        "updatedAt" : "2018-02-25T00:39:46Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "8a1342ed-a72e-446d-be75-33188ba41798",
        "parentId" : "929d6fc7-2550-4b39-aff7-27ae3f80a724",
        "authorId" : "7a97283c-9495-4a1b-82e7-4e34839b21c1",
        "body" : "wait.NeverStop is a global var, it is also used by  container GC, so i define another stopChan same as wait.NeverStop, and the stopChan  just used by imageGC to receive the stop signal then jump out the wait.Util() logic.",
        "createdAt" : "2018-02-25T00:29:04Z",
        "updatedAt" : "2018-02-25T00:39:46Z",
        "lastEditedBy" : "7a97283c-9495-4a1b-82e7-4e34839b21c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "13054295b5ffd3a6dee4e990cfc4784cbcae3779",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1251,1255 @@\tif kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 {\n\t\tglog.V(2).Infof(\"ImageGCHighThresholdPercent is set 100, Disable image GC\")\n\t\tgo func() { stopChan <- struct{}{} }()\n\t}\n"
  },
  {
    "id" : "af343802-ea5c-406a-a4d5-7e09aec28557",
    "prId" : 50984,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50984#pullrequestreview-77623056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31e21dbd-06ab-4608-ac48-ea97dbe6c3e1",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Add a comment warning that this must happen before creating the apiserver source below, or the checkpoint would override the source of truth (apiserver). ",
        "createdAt" : "2017-11-16T21:47:36Z",
        "updatedAt" : "2017-11-21T04:24:51Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "79f72c5a-5721-473f-b536-c79fd774ae2f",
        "parentId" : "31e21dbd-06ab-4608-ac48-ea97dbe6c3e1",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Fixed. ",
        "createdAt" : "2017-11-18T19:10:13Z",
        "updatedAt" : "2017-11-21T04:24:51Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "763122ae4bc86a055e9c0cca5a3ac987caef23e2",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +298,302 @@\t}\n\n\t// Restore from the checkpoint path\n\t// NOTE: This MUST happen before creating the apiserver source\n\t// below, or the checkpoint would override the source of truth."
  },
  {
    "id" : "f4d7b53f-4913-4cbc-bb9c-6be3a4fe787f",
    "prId" : 50984,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50984#pullrequestreview-77994651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "718c0714-89e6-47f6-9d82-874fb41ebd40",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Remove the original (now duplicated) comment below (line 1880)",
        "createdAt" : "2017-11-16T22:03:33Z",
        "updatedAt" : "2017-11-21T04:24:51Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "5a33ea8a-8321-4149-9581-ad3e50b6b6c2",
        "parentId" : "718c0714-89e6-47f6-9d82-874fb41ebd40",
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "Fixed.",
        "createdAt" : "2017-11-21T03:56:23Z",
        "updatedAt" : "2017-11-21T04:24:51Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      }
    ],
    "commit" : "763122ae4bc86a055e9c0cca5a3ac987caef23e2",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +1868,1872 @@\t\t\t// source as ready.\n\n\t\t\t// Mark the source ready after receiving at least one update from the\n\t\t\t// source. Once all the sources are marked ready, various cleanup\n\t\t\t// routines will start reclaiming resources. It is important that this"
  },
  {
    "id" : "81bde32e-27a9-4a8a-ad46-a63b1c39e19b",
    "prId" : 46884,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46884#pullrequestreview-43554461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "325798cb-97c8-4993-a432-6a03a71568aa",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I'd really like to see this change make 1.7. I'd prefer not to have a mix of CSR formats to recognize unnecessarily",
        "createdAt" : "2017-06-11T22:29:45Z",
        "updatedAt" : "2017-06-16T15:47:20Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "fdd698ec-02fb-440c-bd4a-43db6a7737bd",
        "parentId" : "325798cb-97c8-4993-a432-6a03a71568aa",
        "authorId" : "b8d9ba23-8a1c-44a6-afe2-8ac9498838bb",
        "body" : "Working on it.",
        "createdAt" : "2017-06-12T20:34:20Z",
        "updatedAt" : "2017-06-16T15:47:20Z",
        "lastEditedBy" : "b8d9ba23-8a1c-44a6-afe2-8ac9498838bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "334de1cbe1884d96d778e8c2f9fd35e9dabca998",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1135,1139 @@\t\tTemplate: &x509.CertificateRequest{\n\t\t\tSubject: pkix.Name{\n\t\t\t\tCommonName:   fmt.Sprintf(\"system:node:%s\", nodeName),\n\t\t\t\tOrganization: []string{\"system:nodes\"},\n\t\t\t},"
  },
  {
    "id" : "552ce4e5-6f7d-42a8-b647-90c5f95bba4c",
    "prId" : 46089,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46089#pullrequestreview-39526357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5be28574-11e8-48cc-8fd3-9e7ed0754807",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Good catch! :-)",
        "createdAt" : "2017-05-22T16:43:26Z",
        "updatedAt" : "2017-05-25T02:21:26Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb26c9100a78fdb1f2733cbfca727519a7d5ac8f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +571,575 @@\t\t}\n\t\truntimeService, imageService, err := getRuntimeAndImageServices(kubeCfg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}"
  },
  {
    "id" : "c3476094-4cd8-4a9d-afaf-841059873886",
    "prId" : 45059,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45059#pullrequestreview-38736801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fc0d901-2057-409c-8734-22d42e2bf965",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this isn't what a serving cert for a node looks like... it need to have the node's IPs and DNS names as subject alt names... not sure what the subject should be",
        "createdAt" : "2017-05-15T16:30:46Z",
        "updatedAt" : "2017-05-29T19:28:10Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "147268bc-614e-45fa-9142-670e6dda3951",
        "parentId" : "9fc0d901-2057-409c-8734-22d42e2bf965",
        "authorId" : "0af48e59-4be9-46ce-9275-aa218813b6fd",
        "body" : "CertificateRequest looks correct now.",
        "createdAt" : "2017-05-17T17:30:48Z",
        "updatedAt" : "2017-05-29T19:28:10Z",
        "lastEditedBy" : "0af48e59-4be9-46ce-9275-aa218813b6fd",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c22e6bc6a2be95a05077ef95e67a4ead9f3f3d2",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +1114,1118 @@\t\t\tSubject: pkix.Name{\n\t\t\t\tCommonName:   string(nodeName),\n\t\t\t\tOrganization: []string{\"system:nodes\"},\n\t\t\t},\n\t\t\tDNSNames:    hostnames,"
  },
  {
    "id" : "2fca0757-6aec-45b2-b9bd-63d2f32da90a",
    "prId" : 45059,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45059#pullrequestreview-40777950",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1ffa3d2-3f19-4993-80a9-b6c15e1b3ab1",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "nit: might be worth having a helper in the certificates api package that returns usages for a TLS client cert, and one that returns usages for a TLS serving cert.",
        "createdAt" : "2017-05-27T18:51:10Z",
        "updatedAt" : "2017-05-29T19:28:10Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "250e401f-6401-4f80-ac98-60fc84dfd84f",
        "parentId" : "e1ffa3d2-3f19-4993-80a9-b6c15e1b3ab1",
        "authorId" : "b8d9ba23-8a1c-44a6-afe2-8ac9498838bb",
        "body" : "Will follow on with this change.",
        "createdAt" : "2017-05-29T13:50:00Z",
        "updatedAt" : "2017-05-29T19:28:10Z",
        "lastEditedBy" : "b8d9ba23-8a1c-44a6-afe2-8ac9498838bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c22e6bc6a2be95a05077ef95e67a4ead9f3f3d2",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +1119,1123 @@\t\t\tIPAddresses: ips,\n\t\t},\n\t\tUsages: []certificates.KeyUsage{\n\t\t\tcertificates.UsageKeyEncipherment,\n\t\t\tcertificates.UsageServerAuth,"
  },
  {
    "id" : "8dbd3b92-3c61-4fb9-a091-527e22c68180",
    "prId" : 45059,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45059#pullrequestreview-40814567",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f873c94-8a98-49bb-8c0a-1d660aba0b71",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "the kubelet takes an `--address` option that lets you bind to a specific interface. if that is set to a specific IP, we should just use that (we should only iterate over all interfaces if it is binding to 0.0.0.0)",
        "createdAt" : "2017-05-27T18:55:38Z",
        "updatedAt" : "2017-05-29T19:28:10Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ddf4242b-ced0-46f7-8dc8-9107715f3165",
        "parentId" : "0f873c94-8a98-49bb-8c0a-1d660aba0b71",
        "authorId" : "b8d9ba23-8a1c-44a6-afe2-8ac9498838bb",
        "body" : "Done.",
        "createdAt" : "2017-05-29T18:31:15Z",
        "updatedAt" : "2017-05-29T19:28:10Z",
        "lastEditedBy" : "b8d9ba23-8a1c-44a6-afe2-8ac9498838bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c22e6bc6a2be95a05077ef95e67a4ead9f3f3d2",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +1128,1132 @@}\n\nfunc allLocalIPsWithoutLoopback() ([]net.IP, error) {\n\tinterfaces, err := net.Interfaces()\n\tif err != nil {"
  },
  {
    "id" : "5cc86145-31fd-4f6f-95a0-c4d6bb03b695",
    "prId" : 45059,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45059#pullrequestreview-48191989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "260f281d-7d77-4f2d-9d81-4218fe736580",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I'd like to move this into its own package (since it's not dependent on anything in the kubelet) and import it, primarily to separate it from this mega package, but also to make it easier to consume from other contexts (like tools that want to provide their own certificate management in a similar form).\r\n\r\nObjections to a small refactor?  I also want to do that for the client code.",
        "createdAt" : "2017-07-05T21:58:00Z",
        "updatedAt" : "2017-07-05T21:58:01Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "b9f6a3b7-fa48-45ef-bb15-66237702b3da",
        "parentId" : "260f281d-7d77-4f2d-9d81-4218fe736580",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "https://github.com/kubernetes/kubernetes/pull/48518",
        "createdAt" : "2017-07-05T22:10:43Z",
        "updatedAt" : "2017-07-05T22:10:43Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c22e6bc6a2be95a05077ef95e67a4ead9f3f3d2",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +1095,1099 @@}\n\nfunc initializeServerCertificateManager(kubeClient clientset.Interface, kubeCfg *componentconfig.KubeletConfiguration, nodeName types.NodeName, ips []net.IP, hostnames []string) (certificate.Manager, error) {\n\tvar certSigningRequestClient clientcertificates.CertificateSigningRequestInterface\n\tif kubeClient != nil && kubeClient.Certificates() != nil {"
  },
  {
    "id" : "704575ab-4648-437e-9ca4-dc8f64cb545b",
    "prId" : 42195,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42195#pullrequestreview-24131186",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d82ea755-8bcb-4bef-bcc2-dcdeebaea6f0",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Can we also log an information on which logging driver is using as part of NodeStatus? Today we don't have any such information. ",
        "createdAt" : "2017-02-28T00:10:09Z",
        "updatedAt" : "2017-03-01T18:50:19Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "2cd8df7a-dd60-4692-8297-9b89333d4bed",
        "parentId" : "d82ea755-8bcb-4bef-bcc2-dcdeebaea6f0",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Will do that in this PR.",
        "createdAt" : "2017-02-28T00:21:54Z",
        "updatedAt" : "2017-03-01T18:50:19Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c261bfed7a14db5d4c943e445e4b585d3ea39af",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +578,582 @@\t\t\t}\n\t\t\tif !supported {\n\t\t\t\tklet.dockerLegacyService = dockershim.NewDockerLegacyService(klet.dockerClient)\n\t\t\t}\n\t\tcase \"remote\":"
  },
  {
    "id" : "b1c8dafe-d7b9-4629-96bc-58e5cf1436ff",
    "prId" : 36984,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36984#pullrequestreview-17107178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eacb9d1d-4078-4d27-b7c7-09bb4492d1f6",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "If podManager is responsible for keeping the secrets up-to-date, why couldn't we just hide secret manager and its cache compeletely in the pod manager? This way, all components interact with podManager directly and might help simplify the interactions between the kubelet components. WDYT? ",
        "createdAt" : "2017-01-04T17:28:42Z",
        "updatedAt" : "2017-01-19T19:12:15Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "8acbb197-88af-4f36-b63e-a9bb2936231d",
        "parentId" : "eacb9d1d-4078-4d27-b7c7-09bb4492d1f6",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "not sure about that... the volume components that ask directly for configmaps and secrets are not really made simpler by switching to podmanager, imo",
        "createdAt" : "2017-01-04T19:46:19Z",
        "updatedAt" : "2017-01-19T19:12:15Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "e4b7949c-1c49-4056-9a24-3a989101ac24",
        "parentId" : "eacb9d1d-4078-4d27-b7c7-09bb4492d1f6",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "If we expose the \"register/unregister\" functions to other components, eventually some other components may start calling these functions and the logic may grow even more complicated. If the these operations are only used by PodManager, PodManager can embed a new interface to expose `GetSecret` (or `GetConfigMap`) only. This is just a thought to see if we can hide unnecessary details to have tighter control.  I am okay with a separate secret manager as well.",
        "createdAt" : "2017-01-04T23:03:07Z",
        "updatedAt" : "2017-01-19T19:12:15Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "2f3bdcfe-cd59-4548-a8b2-b399110a562f",
        "parentId" : "eacb9d1d-4078-4d27-b7c7-09bb4492d1f6",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I think that we should (at least initially) start with a separate interface, and then see if it makes sense to merge it into podManager (or sth else). We don't yet fully know where it will evolve, I guess.",
        "createdAt" : "2017-01-17T10:54:08Z",
        "updatedAt" : "2017-01-19T19:12:15Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "4f7e36e4-7c6c-4e1f-bf0a-96f313abc3c5",
        "parentId" : "eacb9d1d-4078-4d27-b7c7-09bb4492d1f6",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "sgtm",
        "createdAt" : "2017-01-17T22:10:46Z",
        "updatedAt" : "2017-01-19T19:12:15Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c0d2bb1f0259b6b826b2a12d4e76b2db1e619ed",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +507,511 @@\tklet.podCache = kubecontainer.NewCache()\n\t// podManager is also responsible for keeping secretManager contents up-to-date.\n\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager)\n\n\tif kubeCfg.RemoteRuntimeEndpoint != \"\" {"
  },
  {
    "id" : "63a2d14d-fa89-485d-9954-3d77ce1572a1",
    "prId" : 36280,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36280#pullrequestreview-7524071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f9f87a3-2cd6-4395-b321-b319cd85ca36",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "nit: Add a comment explaining what this is doing and why.\n",
        "createdAt" : "2016-11-07T22:02:24Z",
        "updatedAt" : "2016-11-09T23:16:22Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "4ec81873-8452-4a59-a5d1-1775985f5a07",
        "parentId" : "5f9f87a3-2cd6-4395-b321-b319cd85ca36",
        "authorId" : "3493bb4b-b4bb-4e5d-b6dc-7c87de7da51b",
        "body" : "Done.\n",
        "createdAt" : "2016-11-07T22:53:26Z",
        "updatedAt" : "2016-11-09T23:16:22Z",
        "lastEditedBy" : "3493bb4b-b4bb-4e5d-b6dc-7c87de7da51b",
        "tags" : [
        ]
      }
    ],
    "commit" : "d81e216fc6b421bd15be5bf17c3983f959809083",
    "line" : null,
    "diffHunk" : "@@ -1,1 +719,723 @@\t// If the experimentalMounterPathFlag is set, we do not want to\n\t// check node capabilities since the mount path is not the default\n\tif len(kubeCfg.ExperimentalMounterPath) != 0 {\n\t\tkubeCfg.ExperimentalCheckNodeCapabilitiesBeforeMount = false\n\t}"
  },
  {
    "id" : "e9a3bd23-e5e2-4eb1-988f-014dd14b64ce",
    "prId" : 36253,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36253#pullrequestreview-7320494",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05798a1a-c8ea-46c5-989d-3e562af294ac",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Does this work with the redirect following code in the apiserver? I don't remember relative URL resolution\n",
        "createdAt" : "2016-11-05T01:01:22Z",
        "updatedAt" : "2016-11-08T18:59:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "f64a1021-efdf-4608-b307-0e14410faf81",
        "parentId" : "05798a1a-c8ea-46c5-989d-3e562af294ac",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Yes. The relative URL is resolved here: https://github.com/kubernetes/kubernetes/pull/34987/files#diff-4b9b9659e8f0abce3dab96b924f6b107R312\nI also verified this works manually.\n",
        "createdAt" : "2016-11-05T19:36:18Z",
        "updatedAt" : "2016-11-08T18:59:02Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "7badc1d226624320ac1a13179d13baf868ad5fa4",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +2144,2148 @@\t\t// Use a relative redirect (no scheme or host).\n\t\tBaseURL: &url.URL{\n\t\t\tPath: \"/cri/\",\n\t\t},\n\t\tStreamIdleTimeout:     kubeCfg.StreamingConnectionIdleTimeout.Duration,"
  },
  {
    "id" : "cf429098-73bd-43e0-b627-84aa9c1e69af",
    "prId" : 35839,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35839#pullrequestreview-7111453",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e040531d-0440-4b3c-b9bf-a07301aa5f3a",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "This flag should work with normal CRI integration (which only sets --remote-runtime-endpoint and --remote--image-endpoint). I think this should work:\n\n```\nif kl.kubeletConfiguration.ExperimentalRuntimeIntegrationType == \"cri\" || kl.kubeletConfiguration.ContainerRuntime == \"remote\" {\n```\n",
        "createdAt" : "2016-11-03T21:50:54Z",
        "updatedAt" : "2016-11-05T07:02:23Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "9562a394-cbb6-4754-8b6c-5ede58d5fcba",
        "parentId" : "e040531d-0440-4b3c-b9bf-a07301aa5f3a",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Never mind. I decided to send in #36201 to enforce that all integration using CRI should specify --experimental-runtime-integration-type=cri for now to simplify the code. \n",
        "createdAt" : "2016-11-04T00:43:19Z",
        "updatedAt" : "2016-11-05T07:02:23Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "90fe0c59942f5890cd9398441c1c4f057f5fb187",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1952,1956 @@\t// Only check specific conditions when runtime integration type is cri,\n\t// because the old integration doesn't populate any runtime condition.\n\tif kl.kubeletConfiguration.ExperimentalRuntimeIntegrationType == \"cri\" {\n\t\tif s == nil {\n\t\t\tglog.Errorf(\"Container runtime status is nil\")"
  },
  {
    "id" : "2e2509e9-63f1-4d07-9bb8-cdb1ce5e20ad",
    "prId" : 35342,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35342#pullrequestreview-6740311",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e588703-639f-4a0e-bedd-6702b9ef8f2d",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Can I say that calling this new method as canRunPod, which is the same as what you are using below: pkg/kubelet/util.go::canRunPod(...) is very confusing to me? \n",
        "createdAt" : "2016-11-01T00:45:43Z",
        "updatedAt" : "2016-11-02T18:05:32Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "348a1f78-52a7-43a8-8307-bd50c6077a5b",
        "parentId" : "5e588703-639f-4a0e-bedd-6702b9ef8f2d",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Ack. I left a TODO to get rid of that other method. Do you have a suggestion for a better name?\n",
        "createdAt" : "2016-11-02T02:23:04Z",
        "updatedAt" : "2016-11-02T18:05:32Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec9111d94218cb564c4f9ed06b06a570406966d1",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +1622,1626 @@}\n\nfunc (kl *Kubelet) canRunPod(pod *api.Pod) lifecycle.PodAdmitResult {\n\tattrs := &lifecycle.PodAdmitAttributes{Pod: pod}\n\t// Get \"OtherPods\". Rejected pods are failed, so only include admitted pods that are alive."
  },
  {
    "id" : "681988a1-b483-461f-a72d-04853103ee3d",
    "prId" : 33862,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33862#pullrequestreview-2427900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05a09bb9-d971-433a-9183-501c14acc308",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "why changing this?\n",
        "createdAt" : "2016-09-30T22:02:06Z",
        "updatedAt" : "2016-10-04T09:45:43Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "1e1ea72e-ac77-4300-b9f5-eaefab7cde47",
        "parentId" : "05a09bb9-d971-433a-9183-501c14acc308",
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "It's just gofmt. I removed the two flannel overlay-specific fields\n",
        "createdAt" : "2016-10-01T07:39:47Z",
        "updatedAt" : "2016-10-04T09:45:43Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e52f84e084ed43806e6bccd4496f016bf479488",
    "line" : null,
    "diffHunk" : "@@ -1,1 +425,429 @@\t\tnodeLabels:                kubeCfg.NodeLabels,\n\t\tnodeStatusUpdateFrequency: kubeCfg.NodeStatusUpdateFrequency.Duration,\n\t\tos:                kubeDeps.OSInterface,\n\t\toomWatcher:        oomWatcher,\n\t\tcgroupsPerQOS:     kubeCfg.CgroupsPerQOS,"
  },
  {
    "id" : "e7ed40f6-6ac2-4fc9-b2ab-e0a8a218217e",
    "prId" : 30731,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19143718-4754-4eca-b041-cb9412ed4386",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : ":+1: perfect.\n",
        "createdAt" : "2016-08-17T14:24:55Z",
        "updatedAt" : "2016-08-17T20:21:46Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "6ad027a9-f9a3-4464-ada4-9884028384fd",
        "parentId" : "19143718-4754-4eca-b041-cb9412ed4386",
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "Great :) Unit test coming up next\n",
        "createdAt" : "2016-08-17T14:26:57Z",
        "updatedAt" : "2016-08-17T20:21:46Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3fe759fec556273a30e059cb73f53ffa9e752a0",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2881,2885 @@\toutput := ioutils.WriteCloserWrapper(&buffer)\n\terr = kl.runner.ExecInContainer(container.ID, cmd, nil, output, output, false, nil)\n\t// Even if err is non-nil, there still may be output (e.g. the exec wrote to stdout or stderr but\n\t// the command returned a nonzero exit code). Therefore, always return the output along with the\n\t// error."
  },
  {
    "id" : "4a395e27-c960-40c3-be9e-3abd00027789",
    "prId" : 30417,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2cceeecd-85ed-4472-9986-a7556e1344ee",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "Thanks, Yu-ju. This change is also needed for my PR #27970.  If your change is merged first, I can rebase my code.\n",
        "createdAt" : "2016-08-11T05:27:58Z",
        "updatedAt" : "2016-08-11T05:27:58Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "c5e498b4-b602-4e57-872c-5435c34db89a",
        "parentId" : "2cceeecd-85ed-4472-9986-a7556e1344ee",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Yes, I think this can be merged first. \n",
        "createdAt" : "2016-08-11T17:13:07Z",
        "updatedAt" : "2016-08-11T17:13:07Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e48221c249adaeb1f8a33524c340d31d07a3751",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2208,2212 @@\t\t// takes place only after kubelet calls the update handler to process\n\t\t// the update to ensure the internal pod cache is up-to-date.\n\t\tkl.sourcesReady.AddSource(u.Source)\n\n\tcase e := <-plegCh:"
  },
  {
    "id" : "6fc8d1ff-2ef5-4ec9-82d3-0a727b310ac2",
    "prId" : 29925,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ffc44102-8ffb-4eb7-9817-b2b68fc151af",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "The logic behind is a bit complex, some comments are appreciated. Maybe explain why we can and should delete container here.\n",
        "createdAt" : "2016-08-03T22:46:49Z",
        "updatedAt" : "2016-08-03T22:56:14Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "5ed5b5eb-0c1a-432b-a343-365f56f86abe",
        "parentId" : "ffc44102-8ffb-4eb7-9817-b2b68fc151af",
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "done\n",
        "createdAt" : "2016-08-03T22:55:23Z",
        "updatedAt" : "2016-08-03T22:56:14Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      }
    ],
    "commit" : "8bc4444f1608bc9dec026e6209ed1f36797c2865",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2923,2927 @@\n// Delete the eligible dead container instances in a pod. Depending on the configuration, the latest dead containers may be kept around.\nfunc (kl *Kubelet) cleanUpContainersInPod(podId types.UID, exitedContainerID string) {\n\tif podStatus, err := kl.podCache.Get(podId); err == nil {\n\t\tif status, ok := kl.statusManager.GetPodStatus(podId); ok {"
  },
  {
    "id" : "f70e4fb4-ffa8-4d6f-a18d-36d7f4b53a99",
    "prId" : 29666,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/29666#pullrequestreview-16985038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "828dae54-7447-4485-8baa-22de4bd7de24",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "We could just do it each time we start a new pod, too, and save the goroutine.  I am ambivalent about it.",
        "createdAt" : "2017-01-07T07:43:01Z",
        "updatedAt" : "2017-01-17T13:20:18Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "c65c2c02-624a-4d46-9064-5303c95bce01",
        "parentId" : "828dae54-7447-4485-8baa-22de4bd7de24",
        "authorId" : "94b8bf8e-9b43-4688-80ab-ca5d0064d3e0",
        "body" : "done: added its call into kl.syncLoopIteration in case of ADD op.",
        "createdAt" : "2017-01-13T15:52:55Z",
        "updatedAt" : "2017-01-17T13:20:18Z",
        "lastEditedBy" : "94b8bf8e-9b43-4688-80ab-ca5d0064d3e0",
        "tags" : [
        ]
      },
      {
        "id" : "0b81f48d-f7c9-4c90-b4fa-04604f213b3e",
        "parentId" : "828dae54-7447-4485-8baa-22de4bd7de24",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "You say \"done\" but it is still forking a goroutine?",
        "createdAt" : "2017-01-17T06:43:08Z",
        "updatedAt" : "2017-01-17T13:20:18Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "389a5699-0aea-49e4-af7d-0671fc70107b",
        "parentId" : "828dae54-7447-4485-8baa-22de4bd7de24",
        "authorId" : "94b8bf8e-9b43-4688-80ab-ca5d0064d3e0",
        "body" : "I thought you wanted to trigger node level event as well on pod's creation, now I see it's not what you meant, so removed added call from  kl.syncLoopIteration in case of ADD op. Re gorouting forking, see comment below.",
        "createdAt" : "2017-01-17T13:02:18Z",
        "updatedAt" : "2017-01-17T13:27:30Z",
        "lastEditedBy" : "94b8bf8e-9b43-4688-80ab-ca5d0064d3e0",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9254397272c238feda03abcc57ff3a58bebc20b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1246,1250 @@\t// Start gorouting responsible for checking limits in resolv.conf\n\tif kl.resolverConfig != \"\" {\n\t\tgo wait.Until(func() { kl.checkLimitsForResolvConf() }, 30*time.Second, wait.NeverStop)\n\t}\n"
  },
  {
    "id" : "441a6575-844c-4c80-a41a-dd6ed8624015",
    "prId" : 29404,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c85b470-cba7-456b-a848-bf7b6ae9e2f2",
        "parentId" : null,
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "Please add comment to indicate that eviction manager has to start after cadvisor.\n",
        "createdAt" : "2016-07-22T01:41:37Z",
        "updatedAt" : "2016-07-22T14:19:51Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      },
      {
        "id" : "28ef24ce-d33c-4331-819d-672a07ae8c15",
        "parentId" : "2c85b470-cba7-456b-a848-bf7b6ae9e2f2",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "done\n",
        "createdAt" : "2016-07-22T14:19:58Z",
        "updatedAt" : "2016-07-22T14:19:58Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e75f2effba11e8e847e92738de36a338071365c",
    "line" : null,
    "diffHunk" : "@@ -1,1 +930,934 @@\t}\n\t// eviction manager must start after cadvisor because it needs to know if the container runtime has a dedicated imagefs\n\tif err := kl.evictionManager.Start(kl, kl.getActivePods, evictionMonitoringPeriod); err != nil {\n\t\tkl.runtimeState.setInternalError(fmt.Errorf(\"failed to start eviction manager %v\", err))\n\t}"
  },
  {
    "id" : "b9db6e19-98b9-4237-8dca-8d0ae0e277d0",
    "prId" : 29272,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bc1c02e-60f3-44fa-9b9d-8ec971c0bf6a",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This is broken - if fit is false, you should always return no matter if we have some reasons or not.\n",
        "createdAt" : "2016-08-05T13:00:22Z",
        "updatedAt" : "2016-08-09T12:03:09Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "2749ec75551fdad53e2951919600f472ca8eade1",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2093,2097 @@\t\t\tmessage := fmt.Sprintf(\"GeneralPredicates failed due to %v, which is unexpected.\", r)\n\t\t\tglog.Warningf(\"Failed to admit pod %v - %s\", format.Pod(pod), message)\n\t\t}\n\t\treturn fit, reason, message\n\t}"
  },
  {
    "id" : "ddf9f4e4-616e-40d0-813c-606ab809795f",
    "prId" : 29240,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b18c5953-2930-4dbb-898c-87dd791d2563",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "What else does this affect?\n",
        "createdAt" : "2016-07-21T00:21:42Z",
        "updatedAt" : "2016-07-21T06:05:50Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "e06a04bf-ca68-4f23-a627-7ab02d1031fc",
        "parentId" : "b18c5953-2930-4dbb-898c-87dd791d2563",
        "authorId" : "a70a34d3-f358-4e41-b264-e8b7bb8ada69",
        "body" : "The volume manager expects a nodeName and uses it to query the API server to verify that the attach detach controller has attached the volume to the node. When the hostname does not matches the nodeName, this lookup fails eventually failing volume mounting.\n",
        "createdAt" : "2016-07-21T04:20:04Z",
        "updatedAt" : "2016-07-21T06:05:50Z",
        "lastEditedBy" : "a70a34d3-f358-4e41-b264-e8b7bb8ada69",
        "tags" : [
        ]
      },
      {
        "id" : "2786ab01-a529-4a55-a267-454cc096c277",
        "parentId" : "b18c5953-2930-4dbb-898c-87dd791d2563",
        "authorId" : "a70a34d3-f358-4e41-b264-e8b7bb8ada69",
        "body" : "I haven't seen any other side affect as the nodeName is only used in reconciler and to build cache of current state (volumes and pods that mount them)\n",
        "createdAt" : "2016-07-21T04:27:03Z",
        "updatedAt" : "2016-07-21T06:05:50Z",
        "lastEditedBy" : "a70a34d3-f358-4e41-b264-e8b7bb8ada69",
        "tags" : [
        ]
      }
    ],
    "commit" : "93fb8c93c9d68e4845dd10c286a5afaec5e9fb7d",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +508,512 @@\tklet.volumeManager, err = volumemanager.NewVolumeManager(\n\t\tenableControllerAttachDetach,\n\t\tnodeName,\n\t\tklet.podManager,\n\t\tklet.kubeClient,"
  },
  {
    "id" : "d9726815-e766-436a-bfaf-4113a38227e4",
    "prId" : 29216,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45f2841e-5e0b-4a32-83ff-1dbd50ddb964",
        "parentId" : null,
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Weird -- looks like you found a bug in GitHub's go highlighting.\n",
        "createdAt" : "2016-08-11T21:23:53Z",
        "updatedAt" : "2016-08-25T17:57:46Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "11ada794-06ba-49b4-8fb4-2775ef1a23a7",
        "parentId" : "45f2841e-5e0b-4a32-83ff-1dbd50ddb964",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Oh, that is quite strange!\n",
        "createdAt" : "2016-08-11T22:26:50Z",
        "updatedAt" : "2016-08-25T17:57:46Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ae1458ab090c9e8fbac186c61021d17ae192dfa",
    "line" : null,
    "diffHunk" : "@@ -1,1 +195,199 @@// at runtime that are necessary for running the Kubelet. This is a temporary solution for grouping\n// these objects while we figure out a more comprehensive dependency injection story for the Kubelet.\ntype KubeletDeps struct {\n\t// TODO(mtaufen): KubeletBuilder:\n\t//                Mesos currently uses this as a hook to let them make their own call to"
  },
  {
    "id" : "afa36a71-e831-478f-b613-0fcff3cf08f8",
    "prId" : 29080,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b00bc283-0f4a-4efd-896f-edd4ca876ac9",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "nit: just return `nil` in line 2090. There is not much value of returning the last `err` here.\n",
        "createdAt" : "2016-07-18T21:25:56Z",
        "updatedAt" : "2016-07-18T21:43:52Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "f833bf22-8343-416a-94be-bd29b98f10e8",
        "parentId" : "b00bc283-0f4a-4efd-896f-edd4ca876ac9",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "ACK.\n",
        "createdAt" : "2016-07-18T21:38:08Z",
        "updatedAt" : "2016-07-18T21:43:52Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "62d2979e1b21ff7a0c5207b78b260dd5fef10099",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +2090,2094 @@\t}\n\n\tkl.backOff.GC()\n\treturn nil\n}"
  },
  {
    "id" : "f700c481-3fc4-4100-85ef-b976c5167517",
    "prId" : 29048,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "870b2125-3b4e-4bf9-85b2-108b289953a0",
        "parentId" : null,
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "Is this where you fix the bug #28643?\n",
        "createdAt" : "2016-07-21T18:45:28Z",
        "updatedAt" : "2016-09-27T14:48:09Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      },
      {
        "id" : "1249dce1-731f-43ff-b3cb-1eb4721aff3e",
        "parentId" : "870b2125-3b4e-4bf9-85b2-108b289953a0",
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "Nevermind.\n",
        "createdAt" : "2016-08-04T18:39:31Z",
        "updatedAt" : "2016-09-27T14:48:09Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "54195d590f03a544d78b4449b2fbafaa258fd6df",
    "line" : null,
    "diffHunk" : "@@ -1,1 +713,717 @@\tklet.volumeManager, err = volumemanager.NewVolumeManager(\n\t\tkubeCfg.EnableControllerAttachDetach,\n\t\tnodeName,\n\t\tklet.podManager,\n\t\tklet.kubeClient,"
  },
  {
    "id" : "1d41201e-113e-4055-9579-f53fd5f5709e",
    "prId" : 29009,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67bd795d-b10c-4e8d-ba6b-e18e1ff2189a",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "please add a comment, something like:\n\nkubenet is the Kubernetes network plugin that wraps CNI's bridge plugin. The bridge plugin knows how to set the hairpin veth flag on interfaces it creates for containers, so we assume kubenet is piping the flag right through, and tell the container runtime to back away from setting it. If the kubelet is started with any other plugin, we can't be sure it handles the hairpin case, so we instruct the docker runtime to set the flag instead. \n",
        "createdAt" : "2016-07-27T21:28:35Z",
        "updatedAt" : "2016-07-28T11:34:54Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "a9fedc97-ae56-42d4-95b1-615cd9f3cb00",
        "parentId" : "67bd795d-b10c-4e8d-ba6b-e18e1ff2189a",
        "authorId" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "body" : "Ugh, could we just rework things to pass the full hairpin mode flag through to the plugins and let them do what they want instead of special casing stuff?\n",
        "createdAt" : "2016-07-30T15:17:13Z",
        "updatedAt" : "2016-07-30T15:17:14Z",
        "lastEditedBy" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "tags" : [
        ]
      },
      {
        "id" : "5b041ac9-c6c4-4901-82e2-e10877480359",
        "parentId" : "67bd795d-b10c-4e8d-ba6b-e18e1ff2189a",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "sure but right now it'll break if we abruptly do that for plugins that don't support the flag, because the docker runtime is handling it for them\n",
        "createdAt" : "2016-07-30T17:37:44Z",
        "updatedAt" : "2016-07-30T17:37:57Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "f21d2dde5a0b7b124407ab504c289c6a36f2f3d7",
    "line" : null,
    "diffHunk" : "@@ -1,1 +435,439 @@\t\t\t// sure it handles the hairpin case so we instruct the docker\n\t\t\t// runtime to set the flag instead.\n\t\t\tklet.hairpinMode == componentconfig.HairpinVeth && networkPluginName != \"kubenet\",\n\t\t\tseccompProfileRoot,\n\t\t\tcontainerRuntimeOptions...,"
  },
  {
    "id" : "4292e461-49d9-4667-b236-58c92ceafebc",
    "prId" : 28570,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/28570#pullrequestreview-10655808",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bd61006-b5fe-4bbf-a29f-ee1ea0809ab1",
        "parentId" : null,
        "authorId" : "ffc157a5-b8d0-4a06-b942-f8e5150e4215",
        "body" : "just curious what the reasoning for moving this was? should there be a unit test behind the move?\n",
        "createdAt" : "2016-07-07T17:52:30Z",
        "updatedAt" : "2016-07-11T02:38:31Z",
        "lastEditedBy" : "ffc157a5-b8d0-4a06-b942-f8e5150e4215",
        "tags" : [
        ]
      },
      {
        "id" : "8251d197-09b1-4fea-a2b2-c5ca46de77f5",
        "parentId" : "1bd61006-b5fe-4bbf-a29f-ee1ea0809ab1",
        "authorId" : "842b8549-e186-4ef6-947e-1168108a13c4",
        "body" : "This modification does not affect the original process flow, just remove an additional loop body, to improve the efficiency of the implementation. And I think that the original unit tests can cover this change.\n",
        "createdAt" : "2016-07-08T02:15:21Z",
        "updatedAt" : "2016-07-11T02:38:31Z",
        "lastEditedBy" : "842b8549-e186-4ef6-947e-1168108a13c4",
        "tags" : [
        ]
      },
      {
        "id" : "d4eccad3-d3a4-48cc-be4c-ddb9b7522103",
        "parentId" : "1bd61006-b5fe-4bbf-a29f-ee1ea0809ab1",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "By setting the pod status first without adding the pod to the podManager, I think there will be a window for race condition where the status gets dropped. ",
        "createdAt" : "2016-11-29T22:16:39Z",
        "updatedAt" : "2016-11-29T22:16:39Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "3306dc0f-a945-45a3-a8cf-291b3b31cfae",
        "parentId" : "1bd61006-b5fe-4bbf-a29f-ee1ea0809ab1",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "@yujuhong Yeah, you are right. We should always add the pod.",
        "createdAt" : "2016-11-29T22:38:42Z",
        "updatedAt" : "2016-11-29T22:38:57Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "09344c1ffc2c362de7dfdce290155209bc55f842",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +2655,2659 @@\t\t\tcontinue\n\t\t}\n\t\tkl.podManager.AddPod(pod)\n\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)\n\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)"
  },
  {
    "id" : "a977f5f2-9940-43ac-a2b8-303aa0eaaa2b",
    "prId" : 28160,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "727b2770-03bb-47d5-b0f3-4080a93a3bb0",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "FWIW, I still think it's better to pass the kl.podCache here because otherwise, the code would be calling `docker ps` 10 times per second while waiting for kubelet to stop the container.\n",
        "createdAt" : "2016-06-28T21:39:36Z",
        "updatedAt" : "2016-06-29T04:39:08Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "0d123951-4e82-40e2-9b95-6f73c4b4b4a4",
        "parentId" : "727b2770-03bb-47d5-b0f3-4080a93a3bb0",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Retract my comment since this would slightly increase the window for race condition to happen. \n",
        "createdAt" : "2016-06-29T01:06:54Z",
        "updatedAt" : "2016-06-29T04:39:08Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "c723d9e5c4074c165a558ad148841b54c8bb0117",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +506,510 @@\t\tklet.kubeClient,\n\t\tklet.volumePluginMgr,\n\t\tklet.containerRuntime)\n\n\truntimeCache, err := kubecontainer.NewRuntimeCache(klet.containerRuntime)"
  },
  {
    "id" : "889640e1-e903-4a1a-859a-ff1bbf7f6529",
    "prId" : 28095,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e8ce488-ca47-4aa3-bf2a-2e93a768d981",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "as a follow up, we can consider a force sync. Something like:\n\n```\n    select {\n     case <- forceSync\n     case <-time.After(10*time.Second)\n     }\n    tryUpdateNodestatus()\n```\n\nAnd then pass the forceSync channel down to the volume manager. Something to consider at least, to cut out the 10s delay. \n",
        "createdAt" : "2016-06-27T22:02:55Z",
        "updatedAt" : "2016-06-28T21:06:20Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "6673f09f-3980-4de2-aa30-a0b7c944e8a9",
        "parentId" : "1e8ce488-ca47-4aa3-bf2a-2e93a768d981",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Yep\n",
        "createdAt" : "2016-06-27T23:41:17Z",
        "updatedAt" : "2016-06-28T21:06:20Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "f05388f7-4884-4869-9b7d-42142c366587",
        "parentId" : "1e8ce488-ca47-4aa3-bf2a-2e93a768d981",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "https://github.com/kubernetes/kubernetes/issues/28141\n",
        "createdAt" : "2016-06-28T02:41:30Z",
        "updatedAt" : "2016-06-28T21:06:20Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "e06b32b1ef4ac5919ea24c2e158f0ab6b3b5fe87",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +3407,3411 @@\tupdatedNode, err := kl.kubeClient.Core().Nodes().UpdateStatus(node)\n\tif err == nil {\n\t\tkl.volumeManager.MarkVolumesAsReportedInUse(\n\t\t\tupdatedNode.Status.VolumesInUse)\n\t}"
  },
  {
    "id" : "15822962-1d8e-40a2-8f64-e4541a4e3748",
    "prId" : 27970,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6086d345-55e9-4875-8f7e-d0f050444ed5",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Please modify `cleanupOrphanedPodDirs()`/`podVolumesExist()` to check disk for volumes as well.\n",
        "createdAt" : "2016-08-05T18:18:06Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "3e1e9a53-0f78-420a-b196-a89138747833",
        "parentId" : "6086d345-55e9-4875-8f7e-d0f050444ed5",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "done\n",
        "createdAt" : "2016-08-12T16:23:11Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "f19a1148db1b7584be6b6b60abaf8c0bd1503ed3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2180,2184 @@\t\t// callback.\n\t\tif !open {\n\t\t\tglog.Errorf(\"Update channel is closed. Exiting the sync loop.\")\n\t\t\treturn false\n\t\t}"
  },
  {
    "id" : "ffe5184f-b3fd-4e3e-85d4-d20163dc9514",
    "prId" : 27970,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0dca2f8e-8da0-41f1-b363-25431e368c2d",
        "parentId" : null,
        "authorId" : "9680f0db-1c00-49c9-968a-be3d5084d153",
        "body" : "@jingxu97 @saad-ali \nIf it is important to move this after the handlers are called do we want to comment why? Can we protect it with code?\n",
        "createdAt" : "2016-08-09T05:40:46Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "9680f0db-1c00-49c9-968a-be3d5084d153",
        "tags" : [
        ]
      },
      {
        "id" : "c044ca9c-3ed6-4d60-82a4-2e9c14a1b6e9",
        "parentId" : "0dca2f8e-8da0-41f1-b363-25431e368c2d",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "The reason for moving Addsource() after updating the podManager is to make sure when checking sourcesReady, the podManager is already updated. The change is needed because if update fails, we should not say sources are already ready. Volume reconstruct process and orphaned pods cleanup depends on this checking to make sure all sources are ready before the clean up. \n",
        "createdAt" : "2016-08-09T20:49:24Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "5f7b1d6b-085e-48a9-ba97-d35f1e734d8d",
        "parentId" : "0dca2f8e-8da0-41f1-b363-25431e368c2d",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "yuju also created a PR #30417 for this change explicitly and she added good comments about it. \n",
        "createdAt" : "2016-08-11T05:30:51Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "f19a1148db1b7584be6b6b60abaf8c0bd1503ed3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2215,2219 @@\t\t// takes place only after kubelet calls the update handler to process\n\t\t// the update to ensure the internal pod cache is up-to-date.\n\t\tkl.sourcesReady.AddSource(u.Source)\n\n\tcase e := <-plegCh:"
  },
  {
    "id" : "ff375ead-21e6-4f5c-a0ac-cac166207787",
    "prId" : 27970,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0702950d-4538-4ae0-a780-26d0d843e36c",
        "parentId" : null,
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "Why not unmount unused mounted volumes here?\nI think that's what #31596 is asking for\n",
        "createdAt" : "2016-08-31T05:14:12Z",
        "updatedAt" : "2016-08-31T05:14:12Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      },
      {
        "id" : "ec79aa8e-9314-4dd5-98a3-867c7d70fea3",
        "parentId" : "0702950d-4538-4ae0-a780-26d0d843e36c",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "@luxas https://github.com/luxas, we do unmount unused mounted volumes,\nthis logic is implemented in pkg/kubelet/volumemanager/reconciler/reconciler.go\nbecause this is the place for all mount/unmount related logic.\n\nPlease let me know if you have any questions about it.\n\nJing\n\nOn Tue, Aug 30, 2016 at 10:15 PM, Lucas Kldstrm notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go\n> https://github.com/kubernetes/kubernetes/pull/27970#discussion_r76925660\n> :\n> \n> > ```\n> >         glog.V(3).Infof(\"Orphaned pod %q found, but volumes are not cleaned up; err: %v\", uid, err)\n> >         continue\n> >     }\n> > ```\n> > -       // Check whether volume is still mounted on disk. If so, do not delete directory\n> > -       if volumeNames, err := kl.getPodVolumeNameListFromDisk(uid); err != nil || len(volumeNames) != 0 {\n> > -           glog.V(3).Infof(\"Orphaned pod %q found, but volumes are still mounted; err: %v, volumes: %v \", uid, err, volumeNames)\n> \n> Why not unmount unused mounted volumes here?\n> I think that's what #31596\n> https://github.com/kubernetes/kubernetes/issues/31596 is asking for\n> \n> \n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/27970/files/f19a1148db1b7584be6b6b60abaf8c0bd1503ed3#r76925660,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ASSNxZ2PPiHykY7mWc1u4e7Uagxqpgsgks5qlQ3WgaJpZM4I9M0t\n> .\n\n## \n- Jing\n",
        "createdAt" : "2016-08-31T05:49:01Z",
        "updatedAt" : "2016-08-31T05:49:01Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "f19a1148db1b7584be6b6b60abaf8c0bd1503ed3",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1742,1746 @@\t\t// Check whether volume is still mounted on disk. If so, do not delete directory\n\t\tif volumeNames, err := kl.getPodVolumeNameListFromDisk(uid); err != nil || len(volumeNames) != 0 {\n\t\t\tglog.V(3).Infof(\"Orphaned pod %q found, but volumes are still mounted; err: %v, volumes: %v \", uid, err, volumeNames)\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "798eac0f-1a8e-4e46-a365-5c0ea1c4f43a",
    "prId" : 27778,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68c23dda-1ef6-44b7-bdc9-95c852c5ed82",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "dropped a newline -- was that intentional?\n",
        "createdAt" : "2016-07-07T18:24:02Z",
        "updatedAt" : "2016-08-17T13:54:17Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "cdf7ae05-877f-4871-9b9e-354b3aa90044",
        "parentId" : "68c23dda-1ef6-44b7-bdc9-95c852c5ed82",
        "authorId" : "e15ef128-90ac-4a14-8795-b5be15e790ce",
        "body" : "nope, my bad\n",
        "createdAt" : "2016-07-07T19:36:06Z",
        "updatedAt" : "2016-08-17T13:54:17Z",
        "lastEditedBy" : "e15ef128-90ac-4a14-8795-b5be15e790ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "782d7d9815c18bcbf1272803c25027ebdad7995e",
    "line" : null,
    "diffHunk" : "@@ -1,1 +526,530 @@\t\tmounter,\n\t\tklet.getPodsDir(),\n\t\trecorder)\n\n\truntimeCache, err := kubecontainer.NewRuntimeCache(klet.containerRuntime)"
  },
  {
    "id" : "6ac1bb47-3ad1-4787-a819-b80ff2e22024",
    "prId" : 27441,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7eda85b2-7adb-406d-950e-92439d69368d",
        "parentId" : null,
        "authorId" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "body" : "Wouldn't this error still in the \"standalone\" case? It would seem that `LastNodeUpdateObject` will still be empty as initialNodeStatus() is called from registerWithAPIServer()\n",
        "createdAt" : "2016-06-15T21:54:12Z",
        "updatedAt" : "2016-06-17T18:18:35Z",
        "lastEditedBy" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "tags" : [
        ]
      },
      {
        "id" : "1c05452a-ea7f-42f6-b86a-dc9e4c7fa4d8",
        "parentId" : "7eda85b2-7adb-406d-950e-92439d69368d",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "It was called in getNode and getNodeAnyway\n\nI don't like initial node status modifying Kubelet state.  I will move setting last node update object into Kubelet Run when setup to not register node based on the value returned from initial node status.\n\nThis change will have to wait until tomorrow. \n",
        "createdAt" : "2016-06-15T22:03:54Z",
        "updatedAt" : "2016-06-17T18:18:35Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "b07deb2e-881f-4921-8ede-2288da764b3b",
        "parentId" : "7eda85b2-7adb-406d-950e-92439d69368d",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "We really should refactor some aspects of standalone mode handling in Kubelet.  It's kind of a hidden scenario in code base.\n",
        "createdAt" : "2016-06-15T22:04:44Z",
        "updatedAt" : "2016-06-17T18:18:35Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "0626222a-2e0a-4f00-ac5a-3807277432e3",
        "parentId" : "7eda85b2-7adb-406d-950e-92439d69368d",
        "authorId" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "body" : "ah, right I see.\n+1 on moving to Run()\n",
        "createdAt" : "2016-06-15T22:11:36Z",
        "updatedAt" : "2016-06-17T18:18:35Z",
        "lastEditedBy" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "tags" : [
        ]
      },
      {
        "id" : "abd0a8c8-8a2d-40f5-997f-65e85428d704",
        "parentId" : "7eda85b2-7adb-406d-950e-92439d69368d",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "commenting for future reviewer clarity, I removed 'lastUpdatedNode` altogether from the orginal version of this PR to keep change set minimal at this point in the release.\n",
        "createdAt" : "2016-06-16T17:52:00Z",
        "updatedAt" : "2016-06-17T18:18:35Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "18a206ad569c90eeb8ba98b3a5280a63348e7873",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1907,1911 @@\n\t// Wait for volumes to attach/mount\n\tdefaultedPod, _, err := kl.defaultPodLimitsForDownwardApi(pod, nil)\n\tif err != nil {\n\t\treturn err"
  },
  {
    "id" : "e582a212-3297-4bb6-986a-faec664aeb8e",
    "prId" : 27349,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55e29910-dbd8-4dbe-9655-4b8372262d37",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "nits: move this in front of `SET`, which is never used anymore. :)\n",
        "createdAt" : "2016-07-02T00:24:37Z",
        "updatedAt" : "2016-07-12T06:08:16Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d5dddcb71e76e8dfd3bcef19ba8dea2e18decb2",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2647,2651 @@\t\t\tglog.V(4).Infof(\"SyncLoop (RECONCILE, %q): %q\", u.Source, format.Pods(u.Pods))\n\t\t\thandler.HandlePodReconcile(u.Pods)\n\t\tcase kubetypes.DELETE:\n\t\t\tglog.V(2).Infof(\"SyncLoop (DELETE, %q): %q\", u.Source, format.Pods(u.Pods))\n\t\t\t// DELETE is treated as a UPDATE because of graceful deletion."
  },
  {
    "id" : "bf5bb875-e03e-435d-9ec8-8589c94a5c73",
    "prId" : 27303,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99e3419b-3bc1-444e-8ca8-7155102d03fe",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "comment\n",
        "createdAt" : "2016-06-13T21:43:07Z",
        "updatedAt" : "2016-06-14T16:49:10Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "3f81558c-3acd-4c15-9849-b19927d1eea4",
        "parentId" : "99e3419b-3bc1-444e-8ca8-7155102d03fe",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "ack\n",
        "createdAt" : "2016-06-14T15:31:09Z",
        "updatedAt" : "2016-06-14T16:49:10Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "712860d55face9f7aec2ab6dfef9c370713e31f3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +834,838 @@\n\t// lastUpdatedNodeObject is a cached version of the node as last reported back to the api server.\n\tlastUpdatedNodeObject atomic.Value\n}\n"
  },
  {
    "id" : "845e3940-a094-4dd6-925f-332b7dc5b45e",
    "prId" : 27209,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d12fb30-0159-4482-8302-42f1c6e74dcd",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Could you also update the outdated comment. Thanks!\n",
        "createdAt" : "2016-06-13T15:26:40Z",
        "updatedAt" : "2016-06-13T18:12:01Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d6cdd5d1ba8e0ef6bf316da85f11e58389f50a7",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +2458,2462 @@}\n\n// handleOutOfDisk detects if pods can't fit due to lack of disk space.\nfunc (kl *Kubelet) isOutOfDisk() bool {\n\t// Check disk space once globally and reject or accept all new pods."
  },
  {
    "id" : "4f527126-f884-404a-b990-5b05f56daed4",
    "prId" : 27042,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afa0df07-a85e-48b4-a434-b99ac12bcc54",
        "parentId" : null,
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "@lukaszo Trying to understand, this covers only the case where the container name is specified in the command line. right? (and not the problems mentioned in #27040)\n",
        "createdAt" : "2016-06-08T16:39:39Z",
        "updatedAt" : "2016-06-13T13:12:49Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      },
      {
        "id" : "604707f6-adc3-47f9-a209-8a4decd079e7",
        "parentId" : "afa0df07-a85e-48b4-a434-b99ac12bcc54",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "This one fixes the issue of pulling init container's log. Not addressed #27040.\n",
        "createdAt" : "2016-06-08T16:53:10Z",
        "updatedAt" : "2016-06-13T13:12:49Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "3121621d-7327-479a-b9d1-aa02faa41727",
        "parentId" : "afa0df07-a85e-48b4-a434-b99ac12bcc54",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "@dims yes, exactly. Working on #27040 without this doesn't make sense\n",
        "createdAt" : "2016-06-08T18:22:35Z",
        "updatedAt" : "2016-06-13T13:12:49Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      },
      {
        "id" : "2cab014d-9ca4-4ac3-827a-93606d370664",
        "parentId" : "afa0df07-a85e-48b4-a434-b99ac12bcc54",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "@dchen1107 what is a init container logs\n",
        "createdAt" : "2016-06-09T03:55:11Z",
        "updatedAt" : "2016-06-13T13:12:49Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "5086cdeb-1a44-484f-8d9c-cb89b941b939",
        "parentId" : "afa0df07-a85e-48b4-a434-b99ac12bcc54",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "@krmayankk see here https://github.com/kubernetes/kubernetes/pull/23666\n",
        "createdAt" : "2016-06-09T08:37:50Z",
        "updatedAt" : "2016-06-13T13:12:49Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "07d13c1fb4eade060945305f370370113ff84e44",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2846,2850 @@\n\tcStatus, found := api.GetContainerStatus(podStatus.ContainerStatuses, containerName)\n\t// if not found, check the init containers\n\tif !found {\n\t\tcStatus, found = api.GetContainerStatus(podStatus.InitContainerStatuses, containerName)"
  },
  {
    "id" : "cdb11353-4ce6-4339-913b-5b33da594ba3",
    "prId" : 26801,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b25698ba-c310-4553-9f0a-dc2561bc9e35",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "That's not all it does, is it?\n",
        "createdAt" : "2016-06-07T17:46:08Z",
        "updatedAt" : "2016-06-15T16:35:11Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "3d87f3bf-a8e5-4dd8-b644-58fe0e6eb148",
        "parentId" : "b25698ba-c310-4553-9f0a-dc2561bc9e35",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "IIRC volume manager is tied into cleaning up orphaned volumes as well.\n",
        "createdAt" : "2016-06-07T17:46:25Z",
        "updatedAt" : "2016-06-15T16:35:11Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "b4d80e7c-2e1f-40f0-be75-8ff6a8972757",
        "parentId" : "b25698ba-c310-4553-9f0a-dc2561bc9e35",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Ah, I guess this comment does reflect that.  Carry on\n",
        "createdAt" : "2016-06-07T17:46:52Z",
        "updatedAt" : "2016-06-15T16:35:11Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfab5362d43adced535b08659a061a8ebd21cc2a",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +680,684 @@\tstatusManager status.Manager\n\n\t// VolumeManager runs a set of asynchronous loops that figure out which\n\t// volumes need to be attached/mounted/unmounted/detached based on the pods\n\t// scheduled on this node and makes it so."
  },
  {
    "id" : "b719e99e-750b-47ca-a3b0-8eb02977f287",
    "prId" : 26801,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44d5d738-306d-4430-93f2-b353a201cb31",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "The comment at the beginning of this method needs to change  now.\n",
        "createdAt" : "2016-06-09T22:19:39Z",
        "updatedAt" : "2016-06-15T16:35:11Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "ed7da3db-b598-4c0d-88f1-07e794082a4e",
        "parentId" : "44d5d738-306d-4430-93f2-b353a201cb31",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Done.\n",
        "createdAt" : "2016-06-09T23:58:34Z",
        "updatedAt" : "2016-06-15T16:35:11Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "78878ef9-b759-4b38-9c3f-e6dd7c17e12a",
        "parentId" : "44d5d738-306d-4430-93f2-b353a201cb31",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Add a comment saying that WaitForAttachAndMount implements a timeout.\n",
        "createdAt" : "2016-06-10T21:25:35Z",
        "updatedAt" : "2016-06-15T16:35:11Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfab5362d43adced535b08659a061a8ebd21cc2a",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +1900,1904 @@\t}\n\n\t// Wait for volumes to attach/mount\n\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {\n\t\tref, errGetRef := api.GetReference(pod)"
  },
  {
    "id" : "e93f2b53-fce0-487c-8d50-43f9a291d04b",
    "prId" : 26351,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bbcd4c2-07a8-4772-9eb7-16278fc1dceb",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "What about static pods?  I thought we had to maintain backward compat for those.\n",
        "createdAt" : "2016-06-01T21:06:59Z",
        "updatedAt" : "2016-06-02T23:47:38Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "aeabd83f-c491-4ea7-9758-f426d90a955d",
        "parentId" : "2bbcd4c2-07a8-4772-9eb7-16278fc1dceb",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Discussed offline. Static pods created by kubelet create \"mirror pod objects\" in the API server, and those should get processed exactly the same way.\n",
        "createdAt" : "2016-06-02T00:24:39Z",
        "updatedAt" : "2016-06-02T23:47:38Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dbe943491d69762068c0ac021e1964d9c14873e",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +830,834 @@\t// enableControllerAttachDetach indicates the Attach/Detach controller\n\t// should manage attachment/detachment of volumes scheduled to this node,\n\t// and disable kubelet from executing any attach/detach operations\n\tenableControllerAttachDetach bool\n}"
  },
  {
    "id" : "4289db0e-9274-4b38-8a1b-9711fb068ee0",
    "prId" : 26351,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe367b89-0924-4348-9d21-cb94d753f9cc",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I think a positive log message would be good here.  You'll want it when you're debugging :)\n",
        "createdAt" : "2016-06-01T21:08:43Z",
        "updatedAt" : "2016-06-02T23:47:38Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "592c857c-1662-48b7-9733-efb400464b6a",
        "parentId" : "fe367b89-0924-4348-9d21-cb94d753f9cc",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Agreed, but this code is getting gutted in a follow up PR (mount/unmount redesign). I'll leave it alone for now.\n",
        "createdAt" : "2016-06-02T00:26:02Z",
        "updatedAt" : "2016-06-02T23:47:38Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dbe943491d69762068c0ac021e1964d9c14873e",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +2155,2159 @@\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tglog.Errorf(\"Could not detach volume %q at %q: %v\", name, volumePath, err)\n\t\t\t\t\t}\n\t\t\t\t}\n"
  },
  {
    "id" : "1a77ca42-ff84-417a-a552-ba2e37af7776",
    "prId" : 25328,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3bee1a8-a326-4597-8052-ee71ee78617c",
        "parentId" : null,
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "@resouer @vishh Are we going to expose this as a kubelet flag?\n",
        "createdAt" : "2016-05-10T19:01:26Z",
        "updatedAt" : "2016-05-19T08:41:59Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "00532056-67ab-40f0-8648-e05157c87865",
        "parentId" : "b3bee1a8-a326-4597-8052-ee71ee78617c",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@yifan-gu Why do you think this should be configurable?\n",
        "createdAt" : "2016-05-10T19:57:48Z",
        "updatedAt" : "2016-05-19T08:41:59Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "107d4159-816d-4946-aea0-08f7a859ec63",
        "parentId" : "b3bee1a8-a326-4597-8052-ee71ee78617c",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "cc @ncdc \n",
        "createdAt" : "2016-05-10T19:57:54Z",
        "updatedAt" : "2016-05-19T08:41:59Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "c6b31731-fa91-4c1a-bbbd-9ebf055099bb",
        "parentId" : "b3bee1a8-a326-4597-8052-ee71ee78617c",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Not in my plan\n",
        "createdAt" : "2016-05-14T02:28:14Z",
        "updatedAt" : "2016-05-19T08:41:59Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "d917ed26380ba6465ea3e2423368c89d0db11f1b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +154,158 @@\n\t// maxImagesInStatus is the number of max images we store in image status.\n\tmaxImagesInNodeStatus = 50\n)\n"
  },
  {
    "id" : "0dae3f6f-81e3-4bca-96cf-210e69303bee",
    "prId" : 25065,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65f57a44-c596-416f-81e6-31b58a82aeac",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : ":+1:\n",
        "createdAt" : "2016-05-07T13:25:50Z",
        "updatedAt" : "2016-05-12T23:17:17Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "6fefb428c1242b76b7c66e2d261d6c4cdfc4804b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1699,1703 @@// If any step if this workflow errors, the error is returned, and is repeated\n// on the next syncPod call.\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error {\n\t// pull out the required options\n\tpod := o.pod"
  },
  {
    "id" : "3ccac922-86d5-498a-8869-feb5d2e0ad09",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cbaddb3-0cfd-4fb8-ba2b-acb3da90d4c2",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "in case someone looks at this PR, I have a PR to just move sources ready\nhttps://github.com/kubernetes/kubernetes/pull/24810\n",
        "createdAt" : "2016-04-28T15:04:17Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "694b40dc-e85c-443c-87db-891f145e09b7",
        "parentId" : "9cbaddb3-0cfd-4fb8-ba2b-acb3da90d4c2",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Yep, I'm aware of it and I'll rebase onto it if it merges first.\n",
        "createdAt" : "2016-04-28T19:19:19Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +814,818 @@}\n\n// allSourcesReady returns whether all seen pod sources are ready.\nfunc (kl *Kubelet) allSourcesReady() bool {\n\t// Make a copy of the sourcesSeen list because it's not thread-safe."
  },
  {
    "id" : "34b6acce-38c1-4e84-9567-c8a30914a2a6",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6649ba77-4ed0-4367-b8fb-e19fcf688294",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "can we add a // TODO to move this into a `/kubelet/util/`\n",
        "createdAt" : "2016-04-28T15:08:25Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "94119e81-dcb6-43d7-9461-dcaddfc1ef1b",
        "parentId" : "6649ba77-4ed0-4367-b8fb-e19fcf688294",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Yeah, i was going to mark things for move, but I figured instead I would just get the godoc in now and do the moves in a follow-up.\n",
        "createdAt" : "2016-04-28T19:21:45Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +934,938 @@}\n\n// dirExists returns true if the path exists and represents a directory.\nfunc dirExists(path string) bool {\n\ts, err := os.Stat(path)"
  },
  {
    "id" : "174930fe-4ea6-46f1-ab1d-ca849cf3f861",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c355520a-cb68-4644-bc77-25b015762a1b",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "is this function even needed anymore?  Did we get rid of the \"old\" stuff?\n",
        "createdAt" : "2016-04-28T15:10:33Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "ab47c0bf-94ca-4d9c-bfcd-1e78440e1b04",
        "parentId" : "c355520a-cb68-4644-bc77-25b015762a1b",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I don't know, and I do not want to add or remove code in this PR if at all possible, just change doc for now -- let's address this in a follow up.\n",
        "createdAt" : "2016-04-28T19:21:10Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : null,
    "diffHunk" : "@@ -1,1 +863,867 @@// getPodDir returns the full path to the per-pod directory for the pod with\n// the given UID.\nfunc (kl *Kubelet) getPodDir(podUID types.UID) string {\n\t// Backwards compat.  The \"old\" stuff should be removed before 1.0\n\t// release.  The thinking here is this:"
  },
  {
    "id" : "4d3403ec-df2a-4e01-a863-7e399101b20f",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6edf4932-8e5c-4f33-a40d-bf3852c1b71a",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "// TODO - move into a utility file?\n",
        "createdAt" : "2016-04-28T15:12:10Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +1329,1333 @@// ensureHostsFile ensures that the given host file has an up-to-date ip, host\n// name, and domain name.\nfunc ensureHostsFile(fileName, hostIP, hostName, hostDomainName string) error {\n\tif _, err := os.Stat(fileName); os.IsExist(err) {\n\t\tglog.V(4).Infof(\"kubernetes-managed etc-hosts file exits. Will not be recreated: %q\", fileName)"
  },
  {
    "id" : "eb84a3f1-103d-4b8d-8bdb-d0c56442c926",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "70eb0f75-9889-48ae-acb0-d81ba5505476",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I generally dislike numbers, as the worklflow will grow and change, and the numbers will not\n",
        "createdAt" : "2016-04-28T15:21:33Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "c068349e-9811-4ee0-bac1-e0ae0a454729",
        "parentId" : "70eb0f75-9889-48ae-acb0-d81ba5505476",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I like numbers for things with a bunch of steps, or that are especially important.  IMO it is important to get the behaviors of an important method like this into the comment.  I want to give as much information as possible to reason about qualitatively without having to dive into the kube.  I am cool with replacing the numbers with bullets if people really don't like it, but I have to call out this type of criticism as being equally applicable to any godoc -- if you change the behavior of a function significantly odds are you will be updating the doc for the func.\n",
        "createdAt" : "2016-04-28T19:27:21Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1771,1775 @@// updateType - the type of update (ADD, UPDATE, REMOVE, RECONCILE)\n//\n// The workflow is:\n// * If the pod is being created, record pod worker start latency\n// * Call generateAPIPodStatus to prepare an api.PodStatus for the pod"
  },
  {
    "id" : "234e35c6-2dd1-478c-902b-e8eae05b2067",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d3d79e6-ebba-471e-af8f-37b40f2b43c5",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "maybe add a // TODO move to /kubelet/util/pods/pods.go ?\n",
        "createdAt" : "2016-04-28T15:25:04Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 312,
    "diffHunk" : "@@ -1,1 +1909,1913 @@}\n\n// returns whether the pod uses the host network namespace.\nfunc podUsesHostNetwork(pod *api.Pod) bool {\n\treturn pod.Spec.SecurityContext != nil && pod.Spec.SecurityContext.HostNetwork"
  },
  {
    "id" : "5b8fb2cb-52f3-438c-9563-b7c0f682fef5",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a12daef-c375-45f4-9fea-0e68c93307eb",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "// TODO move this into a utility /kubelet/util/sort  ?\n",
        "createdAt" : "2016-04-28T15:27:51Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 386,
    "diffHunk" : "@@ -1,1 +2349,2353 @@\n// podsByCreationTime makes an array of pods sortable by their creation\n// timestamps.\n// TODO: move into util package\ntype podsByCreationTime []*api.Pod"
  },
  {
    "id" : "8d0ca34e-f986-4b11-b08a-ed8c0fe167f9",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5c44ca9-2262-4c3d-b571-5f5df1e04cf8",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "maybe note that this channel wakes up pod workers?\n",
        "createdAt" : "2016-04-28T15:28:53Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 411,
    "diffHunk" : "@@ -1,1 +2510,2514 @@// 1.  configCh:       a channel to read config events from\n// 2.  handler:        the SyncHandler to dispatch pods to\n// 3.  syncCh:         a channel to read periodic sync events from\n// 4.  houseKeepingCh: a channel to read housekeeping events from\n// 5.  plegCh:         a channel to read PLEG updates from"
  },
  {
    "id" : "fb1d533d-7a2d-44d3-93f3-2943615c34be",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0899cd3f-47bd-4d30-aeb6-7d5795ec9d36",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I think this is needed to ensure that kubelet satisfies `HostInterface` ... we dont see to have a check of that actually declared in the code, i.e. var _ HostInterface = &kubelet{}\n",
        "createdAt" : "2016-04-28T15:36:02Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "649fb4af-cd87-49ec-bcca-372333094354",
        "parentId" : "0899cd3f-47bd-4d30-aeb6-7d5795ec9d36",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Yeah, I was going to go back and add type assertions for interfaces near the methods that implement them, maybe in a follow-up.\n",
        "createdAt" : "2016-04-28T15:41:29Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 554,
    "diffHunk" : "@@ -1,1 +2752,2756 @@}\n\n// PLEGHealthCheck returns whether the PLEG is healty.\nfunc (kl *Kubelet) PLEGHealthCheck() (bool, error) {\n\treturn kl.pleg.Healthy()"
  },
  {
    "id" : "b8ee2d4d-7702-42ef-a9f1-ce168babc7a2",
    "prId" : 24911,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a18c086-61cd-497e-932a-b05bd12dbefc",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "\"update the container runtime uptime in the kubelet runtimeState\" is a legit tongue twister!\n",
        "createdAt" : "2016-04-28T15:36:57Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "b939e2de-e62d-40fe-80ec-caa9e5cc1079",
        "parentId" : "0a18c086-61cd-497e-932a-b05bd12dbefc",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I was laughing as I wrote it, but that's what it does!\n",
        "createdAt" : "2016-04-28T20:18:34Z",
        "updatedAt" : "2016-04-28T21:03:40Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 574,
    "diffHunk" : "@@ -1,1 +2894,2898 @@// the runtime dependent modules when the container runtime first comes up,\n// and returns an error if the status check fails.  If the status check is OK,\n// update the container runtime uptime in the kubelet runtimeState.\nfunc (kl *Kubelet) updateRuntimeUp() {\n\tif err := kl.containerRuntime.Status(); err != nil {"
  },
  {
    "id" : "84359b24-e354-4145-9c84-2385c42aa1fb",
    "prId" : 24911,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/24911#pullrequestreview-71752280",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aed5840f-293d-40be-ba94-7a90893c921e",
        "parentId" : null,
        "authorId" : "9cd12d42-ba38-4d33-b922-51e6f29f48e7",
        "body" : "@pmorie do you happen to remember what you meant to write here?",
        "createdAt" : "2017-10-25T06:27:22Z",
        "updatedAt" : "2017-10-25T06:27:23Z",
        "lastEditedBy" : "9cd12d42-ba38-4d33-b922-51e6f29f48e7",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9f0e8c610fe496e4015561740eb3d4a63781a53",
    "line" : 488,
    "diffHunk" : "@@ -1,1 +2627,2631 @@\n// dispatchWork starts the asynchronous sync of the pod in a pod worker.\n// If the pod is terminated, dispatchWork\nfunc (kl *Kubelet) dispatchWork(pod *api.Pod, syncType kubetypes.SyncPodType, mirrorPod *api.Pod, start time.Time) {\n\tif kl.podIsTerminated(pod) {"
  },
  {
    "id" : "43be9836-c937-447b-9191-15b0e4b6fe36",
    "prId" : 24817,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85dd85d4-f6ab-4bf4-8fd3-0453eab5bda2",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The existing code returns here? Did you change this intentionally?\n",
        "createdAt" : "2016-04-26T21:43:32Z",
        "updatedAt" : "2016-04-27T17:20:57Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "356d6d07-c9fe-4b86-b3fb-4cbb1f67affe",
        "parentId" : "85dd85d4-f6ab-4bf4-8fd3-0453eab5bda2",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I did change it intentionally, and was going to add a note that it seems arbitrary (and undesirable) that we should return here.  Returning an error here means we won't clean up other orphaned dirs if there's a problem with one.  We could also just accumulate errors and return a single error that wraps them if there's a problem at the end of the list.\n\nTLDR: I think we should attempt to clean up every directory here instead of short circuiting for this one particular error condition.\n",
        "createdAt" : "2016-04-27T06:55:51Z",
        "updatedAt" : "2016-04-27T17:20:57Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "edb75975-530e-4511-a1db-b268ce381f01",
        "parentId" : "85dd85d4-f6ab-4bf4-8fd3-0453eab5bda2",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "This sounds reasonable. I was just asking because the rest of PR involves only logging changes :)\nWe should make all other cleanup functions to have the same behavior. \n",
        "createdAt" : "2016-04-27T17:09:53Z",
        "updatedAt" : "2016-04-27T17:20:57Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "11113a00d79e6071df3292efafee300cd26d7cae",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2000,2004 @@\t\t\trefs, err := mount.GetMountRefs(kl.mounter, volumePath)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Could not get mount path references for %q: %v\", volumePath, err)\n\t\t\t}\n\t\t\t//TODO (jonesdl) This should not block other kubelet synchronization procedures"
  },
  {
    "id" : "a22bd42f-842e-48aa-b3a2-21076f1c2317",
    "prId" : 24705,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "163a803e-733c-4b7a-9297-e71aebbf8401",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Question: Who is expected to make pod admission decisions? Should it be pod workers or should be done by kubelet object itself, before spinning up a pod worker?\n",
        "createdAt" : "2016-05-05T15:43:07Z",
        "updatedAt" : "2016-07-07T17:44:09Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "c162fec94da63b3539d5884614d02fc6d389262a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +835,839 @@\tsetNodeStatusFuncs []func(*api.Node) error\n\n\t// TODO: think about moving this to be centralized in PodWorkers in follow-on.\n\t// the list of handlers to call during pod admission.\n\tlifecycle.PodAdmitHandlers"
  },
  {
    "id" : "8e3fff48-f757-40ac-b3b4-23a1a73b963f",
    "prId" : 24705,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7139ab21-4eda-467c-b6cf-32056c11a729",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "It is unfortunate that `syncloop` for a pod is part of kubelet :( Since you are mentioning TODOs for the future, it will be useful to move all the pod sync logic into the pod workers.\n",
        "createdAt" : "2016-05-05T15:44:07Z",
        "updatedAt" : "2016-07-07T17:44:09Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "c162fec94da63b3539d5884614d02fc6d389262a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +837,841 @@\t// TODO: think about moving this to be centralized in PodWorkers in follow-on.\n\t// the list of handlers to call during pod admission.\n\tlifecycle.PodAdmitHandlers\n\n\t// the list of handlers to call during pod sync loop."
  },
  {
    "id" : "29743dcf-2a96-4840-a74f-b8c5d448b9ed",
    "prId" : 24557,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83149a00-e817-4d0c-8ddd-1e29a0f8b52a",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "`WaitForDetach` should not be called here--because it blocks the sync loop. Since it is not actually implemented by any plugin yet, you could just put a comment here.\n",
        "createdAt" : "2016-04-30T00:02:03Z",
        "updatedAt" : "2016-05-04T14:35:01Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "71e7dba84525761db1436cf2c7daaffb2a730d27",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2130,2134 @@\t\t\t\t// TODO(swagiaal): This will block until the sync loop until device is attached\n\t\t\t\t// so all of this should be moved to a mount/unmount manager which does it asynchronously\n\t\t\t\tif err = detacher.WaitForDetach(devicePath, maxWaitForVolumeOps); err != nil {\n\t\t\t\t\tglog.Errorf(\"Error while waiting for detach: %v\", err)\n\t\t\t\t}"
  },
  {
    "id" : "b998e68e-f8d3-4529-9609-aa1e548e291d",
    "prId" : 24459,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc161e5e-d7bd-4f0a-b018-217c2e399589",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Questions (without reading the entire PR): Should `PodScheduled` condition be set even if a pod doesn't go through the scheduler? E.g., (1) user writes the \"Node\" field in the pod spec directly, and (2) pods from non-apiserver sources. The condition doesn't mean much at all in these cases. Why can't kubelet simply copy the existing condition if there is one?\n",
        "createdAt" : "2016-05-11T18:00:45Z",
        "updatedAt" : "2016-05-12T08:21:28Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "a80b1798c45f88e6b38261a1657c0dfa787de136",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3289,3293 @@\t// s (the PodStatus we are creating) will not have a PodScheduled condition yet, because converStatusToAPIStatus()\n\t// does not create one. If the existing PodStatus has a PodScheduled condition, then copy it into s and make sure\n\t// it is set to true. If the existing PodStatus does not have a PodScheduled condition, then create one that is set to true.\n\tif _, oldPodScheduled := api.GetPodCondition(&pod.Status, api.PodScheduled); oldPodScheduled != nil {\n\t\ts.Conditions = append(s.Conditions, *oldPodScheduled)"
  },
  {
    "id" : "0e1fb9c6-cdc3-49d1-b180-7fcc7ffc4547",
    "prId" : 24362,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "461db7e7-ced2-4cee-a65a-aee78ac12fc1",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "else log an error ?  Should this proceed or abort the pod run?\n",
        "createdAt" : "2016-04-18T07:04:57Z",
        "updatedAt" : "2016-04-28T17:57:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a3ed48808185ec410a7450796da790790e1d3bd",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1360,1364 @@\t\t} else {\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Pod Hostname %q is not a valid DNS label.\", pod.Spec.Hostname)\n\t\t}\n\t} else {\n\t\thostnameCandidate := podAnnotations[utilpod.PodHostnameAnnotation]"
  },
  {
    "id" : "037aa5e0-f5e3-4388-8b29-9dcd89210420",
    "prId" : 24362,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4947bb7d-7db4-4f42-9ce8-9a24a3447af3",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "same - handle errors\n",
        "createdAt" : "2016-04-18T07:05:28Z",
        "updatedAt" : "2016-04-28T17:57:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a3ed48808185ec410a7450796da790790e1d3bd",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1379,1383 @@\t\t} else {\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Pod Subdomain %q is not a valid DNS label.\", pod.Spec.Subdomain)\n\t\t}\n\t} else {\n\t\tsubdomainCandidate := pod.Annotations[utilpod.PodSubdomainAnnotation]"
  },
  {
    "id" : "c78ae45b-2154-4e58-a68c-63a1964c84ca",
    "prId" : 24344,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b985fa3-109c-45f0-ad30-dcbfdde2dcba",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Another potential issue (without knowing how pod eviction is implemented) is that the pod workers still don't know that they need to kill the pod. Ignore me if this is not the right PR to discuss this.\n",
        "createdAt" : "2016-04-15T22:17:46Z",
        "updatedAt" : "2016-04-29T14:16:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "b045811e-ea57-416b-9b3b-741ff6ef717f",
        "parentId" : "9b985fa3-109c-45f0-ad30-dcbfdde2dcba",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "For this PR, I think we should look at this as a pattern for active deadline being modularized.  I think how low resource does eviction can be discussed in that PR as latency there is important.\n",
        "createdAt" : "2016-04-28T16:00:15Z",
        "updatedAt" : "2016-04-29T14:16:12Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "033ae3e37ec0c5092fae957590e4f524bdf98ad2",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +3505,3509 @@\t\t\t}\n\t\t}\n\t}\n\n\t// TODO: Consider include the container information."
  },
  {
    "id" : "6520aa02-ad7d-4883-aba8-466b8848ff92",
    "prId" : 23711,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "833d2d9a-f474-46b0-bdb2-468d83f9451f",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "container runtime is not going to be docker in the rkt case. We punted on doing this for rkt because we didn't know how to, but now it looks like they're just using kubenet. rkt still doesn't support HairpinVeth (@yifan-gu) so we still want to go to \"hairpin none\" for that, but now rkt is using kubenet that _does_ support promiscuous, so I think we want the check below moved up into the first part of the if statement (unless you want to teach kubenet about hairpin veth too: https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/dockertools/manager.go#L1895, then it would be around the entire if).\n",
        "createdAt" : "2016-04-02T01:08:10Z",
        "updatedAt" : "2016-05-12T15:02:48Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "6d432ef4-0930-4e2a-b36f-ea8fdde18a46",
        "parentId" : "833d2d9a-f474-46b0-bdb2-468d83f9451f",
        "authorId" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "body" : "@bprashanth actually I'd like to wait until https://github.com/appc/cni/pull/175 lands and then just pass the request through to CNI.  Then we add a new plugin capability that kubenet advertises, that causes the docker Manager to skip hairpin.\n",
        "createdAt" : "2016-04-15T14:34:33Z",
        "updatedAt" : "2016-05-12T15:02:48Z",
        "lastEditedBy" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "tags" : [
        ]
      }
    ],
    "commit" : "b749902b42e4160ddb9bf851c6e9988b6fb03c9c",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +514,518 @@\t\tif containerRuntime != \"docker\" {\n\t\t\tglog.Warningf(\"Hairpin mode set to %q but container runtime is %q, ignoring\", hairpinMode, containerRuntime)\n\t\t\treturn componentconfig.HairpinNone, nil\n\t\t}\n\t\tif hairpinMode == componentconfig.PromiscuousBridge && !configureCBR0 && networkPlugin != \"kubenet\" {"
  },
  {
    "id" : "18d33d34-62ef-4a58-b5d8-725ece459c6a",
    "prId" : 23567,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44b5bb28-21c2-4b30-abc8-38a52acc7dd1",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "How come we run into this state?\n",
        "createdAt" : "2016-05-11T20:14:40Z",
        "updatedAt" : "2016-05-17T04:30:20Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "c37813f6-6274-492c-a430-914f1fb1dd0f",
        "parentId" : "44b5bb28-21c2-4b30-abc8-38a52acc7dd1",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I don't have a concrete scenario where this would occur, but I was also concerned that if the status manager had out of date info we would still be in PodPending.\n",
        "createdAt" : "2016-05-11T20:26:54Z",
        "updatedAt" : "2016-05-17T04:30:20Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a53330700ac39ee61109c748fe665cf38581a5d",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +3338,3342 @@\t\t\tif containerStatus.LastTerminationState.Terminated != nil {\n\t\t\t\tif containerStatus.LastTerminationState.Terminated.ExitCode == 0 {\n\t\t\t\t\tinitialized++\n\t\t\t\t} else {\n\t\t\t\t\tfailedInitialization++"
  },
  {
    "id" : "b0c9182a-d4d9-451b-937e-e045be87e05b",
    "prId" : 22827,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79156895-c443-4e72-8ce4-237bac7be204",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "nit: can you just return this? \n",
        "createdAt" : "2016-03-24T19:44:19Z",
        "updatedAt" : "2016-03-30T16:52:56Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "042a21be-e9f6-473d-82fa-bcf1b23e65e7",
        "parentId" : "79156895-c443-4e72-8ce4-237bac7be204",
        "authorId" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "body" : "Not sure what you mean here...\n",
        "createdAt" : "2016-03-24T20:18:36Z",
        "updatedAt" : "2016-03-30T16:52:56Z",
        "lastEditedBy" : "57f729dd-988a-4d1a-83bf-ee70bf637c64",
        "tags" : [
        ]
      },
      {
        "id" : "36f42215-006a-4231-a3b3-bbf0c263e04e",
        "parentId" : "79156895-c443-4e72-8ce4-237bac7be204",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "return kl.networkPlugin != nil && kl.networkPlugin.Capabilities().Has(network.NET_PLUGIN_CAPABILITY_SHAPING)\n\nAlso why return false if the network plugin has shaping enabled? \n",
        "createdAt" : "2016-03-24T20:21:23Z",
        "updatedAt" : "2016-03-30T16:52:56Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "d43553d2-a874-46b9-9e33-e9a2ca20ce4b",
        "parentId" : "79156895-c443-4e72-8ce4-237bac7be204",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "oh, it just read weird, this is answering the question is shaping enabled inline for the kubelet. Please add comment saying that the plugin will handle in, for hasty people like myself.\n",
        "createdAt" : "2016-03-24T20:23:58Z",
        "updatedAt" : "2016-03-30T16:52:56Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb97b8cdaa9ac073384a8e2b5258f40e63e5be64",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +3604,3608 @@func (kl *Kubelet) shapingEnabled() bool {\n\t// Disable shaping if a network plugin is defined and supports shaping\n\tif kl.networkPlugin != nil && kl.networkPlugin.Capabilities().Has(network.NET_PLUGIN_CAPABILITY_SHAPING) {\n\t\treturn false\n\t}"
  },
  {
    "id" : "307e0f42-340e-4611-b444-18374c3da6b8",
    "prId" : 22666,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ebdf3d96-426d-4d32-bcc8-9b747502a68c",
        "parentId" : null,
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "So if podIP is empty, then it returns an empty environment for PodIP?\n",
        "createdAt" : "2016-03-10T23:35:48Z",
        "updatedAt" : "2016-03-10T23:35:48Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "247a415a-3cc4-4511-bd8d-4237249e6d54",
        "parentId" : "ebdf3d96-426d-4d32-bcc8-9b747502a68c",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "It does, would you prefer to error out?\n",
        "createdAt" : "2016-03-11T13:30:04Z",
        "updatedAt" : "2016-03-11T13:30:04Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "5194c12d9e602d25f5a36649670718d46daa1f7b",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +1456,1460 @@\n// Make the service environment variables for a pod in the given namespace.\nfunc (kl *Kubelet) makeEnvironmentVariables(pod *api.Pod, container *api.Container, podIP string) ([]kubecontainer.EnvVar, error) {\n\tvar result []kubecontainer.EnvVar\n\t// Note:  These are added to the docker.Config, but are not included in the checksum computed"
  },
  {
    "id" : "41fbbcd8-c2d0-4fba-a1fe-30f74718c89a",
    "prId" : 21373,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f719515-0389-4a0a-80f8-fc09505596cd",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "My lack of cadvisor knowledge shows here.  Is this guaranteed to have a value?  Setting it to `:////` seems less than ideal.\n",
        "createdAt" : "2016-02-17T13:55:54Z",
        "updatedAt" : "2016-05-22T15:42:06Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "d49328c9-bb74-4efd-ac7e-9529a6ea2a79",
        "parentId" : "1f719515-0389-4a0a-80f8-fc09505596cd",
        "authorId" : "3bade44d-906a-43e3-9e25-ddff73117278",
        "body" : "Sorry, I probably should have documented it. \nIf info.CloudProvider will always have a non empty value (it has a set of defined values in info/v1/machine.go. If it is not UnkownProvider (typo in code, will fix here) nor Baremetal then it is the name of a real cloud service. Same thing goes for info.InstanceID , it will be receive its value from a function that is supposed to return \"Unknown\" if there was an error while retrieving the value.\n\":////\" is to conform with kubernetes's cloudproviders (pkg/cloudprovider/providers/*) that will write to ProviderID if cloud-provider flag is given to kubelet. The most verbose one write in the form of \"cloudprovider://project/availability_zone/instance_name\". Here I dont have project name or availability zone so I return them empty.\n",
        "createdAt" : "2016-02-17T15:25:43Z",
        "updatedAt" : "2016-05-22T15:42:06Z",
        "lastEditedBy" : "3bade44d-906a-43e3-9e25-ddff73117278",
        "tags" : [
        ]
      },
      {
        "id" : "d5158824-1f82-4bcf-8774-1b1ff5ff13aa",
        "parentId" : "1f719515-0389-4a0a-80f8-fc09505596cd",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@enoodle Can you add some comments explaining this?\n",
        "createdAt" : "2016-05-19T19:09:04Z",
        "updatedAt" : "2016-05-22T15:42:06Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "7fb82d543f7a7514cb54827a4824de8e21c1aafc",
    "line" : null,
    "diffHunk" : "@@ -1,1 +3025,3029 @@\t\t// and availability zone empty for compatibility.\n\t\tnode.Spec.ProviderID = strings.ToLower(string(info.CloudProvider)) +\n\t\t\t\":////\" + string(info.InstanceID)\n\t}\n}"
  },
  {
    "id" : "88d49e4c-5b1b-418d-8de2-44c3d676a17e",
    "prId" : 21373,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a01e3a29-312c-4e65-bd74-f250f89dd642",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "FYI @roberthbailey: cAdvisor is already using the local metadata server to access GCE specific information. Relevant logic [here](https://github.com/google/cadvisor/blob/eab201fdf51c033299c8f4b43c13aec5d565d14c/utils/cloudinfo/gce.go).\n",
        "createdAt" : "2016-03-03T18:34:37Z",
        "updatedAt" : "2016-05-22T15:42:06Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "7fb82d543f7a7514cb54827a4824de8e21c1aafc",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1066,1070 @@\t\tnode.Spec.ExternalID = kl.hostname\n\t\t// If no cloud provider is defined - use the one detected by cadvisor\n\t\tinfo, err := kl.GetCachedMachineInfo()\n\t\tif err == nil {\n\t\t\tkl.updateCloudProviderFromMachineInfo(node, info)"
  },
  {
    "id" : "da081264-1f3b-42f8-aba4-29ba7c3f2abc",
    "prId" : 21274,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d9c4a40-39da-456b-a7f4-780b6e24c015",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "How do we keep this in sync with cAdvisor housekeeping interval...?\n",
        "createdAt" : "2016-05-13T22:10:23Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "0972cfd8-199e-4459-bb36-674567c5d6a3",
        "parentId" : "9d9c4a40-39da-456b-a7f4-780b6e24c015",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Do we expose that flag?  I would like to keep it common but meant to leave a todo here for doing just that.  \n",
        "createdAt" : "2016-05-13T23:09:38Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "fae7d9a0-d0d9-4a4d-a881-1662307ea074",
        "parentId" : "9d9c4a40-39da-456b-a7f4-780b6e24c015",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "added a TODO\n",
        "createdAt" : "2016-05-14T15:25:38Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "edc76f6d4fd40749aa5fc4545007043086790a75",
    "line" : null,
    "diffHunk" : "@@ -1,1 +120,124 @@\t// Period for performing eviction monitoring.\n\t// TODO ensure this is in sync with internal cadvisor housekeeping.\n\tevictionMonitoringPeriod = time.Second * 10\n\n\t// The path in containers' filesystems where the hosts file is mounted."
  },
  {
    "id" : "9b5becbe-b671-461a-87fd-c0514d9d738c",
    "prId" : 21274,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad9737d9-dcd9-4050-b79a-c75db6b3a3b6",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Would it make sense to return the conditions to be added from eviction manager?\n",
        "createdAt" : "2016-05-13T22:39:35Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "02fb3329-433c-4558-b1cc-091774640536",
        "parentId" : "ad9737d9-dcd9-4050-b79a-c75db6b3a3b6",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I am not in love with node conditions are managed by the kubelet today.  Are you fine deferring this until we can look to consolidate node conditions later?  This at least keeps it in-line with the current practice.\n",
        "createdAt" : "2016-05-14T15:27:19Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "25cd27e9-7ab0-4169-9e0a-4e08f0fb0238",
        "parentId" : "ad9737d9-dcd9-4050-b79a-c75db6b3a3b6",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "SGTM Add a TODO or better file an issue :)\n",
        "createdAt" : "2016-05-17T00:18:44Z",
        "updatedAt" : "2016-05-17T00:18:44Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "edc76f6d4fd40749aa5fc4545007043086790a75",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +3205,3209 @@\t// condition.Status != api.ConditionFalse in the conditions below depending on whether\n\t// the kubelet is under memory pressure or not.\n\tif kl.evictionManager.IsUnderMemoryPressure() {\n\t\tif condition.Status != api.ConditionTrue {\n\t\t\tcondition.Status = api.ConditionTrue"
  },
  {
    "id" : "b95e3c64-f05d-4018-9cae-27dfe640bc4a",
    "prId" : 21224,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eee8f4d8-d464-4d27-baf9-79f38d2ec3cc",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "What's the reason of counting the containers? \n",
        "createdAt" : "2016-04-12T22:26:24Z",
        "updatedAt" : "2016-04-12T23:37:37Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "6d74f825-5a43-4aaf-8147-cd0230d0b69a",
        "parentId" : "eee8f4d8-d464-4d27-baf9-79f38d2ec3cc",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Because we've already set the default status in the statuses, we can't rely on `_, found = statuses` + `containerDone` now.\n",
        "createdAt" : "2016-04-12T22:28:44Z",
        "updatedAt" : "2016-04-12T23:37:37Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "41953bae49d15d356f03a53afaac49c9538238e9",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +3374,3378 @@\n\t// Set container statuses according to the statuses seen in pod status\n\tcontainerSeen := map[string]int{}\n\tfor _, cStatus := range podStatus.ContainerStatuses {\n\t\tcName := cStatus.Name"
  },
  {
    "id" : "0f489770-30f7-427b-8c15-ffa87ba376ff",
    "prId" : 21224,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7cf690c8-53aa-4300-9f45-02c70b08b322",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Is the original state \"Terminated\"?\n",
        "createdAt" : "2016-04-12T22:28:23Z",
        "updatedAt" : "2016-04-12T23:37:37Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "25771624-8777-43fd-a5ea-9bd0c59aeac0",
        "parentId" : "7cf690c8-53aa-4300-9f45-02c70b08b322",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "If we don't make any assumption of the implementation of `ShouldContainerBeRestarted`, it could also be `Waiting` or even `Running`.\nBut considering the current implementation of `ShouldContainerBeRestarted`, it could be `Waiting`(There is no associated historical container and start failure record) or `Terminated`(The container is terminated).\n",
        "createdAt" : "2016-04-12T22:31:53Z",
        "updatedAt" : "2016-04-12T23:37:37Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "1c55b29e-5460-4ba1-8780-19f491963407",
        "parentId" : "7cf690c8-53aa-4300-9f45-02c70b08b322",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The comment seems to suggest that the original state is a \"non-waiting\" state. Could you add the potential states to the comment? That'd help people understand the code.\n",
        "createdAt" : "2016-04-12T22:42:56Z",
        "updatedAt" : "2016-04-12T23:37:37Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "53b1f3ef-8bee-4795-bb7b-a40a7de59ea7",
        "parentId" : "7cf690c8-53aa-4300-9f45-02c70b08b322",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Sure! :) Will do that.\n",
        "createdAt" : "2016-04-12T22:44:11Z",
        "updatedAt" : "2016-04-12T23:37:37Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "d260f728-2f17-4cdb-a15d-5e8b6018d9c7",
        "parentId" : "7cf690c8-53aa-4300-9f45-02c70b08b322",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Done.\n",
        "createdAt" : "2016-04-12T23:37:43Z",
        "updatedAt" : "2016-04-12T23:37:43Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "41953bae49d15d356f03a53afaac49c9538238e9",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +3403,3407 @@\t\tif !ok {\n\t\t\t// In fact, we could also apply Waiting state here, but it is less informative,\n\t\t\t// and the container will be restarted soon, so we prefer the original state here.\n\t\t\t// Note that with the current implementation of ShouldContainerBeRestarted the original state here\n\t\t\t// could be:"
  },
  {
    "id" : "23bef0d6-4909-4a1e-81c0-ddc9747d7031",
    "prId" : 20688,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8703e945-1ff9-4174-91d5-7c2bbb02b6bc",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "is there a test that covers this function?\n",
        "createdAt" : "2016-02-24T18:59:04Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "3d3d211f-da9d-49b1-8995-896e2f1134fe",
        "parentId" : "8703e945-1ff9-4174-91d5-7c2bbb02b6bc",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "now there is.\n",
        "createdAt" : "2016-03-01T17:21:01Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      },
      {
        "id" : "c5a6753e-7fa4-41c4-94c9-302b28d80543",
        "parentId" : "8703e945-1ff9-4174-91d5-7c2bbb02b6bc",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "where?\n",
        "createdAt" : "2016-03-02T01:47:34Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "5b0fd7ef-07d1-47b6-8449-e6168840bd26",
        "parentId" : "8703e945-1ff9-4174-91d5-7c2bbb02b6bc",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "here: https://github.com/kubernetes/kubernetes/pull/20688/files#diff-3d761026fe20ec73f4c03d8b0a32d351R132\n\nlet me know if you had something else in mind.\n",
        "createdAt" : "2016-03-03T00:18:46Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3c00aadd5da91288cca856dabbefbc9f261be69",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +1287,1291 @@\tbuffer.WriteString(\"fe00::1\\tip6-allnodes\\n\")\n\tbuffer.WriteString(\"fe00::2\\tip6-allrouters\\n\")\n\tif len(hostDomainName) > 0 {\n\t\tbuffer.WriteString(fmt.Sprintf(\"%s\\t%s.%s\\t%s\\n\", hostIP, hostName, hostDomainName, hostName))\n\t} else {"
  },
  {
    "id" : "a96fcea0-cf33-40df-b3ec-202a87890ab6",
    "prId" : 20688,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02676d9d-decd-4486-875d-755a9534aeae",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Should make sure the segments are valid DNS segments when pulling from annotations. Someone could set those on a pre-1.2 cluster without validation. \n",
        "createdAt" : "2016-03-01T01:21:00Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "090ed7e1-cb0b-41f7-921f-bfed06572207",
        "parentId" : "02676d9d-decd-4486-875d-755a9534aeae",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "I wouldnt worry about it. The annotation key is quite specific and meant to be reserved/understood by kubernetes system.\n",
        "createdAt" : "2016-03-01T17:21:57Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      },
      {
        "id" : "10261fc2-730a-44e8-9deb-0cc98819e010",
        "parentId" : "02676d9d-decd-4486-875d-755a9534aeae",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "> I wouldnt worry about it\n\nI don't think anyone would set it accidentally. If current releases allow setting this annotation without validation, this needs to make sure the data is correct before we use it to create DNS records\n",
        "createdAt" : "2016-03-01T17:25:58Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "b4a42b26-9c18-4d6d-8d1f-a85cb3f97996",
        "parentId" : "02676d9d-decd-4486-875d-755a9534aeae",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "That's a good observation\n",
        "createdAt" : "2016-03-02T01:18:54Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "be3512d0-30f8-45e7-a35b-d691e7cff365",
        "parentId" : "02676d9d-decd-4486-875d-755a9534aeae",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "added dns validation.\n",
        "createdAt" : "2016-03-03T00:41:51Z",
        "updatedAt" : "2016-03-04T21:32:45Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3c00aadd5da91288cca856dabbefbc9f261be69",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1348,1352 @@\tif utilvalidation.IsDNS1123Label(subdomainCandidate) {\n\t\thostDomain = fmt.Sprintf(\"%s.%s.svc.%s\", subdomainCandidate, pod.Namespace, clusterDomain)\n\t}\n\treturn hostname, hostDomain\n}"
  },
  {
    "id" : "cf40179b-c35e-4e48-85b6-f00661a6f9df",
    "prId" : 20312,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3dcb1d5-c0a8-4cfa-bd53-0cbd8d954a74",
        "parentId" : null,
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "I make kubelet containing both the old client and the new clientset. I will remove the old client after when refactoring kubelet.\n",
        "createdAt" : "2016-01-29T04:47:46Z",
        "updatedAt" : "2016-01-31T23:42:10Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "7722a50647e4285d607ccdd0c9be2038fee5371d",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +275,279 @@\t\tdockerClient:                   dockerClient,\n\t\tkubeClient:                     kubeClient,\n\t\tclientset:                      clientset,\n\t\trootDirectory:                  rootDirectory,\n\t\tresyncInterval:                 resyncInterval,"
  },
  {
    "id" : "ca0b15a1-1195-466d-b92b-d052b424786e",
    "prId" : 20258,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a34ab908-756b-4b25-8cd1-7f1687590652",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Is it possible that this is still running?\n",
        "createdAt" : "2016-02-01T09:29:38Z",
        "updatedAt" : "2016-02-01T09:29:38Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "0ff3e5bd-4445-4713-9dea-f8bbe6b46073",
        "parentId" : "a34ab908-756b-4b25-8cd1-7f1687590652",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Unless the rest of the Kubelet is broken, no.  Kubelet doesn't set lastState.Terminated until the container is observed killed and stopped.\n",
        "createdAt" : "2016-02-01T18:33:37Z",
        "updatedAt" : "2016-02-01T18:33:37Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "b988c510-c2e8-4903-83d4-c75192f3bce1",
        "parentId" : "a34ab908-756b-4b25-8cd1-7f1687590652",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "ok\n",
        "createdAt" : "2016-02-01T18:36:33Z",
        "updatedAt" : "2016-02-01T18:36:33Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6d4a17db661ff0bb89c267d67c3af879e1b3679",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +2466,2470 @@\tswitch {\n\tcase previous:\n\t\tif lastState.Terminated == nil {\n\t\t\treturn kubecontainer.ContainerID{}, fmt.Errorf(\"previous terminated container %q in pod %q not found\", containerName, podName)\n\t\t}"
  },
  {
    "id" : "5b23649c-5522-4ba2-aeb2-ee841526903f",
    "prId" : 20204,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81fdf15a-9e8a-43fa-9724-869e1258eb8b",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "For the standalone mode, you'd need to return anyway. You don't want to proceed to asking the apiserver.\n\n```\nif kl.standaloneMode {\n   return kl.initialNodeStatus()\n}\n```\n",
        "createdAt" : "2016-02-23T23:59:44Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "9901973f-e179-4237-b845-b70dfe167273",
        "parentId" : "81fdf15a-9e8a-43fa-9724-869e1258eb8b",
        "authorId" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "body" : "Yes, thanks.\n",
        "createdAt" : "2016-02-25T09:42:29Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "tags" : [
        ]
      }
    ],
    "commit" : "41ed85479a73924a69f915e68dfaf005020a4e45",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2334,2338 @@\t\tif n, err := kl.nodeInfo.GetNodeInfo(kl.nodeName); err == nil {\n\t\t\treturn n, nil\n\t\t}\n\t}\n\treturn kl.initialNodeStatus()"
  },
  {
    "id" : "feb8e9d3-7301-4ca2-a1e6-e4ee043a4d9d",
    "prId" : 20204,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7cc58108-7493-4d68-a4ad-0a020a08d58a",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "It seems like we are changing kubelet's behavior in the standalone mode -- if we cannot get the node information completely, we cannot admit pods. /cc @dchen1107 \n",
        "createdAt" : "2016-02-24T00:16:18Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "82fa2e80-2a95-47fe-9242-77c21660ded9",
        "parentId" : "7cc58108-7493-4d68-a4ad-0a020a08d58a",
        "authorId" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "body" : "node information is useful when RunGeneralPredicates() checks the Resource and NodeLabel condition, and an empty *api.Node (&api.Node) is OK for RunGeneralPredicates() to make the right decision.\n\nWe could avoid return \"false\" (i.e. denying the pod) when getNodeAnyWay() returns non-nil error and only let RunGeneralPredicates() decides whether to admit a pod, which is logical as well.\n",
        "createdAt" : "2016-02-25T09:46:21Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "367ad63e-2fc8-4db1-949a-10424aaf7469",
        "tags" : [
        ]
      },
      {
        "id" : "7c2b42fb-8b3f-4f1c-a25e-a5e4d9a5c8b3",
        "parentId" : "7cc58108-7493-4d68-a4ad-0a020a08d58a",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "If we cannot get system resources for some reason, it is a legitimate error and we should fail to schedule pods. We should not hide errors, but rather fail explicitly. Silent failures are hard to detect.\n",
        "createdAt" : "2016-02-25T21:30:34Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "12af1ff2-45d2-43ab-9d97-145950e25ce7",
        "parentId" : "7cc58108-7493-4d68-a4ad-0a020a08d58a",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I'm okay with failing. If we are going to do that,  we should fail explicitly.\n",
        "createdAt" : "2016-02-25T22:58:58Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "d60a13f9-3c16-4167-b10c-3a4d74af66d3",
        "parentId" : "7cc58108-7493-4d68-a4ad-0a020a08d58a",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "@yujuhong's concern is valid here, and we might break the current users of standalone containerVM instances. But failing explicitly when failing to get node information here is desired, and I think we want to take the risk of breaking those users here. \n",
        "createdAt" : "2016-02-29T19:43:41Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "41ed85479a73924a69f915e68dfaf005020a4e45",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +2346,2350 @@func (kl *Kubelet) canAdmitPod(pods []*api.Pod, pod *api.Pod) (bool, string, string) {\n\tnode, err := kl.getNodeAnyWay()\n\tif err != nil {\n\t\tglog.Errorf(\"Cannot get Node info: %v\", err)\n\t\treturn false, \"InvalidNodeInfo\", \"Kubelet cannot get node info.\""
  },
  {
    "id" : "79245496-22b8-4742-8071-69dc9a21ad8d",
    "prId" : 20204,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9abad995-8626-49a6-b47f-a594636081e6",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "nit: can be more concise (need to fix the comments above)\n\n```\nif !kl.standaloneMode {\n    if n, err := kl.nodeInfo.GetNodeInfo(kl.nodeName); err == nil {\n        return n, nil\n     }\n }\n return kl.initialNodeStatus()\n```\n",
        "createdAt" : "2016-04-08T17:25:54Z",
        "updatedAt" : "2016-04-10T17:31:11Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "41ed85479a73924a69f915e68dfaf005020a4e45",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2330,2334 @@// in which case return a manufactured nodeInfo representing a node with no pods,\n// zero capacity, and the default labels.\nfunc (kl *Kubelet) getNodeAnyWay() (*api.Node, error) {\n\tif !kl.standaloneMode {\n\t\tif n, err := kl.nodeInfo.GetNodeInfo(kl.nodeName); err == nil {"
  },
  {
    "id" : "20ce7bda-5a7f-4b67-8b5b-9c9b28105312",
    "prId" : 20202,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5f02bac-2626-4546-a1f6-7d43e3b50b21",
        "parentId" : null,
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "@yujuhong @smarterclayton I changed the HandlePodDeletions to pass the entire `api#Pod` to the `deletePod`, so that later the deletion routine will have the pod spec, rather than having to read the container label. This change makes the HandlePodDeletions respect the grace period set through DeleteOptions. \nHandlePodCleanup still doesn't respect the grace period because it doesn't have the pod spec. I think this could be fixed in another PR, possibly by the node team.\n",
        "createdAt" : "2016-02-01T17:27:47Z",
        "updatedAt" : "2016-02-02T21:01:32Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      },
      {
        "id" : "716c0514-3500-4de5-92b6-6b597f3400e3",
        "parentId" : "d5f02bac-2626-4546-a1f6-7d43e3b50b21",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Fine with me\n",
        "createdAt" : "2016-02-01T17:48:43Z",
        "updatedAt" : "2016-02-02T21:01:32Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "a9bef48a-e3cd-4566-b4bf-4430b974619b",
        "parentId" : "d5f02bac-2626-4546-a1f6-7d43e3b50b21",
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "@yujuhong could you take a look? Thanks.\n",
        "createdAt" : "2016-02-02T18:54:14Z",
        "updatedAt" : "2016-02-02T21:01:32Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      },
      {
        "id" : "4cc0c2d9-915a-4f86-88bb-478be0f0d86c",
        "parentId" : "d5f02bac-2626-4546-a1f6-7d43e3b50b21",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Looking. Sorry for the delay.\n",
        "createdAt" : "2016-02-02T19:22:06Z",
        "updatedAt" : "2016-02-02T21:01:32Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "afa2f7fd-e515-4fe8-924b-3e887a70d955",
        "parentId" : "d5f02bac-2626-4546-a1f6-7d43e3b50b21",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The change LGTM.\n",
        "createdAt" : "2016-02-02T19:28:44Z",
        "updatedAt" : "2016-02-02T21:01:32Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "56e53cb3-becb-47a3-a9b4-5beb6650f953",
        "parentId" : "d5f02bac-2626-4546-a1f6-7d43e3b50b21",
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "Thanks.\n",
        "createdAt" : "2016-02-02T21:01:52Z",
        "updatedAt" : "2016-02-02T21:01:52Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6d96a04d05f546b16b4199bbc8aeda6102aedad",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +2395,2399 @@\t\t// Deletion is allowed to fail because the periodic cleanup routine\n\t\t// will trigger deletion again.\n\t\tif err := kl.deletePod(pod); err != nil {\n\t\t\tglog.V(2).Infof(\"Failed to delete pod %q, err: %v\", format.Pod(pod), err)\n\t\t}"
  },
  {
    "id" : "e1c1e6ba-dec8-472d-a66c-f8b310f3b53f",
    "prId" : 18795,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a26f1f6-532b-4d1b-95ad-8657fcfc4873",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "we should document the final form of this at the flag-decl site\n",
        "createdAt" : "2016-01-11T21:31:23Z",
        "updatedAt" : "2016-02-03T16:33:38Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "fabb65c13f2472ae00a693db2fac10d58f689f12",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +273,277 @@\t// of the kubenet network plugin\n\tif networkPluginName == \"kubenet\" {\n\t\tconfigureCBR0 = false\n\t\tflannelExperimentalOverlay = false\n\t}"
  },
  {
    "id" : "e5bdad61-c10b-483f-a8ca-6ad3c3ca9a7d",
    "prId" : 18401,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a47306a4-d960-498c-a472-ef310fb701d0",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "It'd be useful to specify the date/requirement for dropping this block of code.\n",
        "createdAt" : "2015-12-09T00:51:07Z",
        "updatedAt" : "2015-12-16T21:23:54Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "b7f31104-f49c-42c8-8ac4-eb222c9b33e1",
        "parentId" : "a47306a4-d960-498c-a472-ef310fb701d0",
        "authorId" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "body" : "In the release-1.1 branch, I don't think it would be dropped. This is a backport of a change that was made in master, and I could add a note there (although, it might be worthwhile just removing it from master altogether, as 1.2/1.0 version skew is not likely to be supported).\n",
        "createdAt" : "2015-12-09T01:03:25Z",
        "updatedAt" : "2015-12-16T21:23:54Z",
        "lastEditedBy" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "tags" : [
        ]
      },
      {
        "id" : "25843c38-0ab2-4a8e-b89c-3e0f06ce9c07",
        "parentId" : "a47306a4-d960-498c-a472-ef310fb701d0",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "How about: TODO: Remove the code after we drop support for v1.0 kubelets.\n",
        "createdAt" : "2015-12-09T16:14:40Z",
        "updatedAt" : "2015-12-16T21:23:54Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "3510c9c6-64f3-40f5-883f-26b173cb0434",
        "parentId" : "a47306a4-d960-498c-a472-ef310fb701d0",
        "authorId" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "body" : "Sure. Added.\n",
        "createdAt" : "2015-12-09T17:39:46Z",
        "updatedAt" : "2015-12-16T21:23:54Z",
        "lastEditedBy" : "b04ab4f5-5d69-4d32-9c1b-fecc0eb76d11",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbbf23e27ecaaf7c7aac06e289a2f73a53fd542d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +2570,2574 @@\t// NOTE(aaronlevy): NodeReady condition needs to be the last in the list of node conditions.\n\t// This is due to an issue with version skewed kubelet and master components.\n\t// ref: https://github.com/kubernetes/kubernetes/issues/16961\n\t// TODO: Remove the code after we drop support for v1.0 kubelets\n\tlastIndex := len(node.Status.Conditions) - 1"
  },
  {
    "id" : "8de6cf26-310d-468e-a477-e3697c3017d4",
    "prId" : 18237,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8768144e-0675-49f0-882d-ba15e162bf73",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The definition of `stopped` is unclear at best. I think #17971 also needs to be clearly defined...\n",
        "createdAt" : "2015-12-04T23:17:41Z",
        "updatedAt" : "2015-12-05T02:43:28Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "caaf7eaa-8000-4649-9f4c-598c96d6b11f",
        "parentId" : "8768144e-0675-49f0-882d-ba15e162bf73",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Yeah...I'm totally confused now...Why the waiting container should be calculated as stopped here? @yujuhong \n",
        "createdAt" : "2015-12-04T23:23:39Z",
        "updatedAt" : "2015-12-05T02:43:28Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "4e643058-6682-428e-9f77-3decac5f9b12",
        "parentId" : "8768144e-0675-49f0-882d-ba15e162bf73",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Because they are in the crashloop backoff state :-(\n",
        "createdAt" : "2015-12-04T23:25:14Z",
        "updatedAt" : "2015-12-05T02:43:28Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "fd908d8f-a778-46e7-800c-20b2c53a212a",
        "parentId" : "8768144e-0675-49f0-882d-ba15e162bf73",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Ah...I didn't read the following code just now...Thanks! :)\n",
        "createdAt" : "2015-12-04T23:27:22Z",
        "updatedAt" : "2015-12-05T02:43:28Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ac6129578af00f3d3d845a2c67850ef1e0f0260",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +2973,2977 @@\t\t\t}\n\t\tcase containerStatus.State.Waiting != nil:\n\t\t\tif containerStatus.LastTerminationState.Terminated != nil {\n\t\t\t\tstopped++\n\t\t\t} else {"
  },
  {
    "id" : "e7e4d0f7-841d-473c-8b99-738fbfa5314a",
    "prId" : 17700,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db993d48-8f68-4883-9483-d3c33f2f0e6c",
        "parentId" : null,
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "[This](https://github.com/kubernetes/kubernetes/pull/17700#discussion_r49015593) comment get lost\n\n> This line is the same as line 373, can you move it outside of the switch-case ?\n\nCan you keep only one `klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, nil)` ?\n",
        "createdAt" : "2016-01-10T07:16:12Z",
        "updatedAt" : "2016-01-13T18:20:05Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "2139aada-06e3-4b9e-b219-ddb68e1b0435",
        "parentId" : "db993d48-8f68-4883-9483-d3c33f2f0e6c",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Nope, I split the PR out to #19436 and addressed the comment there.\n",
        "createdAt" : "2016-01-11T17:33:27Z",
        "updatedAt" : "2016-01-13T18:20:05Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b56ed1a8c22263a1760aaeecee8c0768ac165dce",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +389,393 @@\t\t}\n\t\tklet.containerRuntime = rktRuntime\n\t\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, nil)\n\n\t\t// No Docker daemon to put in a container."
  },
  {
    "id" : "4c2b9955-2312-42d3-ab13-fa2da9902ca4",
    "prId" : 17401,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9c5c4f3-a187-4706-a7e3-968f1f378f25",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "cc @ihmccreery \n",
        "createdAt" : "2015-11-26T02:59:36Z",
        "updatedAt" : "2015-11-26T02:59:36Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c72696aad662bfd19604879c542cb8b700e78df",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +2830,2834 @@\t// NOTE(aaronlevy): NodeReady condition needs to be the last in the list of node conditions.\n\t// This is due to an issue with version skewed kubelet and master components.\n\t// ref: https://github.com/kubernetes/kubernetes/issues/16961\n\tvar newNodeReadyCondition api.NodeCondition\n\tvar oldNodeReadyConditionStatus api.ConditionStatus"
  },
  {
    "id" : "2586fafe-bc71-4be6-b84d-6ab125abe88e",
    "prId" : 17149,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f98fcc9a-baed-4ce7-8c95-b260567737e3",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Hmm...I don't like the global variable here. I prefer the old logic.  Also, why factoring out this as a function when you don't do that for the node ready status event.\n",
        "createdAt" : "2015-11-17T23:55:35Z",
        "updatedAt" : "2015-12-04T11:02:04Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "7d823784-099c-49c5-98e0-3ed29f58c71a",
        "parentId" : "f98fcc9a-baed-4ce7-8c95-b260567737e3",
        "authorId" : "8a12c756-879a-4c67-a5e1-bf34725a28de",
        "body" : "this global variable is exist before, i just move here to let it closer to where it used.\n:) \n",
        "createdAt" : "2015-11-19T02:03:27Z",
        "updatedAt" : "2015-12-04T11:02:04Z",
        "lastEditedBy" : "8a12c756-879a-4c67-a5e1-bf34725a28de",
        "tags" : [
        ]
      },
      {
        "id" : "569682de-ef48-4682-ae65-fd10f9b1abb5",
        "parentId" : "f98fcc9a-baed-4ce7-8c95-b260567737e3",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : ":-(\n",
        "createdAt" : "2015-12-03T00:11:40Z",
        "updatedAt" : "2015-12-04T11:02:04Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c6bab575b299250e4d300d6719406aecb09256",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2897,2901 @@\n// Maintains Node.Spec.Unschedulable value from previous run of tryUpdateNodeStatus()\nvar oldNodeUnschedulable bool\n\n// record if node schedulable change."
  },
  {
    "id" : "7c229380-4023-4d7c-a1f7-ad185d6a9247",
    "prId" : 16900,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c663b67-9b5c-4e74-bf83-baba72c3f811",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "this appears to be unaligned; you probably need to run gofmt\n",
        "createdAt" : "2015-11-25T05:34:18Z",
        "updatedAt" : "2016-01-04T20:23:47Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "cb94e83f-aacc-42da-aa57-56404e93b22e",
        "parentId" : "3c663b67-9b5c-4e74-bf83-baba72c3f811",
        "authorId" : "f6905dc9-1980-4381-8220-a1c7ad8d5c82",
        "body" : "I think this is a gofmt bug. Since gofmt is run every time I save this file in the editor, this strange alignment keeps coming back. I even tried opening this file in an editor without gofmt integration, manually adjusting the alignment and then running gofmt on the command line again. This unaligned formatting just keeps happening.\n",
        "createdAt" : "2015-12-16T01:48:45Z",
        "updatedAt" : "2016-01-04T20:23:47Z",
        "lastEditedBy" : "f6905dc9-1980-4381-8220-a1c7ad8d5c82",
        "tags" : [
        ]
      }
    ],
    "commit" : "059c2aa7991060890fbd68493be3acc0c4d6a181",
    "line" : null,
    "diffHunk" : "@@ -1,1 +317,321 @@\t\tnodeIP:                         nodeIP,\n\t\tclock:                          util.RealClock{},\n\t\toutOfDiskTransitionFrequency: outOfDiskTransitionFrequency,\n\t}\n\tif klet.flannelExperimentalOverlay {"
  },
  {
    "id" : "1f27cd5e-bbf8-4935-8e65-46dd22f17176",
    "prId" : 16894,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "472ba628-8e09-48d2-99d4-a3833cb99104",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Sorry that I didn't express more clear in the previous comment. What's the point of creating the pod at the end. I think you can remove the entire block.\n",
        "createdAt" : "2015-11-06T22:19:08Z",
        "updatedAt" : "2015-11-12T23:37:29Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "3587951e-e49a-4973-bbdb-7fb2ce5ee3cd",
        "parentId" : "472ba628-8e09-48d2-99d4-a3833cb99104",
        "authorId" : "e6835e94-1af7-44e8-ae1b-90dd34305f57",
        "body" : "My thought was that pod status could change during sync, and thus the mirror needed to be updated, but on second thought that may not be necessary.\n",
        "createdAt" : "2015-11-06T22:34:56Z",
        "updatedAt" : "2015-11-12T23:37:29Z",
        "lastEditedBy" : "e6835e94-1af7-44e8-ae1b-90dd34305f57",
        "tags" : [
        ]
      },
      {
        "id" : "e0e1036c-063d-4c1c-a86a-06a7ec6865db",
        "parentId" : "472ba628-8e09-48d2-99d4-a3833cb99104",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Creating a pod is different than updating the status. We only need to pod spec to create the mirror pod, and pod spec should not change during the sync. I think it's fine to remove them.\n",
        "createdAt" : "2015-11-06T23:27:18Z",
        "updatedAt" : "2015-11-12T23:37:29Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "0584f9ba7a590d16979813af4a8a982e2fd60b22",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1481,1485 @@\t\t}\n\t}\n\n\tif err := kl.makePodDataDirs(pod); err != nil {\n\t\tglog.Errorf(\"Unable to make pod data directories for pod %q (uid %q): %v\", podFullName, uid, err)"
  },
  {
    "id" : "287a1735-14fd-4c7b-8d28-ddfedd530302",
    "prId" : 16866,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Why expose these labels directly from the kubelet? Why can't we run a helper pod on the nodes that will update the node object directly on the apiserver with these labels?\n",
        "createdAt" : "2015-11-05T21:35:32Z",
        "updatedAt" : "2015-11-05T21:35:32Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "addb471e-4141-4fde-8424-ec0d2465cb9e",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "We could do that, but a few reasons why I'd rather not:\n- This means that the node comes up with the zone information, avoiding a window where clients would have to deal with the zone information not yet being available (and differentiating between that case vs the case where the helper pods isn't configured)\n- It's pretty easy to do it here in the kubelet - we're already creating the node, we already have the cloudprovider spun up -  so I do wonder if it is worth the complexity of creating a separate helper program & podifying it & ensuring it is configured correctly etc.\n- Having this information reliably on the nodes lets people target it confidently.  I want to use it for Ubernetes-Lite, where we will use this information to have the scheduler spread pods in an RC across zones when a cluster is running in multiple-AZs.  Ubernetes-Full will have a richer model, but I do think Uberentes-Lite will be in use for a while yet.\n",
        "createdAt" : "2015-11-05T22:38:19Z",
        "updatedAt" : "2015-11-05T22:38:19Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "c641f39d-85cb-44ec-886d-568f65152e52",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : null,
        "body" : "@vishh What would be your motivation fro doing it from a helper pod rather than kubelet? (other than the basic decoupling argument)\n",
        "createdAt" : "2015-11-05T22:43:32Z",
        "updatedAt" : "2015-11-05T22:43:32Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "d719b380-0b71-4344-b2c5-113e43e4698b",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Can we have a phase where the cluster is still not `logically` up? I'd imagine moving logging and monitoring agents to be helper pods (daemonSet) in the future and the cluster isn't logically usable until those pods come up.\n\n@quinton-hoole: My concern is that this PR will open up a doorway for exposing more and more labels from the kubelet directly. There are many other use-cases already where people want the node to expose custom labels.\nFrom core kubernetes perspective, I see these labels are being extraneous. The cluster can function without them. \n\nExisting issues related to node labels: #13524 #13520\n",
        "createdAt" : "2015-11-05T22:57:48Z",
        "updatedAt" : "2015-11-05T22:57:48Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "f2d63709-b6d8-4902-a1b3-9d16ce504c6d",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "I think of nodes as being very dynamic, so I think it would have to be Node state being \"up\" rather than the cluster state.  But pedantry aside, I'd rather avoid having a complex Node lifecycle if we can avoid it - it's been the cause of many bugs in my own code dealing with cloud APIs when I assumed that property X would be set but it was actually set asynchronously with an unknown delay.\n\nI do think that we will end up exposing all that extra information in the end (I'm looking in particular at NodeSystemInfo).  I did originally try an alternate implementation where I added a typed Zone data structure to the Node, but I actually think labels are \"more kubernetes\".  Having labels lets us use the selector approach (which I think is a great kubernetes 'core idea') - i.e. we get to use the NodeSelector predicates in the scheduler.\n\nThe other downside of having a strongly typed Zone object is that we want to add this information to every type that has zone restrictions; Nodes & Volumes for a simple multi-AZ kubernetes, probably every type in full federated kubernetes (Ubernetes).\n\nIn any case, we can start with labels and map labels <-> objects later if we do decide that's what we want to do; as these are in the \"kubernetes.io\" reserved prefix.\n\nThis is part of the Ubernetes (Lite) effort, so while today \"the cluster can function without them\", that won't be true for some of the cool stuff that is coming :-)  You can check out the branch where I have multi-AZ working if you want to see where this is going: https://github.com/justinsb/kubernetes/tree/multizone_with_labels\n\nA potential compromise: we could gate this behind an experimental flag if you'd like?  I'd prefer not to, but it wouldn't really impair the user experience if we automatically set it for the user when they are running multi-AZ.\n",
        "createdAt" : "2015-11-06T14:55:00Z",
        "updatedAt" : "2015-11-06T14:55:00Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "c641ff92-4932-48d3-a94c-598fd75d1740",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "cc @bgrant0607 @pwittrock @dchen1107 \n",
        "createdAt" : "2015-11-06T18:13:02Z",
        "updatedAt" : "2015-11-06T18:13:02Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "d45b5bfc-0544-4e58-a1c8-dd52d2afd82a",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "My main concern is that this PR will open up the door for adding more and more node specific labels into the kubelet. \nOne compromise could be to explicitly define the limited subset of labels that the kubelet will surface. We need some process for enforcing this though.\nThe other alternative I can think of is to store all custom labels into a file on the node, which the kubelet can then use. This approach requires labels to be pre-populated though.\n",
        "createdAt" : "2015-11-06T18:24:25Z",
        "updatedAt" : "2015-11-06T18:24:25Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "8a57c39a-be1a-4d48-aa33-45b8391b5018",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "See also #9044\n\ncc @davidopp \n",
        "createdAt" : "2015-11-06T18:42:05Z",
        "updatedAt" : "2015-11-06T18:42:05Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "c8f7c455-df8a-43b7-bdeb-12b074e3c0af",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@justinsb: To make kubelet support for custom labels extensible we need an `exec` based plugin model. This PR can go in as-is, but once we get around to adding support for custom label plugins, this code needs to be refactored. \n",
        "createdAt" : "2015-11-06T22:36:37Z",
        "updatedAt" : "2015-11-06T22:36:37Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "7dddf629-f5bd-4799-9e43-d758046aeb2b",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Personally I think @justinsb makes some good arguments, but ultimately as @vishh says we will need an exec-based model to support custom label plugins.\n",
        "createdAt" : "2015-11-07T06:37:55Z",
        "updatedAt" : "2015-11-07T06:37:55Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "3cd8eccf-344f-469e-8ecb-b9d823d18b99",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "I agree that letting this in, with the intention that I will refactor it out into a future plugin system makes a lot of sense: it solves a problem we have now, it won't be hard to refactor out, and having this as a \"real-world\" example will also help make plugins better.  For example it will help us think about what configuration/credentials we need to provide to plugins, and whether we expect plugins to reuse code e.g. the cloudprovider implementations.\n",
        "createdAt" : "2015-11-07T13:09:32Z",
        "updatedAt" : "2015-11-07T13:09:32Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "d903813b-579e-4c42-bffe-b35a874c110a",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "FWIW I'm fine with publishing this from the Kubelet. My personal opinion is that it's reasonable to think of \"system node labels\" and \"user node labels\" separately, and publish the former directly from the Kubelet rather than from a separate pod. But regardless, as @justinsb points out, it should be possible to move this back out of Kubelet later, so we don't really need to settle this debate in order to accept this PR.\n",
        "createdAt" : "2015-11-11T21:49:49Z",
        "updatedAt" : "2015-11-11T21:49:49Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "6e9915f9-50cf-4e11-8d47-d28e9390b276",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "71c7590a-3e54-4928-afac-4829d5cfea66",
        "body" : "@justinsb where we will use this information to have the scheduler spread pods in an RC across zones when a cluster is running in multiple-AZs.\n+1\nwe have done it in version 0.12and migrate it to 1.0. we tag zones information with label ,maybe can move it into nodeSpec,because kubelet can get zones in VM exposed by IaaS.\n",
        "createdAt" : "2015-12-25T07:10:12Z",
        "updatedAt" : "2015-12-25T07:10:12Z",
        "lastEditedBy" : "71c7590a-3e54-4928-afac-4829d5cfea66",
        "tags" : [
        ]
      },
      {
        "id" : "fdfc3c4a-5d82-4e4b-89db-639c91cfeb5f",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "+1 for moving region/zone information to node.spec, instead of as a label, since this is the information which kubelet could get from cloud provider, just like `ExternalID` and `ProviderID`.\n\nAfter all, if we make region/zone information a node label, why not make ExternalID and ProviderID as label, too?\n",
        "createdAt" : "2015-12-25T10:36:17Z",
        "updatedAt" : "2015-12-25T10:36:17Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      },
      {
        "id" : "a7eff195-94b8-44a0-918b-92e78f445413",
        "parentId" : "ba267acf-c28f-49c9-84ad-eb4aee713de8",
        "authorId" : "1b6f39c8-c2c3-4aa8-ac5e-4961cd5b0253",
        "body" : "Cloud providers may have placement abstractions that do not clearly map to other cloud providers, in which case labels may make more sense.  For instance, AWS has the concept of placement groups in addition to regions and zones.\n",
        "createdAt" : "2015-12-26T02:48:47Z",
        "updatedAt" : "2015-12-26T02:48:47Z",
        "lastEditedBy" : "1b6f39c8-c2c3-4aa8-ac5e-4961cd5b0253",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2c2d617cf09fcc809b7e5e70f68d8fd622d1c58",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +904,908 @@\n\t\t// If the cloud has zone information, label the node with the zone information\n\t\tzones, ok := kl.cloud.Zones()\n\t\tif ok {\n\t\t\tzone, err := zones.GetZone()"
  },
  {
    "id" : "a727e371-1318-4ecc-a5e9-65f89d0574e4",
    "prId" : 16782,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "199b91bb-01bf-46ad-9bc3-b140fc639e16",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Might be easier to build a set out of the list, e.g., `util.set.NewString()`, then check whether a pod uid is in the set without having to use a for loop. \n",
        "createdAt" : "2015-11-10T15:41:44Z",
        "updatedAt" : "2015-11-11T18:05:49Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "14d0dc2f-4d72-4b11-9205-020a2ea685a5",
        "parentId" : "199b91bb-01bf-46ad-9bc3-b140fc639e16",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Yeah, that's more efficient :) I'll do it!\n",
        "createdAt" : "2015-11-10T19:21:48Z",
        "updatedAt" : "2015-11-11T18:05:49Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6b93cdfe1383744093561053dd16244a2830a79",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1773,1777 @@func (kl *Kubelet) getPodsToSync() []*api.Pod {\n\tallPods := kl.podManager.GetPods()\n\tpodUIDs := kl.workQueue.GetWork()\n\tpodUIDSet := sets.NewString()\n\tfor _, podUID := range podUIDs {"
  },
  {
    "id" : "3773ea94-523e-451e-a451-16a4e46b4e1b",
    "prId" : 16782,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ada031e-3238-49b5-b629-90f3f3f3b41e",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "minor nit: add `continue` here and after line 1788. This removes the else block and allows you to add more `if` in the future.\n",
        "createdAt" : "2015-11-10T22:09:30Z",
        "updatedAt" : "2015-11-11T18:05:49Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "2785a5ff-8737-4f7e-8c30-70c35449ea57",
        "parentId" : "6ada031e-3238-49b5-b629-90f3f3f3b41e",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Wow! Got it! It seems that because I modified the code in a hurry, it's really ugly. Shame on me! Hahaha~\n",
        "createdAt" : "2015-11-11T03:41:46Z",
        "updatedAt" : "2015-11-11T18:05:49Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6b93cdfe1383744093561053dd16244a2830a79",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1782,1786 @@\t\tif kl.pastActiveDeadline(pod) {\n\t\t\t// The pod has passed the active deadline\n\t\t\tpodsToSync = append(podsToSync, pod)\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "e0235e14-3435-4baf-abd7-71652b6c7c12",
    "prId" : 16261,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d409a7c-9deb-4948-a4b7-45ebabbc0552",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "If you accept my suggestions below to not print unreadyMessages, then you move the `if podPhase == api.PodSucceeded` checks to here, avoiding duplication of that check.\n",
        "createdAt" : "2015-10-29T23:14:53Z",
        "updatedAt" : "2015-11-10T07:38:54Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f630d8ad7c162d7cac8fedfb32b35f0cc7240ff",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2683,2687 @@\tif len(unknownContainers) > 0 {\n\t\tunreadyMessages = append(unreadyMessages, fmt.Sprintf(\"containers with unknown status: %s\", unknownContainers))\n\t}\n\tif len(unreadyContainers) > 0 {\n\t\tunreadyMessages = append(unreadyMessages, fmt.Sprintf(\"containers with unready status: %s\", unreadyContainers))"
  },
  {
    "id" : "5a01a910-162b-4376-bc31-a401161774fc",
    "prId" : 16178,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8af5f643-40be-4cf8-87d9-1767a3dceddf",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Upon OOD situation with this change, shouldn't you send events on every NodeStatus update cycle? Also this will introduce more logs to the node.\n",
        "createdAt" : "2015-10-23T22:26:22Z",
        "updatedAt" : "2015-10-26T22:02:08Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "13a13c0b-6aac-469c-a968-e1f0c2c379fe",
        "parentId" : "8af5f643-40be-4cf8-87d9-1767a3dceddf",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Never mind. I see you only do this upon the transition. \n",
        "createdAt" : "2015-10-23T22:32:56Z",
        "updatedAt" : "2015-10-26T22:02:08Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c4424f0bd55b8f2fb4610a4616fbd5cd30abc9a",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +2591,2595 @@\t\t\tnodeOODCondition.Message = \"out of disk space\"\n\t\t\tnodeOODCondition.LastTransitionTime = currentTime\n\t\t\tkl.recordNodeStatusEvent(\"NodeOutOfDisk\")\n\t\t}\n\t} else {"
  },
  {
    "id" : "f0a22cdc-4066-4702-a98e-321e15337aec",
    "prId" : 16174,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7404efec-02c2-4cbc-9c9a-356f21ecee86",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "So there is no defaulting for SecurityContext? \n",
        "createdAt" : "2015-10-23T19:21:41Z",
        "updatedAt" : "2015-10-23T19:21:41Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "c3e332a7-9951-4861-8461-2648498a0d4e",
        "parentId" : "7404efec-02c2-4cbc-9c9a-356f21ecee86",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "It is a pointer..\n",
        "createdAt" : "2015-10-23T19:23:23Z",
        "updatedAt" : "2015-10-23T19:23:31Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      },
      {
        "id" : "b3cd1056-6434-4f79-bae7-7dc9afbe5dde",
        "parentId" : "7404efec-02c2-4cbc-9c9a-356f21ecee86",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "I know it is defined as a pointer. I just thought we initialized it at defaulting stage. Anyway, let's fix this for now. \n",
        "createdAt" : "2015-10-23T20:01:40Z",
        "updatedAt" : "2015-10-23T20:01:40Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a7b4f2ed24599198acad7c422dc83de11c7adac",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +971,975 @@\t// - container is not already mounting on /etc/hosts\n\t// When the pause container is being created, its IP is still unknown. Hence, PodIP will not have been set.\n\tmountEtcHostsFile := (pod.Spec.SecurityContext == nil || !pod.Spec.SecurityContext.HostNetwork) && len(pod.Status.PodIP) > 0\n\tglog.V(4).Infof(\"Will create hosts mount for container:%q, podIP:%s: %v\", container.Name, pod.Status.PodIP, mountEtcHostsFile)\n\tmounts := []kubecontainer.Mount{}"
  },
  {
    "id" : "f2d0d3fc-0ddf-42df-9c93-66b1feda37d8",
    "prId" : 16052,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f08968f-d1ac-43a5-9993-76cdf46bdcd2",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Please comment here: You only create etc_hosts file for containers meeting both requirements:\n1) not using host network. This is pretty straight-forward. But worth a comment to make sure other maintainer won't break this logic in the future.\n2) podIP is available already. This means network infrastructure container's etc_hosts file is different from rest of containers. It should be ok since that is a hidden container, and not used by the users anyway. But it is confusing. I did stair at it for a couple of minutes to figure it out. :-) \n",
        "createdAt" : "2015-10-22T17:01:59Z",
        "updatedAt" : "2015-10-22T20:27:23Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "6539d2e4-0b92-4b0e-abfa-ccea90993b7e",
        "parentId" : "5f08968f-d1ac-43a5-9993-76cdf46bdcd2",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "done\n",
        "createdAt" : "2015-10-22T20:27:32Z",
        "updatedAt" : "2015-10-22T20:27:32Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba6469d47885ae876d4ce67087457315ccc6ebcf",
    "line" : null,
    "diffHunk" : "@@ -1,1 +965,969 @@\t// - container is not already mounting on /etc/hosts\n\t// When the pause container is being created, its IP is still unknown. Hence, PodIP will not have been set.\n\tmountEtcHostsFile := !pod.Spec.SecurityContext.HostNetwork && len(pod.Status.PodIP) > 0\n\tglog.V(4).Infof(\"Will create hosts mount for container:%q, podIP:%s: %v\", container.Name, pod.Status.PodIP, mountEtcHostsFile)\n\tmounts := []kubecontainer.Mount{}"
  },
  {
    "id" : "4b3d15c9-6404-4ae2-901d-d776e07cb784",
    "prId" : 15994,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be3d8c72-c9e8-4d9b-8f01-619010a63cfb",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "nit: log the error so that we know why it failed.\n",
        "createdAt" : "2015-10-29T21:00:53Z",
        "updatedAt" : "2015-10-30T20:49:13Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "660f6b61-bf72-48de-9422-b44765bacb26",
        "parentId" : "be3d8c72-c9e8-4d9b-8f01-619010a63cfb",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Done!\n",
        "createdAt" : "2015-10-29T21:37:21Z",
        "updatedAt" : "2015-10-30T20:49:13Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "a39e1e96dc1e8385f4434b0120d061211fa31d64",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +2708,2712 @@\t\t// Verify the docker version.\n\t\tresult, err := version.Compare(dockertools.MinimumDockerAPIVersion)\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Cannot compare current docker version %v with minimum support Docker version %q\", version, dockertools.MinimumDockerAPIVersion)\n\t\t\treturn false"
  },
  {
    "id" : "3a9eaf27-4991-4c06-bf87-da2fe76c7cac",
    "prId" : 15645,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "849cfa13-3cac-4c19-b456-cc81ca8abca2",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Comment on this, please.  It's less obvious now.  Also comment on L1163 is now wrong.  Please audit this function to make sure comments match reality\n",
        "createdAt" : "2015-10-15T04:37:21Z",
        "updatedAt" : "2015-11-25T23:03:23Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "015df14a4ae1f0f4b0c3f317c36121d696451f7d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1499,1503 @@\t\t// the pod. The cluster DNS server itself will forward queries to other nameservers that is configured to use,\n\t\t// in case the cluster DNS server cannot resolve the DNS query itself\n\t\tdns = []string{kl.clusterDNS.String()}\n\t} else {\n\t\t// clusterDNS is not known."
  },
  {
    "id" : "10cf53aa-ae5e-4769-9743-fd03dd0127f3",
    "prId" : 15323,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fe3c408-d59b-432b-b946-0f15885126bc",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "This is the right place to calculate whether this is the first container that mounts the volume and drive `SELinuxRelabel` based on that.\n",
        "createdAt" : "2015-10-09T07:19:22Z",
        "updatedAt" : "2015-10-28T13:27:06Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d352a16b8e766eabe7ab75ebddc43f07a6fcdd0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1045,1049 @@\t\t\trelabelVolume = true\n\t\t}\n\t\tmounts = append(mounts, kubecontainer.Mount{\n\t\t\tName:           mount.Name,\n\t\t\tContainerPath:  mount.MountPath,"
  },
  {
    "id" : "e2bdba29-ea85-4aad-928e-52faedc7ee43",
    "prId" : 15323,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe266a84-5db1-474c-982f-c716696ce6eb",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "curiosity: if we feel like we know how to do the labelling ourselves, why should we ever defer to docker?  Isn't our code net simpler, less coupled, and more fixable if we just do it ourselves?\n",
        "createdAt" : "2015-10-25T03:53:51Z",
        "updatedAt" : "2015-10-28T13:27:06Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "dc7e6770-bd3f-4df6-9790-4d8611869cd7",
        "parentId" : "fe266a84-5db1-474c-982f-c716696ce6eb",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "e.g. what about non-docker runtimes?\n",
        "createdAt" : "2015-10-25T03:54:20Z",
        "updatedAt" : "2015-10-28T13:27:06Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "475f7d8c-ffc5-4e8d-9f19-6fcf0f5d5e93",
        "parentId" : "fe266a84-5db1-474c-982f-c716696ce6eb",
        "authorId" : null,
        "body" : "@thockin I think that we are better off letting docker, or non-docker rumtimes, do the labeling. Because docker is in a better position to know what label the container will run as if one is not provided. In this PR I use the rootContext as a heuristic to decide what the appropriate label 'type' will be. That works well because there are only two answers svirt_sandbox_file_t or \"it does not matter because the container is privileged\" but if docker changes its defaults we would have to handle that.\n\nIn addition I am investigating an issue where containers which run in the hostIPC or hostPID are unconfined by SELinux. That is true with pure docker but not in kube. If I can fix that issue this whole code pathway becomes unnecessary.\n",
        "createdAt" : "2015-10-26T14:52:01Z",
        "updatedAt" : "2015-10-28T13:27:06Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "ba0fb6df-2ca0-4c33-820d-0db4ed0ef312",
        "parentId" : "fe266a84-5db1-474c-982f-c716696ce6eb",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I am not sure I buy that, but this PR is long overdue, so I'll stop arguing.\n",
        "createdAt" : "2015-10-27T06:27:52Z",
        "updatedAt" : "2015-10-28T13:27:06Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "0030531d-dbf2-454b-8027-ad7ba0fc798a",
        "parentId" : "fe266a84-5db1-474c-982f-c716696ce6eb",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "@thockin The chcon runner can definitely be re-used to provide support for rkt and other runtimes.  I'll make a follow-up issue.\n",
        "createdAt" : "2015-10-28T16:21:29Z",
        "updatedAt" : "2015-10-28T16:21:29Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d352a16b8e766eabe7ab75ebddc43f07a6fcdd0",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +981,985 @@// relabelVolumes relabels SELinux volumes to match the pod's\n// SELinuxOptions specification. This is only needed if the pod uses\n// hostPID or hostIPC. Otherwise relabeling is delegated to docker.\nfunc (kl *Kubelet) relabelVolumes(pod *api.Pod, volumes kubecontainer.VolumeMap) error {\n\tif pod.Spec.SecurityContext.SELinuxOptions == nil {"
  },
  {
    "id" : "ecb10336-874f-4ae1-9487-98b16aecebcb",
    "prId" : 15323,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d13624c-6755-4abc-b69e-a64ba6d748f6",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "is it a problem that this whole function could fail and the labels are not reverted?  I think it is ok, but have to ask.\n",
        "createdAt" : "2015-10-25T03:58:31Z",
        "updatedAt" : "2015-10-28T13:27:06Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "538d792e-b363-432f-a729-b7b49d93c7d8",
        "parentId" : "8d13624c-6755-4abc-b69e-a64ba6d748f6",
        "authorId" : null,
        "body" : "@thockin With this PR it would not be too bad as the next pod will relabel the volume to match its label. So the volume is not rendered unusable. If we change things so that volumes are only relabeled once, only when newly formatted for example, then we might have to handle that. Although TBH I don't think there is value in putting effort into preserving the previous labels; default labels are not usable by pods, and presumably the user does not care about an older pod's labels because the want this pod to use the volume exclusively.\n",
        "createdAt" : "2015-10-26T14:52:10Z",
        "updatedAt" : "2015-10-28T13:27:06Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "1d352a16b8e766eabe7ab75ebddc43f07a6fcdd0",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +1139,1143 @@\t// relabel the volumes\n\tif pod.Spec.SecurityContext != nil && (pod.Spec.SecurityContext.HostIPC || pod.Spec.SecurityContext.HostPID) {\n\t\terr = kl.relabelVolumes(pod, vol)\n\t\tif err != nil {\n\t\t\treturn nil, err"
  },
  {
    "id" : "5860db6a-1bb2-45d8-9ece-5f7fc3b8ca2d",
    "prId" : 15264,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91632094-e675-4c3b-bf7f-122bdc4d611b",
        "parentId" : null,
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "What if WorkQueue provided the channel instead? It could either work the same as what you have now:\n- just move ticker into the WorkQueue\n- on each tick look for ready work and send either a slice of ids over the channel, or each id one at a time\n\nOr the work queue could maintain a timer until the next work item is ready, and update it either when a new item is added (with shorter duration), or an item becomes ready.\n",
        "createdAt" : "2015-10-30T23:05:50Z",
        "updatedAt" : "2015-11-03T21:29:19Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "0a921b60-0c16-4d47-81e2-c861bd310913",
        "parentId" : "91632094-e675-4c3b-bf7f-122bdc4d611b",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I haven't found the need to create a goroutine in the WorkQueue yet, since we already have this busy loop here. I am not against it either, but would rather wait until we find the need to do so. What do you think? \n",
        "createdAt" : "2015-11-02T17:31:54Z",
        "updatedAt" : "2015-11-03T21:29:19Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "79797a7a-a2c2-4d81-b99e-0af88c8f0f2b",
        "parentId" : "91632094-e675-4c3b-bf7f-122bdc4d611b",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "I think kubelet.go is too large and holds too much state currently, so I'm generally in favor of any ways to move state (the ticker, in this case) out of kubelet. That said, I recognize it does add non-trivial complexity to the WorkQueue, so whichever way you want to go is fine with me.\n",
        "createdAt" : "2015-11-02T19:40:56Z",
        "updatedAt" : "2015-11-03T21:29:19Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "196f92bb-0fe6-44b7-b423-0c39f5e7a44d",
        "parentId" : "91632094-e675-4c3b-bf7f-122bdc4d611b",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I agree kubelet.go is too large, but the sync ticker is not remotely concerning to me. We have node status updates, and various volume/network related code in this file. For me, those are on the top of the list to get moved out. In this case, I think it's actually clear to have the ticker in the main sync loop and it also avoids unnecessary goroutine/complexity in the work queue.\n",
        "createdAt" : "2015-11-02T20:27:11Z",
        "updatedAt" : "2015-11-03T21:29:19Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2eb17df46b1dffa3b0a4cbc7133fdd18ed6d3223",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +2151,2155 @@\t\t\tglog.Errorf(\"Kubelet does not support snapshot update\")\n\t\t}\n\tcase <-kl.resyncTicker.C:\n\t\tpodUIDs := kl.workQueue.GetWork()\n\t\tvar podsToSync []*api.Pod"
  },
  {
    "id" : "d032fc01-5f70-4baf-9b21-da83186bec68",
    "prId" : 14542,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c72c475-041d-4832-9cca-088513427ba7",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "This seems semantically different from previous code because runtimeState would start with no errors. If syncNetworkStatus hasn't run even once, we'd assume the network is configured? \n",
        "createdAt" : "2015-09-25T17:09:55Z",
        "updatedAt" : "2015-11-11T23:20:23Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "3acf388b-8a98-4710-9a7b-0397c040e754",
        "parentId" : "8c72c475-041d-4832-9cca-088513427ba7",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Yes. Fixed. \n",
        "createdAt" : "2015-09-25T17:49:41Z",
        "updatedAt" : "2015-11-11T23:20:23Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1770537ab0bdb54c15073e0d7c1d234d35074ac",
    "line" : 281,
    "diffHunk" : "@@ -1,1 +2081,2085 @@\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tfor {\n\t\tif rs := kl.runtimeState.errors(); len(rs) != 0 {\n\t\t\tglog.Infof(\"skipping pod synchronization - %v\", rs)\n\t\t\ttime.Sleep(5 * time.Second)"
  },
  {
    "id" : "89315965-97a7-4805-83e4-546767561557",
    "prId" : 14221,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09af39e1-2fd5-458d-85b6-3cfe76462027",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "You need to add this in HandlePodCleanup() too for the cases where pods were deleted during kubelet restarts. (kubelet doesn't have a checkpoint and it would not recognize that the pods were deleted at all).\n",
        "createdAt" : "2015-09-22T00:38:51Z",
        "updatedAt" : "2015-10-02T22:37:21Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "b659634b-1dbe-4217-a30c-83574e4f00d6",
        "parentId" : "09af39e1-2fd5-458d-85b6-3cfe76462027",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Done.\n",
        "createdAt" : "2015-09-23T20:45:54Z",
        "updatedAt" : "2015-10-02T22:37:21Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "52ece0c34e8666f74c99906a73472b44fc9af4c0",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +2035,2039 @@\t\t\tglog.V(2).Infof(\"Failed to delete pod %q, err: %v\", kubeletUtil.FormatPodName(pod), err)\n\t\t}\n\t\tkl.probeManager.RemovePod(pod)\n\t}\n}"
  },
  {
    "id" : "92a08fff-4f56-4385-a80b-40b5e3df488a",
    "prId" : 14054,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "If a node is unschedulable, should the kubelet not watch for pods to run? That would be much better security than setting a flag that could be overridden by anyone with api access. \n",
        "createdAt" : "2015-09-16T20:04:38Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "95db1205-24bb-4d40-b369-bf6048cd2700",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "@mikedanese had a brief chat over a similar topic earlier this morning. At the end, we still want master node(s) be schedulable, not ready today. This is not recommended by us, we are going to document it, but it is up to the user / cluster admin to decide if making such node schedulable in a OSS environment. \n",
        "createdAt" : "2015-09-16T20:47:08Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "eac2fae0-5024-4535-857a-8e79919945d0",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "The behavior would still be controlled by this flag though. \n\nRight now, you could post a node status manually to change the node to scehduleable, but that will get flipped back every N seconds as the kubelet keeps posting status. In the intervening window, you could pin a pod to the master node to run arbitrary code on it. \n\nIf the kubelet (when told to be unschedulable) both told the master it's intent _and_ explicitly didn't run any pods assigned to it, that would close the hole. A cluster admin could still create a master as a schedulable node by changing this flag. \n",
        "createdAt" : "2015-09-16T21:03:57Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "851a75ae-dcd3-4970-a0b2-4c2eb1654c46",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "> Right now, you could post a node status manually to change the node to scehduleable, but that will get flipped back every N seconds as the kubelet keeps posting status.\n\nUnschedulable is part of Spec not Status, and is posted only once during initial registration.\n\nThe `--register-schedulable=false` flag is meant to tell a kubelet to register as unscheduled initially but makes no guarantees that the node will remain unschedulable. It may even have a broader use case than just the master's kubelet.\n\nI can make this explicit in the flag documentation.\n\n> If the kubelet (when told to be unschedulable) both told the master it's intent and explicitly didn't run any pods assigned to it, that would close the hole. A cluster admin could still create a master as a schedulable node by changing this flag.\n\nIMHO that doesn't provide any better security or user experince than just locking down the node resource.\n",
        "createdAt" : "2015-09-16T21:27:55Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "63ca0bb0-797c-4142-ba25-9471ead7e7c3",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "I don't think this introduces any extra security hole. Honestly we don't have a real security solution for OSS kubernetes yet, and the feature is completely disabled for GKE environment. \n",
        "createdAt" : "2015-09-16T22:45:22Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "66f0db93-7546-4470-9ecf-15eca3374123",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "@roberthbailey Also I don't understand your earlier comment on Kubelet not watching pods? Kubelet on master node has to start those static pods, ie, master components, and create corresponding mirror pods, and saved them to etcd through kube-apiserver, right? \n",
        "createdAt" : "2015-09-16T22:48:25Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "3fc5713e-e4dc-47c6-a906-4750eb29ff7c",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "@mikedanese Having it be part of Spec makes it worse, because then it's trivially easy to flip the value and run pods on the master.\n\n@dchen1107 Folks running the OSS code may want to provide API access (but not direct machine access) to their cluster to other users (e.g. here's a copy of my kubeconfig but you don't have ssh access to the master). This change means that you can't effectively do that because I could run whatever I want on the master node. \n",
        "createdAt" : "2015-09-16T22:51:22Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "dc7156c7-2602-43d3-90d2-c145384a5627",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "@dchen1107  Re: kubelet watching pods - I was wondering if it would be difficult for the kubelet to do everything it does today except not run any pods assigned to it by the apiserver. It would still run pods from the other sources (metadata server, manifest file, etc), post mirror pods, post node status, etc. It would just refuse to start any pods assigned by the apiserver (making itself truly unschedulable). \n",
        "createdAt" : "2015-09-16T22:53:06Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "217dc5c9-8d79-409c-8f46-cae61067f5ff",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "> Folks running the OSS code may want to provide API access (but not direct machine access) to their cluster to other users (e.g. here's a copy of my kubeconfig but you don't have ssh access to the master). This change means that you can't effectively do that\n\nIn OSS today, you can create users with access to only specific resources and specific namespaces with ABAC authorizer. An admin can restrict write access to the node resource.\n",
        "createdAt" : "2015-09-16T23:03:15Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "8147a183-30a5-4482-93f4-786dc6ce72f8",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "One of major benefits introduced by Kubernetes is decoupling cluster operations from application operations. If a cluster admin decide to allow master node schedulable, that is his decision. It shouldn't allow arbitrary application users to change node configurations / flags. If they do, that is bad operations!\n",
        "createdAt" : "2015-09-16T23:09:01Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "a5133184-19b2-4bd5-b063-433e6790ef2f",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@mikedanese: What is the use case for making `--register-schedulable` flag an initialization only option instead of being valid for the entire lifetime? \nIn general, I find multiple ways to configure the same behavior confusing. As a user, I'd prefer to either flip a flag or post an update to an API server object and not deal with how these two options interact.\n",
        "createdAt" : "2015-09-17T22:37:34Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "c63147e5-538a-4633-9be1-26c723e1262c",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@mikedanese: Ping.\n",
        "createdAt" : "2015-09-18T16:58:58Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "e082c59e-992b-4d46-8a0c-3ae70d896cf8",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "The intent is only to control initial registration configuration, not to control it continuously. We don't want to take away power from the API. That's the domain of auth policy, as discussed above.\n\nIf were to control behavior continuously, that would require a different means of communicating Kubelet's configuration back up to the scheduler. More details are discussed in #3885. We're not ready to do that, though.\n\nIf we wanted to materially impact the security profile of Kubelet, we need a more comprehensive design. For instance, if exec weren't blocked as well as pod creation, there's no point. OTOH, blocking exec would impact debuggability.\n",
        "createdAt" : "2015-10-06T21:20:38Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "d096ccea-6c1c-4850-86cc-afbd6b4ef33b",
        "parentId" : "fb07cb86-01dc-4bbb-92de-60183041b2fd",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "I am glad to see that we are converging on this. To me that flag --register-schedulable is a temporary hack because we don't have config object defined to generate NodeSpec yet. But to Kubelet, I want NodeSpec is the only source of truth here. \n",
        "createdAt" : "2015-10-07T17:55:56Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa60bbe8e6b1b50bfc82f13a8c1e7ed059d8ea99",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +827,831 @@\t\t\tLabels: map[string]string{\"kubernetes.io/hostname\": kl.hostname},\n\t\t},\n\t\tSpec: api.NodeSpec{\n\t\t\tUnschedulable: !kl.registerSchedulable,\n\t\t},"
  },
  {
    "id" : "f59a3991-e2d5-476a-8918-7db959aa0489",
    "prId" : 14054,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34bcbad8-6916-4857-a7ed-a9929c50d34c",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "What do we expect node.Spec.PodCIDR to be set to for the master node?\n",
        "createdAt" : "2015-10-06T21:03:20Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "013773bc-7fb9-4f90-97ba-6bdd2c8cb20c",
        "parentId" : "34bcbad8-6916-4857-a7ed-a9929c50d34c",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "If we expect Spec.PodCIDR to be unset, we could just not clobber kl.podCIDR in that case, and remove the flag.\n",
        "createdAt" : "2015-10-06T21:21:43Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "d606c164-8522-4036-820b-35dce30b4974",
        "parentId" : "34bcbad8-6916-4857-a7ed-a9929c50d34c",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "The nodecontroller will write a cidr into the nodespec for the master kubelet as it does not differentiate it from other kubelets. We don't want master kubelet to use this cidr because this causes the crashloop described here:\n\nhttps://github.com/kubernetes/kubernetes/issues/13102#issuecomment-134460647\n",
        "createdAt" : "2015-10-06T21:46:56Z",
        "updatedAt" : "2015-10-09T04:21:34Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa60bbe8e6b1b50bfc82f13a8c1e7ed059d8ea99",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +2465,2469 @@\tkl.networkConfigMutex.Lock()\n\tif kl.reconcileCIDR {\n\t\tkl.podCIDR = node.Spec.PodCIDR\n\t}\n\tkl.networkConfigMutex.Unlock()"
  },
  {
    "id" : "affb95e8-8f58-49f2-9052-206f76e4a82d",
    "prId" : 13571,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae6ab6fc-0fe2-4249-99cf-b96ee46694cf",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Our default logging level is set to `v2`. Should we lower this?\n",
        "createdAt" : "2015-11-11T23:55:59Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "12614304-046f-48f9-b5fe-32b8466751c4",
        "parentId" : "ae6ab6fc-0fe2-4249-99cf-b96ee46694cf",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Why do we want to lower this? I think it should be consistent with the logging level of other SyncLoop events (e.g., line 2123).\n",
        "createdAt" : "2015-11-12T19:18:00Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "706a7886-432b-4f63-8581-61ace87a1563",
        "parentId" : "ae6ab6fc-0fe2-4249-99cf-b96ee46694cf",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Do we need this information most of the times? I'd expect to see only useful logs with V(2). What do you think will be the frequency of this log? \n",
        "createdAt" : "2015-11-12T19:41:42Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "862eb8cd-20dd-4e6d-b93b-98237d50ab9a",
        "parentId" : "ae6ab6fc-0fe2-4249-99cf-b96ee46694cf",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "This would only happen if there is a container event, which would be useful to know IMO. I think it would be okay as V(3) though. How about I do a followup PR to change logging levels of all messages in this function?\n",
        "createdAt" : "2015-11-12T22:07:58Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "52044960-8511-4a79-88b3-a46689fae094",
        "parentId" : "ae6ab6fc-0fe2-4249-99cf-b96ee46694cf",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "SGTM.\n\nOn Thu, Nov 12, 2015 at 2:08 PM, Yu-Ju Hong notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go\n> https://github.com/kubernetes/kubernetes/pull/13571#discussion_r44722224\n> :\n> \n> > @@ -2112,6 +2132,25 @@ func (kl *Kubelet) syncLoopIteration(updates <-chan kubetypes.PodUpdate, handler\n> >             // TODO: Do we want to support this?\n> >             glog.Errorf(\"Kubelet does not support snapshot update\")\n> >         }\n> > -   case e := <-plegCh:\n> > -       // Filter out started events since we don't use them now.\n> > -       if e.Type == pleg.ContainerStarted {\n> > -           break\n> > -       }\n> > -       pod, ok := kl.podManager.GetPodByUID(e.ID)\n> > -       if !ok {\n> > -           // If the pod no longer exists, ignore the event.\n> > -           glog.V(4).Infof(\"SyncLoop (PLEG): ignore irrelevant event: %#v\", e)\n> > -           break\n> > -       }\n> > -       glog.V(2).Infof(\"SyncLoop (PLEG): %q, event: %#v\", kubeletutil.FormatPodName(pod), e)\n> \n> This would only happen if there is a container event, which would be\n> useful to know IMO. I think it would be okay as V(3) though. How about I do\n> a followup PR to change logging levels of all messages in this function?\n> \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/13571/files#r44722224.\n",
        "createdAt" : "2015-11-12T22:28:14Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac778e8203e1f6ea43555152c48f130063cc20c2",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +2193,2197 @@\t\t\tbreak\n\t\t}\n\t\tglog.V(2).Infof(\"SyncLoop (PLEG): %q, event: %#v\", kubeletutil.FormatPodName(pod), e)\n\t\t// Force the container runtime cache to update.\n\t\tif err := kl.runtimeCache.ForceUpdateIfOlder(time.Now()); err != nil {"
  },
  {
    "id" : "d1ce95b0-c0fd-43af-834b-06877b468cba",
    "prId" : 13571,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c247da87-7b4c-4c8b-8d38-419d3d6a68fa",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Why is this needed? Doesn't PLEG already handle relisting?\n",
        "createdAt" : "2015-11-11T23:56:43Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "462ad3b9-fb9d-4355-bc86-a30cb5755afd",
        "parentId" : "c247da87-7b4c-4c8b-8d38-419d3d6a68fa",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "PLGE doesn't populate this runtime cache shared by the pod workers. As mentioned in another comment, I plan to add a more general purpose cache that would eliminate the need of `docker inspect` as well. I can, of course, let PLEG populate this runtime cache first, but I feel like the benefits are marginal.\n",
        "createdAt" : "2015-11-12T19:20:56Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "e788e5d3-1866-4551-bf76-05b5805e2724",
        "parentId" : "c247da87-7b4c-4c8b-8d38-419d3d6a68fa",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Acknowledged!\n",
        "createdAt" : "2015-11-12T19:42:10Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac778e8203e1f6ea43555152c48f130063cc20c2",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +2195,2199 @@\t\tglog.V(2).Infof(\"SyncLoop (PLEG): %q, event: %#v\", kubeletutil.FormatPodName(pod), e)\n\t\t// Force the container runtime cache to update.\n\t\tif err := kl.runtimeCache.ForceUpdateIfOlder(time.Now()); err != nil {\n\t\t\tglog.Errorf(\"SyncLoop: unable to update runtime cache\")\n\t\t\t// TODO (yujuhong): should we delay the sync until container"
  },
  {
    "id" : "627c38b1-173c-4bed-ba9f-653ff6ac1e74",
    "prId" : 13571,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "facfc615-0d26-4d70-b521-b99a1d2425f2",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Can this be proportional to the machine capacity? \n",
        "createdAt" : "2015-11-12T19:26:58Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "8537e9f3-9354-476f-9865-fc5ca26713f0",
        "parentId" : "facfc615-0d26-4d70-b521-b99a1d2425f2",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Yes, it could be. I don't think there is much value of calibrating this now, but we can certainly do that later.\n",
        "createdAt" : "2015-11-12T19:59:54Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac778e8203e1f6ea43555152c48f130063cc20c2",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +115,119 @@\n\t// Capacity of the channel for recieving pod lifecycle events. This number\n\t// is a bit arbitrary and may be adjusted in the future.\n\tplegChannelCapacity = 1000\n"
  },
  {
    "id" : "9c4523fd-0a86-4063-9151-29ce6e9a3d59",
    "prId" : 13571,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "785b6571-fe8a-4eb3-b59f-6fd57ceef99d",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "This is an implementation detail. We can have a flag just for the stabilization phase maybe, but I'd prefer not cutting a release with the flag. \nMoreover, once we centralize all the pod interactions, the re-listing period should not be of concern right?\n",
        "createdAt" : "2015-11-12T19:28:59Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "bfc28ff8-082d-4d9f-9526-546b3a8a3aa4",
        "parentId" : "785b6571-fe8a-4eb3-b59f-6fd57ceef99d",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I am okay with not exposing this as a flag. It is a bit similar to the resyncInterval, which we expose now though. \n\nRelisting period will always be a concern unless we adopt docker/rkt event stream completely. But even then, the relisting period would affect container runtimes that don't have an event stream.\n",
        "createdAt" : "2015-11-12T20:03:02Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "ae13f9f5-5ed2-449a-af6f-53843d1246a0",
        "parentId" : "785b6571-fe8a-4eb3-b59f-6fd57ceef99d",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Removed the TODO. \n",
        "createdAt" : "2015-11-12T20:07:23Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "0a11316c-dee8-49e0-bd5d-3f805667f583",
        "parentId" : "785b6571-fe8a-4eb3-b59f-6fd57ceef99d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Why is it of concern? Can you elaborate? Once you add the specialized cache, are there any other issues ?\n",
        "createdAt" : "2015-11-12T20:07:52Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "86a172fd-4d5b-489a-9888-47dae219a03a",
        "parentId" : "785b6571-fe8a-4eb3-b59f-6fd57ceef99d",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Relisting is still required to populate the cache. The frequency of relisting affects how quick kubelet discovers container events (e.g., containers died recently). \n",
        "createdAt" : "2015-11-12T22:09:26Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "444a289c-7e2c-4dd7-98de-b7f457fcf9ef",
        "parentId" : "785b6571-fe8a-4eb3-b59f-6fd57ceef99d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Acknowledged!\n\nOn Thu, Nov 12, 2015 at 2:10 PM, Yu-Ju Hong notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go\n> https://github.com/kubernetes/kubernetes/pull/13571#discussion_r44722406\n> :\n> \n> > @@ -110,6 +111,18 @@ const (\n> >     housekeepingPeriod = time.Second \\* 2\n> > \n> > ```\n> > etcHostsPath = \"/etc/hosts\"\n> > ```\n> > \n> > +\n> > -   // Capacity of the channel for recieving pod lifecycle events. This number\n> > -   // is a bit arbitrary and may be adjusted in the future.\n> > -   plegChannelCapacity = 1000\n> >   +\n> > -   // TODO(yujuhong): migrate some of the parameters to kubelet flags.\n> > -   // Relisting is used to discover missing container events.\n> > -   // Use a shorter period because generic PLEG relies on relisting for\n> > -   // discovering container events.\n> > -   plegRelistPeriod = time.Second \\* 3\n> \n> Relisting is still required to populate the cache. The frequency of\n> relisting affects how quick kubelet discovers container events (e.g.,\n> containers died recently).\n> \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/13571/files#r44722406.\n",
        "createdAt" : "2015-11-12T22:28:24Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac778e8203e1f6ea43555152c48f130063cc20c2",
    "line" : null,
    "diffHunk" : "@@ -1,1 +120,124 @@\t// Generic PLEG relies on relisting for discovering container events.\n\t// The period directly affects the response time of kubelet.\n\tplegRelistPeriod = time.Second * 3\n\n\t// backOffPeriod is the period to back off when pod syncing resulting in an"
  },
  {
    "id" : "d704ad11-617c-4a53-a166-4fed6a9c93e8",
    "prId" : 13571,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34bcb6e8-fa54-46a7-a8b7-c14c32949860",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "nit: Document this value.\n",
        "createdAt" : "2015-11-12T19:29:30Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "318601d8-9a34-4954-9701-2dd95fb5b72d",
        "parentId" : "34bcb6e8-fa54-46a7-a8b7-c14c32949860",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Done.\n",
        "createdAt" : "2015-11-12T20:07:16Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac778e8203e1f6ea43555152c48f130063cc20c2",
    "line" : null,
    "diffHunk" : "@@ -1,1 +125,129 @@\t// error. It is also used as the base period for the exponential backoff\n\t// container restarts and image pulls.\n\tbackOffPeriod = time.Second * 10\n)\n"
  },
  {
    "id" : "11aad523-68b7-4f05-9409-f862b45f46f6",
    "prId" : 13571,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc0a572f-de55-407d-8894-360d7caa772d",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "We should drain the `plegCh` during every `syncLoopIteration` right? IIUC, this logic processes only one event as of now.\n",
        "createdAt" : "2015-11-12T19:38:37Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "5eedf825-cdf3-4501-aa7e-6ddddcd2fe05",
        "parentId" : "dc0a572f-de55-407d-8894-360d7caa772d",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "This sounds like an optimization. I'd prefer doing that later if you don't mind. Right now, it seems pretty fair to get only one event from the channel, when there are multiple channels in `select`. We can consider draining and batching some of them in the future.\n",
        "createdAt" : "2015-11-12T20:19:33Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "29065fad-5e8d-4854-91dd-2272340edaae",
        "parentId" : "dc0a572f-de55-407d-8894-360d7caa772d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I clarified the existing behavior offline with @yujuhong. No changes required. I take back my comment.\n",
        "createdAt" : "2015-11-12T20:40:00Z",
        "updatedAt" : "2015-11-13T17:57:35Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac778e8203e1f6ea43555152c48f130063cc20c2",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +2182,2186 @@\t\t\tglog.Errorf(\"Kubelet does not support snapshot update\")\n\t\t}\n\tcase e := <-plegCh:\n\t\t// Filter out started events since we don't use them now.\n\t\tif e.Type == pleg.ContainerStarted {"
  },
  {
    "id" : "af5485ee-3023-49ef-b60b-c96680f25627",
    "prId" : 13571,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bc98a5b-a9ce-41a7-b8df-a7a53c81e42e",
        "parentId" : null,
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Nit: receiving\n",
        "createdAt" : "2015-11-13T19:25:48Z",
        "updatedAt" : "2015-11-13T19:25:48Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac778e8203e1f6ea43555152c48f130063cc20c2",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +114,118 @@\tetcHostsPath = \"/etc/hosts\"\n\n\t// Capacity of the channel for recieving pod lifecycle events. This number\n\t// is a bit arbitrary and may be adjusted in the future.\n\tplegChannelCapacity = 1000"
  },
  {
    "id" : "d94ff176-ad04-44da-b451-dce8e1629590",
    "prId" : 13293,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf8ab585-75fa-4f0b-8996-931ed27a2489",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "nit: this is a little awkward, the remove can fail for multiple reasons but we seem to be using it as a signal that the volume plugin hasn't unmounted the device. Though it will have the intended effect of retrying till the plugin has. \n",
        "createdAt" : "2015-08-28T19:11:52Z",
        "updatedAt" : "2015-08-28T19:11:52Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "78744469-a3c9-4089-94e4-2ef8c4cc12ea",
        "parentId" : "bf8ab585-75fa-4f0b-8996-931ed27a2489",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Not that i know this code well enough to comment, just a random drive by nit\n",
        "createdAt" : "2015-08-28T19:12:39Z",
        "updatedAt" : "2015-08-28T19:12:39Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "2bfe5818-ad3b-4e57-8b69-4d8534b7fc50",
        "parentId" : "bf8ab585-75fa-4f0b-8996-931ed27a2489",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "It's used here as a signal that would stop kubelet from removing the pod directory, regardless of what the error is. The other alternative is to read the directory and figure out whether it's empty or not. It doesn't seem any simpler.\n",
        "createdAt" : "2015-08-28T21:24:24Z",
        "updatedAt" : "2015-08-28T21:24:24Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "63284af5-e4b5-41e7-9647-02b8c196ae15",
        "parentId" : "bf8ab585-75fa-4f0b-8996-931ed27a2489",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Agreed, not holding up this pr. \n\nWhat i meant was if you unmount a network device the directory will look empty whereas the rmdir syscall that os.remove invokes can returns all sorts of errors (http://man7.org/linux/man-pages/man2/rmdir.2.html).\n",
        "createdAt" : "2015-09-01T05:58:21Z",
        "updatedAt" : "2015-09-01T05:58:34Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "8774f6efa28a13ffa22b80abf7bdfd448f43381a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1355,1359 @@\t\t\tcontinue\n\t\t}\n\t\tif err := os.Remove(kl.getPodVolumesDir(uid)); err != nil {\n\t\t\t// If we cannot remove the volumes directory, it implies that the\n\t\t\t// the directory is not empty. We should wait for the volume"
  },
  {
    "id" : "bc0d79ff-6f9b-4721-8d6a-e1edf00d21a9",
    "prId" : 13103,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b1e71cb-4dfa-4dba-890c-defc434a6ef4",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Can we add extra labels to the docker container, to track re-start counts?\n",
        "createdAt" : "2015-08-24T20:02:55Z",
        "updatedAt" : "2015-08-24T20:02:55Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "e171181e-437c-4fad-b29b-89a33570739d",
        "parentId" : "6b1e71cb-4dfa-4dba-890c-defc434a6ef4",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Good point. Using labels would make restart count survive through kubelet restarts, if the container doesn't get removed. I looked at all the labels we create, and they seem to be more intrinsic to the container itself (e.g., pod/container name, graceful termination period, etc). Plus I am not sure about introducing dependency on getting the last restart count when creating a new container for the pod, as well as using docker as our checkpoint for status. What do you think?  \n",
        "createdAt" : "2015-08-24T20:19:24Z",
        "updatedAt" : "2015-08-24T20:19:24Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "a332d363-14ab-4c17-8d18-f6f3e093f00a",
        "parentId" : "6b1e71cb-4dfa-4dba-890c-defc434a6ef4",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I think it satisfies our use case. It takes care of persistency issues. Unless we have a technical reason against using labels, I'd say we should go ahead and use labels. \n",
        "createdAt" : "2015-08-24T20:24:41Z",
        "updatedAt" : "2015-08-24T20:24:41Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "ebf5aaa6-2db2-44c4-ae7e-91e7706da54d",
        "parentId" : "6b1e71cb-4dfa-4dba-890c-defc434a6ef4",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The thing is...kubelet has other persistence issues, and this is not a general solution :) We'd need to develop _some_ solution eventually. I will update the PR later with the label change.\n",
        "createdAt" : "2015-08-24T21:21:22Z",
        "updatedAt" : "2015-08-24T21:21:22Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "615ec5fd-8e83-41ee-b26f-316bb53111d9",
        "parentId" : "6b1e71cb-4dfa-4dba-890c-defc434a6ef4",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Agreed! Labels are probably not the solution for all persistency problems.\nLet's see how far we can get by with labels :) One more point - we will\nneed the existing logic, to handle backwards compat.\n\nOn Mon, Aug 24, 2015 at 2:21 PM, Yu-Ju Hong notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go\n> https://github.com/kubernetes/kubernetes/pull/13103#discussion_r37806368\n> :\n> \n> > @@ -2353,6 +2353,18 @@ func (kl *Kubelet) generatePodStatus(pod *api.Pod) (api.PodStatus, error) {\n> >     podFullName := kubecontainer.GetPodFullName(pod)\n> >     glog.V(3).Infof(\"Generating status for %q\", podFullName)\n> > -   if existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID); ok {\n> > -       // This is a hacky fix to ensure container restart counts increment\n> > -       // monotonically. Normally, we should not modify given pod. In this\n> > -       // case, we check if there are cached status for this pod, and update\n> > -       // the pod so that we update restart count appropriately.\n> > -       // TODO(yujuhong): We will not need to count dead containers every time\n> > -       // once we add the runtime pod cache.\n> > -       // Note that kubelet restarts may still cause temporarily setback of\n> \n> The thing is...kubelet has other persistence issues, and this is not a\n> general solution :) We'd need to develop _some_ solution eventually. I\n> will update the PR later with the label change.\n> \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/13103/files#r37806368.\n",
        "createdAt" : "2015-08-24T21:23:47Z",
        "updatedAt" : "2015-08-24T21:23:47Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "bb3ba1a5-f249-42d3-a184-e85b8c2f874a",
        "parentId" : "6b1e71cb-4dfa-4dba-890c-defc434a6ef4",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "> One more point - we will\n> need the existing logic, to handle backwards compat.\n\nHmm...this is broken in v1.0, so maybe we can get by without worrying about backward compatibility?\n",
        "createdAt" : "2015-08-26T01:15:20Z",
        "updatedAt" : "2015-08-26T01:15:20Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb3c8c5bcc96915e5464d5db14fee4d0d262401c",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2361,2365 @@\t\t// TODO(yujuhong): We will not need to count dead containers every time\n\t\t// once we add the runtime pod cache.\n\t\t// Note that kubelet restarts may still cause temporarily setback of\n\t\t// restart counts.\n\t\tpod.Status = existingStatus"
  },
  {
    "id" : "c0dea020-26f9-46bc-a638-da6118137382",
    "prId" : 12894,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb1a8c4e-b0f2-4a94-a022-d7e77ef8e0ca",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "This causes kubelet to send drastically more status messages! https://github.com/kubernetes/kubernetes/issues/14273#issuecomment-142451861\n",
        "createdAt" : "2015-09-22T23:35:41Z",
        "updatedAt" : "2015-09-22T23:35:41Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "6523ec142b31f82ad49b5ac46b11f01fc6661418",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +2137,2141 @@\t\tcondition.Status = api.ConditionFalse\n\t}\n\tcondition.LastProbeTime = currentTime\n\tcondition.Reason = reason\n\tcondition.Message = message"
  },
  {
    "id" : "1cd8385d-43c8-4eb6-8ee9-5ad842926c3c",
    "prId" : 12736,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29cc30f0-1f46-42e5-942c-9434976d713d",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "maybe just tolerate not found errors?\n",
        "createdAt" : "2015-08-14T17:10:21Z",
        "updatedAt" : "2015-08-14T17:10:21Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "9dedb955-25d5-4789-b1a9-0b125c051b5b",
        "parentId" : "29cc30f0-1f46-42e5-942c-9434976d713d",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> maybe just tolerate not found errors?\n\nThe real distinction being?  If it fails, syncPod will try again later, right?  Other types of errors should be transient or the whole kubelet is going to have trouble, right?\n",
        "createdAt" : "2015-08-14T17:14:38Z",
        "updatedAt" : "2015-08-14T17:14:38Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c24d176f6569113058385028ea820ca91fc2da32",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1287,1291 @@\t\tsecret, err := kl.kubeClient.Secrets(pod.Namespace).Get(secretRef.Name)\n\t\tif err != nil {\n\t\t\tglog.Warningf(\"Unable to retrieve pull secret %s/%s for %s/%s due to %v.  The image pull may not succeed.\", pod.Namespace, secretRef.Name, pod.Namespace, pod.Name, err)\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "8816f731-47e1-4145-b1c7-9edbfbca1cc1",
    "prId" : 12595,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d686b6a2-e2a2-423a-9bd0-84776d666451",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Is the reason this can't go into ExtractFieldPathAsString because that function is the same for all resource types? A comment here to that effect would be useful.\n",
        "createdAt" : "2015-08-12T22:45:24Z",
        "updatedAt" : "2015-08-14T21:51:29Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "978483a3-7b2c-41ad-95f1-004bcedd6c3a",
        "parentId" : "d686b6a2-e2a2-423a-9bd0-84776d666451",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Ok\n\nOn Aug 12, 2015, at 6:45 PM, Brian Grant notifications@github.com wrote:\n\nIn pkg/kubelet/kubelet.go\nhttps://github.com/kubernetes/kubernetes/pull/12595#discussion_r36923772:\n\n> @@ -1052,7 +1052,10 @@ func (kl *Kubelet) podFieldSelectorRuntimeValue(fs *api.ObjectFieldSelector, pod\n>   if err != nil {\n>       return \"\", err\n> \n> ##   }\n> - switch internalFieldPath {\n> - case \"status.podIP\":\n\nIs the reason this can't go into ExtractFieldPathAsString because that\nfunction is the same for all resource types? A comment here to that effect\nwould be useful.\n\n\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/12595/files#r36923772.\n",
        "createdAt" : "2015-08-13T02:26:50Z",
        "updatedAt" : "2015-08-14T21:51:29Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "01f378542694a7ca17bbd1fabea64b87b5f38132",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1053,1057 @@\t}\n\tswitch internalFieldPath {\n\tcase \"status.podIP\":\n\t\treturn pod.Status.PodIP, nil\n\t}"
  },
  {
    "id" : "de3c3c87-badf-456a-877f-df7818888d67",
    "prId" : 12513,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98ca930a-29d4-43bb-a66c-212a460116ca",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Should we do some validation on quantity value for some reasonable range? \n",
        "createdAt" : "2015-08-14T23:16:45Z",
        "updatedAt" : "2015-08-31T05:24:27Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "7903c991-21e5-44c9-8941-42ffc4231ea5",
        "parentId" : "98ca930a-29d4-43bb-a66c-212a460116ca",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done.\n",
        "createdAt" : "2015-08-17T21:37:26Z",
        "updatedAt" : "2015-08-31T05:24:27Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f3ef68ebc3afe285b2b6fffb01a9137ba3e2e91",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +2740,2744 @@\tstr, found := pod.Annotations[\"kubernetes.io/ingress-bandwidth\"]\n\tif found {\n\t\tif ingress, err = resource.ParseQuantity(str); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}"
  },
  {
    "id" : "3055642b-8dbd-4d13-993d-2ff79e5867f2",
    "prId" : 11955,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4cbc944-6440-4361-9665-67c64a602f58",
        "parentId" : null,
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "we should append only if pod.Spec.DNSPolicy == api.DNSClusterFirst\nwithout this check, we will incorrectly append cluster nameserver even if the pod did not select DNSClusterFirst\n",
        "createdAt" : "2015-08-05T21:22:29Z",
        "updatedAt" : "2015-08-26T22:43:58Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      },
      {
        "id" : "4a077536-d5e8-4c1e-83e4-c788abfe3ac6",
        "parentId" : "d4cbc944-6440-4361-9665-67c64a602f58",
        "authorId" : "652b00ac-f6a2-4590-87d7-c8207ef737dd",
        "body" : "Good catch. Should be fixed in the latest diff.\n",
        "createdAt" : "2015-08-05T22:56:10Z",
        "updatedAt" : "2015-08-26T22:43:58Z",
        "lastEditedBy" : "652b00ac-f6a2-4590-87d7-c8207ef737dd",
        "tags" : [
        ]
      }
    ],
    "commit" : "99b1da848d49b82f4299b2b0c862d021d2b2c77a",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +1128,1132 @@\n\tif kl.clusterDNS != nil {\n\t\tdns = append([]string{kl.clusterDNS.String()}, hostDNS...)\n\t} else {\n\t\tdns = hostDNS"
  },
  {
    "id" : "b17b04d6-a2b1-4df6-9fe2-3a219a70f844",
    "prId" : 11955,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5a41586-bf6a-403a-81b0-b0095cb6b06d",
        "parentId" : null,
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "we should append only if pod.Spec.DNSPolicy == api.DNSClusterFirst. \nwithout this check, we will incorrectly append cluster search paths even if the pod did not select DNSClusterFirst\n",
        "createdAt" : "2015-08-05T21:22:55Z",
        "updatedAt" : "2015-08-26T22:43:58Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      }
    ],
    "commit" : "99b1da848d49b82f4299b2b0c862d021d2b2c77a",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +1135,1139 @@\t\tnsSvcDomain := fmt.Sprintf(\"%s.svc.%s\", pod.Namespace, kl.clusterDomain)\n\t\tsvcDomain := fmt.Sprintf(\"svc.%s\", kl.clusterDomain)\n\t\tdnsSearch = append([]string{nsSvcDomain, svcDomain, kl.clusterDomain}, hostSearch...)\n\t} else {\n\t\tdnsSearch = hostSearch"
  },
  {
    "id" : "ff306e57-a4e0-4be9-ab3d-54d1647cfbac",
    "prId" : 10593,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82d71bcc-f28a-4a0d-87ce-73f5a24dc880",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Do we need a lock around this bool? If folks read the func doc they might call it simultaneously from multiple go routines. \n",
        "createdAt" : "2015-07-01T02:39:11Z",
        "updatedAt" : "2015-07-01T17:21:42Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "e03c3a9c-e6cc-4e19-a378-15c0a1c02a3d",
        "parentId" : "82d71bcc-f28a-4a0d-87ce-73f5a24dc880",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I will document.\n",
        "createdAt" : "2015-07-01T17:19:40Z",
        "updatedAt" : "2015-07-01T17:21:42Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "5911a12b14d3decd9913572a4264d63fa57b0ac9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +771,775 @@// not locked).\nfunc (kl *Kubelet) registerWithApiserver() {\n\tif kl.registrationCompleted {\n\t\treturn\n\t}"
  },
  {
    "id" : "8b9b3a60-3bb0-4adc-9a64-fecaf7f848c3",
    "prId" : 10593,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc470f56-26ea-4804-93a1-24af3aba5352",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "nit: Can you add a comment on why the method is safe to  be called multiple times, but not concurrently?\n",
        "createdAt" : "2015-07-01T18:14:39Z",
        "updatedAt" : "2015-07-01T18:14:39Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "cd43b256-8edb-4fce-9dfa-ef70b8298c25",
        "parentId" : "bc470f56-26ea-4804-93a1-24af3aba5352",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "It's already there `(kl.registrationCompleted is not locked)`\n",
        "createdAt" : "2015-07-01T19:54:13Z",
        "updatedAt" : "2015-07-01T19:54:13Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "5911a12b14d3decd9913572a4264d63fa57b0ac9",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +769,773 @@// registerWithApiserver registers the node with the cluster master. It is safe\n// to call multiple times, but not concurrently (kl.registrationCompleted is\n// not locked).\nfunc (kl *Kubelet) registerWithApiserver() {\n\tif kl.registrationCompleted {"
  },
  {
    "id" : "d7f857f6-725a-4b09-95e8-c6dd6e8b9da6",
    "prId" : 10313,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd868a3f-a867-4b81-acab-f685658802f2",
        "parentId" : null,
        "authorId" : null,
        "body" : "The kubelet code all looks reasonably ok to me barring my comments, but cc @dchen1107 for a second set of eyes, as she is much more familiar with this code than I am. \n",
        "createdAt" : "2015-06-26T00:08:55Z",
        "updatedAt" : "2015-07-01T23:25:08Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "a10a6a296e4efa90483febce5f6147b620e84e7b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1855,1859 @@// api.Pod, so only the fields that exist in both kubecontainer.Pod and\n// api.Pod are considered meaningful.\nfunc (kl *Kubelet) GetRunningPods() ([]*api.Pod, error) {\n\tpods, err := kl.runtimeCache.GetPods()\n\tif err != nil {"
  },
  {
    "id" : "afc6d983-e8ee-489a-97d0-12df4bee6e47",
    "prId" : 10066,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e190f986-a113-4d3b-8f9a-a2ef97498509",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "I think setPodStatus will already set the time if it's unset. \n\nThe impact here is extra qps, because we'll send one update for the time stamp and one update for the running in the defer call, but I'm hoping it won't be too bad. \n\nSo I actually wanted to do something a little different (just mentioning it here, not sure if you want to do it): Call SetPodStatus here with the actual pod.Status given to this method, and set the StartTime on that pod when we receive the watch notification (https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/config/config.go#L264 is close enough to the entry point). That would help our e2e tests gather latency numbers, but I'm not going to push too hard for it because it might shortchange the activeDeadline feature :)\n",
        "createdAt" : "2015-06-18T23:22:22Z",
        "updatedAt" : "2015-06-18T23:22:22Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "266cd36b-4f33-42b2-9817-fcd990ff00b0",
        "parentId" : "e190f986-a113-4d3b-8f9a-a2ef97498509",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Yes, setPodStatus does set the time if it's unset. The only reason I am doing it here is that I can reuse the initial timestamp: start. Before initializing PodStatus, kubelet does setup for pod, such as mount. Based on my previous experience, mount could be very slow operation taking many minutes under resource pressure, especially with memory.  \n\nI did consider to set StartTim on the watch notification initially. But like what you mentioned, I have concern with activeDeadline feature.\n",
        "createdAt" : "2015-06-19T00:23:59Z",
        "updatedAt" : "2015-06-19T00:23:59Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "9e001d1e-b6d6-4b4b-92e3-7de713079a58",
        "parentId" : "e190f986-a113-4d3b-8f9a-a2ef97498509",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Sounds good, guess we discussed the rest of it irl\n",
        "createdAt" : "2015-06-19T00:38:00Z",
        "updatedAt" : "2015-06-19T00:38:00Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "1145e4b80a2190d04cee2bb7ebc4f0bddb0aa53b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1172,1176 @@\tif updateType == SyncPodCreate {\n\t\tpodStatus = pod.Status\n\t\tpodStatus.StartTime = &util.Time{start}\n\t\tkl.statusManager.SetPodStatus(pod, podStatus)\n\t\tglog.V(3).Infof(\"Not generating pod status for new pod %q\", podFullName)"
  },
  {
    "id" : "0198dd05-cba1-4261-a23f-082f1b09c63d",
    "prId" : 10004,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e7933f6-0e17-4cdb-811b-56768b27c93e",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Do you want this method accessible by other package? made it lower-case?\n",
        "createdAt" : "2015-06-19T19:57:05Z",
        "updatedAt" : "2015-06-20T01:14:14Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "35569931794322924d61e59406d220c32e8634b6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1733,1737 @@}\n\nfunc (kl *Kubelet) LatestLoopEntryTime() time.Time {\n\tval := kl.syncLoopMonitor.Load()\n\tif val == nil {"
  },
  {
    "id" : "22f33fef-8c2d-43f6-a483-1333cd914c64",
    "prId" : 10004,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9110e012-bca4-440d-8982-7211da09af71",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Lower case too?\n",
        "createdAt" : "2015-06-19T19:57:19Z",
        "updatedAt" : "2015-06-20T01:14:14Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "3de3e46a-85a9-4994-844a-290e3370f611",
        "parentId" : "9110e012-bca4-440d-8982-7211da09af71",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "its used in an interface. It has to be exposed..\n",
        "createdAt" : "2015-06-20T01:08:30Z",
        "updatedAt" : "2015-06-20T01:14:14Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      }
    ],
    "commit" : "35569931794322924d61e59406d220c32e8634b6",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +2285,2289 @@}\n\nfunc (kl *Kubelet) ResyncInterval() time.Duration {\n\treturn kl.resyncInterval\n}"
  },
  {
    "id" : "813236cd-4570-478d-9b2c-5380ce2da378",
    "prId" : 10004,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e991b09-99b6-4f5a-800b-5844d0e2ab6a",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Why not use golang sync/atomic instead re-invent a new one?\n",
        "createdAt" : "2015-06-19T20:08:44Z",
        "updatedAt" : "2015-06-20T01:14:14Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "e714c2f6-e2b0-42e4-af07-3b005d46ba5a",
        "parentId" : "9e991b09-99b6-4f5a-800b-5844d0e2ab6a",
        "authorId" : "0970b119-085d-41b4-8f33-e10409965eba",
        "body" : "sync/atomic/Value does not exist in 1.3.\nTill we support 1.3, we cant use that go type.\n",
        "createdAt" : "2015-06-20T01:05:49Z",
        "updatedAt" : "2015-06-20T01:14:14Z",
        "lastEditedBy" : "0970b119-085d-41b4-8f33-e10409965eba",
        "tags" : [
        ]
      }
    ],
    "commit" : "35569931794322924d61e59406d220c32e8634b6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +495,499 @@\n\t// Monitor Kubelet's sync loop\n\tsyncLoopMonitor util.AtomicValue\n}\n"
  },
  {
    "id" : "856111db-d155-4505-8f65-51ce6ecabe1f",
    "prId" : 9549,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b406b970-fe0d-4404-bf26-a41b7ffbd340",
        "parentId" : null,
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "Let's kill the `else`, e.g.:\n\n``` go\nvar podStatus api.PodStatus\nvar err error\nif updateType != metrics.SyncPodCreate {\n    podStatus, err = kl.generatePodStatus(pod)\n    ...\n}\n```\n",
        "createdAt" : "2015-06-11T17:53:03Z",
        "updatedAt" : "2015-06-12T00:18:31Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "967434eb-f07a-4009-93a1-63eb7ad49b62",
        "parentId" : "b406b970-fe0d-4404-bf26-a41b7ffbd340",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "I wanted the logging so I kept the else \n",
        "createdAt" : "2015-06-11T20:02:41Z",
        "updatedAt" : "2015-06-12T00:18:31Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5ed0e9b139eaa725b842d021a6d55a7ed7d53d1",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1156,1160 @@\t\tpodStatus = pod.Status\n\t\tglog.V(3).Infof(\"Not generating pod status for new pod %v\", podFullName)\n\t} else {\n\t\tvar err error\n\t\tpodStatus, err = kl.generatePodStatus(pod)"
  },
  {
    "id" : "ded13a0b-217a-4ba8-8e6e-c6b7c5f60dd7",
    "prId" : 9265,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "641a846e-c68b-43fc-85aa-7aab849f7954",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Comment on why this is expoorted\n",
        "createdAt" : "2015-06-05T23:12:52Z",
        "updatedAt" : "2015-06-12T14:03:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "022ff5196d21fb24cf02fa1e571bb46884c0e571",
    "line" : null,
    "diffHunk" : "@@ -1,1 +2080,2084 @@\n// By passing the pod directly, this method avoids pod lookup, which requires\n// grabbing a lock.\nfunc (kl *Kubelet) generatePodStatus(pod *api.Pod) (api.PodStatus, error) {\n\tpodFullName := kubecontainer.GetPodFullName(pod)"
  },
  {
    "id" : "85671046-0ba8-49a0-815b-fcd5b95a446c",
    "prId" : 8681,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "034cab4d-e1b5-4c46-93aa-0b93810db299",
        "parentId" : null,
        "authorId" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "body" : "Add a comment that this can fail if devices hierarchy is not mounted.\n",
        "createdAt" : "2015-05-22T17:35:21Z",
        "updatedAt" : "2015-05-27T03:52:29Z",
        "lastEditedBy" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "tags" : [
        ]
      },
      {
        "id" : "cb7658a6-79a5-4e4a-9cca-71c02208061a",
        "parentId" : "034cab4d-e1b5-4c46-93aa-0b93810db299",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Will do, this makes it depend on #8586\n",
        "createdAt" : "2015-05-22T17:35:52Z",
        "updatedAt" : "2015-05-27T03:52:29Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "c97dda068dd96ab3ca759453a1e77791c98eb257",
    "line" : null,
    "diffHunk" : "@@ -1,1 +301,305 @@\t// Setup container manager, can fail if the devices hierarchy is not mounted\n\t// (it is required by Docker however).\n\tcontainerManager, err := newContainerManager(dockerDaemonContainer, systemContainer)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create the Container Manager: %v\", err)"
  },
  {
    "id" : "5effd21c-0313-461f-9e31-563595737edc",
    "prId" : 8582,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5f014d4-f913-438f-8558-90d958f08020",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I know this is outside the diff (sorry), but should one missing secret cause an error? worst case, the pull fails, right?\n",
        "createdAt" : "2015-05-20T19:29:34Z",
        "updatedAt" : "2015-05-22T18:05:19Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "fc12b54f-3b98-4427-af9f-68c79b6f8392",
        "parentId" : "b5f014d4-f913-438f-8558-90d958f08020",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I'm envisioning a service account with stale ImagePullSecret references... those would get appended to the pod's imagepullsecrets, which would then fail... not sure what failing in that case buys us.\n",
        "createdAt" : "2015-05-20T19:31:07Z",
        "updatedAt" : "2015-05-22T18:05:19Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "590bd048a5f8913d014aab1e7dcbddbf5f446f6c",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1099,1103 @@\tfor _, secretRef := range pod.Spec.ImagePullSecrets {\n\t\tsecret, err := kl.kubeClient.Secrets(pod.Namespace).Get(secretRef.Name)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}"
  },
  {
    "id" : "a007a77a-8931-4f42-a420-35f59acce10f",
    "prId" : 8149,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ab8774-108f-456d-a17a-c37014e531ae",
        "parentId" : null,
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "Is there any particular reason not to append this to the result right here, where it used to be? For readability? Because of concern about duplicated names?\n",
        "createdAt" : "2015-05-19T21:34:59Z",
        "updatedAt" : "2015-05-27T04:14:15Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "3ee896c3-743d-4222-81ab-ec37bc6fed0d",
        "parentId" : "f1ab8774-108f-456d-a17a-c37014e531ae",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I think you're right, we can write them into the result set as the values are determined -- there shouldn't be any duplicate names; those are guarded against with a validation.\n",
        "createdAt" : "2015-05-20T20:58:08Z",
        "updatedAt" : "2015-05-27T04:14:15Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "a3e344ee-90c1-4c5b-959b-77832b91abb4",
        "parentId" : "f1ab8774-108f-456d-a17a-c37014e531ae",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Fixed in next push\n",
        "createdAt" : "2015-05-22T22:13:34Z",
        "updatedAt" : "2015-05-27T04:14:15Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b338860aa740a05b3b27ce7c7aaef50d215b6d7",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +960,964 @@\t\t}\n\n\t\ttmpEnv[envVar.Name] = runtimeVal\n\t\tresult = append(result, kubecontainer.EnvVar{Name: envVar.Name, Value: tmpEnv[envVar.Name]})\n\t}"
  },
  {
    "id" : "65bd1251-fe02-4917-8ee3-e811567117f2",
    "prId" : 8108,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4fb5912-3eb1-4b26-9797-af749ff8fb16",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Should we log errors on both these cases?\n",
        "createdAt" : "2015-05-12T15:58:56Z",
        "updatedAt" : "2015-05-12T18:23:21Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "5e1d721d-64c2-4093-b92c-ff94efa3925b",
        "parentId" : "f4fb5912-3eb1-4b26-9797-af749ff8fb16",
        "authorId" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "body" : "Added an info log to disk-manager where we have more info. Info because it is not kubelet's error.\n",
        "createdAt" : "2015-05-12T17:27:56Z",
        "updatedAt" : "2015-05-12T18:23:21Z",
        "lastEditedBy" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2cf0dfb79d2bdd34369171059961d527bada12dd",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +1310,1314 @@\toutOfRootDisk := false\n\t// Check disk space once globally and reject or accept all new pods.\n\twithinBounds, err := kl.diskSpaceManager.IsDockerDiskSpaceAvailable()\n\t// Assume enough space in case of errors.\n\tif err == nil && !withinBounds {"
  },
  {
    "id" : "3bd33da4-c9a2-4696-9ada-de38ebdf620f",
    "prId" : 7984,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a76d5ebe-2f7d-4e55-8f26-1423e8cc9f25",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I don't see the \"gross shelling out\" or an implementation of ensureCbr0 in this PR. Did you miss part of the change?\n",
        "createdAt" : "2015-05-08T20:54:05Z",
        "updatedAt" : "2015-05-13T06:00:59Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "0c29be1a-95a5-4922-b4a7-8458cde4dd9a",
        "parentId" : "a76d5ebe-2f7d-4e55-8f26-1423e8cc9f25",
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "Oops. Forgot a file.\n",
        "createdAt" : "2015-05-08T20:58:16Z",
        "updatedAt" : "2015-05-13T06:00:59Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      }
    ],
    "commit" : "31ea7d12954126d5d9cb95011dec4bd7ed60fa06",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +1573,1577 @@\t// Set cbr0 interface address to first address in IPNet\n\tcidr.IP.To4()[3] += 1\n\tif err := ensureCbr0(cidr); err != nil {\n\t\treturn err\n\t}"
  },
  {
    "id" : "ee62c0f5-7e47-4202-8856-979d80e145d4",
    "prId" : 7984,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c2360d5-d7d2-4491-8aed-3b8f26f57ee9",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "What is it going to happen if configureCBR0 is false? In this PR, you removed the old way to configure cbr0 through configure-vm.sh, but only configure it through kubelet if the flag is true. Shouldn't configure-vm.sh need to check the same flag / env variable?\n",
        "createdAt" : "2015-05-12T22:20:57Z",
        "updatedAt" : "2015-05-13T06:00:59Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "ad6b3819-d7fb-4382-ae45-c668ca7ed653",
        "parentId" : "8c2360d5-d7d2-4491-8aed-3b8f26f57ee9",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "The intent is to let it be set to false for other node platforms (e.g. aws, vagrant, coreos, etc) but enable this unconditionally for gce/debian. \n",
        "createdAt" : "2015-05-12T22:30:11Z",
        "updatedAt" : "2015-05-13T06:00:59Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "31ea7d12954126d5d9cb95011dec4bd7ed60fa06",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1625,1629 @@\n\tnetworkConfigured := true\n\tif kl.configureCBR0 {\n\t\tif err := kl.reconcileCBR0(node.Spec.PodCIDR); err != nil {\n\t\t\tnetworkConfigured = false"
  },
  {
    "id" : "5a1012b9-6eae-42b4-b590-10812d519f36",
    "prId" : 7984,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0290ef87-9ad3-4f83-9cbb-3106488efd48",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "When configuring cbr0 failed, shouldn't we change NodeCondition to some bad state, at least NotReady state? So that master won't schedule work to the node? \n\ncc/ @bgrant0607 to add new NodeCondition for network configure issue. \n",
        "createdAt" : "2015-05-12T22:26:09Z",
        "updatedAt" : "2015-05-13T06:00:59Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "16496078-f84b-42b7-acb6-87adbe428fca",
        "parentId" : "0290ef87-9ad3-4f83-9cbb-3106488efd48",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "NotReady sounds reasonable to me. \n",
        "createdAt" : "2015-05-12T22:30:35Z",
        "updatedAt" : "2015-05-13T06:00:59Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "46482272-96b0-4c46-ab4d-27c8d9ed99a8",
        "parentId" : "0290ef87-9ad3-4f83-9cbb-3106488efd48",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "NotReady with a specific Reason and Message.\n",
        "createdAt" : "2015-05-12T23:07:39Z",
        "updatedAt" : "2015-05-13T06:00:59Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "31ea7d12954126d5d9cb95011dec4bd7ed60fa06",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1628,1632 @@\t\tif err := kl.reconcileCBR0(node.Spec.PodCIDR); err != nil {\n\t\t\tnetworkConfigured = false\n\t\t\tglog.Errorf(\"Error configuring cbr0: %v\", err)\n\t\t}\n\t}"
  },
  {
    "id" : "397c08cd-0ce5-4072-8a27-b8c04a429385",
    "prId" : 7974,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac3de7a0-37a3-4303-ba65-8977d4f5f24b",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Add a TODO here that duplicate secrets are being pulled and that we probably need a secret manager interface in the future.\n",
        "createdAt" : "2015-05-18T20:12:41Z",
        "updatedAt" : "2015-05-19T16:41:15Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "c004a23e-8cfb-4353-91f3-1acbc593c6ce",
        "parentId" : "ac3de7a0-37a3-4303-ba65-8977d4f5f24b",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> Add a TODO here that duplicate secrets are being pulled and that we probably need a secret manager interface in the future.\n\nTODO added on the `getPullSecretsForPod` method.\n",
        "createdAt" : "2015-05-18T20:19:05Z",
        "updatedAt" : "2015-05-19T16:41:15Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "72c0709f18cbeff1efef3c2bd601db5f1e117509",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1064,1068 @@\t}\n\n\tpullSecrets, err := kl.getPullSecretsForPod(pod)\n\tif err != nil {\n\t\tglog.Errorf(\"Unable to get pull secrets for pod %q (uid %q): %v\", podFullName, uid, err)"
  },
  {
    "id" : "1be89af9-0640-42e8-be83-50a5c3408d5f",
    "prId" : 7775,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6883ef49-03c5-45e4-91ea-fcd1f530e2b0",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "It shouldn't be difficult to use the local metadata server in the GCE implementation of this method (since it's only ever used from the local node). If you don't want to fix it as part of this PR, feel free to create an issue and assign it to me. \n",
        "createdAt" : "2015-05-27T21:01:51Z",
        "updatedAt" : "2015-05-28T17:08:38Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a41082ca87026d7ccc4f70293428fdd321443be",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +709,713 @@\t\t// TODO: We can't assume that the node has credentials to talk to the\n\t\t// cloudprovider from arbitrary nodes. At most, we should talk to a\n\t\t// local metadata server here.\n\t\tnode.Spec.ProviderID, err = cloudprovider.GetInstanceProviderID(kl.cloud, kl.hostname)\n\t\tif err != nil {"
  },
  {
    "id" : "27550d00-c923-4857-97e3-8724bce77de2",
    "prId" : 7775,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d2c6e9c-a0ea-4db3-b786-b2ebf3c0a9fd",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Do we not want/need to set the ProviderID when no cloud provider is specified? If ExternalID is deprecated and will be removed, then we are going to lose this \"else\" case once it is removed. \n",
        "createdAt" : "2015-05-27T21:02:58Z",
        "updatedAt" : "2015-05-28T17:08:38Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "b7083149-4795-4c35-9ffd-70a3b20d14d3",
        "parentId" : "7d2c6e9c-a0ea-4db3-b786-b2ebf3c0a9fd",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "That seems reasonable. none:///hostname.\n",
        "createdAt" : "2015-05-27T21:30:49Z",
        "updatedAt" : "2015-05-28T17:08:38Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "b0b34b48-0068-491a-a3f9-b90d323cf4cc",
        "parentId" : "7d2c6e9c-a0ea-4db3-b786-b2ebf3c0a9fd",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Or IP address, similar to Vagrant.\n\nReally, this case shouldn't exist. We should have a baremetal cloudprovider or somesuch.\n",
        "createdAt" : "2015-05-27T21:32:33Z",
        "updatedAt" : "2015-05-28T17:08:38Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "0eb26da0-362a-4287-bb34-ab2eb982ed4d",
        "parentId" : "7d2c6e9c-a0ea-4db3-b786-b2ebf3c0a9fd",
        "authorId" : "79abcd26-e388-4bae-92dd-9217be72eac3",
        "body" : "@roberthbailey @bgrant0607 I was trying to be more consistent than `ExternalID`: if there's no provider then there's no `ProviderID`. I was preferring an empty string rather than forging a `\"none://\" + kl.hostname` uri. In fact `node.Spec.ExternalID = kl.hostname` is also part of the original problem that I am trying to solve with this PR. So unless you really think I should do it I'd skip this suggestion. When we'll have a \"baremetal\" provider then this will be automatically filled it with the proper name and instance id.\n",
        "createdAt" : "2015-05-28T09:59:50Z",
        "updatedAt" : "2015-05-28T17:08:38Z",
        "lastEditedBy" : "79abcd26-e388-4bae-92dd-9217be72eac3",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a41082ca87026d7ccc4f70293428fdd321443be",
    "line" : null,
    "diffHunk" : "@@ -1,1 +715,719 @@\t\t}\n\t} else {\n\t\tnode.Spec.ExternalID = kl.hostname\n\t}\n\tif err := kl.setNodeStatus(node); err != nil {"
  },
  {
    "id" : "e6f89368-2772-40ca-b76a-5380f87b3da5",
    "prId" : 7763,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eaa3c19b-eb39-4eb8-a644-dfa06148b49e",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Looks like we don't actually call this anywhere in the Kubelet?\n",
        "createdAt" : "2015-05-06T16:16:47Z",
        "updatedAt" : "2015-05-07T09:20:41Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "f54651c6-d5d6-4583-b4d2-1ede821573ed",
        "parentId" : "eaa3c19b-eb39-4eb8-a644-dfa06148b49e",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Good catch! Thanks.\n",
        "createdAt" : "2015-05-06T17:01:33Z",
        "updatedAt" : "2015-05-07T09:20:41Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "e26da316dc86ab3db6bceaa97b6024adba7dd124",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +1457,1461 @@}\n\nfunc (kl *Kubelet) updateRuntimeUp() {\n\terr := waitUntilRuntimeIsUp(kl.containerRuntime, 100*time.Millisecond)\n\tkl.runtimeMutex.Lock()"
  },
  {
    "id" : "295e9326-83c1-4b8e-b16f-5f49e382ff7d",
    "prId" : 7659,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6dac3ecd-b136-483d-a9fb-7658ce3e586c",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Is it still necessary to get the pod status and perform various checks above? It feels like those could be redundant after your change.\n",
        "createdAt" : "2015-05-04T17:25:11Z",
        "updatedAt" : "2015-05-04T18:16:48Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "6fd9b23d-f77b-4a92-9d11-6ba73c0f229e",
        "parentId" : "6dac3ecd-b136-483d-a9fb-7658ce3e586c",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I'd be fine with adding a TODO to re-evaluate and remove the redundancy later, if it's preferred :)\n",
        "createdAt" : "2015-05-04T17:40:46Z",
        "updatedAt" : "2015-05-04T18:16:48Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "cec32b4d-4b40-43b8-ab96-078b56a252af",
        "parentId" : "6dac3ecd-b136-483d-a9fb-7658ce3e586c",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "We do today to get the containerID, I think we may be able to remove that need with using the container name. I'll add a TODO, it's not super clear to me what to do as of yet though.\n",
        "createdAt" : "2015-05-04T18:02:31Z",
        "updatedAt" : "2015-05-04T18:16:48Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "b670bfe8-aef8-4fa6-87f2-55b68127b101",
        "parentId" : "6dac3ecd-b136-483d-a9fb-7658ce3e586c",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "IMO, GetContainerLogs should simply find the containerID itself and return error if it's not found. Also, not relevant to this PR, but why can't GetContainerLogs take containerName as an argument -- this seems to fit well with our pod-level abstraction.\n",
        "createdAt" : "2015-05-04T18:10:39Z",
        "updatedAt" : "2015-05-04T18:16:48Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "85005bbf-9f7e-4e53-a378-49a076f50bb8",
        "parentId" : "6dac3ecd-b136-483d-a9fb-7658ce3e586c",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "+1 that's what the interface should be.\n",
        "createdAt" : "2015-05-04T18:17:50Z",
        "updatedAt" : "2015-05-04T18:17:50Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba7e940a575e64732108a8b7a7d21f85b5d952eb",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1385,1389 @@\t\treturn err\n\t}\n\tpod, ok := kl.GetPodByFullName(podFullName)\n\tif !ok {\n\t\treturn fmt.Errorf(\"unable to get logs for container %q in pod %q: unable to find pod\", containerName, podFullName)"
  },
  {
    "id" : "ccbc84bb-e452-4ad1-851b-a494d7b617c3",
    "prId" : 7586,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea1a976f-17a8-410b-9db5-72f3842869a2",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "There is also `cadvisor.GetContainerInfo()`, which seems like a more generic function. I briefly glanced the cadvisor code and it handles the namespaces differently. I think we are not ready to switch to this function yet. Please let me know otherwise :) \n",
        "createdAt" : "2015-04-30T20:22:02Z",
        "updatedAt" : "2015-04-30T20:37:07Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "74296a64-48fd-4192-947d-fe529a838c4a",
        "parentId" : "ea1a976f-17a8-410b-9db5-72f3842869a2",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Yeah the other function assumes full container names which Docker doesn't output (neither does rkt). We probably won't be able to do stats initially in rkt since we have no way to know what container the stats are from.\n",
        "createdAt" : "2015-04-30T20:34:17Z",
        "updatedAt" : "2015-04-30T20:37:07Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "644ec0dbfba5fe5f7c7cc1280714b026feaf34cc",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +1864,1868 @@\t}\n\n\tci, err := kl.cadvisor.DockerContainer(string(container.ID), req)\n\tif err != nil {\n\t\treturn nil, err"
  },
  {
    "id" : "ec707e4e-c95c-4bf9-8373-f3be2939187e",
    "prId" : 7401,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d09e8fb5-dd5b-465d-943e-102d77df10d3",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The difference here is that the error would not be reported upstream.  If we absolutely want to return all errors, we could aggregate the errors again before returning. I'm not sure if it's needed and I am okay with it, so I'll leave it up to you :) \n",
        "createdAt" : "2015-04-28T00:03:48Z",
        "updatedAt" : "2015-04-28T00:15:50Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "3df4b5cb-84d4-4bec-a431-0e9cc90d3811",
        "parentId" : "d09e8fb5-dd5b-465d-943e-102d77df10d3",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "You're right, we don't surface the error as we used to and this is a potentially important error. Fixing...\n",
        "createdAt" : "2015-04-28T00:11:48Z",
        "updatedAt" : "2015-04-28T00:15:50Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b0db76e85f4f8b3752a24b6095a5358340da6ec",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +934,938 @@\t\terr = kl.networkPlugin.TearDownPod(pod.Namespace, pod.Name, dockertools.DockerID(container.ID))\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Failed tearing down the network plugin for pod %q: %v\", pod.ID, err)\n\t\t\terrList = append(errList, err)\n\t\t}"
  },
  {
    "id" : "8ad9c30d-b1a0-4466-a536-d1e76a26cd4c",
    "prId" : 7401,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee0bf2eb-cb56-4e54-b4a3-30d5daa57339",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "How badly we inject networkPlugin to ContainerRuntime, so that PodInfraContainer can be completely handled by ContainerRuntime, such as docker runtime here? \n",
        "createdAt" : "2015-04-28T00:08:33Z",
        "updatedAt" : "2015-04-28T00:15:50Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "98f7f1ae-ef5c-4bad-ba07-ac5dea6919ec",
        "parentId" : "ee0bf2eb-cb56-4e54-b4a3-30d5daa57339",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "I tend to think that the network plugin is in the realm of the Kubelet and not the runtime. In the future the plugin may work with non-infra containers and it'll be better if it is handled here.\n\nHappy to change if you feel strongly though :)\n",
        "createdAt" : "2015-04-28T00:13:04Z",
        "updatedAt" : "2015-04-28T00:15:50Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "15bdd74a-c31b-4238-a62d-48f40243a03b",
        "parentId" : "ee0bf2eb-cb56-4e54-b4a3-30d5daa57339",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Hmmm we may have to go that route anyways since the start of the containers is also customized today. I will change in a followup PR.\n",
        "createdAt" : "2015-04-28T00:45:47Z",
        "updatedAt" : "2015-04-28T00:45:47Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "f3cf6541-f077-42e0-a296-5c92cd461dfd",
        "parentId" : "ee0bf2eb-cb56-4e54-b4a3-30d5daa57339",
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "Sounds good\n",
        "createdAt" : "2015-04-28T01:10:59Z",
        "updatedAt" : "2015-04-28T01:10:59Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b0db76e85f4f8b3752a24b6095a5358340da6ec",
    "line" : null,
    "diffHunk" : "@@ -1,1 +928,932 @@func (kl *Kubelet) killPod(pod kubecontainer.Pod) error {\n\t// TODO(vmarmol): Consider handling non-Docker runtimes, the plugins are not friendly to it today.\n\tcontainer, err := kl.containerManager.GetPodInfraContainer(pod)\n\terrList := []error{}\n\tif err == nil {"
  },
  {
    "id" : "8992fbfd-9350-4ed3-bcd6-422164c9fa70",
    "prId" : 7048,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "455b153d-46f2-40cd-a122-a2329f944c5c",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "This change seems unrelated from the rest of this PR. Is it required by it or can we split it out into its own logical comit/PR?\n",
        "createdAt" : "2015-04-20T15:48:10Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "8c3845d4-223f-4dbc-ab67-7f79984c8105",
        "parentId" : "455b153d-46f2-40cd-a122-a2329f944c5c",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I don't think it's unrelated. It was used as part of the logic to kill pods in SyncPods. I moved it here since I was going to resue this function. I am not sure why this function didn't call TearDownPod initially though.\n",
        "createdAt" : "2015-04-20T18:39:16Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "3a15550e-1151-4a2c-a255-64b5c7d7433f",
        "parentId" : "455b153d-46f2-40cd-a122-a2329f944c5c",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Ah I see what you mean.\n",
        "createdAt" : "2015-04-20T20:16:47Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "275002173e2d7c53b86965016fb0f180f92f8ec9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +950,954 @@\t\t\t// adapt to the generic container runtime.\n\t\t\tif container.Name == dockertools.PodInfraContainerName {\n\t\t\t\terr := kl.networkPlugin.TearDownPod(pod.Namespace, pod.Name, dockertools.DockerID(container.ID))\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Errorf(\"Failed tearing down the infra container: %v\", err)"
  },
  {
    "id" : "2a2a2563-d4ed-4f4f-bc20-c43085cad647",
    "prId" : 7048,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccfba68e-5580-4bcf-997e-70b68eb58208",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Lets break this into a different method, SyncPods() is huge already.\n",
        "createdAt" : "2015-04-20T15:51:57Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "766eac11-c18e-4092-a8c6-d1199debdcae",
        "parentId" : "ccfba68e-5580-4bcf-997e-70b68eb58208",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I moved the whole block (including result aggregation) to a different function. This should improve the readability a little bit. \n",
        "createdAt" : "2015-04-20T18:40:06Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "275002173e2d7c53b86965016fb0f180f92f8ec9",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1508,1512 @@\t\t}\n\t\tnumWorkers++\n\t\tgo func(pod *kubecontainer.Pod, ch chan result) {\n\t\t\tdefer func() {\n\t\t\t\t// Send the IDs of the containers that we failed to killed."
  },
  {
    "id" : "a7cf6417-eecc-402c-98dc-c3ec2efc9ca8",
    "prId" : 7048,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6d5ac5c-d60d-4bdf-871b-5bf286a693e4",
        "parentId" : null,
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "Thought not related to this PR, but I always have a question about this, I am not sure why the running containers are necessary for removing volumes. I suspect that if the volume is still being used by some container, TearDown() will just return an error (such as device busy).I think if we can remove this logic, it would be great.\n\nAs I can remember, this is part of a fix for volume leaks, but I am not sure the original problem is caused by deleting alive volumes..\n\nLet me find out the issue number...\n",
        "createdAt" : "2015-04-20T19:26:53Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "ac261a73-c3ad-46ba-b232-6f17c8d84dfd",
        "parentId" : "e6d5ac5c-d60d-4bdf-871b-5bf286a693e4",
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "#3965 #4076 #4086\n",
        "createdAt" : "2015-04-20T19:35:13Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "8da541b3-d057-4c1f-ba29-50debbcd1168",
        "parentId" : "e6d5ac5c-d60d-4bdf-871b-5bf286a693e4",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "I believe that in theory we should be able to remove it and the volume plugins would handle it. However, it seems like we've had quite a few bugs in this area which lead to data loss. That is what has kept the current logic I believe.\n",
        "createdAt" : "2015-04-20T20:40:48Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "993126e4-f9b5-4207-9c5f-537fb92093df",
        "parentId" : "e6d5ac5c-d60d-4bdf-871b-5bf286a693e4",
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "ack\n",
        "createdAt" : "2015-04-20T21:24:06Z",
        "updatedAt" : "2015-04-20T21:42:28Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      }
    ],
    "commit" : "275002173e2d7c53b86965016fb0f180f92f8ec9",
    "line" : 160,
    "diffHunk" : "@@ -1,1 +1549,1553 @@\t\tcontainerIDs[i] = string(c.ID)\n\t}\n\treturn kl.containerManager.GetRunningContainers(containerIDs)\n}\n"
  },
  {
    "id" : "bec65f8f-aa6c-4bc3-984d-554d039086b7",
    "prId" : 6949,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69b66dcd-ed59-4fc9-94d9-8b2024b12688",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Does this correctly handle the case where the node is already registered?\n",
        "createdAt" : "2015-04-18T00:24:56Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "8e5e7eb3-b6aa-4516-99e1-9e60237483bd",
        "parentId" : "69b66dcd-ed59-4fc9-94d9-8b2024b12688",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "No it doesn't. I thought of that in the shower this morning and was discussing it with @zmerlynn in regards to reboot/update but got sidetracked with meetings and didn't get around to fixing it. The fix shouldn't be difficult. Thanks for the reminder. \n",
        "createdAt" : "2015-04-18T00:26:30Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "426de7f4-025f-4244-afae-d7857343bfe9",
        "parentId" : "69b66dcd-ed59-4fc9-94d9-8b2024b12688",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "This now handles previously registered nodes. I did a reboot test and verified that the node switches from Ready -> NotReady -> Ready. \n",
        "createdAt" : "2015-04-26T15:09:15Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e356f84396642ecd5529ac5f4eb57e936e38922",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +730,734 @@\t\t}\n\t\tglog.V(2).Infof(\"Attempting to register node %s\", node.Name)\n\t\tif _, err := kl.kubeClient.Nodes().Create(node); err != nil {\n\t\t\tif apierrors.IsAlreadyExists(err) {\n\t\t\t\tcurrentNode, err := kl.kubeClient.Nodes().Get(kl.hostname)"
  },
  {
    "id" : "6cfc4a85-9741-41d6-9d02-68f48902d42d",
    "prId" : 6949,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fecbabab-c3ff-4681-bfbf-3e08eae6e186",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I don't think that using LegacyHostIP is a good way to go, as it'll be phased out eventually.\n",
        "createdAt" : "2015-04-28T14:16:35Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "ed4a5dc0-2c37-46f7-9066-9a88ddd1cd47",
        "parentId" : "fecbabab-c3ff-4681-bfbf-3e08eae6e186",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "@gmarek - @bgrant0607 said that we had to keep this behavior in until we remove the v1beta1/v1beta2 apis for backwards compatibility. It should be removed once the older API versions are phased out. \n",
        "createdAt" : "2015-04-28T14:43:00Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "477bd043-5fe1-4521-a44b-a848d5b21b46",
        "parentId" : "fecbabab-c3ff-4681-bfbf-3e08eae6e186",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "IIUC v1beta1 and v1beta2 are in the process of being removed. I'm not quite clear how advanced it is, but #8087 is there.\n",
        "createdAt" : "2015-05-15T08:39:50Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "f71d3773-a552-4dec-abfd-9f0d6b805eaf",
        "parentId" : "fecbabab-c3ff-4681-bfbf-3e08eae6e186",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I'm going to leave it in in the hope that my PR gets merged before v1beta1 is completely ripped out. \n",
        "createdAt" : "2015-05-15T23:49:09Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e356f84396642ecd5529ac5f4eb57e936e38922",
    "line" : 219,
    "diffHunk" : "@@ -1,1 +1797,1801 @@\t\taddr := net.ParseIP(kl.hostname)\n\t\tif addr != nil {\n\t\t\tnode.Status.Addresses = []api.NodeAddress{{Type: api.NodeLegacyHostIP, Address: addr.String()}}\n\t\t} else {\n\t\t\taddrs, err := net.LookupIP(node.Name)"
  },
  {
    "id" : "a5b80a3c-2490-474c-b437-791f0a337640",
    "prId" : 6949,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "@vishh @vmarmol Can you help answer the question I put in this TODO? Should I put a default capacity here (which is what test-cmd.sh expects) or should we not be picking a random capacity if cadvisor doesn't let us know what the node capacity is (e.g. update the test to stop checking for this)? It would be better if cadvisor just worked in test-cmd.sh but until then we need to decide which way to go here. Thanks. \n",
        "createdAt" : "2015-05-15T04:07:36Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "64bf374a-c423-4482-887d-06731601a5b5",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "IMO if cAdvisor is the only way for us to get capacity for the node, then we should report zero capacity until the information is available.\n",
        "createdAt" : "2015-05-15T07:04:33Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "e1c51d38-f7fa-474e-b01a-226eb084ea2c",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "+1 to what @davidopp wrote.\n",
        "createdAt" : "2015-05-15T08:36:32Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "138db661-aaef-4a47-8f40-ab3b9a271d95",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "+1 to a zero capacity being a sane default. cAdvisor should work in test-cmd in Linux builds (unless we're injecting a fake).\n",
        "createdAt" : "2015-05-15T16:38:33Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "a42241c1-d8e8-45f9-8a4d-88765678baaf",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Does this remove the ability of the administrator to set arbitrary capacity to a node via the API (if the kubelet always writes 0)?\n\n----- Original Message -----\n\n> > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error {\n> >     // cAdvisor locally, e.g. for test-cmd.sh, and in integration test.\n> >     info, err := kl.GetCachedMachineInfo()\n> >     if err != nil {\n> > -       glog.Errorf(\"error getting machine info: %v\", err)\n> > -       // TODO(roberthbailey): This is required for test-cmd.sh to pass.\n> \n> +1 to a zero capacity being a sane default. cAdvisor should work in test-cmd\n> in Linux builds (unless we're injecting a fake).\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30424260\n",
        "createdAt" : "2015-05-15T16:51:39Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "66bbb656-40ea-4da8-a74e-663b8116ba1d",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "@smarterclayton, that exists?\n\nOn Fri, May 15, 2015 at 9:51 AM, Clayton Coleman notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949#discussion_r30425249\n> :\n> \n> > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error {\n> >     // cAdvisor locally, e.g. for test-cmd.sh, and in integration test.\n> >     info, err := kl.GetCachedMachineInfo()\n> >     if err != nil {\n> > -       glog.Errorf(\"error getting machine info: %v\", err)\n> > -       // TODO(roberthbailey): This is required for test-cmd.sh to pass.\n> \n> Does this remove the ability of the administrator to set arbitrary\n> capacity to a node via the API (if the kubelet always writes 0)?\n>  <#14d587d3cb08fee9_>\n> ----- Original Message -----\n> \n> > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error\n> > { > // cAdvisor locally, e.g. for test-cmd.sh, and in integration test. >\n> > info, err := kl.GetCachedMachineInfo() > if err != nil { > -\n> > glog.Errorf(\"error getting machine info: %v\", err) > + //\n> > TODO(roberthbailey): This is required for test-cmd.sh to pass. +1 to a zero\n> > capacity being a sane default. cAdvisor should work in test-cmd in Linux\n> > builds (unless we're injecting a fake). --- Reply to this email directly or\n> > view it on GitHub:\n> > https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30424260\n> \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30425249\n> .\n",
        "createdAt" : "2015-05-15T17:02:13Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "a9002497-0bfb-481f-9d34-d85353b3508b",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Anyone can call the node api.  If cadvisor can't report anything, and the node already has something, should the node overwrite the capacity?  I would argue it should set zero if nothing is set, but if it's explicitly set (perhaps from the last time) it shouldn't be reset.  Not a massively strong feeling, but if cadvisor is dead I don't know that we should be making the node completely unschedulable (by forcing capacity to 0)\n",
        "createdAt" : "2015-05-15T17:08:10Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "efa3e949-d9c4-4b52-bb5a-065316957c4d",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "That sounds reasonable and it'll allow us to keep data we knew to be true.\n\nIt is important to note that under normal circumstances cAdvisor is\nrunning, I have yet to see a situation in production where it is not. These\nedge cases happen in testing where we mock or stub out different parts of\nthe Kubelet.\n\nOn Fri, May 15, 2015 at 10:08 AM, Clayton Coleman notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949#discussion_r30426565\n> :\n> \n> > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error {\n> >     // cAdvisor locally, e.g. for test-cmd.sh, and in integration test.\n> >     info, err := kl.GetCachedMachineInfo()\n> >     if err != nil {\n> > -       glog.Errorf(\"error getting machine info: %v\", err)\n> > -       // TODO(roberthbailey): This is required for test-cmd.sh to pass.\n> \n> Anyone can call the node api. If cadvisor can't report anything, and the\n> node already has something, should the node overwrite the capacity? I would\n> argue it should set zero if nothing is set, but if it's explicitly set\n> (perhaps from the last time) it shouldn't be reset. Not a massively strong\n> feeling, but if cadvisor is dead I don't know that we should be making the\n> node completely unschedulable (by forcing capacity to 0)\n> \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30426565\n> .\n",
        "createdAt" : "2015-05-15T17:14:34Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "12bacb74-a9df-4e7e-920a-f097c3a07603",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "If we ever split cadvisor out into its own process we'd potentially have problems with dead cadvisor.  Was there any plan for that as an option?\n\n----- Original Message -----\n\n> > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error {\n> >     // cAdvisor locally, e.g. for test-cmd.sh, and in integration test.\n> >     info, err := kl.GetCachedMachineInfo()\n> >     if err != nil {\n> > -       glog.Errorf(\"error getting machine info: %v\", err)\n> > -       // TODO(roberthbailey): This is required for test-cmd.sh to pass.\n> \n> That sounds reasonable and it'll allow us to keep data we knew to be true.\n> \n> It is important to note that under normal circumstances cAdvisor is\n> running, I have yet to see a situation in production where it is not. These\n> edge cases happen in testing where we mock or stub out different parts of\n> the Kubelet.\n> \n> On Fri, May 15, 2015 at 10:08 AM, Clayton Coleman notifications@github.com\n> wrote:\n> \n> > In pkg/kubelet/kubelet.go\n> > https://github.com/GoogleCloudPlatform/kubernetes/pull/6949#discussion_r30426565\n> > :\n> > \n> > > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error {\n> > >   // cAdvisor locally, e.g. for test-cmd.sh, and in integration test.\n> > >   info, err := kl.GetCachedMachineInfo()\n> > >   if err != nil {\n> > > -     glog.Errorf(\"error getting machine info: %v\", err)\n> > > -     // TODO(roberthbailey): This is required for test-cmd.sh to pass.\n> > \n> > Anyone can call the node api. If cadvisor can't report anything, and the\n> > node already has something, should the node overwrite the capacity? I would\n> > argue it should set zero if nothing is set, but if it's explicitly set\n> > (perhaps from the last time) it shouldn't be reset. Not a massively strong\n> > feeling, but if cadvisor is dead I don't know that we should be making the\n> > node completely unschedulable (by forcing capacity to 0)\n> > \n> > \n> > Reply to this email directly or view it on GitHub\n> > https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30426565\n> > .\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30427038\n",
        "createdAt" : "2015-05-15T18:37:06Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "71b6cbfa-1c9b-45f7-8e07-008dd555879b",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "We never got that far AFAIK. Avoiding some of that was why we merged the\ntwo.\n\nOn Fri, May 15, 2015 at 11:37 AM, Clayton Coleman notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949#discussion_r30434044\n> :\n> \n> > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error {\n> >     // cAdvisor locally, e.g. for test-cmd.sh, and in integration test.\n> >     info, err := kl.GetCachedMachineInfo()\n> >     if err != nil {\n> > -       glog.Errorf(\"error getting machine info: %v\", err)\n> > -       // TODO(roberthbailey): This is required for test-cmd.sh to pass.\n> \n> If we ever split cadvisor out into its own process we'd potentially have\n> problems with dead cadvisor. Was there any plan for that as an option?\n>  <#14d58ddd22a4fbf6_>\n> ----- Original Message -----\n> \n> > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus() error\n> > { > // cAdvisor locally, e.g. for test-cmd.sh, and in integration test. >\n> > info, err := kl.GetCachedMachineInfo() > if err != nil { > -\n> > glog.Errorf(\"error getting machine info: %v\", err) > + //\n> > TODO(roberthbailey): This is required for test-cmd.sh to pass. That sounds\n> > reasonable and it'll allow us to keep data we knew to be true. It is\n> > important to note that under normal circumstances cAdvisor is running, I\n> > have yet to see a situation in production where it is not. These edge cases\n> > happen in testing where we mock or stub out different parts of the Kubelet.\n> > On Fri, May 15, 2015 at 10:08 AM, Clayton Coleman <\n> > notifications@github.com> wrote: > In pkg/kubelet/kubelet.go > <\n> > https://github.com/GoogleCloudPlatform/kubernetes/pull/6949#discussion_r30426565>\n> > : > > > @@ -1668,11 +1753,18 @@ func (kl *Kubelet) tryUpdateNodeStatus()\n> > error { > > // cAdvisor locally, e.g. for test-cmd.sh, and in integration\n> > test. > > info, err := kl.GetCachedMachineInfo() > > if err != nil { > > -\n> > glog.Errorf(\"error getting machine info: %v\", err) > > + //\n> > TODO(roberthbailey): This is required for test-cmd.sh to pass. > > Anyone\n> > can call the node api. If cadvisor can't report anything, and the > node\n> > already has something, should the node overwrite the capacity? I would >\n> > argue it should set zero if nothing is set, but if it's explicitly set >\n> > (perhaps from the last time) it shouldn't be reset. Not a massively strong\n> > feeling, but if cadvisor is dead I don't know that we should be making\n> > the > node completely unschedulable (by forcing capacity to 0) > >  >\n> > Reply to this email directly or view it on GitHub > <\n> > https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30426565>\n> > . > --- Reply to this email directly or view it on GitHub:\n> > https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30427038\n> \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/6949/files#r30434044\n> .\n",
        "createdAt" : "2015-05-15T18:42:51Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "46ac86b9-664b-4a2c-8f37-3b60048c6559",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "If cAdvisor is integral to the system (which it seems it is), maybe we should refuse to register a node if cAdvisor is down (except in tests when it can't be run, I guess), and if it dies later for some amount of time we should report NodeReady==false? \n",
        "createdAt" : "2015-05-15T19:58:30Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "44a44501-2010-4ced-aa39-596abb6a15f4",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Would folks be ok if we set the capacity to zero for the sake of this PR and filed an issue to continue the discussion?\n",
        "createdAt" : "2015-05-15T23:27:19Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "b14b5522-2ef3-4497-80b2-32247d959fad",
        "parentId" : "5a81599b-466b-4a03-b044-ec389454aa51",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "Filed https://github.com/GoogleCloudPlatform/kubernetes/issues/8361\n",
        "createdAt" : "2015-05-15T23:48:44Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e356f84396642ecd5529ac5f4eb57e936e38922",
    "line" : 238,
    "diffHunk" : "@@ -1,1 +1822,1826 @@\tinfo, err := kl.GetCachedMachineInfo()\n\tif err != nil {\n\t\t// TODO(roberthbailey): This is required for test-cmd.sh to pass.\n\t\t// See if the test should be updated instead.\n\t\tnode.Status.Capacity = api.ResourceList{"
  },
  {
    "id" : "eeac5032-d35b-45c2-8a04-ba98d64ca8f3",
    "prId" : 6949,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0414654-c203-48f7-b58a-6a776df286c7",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "What does it mean if we get here and these are != ? Should we delete and re-register the node instead of just returning?\n",
        "createdAt" : "2015-05-15T19:56:16Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "c7742d33-f15f-485a-b1be-57e4ac002ddb",
        "parentId" : "a0414654-c203-48f7-b58a-6a776df286c7",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "If they aren't equal we don't return, we loop forever. This check is basically making sure that both the node name and the external id match for the existing entry to be the same as this node. \n",
        "createdAt" : "2015-05-15T23:50:31Z",
        "updatedAt" : "2015-05-19T16:55:22Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e356f84396642ecd5529ac5f4eb57e936e38922",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +741,745 @@\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif currentNode.Spec.ExternalID == node.Spec.ExternalID {\n\t\t\t\t\tglog.Infof(\"Node %s was previously registered\", node.Name)\n\t\t\t\t\treturn"
  },
  {
    "id" : "0333a419-bf38-4708-8727-d95453efd976",
    "prId" : 6607,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e9d5174-3f88-4a4c-beea-3ef5317f99e3",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "WDYT of setting mirrorPod = nil so we re-create it below?\n",
        "createdAt" : "2015-04-09T00:36:36Z",
        "updatedAt" : "2015-04-09T01:06:36Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "978dceb2-7bb7-43cf-8e84-f7ce740c2506",
        "parentId" : "8e9d5174-3f88-4a4c-beea-3ef5317f99e3",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "That's what I did initially, but I changed it back because \n- We don't retry deletes/creates; either one of them could fail.\n- There could be watch delay for receiving pods.\n  The combination of the two could lead to kubelet deleting the re-created mirror pod, or something even worse. Instead of working out all the corner cases, I opted to do this in a cleaner way: wait until we know the pod is not there, then recreate. Does that sound reasonable? :)\n",
        "createdAt" : "2015-04-09T01:04:29Z",
        "updatedAt" : "2015-04-09T01:07:58Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "0d9ed6f0-d6cf-4321-bea1-48a7dcddb9c1",
        "parentId" : "8e9d5174-3f88-4a4c-beea-3ef5317f99e3",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "SGTM :)\n",
        "createdAt" : "2015-04-09T01:06:28Z",
        "updatedAt" : "2015-04-09T01:06:36Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7cf294c99242cac2fbb73ba24f4b4cfff46f9c1",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1340,1344 @@\t\t\tif err := kl.podManager.DeleteMirrorPod(podFullName); err != nil {\n\t\t\t\tglog.Errorf(\"Failed deleting mirror pod %q: %v\", podFullName, err)\n\t\t\t}\n\t\t}\n\t\tif mirrorPod == nil {"
  },
  {
    "id" : "b6ac9237-7b0e-4645-9193-9c75975ae80e",
    "prId" : 6595,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d13c909d-9f63-4606-ba84-cc216b8e5624",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Could you add a logging message here?\n",
        "createdAt" : "2015-04-08T23:28:35Z",
        "updatedAt" : "2015-04-11T00:30:19Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "3932dfd8bbdf0809fad5ec7331b7ca364d455f2a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1634,1638 @@\t\tselect {\n\t\tcase u, ok := <-updates:\n\t\t\tif !ok {\n\t\t\t\tglog.Errorf(\"Update channel is closed. Exiting the sync loop.\")\n\t\t\t\treturn"
  },
  {
    "id" : "7a259cb8-2a14-44fa-9ba1-34169ca1c682",
    "prId" : 6322,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78ef7e64-db5b-4471-b2ca-fc5d26acf0dd",
        "parentId" : null,
        "authorId" : "8076e56f-768c-4f89-90e6-045ef34e1525",
        "body" : "Have we updated the build script/makefile so we push this to gcr.io when updated? Also, I haven't seen the 0.8.0 tag, is that one you pushed?\n",
        "createdAt" : "2015-04-01T23:23:48Z",
        "updatedAt" : "2015-04-01T23:23:48Z",
        "lastEditedBy" : "8076e56f-768c-4f89-90e6-045ef34e1525",
        "tags" : [
        ]
      },
      {
        "id" : "da3b7bb6-7aa2-41fc-827d-464b595644c1",
        "parentId" : "78ef7e64-db5b-4471-b2ca-fc5d26acf0dd",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "pause image was built and pushed through build/pause/Makefile. I did this yesterday.\n\nWith this change, I just did a test\n\n```\nroot@kubernetes-minion-4ztj:/var/log# docker ps \nCONTAINER ID        IMAGE                                                COMMAND                CREATED             STATUS              PORTS               NAMES\n843d8ba08368        gcr.io/google_containers/pause:0.8.0                 \"/pause\"               5 minutes ago       Up 5 minutes                            k8s_POD.1a1fe303_valid-pod_default_f70a30fc-d8c3-11e4-8e5e-42010af00363_9ec7e923                                            \n```\n",
        "createdAt" : "2015-04-01T23:27:33Z",
        "updatedAt" : "2015-04-01T23:27:33Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "2064a988-7cee-45a1-a091-5c614182ee3f",
        "parentId" : "78ef7e64-db5b-4471-b2ca-fc5d26acf0dd",
        "authorId" : "8076e56f-768c-4f89-90e6-045ef34e1525",
        "body" : "Exellent, thanks!\n",
        "createdAt" : "2015-04-01T23:28:23Z",
        "updatedAt" : "2015-04-01T23:28:23Z",
        "lastEditedBy" : "8076e56f-768c-4f89-90e6-045ef34e1525",
        "tags" : [
        ]
      },
      {
        "id" : "b858c88a-1872-49de-96c4-114e158202cf",
        "parentId" : "78ef7e64-db5b-4471-b2ca-fc5d26acf0dd",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Wait...\nNot sure what changed since yesterday afternoon, but when I manually docker pull this image, I got message as the following:\n\n```\n$ docker pull gcr.io/google_containers/pause:0.8.0\nPulling repository gcr.io/google_containers/pause\n2015/04/01 16:28:38 Authentication is required.\n```\n",
        "createdAt" : "2015-04-01T23:30:32Z",
        "updatedAt" : "2015-04-01T23:30:32Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "db28ca67f60ac74b43977b756b8788d36d560404",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +859,863 @@\nconst (\n\tPodInfraContainerImage = \"gcr.io/google_containers/pause:0.8.0\"\n)\n"
  },
  {
    "id" : "a0a7ccbc-1da4-4017-93fa-49b8229a6869",
    "prId" : 6054,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84b767cb-8120-4093-b933-008c207c774f",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "nit: Can specify for what pod?\n",
        "createdAt" : "2015-03-27T15:07:10Z",
        "updatedAt" : "2015-03-27T15:07:10Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe2f7f25c68c0556489a5ecfba60ed0881402a79",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1925,1929 @@\thostIP, err := kl.GetHostIP()\n\tif err != nil {\n\t\tglog.Errorf(\"Cannot get host IP: %v\", err)\n\t} else {\n\t\tpodStatus.HostIP = hostIP.String()"
  },
  {
    "id" : "46e67970-cfab-43b0-a0f4-ce525dae358d",
    "prId" : 5995,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc624657-169a-4270-9607-35a3d9658bf9",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "We already have a killContainersInPod() we can re-use I think.\n",
        "createdAt" : "2015-03-26T17:17:51Z",
        "updatedAt" : "2015-03-26T17:17:51Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "b7a0b4f1-23c0-4e3c-9b10-5744a61a5ef3",
        "parentId" : "fc624657-169a-4270-9607-35a3d9658bf9",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Spoke offline, we'll keep for now since the behavior is not the same.\n",
        "createdAt" : "2015-03-26T18:03:37Z",
        "updatedAt" : "2015-03-26T18:03:37Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c42070eeba44ec7cd8c13cd32822880f4ec4a21",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1388,1392 @@\terr := kl.canRunPod(pod)\n\tif err != nil {\n\t\tkl.killContainersInRunningPod(runningPod)\n\t\treturn err\n\t}"
  },
  {
    "id" : "635fb40d-2995-4e09-afc9-dc036fd16ad9",
    "prId" : 5975,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d7ec0d5-f92a-4c1e-af20-bfdf5cb31a45",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "nit: can we document the outputs :)\n",
        "createdAt" : "2015-03-26T16:06:07Z",
        "updatedAt" : "2015-03-26T20:43:51Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f08555f457ac7ed8396e915e8c4d41a55e6539b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +754,758 @@// getClusterDNS returns a list of the DNS servers and a list of the DNS search\n// domains of the cluster.\nfunc (kl *Kubelet) getClusterDNS(pod *api.Pod) ([]string, []string, error) {\n\t// Get host DNS settings and append them to cluster DNS settings.\n\tf, err := os.Open(\"/etc/resolv.conf\")"
  },
  {
    "id" : "43b1aafd-328b-4806-883a-7ff57876e720",
    "prId" : 5928,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aaa8c81a-cbf2-4b72-8f76-d2bc3438db2e",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Isn't this a no-op?\n",
        "createdAt" : "2015-03-25T16:42:17Z",
        "updatedAt" : "2015-03-25T16:58:58Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "2143352e-c010-4af4-aea6-e54f80418a79",
        "parentId" : "aaa8c81a-cbf2-4b72-8f76-d2bc3438db2e",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Did you mean to set it to mirrorPod?\n",
        "createdAt" : "2015-03-25T16:42:38Z",
        "updatedAt" : "2015-03-25T16:58:58Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "fa0a367d-2d74-4040-9ef8-0f8a00cb5a1f",
        "parentId" : "aaa8c81a-cbf2-4b72-8f76-d2bc3438db2e",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Yes, my mistake.\n\nI think we don't test the pod status syncing properly. All the integration tests get pod status from the apiserver, and apiserver queries kubelet if the status does not exist. I guess this would be improved after we remove the pod cache... \n",
        "createdAt" : "2015-03-25T17:03:42Z",
        "updatedAt" : "2015-03-25T17:03:42Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4b0bc75c4cd19eae25d5025d7e095f908e256b9",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1346,1350 @@\t\t} else {\n\t\t\tpodToUpdate := pod\n\t\t\tif mirrorPod != nil {\n\t\t\t\tpodToUpdate = mirrorPod\n\t\t\t}"
  },
  {
    "id" : "da42d740-dec1-448f-bdd6-dfb979f7e4f2",
    "prId" : 5830,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/5830#pullrequestreview-23278001",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71217bf9-2a72-4b85-b3a7-02bf90c54275",
        "parentId" : null,
        "authorId" : "eb6117db-dc1a-4e53-bf6d-061dfbf21dc7",
        "body" : "coff coff... any follow up on this? :) @vmarmol ?",
        "createdAt" : "2017-02-22T12:14:47Z",
        "updatedAt" : "2017-02-22T12:14:47Z",
        "lastEditedBy" : "eb6117db-dc1a-4e53-bf6d-061dfbf21dc7",
        "tags" : [
        ]
      },
      {
        "id" : "48ac5f5f-e5da-4630-affd-98b2ed6bcda3",
        "parentId" : "71217bf9-2a72-4b85-b3a7-02bf90c54275",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Nope, but feel free to upload a PR if you have any ideas. ",
        "createdAt" : "2017-02-22T17:18:55Z",
        "updatedAt" : "2017-02-22T17:18:55Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "07f928be644568420d194c825387f9798565f8b1",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +766,770 @@\texposedPorts, portBindings := makePortsAndBindings(container)\n\n\t// TODO(vmarmol): Handle better.\n\t// Cap hostname at 63 chars (specification is 64bytes which is 63 chars and the null terminating char).\n\tconst hostnameMaxLen = 63"
  },
  {
    "id" : "43645acc-c871-4302-a4c1-4bbbbe62dd3e",
    "prId" : 5714,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec3479af-49f9-4496-8912-021d87bddb1a",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Eventually we should subsume this into the new status object too. Not for this PR though :) there is some more refactoring needed before that I think.\n",
        "createdAt" : "2015-03-20T16:56:04Z",
        "updatedAt" : "2015-03-24T15:41:31Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "632ca506cefcc03bc029d8b5814b8d00b89deeda",
    "line" : 296,
    "diffHunk" : "@@ -1,1 +1957,1961 @@}\n\nfunc (kl *Kubelet) generatePodStatus(podFullName string) (api.PodStatus, error) {\n\tpod, found := kl.GetPodByFullName(podFullName)\n\tif !found {"
  },
  {
    "id" : "0f41aabe-cf07-4082-bf06-7c5b93a85d5b",
    "prId" : 5714,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f6a1aa3-87e5-45d3-98dc-cd32cf2355f6",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Kubelet failed to retrieve cached PodStatus from statusManager, regenerate it from scratch. Shouldn't this update the cached one in statusManager too? \n",
        "createdAt" : "2015-03-23T22:12:10Z",
        "updatedAt" : "2015-03-24T15:41:31Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "632ca506cefcc03bc029d8b5814b8d00b89deeda",
    "line" : 292,
    "diffHunk" : "@@ -1,1 +1954,1958 @@\t\treturn cachedPodStatus, nil\n\t}\n\treturn kl.generatePodStatus(podFullName)\n}\n"
  },
  {
    "id" : "0657b1cc-2fc6-4caf-a5d0-f3e0970f2102",
    "prId" : 5702,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74efb277-5ee0-4498-afed-80836a192b0a",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "We might misread the logic here, but shouldn't you introduce a bug in a update case described as the below:\n\n A running pod with two containers: c1 and c2, now update pod with only c1. With your new change, c2 couldn't be removed? \n",
        "createdAt" : "2015-03-24T05:38:01Z",
        "updatedAt" : "2015-03-24T23:08:22Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "7c473391-0974-4b93-a293-ac795b8b81d6",
        "parentId" : "74efb277-5ee0-4498-afed-80836a192b0a",
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "@dchen1107 I think the syncPod() will take care of the update case.\n\nSo the logic I want to express here is:\n\n```\nforeach desired pod:\n    find its corresponding running pod, and do syncPod.\n\nforeach running pod:\n    if it is not desired, kill the whole pod.\n```\n\nThe first loop makes sure we have all the pod up to the desired state. The second loop makes sure we remove any unnecessary pods.\n\nThe assumption here is all containers we created can be grouped into a pod.\n",
        "createdAt" : "2015-03-24T16:17:15Z",
        "updatedAt" : "2015-03-24T23:08:22Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "b2fc144b-4621-46d4-8858-91fd36e41ba7",
        "parentId" : "74efb277-5ee0-4498-afed-80836a192b0a",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "ACK.\n",
        "createdAt" : "2015-03-24T17:04:05Z",
        "updatedAt" : "2015-03-24T23:08:22Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "e02c6994dc3aaa9dd1079289f9eb9d5d94a4fddf",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +1567,1571 @@\t// Kill any containers we don't need.\n\tkilled := []string{}\n\tfor _, pod := range runningPods {\n\t\tif _, found := desiredPods[pod.ID]; found {\n\t\t\t// syncPod() will handle this one."
  },
  {
    "id" : "021349c1-d402-483b-b34b-a2f46379dab9",
    "prId" : 5685,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f9caeca-0831-4f82-93b8-04b8aaf2ddc1",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "+1 this is exactly what I thought this should be doing when I saw this code before :)\n",
        "createdAt" : "2015-03-20T00:42:24Z",
        "updatedAt" : "2015-03-20T01:26:54Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d0fb5f07b113c4ca382631dbf7165663c56a003",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +1888,1892 @@}\n\nfunc (kl *Kubelet) GetPodByFullName(podFullName string) (*api.Pod, bool) {\n\tname, namespace, err := ParsePodFullName(podFullName)\n\tif err != nil {"
  },
  {
    "id" : "c6885a72-c87c-4a15-8f96-0f388d73d0cd",
    "prId" : 5501,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87d9f64e-2595-47fc-ba24-a3442254850f",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Ideally, would be useful to indicate which resource, and by how much (think \"why pending\").\n",
        "createdAt" : "2015-03-19T05:13:05Z",
        "updatedAt" : "2015-03-19T09:50:31Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845f6ad480677fae65899d5875cebd9850da4bf",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +1677,1681 @@\t\tkl.setPodStatusInCache(GetPodFullName(&pod), api.PodStatus{\n\t\t\tPhase:   api.PodFailed,\n\t\t\tMessage: \"Pod cannot be started due to exceeded capacity\"})\n\t}\n}"
  },
  {
    "id" : "78e102d9-f34b-4cd8-bc05-2b925ed69746",
    "prId" : 5501,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b3cd0b0-78bc-4a9c-8b30-07b1170edf3a",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "I know this wasn't introduced by this PR, but ideally, it would be useful to indicate which port(s).\n",
        "createdAt" : "2015-03-19T05:13:47Z",
        "updatedAt" : "2015-03-19T09:50:31Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845f6ad480677fae65899d5875cebd9850da4bf",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +1663,1667 @@\t\tkl.setPodStatusInCache(GetPodFullName(&pod), api.PodStatus{\n\t\t\tPhase:   api.PodFailed,\n\t\t\tMessage: \"Pod cannot be started due to host port conflict\"})\n\t\tconflictsMap[pod.UID] = true\n\t}"
  },
  {
    "id" : "0f0afba8-9447-4424-a2bd-4acd905844b6",
    "prId" : 5401,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cec86e3-4d57-41be-a32e-fa95a495e9f0",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "WDYT of making this a member of mirrorManager?\n",
        "createdAt" : "2015-03-16T21:20:13Z",
        "updatedAt" : "2015-03-17T15:46:05Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "6638c2b1-66df-456c-8ef7-76d242558cbd",
        "parentId" : "0cec86e3-4d57-41be-a32e-fa95a495e9f0",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "No, I prefer not to. I explicitly made it not a member because it's guarded by the same lock as kubelet.pods, and they have to be in sync.\n\nA more reasonable approach would be to have a pod manager that encompasses both (regular and mirror pods). That'd be yet another large kubelet refactoring, which I'd like to avoid in the current PR. \n",
        "createdAt" : "2015-03-16T22:46:59Z",
        "updatedAt" : "2015-03-17T15:46:05Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "61b504a3-b28b-4942-a5ea-4bbc59f99390",
        "parentId" : "0cec86e3-4d57-41be-a32e-fa95a495e9f0",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "SGTM, the longer-term podManager sounds like the right approach.\n",
        "createdAt" : "2015-03-16T23:59:03Z",
        "updatedAt" : "2015-03-17T15:46:05Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "929fb63b3375ec4874abdd01080b3071adecb12d",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +1748,1752 @@\tcase SET:\n\t\tglog.V(3).Infof(\"SET: Containers changed\")\n\t\tnewPods, newMirrorPods := filterAndCategorizePods(u.Pods)\n\n\t\t// Store the new pods. Don't worry about filtering host ports since those"
  },
  {
    "id" : "0eff8a15-3397-4b23-8e0d-8dbfe464e5d3",
    "prId" : 5399,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75537c93-b15e-4182-ab9d-e45f23f52b7d",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "make this a flag?\n",
        "createdAt" : "2015-03-20T08:31:37Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "b4cda167-b8ce-4aa6-90fb-0842dd696fdc",
        "parentId" : "75537c93-b15e-4182-ab9d-e45f23f52b7d",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "We deliberately change this to a constant.  It's somewhat internal mechanism, and using a wrong value will result in an unstable state. #5265\n",
        "createdAt" : "2015-03-20T17:07:09Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "5b18ac1f-7dde-44f7-a786-fcdd198a110f",
        "parentId" : "75537c93-b15e-4182-ab9d-e45f23f52b7d",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Sounds good.\n",
        "createdAt" : "2015-03-20T23:25:09Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5675b8924ace47762289bb4e6f1545ebca625f4",
    "line" : null,
    "diffHunk" : "@@ -1,1 +87,91 @@\t//    status. Kubelet may fail to update node status reliablly if the value is too small,\n\t//    as it takes time to gather all necessary node information.\n\tnodeStatusUpdateFrequency = 2 * time.Second\n\t// nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed.\n\tnodeStatusUpdateRetry = 5"
  },
  {
    "id" : "2f710817-0348-4367-b100-a60a6c4de0d2",
    "prId" : 5395,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d910854-d8bf-4bda-8af9-306cb8ffd828",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Are you removing the user created dead containers which are not managed by kubernetes. Is that what we want?\n",
        "createdAt" : "2015-03-12T18:36:57Z",
        "updatedAt" : "2015-03-12T18:41:00Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "1032088c-36de-4356-9ae7-218136e0a005",
        "parentId" : "1d910854-d8bf-4bda-8af9-306cb8ffd828",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Ahh, never mind. Expanding the code a little, the logic is against kubernetes containers only. \n",
        "createdAt" : "2015-03-12T18:37:47Z",
        "updatedAt" : "2015-03-12T18:41:00Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "51122998e3794d20ce0cb9d908a714f0d7d6f4d3",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +476,480 @@\t}\n\n\tunidentifiedContainers := make([]unidentifiedContainer, 0)\n\tuidToIDMap := map[string][]string{}\n\tfor _, container := range containers {"
  },
  {
    "id" : "31faa708-df5f-4ca5-bd4b-63ffaa3ab5aa",
    "prId" : 5395,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d34f6521-1241-4ff7-acba-0cdfe35d75d3",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I am not sure if this condition covers all cases (and you probably know better than me). It'd be more clear if we can get an error from ParseDockerName() if anything goes wrong, which will help if we ever change the naming scheme again. I see you have a TODO there though, so I guess it's fine for now.\n",
        "createdAt" : "2015-03-12T19:25:19Z",
        "updatedAt" : "2015-03-12T19:25:19Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "90b3181d-a8fc-49f1-9623-bd504f8005b5",
        "parentId" : "d34f6521-1241-4ff7-acba-0cdfe35d75d3",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "I am equally unhappy and I agree the error route is more correct. I guess I should tackle that sooner rather than later :P I'll send a PR your way sometime today.\n",
        "createdAt" : "2015-03-12T19:27:39Z",
        "updatedAt" : "2015-03-12T19:27:39Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "51122998e3794d20ce0cb9d908a714f0d7d6f4d3",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +480,484 @@\tfor _, container := range containers {\n\t\t_, uid, name, _ := dockertools.ParseDockerName(container.Names[0])\n\t\tif uid == \"\" && name == \"\" {\n\t\t\tunidentifiedContainers = append(unidentifiedContainers, unidentifiedContainer{\n\t\t\t\tid:   container.ID,"
  },
  {
    "id" : "a5302c72-4570-4eb9-bebc-87938f02811a",
    "prId" : 5094,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4fbcd14-684f-467f-9102-b283b1d90f88",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Lets document this method (mainly worried about the return values).\n",
        "createdAt" : "2015-03-05T17:30:57Z",
        "updatedAt" : "2015-03-06T17:32:30Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "e3af0d29-14ee-4df9-b630-8b42836abe20",
        "parentId" : "d4fbcd14-684f-467f-9102-b283b1d90f88",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2015-03-06T13:43:23Z",
        "updatedAt" : "2015-03-06T17:32:30Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a01a4dbf595c1ee45cd3473dbf0766f115c8160",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1052,1056 @@// Finds an infra container for a pod given by podFullName and UID in dockerContainers. If there is an infra container\n// return its ID and true, otherwise it returns empty ID and false.\nfunc (kl *Kubelet) getPodInfraContainer(podFullName string, uid types.UID,\n\tdockerContainers dockertools.DockerContainers) (dockertools.DockerID, bool) {\n\tif podInfraDockerContainer, found, _ := dockerContainers.FindPodContainer(podFullName, uid, dockertools.PodInfraContainerName); found {"
  },
  {
    "id" : "efaa45ff-c3fd-4f3c-ac53-a4888b69e75c",
    "prId" : 5094,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c486880-c5c0-445d-bcfc-f9acae7f5368",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Lets document this method (mainly worried about the return values).\n\nWe should be able to return error instead of the last bool too btw. Seems like we always log the error instead of returning it. I know the flow was like that before, but it should be small enough to change safely :)\n",
        "createdAt" : "2015-03-05T17:31:00Z",
        "updatedAt" : "2015-03-06T17:32:30Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "91ce0f1b-7876-4dd0-af1f-3535f06e0176",
        "parentId" : "7c486880-c5c0-445d-bcfc-f9acae7f5368",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2015-03-06T13:45:40Z",
        "updatedAt" : "2015-03-06T17:32:30Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a01a4dbf595c1ee45cd3473dbf0766f115c8160",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1063,1067 @@// Attempts to start a container pulling the image before that if necessary. It returns DockerID of a started container\n// if it was successful, and a non-nil error otherwise.\nfunc (kl *Kubelet) pullImageAndRunContainer(pod *api.BoundPod, container *api.Container, podVolumes *volumeMap,\n\tpodInfraContainerID dockertools.DockerID) (dockertools.DockerID, error) {\n\tpodFullName := GetPodFullName(pod)"
  },
  {
    "id" : "c05f1de0-daec-4e1b-a989-7a7abe66c251",
    "prId" : 5094,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61a9dddc-f737-44fb-b90b-780722a5f69b",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Don't we already do this in getPodInfraContainer? Maybe we can remove it from there and have it in one place after this if statement.\n",
        "createdAt" : "2015-03-05T17:31:05Z",
        "updatedAt" : "2015-03-06T17:32:30Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "abce8ff6-4bd6-473b-9564-df1f65f8e6ba",
        "parentId" : "61a9dddc-f737-44fb-b90b-780722a5f69b",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yeah, the thing is I'll remove it from one branch in the other refactor and it'll be necessary. I wanted to move it to this PR to make the cut 'cleaner'.\n",
        "createdAt" : "2015-03-06T15:16:06Z",
        "updatedAt" : "2015-03-06T17:32:30Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a01a4dbf595c1ee45cd3473dbf0766f115c8160",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +1129,1133 @@\t\t\t}\n\t\t}\n\t\tpodStatus, err = kl.GetPodStatus(podFullName, uid)\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Unable to get pod with name %q and uid %q info with error(%v)\", podFullName, uid, err)"
  },
  {
    "id" : "a8bd7569-ce3d-4b12-8d12-3d864facd1f4",
    "prId" : 5022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3aff47b4-c9ff-43e4-bb2f-0f6d2b3154c2",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Not really anything wrong, but: We don't necessarily need to do this. Today we do for a lot of cases we shouldn't and was a source of issues for the last bug in this code path. May be worth to change that as a separate PR after this.\n",
        "createdAt" : "2015-03-05T02:08:52Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "3138f253-9924-4158-9399-63d0ec4ce6b4",
        "parentId" : "3aff47b4-c9ff-43e4-bb2f-0f6d2b3154c2",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I believe @dchen1107 is working on this particular issue. I believe it'll be much easier after this refactoring.\n",
        "createdAt" : "2015-03-05T10:39:24Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3489d1ae01c3a030068f1970f5ffa8aa6dc6c559",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1254,1258 @@\t\t\t\t} else {\n\t\t\t\t\tglog.Infof(\"pod %q container %q hash changed (%d vs %d). Pod will be killed and re-created.\", podFullName, container.Name, hash, expectedHash)\n\t\t\t\t\tcreatePodInfraContainer = true\n\t\t\t\t\tdelete(containersToKeep, podInfraContainerID)\n\t\t\t\t\t// If we are to restart Infra Container then we move containersToKeep into containersToStart"
  },
  {
    "id" : "fe8c8623-3ff8-40bf-9dfc-feb47b34498a",
    "prId" : 5022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87a9ed1a-bac1-4c12-bd17-0499d1099ea7",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "We can probably do this logic in one place, we do it in quite a few places today. Once we know we need to restart the podContainer we add all containers to the restart queue and check them one by one to remove them if they say restart never.\n",
        "createdAt" : "2015-03-05T02:11:15Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "c67ab79b-896e-4560-a880-ca99a13e7d03",
        "parentId" : "87a9ed1a-bac1-4c12-bd17-0499d1099ea7",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I thought about that. The issue is that we would also need to check for successful runs for restart on failure. It is doable, but I thought that it'll be more complex. If you think otherwise I can rewrite it in such way. In addition, AFAICT, restart policy is per pod, not per container.\n",
        "createdAt" : "2015-03-05T10:38:30Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3489d1ae01c3a030068f1970f5ffa8aa6dc6c559",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1270,1274 @@\t\t\t\t// If RestartPolicy is Always or OnFailure we restart containers that were running before we\n\t\t\t\t// killed them when restarting Infra Container.\n\t\t\t\tif pod.Spec.RestartPolicy.Never == nil {\n\t\t\t\t\tglog.V(1).Infof(\"Infra Container is being recreated. %q will be restarted.\", container.Name)\n\t\t\t\t\tcontainersToStart[index] = empty{}"
  },
  {
    "id" : "4653614e-1e19-4ba5-99c0-476c80b0615e",
    "prId" : 5022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c5ff28f-4303-41be-bc9f-9bd95e18379e",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Lets document these types please :)\n",
        "createdAt" : "2015-03-12T06:17:07Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "2fa26902-dfd3-4e48-aefe-6cb176a78021",
        "parentId" : "7c5ff28f-4303-41be-bc9f-9bd95e18379e",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done. I guess this is a good place to describe semantics, so it's a long comment.\n",
        "createdAt" : "2015-03-12T11:35:37Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3489d1ae01c3a030068f1970f5ffa8aa6dc6c559",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1194,1198 @@//   Infra Container should be killed, hence it's removed from this map.\n// - all running containers which are NOT contained in containersToKeep should be killed.\ntype podContainerChangesSpec struct {\n\tstartInfraContainer bool\n\tinfraContainerId    dockertools.DockerID"
  },
  {
    "id" : "11051708-5661-46e1-81fa-d66f8287bc53",
    "prId" : 5022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba5476b0-8fcc-4ae7-9299-14030e18e01a",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Maybe we can do this killing in the function below. It will simplify this function and that function. Doesn't have to be in this PR.\n",
        "createdAt" : "2015-03-12T06:17:19Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "a50661b0-a54a-4568-997f-a23f04d2f5c3",
        "parentId" : "ba5476b0-8fcc-4ae7-9299-14030e18e01a",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yeah, I thought about adding 'containersToKill' to changes struct to make everything explicit. This would move all this logic to the analysis part where it belong, but the cost is some information redundancy. I don't feel strongly about any one of those solutions...\n",
        "createdAt" : "2015-03-12T13:09:04Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "f63b64e7-43e7-415e-a0d2-fa4cb20c9d01",
        "parentId" : "ba5476b0-8fcc-4ae7-9299-14030e18e01a",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "At least the killAllContainersInPod including the infra container should be overall simplifying without needing the extra map no? We can do that in a followup, unless I'm missing something.\n",
        "createdAt" : "2015-03-12T17:05:47Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "3489d1ae01c3a030068f1970f5ffa8aa6dc6c559",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1320,1324 @@\t\t}\n\t\t// Killing phase: if we want to start new infra container, or nothing is running kill everything (including infra container)\n\t\tif podInfraContainer, found, _ := containersInPod.FindPodContainer(podFullName, uid, dockertools.PodInfraContainerName); found {\n\t\t\tif err := kl.killContainer(podInfraContainer); err != nil {\n\t\t\t\tglog.Warningf(\"Failed to kill pod infra container %q: %v\", podInfraContainer.ID, err)"
  },
  {
    "id" : "fc1262cc-f434-43de-92a8-1441ae364605",
    "prId" : 5022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e1aa0a8-9a4c-4a68-ac3e-e6266cf5b172",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Lets add the container ID on this mesage\n",
        "createdAt" : "2015-03-12T06:17:22Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "e1442fd0-e325-4a89-a438-b8d3d068864f",
        "parentId" : "2e1aa0a8-9a4c-4a68-ac3e-e6266cf5b172",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "It's just an int... IIUC container will contain everything, as it's a DockerContainer.\n",
        "createdAt" : "2015-03-12T13:10:44Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "c0128ed8-a1bc-41de-8b1d-95212976bef2",
        "parentId" : "2e1aa0a8-9a4c-4a68-ac3e-e6266cf5b172",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Sorry I meant `container.ID` so we know what container we were trying to kill. The glog above it is a v=3 log so it may not show up.\n",
        "createdAt" : "2015-03-12T17:05:50Z",
        "updatedAt" : "2015-03-13T17:23:19Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "3489d1ae01c3a030068f1970f5ffa8aa6dc6c559",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1337,1341 @@\t\t\t\terr = kl.killContainer(container)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Errorf(\"Error killing container: %v\", err)\n\t\t\t\t}\n\t\t\t}"
  },
  {
    "id" : "6c99c4df-a1c0-4647-a0af-88e7c239d2ec",
    "prId" : 4915,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca5b222c-344c-4603-aede-48406d20cdb3",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Sending events is definitely necessary, but should we also reject such pod request somewhere?\n",
        "createdAt" : "2015-02-27T23:36:56Z",
        "updatedAt" : "2015-02-27T23:58:53Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "1109413b-5d86-4dd0-8b8b-1ae5792bdcb0",
        "parentId" : "ca5b222c-344c-4603-aede-48406d20cdb3",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Can't we not reject today?\n",
        "createdAt" : "2015-02-27T23:37:57Z",
        "updatedAt" : "2015-02-27T23:58:53Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "f85927c9-9444-45a9-a6fc-8e8d081d2f1b",
        "parentId" : "ca5b222c-344c-4603-aede-48406d20cdb3",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "We are not reject this today because we believe we should't run into this issue except someone is going to use cadvisor's port (which we talked about to make it publicly registered); otherwise apiserver will reject it. But current discussion is that we plan to remove that check in apiserver, only have this one in kubelet. In this case, reject request is important now. \n",
        "createdAt" : "2015-02-27T23:43:29Z",
        "updatedAt" : "2015-02-27T23:58:53Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "d2f8120c-c768-401f-ade4-1c64a36e86e9",
        "parentId" : "ca5b222c-344c-4603-aede-48406d20cdb3",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "@yujuhong has a TODO which should be okay for now. Can the Kubelet reject workload from the master? I thought the answer to that was no (or not yet).\n",
        "createdAt" : "2015-02-27T23:45:03Z",
        "updatedAt" : "2015-02-27T23:58:53Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "95222e81-85bc-4f03-a765-368ab2330a3b",
        "parentId" : "ca5b222c-344c-4603-aede-48406d20cdb3",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I think I need some clarification on \"rejections\", that's why I left a TODO in the code :)\n\nThe only way to reject that I am aware of is to set the pod status to fail. Given that kubelet still doesn't post pod status to the apiserver yet, we'd need to save this information somewhere and wait until next round of pod status polling (`GetPodStatus()`) to return it.\n\nAm I missing something here?\n",
        "createdAt" : "2015-02-27T23:45:04Z",
        "updatedAt" : "2015-02-27T23:58:53Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "a5a2f172-0506-403d-94ae-239fb61c9d41",
        "parentId" : "ca5b222c-344c-4603-aede-48406d20cdb3",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Yes, my earlier comment missed TODO there. \n",
        "createdAt" : "2015-02-27T23:56:46Z",
        "updatedAt" : "2015-02-27T23:58:53Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "241df2d3be21467799e997fb5600cc408fe85ac7",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1406,1410 @@\t\tif errs := validation.AccumulateUniquePorts(pod.Spec.Containers, ports, extract); len(errs) != 0 {\n\t\t\tglog.Warningf(\"Pod %q: HostPort is already allocated, ignoring: %v\", GetPodFullName(pod), errs)\n\t\t\trecord.Eventf(pod, \"hostPortConflict\", \"Cannot start the pod due to host port conflict.\")\n\t\t\t// TODO: Set the pod status to fail.\n\t\t\tcontinue"
  },
  {
    "id" : "6d21e76a-1e9c-4277-a1ab-84b3c01b1524",
    "prId" : 4905,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62207ad6-b2f7-42f9-81cd-3afa16c3fb91",
        "parentId" : null,
        "authorId" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "body" : "WDYT about changing this to ...(metrics.SinceInMicroseconds(start)/len(pods))?\n",
        "createdAt" : "2015-02-27T19:00:56Z",
        "updatedAt" : "2015-02-27T19:12:03Z",
        "lastEditedBy" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "tags" : [
        ]
      },
      {
        "id" : "5458fd6b-0fef-44dc-b6af-42e880281168",
        "parentId" : "62207ad6-b2f7-42f9-81cd-3afa16c3fb91",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Mentioned a bit offline, but I think we want to measure this raw. This is the interval at which we notice and enact changes. If this gets too long we will take long to respond. I think we want the non-normalized version.\n\nIt should also depend more on containers being deleted than number of pods since the syncing of pods is done async.\n",
        "createdAt" : "2015-02-27T19:05:02Z",
        "updatedAt" : "2015-02-27T19:12:03Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "3c6ede5a-c5db-47ab-a550-2a65c9a4c6a8",
        "parentId" : "62207ad6-b2f7-42f9-81cd-3afa16c3fb91",
        "authorId" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "body" : "SG, since we do most of the work for cleanup.\n",
        "createdAt" : "2015-02-27T19:12:02Z",
        "updatedAt" : "2015-02-27T19:12:03Z",
        "lastEditedBy" : "88a0ee93-4188-47b5-8881-3624e4a411f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9301b1996955874c2254c66b68ccfdf5260211e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1277,1281 @@func (kl *Kubelet) SyncPods(pods []api.BoundPod, podSyncTypes map[types.UID]metrics.SyncPodType, start time.Time) error {\n\tdefer func() {\n\t\tmetrics.SyncPodsLatency.Observe(metrics.SinceInMicroseconds(start))\n\t}()\n\tglog.V(4).Infof(\"Desired: %#v\", pods)"
  },
  {
    "id" : "66eb65c2-88da-4829-87fd-eb9aa9e6f028",
    "prId" : 4784,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "312809cd-2b01-4471-9810-72b5e7446f9b",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Should this be a VLOG?  Should we generate events for such failures?\n",
        "createdAt" : "2015-02-24T20:29:56Z",
        "updatedAt" : "2015-02-24T20:29:56Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "ac9f5729-5b89-4f36-8381-3ac49e48aed0",
        "parentId" : "312809cd-2b01-4471-9810-72b5e7446f9b",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "This was a log before and a temp fix, we're gonna work on a more permanent fix. Today we don't surface these failures as events just yet.\n",
        "createdAt" : "2015-02-24T20:33:27Z",
        "updatedAt" : "2015-02-24T20:33:27Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "aadbf221d2a403d57da2f221ad40a1ccf3e14c63",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +1135,1139 @@\t\t\t\tif podInfraContainer, found, _ := dockerContainers.FindPodContainer(podFullName, uid, dockertools.PodInfraContainerName); found {\n\t\t\t\t\tif err := kl.killContainer(podInfraContainer); err != nil {\n\t\t\t\t\t\tglog.V(1).Infof(\"Failed to kill pod infra container %q: %v\", podInfraContainer.ID, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}"
  },
  {
    "id" : "35f0eb24-6395-47e7-96b9-aed43e4f5f20",
    "prId" : 4563,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75268325-b7ca-4201-9ca4-6f33920d084c",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Is this necessary? At the beginning of Sync Loop, we should already put PodInfraContainerID to containersToKeep, right?\n",
        "createdAt" : "2015-02-20T00:18:06Z",
        "updatedAt" : "2015-02-20T00:18:06Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "e06a10a5-36a5-4fad-9bf2-7451a2241c84",
        "parentId" : "75268325-b7ca-4201-9ca4-6f33920d084c",
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "`podInfraContainerID` is returned by createPodInfraContainer. It is called at line 1169.\n\nI mean they are the same variable name but might have different IDs. (I assume docker will not generate duplicate IDs for containers) \n",
        "createdAt" : "2015-02-20T00:22:28Z",
        "updatedAt" : "2015-02-20T00:23:50Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e20f7d73659c8dcec771f8d72f525360c26fcc3",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1171,1175 @@\t\t\tglog.Errorf(\"Failed to recreate pod infra container: %v for pod %q\", err, podFullName)\n\t\t}\n\t\tcontainersToKeep[podInfraContainerID] = empty{}\n\t}\n"
  },
  {
    "id" : "9a1808d7-fa6f-4df9-b07c-a0b7cda4eecb",
    "prId" : 4563,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9458adff-c4e2-4f63-bee4-67fa5f280e10",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Why would we restart the pod if just one container changed?\n",
        "createdAt" : "2015-02-20T00:39:30Z",
        "updatedAt" : "2015-02-20T00:39:30Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "cdfd16ad-cfe4-488c-8fa8-5d2c1140ebcd",
        "parentId" : "9458adff-c4e2-4f63-bee4-67fa5f280e10",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Ugh, there is an escape here - if a container adds/removes/changes a host port, we won't restart the network container.  I'm not too worried about that by itself, but are there any other aspects of a single container that might require the pod itself to be recreated? \n",
        "createdAt" : "2015-02-20T00:40:58Z",
        "updatedAt" : "2015-02-20T00:40:58Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "f315edde-90f9-4a11-aec0-537ee18cc6b5",
        "parentId" : "9458adff-c4e2-4f63-bee4-67fa5f280e10",
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "@thockin previously, we kill the infra pod container whenever there is a container change. The infra container will not be recreated until the next sync happens. This pull request does not change the killing behavior. The only change is to recreate the killed infra container right after we finish the sync.\n",
        "createdAt" : "2015-02-20T00:44:25Z",
        "updatedAt" : "2015-02-20T00:46:37Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      },
      {
        "id" : "187f642f-3134-4c52-b2ac-94136e5cc72b",
        "parentId" : "9458adff-c4e2-4f63-bee4-67fa5f280e10",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "You're right.  That's awful\n",
        "createdAt" : "2015-02-20T00:47:11Z",
        "updatedAt" : "2015-02-20T00:47:11Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "9dabb754-8124-483f-a4e6-721a8e459252",
        "parentId" : "9458adff-c4e2-4f63-bee4-67fa5f280e10",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "The only reason killing infra pod container when a container is changed is that we don't know if network related config is changed, such as port.\n",
        "createdAt" : "2015-02-20T08:29:17Z",
        "updatedAt" : "2015-02-20T08:29:17Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e20f7d73659c8dcec771f8d72f525360c26fcc3",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +1090,1094 @@\t\t\t\tglog.Infof(\"pod %q container %q is unhealthy. Container will be killed and re-created.\", podFullName, container.Name, live)\n\t\t\t} else {\n\t\t\t\tpodChanged = true\n\t\t\t\tglog.Infof(\"pod %q container %q hash changed (%d vs %d). Container will be killed and re-created.\", podFullName, container.Name, hash, expectedHash)\n\t\t\t}"
  },
  {
    "id" : "38d29d4b-ef5f-46c1-ac7f-7795c9300a93",
    "prId" : 4494,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfcd6469-948c-4a19-99ae-5a62f76b3b9a",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Please document what this lock covers.\n",
        "createdAt" : "2015-02-17T21:22:16Z",
        "updatedAt" : "2015-02-25T03:36:32Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "63edf84b-ddbe-4415-aa04-f5eb1e3ac18a",
        "parentId" : "cfcd6469-948c-4a19-99ae-5a62f76b3b9a",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done.\n",
        "createdAt" : "2015-02-18T22:37:24Z",
        "updatedAt" : "2015-02-25T03:36:32Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "801ad909ca1104930bd468fe6c85851c5a79c578",
    "line" : null,
    "diffHunk" : "@@ -1,1 +163,167 @@\t// We make complete array copies out of this while locked, which is OK because once added to this array,\n\t// pods are immutable\n\tpodLock sync.RWMutex\n\tpods    []api.BoundPod\n"
  },
  {
    "id" : "44ef7fc8-1c0f-446d-89ca-9a55a4ac2a28",
    "prId" : 4494,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d73e72e-405c-42bc-bcaa-1424c3332589",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "If it is truly necessary to copy in the GetBoundPods function, then it must also be unsafe to return a pointer in this function, as is done below.\n",
        "createdAt" : "2015-02-17T21:24:23Z",
        "updatedAt" : "2015-02-25T03:36:32Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "361cce34-a55f-4a3d-9f50-42b3f335c0c3",
        "parentId" : "7d73e72e-405c-42bc-bcaa-1424c3332589",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done.\n",
        "createdAt" : "2015-02-18T22:37:28Z",
        "updatedAt" : "2015-02-25T03:36:32Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "801ad909ca1104930bd468fe6c85851c5a79c578",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1550,1554 @@func (kl *Kubelet) GetPodByName(namespace, name string) (*api.BoundPod, bool) {\n\tkl.podLock.RLock()\n\tdefer kl.podLock.RUnlock()\n\tfor i := range kl.pods {\n\t\tpod := kl.pods[i]"
  },
  {
    "id" : "30a08b81-2348-4be5-82bf-d27c38f0ace1",
    "prId" : 4494,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "939657a3-4c1e-4a37-82fa-bf3a239e81f3",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I believe we should not be returning pointer here, so user will know that he's getting a copy of the pod.\n",
        "createdAt" : "2015-02-23T10:22:22Z",
        "updatedAt" : "2015-02-25T03:36:32Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "eba15a8a-cd3c-4e18-b84f-dd900bf39123",
        "parentId" : "939657a3-4c1e-4a37-82fa-bf3a239e81f3",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "IMO, pointer is OK here. In either case it's a shallow copy, so not a \"real\" copy anyway. Returning non-pointer won't change that. See the comment I asked for in the struct definition.\n",
        "createdAt" : "2015-02-23T16:19:51Z",
        "updatedAt" : "2015-02-25T03:36:32Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "801ad909ca1104930bd468fe6c85851c5a79c578",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +1548,1552 @@\n// GetPodByName provides the first pod that matches namespace and name, as well as whether the node was found.\nfunc (kl *Kubelet) GetPodByName(namespace, name string) (*api.BoundPod, bool) {\n\tkl.podLock.RLock()\n\tdefer kl.podLock.RUnlock()"
  },
  {
    "id" : "02a12b0f-9453-4ef2-a196-33f25dd00ed7",
    "prId" : 4494,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3933bab7-8047-4fd6-a338-a735708e086e",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "For consistency I believe it should be near GetPodByName, and return *?api.BoundPod.\n",
        "createdAt" : "2015-02-23T10:33:03Z",
        "updatedAt" : "2015-02-25T03:36:32Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "801ad909ca1104930bd468fe6c85851c5a79c578",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +1647,1651 @@}\n\nfunc (kl *Kubelet) GetPodByFullName(podFullName string) (*api.PodSpec, bool) {\n\tkl.podLock.RLock()\n\tdefer kl.podLock.RUnlock()"
  },
  {
    "id" : "687efd76-8996-4856-98d3-c40b447ba6a7",
    "prId" : 4373,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34a3b3c8-af01-40df-b925-c74e1b59e331",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Specifically, this check is too aggressive, it means you cannot return logs for a container that ran to completion and succeeded (a RestartPolicyNever)\n",
        "createdAt" : "2015-02-23T20:14:54Z",
        "updatedAt" : "2015-02-23T20:14:54Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "922881fcd2994027d0234bdb8a75fa492f080b3a",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +1432,1436 @@\t\tif containerName == cName {\n\t\t\texists = true\n\t\t\tif !cStatus.Ready {\n\t\t\t\treturn fmt.Errorf(\"container %q is not ready.\", containerName)\n\t\t\t}"
  },
  {
    "id" : "acb405a3-b277-4f90-84cd-892294a62b7f",
    "prId" : 4076,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90cae9cd-5ae4-49e0-af7c-8983715994ab",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Maybe I a little bit paranoid, but killing might take long time to release the resource, especially when there are many pending io operations to disk. In this case, waiting here might end up have a very long SyncPods, which potential increase newly coming pod startup latency. How about just issuing killing, and checking the running or not. If running, wait for next loop; otherwise, clean orphan ones? \n",
        "createdAt" : "2015-02-03T22:34:30Z",
        "updatedAt" : "2015-02-05T01:28:52Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "e6102060-4261-4224-bbfe-8da82be7a669",
        "parentId" : "90cae9cd-5ae4-49e0-af7c-8983715994ab",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "or fire them off in goroutines?  Isn't this what go is supposed to be good at?  what will docker do itf we async blast a bunch of kills at it?\n",
        "createdAt" : "2015-02-03T22:48:14Z",
        "updatedAt" : "2015-02-05T01:28:52Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "800521e0-a553-4c4f-8c2f-3b46a2a578b6",
        "parentId" : "90cae9cd-5ae4-49e0-af7c-8983715994ab",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Yeah, but my point is we shouldn't wait here in a for loop.\n",
        "createdAt" : "2015-02-03T23:00:35Z",
        "updatedAt" : "2015-02-05T01:28:52Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "1b3f51cb-f640-410f-bd36-6ac75c07376b",
        "parentId" : "90cae9cd-5ae4-49e0-af7c-8983715994ab",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "When we stop a container, we set a timeout of 10s for the Docker stop call, which as I understand it sends a sigterm, followed by a sigkill 10s later.\n\nHowever, I'm ok with trying and then looping as Dawn suggests.\n\nThe goroutines don't help, because we still need to wait for the containers to die, the kill command is async.\n",
        "createdAt" : "2015-02-03T23:36:20Z",
        "updatedAt" : "2015-02-05T01:28:52Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "39c4e4e9-47dc-409e-a847-d262d142e012",
        "parentId" : "90cae9cd-5ae4-49e0-af7c-8983715994ab",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "I thought what Tim suggested above is firing a goroutine for each unwanted pod / container separately. In that goroutine, you can wait and after everything is done, you could clean up those orphan ones. \n",
        "createdAt" : "2015-02-04T00:02:44Z",
        "updatedAt" : "2015-02-05T01:28:52Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "df8e2708-2f1e-448c-89c5-0259b1e38c19",
        "parentId" : "90cae9cd-5ae4-49e0-af7c-8983715994ab",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I meant to fire off a goroutine for each pod that we want to kill, which could block on containers stopping and then do volume cleanup and whatever.  This works for now.\n",
        "createdAt" : "2015-02-04T00:04:21Z",
        "updatedAt" : "2015-02-05T01:28:52Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfe5b1411471b285675c9a8ea7b33b90443cb9a0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1292,1296 @@\t\tglog.Errorf(\"Failed to poll container state: %v\", err)\n\t\treturn err\n\t}\n\n\t// Remove any orphaned volumes."
  },
  {
    "id" : "00576b2b-7118-4890-a8b8-a059e182d253",
    "prId" : 4048,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f72b8142-43b0-4bc6-b585-08c29eaa94e0",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "A comment about what this does in corner cases would be helpful, such as when no readiness probe is provided, or before the initial period passes.\n",
        "createdAt" : "2015-02-09T18:57:07Z",
        "updatedAt" : "2015-02-10T16:30:18Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "c72c2a0d1e654d9b6c418a7f4fc5a5e827b0e18c",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1062,1066 @@\t\t\t\tlive, err := kl.probeContainer(container.LivenessProbe, podFullName, uid, podStatus, container, dockerContainer, probe.Success)\n\t\t\t\tif live == probe.Success {\n\t\t\t\t\tready, _ = kl.probeContainer(container.ReadinessProbe, podFullName, uid, podStatus, container, dockerContainer, probe.Failure)\n\t\t\t\t}\n\t\t\t\tif ready == probe.Success {"
  },
  {
    "id" : "f5563e11-c700-4550-b119-77001d03939b",
    "prId" : 3423,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aae6e54e-f1d1-4ce5-abf0-b8b57adbf3ab",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "@dchen1107  How long has it been possible to extract podAnnotations from a pod's name?  I'm wondering if this will work right if a user upgrades a kubelet on a machine which has older pods on it which use an older docker container naming convention?  \n\nI guess as long as the source == \"\" and that source has not been seen, then we won't delete the pod, which is good.  Right?\n",
        "createdAt" : "2015-01-13T22:35:16Z",
        "updatedAt" : "2015-01-13T22:35:16Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "f35999bd-4de0-437b-9771-7ec402168898",
        "parentId" : "aae6e54e-f1d1-4ce5-abf0-b8b57adbf3ab",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "I guess this works fine in the case where the name cannot be parsed.\n",
        "createdAt" : "2015-01-13T22:42:25Z",
        "updatedAt" : "2015-01-13T22:42:25Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "25fb21f4-2523-46f3-a1b0-333e12918560",
        "parentId" : "aae6e54e-f1d1-4ce5-abf0-b8b57adbf3ab",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Yep, if `ParsePodFullName(...)` is unable to parse the container name then `podAnnotations` will be `map[string]string(nil)` and thus `source` will be empty string. And `sourcesSeen.HasAll` will return false indicating it has never seen the empty string source before, thus the container will not be killed.\n",
        "createdAt" : "2015-01-13T22:53:03Z",
        "updatedAt" : "2015-01-13T22:53:03Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "0f782a0b-9ca5-4a47-8c72-bcc1bf0d1723",
        "parentId" : "aae6e54e-f1d1-4ce5-abf0-b8b57adbf3ab",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Frankly, I am not super concerned at this point if we bounce pods.  we shouldn't be careless about it, but I would not sweat it too much.\n",
        "createdAt" : "2015-01-13T23:01:26Z",
        "updatedAt" : "2015-01-13T23:01:26Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "110ab6f1bd54e10d9e323fdb4534638d0c1c637c",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +1072,1076 @@\t\t}\n\t\t_, _, podAnnotations := ParsePodFullName(podFullName)\n\t\tif source := podAnnotations[ConfigSourceAnnotationKey]; !kl.sourceReady(source) {\n\t\t\t// If the source for this container is not ready, skip deletion, so that we don't accidentally\n\t\t\t// delete containers for sources that haven't reported yet."
  },
  {
    "id" : "302b7f97-0ce3-4b54-a7c1-21eff907c9f6",
    "prId" : 3403,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d569a198-db5a-45e4-8f27-8aec3df20d46",
        "parentId" : null,
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "I worry about security here.  A sanity check that there are no special characters here would help.  This is the type of code that can often lead to a directory traversal attack.\n",
        "createdAt" : "2015-01-12T17:49:55Z",
        "updatedAt" : "2015-01-12T17:49:55Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      },
      {
        "id" : "abed93ff-3e28-489b-a6b1-62e8c8c1d720",
        "parentId" : "d569a198-db5a-45e4-8f27-8aec3df20d46",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I'm not sure I follow.  GetRootDir() is controlled by kubelet, so any trickery would have to come through the flag that controls root dir.  If you are setting flags you are already root.  The joining of pod UID and container name predicate on the validation done before we accept a pod.\n\nCan you clarify what sort of attack you are conceiving of?  Caveat, I suck at security analysis.\n",
        "createdAt" : "2015-01-12T19:03:15Z",
        "updatedAt" : "2015-01-12T19:03:15Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "5ce8e4bd-e3a6-4b3f-94f6-6b764d041bdb",
        "parentId" : "d569a198-db5a-45e4-8f27-8aec3df20d46",
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "I'm mostly worried about the container name (and perhaps the UID).  While I know we validate that higher up, defense in depth is the name of the game.\n",
        "createdAt" : "2015-01-12T20:00:21Z",
        "updatedAt" : "2015-01-12T20:00:21Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      }
    ],
    "commit" : "523a80bec71533ba6521870c244eb2a0acb1d32d",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +177,181 @@// GetPodDir returns the full path to the per-pod data directory for the\n// specified pod.  This directory may not exist if the pod does not exist.\nfunc (kl *Kubelet) GetPodDir(podUID string) string {\n\t// Backwards compat.  The \"old\" stuff should be removed before 1.0\n\t// release.  The thinking here is this:"
  },
  {
    "id" : "35c7483b-2efa-4a27-972e-8edf30c63147",
    "prId" : 3331,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6e4933d-efd3-4973-9f28-8c38a39cc8a2",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "If `kl.masterServiceNamespace == ns`, what should happen, and what does this switch do?\n",
        "createdAt" : "2015-01-13T04:42:42Z",
        "updatedAt" : "2015-01-14T22:06:56Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "66f58fbe-b1cd-4821-a6ba-5419b316275d",
        "parentId" : "c6e4933d-efd3-4973-9f28-8c38a39cc8a2",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "It seems like if `kl.masterServiceName == ns`, then any service in `ns` but not in `masterServices` will not be added to the serviceMap.\n",
        "createdAt" : "2015-01-13T04:47:58Z",
        "updatedAt" : "2015-01-14T22:06:56Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "ab7ef1d7-768f-4685-a23a-8a29d44fb1b1",
        "parentId" : "c6e4933d-efd3-4973-9f28-8c38a39cc8a2",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "Agree, nice catch of edge case\n",
        "createdAt" : "2015-01-13T06:28:09Z",
        "updatedAt" : "2015-01-14T22:06:56Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd834ae84d43d59bf29e69a4442e4242fe6d5ed4",
    "line" : null,
    "diffHunk" : "@@ -1,1 +738,742 @@\t\tcase ns:\n\t\t\tserviceMap[serviceName] = service\n\t\tcase kl.masterServiceNamespace:\n\t\t\tif masterServices.Has(serviceName) {\n\t\t\t\t_, exists := serviceMap[serviceName]"
  },
  {
    "id" : "324bebb3-e21c-4bbb-a0bb-c0f4fc0fc786",
    "prId" : 3331,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a86272d5-29b6-4865-8a78-a1df53d5b679",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Error value not checked?\n",
        "createdAt" : "2015-01-14T21:25:02Z",
        "updatedAt" : "2015-01-14T22:06:56Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "e13330ba-9071-4f45-9eea-a63df68fd8a5",
        "parentId" : "a86272d5-29b6-4865-8a78-a1df53d5b679",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "good catch @lavalamp, will fix\n",
        "createdAt" : "2015-01-14T21:42:46Z",
        "updatedAt" : "2015-01-14T22:06:56Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd834ae84d43d59bf29e69a4442e4242fe6d5ed4",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +619,623 @@\t}\n\n\tenvVariables, err := kl.makeEnvironmentVariables(pod.Namespace, container)\n\tif err != nil {\n\t\treturn \"\", err"
  },
  {
    "id" : "f47f7f20-c9d3-4dc2-b73c-07f6b9bd6768",
    "prId" : 3264,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c8f9684-bf7d-4e46-8d50-4c537f1d91a3",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Don't you need to hold the lock until the container has been started?  Otherwise teh GC could delete it again between your unlock and your run?\n",
        "createdAt" : "2015-01-07T17:44:21Z",
        "updatedAt" : "2015-01-07T17:44:21Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "b431b6f4-a3dc-4dec-927d-33cdee9773aa",
        "parentId" : "8c8f9684-bf7d-4e46-8d50-4c537f1d91a3",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Actually, the list with \"dangling=true\" only returns images with no tag, images that you have just pulled will be tagged, so they won't be returned in the query for images to GC.\n\nSo all you have to do is protect the query against the pull and it should work correctly.\n\nThere is a small race window wherein if you take a dependency on an existing layer that was previously untagged, it will get re-tagged between the listing of untagged images and the delete of the image.\n\nThat's the other race that @anguslees 's PR was fixing, it is not addressed in this PR.  We should address it, but it's a much narrower race window than this race.\n",
        "createdAt" : "2015-01-07T17:52:43Z",
        "updatedAt" : "2015-01-07T17:52:43Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "d5f2a316-4e2b-4f34-b2be-91342f638548",
        "parentId" : "8c8f9684-bf7d-4e46-8d50-4c537f1d91a3",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Ugh, that's sort of a disaster to comprehend.  OK.\n",
        "createdAt" : "2015-01-07T17:54:35Z",
        "updatedAt" : "2015-01-07T17:54:35Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "2e402237-b318-465a-9d88-99571fb0419e",
        "parentId" : "8c8f9684-bf7d-4e46-8d50-4c537f1d91a3",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "There's a function that does this thing already... ? See #3302\n",
        "createdAt" : "2015-01-08T01:03:24Z",
        "updatedAt" : "2015-01-08T01:03:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c69570663b0cf2ba508c6303ca03c947b42e893",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +913,917 @@\t\t\t\t\t}\n\t\t\t\t\tglog.Errorf(\"Failed to pull image %q: %v; skipping pod %q container %q.\", container.Image, err, podFullName, container.Name)\n\t\t\t\t\tkl.pullLock.RUnlock()\n\t\t\t\t\tcontinue\n\t\t\t\t}"
  },
  {
    "id" : "9f6ad3b5-3c63-424e-a5c6-ee259061ab2b",
    "prId" : 3260,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a21083c-b62c-4d11-b5e7-da8eda9533c8",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "This is allowed to be 0 in purgeOldest() - you probably should remove that case\n",
        "createdAt" : "2015-01-07T18:30:50Z",
        "updatedAt" : "2015-01-08T02:51:41Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "1aa182b7-3e79-46a9-a6cc-85c5df57524a",
        "parentId" : "6a21083c-b62c-4d11-b5e7-da8eda9533c8",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "To be clear - either decide that 0 is valid and allow it here, or remove the check for == 0\n",
        "createdAt" : "2015-01-07T18:35:04Z",
        "updatedAt" : "2015-01-08T02:51:41Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "32e2c496-630b-4d8a-b7e5-9c7cf47b42c7",
        "parentId" : "6a21083c-b62c-4d11-b5e7-da8eda9533c8",
        "authorId" : "27deb932-ef03-4f78-88da-2c9e62e10242",
        "body" : "Thanks for the clarification. I just pushed changes addressing all the comments. \n",
        "createdAt" : "2015-01-08T02:58:03Z",
        "updatedAt" : "2015-01-08T02:58:03Z",
        "lastEditedBy" : "27deb932-ef03-4f78-88da-2c9e62e10242",
        "tags" : [
        ]
      },
      {
        "id" : "93b3c40e-d97f-4564-a9cd-9cae612a10af",
        "parentId" : "6a21083c-b62c-4d11-b5e7-da8eda9533c8",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "you could still check that maxContainers is not < 0, but that's pretty minor\n",
        "createdAt" : "2015-01-08T03:57:46Z",
        "updatedAt" : "2015-01-08T03:57:46Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "06475fc4b417328df4a2d7c6b231079401a50c18",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +78,82 @@\t\treturn nil, fmt.Errorf(\"invalid sync frequency %d\", resyncInterval)\n\t}\n\tif minimumGCAge <= 0 {\n\t\treturn nil, fmt.Errorf(\"invalid minimum GC age %d\", minimumGCAge)\n\t}"
  },
  {
    "id" : "c052b99f-02c4-44b0-8235-9c47326703cb",
    "prId" : 3183,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a014d07-f10b-455f-b43e-2d09b7425820",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "this won't work right.  the defer is scoped to the function.  The right answer is to make this use the pullImage function.\n",
        "createdAt" : "2015-01-07T05:16:11Z",
        "updatedAt" : "2015-01-07T05:16:11Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f14e2e210965494be5638f1d39584d920dd9d12",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +820,824 @@\t\t}\n\t\tkl.pullLock.RLock()\n\t\tdefer kl.pullLock.RUnlock()\n\t\tif !api.IsPullNever(container.ImagePullPolicy) {\n\t\t\tpresent, err := kl.dockerPuller.IsImagePresent(container.Image)"
  },
  {
    "id" : "6861b8fa-ba59-4464-8ccd-c3249e7f9096",
    "prId" : 3170,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5668ed9b-9144-43a5-8369-756543903f57",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "LGTM, but can you please also update the comment for ObjectReference in api/types.go?\n",
        "createdAt" : "2014-12-30T02:25:35Z",
        "updatedAt" : "2014-12-30T05:33:40Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "1030a293-3a9a-4410-b5e3-ac555edc8433",
        "parentId" : "5668ed9b-9144-43a5-8369-756543903f57",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Done.\n",
        "createdAt" : "2014-12-30T05:34:14Z",
        "updatedAt" : "2014-12-30T05:34:14Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8d30f019dc48f443dd0c618423172e7c6ad178a",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +467,471 @@\t\t\t\treturn fmt.Sprintf(\"spec.containers[%d]\", i), nil\n\t\t\t} else {\n\t\t\t\treturn fmt.Sprintf(\"spec.containers{%s}\", here.Name), nil\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "f5121423-608f-4b36-a4c9-b6245c156fb6",
    "prId" : 3144,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f5b0a28-b81b-4f71-9446-2ae78d4837df",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "if you clear error above, you can keep this code as is.  We shouldn't kill on errors.\n",
        "createdAt" : "2014-12-24T04:22:42Z",
        "updatedAt" : "2014-12-29T19:09:56Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "0f464afc-c080-4517-a6a0-c9276219ed9e",
        "parentId" : "4f5b0a28-b81b-4f71-9446-2ae78d4837df",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Done.\n",
        "createdAt" : "2014-12-24T05:00:47Z",
        "updatedAt" : "2014-12-29T19:09:56Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "99acb9688c09c304795cfd54b05d0bb524a2c71c",
    "line" : null,
    "diffHunk" : "@@ -1,1 +833,837 @@\t\t\t\t// TODO: This should probably be separated out into a separate goroutine.\n\t\t\t\thealthy, err := kl.healthy(podFullName, uuid, podStatus, container, dockerContainer)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.V(1).Infof(\"health check errored: %v\", err)\n\t\t\t\t\tcontainersToKeep[containerID] = empty{}"
  },
  {
    "id" : "bfcc91df-26e5-48de-913e-ca67a76650a1",
    "prId" : 2667,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d941f97f-5cde-4e69-9b35-cd9e639e7a40",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Do these functions need to be public?\n",
        "createdAt" : "2014-12-01T21:56:44Z",
        "updatedAt" : "2014-12-06T19:00:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "2d3f8c42-8f6c-4edd-a8a8-2bc52f3f2246",
        "parentId" : "d941f97f-5cde-4e69-9b35-cd9e639e7a40",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "For volume plugins they comprise an interface\n",
        "createdAt" : "2014-12-02T04:18:23Z",
        "updatedAt" : "2014-12-06T19:00:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "43910c7b-a780-4bde-b19f-02563c0b4237",
        "parentId" : "d941f97f-5cde-4e69-9b35-cd9e639e7a40",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Ah, I see-- this may be worth a mention in the comment.\n",
        "createdAt" : "2014-12-03T07:01:38Z",
        "updatedAt" : "2014-12-06T19:00:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "8d77be29-101a-48d1-9f71-97d65747a76f",
        "parentId" : "d941f97f-5cde-4e69-9b35-cd9e639e7a40",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Actually I am super confused-- what interface? I can't find it? Why do volumes need to call these methods on kubelet?\n",
        "createdAt" : "2014-12-03T07:05:01Z",
        "updatedAt" : "2014-12-06T19:00:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "43f09151-291d-41ab-8578-297d901ad810",
        "parentId" : "d941f97f-5cde-4e69-9b35-cd9e639e7a40",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Volume plugins are not committed yet.\n\nTL;DR: volume plugins need to know about where to write files, such as the empty directory foir EmptyDir, but we don;t want them to know all about kubelets.  These let me define a simple interface that publishes various Get*Dir() methods.  Commented\n",
        "createdAt" : "2014-12-03T07:54:57Z",
        "updatedAt" : "2014-12-06T19:00:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "acc6b95c21fe3973d2a94a48b69df910b4765ebb",
    "line" : null,
    "diffHunk" : "@@ -1,1 +158,162 @@// TODO(thockin): For now, this is the same as the root because that is assumed\n// in other code.  Will fix.\nfunc (kl *Kubelet) GetPodsDir() string {\n\treturn kl.GetRootDir()\n}"
  },
  {
    "id" : "766628fb-2509-43df-8025-745a95295564",
    "prId" : 2267,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "273238c0-5859-4e27-ab8c-10e638f6ecc9",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "\"running\" for status (instead of \"\")? I didn't fill out a status for the container events you probably saw, but that's because a container is only a part of a pod. Here's it's probably safe to declare a status and not only a reason.\n",
        "createdAt" : "2014-11-10T21:49:24Z",
        "updatedAt" : "2014-11-10T21:49:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c8f2cde1400529d3d2f851aa1f79d48de56b8f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1010,1014 @@\t\tUID:  kl.hostname,\n\t}\n\trecord.Eventf(ref, \"\", \"starting\", \"Starting kubelet.\")\n}"
  },
  {
    "id" : "3c20c2e2-6ae3-4811-9f70-1b0e5dd4be2d",
    "prId" : 2224,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea4b1aaf-b2d6-4441-b27b-0aabbf51a352",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "That is a totally ridiculous amount of parameters for one function-- why don't we have a config struct like other places?\n\nWilling to let this slide if we have a deal that the next person to touch this has to change it :)\n",
        "createdAt" : "2014-12-24T00:51:32Z",
        "updatedAt" : "2014-12-29T17:18:18Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "981473f7-9be8-49e7-9852-f07f8504527b",
        "parentId" : "ea4b1aaf-b2d6-4441-b27b-0aabbf51a352",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Agree - lots of cleanup needed here, but not in this PR\n",
        "createdAt" : "2014-12-24T01:05:41Z",
        "updatedAt" : "2014-12-29T17:18:18Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "32a59477a5a39647bd6519446a393fb02c9bf95e",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +74,78 @@\tsourcesReady SourcesReadyFn,\n\tclusterDomain string,\n\tclusterDNS net.IP) *Kubelet {\n\treturn &Kubelet{\n\t\thostname:              hn,"
  },
  {
    "id" : "fe3de26e-44aa-4ee0-bd02-55bca68cd9e0",
    "prId" : 2160,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de95a3fe-8a3f-4c5a-ba41-281fa24f0527",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Won't this kill the network container multiple times?\n",
        "createdAt" : "2014-11-04T23:35:38Z",
        "updatedAt" : "2014-11-04T23:35:38Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "93307e06-eed5-499b-92fc-b5684b21fd9d",
        "parentId" : "de95a3fe-8a3f-4c5a-ba41-281fa24f0527",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Yeah when there are several containers in a pod, and more than one container with new configurations. If only one has changed, we only kill it once. I could optimize that like what I mentioned in pr description but thought not necessary since we plan to replace network container soon. \n",
        "createdAt" : "2014-11-05T00:19:56Z",
        "updatedAt" : "2014-11-05T00:19:56Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "03958f50496052462b2376d0ca742036877d9a9d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +590,594 @@\t\t\tkilledContainers[containerID] = empty{}\n\n\t\t\t// Also kill associated network container\n\t\t\tif netContainer, found, _ := dockerContainers.FindPodContainer(podFullName, uuid, networkContainerName); found {\n\t\t\t\tif err := kl.killContainer(netContainer); err != nil {"
  },
  {
    "id" : "8e93911b-af17-410c-ae00-7366efd70bd5",
    "prId" : 2022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86a8d575-6da7-445a-a271-b857407adaa1",
        "parentId" : null,
        "authorId" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "body" : "maybe add more context with `fmt.Errorf`? (ex: the id of the container you were trying to remove)\n",
        "createdAt" : "2014-10-28T16:54:22Z",
        "updatedAt" : "2014-10-28T19:58:21Z",
        "lastEditedBy" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "tags" : [
        ]
      },
      {
        "id" : "47757661-a32c-46e6-a741-2df1890f86d3",
        "parentId" : "86a8d575-6da7-445a-a271-b857407adaa1",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Prefer to leave the original error.  fmt.Errorf is an anti-pattern as it creates errors that are very hard to reason about programatically.\n",
        "createdAt" : "2014-10-28T17:04:39Z",
        "updatedAt" : "2014-10-28T19:58:21Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "51bf451932cab4ab1bf2315ca602d8b51333f2f8",
    "line" : null,
    "diffHunk" : "@@ -1,1 +162,166 @@\tfor _, data := range dockerData {\n\t\tif err := kl.dockerClient.RemoveContainer(docker.RemoveContainerOptions{ID: data.ID}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}"
  },
  {
    "id" : "60e5111c-bf69-43bc-8e53-1a67ccf3e141",
    "prId" : 2022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14167142-90c6-4440-9eb8-48983dd9d127",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Won't this race with and cause flaky errors in the syncPods loop, which call GetRecent... ?\n",
        "createdAt" : "2014-10-28T17:27:25Z",
        "updatedAt" : "2014-10-28T19:58:21Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "39716daf-cee0-40ed-909d-cd23907965f2",
        "parentId" : "14167142-90c6-4440-9eb8-48983dd9d127",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Good point. My original thought is garbage collecting the oldest containers based on 3 rules 1) flag controlled time 2) cap of per UUID 3) cap of recent containers at GetRecentContainers during sync. But I guess ok with this for now since it only mess up with restart count, and it cannot be trusted anyway without the checkpoint. \n",
        "createdAt" : "2014-10-28T17:35:10Z",
        "updatedAt" : "2014-10-28T19:58:21Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "51bf451932cab4ab1bf2315ca602d8b51333f2f8",
    "line" : null,
    "diffHunk" : "@@ -1,1 +170,174 @@\n// TODO: Also enforce a maximum total number of containers.\nfunc (kl *Kubelet) GarbageCollectContainers() error {\n\tif kl.maxContainerCount == 0 {\n\t\treturn nil"
  },
  {
    "id" : "22831aed-5f3f-48d3-bb87-fcf31c4e6c17",
    "prId" : 2022,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0df0ccc8-5501-4517-85fd-958f9d4100f0",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Log?\n",
        "createdAt" : "2014-10-28T17:36:00Z",
        "updatedAt" : "2014-10-28T19:58:21Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "b1bafff9-5d45-44f8-a2ff-5504d5a691cb",
        "parentId" : "0df0ccc8-5501-4517-85fd-958f9d4100f0",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Why?  The error is going to get passed upstream, we log there.\n",
        "createdAt" : "2014-10-28T17:47:13Z",
        "updatedAt" : "2014-10-28T19:58:21Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "51bf451932cab4ab1bf2315ca602d8b51333f2f8",
    "line" : null,
    "diffHunk" : "@@ -1,1 +161,165 @@\tdockerData = dockerData[kl.maxContainerCount:]\n\tfor _, data := range dockerData {\n\t\tif err := kl.dockerClient.RemoveContainer(docker.RemoveContainerOptions{ID: data.ID}); err != nil {\n\t\t\treturn err\n\t\t}"
  },
  {
    "id" : "3fba11c0-1075-4f03-ad03-b041036f960b",
    "prId" : 1569,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46cd96ae-6340-4573-abec-5fb07ca66396",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Do we need to return a \"not found\" error if we don't find the pod?\n",
        "createdAt" : "2014-10-04T05:07:53Z",
        "updatedAt" : "2014-10-06T21:41:31Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "4a7436ef-ef8a-4518-8ca7-dd13b80bae97",
        "parentId" : "46cd96ae-6340-4573-abec-5fb07ca66396",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Today, it returns dockertool.ErrNoContainersInPod through GetDockerPodInfo already. I didn't think about defining more meaning errors, such as Pod_Not_Found earlier. The problem is that kubelet doesn't have the entire desireState, only has ContainerManifest part. In above comment, I mentioned that my initial PR for this actually wanted to have both DesireState and CurrentState, but the change it too big for this, and not worth it since we soon should migrate to support v1beta3. I did plan to change that in supporting v1beta3. \n",
        "createdAt" : "2014-10-06T18:54:10Z",
        "updatedAt" : "2014-10-06T21:41:31Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fdfeaa30e6efe87851fc0e76f251e5483e3fdba",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +777,781 @@\t\t}\n\t}\n\treturn dockertools.GetDockerPodInfo(kl.dockerClient, manifest, podFullName, uuid)\n}\n"
  },
  {
    "id" : "b14af07a-b6be-4147-8f16-a51c756dbeb0",
    "prId" : 1546,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1219eb5-c7d9-49cb-9373-baea5f301e59",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I don't like that this will never try to update 'pause' - are we just assuming that this will become one more out-of-band operations?  or that one day we will distribute kubelet config somehow and the ID of the correct pause image will be part of that? or?\n",
        "createdAt" : "2014-10-02T19:38:17Z",
        "updatedAt" : "2014-10-02T19:56:06Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "1a11cef6-5f86-4620-88ed-0aca3f6130e3",
        "parentId" : "e1219eb5-c7d9-49cb-9373-baea5f301e59",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I was mostly thinking that as soon as we have TTL this problem gets solved.\n\n----- Original Message -----\n\n> > ```\n> >     Ports: ports,\n> > }\n> > ```\n> > -   if err := kl.dockerPuller.Pull(networkContainerImage); err != nil {\n> > -   ok, err := kl.dockerPuller.IsImagePresent(container.Image)\n> \n> I don't like that this will never try to update 'pause' - are we just\n> assuming that this will become one more out-of-band operations?  or that one\n> day we will distribute kubelet config somehow and the ID of the correct\n> pause image will be part of that? or?\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/1546/files#r18362079\n",
        "createdAt" : "2014-10-02T19:41:01Z",
        "updatedAt" : "2014-10-02T19:56:07Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "a7e7c3a2-f940-4d4a-baa0-27ee09bbcb30",
        "parentId" : "e1219eb5-c7d9-49cb-9373-baea5f301e59",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "TODO?\n",
        "createdAt" : "2014-10-02T19:47:11Z",
        "updatedAt" : "2014-10-02T19:56:07Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "6c892aaf-cc80-498d-85dc-e824330684e6",
        "parentId" : "e1219eb5-c7d9-49cb-9373-baea5f301e59",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Done\n\n----- Original Message -----\n\n> > ```\n> >     Ports: ports,\n> > }\n> > ```\n> > -   if err := kl.dockerPuller.Pull(networkContainerImage); err != nil {\n> > -   ok, err := kl.dockerPuller.IsImagePresent(container.Image)\n> \n> TODO?\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/1546/files#r18362619\n",
        "createdAt" : "2014-10-02T19:56:14Z",
        "updatedAt" : "2014-10-02T19:56:14Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "6881db64a95e02c95cdd884a801acdc4f6f77ff0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +390,394 @@\t}\n\t// TODO: make this a TTL based pull (if image older than X policy, pull)\n\tok, err := kl.dockerPuller.IsImagePresent(container.Image)\n\tif err != nil {\n\t\treturn \"\", err"
  },
  {
    "id" : "18a865fb-d89a-4b97-bdec-e15c4a28ce56",
    "prId" : 1458,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23bf2409-8b4f-4056-8fd9-7920aa73d5c3",
        "parentId" : null,
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "From the docker command line `docker run` will pull if the image isn't already there.  Does the remote API do the same?  If it just mirrors the CLI then this is the \"pull if not present\" policy.\n",
        "createdAt" : "2014-09-26T17:07:35Z",
        "updatedAt" : "2014-10-01T19:34:35Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      },
      {
        "id" : "e088e223-8867-44ae-b977-c0da980e3c7c",
        "parentId" : "23bf2409-8b4f-4056-8fd9-7920aa73d5c3",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Good point.  Maybe it's not worth having a mode that simply checks locally and aborts, then?\n",
        "createdAt" : "2014-09-26T17:34:21Z",
        "updatedAt" : "2014-10-01T19:34:35Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "41318b1a-18fc-4b52-bc48-02d2aee54620",
        "parentId" : "23bf2409-8b4f-4056-8fd9-7920aa73d5c3",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done.\n",
        "createdAt" : "2014-09-26T18:01:45Z",
        "updatedAt" : "2014-10-01T19:34:35Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c456015b6ebb573cd0471f1673ded2f06980888",
    "line" : null,
    "diffHunk" : "@@ -1,1 +543,547 @@\t\t}\n\t\t// TODO(dawnchen): Check RestartPolicy.DelaySeconds before restart a container\n\t\tcontainerID, err := kl.runContainer(pod, &container, podVolumes, \"container:\"+string(netID))\n\t\tif err != nil {\n\t\t\t// TODO(bburns) : Perhaps blacklist a container after N failures?"
  },
  {
    "id" : "e5883488-a4c6-467a-a6f7-51b8fa339d7d",
    "prId" : 1439,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6cb1dccc-9e79-4060-8099-21747cade4ff",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "last nit: Can you add a comment that the \"\" is uuid, which is allowed to be blank (and if you can explain why it is blank, rather than valid, even better)\n",
        "createdAt" : "2014-09-25T19:09:40Z",
        "updatedAt" : "2014-09-25T20:57:17Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5d8ee381126dc623c30c2283d0b52f9097a7717",
    "line" : null,
    "diffHunk" : "@@ -1,1 +751,755 @@// The second parameter of GetPodInfo and FindPodContainer methods represents pod UUID, which is allowed to be blank\nfunc (kl *Kubelet) GetKubeletContainerLogs(podFullName, containerName, tail string, follow bool, stdout, stderr io.Writer) error {\n\t_, err := kl.GetPodInfo(podFullName, \"\")\n\tif err == dockertools.ErrNoContainersInPod {\n\t\treturn fmt.Errorf(\"Pod not found (%s)\\n\", podFullName)"
  },
  {
    "id" : "f9ee2f3e-d20a-442e-a6a6-65126370f678",
    "prId" : 1354,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0044ff4a-4b89-4590-b77e-1c8b88a8b3c8",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "TODO?\n",
        "createdAt" : "2014-09-24T22:25:52Z",
        "updatedAt" : "2014-09-25T18:33:00Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "283eaf3931322cee7f793052dcbbbdb87951fb21",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +175,179 @@// LogEvent reports an event.\nfunc (kl *Kubelet) LogEvent(event *api.Event) error {\n\treturn nil\n}\n"
  },
  {
    "id" : "935b3939-0bf5-4f1e-bfb6-73d44180282c",
    "prId" : 1156,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e02be9a-cea9-4875-96d8-909b1890b42d",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Don't do this with a switch. Make a factory that returns something that implements a common interface.\n",
        "createdAt" : "2014-09-03T21:00:00Z",
        "updatedAt" : "2014-09-05T19:38:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "1d84563e-bc49-4028-b87b-a0bba00d66ec",
        "parentId" : "1e02be9a-cea9-4875-96d8-909b1890b42d",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "why?  I'm going to have the switch somewhere anyway, whether its in the NewFactory() method or the runHandler() method.\n",
        "createdAt" : "2014-09-03T21:52:19Z",
        "updatedAt" : "2014-09-05T19:38:03Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "d6c29b0b-7df7-47ea-8705-edbe7b21d391",
        "parentId" : "1e02be9a-cea9-4875-96d8-909b1890b42d",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "That will force you to not make a big long hairy function like this one, instead you'll have multiple small easy to test functions. It also doesn't mix up the conceptually separate tasks of parsing the api object and calling a handler.\n",
        "createdAt" : "2014-09-03T22:21:21Z",
        "updatedAt" : "2014-09-05T19:38:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "d7b67ab6-8667-4604-85cd-9e812e885a8a",
        "parentId" : "1e02be9a-cea9-4875-96d8-909b1890b42d",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done, although I'm not sure it's really all that cleaner or easier to test.\n",
        "createdAt" : "2014-09-04T04:01:11Z",
        "updatedAt" : "2014-09-05T19:38:03Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0884accd742c4d1c7b633c79f8aac610c02689e",
    "line" : null,
    "diffHunk" : "@@ -1,1 +298,302 @@func (kl *Kubelet) newActionHandler(handler *api.Handler) actionHandler {\n\tswitch {\n\tcase handler.Exec != nil:\n\t\treturn &execActionHandler{kubelet: kl}\n\tcase handler.HTTPGet != nil:"
  },
  {
    "id" : "d525d44d-e5a2-4ddd-a6fc-37d1a09c0c69",
    "prId" : 1156,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1b4f215-bc26-4452-abcc-9f9428116915",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "move to handlers.go?\n",
        "createdAt" : "2014-09-04T18:06:53Z",
        "updatedAt" : "2014-09-05T19:38:03Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "c64af885-6c33-4069-9abb-d24cb398a1b4",
        "parentId" : "c1b4f215-bc26-4452-abcc-9f9428116915",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "No, I'd argue that the interface is a property of the kubelet, the implementation is separated out, but the definition of the interface is essential to the operation of the kubelet, so belongs in kubelet.go.\n",
        "createdAt" : "2014-09-05T19:37:38Z",
        "updatedAt" : "2014-09-05T19:38:03Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0884accd742c4d1c7b633c79f8aac610c02689e",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +296,300 @@}\n\nfunc (kl *Kubelet) newActionHandler(handler *api.Handler) actionHandler {\n\tswitch {\n\tcase handler.Exec != nil:"
  },
  {
    "id" : "52ba0c4f-c25b-4d0a-9ff8-881cbaaf27a5",
    "prId" : 830,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4b19f39-7481-4daf-a328-c514d8cc6b24",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "How do you know the killing process has completed? is deleteAllContainers synchronous?\n",
        "createdAt" : "2014-08-08T17:44:26Z",
        "updatedAt" : "2014-08-08T19:35:12Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "53b73ddd-ea0c-4d54-a8e7-a4208c6d514d",
        "parentId" : "c4b19f39-7481-4daf-a328-c514d8cc6b24",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "yes, its synchronous.\n",
        "createdAt" : "2014-08-08T18:06:56Z",
        "updatedAt" : "2014-08-08T19:35:12Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "38900a9c58440ff61f74043fe21f5d5bdee5b070",
    "line" : null,
    "diffHunk" : "@@ -1,1 +407,411 @@\t\tif count > 0 {\n\t\t\t// relist everything, otherwise we'll think we're ok\n\t\t\tdockerContainers, err = getKubeletDockerContainers(kl.dockerClient)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Error listing containers %#v\", dockerContainers)"
  },
  {
    "id" : "ae8f139f-f19a-42ec-8bd5-2f563ecc0a4a",
    "prId" : 830,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b481221-a5db-4e68-b858-cc35b518e72e",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Suppose a container didn't die. Next time around no attempt will be made to kill it or the rest of the containers. Seems like a difficult to track down bug should it ever occur.\n",
        "createdAt" : "2014-08-08T17:46:26Z",
        "updatedAt" : "2014-08-08T19:35:12Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "204fc1d5-4a5b-4789-8565-6cc7223c1568",
        "parentId" : "3b481221-a5db-4e68-b858-cc35b518e72e",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "switched the logic around so that it tries to delete first, and only creates the network container if all deletes are successful.\n",
        "createdAt" : "2014-08-08T18:06:48Z",
        "updatedAt" : "2014-08-08T19:35:12Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "38900a9c58440ff61f74043fe21f5d5bdee5b070",
    "line" : null,
    "diffHunk" : "@@ -1,1 +402,406 @@\t\tif err != nil {\n\t\t\tglog.Errorf(\"Failed to introspect network container. (%v)  Skipping pod %s\", err, podFullName)\n\t\t\treturn err\n\t\t}\n\t\tnetID = dockerNetworkID"
  },
  {
    "id" : "e5d75ab7-dcf5-4b2c-9802-f95387ba1f18",
    "prId" : 827,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a0a95ea-a2b3-4c72-80c7-86a4ad0dce3e",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "The hash is close to the intended podInstanceID from https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/identifiers.md in 8-3.  Should we just hash the entire pod and then kill containers that don't match it?  Or, pod instance id could be an extra field and complement the hash?  The pod changed, but we don't have anything to track new container x is part of new pod version y.  I guess that can be resource version.\n\nIt seems more correct to only restart the affected container, but there's nothing a user could easily get at to know whether events from the containers are from the current pod version.\n",
        "createdAt" : "2014-08-08T05:37:52Z",
        "updatedAt" : "2014-08-08T20:32:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "25d5bb42-0ce7-4355-bd28-37e0f1d6a13f",
        "parentId" : "0a0a95ea-a2b3-4c72-80c7-86a4ad0dce3e",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "I agree this is worth doing.  Can we do it in a different PR?  It seems like its a second level of check/restart?  (and it will benefit from the \"kill all containers\" method I added in my other PR about the network container)\n",
        "createdAt" : "2014-08-08T16:35:43Z",
        "updatedAt" : "2014-08-08T20:32:22Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "2986db9885f44b52ff663de0a078c0e1bb221049",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +443,447 @@\n\tfor _, container := range pod.Manifest.Containers {\n\t\texpectedHash := hashContainer(&container)\n\t\tif dockerContainer, found, hash := dockerContainers.FindPodContainer(podFullName, container.Name); found {\n\t\t\tcontainerID := DockerID(dockerContainer.ID)"
  },
  {
    "id" : "9cad0f65-778e-49a1-b773-d3b539b42fb1",
    "prId" : 825,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f59f61a-5884-4e66-8f4c-1a7e77f0fa30",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I recommend splitting this function here-- everything above is testable, and everything below should eventually be replaced when docker & our client support this natively.\n",
        "createdAt" : "2014-08-07T18:19:12Z",
        "updatedAt" : "2014-08-08T03:28:08Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "f36fc815-0101-410a-9b30-092d6006e2e9",
        "parentId" : "3f59f61a-5884-4e66-8f4c-1a7e77f0fa30",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Done.\n",
        "createdAt" : "2014-08-07T22:04:37Z",
        "updatedAt" : "2014-08-08T03:28:08Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "9bc1b80c-b215-447c-8a6f-6cf935a5835a",
        "parentId" : "3f59f61a-5884-4e66-8f4c-1a7e77f0fa30",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Thanks, this is more what I had in mind now.\n",
        "createdAt" : "2014-08-08T00:40:36Z",
        "updatedAt" : "2014-08-08T03:28:08Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "d31d0781b247dbb56dd3916b6cd5d3e685001738",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +680,684 @@\tif !found {\n\t\treturn nil, fmt.Errorf(\"container not found (%s)\", container)\n\t}\n\treturn kl.runner.RunInContainer(dockerContainer.ID, cmd)\n}"
  },
  {
    "id" : "b8989a1d-a934-4404-ad9a-ad85be11471b",
    "prId" : 825,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6935605a-c076-4f8f-9b7d-60120576634f",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "\"runner\" feels too generic, but I don't have a better suggestion.\n",
        "createdAt" : "2014-08-08T00:40:00Z",
        "updatedAt" : "2014-08-08T03:28:08Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "9a00b0e6-09bc-41ad-8075-837ffc4d7068",
        "parentId" : "6935605a-c076-4f8f-9b7d-60120576634f",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "leaving it runner for now.\n",
        "createdAt" : "2014-08-08T03:28:55Z",
        "updatedAt" : "2014-08-08T03:28:55Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "d31d0781b247dbb56dd3916b6cd5d3e685001738",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +109,113 @@\tlogServer http.Handler\n\t// Optional, defaults to simple Docker implementation\n\trunner ContainerCommandRunner\n}\n"
  },
  {
    "id" : "7daab8f0-c156-4a93-801e-83eeae4e22de",
    "prId" : 812,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31af510e-e988-48d4-aa5d-41d1880056fc",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Will lead to repeatedly passing the same group of pods through filterHostPortConflicts-- is that OK?\n",
        "createdAt" : "2014-08-06T20:34:39Z",
        "updatedAt" : "2014-08-07T14:40:19Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "a5819240-cd4a-46d0-9ced-259304bd2015",
        "parentId" : "31af510e-e988-48d4-aa5d-41d1880056fc",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "good catch, let me move that up into the block.\n",
        "createdAt" : "2014-08-06T21:11:52Z",
        "updatedAt" : "2014-08-07T14:40:19Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7f46718a8e599afefe8915e558c369a6e3f8817",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +566,570 @@// state every sync_frequency seconds. Never returns.\nfunc (kl *Kubelet) syncLoop(updates <-chan PodUpdate, handler SyncHandler) {\n\tvar pods []Pod\n\tfor {\n\t\tselect {"
  },
  {
    "id" : "cb115199-480b-49cf-9fac-fe9dfc853b73",
    "prId" : 688,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4413e5b7-cbd0-4baf-8d43-d411d92bafc7",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Why?  If I read this correctly, you're doing this to dodge having a volume object which does not have a corresponding on-disk representation - is that right?  This feels hacky to me, because we still have teardown state which is the inverse of this condition.\n\nWe're sort of dancing around the topic of state machine, still.\n\nOr am I mis-reading the intent here?\n",
        "createdAt" : "2014-07-30T22:44:15Z",
        "updatedAt" : "2014-08-06T18:20:43Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "c28db76e-3bd2-4890-946e-9d230837bda6",
        "parentId" : "4413e5b7-cbd0-4baf-8d43-d411d92bafc7",
        "authorId" : "5047b1e4-a97e-4cdf-96ee-2e59b9b3fe43",
        "body" : "It's more related to a lack of information upon losing the manifest. When a pod is deleted, it's representation in the manifest is no longer available(I believe). The Builders use the api information to construct and mount the volume while the Cleaners (at least at this point) just need to know what type of volume is present and where it is mounted. Without some sort of state machine we can't rely on any information except what is \"encoded\" in the mount path, so in order to reconstruct the correct driver, the Cleaner needs a different, smaller set of information. I would ideally like to use the same type for both, but I couldn't figure out how to get around losing that information in the event of a crash.\n\nOne thought I had was to have one struct implement all of the interfaces, but only partially create it with the information needed for the scenario. Typing it as the interface would then only expose the methods that can be called without breaking, but I was unsure if this is kosher or not. This would have two factory methods for the same struct, but one of them would be incomplete.\n",
        "createdAt" : "2014-07-30T23:27:29Z",
        "updatedAt" : "2014-08-06T18:20:43Z",
        "lastEditedBy" : "5047b1e4-a97e-4cdf-96ee-2e59b9b3fe43",
        "tags" : [
        ]
      },
      {
        "id" : "7584cb2e-dddc-4eca-a448-627cb2ece89c",
        "parentId" : "4413e5b7-cbd0-4baf-8d43-d411d92bafc7",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I like that better, I think.  If you have lost the manifest, the volume is orphaned, and it's OK to have a partial struct or even a different type that implements the cleaner interface\n",
        "createdAt" : "2014-07-31T00:06:17Z",
        "updatedAt" : "2014-08-06T18:20:43Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "08fa1290-1979-42b8-b165-866dc306549b",
        "parentId" : "4413e5b7-cbd0-4baf-8d43-d411d92bafc7",
        "authorId" : "5047b1e4-a97e-4cdf-96ee-2e59b9b3fe43",
        "body" : "Done, both interfaces are now implemented by the same struct.\n",
        "createdAt" : "2014-08-01T21:32:49Z",
        "updatedAt" : "2014-08-06T18:20:43Z",
        "lastEditedBy" : "5047b1e4-a97e-4cdf-96ee-2e59b9b3fe43",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c28e0849f2617995b5b902a25e1c2c66d008e84",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +259,263 @@\tpodVolumes := make(volumeMap)\n\tfor _, vol := range manifest.Volumes {\n\t\textVolume, err := volume.CreateVolumeBuilder(&vol, manifest.ID, kl.rootDirectory)\n\t\tif err != nil {\n\t\t\treturn nil, err"
  },
  {
    "id" : "be7967d8-26db-4a1a-a0fc-0a7ed0d98205",
    "prId" : 588,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "477fa6ff-72b4-4998-afca-7b17eea66f1e",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Suppose the worker finishes after the unlock but before this line. Is that OK? Recommend moving everything everything in this function between the lock & unlock. (and use defer!)\n",
        "createdAt" : "2014-07-23T18:36:34Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "1fce7953-4653-4615-b8c8-9d4227e466e7",
        "parentId" : "477fa6ff-72b4-4998-afca-7b17eea66f1e",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I wonder if it makes sense to have a queue for workers that are busy. I feel like this method might lose updates? (not sure if we support updates yet, though)\n",
        "createdAt" : "2014-07-23T18:38:01Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "43433743-ec83-49da-a4cc-96e67cf7ce7a",
        "parentId" : "477fa6ff-72b4-4998-afca-7b17eea66f1e",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "We don't support updates yet.  So just as a general approach... why not make each pod a go routine with a channel of instructions to it?  Seems like sync is checking current state against expected, and sync wants to ensure that pods get a linear sequence of operations.  But if a kill is in progress, seems like you want to queue expected behavior on it or potentially cancel a new create.\n",
        "createdAt" : "2014-07-23T18:41:57Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "8783731c-6518-4f23-8575-c1d81801cb0b",
        "parentId" : "477fa6ff-72b4-4998-afca-7b17eea66f1e",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "For small lock/unlock sections (particularly those without returns in the middle). It is lighter weight to not use defer (there are benchmarks for this). I'm also a huge fan of defer, but it seemed a bit overkill here.\n\nThe lost update shouldn't matter as we're going to send it again in the next sync.\n\n@smarterclayton I think that's where we're going and this is a step in that direction.\n",
        "createdAt" : "2014-07-23T18:47:01Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "e4a3130b-ef9d-4338-8fc9-181560681741",
        "parentId" : "477fa6ff-72b4-4998-afca-7b17eea66f1e",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "A channel of instructions to the worker does sound pretty cool. I think I'm ok with this as a first step.\n",
        "createdAt" : "2014-07-23T18:58:57Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "26fc014f-345d-4172-a382-cb2e8cb62d51",
        "parentId" : "477fa6ff-72b4-4998-afca-7b17eea66f1e",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "hm-- I think I still prefer defer until we start thinking about optimizations... but I guess I don't feel super strongly.\n",
        "createdAt" : "2014-07-23T19:00:02Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "b131da1cf5c7705a3b25bcae9f2d12e93e0f1a23",
    "line" : null,
    "diffHunk" : "@@ -1,1 +141,145 @@\t// This worker is already running, let it finish.\n\tif self.workers.Has(podFullName) {\n\t\treturn\n\t}\n\tself.workers.Insert(podFullName)"
  },
  {
    "id" : "893df62f-0612-4d65-97dd-58a86292e51c",
    "prId" : 588,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1672c49c-c9d9-4bf6-a1c6-d86802f90bfb",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Is it possible to need to kill something here that is currently pulling / etc?  We can't cancel pull yet so not huge, but we could.\n",
        "createdAt" : "2014-07-23T18:44:02Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "886c0d89-6fd2-47e9-b3d1-885d0ce0e353",
        "parentId" : "1672c49c-c9d9-4bf6-a1c6-d86802f90bfb",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Also, shouldn't this loop also be stopping podWorkers that are no longer relevant?\n",
        "createdAt" : "2014-07-23T18:45:47Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "172e4204-af51-4d58-b1fe-8849171524c0",
        "parentId" : "1672c49c-c9d9-4bf6-a1c6-d86802f90bfb",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Yes, but I think we'll need more infrastructure for that. Cancelling a pull/start is a bit undefined today.\n",
        "createdAt" : "2014-07-23T18:48:43Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "407f6c7a-450d-4e8d-b3cf-9125db427967",
        "parentId" : "1672c49c-c9d9-4bf6-a1c6-d86802f90bfb",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "This won't kill duplicate containers started at different times.  In the current code, only containers that correspond to the most recent container id of a running pod are used.  I'm not sure how you could get into this state, but in this new logic if you have:\n\n```\npod-A-container-A-execution-1\npod-A-container-A-execution-2\n```\n\nrunning at the same time, the kill loop will not remove either 1 or 2.  The algorithm should guarantee that only the later execution of 2 should be running at any one time.  \n",
        "createdAt" : "2014-07-23T21:35:15Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "eb7e71b9-adeb-4c22-a73a-31cfb94baf3d",
        "parentId" : "1672c49c-c9d9-4bf6-a1c6-d86802f90bfb",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "I don't think this will realistically happen (or at least the probability of it happening has not changed from the code before I think). Before we start a container (and only one thread is ever attempting to start a container for a given pod) we check if the container exists by listing Docker containers and checking podFullName and containerName. If there is a race there (maybe the last Kubelet tried to start it and Docker is just getting around to do that), it is a race that exists in the current code too. I don't _think_ this interacts badly with the asyn kills since they won't get touched for a pod in progress.\n\nThe argument could be made that if a user maliciously made the container name then we could run into this (the current code would just arbitrarily kill the bad one or the good one). We can probably catch it by checking that we only exclude one container per entry. The last ID in the name is random, so we'd need to talk to Docker to see which one is newer and keep that one. I'm not sure that is worth worrying about, but if you do I can have a PR to do that.\n",
        "createdAt" : "2014-07-23T21:47:27Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "81bb8bdb-2a5d-42ba-aabd-cd2dfb9e0af0",
        "parentId" : "1672c49c-c9d9-4bf6-a1c6-d86802f90bfb",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I guess I'm worried about weakening the syncloop invariant \"after each sync only one instance of each container is running\".  Whatever new solution we have breaking the sync loop up should still have that property within a deterministic period of time.  Can you make sync pod guarantee that property instead, with a separate guarantee here that any pod not synced gets all it's containers killed?\n",
        "createdAt" : "2014-07-23T22:36:13Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "a01ddb5b-49a6-4ebd-ab0f-4d8a4b111820",
        "parentId" : "1672c49c-c9d9-4bf6-a1c6-d86802f90bfb",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "Added logic to kill any duplicate containers in SyncPod. Also added a unit test to verify the behavior. PTAL.\n",
        "createdAt" : "2014-07-23T23:11:43Z",
        "updatedAt" : "2014-07-24T20:48:22Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "b131da1cf5c7705a3b25bcae9f2d12e93e0f1a23",
    "line" : null,
    "diffHunk" : "@@ -1,1 +473,477 @@\t\t// Don't kill containers that are in the desired pods.\n\t\tpodFullName, containerName := parseDockerName(container.Names[0])\n\t\tif _, ok := desiredContainers[podContainer{podFullName, containerName}]; !ok {\n\t\t\terr = kl.killContainer(*container)\n\t\t\tif err != nil {"
  },
  {
    "id" : "5409e73d-046c-4603-a7d5-4fc7258f8468",
    "prId" : 551,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60c52026-6cea-44bf-9b65-bbb362bc4e3f",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Shouldn't an integration test kubelet let me test all interfaces, not just the two here?  What if I want to mock the cAdvisor client for a cAdvisor specific integration test?\n",
        "createdAt" : "2014-07-22T20:25:08Z",
        "updatedAt" : "2014-07-22T21:49:42Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "ded67ead1ee73d48ce761e3aeac870ea48a9d407",
    "line" : null,
    "diffHunk" : "@@ -1,1 +75,79 @@// TODO: add more integration tests, and expand parameter list as needed.\nfunc NewIntegrationTestKubelet(hn string, dc DockerInterface) *Kubelet {\n\treturn &Kubelet{\n\t\thostname:     hn,\n\t\tdockerClient: dc,"
  },
  {
    "id" : "427b8465-a686-4552-be29-a5bb641f8afd",
    "prId" : 457,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bf6a35d-5f00-4e44-96c5-d24a44d950d7",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Perhaps do this in New()?\n",
        "createdAt" : "2014-07-15T20:36:04Z",
        "updatedAt" : "2014-07-21T21:47:26Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "617a71cc-edda-4bb6-9979-9dca545a0ae7",
        "parentId" : "2bf6a35d-5f00-4e44-96c5-d24a44d950d7",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Please move to the New() function.\n",
        "createdAt" : "2014-07-17T17:46:13Z",
        "updatedAt" : "2014-07-21T21:47:26Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "67aa9b8e-4a83-4f7f-bb6c-be37da996172",
        "parentId" : "2bf6a35d-5f00-4e44-96c5-d24a44d950d7",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "This should probably be locked down further eventually - maybe a TODO to whitelist logs we are willing to serve\n",
        "createdAt" : "2014-07-17T17:56:03Z",
        "updatedAt" : "2014-07-21T21:47:26Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "7bd39678-f024-4037-9360-14635a62756a",
        "parentId" : "2bf6a35d-5f00-4e44-96c5-d24a44d950d7",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "@lavalamp: New() function is only called in kubelet_test.go, not by any production code. I added a TODO there for refactory later. But don't think that change belongs here.\n\n@thockin Done. \n",
        "createdAt" : "2014-07-18T16:35:50Z",
        "updatedAt" : "2014-07-21T21:47:26Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "8f981379-90af-4283-8f5f-66442f71b8e0",
        "parentId" : "2bf6a35d-5f00-4e44-96c5-d24a44d950d7",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Wow, that's a bug. OK, yeah fix in another PR.\n",
        "createdAt" : "2014-07-18T17:19:48Z",
        "updatedAt" : "2014-07-21T21:47:26Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc921b8c65a4ca16443ac03c3d44545cefbde479",
    "line" : null,
    "diffHunk" : "@@ -1,1 +99,103 @@func (kl *Kubelet) RunKubelet(dockerEndpoint, configPath, manifestURL string, etcdServers []string, address string, port uint) {\n\tif kl.LogServer == nil {\n\t\tkl.LogServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\")))\n\t}\n\tif kl.CadvisorClient == nil {"
  },
  {
    "id" : "acf0e11b-2638-4a00-9168-9b96ac90f9ef",
    "prId" : 452,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b82cbbd0-4907-4f3e-8e48-7ca545003ab0",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I don't think tehre should be an else here, once you explicitly support EmptyDir\n",
        "createdAt" : "2014-07-15T20:41:10Z",
        "updatedAt" : "2014-07-17T22:26:07Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb2843498df6d534741fa7554bc36502dc9c603a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +193,197 @@\t\t\t// DEPRECATED: VolumeMount.MountType will be handled by the Volume struct.\n\t\t\tbasePath = fmt.Sprintf(\"%s:%s\", volume.MountPath, volume.MountPath)\n\t\t} else {\n\t\t\t// TODO(jonesdl) This clause should be deleted and an error should be thrown. The default\n\t\t\t// behavior is now supported by the EmptyDirectory type."
  },
  {
    "id" : "d3114655-e5c6-4102-9d4b-7178d9b34ec9",
    "prId" : 377,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c41cf4e-cb02-460b-8823-cfb6cd59296e",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "idiomatically I think this should be:\n\nif kl.CadvisorClient, err := cadvisor.NewClient(\"http://127.0.0.1:5000\"); err != nil {\n    glog.Errorf(\"Error on creating cadvisor client: %v\", err)\n}\n\nI would also argue that 127.0.0.1 be replaced with \"localhost\"\n",
        "createdAt" : "2014-07-09T04:11:48Z",
        "updatedAt" : "2014-07-09T04:11:48Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "4803b70d-4526-40f9-865b-797b2979967d",
        "parentId" : "2c41cf4e-cb02-460b-8823-cfb6cd59296e",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I just realized that field init does not work with := assignment.  Sigh.  Go.\n",
        "createdAt" : "2014-07-09T04:47:23Z",
        "updatedAt" : "2014-07-09T04:47:23Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "9eb688df-0135-4485-80bf-020a6ac9aebe",
        "parentId" : "2c41cf4e-cb02-460b-8823-cfb6cd59296e",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "On Tue, Jul 8, 2014 at 9:47 PM, Tim Hockin notifications@github.com wrote:\n\n> In pkg/kubelet/kubelet.go:\n> \n> > @@ -108,6 +109,13 @@ const (\n> >  // Starts background goroutines. If config_path, manifest_url, or address are empty,\n> >  // they are not watched. Never returns.\n> >  func (kl *Kubelet) RunKubelet(dockerEndpoint, config_path, manifest_url, etcd_servers, address string, port uint) {\n> > -   if kl.CadvisorClient == nil {\n> > -       var err error\n> > -       kl.CadvisorClient, err = cadvisor.NewClient(\"http://127.0.0.1:5000\")\n> \n> I just realized that field init does not work with := assignment. Sigh. Go.\n> \n> Yeah, learnt it today too. :-)\n> \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/377/files#r14693429\n> .\n",
        "createdAt" : "2014-07-09T05:31:45Z",
        "updatedAt" : "2014-07-09T05:31:45Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0f94757a4680f517b3327cad1b52c5e0d62410b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +112,116 @@\tif kl.CadvisorClient == nil {\n\t\tvar err error\n\t\tkl.CadvisorClient, err = cadvisor.NewClient(\"http://127.0.0.1:5000\")\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Error on creating cadvisor client: %v\", err)"
  },
  {
    "id" : "99bdafe2-6032-429b-837b-c94ee6bffbc3",
    "prId" : 365,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53f054d5-6ba6-442d-937c-daca4188060f",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Is the HTTP request synchronous?\n",
        "createdAt" : "2014-07-07T21:54:47Z",
        "updatedAt" : "2014-07-09T19:01:52Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "e989bb79-08da-43c6-a130-f30d199e4d16",
        "parentId" : "53f054d5-6ba6-442d-937c-daca4188060f",
        "authorId" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "body" : "@bgrant0607 I think it is. Normally, most go functions are synchronous and we could always start a new goroutine if we want async. \n",
        "createdAt" : "2014-07-07T23:02:51Z",
        "updatedAt" : "2014-07-09T19:01:52Z",
        "lastEditedBy" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "tags" : [
        ]
      },
      {
        "id" : "a5ddf797-01c3-4b2f-89d2-f81cce4f4412",
        "parentId" : "53f054d5-6ba6-442d-937c-daca4188060f",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "@monnand I figured it was. Given that the application might not respond for tens of seconds, if at all, it should be async.\n",
        "createdAt" : "2014-07-07T23:39:23Z",
        "updatedAt" : "2014-07-09T19:01:52Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "6247c272-98cf-46fb-b729-be3a3f3695cd",
        "parentId" : "53f054d5-6ba6-442d-937c-daca4188060f",
        "authorId" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "body" : "@bgrant0607 Since this function itself will be called within a separate goroutine, so it won't block other goroutines unless they need the results from this function (in this case, they will be blocked even if it is async.)\n\nOne approach to avoid the problem you mentioned is to start one goroutine per iteration. WDYT?\n",
        "createdAt" : "2014-07-07T23:55:49Z",
        "updatedAt" : "2014-07-09T19:01:52Z",
        "lastEditedBy" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "tags" : [
        ]
      },
      {
        "id" : "3e771581-1961-47f8-aa46-e6a2ddbef335",
        "parentId" : "53f054d5-6ba6-442d-937c-daca4188060f",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "One goroutine per iteration SGTM.\n",
        "createdAt" : "2014-07-08T02:53:04Z",
        "updatedAt" : "2014-07-09T19:01:52Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "e50cf7f9-aa17-4d51-b687-f968aba433b1",
        "parentId" : "53f054d5-6ba6-442d-937c-daca4188060f",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Yeah, I'd prefer to leave this inline for now.  It will only delay the specific pod, not all pods.\n\nWhen I refactor to move the health checking into its own loop, I'll address this there.\n",
        "createdAt" : "2014-07-08T05:30:27Z",
        "updatedAt" : "2014-07-09T19:01:52Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "41c6680943c620b90c598dc6d12a817cfb625820",
    "line" : null,
    "diffHunk" : "@@ -1,1 +726,730 @@\t\t\t// TODO: This should probably be separated out into a separate goroutine.\n\t\t\thealthy, err := kl.healthy(container, dockerContainer)\n\t\t\tif err != nil {\n\t\t\t\tglog.V(1).Infof(\"health check errored: %v\", err)\n\t\t\t\tcontinue"
  },
  {
    "id" : "b3c6f7f4-e249-436b-ab69-48c768e51aa3",
    "prId" : 358,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2f2ba7d-335a-45f8-b8b8-2bd0d783909c",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Do we really want to accept \"TcP\" (I mean, I suppose why not, but...)\n",
        "createdAt" : "2014-07-08T04:59:37Z",
        "updatedAt" : "2014-07-08T22:23:00Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "92fb7ce9-6db6-4565-94bc-821e53d206b8",
        "parentId" : "f2f2ba7d-335a-45f8-b8b8-2bd0d783909c",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I could be talked out of this.  It was coded to accept lower-case before, but the original manifest format was uppercase.  I figured making it caseless was harmless.  Object now or forever STFU. :P\n",
        "createdAt" : "2014-07-08T06:22:38Z",
        "updatedAt" : "2014-07-08T22:23:00Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "66e10359-efa0-40dd-b85f-5aa6fa7f6af1",
        "parentId" : "f2f2ba7d-335a-45f8-b8b8-2bd0d783909c",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "ok, not objecting and sealing my mouth ;)\n",
        "createdAt" : "2014-07-08T19:56:46Z",
        "updatedAt" : "2014-07-08T22:23:00Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "2eb2784725710e7e831e046f332bb3086a53856e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +311,315 @@\t\t// See http://stackoverflow.com/questions/20428302/binding-a-port-to-a-host-interface-using-the-rest-api\n\t\tvar protocol string\n\t\tswitch strings.ToUpper(port.Protocol) {\n\t\tcase \"UDP\":\n\t\t\tprotocol = \"/udp\""
  },
  {
    "id" : "d424e629-a7f7-45b7-807c-09e959ce68c9",
    "prId" : 358,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8eaeecc-25ea-41d3-8e4e-16271d268ecb",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Any reason not to do:\n\nfor _, manifest := range allManifests {\n  ...\n}\n",
        "createdAt" : "2014-07-08T05:00:24Z",
        "updatedAt" : "2014-07-08T22:23:00Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "cbebc266-9fc9-4e7f-8eb8-bc6f011abe25",
        "parentId" : "d8eaeecc-25ea-41d3-8e4e-16271d268ecb",
        "authorId" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "body" : "In general, I prefer `for _, obj := range someSlice`. However, specific to this one (and another one above): Is there any reason we want to use `[]api.ContainerManifest` instead of `[]*api.ContainerManifest`? I know we may want to have some kind of immutable semantics, but `api.ContainerManifest` may be very large and might be inefficient to copy it around.\n",
        "createdAt" : "2014-07-08T05:20:13Z",
        "updatedAt" : "2014-07-08T22:23:00Z",
        "lastEditedBy" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "tags" : [
        ]
      },
      {
        "id" : "cf31f2cc-9b18-439f-a67a-3eacc9603c91",
        "parentId" : "d8eaeecc-25ea-41d3-8e4e-16271d268ecb",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I read this in one of the go blogs or docs or something.  ranging over structs always makes a copy of the struct, even if you do not modify it.  Do not count on the compiler to elide that copy.  The suggested solution was to never for-each over structs unless you know they will be small.\n\nThis bit us hard in internal code where things like the commandline ended up being hundreds of KB or more.\n\nThat stuck with me, and I adopted this syntax instead.  I'll switch it back, if you want.  I know I am but a cub in the Go world.   Thi sjust feels like a premature pessimization that the language has not figured out how to avoid.\n",
        "createdAt" : "2014-07-08T06:30:28Z",
        "updatedAt" : "2014-07-08T22:23:00Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "cb87929e-357e-4c50-b71b-b4f808806966",
        "parentId" : "d8eaeecc-25ea-41d3-8e4e-16271d268ecb",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "@monnand Precisely.  Storing it by value in the slice is fine, but ranging it by value is dumb.  I have a TODO in this PR I think to collect manifests by pointer instead.  One change at a time. :)\n",
        "createdAt" : "2014-07-08T06:36:56Z",
        "updatedAt" : "2014-07-08T22:23:00Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "2eb2784725710e7e831e046f332bb3086a53856e",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +766,770 @@\textract := func(p *api.Port) int { return p.HostPort }\n\tfor i := range allManifests {\n\t\tmanifest := &allManifests[i]\n\t\terr := api.AccumulateUniquePorts(manifest.Containers, allPorts, extract)\n\t\tif err != nil {"
  },
  {
    "id" : "7830407a-ac77-431c-b5e8-639db8ebfbe9",
    "prId" : 356,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc9278c0-cab8-4bf3-9741-2d350cd346df",
        "parentId" : null,
        "authorId" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "body" : "@smarterclayton How about close(ch)? Since the channel is just used for sending a `completed` signal.\nAlso how about `make(chan struct{})` or `make(chan empty)`, which is more preferred go convention[1] IMO.\n\n[1]http://blog.golang.org/pipelines\n",
        "createdAt" : "2014-07-15T06:42:39Z",
        "updatedAt" : "2014-07-22T01:29:51Z",
        "lastEditedBy" : "227eb550-8b08-4420-9a78-279f840bd8de",
        "tags" : [
        ]
      },
      {
        "id" : "462ba0c8-eba2-40b2-80f4-241834ab1e0f",
        "parentId" : "cc9278c0-cab8-4bf3-9741-2d350cd346df",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Agreed, should be a separate pull request though I think.\n",
        "createdAt" : "2014-07-15T22:22:31Z",
        "updatedAt" : "2014-07-22T01:29:51Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "7767c2a2ace79f55d1f8e4cb75eb1e3bd48d2db9",
    "line" : 577,
    "diffHunk" : "@@ -1,1 +378,382 @@\t\t\tdockerIdsToKeep[id] = empty{}\n\t\t}\n\t\tch <- true\n\t}()\n\tif len(pods) > 0 {"
  },
  {
    "id" : "cf1ac1ce-40f4-49a6-8d68-d09c7e8e94c5",
    "prId" : 351,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3fc79f7-0cab-4964-a95d-c5552a665353",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Is this a clean implementation or a copy of Docker code?  Just concerned about copyright and attribution.\n",
        "createdAt" : "2014-07-07T17:49:13Z",
        "updatedAt" : "2014-07-08T10:27:13Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "e4461bd8-1e99-4392-adfb-4fbd269122f7",
        "parentId" : "a3fc79f7-0cab-4964-a95d-c5552a665353",
        "authorId" : "06be2998-8e58-49a1-960a-716ab2ae239d",
        "body" : "Yes, that's a clean, simplified 'reimplementation' like pretty much all clients have it.\n",
        "createdAt" : "2014-07-08T10:18:17Z",
        "updatedAt" : "2014-07-08T10:27:13Z",
        "lastEditedBy" : "06be2998-8e58-49a1-960a-716ab2ae239d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b63a275ec3d3c3346fe56777e35d486e17c0eff7",
    "line" : null,
    "diffHunk" : "@@ -1,1 +335,339 @@// https://github.com/dotcloud/docker/issues/6876\n// So this can be deprecated at some point.\nfunc parseImageName(image string) (string, string) {\n\ttag := \"\"\n\tparts := strings.SplitN(image, \"/\", 2)"
  },
  {
    "id" : "344761ba-224d-4798-8fbf-fea4875b5132",
    "prId" : 320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2246f89-b086-470c-956f-de644f8f5f3b",
        "parentId" : null,
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "This parallelizes a single manifest, but what about multiple manifests? It seems that those will still be serialized since we only get one update at a time no?\n",
        "createdAt" : "2014-07-01T16:45:03Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      },
      {
        "id" : "816a94b3-7f68-41ee-9255-787be6874433",
        "parentId" : "c2246f89-b086-470c-956f-de644f8f5f3b",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "This parallelizes the manifests as far as I can tell?\n",
        "createdAt" : "2014-07-01T16:53:23Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "02fad6ce-27e0-4232-8f14-443a7e500cf4",
        "parentId" : "c2246f89-b086-470c-956f-de644f8f5f3b",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Yes, this parallelizes all manifests within a manifest list.\n\nIt is true that it doesn't parallelize across multiple sources (e.g. file, etcd, http), we could do that as well, but I'd rather do it in a different PR. \n",
        "createdAt" : "2014-07-01T16:58:32Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "c91bd8cd-a5ef-4662-9f97-441c421ab823",
        "parentId" : "c2246f89-b086-470c-956f-de644f8f5f3b",
        "authorId" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "body" : "SGTM, I started work on that, but let me know if you are working on that today and I'll drop it :)\n",
        "createdAt" : "2014-07-01T16:59:32Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b7abca6a-d6be-4008-a014-3b5ca0e90529",
        "tags" : [
        ]
      }
    ],
    "commit" : "1798e0fea381a35fb84d25552f112a73023ea104",
    "line" : null,
    "diffHunk" : "@@ -1,1 +709,713 @@\t}()\n\tif len(config) > 0 {\n\t\twaitGroup.Wait()\n\t\tclose(keepChannel)\n\t}"
  },
  {
    "id" : "291ed7d9-13ed-48c7-8b09-042d96888184",
    "prId" : 320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99b19c49-502f-4ff4-bd49-3b55a1b31bd6",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "first `defer util.HandleCrash()`\n",
        "createdAt" : "2014-07-01T16:51:09Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "914d3260-51d9-48ee-993e-b2d01348aca7",
        "parentId" : "99b19c49-502f-4ff4-bd49-3b55a1b31bd6",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Done.\n",
        "createdAt" : "2014-07-01T16:59:21Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "ab32d3ce-6ff6-426d-b9de-f6dbdea86993",
        "parentId" : "99b19c49-502f-4ff4-bd49-3b55a1b31bd6",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Hm-- it seems the `defer util.HandleCrash()` has vanished again?\n",
        "createdAt" : "2014-07-01T19:13:21Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "7d46cb59-3b06-4a88-9547-06e219e2f509",
        "parentId" : "99b19c49-502f-4ff4-bd49-3b55a1b31bd6",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "It was in syncManifest, moved it here.\n",
        "createdAt" : "2014-07-01T19:38:02Z",
        "updatedAt" : "2014-07-01T19:38:02Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "e95e3ab8-3d20-481e-8c23-98c12180c676",
        "parentId" : "99b19c49-502f-4ff4-bd49-3b55a1b31bd6",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Ah, I missed that. It's better here anyway, though-- it protects slightly more and readers don't have to wonder about it.\n",
        "createdAt" : "2014-07-01T19:43:38Z",
        "updatedAt" : "2014-07-01T19:43:38Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "1798e0fea381a35fb84d25552f112a73023ea104",
    "line" : null,
    "diffHunk" : "@@ -1,1 +696,700 @@\t\tgo func() {\n\t\t\tdefer util.HandleCrash()\n\t\t\tdefer waitGroup.Done()\n\t\t\terr := kl.syncManifest(&manifest, keepChannel)\n\t\t\tif err != nil {"
  },
  {
    "id" : "72c270a2-59ed-4b1f-89da-0d736d4e6109",
    "prId" : 320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdd51260-854a-4747-8637-6dc8608afb93",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Given that range makes copies of structs, I have started always ranging them as\n\nfor i := range config {\n    manifest := &config[i]\n\nIt's unfortunate that the language doesn't handle this cleanly.\n",
        "createdAt" : "2014-07-01T17:03:59Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "3a79dd08-3a9c-4dfd-9920-523bbae0bc2e",
        "parentId" : "cdd51260-854a-4747-8637-6dc8608afb93",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "The copy gives read-only semantics, which is probably appropriate here. Worrying about the overhead of copying a struct is almost certainly premature optimization.\n",
        "createdAt" : "2014-07-01T17:06:46Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "91081770-52e6-4bc8-87b8-1c501da587eb",
        "parentId" : "cdd51260-854a-4747-8637-6dc8608afb93",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Although, having said that, I see we're taking the address before giving this to syncManifest, which means it's probably going to be a heap allocation. I could go either way.\n",
        "createdAt" : "2014-07-01T17:08:42Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "af551c8b-acec-47ad-9c29-3e5fc8043ac3",
        "parentId" : "cdd51260-854a-4747-8637-6dc8608afb93",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Reading around on go, there's mixed feelings.  Some seem to advocate never\nranging structs by value, others call it premature.  To me it is\ninstinctive to not copy structs, especially when we know how large some of\nthe internal container specs become (with lots of flags and stuff)\n\nOn Tue, Jul 1, 2014 at 10:08 AM, Daniel Smith notifications@github.com\nwrote:\n\n> In pkg/kubelet/kubelet.go:\n> \n> > +// Sync the configured list of containers (desired state) with the host current state\n> > +func (kl *Kubelet) SyncManifests(config []api.ContainerManifest) error {\n> > -   glog.Infof(\"Desired: %+v\", config)\n> > -   var err error\n> > -   dockerIdsToKeep := map[DockerId]bool{}\n> > -   mapLock := sync.Mutex{}\n> > -   waitGroup := sync.WaitGroup{}\n> >   +\n> > -   // Check for any containers that need starting\n> > -   for _, manifest := range config {\n> \n> Although, having said that, I see we're taking the address before giving\n> this to syncManifest, which means it's probably going to be a heap\n> allocation. I could go either way.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/GoogleCloudPlatform/kubernetes/pull/320/files#r14416373\n> .\n",
        "createdAt" : "2014-07-01T17:11:24Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "8ef59ea2-321f-4de7-aa9e-0c99cec0034c",
        "parentId" : "cdd51260-854a-4747-8637-6dc8608afb93",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yeah, I could really go either way on this, only because our structs here are likely to get quite big.\n",
        "createdAt" : "2014-07-01T17:14:30Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "1798e0fea381a35fb84d25552f112a73023ea104",
    "line" : null,
    "diffHunk" : "@@ -1,1 +692,696 @@\n\t// Check for any containers that need starting\n\tfor _, manifest := range config {\n\t\twaitGroup.Add(1)\n\t\tgo func() {"
  },
  {
    "id" : "2a99d476-469c-48dc-a1a4-13695eae3e6f",
    "prId" : 320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43f1c22c-2b5d-4029-8cce-051850904b98",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Same comment as elsewhere about ranging - I'm not sure it matters much, but it feels like a \"don't pessimize\" situation\n",
        "createdAt" : "2014-07-01T17:07:40Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "1798e0fea381a35fb84d25552f112a73023ea104",
    "line" : null,
    "diffHunk" : "@@ -1,1 +656,660 @@\t}\n\tkeepChannel <- netId\n\tfor _, container := range manifest.Containers {\n\t\tcontainerId, err := kl.getContainerId(manifest, &container)\n\t\tif err != nil {"
  },
  {
    "id" : "c11fddc0-c1bf-4d2f-8c77-1c17b7efe771",
    "prId" : 320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8584ca41-632b-4440-8dba-b43a2d9a6942",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Can't do this after the range. It'll never stop...\n",
        "createdAt" : "2014-07-01T17:12:03Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "06c2f216-5fa5-4968-abb2-bf9a6b21460f",
        "parentId" : "8584ca41-632b-4440-8dba-b43a2d9a6942",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "It's been 9 minutes and travis is still going :)\n",
        "createdAt" : "2014-07-01T17:22:04Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "57cf89cf-ee40-411f-9b2b-5410fb8d9621",
        "parentId" : "8584ca41-632b-4440-8dba-b43a2d9a6942",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Done, ptal.\n",
        "createdAt" : "2014-07-01T17:49:37Z",
        "updatedAt" : "2014-07-01T19:20:24Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "1798e0fea381a35fb84d25552f112a73023ea104",
    "line" : null,
    "diffHunk" : "@@ -1,1 +709,713 @@\t}()\n\tif len(config) > 0 {\n\t\twaitGroup.Wait()\n\t\tclose(keepChannel)\n\t}"
  },
  {
    "id" : "abab663e-8324-4399-8f97-9bed015806ac",
    "prId" : 320,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d30e8f1-c0a4-4626-930e-e237fb2253b1",
        "parentId" : null,
        "authorId" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "body" : "`defer close(keepChannel)`? I thought it is the writer's responsibility to close the channel.\n",
        "createdAt" : "2014-07-01T19:20:58Z",
        "updatedAt" : "2014-07-01T19:20:58Z",
        "lastEditedBy" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "tags" : [
        ]
      },
      {
        "id" : "72674232-e4fd-4f8a-835d-7c8dc452e75f",
        "parentId" : "0d30e8f1-c0a4-4626-930e-e237fb2253b1",
        "authorId" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "body" : "Never mind. This method is called by several goroutine and there are several writers. Please ignore my comment above.\n",
        "createdAt" : "2014-07-01T19:22:13Z",
        "updatedAt" : "2014-07-01T19:22:31Z",
        "lastEditedBy" : "ec51f754-9844-4e72-8c9e-8d1105d99228",
        "tags" : [
        ]
      },
      {
        "id" : "50fc2125-534f-488a-bba3-df67ab968c32",
        "parentId" : "0d30e8f1-c0a4-4626-930e-e237fb2253b1",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Note that close() is optional anyway, and is only used to signal that no further values will be sent down the channel.\n",
        "createdAt" : "2014-07-01T19:41:34Z",
        "updatedAt" : "2014-07-01T19:41:34Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "1798e0fea381a35fb84d25552f112a73023ea104",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +641,645 @@\nfunc (kl *Kubelet) syncManifest(manifest *api.ContainerManifest, keepChannel chan<- DockerId) error {\n\t// Make sure we have a network container\n\tnetId, err := kl.getNetworkContainerId(manifest)\n\tif err != nil {"
  },
  {
    "id" : "208b3150-6c7f-4d18-b5e5-48880e2abe3c",
    "prId" : 254,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8236acd3-fc31-4459-b0bd-02531509cd6a",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "FYI: Once I get the validation change in (just need to finish tests) we can change \"--\" to \"_\" which feels a lot less kludgey :)\n",
        "createdAt" : "2014-06-27T18:54:58Z",
        "updatedAt" : "2014-06-27T21:04:27Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c5562ef77f51f1963e4a02fdfb6b148ddb9968c",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +183,187 @@\t\t// Skip containers that we didn't create to allow users to manually\n\t\t// spin up their own containers if they want.\n\t\tif !strings.HasPrefix(value.Names[0], \"/\"+containerNamePrefix+\"--\") {\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "45a86ecf-b89e-4cb6-b31f-d858d5c26853",
    "prId" : 216,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1cc0618-95ee-48fb-993f-5d64a136845a",
        "parentId" : null,
        "authorId" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "body" : "you could use `log.Println` here, as there is no arg.\n",
        "createdAt" : "2014-06-24T04:51:13Z",
        "updatedAt" : "2014-06-24T05:11:42Z",
        "lastEditedBy" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "tags" : [
        ]
      },
      {
        "id" : "9a602e40-959e-439f-8b16-e3da1304717f",
        "parentId" : "d1cc0618-95ee-48fb-993f-5d64a136845a",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Everywhere else uses printf() so I think consistency wins, but I don't feel too strongly..  I just see no other instances of that.\n",
        "createdAt" : "2014-06-24T05:12:58Z",
        "updatedAt" : "2014-06-24T05:12:58Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "60ad86c972ac3f82900424201b20bc82d41e7752",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +654,658 @@\t\t}\n\t\tif !exists {\n\t\t\tlog.Printf(\"Network container doesn't exist, creating\")\n\t\t\tnetName, err = kl.createNetworkContainer(&manifest)\n\t\t\tif err != nil {"
  },
  {
    "id" : "420f3ef9-3d64-4298-803b-f5a51a2a6497",
    "prId" : 213,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4880fd5-abee-48ac-8985-6f94a8370ebf",
        "parentId" : null,
        "authorId" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "body" : "do you still want to try parsing an 'array of manifest' if that happens?\n",
        "createdAt" : "2014-06-24T05:12:46Z",
        "updatedAt" : "2014-06-24T23:57:46Z",
        "lastEditedBy" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "tags" : [
        ]
      },
      {
        "id" : "2d8ea142-9024-459f-8561-9e35035eb58d",
        "parentId" : "a4880fd5-abee-48ac-8985-6f94a8370ebf",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Yes, that's the intention. If data is a []ContainerManifest, trying to put it into a ContainerManifest will not give an error but also won't set any of the fields. Our docs say that the version field is mandatory. (Adding this as a comment.)\n",
        "createdAt" : "2014-06-24T17:24:41Z",
        "updatedAt" : "2014-06-24T23:57:46Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7968ce00b2a66b9d7d3067404e406d19a1ddd55",
    "line" : null,
    "diffHunk" : "@@ -1,1 +523,527 @@\t\t// Our docs say that the version field is mandatory, so using that to judge wether\n\t\t// this was actually successful.\n\t\tsingleErr = fmt.Errorf(\"got blank version field\")\n\t}\n\tif singleErr == nil {"
  },
  {
    "id" : "519afb57-1bff-4b6d-9b17-785fda219f2d",
    "prId" : 213,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1b850df-9825-4343-9f02-cd2b02b7560f",
        "parentId" : null,
        "authorId" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "body" : "can we just return here and \n",
        "createdAt" : "2014-06-24T05:13:51Z",
        "updatedAt" : "2014-06-24T23:57:46Z",
        "lastEditedBy" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "tags" : [
        ]
      },
      {
        "id" : "ba9b4520-d9c7-47ed-b079-fa5c576c3dd5",
        "parentId" : "d1b850df-9825-4343-9f02-cd2b02b7560f",
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "No, to both return questions, because we're not sure if the person reading the logs is going to care about the single or multiple manifest unmarshalling attempt, so we need to put both in the logs, as is done at the end. (added comment)\n",
        "createdAt" : "2014-06-24T17:26:39Z",
        "updatedAt" : "2014-06-24T23:57:46Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7968ce00b2a66b9d7d3067404e406d19a1ddd55",
    "line" : null,
    "diffHunk" : "@@ -1,1 +537,541 @@\t// done at the end. Hence not returning early here.\n\tif multiErr == nil && len(manifests) == 0 {\n\t\tmultiErr = fmt.Errorf(\"no elements in ContainerManifest array\")\n\t}\n\tif multiErr == nil && manifests[0].Version == \"\" {"
  },
  {
    "id" : "5f3f0a88-bacb-492c-bb0d-0ba1132f9502",
    "prId" : 213,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59589ebf-2c32-483f-a6a2-4dc2e0e2a17b",
        "parentId" : null,
        "authorId" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "body" : "can we just return here?\n",
        "createdAt" : "2014-06-24T05:13:56Z",
        "updatedAt" : "2014-06-24T23:57:46Z",
        "lastEditedBy" : "6001b825-f0f9-4fc3-b624-34a076b031e1",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7968ce00b2a66b9d7d3067404e406d19a1ddd55",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +540,544 @@\t}\n\tif multiErr == nil && manifests[0].Version == \"\" {\n\t\tmultiErr = fmt.Errorf(\"got blank version field\")\n\t}\n\tif multiErr == nil {"
  },
  {
    "id" : "845f0e55-76cb-4790-b313-7b61bf288377",
    "prId" : 174,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2fb3db5f-f2e6-4dcd-a647-813577205cb8",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "comment explaining what this means?  (I assume no data?)\n",
        "createdAt" : "2014-06-20T04:02:54Z",
        "updatedAt" : "2014-06-20T04:22:14Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10a64856d0ce30c8cdc25a8e9fd0d2eb64739da",
    "line" : null,
    "diffHunk" : "@@ -1,1 +667,671 @@\t}\n\t// When the stats data for the container is not available yet.\n\tif info.StatsPercentiles == nil {\n\t\treturn nil, nil\n\t}"
  },
  {
    "id" : "07492f45-58c3-4d50-8abf-164010e47ed5",
    "prId" : 173,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9178c62e-887c-4589-88f6-8d374f26541e",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Can you add a defer f.Close()? Looks like we were leaving these open.\n",
        "createdAt" : "2014-06-19T20:47:00Z",
        "updatedAt" : "2014-06-20T16:32:40Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "f9402d44-650c-4c2e-9671-a83b7faa86f0",
        "parentId" : "9178c62e-887c-4589-88f6-8d374f26541e",
        "authorId" : "0e32dd54-124a-4653-a846-2445d140e0ac",
        "body" : "Good catch. Added.\n",
        "createdAt" : "2014-06-19T22:00:22Z",
        "updatedAt" : "2014-06-20T16:32:40Z",
        "lastEditedBy" : "0e32dd54-124a-4653-a846-2445d140e0ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecf7d1147764ca7e2950c239dfc67edccae89a11",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +374,378 @@\tif file, err = os.Open(name); err != nil {\n\t\treturn manifest, err\n\t}\n\tdefer file.Close()\n"
  },
  {
    "id" : "b39d233f-cb64-4ffc-8f5d-5399fcfd6057",
    "prId" : 71,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9a09491-824f-4a73-a4d4-c3fd8e0b99a1",
        "parentId" : null,
        "authorId" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "body" : "What does the bool here mean?  You could used name return params to be a little self documenting.  Or write some godoc compatible comments. http://blog.golang.org/godoc-documenting-go-code\n",
        "createdAt" : "2014-06-12T04:44:10Z",
        "updatedAt" : "2014-06-12T18:30:11Z",
        "lastEditedBy" : "e19009d8-ed5c-45bb-b5ce-4f8d956c6c45",
        "tags" : [
        ]
      },
      {
        "id" : "88d3067f-8ff6-4d47-bd0d-a3447aefce92",
        "parentId" : "c9a09491-824f-4a73-a4d4-c3fd8e0b99a1",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Added godoc.  PTAL\n\nThanks!\n--brendan\n",
        "createdAt" : "2014-06-12T18:30:29Z",
        "updatedAt" : "2014-06-12T18:30:29Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "c36a7896fd463a5b3241291ebe553c014e7cff11",
    "line" : null,
    "diffHunk" : "@@ -1,1 +156,160 @@// matches 'name'.  It returns the name of the container, or empty string, if the container isn't found.\n// it returns true if the container is found, false otherwise, and any error that occurs.\nfunc (kl *Kubelet) GetContainerID(name string) (string, bool, error) {\n\tcontainerList, err := kl.DockerClient.ListContainers(docker.ListContainersOptions{})\n\tif err != nil {"
  }
]