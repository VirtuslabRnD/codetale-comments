[
  {
    "id" : "01e4f927-5477-43c6-8d1f-cf5f292a1522",
    "prId" : 23567,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9aea1307-0c26-4ac4-a250-b7709a04dfe1",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "why not just iterate over a single loop of containerStatuses? \n",
        "createdAt" : "2016-04-08T20:41:41Z",
        "updatedAt" : "2016-05-17T04:30:20Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "563fab80-53b5-4ae3-a08d-cb0b8c76cb90",
        "parentId" : "9aea1307-0c26-4ac4-a250-b7709a04dfe1",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "InitContainer ~= InitContainerStatuses as Container =~ ContainerStatuses.  The parallel works more cleanly I think.\n",
        "createdAt" : "2016-04-13T00:48:17Z",
        "updatedAt" : "2016-05-17T04:30:20Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a53330700ac39ee61109c748fe665cf38581a5d",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +216,220 @@\t// Find the container to update.\n\tcontainerIndex := -1\n\tfor i, c := range status.ContainerStatuses {\n\t\tif c.ContainerID == containerID {\n\t\t\tcontainerIndex = i"
  },
  {
    "id" : "df3ced10-52c8-482c-8c1d-9b154a2d8a4b",
    "prId" : 21448,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "629330d4-bbbc-41cc-a19d-ef7317bc44b7",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Do we need to normalize the status? The api.Pod.Status should already be normalized. I guess it doesn't hurt to normalize again.\n",
        "createdAt" : "2016-02-18T23:17:33Z",
        "updatedAt" : "2016-02-22T18:28:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "846ac6b4-021a-47cb-963a-7c61f89beae5",
        "parentId" : "629330d4-bbbc-41cc-a19d-ef7317bc44b7",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "I think it's safer to normalize it here. Couldn't the api.Pod.Status be from the api server rather than the status manager?\n",
        "createdAt" : "2016-02-18T23:43:42Z",
        "updatedAt" : "2016-02-22T18:28:35Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "f74d474e-6b7a-46ea-95fe-48ad3d77659f",
        "parentId" : "629330d4-bbbc-41cc-a19d-ef7317bc44b7",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The statuses from the apiserver don't need to be normalized because they have limited time precision. It's the internal status kubelet generates that need to be normalized. It doesn't hurt to do it again, of course.\n",
        "createdAt" : "2016-02-18T23:59:08Z",
        "updatedAt" : "2016-02-22T18:28:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "e92f3f03-3bd9-4997-9bf9-25a93e177b62",
        "parentId" : "629330d4-bbbc-41cc-a19d-ef7317bc44b7",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "The normalization also sorts the container statuses though, which could be different from the API server.\n",
        "createdAt" : "2016-02-19T00:04:08Z",
        "updatedAt" : "2016-02-22T18:28:35Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "fbc88218-da4d-4685-90a8-bf823b2a2b6b",
        "parentId" : "629330d4-bbbc-41cc-a19d-ef7317bc44b7",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "You're right. I forgot about the sorting. \n",
        "createdAt" : "2016-02-19T00:38:06Z",
        "updatedAt" : "2016-02-22T18:28:35Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "15d44d182a024f9da4ccc91d56a0a976f3bd715e",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +441,445 @@\t\treturn false\n\t}\n\tnormalizeStatus(&podStatus)\n\n\tif isStatusEqual(&podStatus, &status) {"
  },
  {
    "id" : "adafd7ea-21f6-45e4-b561-d6928e8fb161",
    "prId" : 21438,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2ef924d-20bf-4657-8301-d74b72ffc6a7",
        "parentId" : null,
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "If the cached status is not found, oldStatus will be `nil` here, which I think will lead to a nil pointer dereference in copyStatus?\n",
        "createdAt" : "2016-02-18T00:55:15Z",
        "updatedAt" : "2016-02-18T19:01:18Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "66bdf4f9-a252-4605-a116-d12d75bec1db",
        "parentId" : "a2ef924d-20bf-4657-8301-d74b72ffc6a7",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "ah I meant to set that to the pod.Status, but lost it during rebasing. Will fix it. \n",
        "createdAt" : "2016-02-18T00:57:13Z",
        "updatedAt" : "2016-02-18T19:01:18Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "860a4e3f-a077-450a-a940-f239343fa7dd",
        "parentId" : "a2ef924d-20bf-4657-8301-d74b72ffc6a7",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Done.\n",
        "createdAt" : "2016-02-18T01:04:19Z",
        "updatedAt" : "2016-02-18T19:01:18Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "386453a09d55024208b4a66f923ee81498a33f0a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +219,223 @@\t\toldStatus = &cachedStatus.status\n\t}\n\tstatus, err := copyStatus(oldStatus)\n\tif err != nil {\n\t\treturn"
  },
  {
    "id" : "52cbfb0b-4f6b-451b-9fd8-af6cda1298ab",
    "prId" : 18410,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "155d9f0f-ae44-41b6-af47-3630eb95a9c8",
        "parentId" : null,
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "(nit)\n\n```\nfor i, condition := range status.Conditions {\n```\n\nSame below.\n",
        "createdAt" : "2016-01-04T23:26:20Z",
        "updatedAt" : "2016-01-10T09:35:25Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "502a2eda-4375-4f4e-9a9c-af143eb74558",
        "parentId" : "155d9f0f-ae44-41b6-af47-3630eb95a9c8",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "If implementing like this, the `condition` will be a copy, right? I need to modify the original object, so I use index instead. Is there any other ways to do that?\n",
        "createdAt" : "2016-01-05T00:25:50Z",
        "updatedAt" : "2016-01-10T09:35:25Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "4dcf85a8-967e-40a4-983c-62b7be25c74e",
        "parentId" : "155d9f0f-ae44-41b6-af47-3630eb95a9c8",
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Eh...It seems that you are right, although the higher layer object is copied, all the timestamp are shared. However, I still think referencing the original object is more intuitive than a copied one.\n",
        "createdAt" : "2016-01-05T00:30:30Z",
        "updatedAt" : "2016-01-10T09:35:25Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "974a294e-5980-4a61-88e1-0859517f2a67",
        "parentId" : "155d9f0f-ae44-41b6-af47-3630eb95a9c8",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Oh yeah, I wasn't thinking about the fact that you're modifying the value - good point, keep it as is.\n",
        "createdAt" : "2016-01-05T01:16:49Z",
        "updatedAt" : "2016-01-10T09:35:25Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b4a210d49c58e4e517738912330c294a32377b4",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +459,463 @@\t\tnormalizeTimeStamp(status.StartTime)\n\t}\n\tfor i := range status.Conditions {\n\t\tcondition := &status.Conditions[i]\n\t\tnormalizeTimeStamp(&condition.LastProbeTime)"
  },
  {
    "id" : "eb96d959-dc6d-45d5-8cf0-2c2169b36195",
    "prId" : 18240,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44a25ece-b1f9-46b8-a879-0191242580d6",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Why printing a warning? I assume a new pod would not have the ready condition.\n",
        "createdAt" : "2015-12-05T01:18:18Z",
        "updatedAt" : "2015-12-05T01:18:18Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "f698070e-4a81-4b1c-b190-cf7b218f5a9b",
        "parentId" : "44a25ece-b1f9-46b8-a879-0191242580d6",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "After reading through [kubelet.generatePodStatus](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet.go#L3108), I don't think this should ever happen. I think in all the cases where there wouldn't be a ready condition, there also wouldn't be container statuses, so this function would abort earlier.\n\nI can change it to a higher info level if you prefer though.\n",
        "createdAt" : "2015-12-05T01:26:01Z",
        "updatedAt" : "2015-12-05T01:26:01Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "55e9964e-915d-4da5-b11f-ee8436928907",
        "parentId" : "44a25ece-b1f9-46b8-a879-0191242580d6",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "The logging level is fine if it's something that should not happen at all.\n",
        "createdAt" : "2015-12-05T01:30:01Z",
        "updatedAt" : "2015-12-05T01:30:01Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2430454eea2c1a12eff04b9a2e5386dbb2909a41",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +203,207 @@\t\tstatus.Conditions[readyConditionIndex] = readyCondition\n\t} else {\n\t\tglog.Warningf(\"PodStatus missing PodReady condition: %+v\", status)\n\t\tstatus.Conditions = append(status.Conditions, readyCondition)\n\t}"
  },
  {
    "id" : "02517bb4-7fe5-439d-9aa1-be32e3593b2a",
    "prId" : 17270,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "756ab558-d537-4374-808c-57bf40b57882",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Hmm..determining whether to send an update this late would mean that we need to always get the pod from the apiserver. I don't think we need to move this check.\n\nIf a mirror pod gets recreated, we will eventually see it from the watch, triggering a new call to `SetPodStatus`. At this time, if you check the `apiStatusVersions[newMirrorPod.UID]`, you would find nothing and send the update. \n",
        "createdAt" : "2015-11-19T18:41:51Z",
        "updatedAt" : "2015-11-19T18:41:51Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "2bbd1b88-c3c1-48e9-8b41-2a5f895badb6",
        "parentId" : "756ab558-d537-4374-808c-57bf40b57882",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "So the reason I moved the check to here, is we need the mirror pod UID. I could look it up in the pod manager, but that would require getting a mutex and either constructing the full name artificially. Moving this method to here means that pod.UID is the UID we need to check (mirror pod, when relevant).\n\nAlso, it should be extremely rare that this check ever fails since we only call syncPod if we've already established that an update is needed. The only way this check would fail is if multiple updates were sent in rapid succession. (e.g. status is updated right before syncBatch runs).\n",
        "createdAt" : "2015-11-19T18:54:54Z",
        "updatedAt" : "2015-11-19T18:54:54Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbc5a7d0346b5eecf9c4038aeac70b83e0dff9c1",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +335,339 @@\t\t\treturn\n\t\t}\n\t\tif !m.needsUpdate(pod.UID, status) {\n\t\t\tglog.Warningf(\"Status is up-to-date; skipping: %q %+v\", uid, status)\n\t\t\treturn"
  },
  {
    "id" : "75aafc07-4845-45b0-acdd-1586362c8f25",
    "prId" : 16223,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e54fb388-6419-4551-a200-5d42b7c129f3",
        "parentId" : null,
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Note to reviewers: This is a slight change in behavior, as we weren't caching the termination status before. I think this is the more correct way, although it does not enforce the same invariants as `SetPodStatus` (maintain `StartTime` and `LastTransitionTime`) - but I don't think that is necessary here.\n",
        "createdAt" : "2015-10-24T01:28:40Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "c595325f-767e-409b-8cc5-fe4619fc3bee",
        "parentId" : "e54fb388-6419-4551-a200-5d42b7c129f3",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "That should be fine - my primary concern originally was not introducing more change than necessary in status, and fixing that is definitely desirable.\n",
        "createdAt" : "2015-10-24T21:41:15Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a2089adc8b60e9a33ca59cdc931e21d956f1ff8",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +198,202 @@\t\t\t}\n\t\t}\n\t\tnewStatus := m.updateStatusInternal(pod, pod.Status)\n\t\tif newStatus != nil {\n\t\t\tselect {"
  },
  {
    "id" : "f542f650-9d63-456b-b078-0395a18c18b6",
    "prId" : 16223,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bce02fc-1e1d-4eaf-a5fe-4080513a92e4",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Do we still need to hold the lock while sending the request to the channel? I think we no longer have to do this, similar to what SetPodStatus does now. \n",
        "createdAt" : "2015-10-28T15:43:40Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "539423ad-a012-4864-b7c0-5a63fd0e9925",
        "parentId" : "2bce02fc-1e1d-4eaf-a5fe-4080513a92e4",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "We can do what SetPodStatus does, although now that we have the periodic sync, I actually wonder which approach is better. Perhaps it would be better to drop the update in the full channel case in both places and let the periodic sync pick up the update? Perhaps I'm over optimizing here.\n",
        "createdAt" : "2015-10-28T17:28:52Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "4b2a23dd-4725-4002-9324-9908e1662ca2",
        "parentId" : "2bce02fc-1e1d-4eaf-a5fe-4080513a92e4",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I like the idea of letting periodic sync dealing with this, but I am afraid that it may increase the latency -- user deletes a terminated pod has to wait the extra 0~10s for the pod to actually be deleted.\n",
        "createdAt" : "2015-10-28T17:55:38Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "45091db5-2d14-4548-b734-b023c9948308",
        "parentId" : "2bce02fc-1e1d-4eaf-a5fe-4080513a92e4",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Only in the case where the channel is full - I'm not proposing removing the channel altogether, just falling back to the periodic sync when the channel is full. \n",
        "createdAt" : "2015-10-28T18:04:55Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "49b69102-5d9e-482d-926a-810e1ab23fb2",
        "parentId" : "2bce02fc-1e1d-4eaf-a5fe-4080513a92e4",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Ah I see. Sounds good. The \"channel is full case\" currently relies on our periodic housekeeping in kubelet, but would be good to simply fall back to the periodic sync. \n",
        "createdAt" : "2015-10-28T18:09:38Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a2089adc8b60e9a33ca59cdc931e21d956f1ff8",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +201,205 @@\t\tif newStatus != nil {\n\t\t\tselect {\n\t\t\tcase m.podStatusChannel <- podStatusSyncRequest{pod.UID, *newStatus}:\n\t\t\tdefault:\n\t\t\t\tsent = false"
  },
  {
    "id" : "aab79065-e6f2-4007-8ead-02a5341da949",
    "prId" : 16223,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce1ae928-cc23-4e16-b416-48bb09fb7cc7",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Is this method thread-safe? Given that it's accessing the apiStatusVersions map, I'd assume it's not thread-safe. I guess only one goroutine can call this function, so we don't care?\n",
        "createdAt" : "2015-10-28T15:50:19Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "42ab1b0c-7397-4e31-82df-5f2553345a9a",
        "parentId" : "ce1ae928-cc23-4e16-b416-48bb09fb7cc7",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "It's not thread safe, but doesn't need to be. I added a couple of comments to that effect.\n",
        "createdAt" : "2015-10-28T17:32:31Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "444ef4d1-6aae-4049-81eb-87b4e0c447a4",
        "parentId" : "ce1ae928-cc23-4e16-b416-48bb09fb7cc7",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "SG. just wanted to confirm :)\n",
        "createdAt" : "2015-10-28T17:57:38Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a2089adc8b60e9a33ca59cdc931e21d956f1ff8",
    "line" : 313,
    "diffHunk" : "@@ -1,1 +326,330 @@}\n\n// needsUpdate returns whether the status is stale for the given pod UID.\n// This method is not thread safe, and most only be accessed by the sync thread.\nfunc (m *manager) needsUpdate(uid types.UID, status versionedPodStatus) bool {"
  },
  {
    "id" : "c797c10b-6a00-41b0-8219-b00a1aa81a20",
    "prId" : 16223,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fcba437-8f0e-4027-bf08-2043000b4a96",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "In line 266, the caller holds the podStatusesLock...\n",
        "createdAt" : "2015-10-28T15:51:05Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "07c3d6ae-ea43-4d41-bd0c-367301940d2b",
        "parentId" : "6fcba437-8f0e-4027-bf08-2043000b4a96",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Oo... that's embarrassing, thanks for catching this! I went back and forth on the design here, and apparently left it in a half-way state. Fixed caller & fixed deadlock test to actually catch this.\n",
        "createdAt" : "2015-10-28T18:13:52Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a2089adc8b60e9a33ca59cdc931e21d956f1ff8",
    "line" : null,
    "diffHunk" : "@@ -1,1 +279,283 @@}\n\n// syncPod syncs the given status with the API server. The caller must not hold the lock.\nfunc (m *manager) syncPod(uid types.UID, status versionedPodStatus) {\n\tif !m.needsUpdate(uid, status) {"
  },
  {
    "id" : "de2efe43-f5f5-4385-a25c-3589e93b17ef",
    "prId" : 16223,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "201e5154-39fd-4fc6-81cf-8e5f190933fa",
        "parentId" : null,
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I think we need to delete the status and cached apiserver version if pod is not found here. Otherwise, we'd have leaked memory if someone else (e.g. a garbage collector) deletes the pod from etcd.\n",
        "createdAt" : "2015-10-28T15:59:50Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "c2ca7a3e-d29b-46cc-bc03-5133f5519b4a",
        "parentId" : "201e5154-39fd-4fc6-81cf-8e5f190933fa",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "I'm a little confused by the positioning of this comment - where is \"here\"? Do you mean above where we handle the NotFound error? And maybe also where we handle the recreated case? We have cleanup loops for both orphaned statuses and apiStatusVersions, but I agree that we should try to minimize the need for cleaning up there.\n",
        "createdAt" : "2015-10-28T18:01:40Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "b8847fec-3429-4618-8271-0eaa8d217006",
        "parentId" : "201e5154-39fd-4fc6-81cf-8e5f190933fa",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Yes, I meant handling the NotFound error here, but you're right about the cleanup loops. There should be no need to clean up immediately. In that case, perhaps we should simply remove the `DeletePodStatus`method completely. The only use case is line 307, which is already covered by the `RemoveOrphanedStatuses`. WDYT?\n",
        "createdAt" : "2015-10-28T18:16:09Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "a80156cc-5cf8-44cc-b282-977e58aad7b2",
        "parentId" : "201e5154-39fd-4fc6-81cf-8e5f190933fa",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "My inclination is to cleanup ASAP and only use the periodic cleanup loops as backup, because it doesn't keep stale data lying around and is more clear that it is actually getting cleaned up.\n",
        "createdAt" : "2015-10-28T18:23:12Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "80dc1f22-8496-4adb-a5ca-286369b6ab5b",
        "parentId" : "201e5154-39fd-4fc6-81cf-8e5f190933fa",
        "authorId" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "body" : "Discussed offline. Use cleanup loop for apiStatusVersion cleanup since it's internal only. Delete cached podStatus ASAP since other parts of the system (e.g. probe workers) rely on it.\n",
        "createdAt" : "2015-10-28T18:41:06Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "0adf587c-aaa2-4e47-be0f-a26d4fde14ac",
        "tags" : [
        ]
      },
      {
        "id" : "90a9bacb-596b-4fd1-a295-fad67d94de5a",
        "parentId" : "201e5154-39fd-4fc6-81cf-8e5f190933fa",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "> Delete cached podStatus ASAP since other parts of the system (e.g. probe workers) rely on it.\n\nJust wanted to clarify: not deleting right away should not cause any problem, so this is simply a slight performance optimization. In some cases it wouldn't help, e.g., if the watch is late to receive the pod removal, pod worker may still try to create containers and posts statuses.\n",
        "createdAt" : "2015-10-28T20:26:12Z",
        "updatedAt" : "2015-10-29T00:41:12Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a2089adc8b60e9a33ca59cdc931e21d956f1ff8",
    "line" : 273,
    "diffHunk" : "@@ -1,1 +302,306 @@\t\tpod.Status = status.status\n\t\t// TODO: handle conflict as a retry, make that easier too.\n\t\tpod, err = m.kubeClient.Pods(pod.Namespace).UpdateStatus(pod)\n\t\tif err == nil {\n\t\t\tglog.V(3).Infof(\"Status for pod %q updated successfully\", kubeletutil.FormatPodName(pod))"
  }
]