[
  {
    "id" : "1ddf8b1d-5cc8-4457-8fae-c6d9466ead46",
    "prId" : 80831,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80831#pullrequestreview-424559641",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f9b349f-b028-4724-960e-779ffefd7abe",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I need to catch-up on latest state of this code, but we traditionally do not support hot-swap of other resources like cpu or memory for a same named node without a restart.  i thought i had seen a similar pr with an update in this space last year, and so I need to refresh my own memory if we handled hugepages separately.",
        "createdAt" : "2020-04-21T05:09:48Z",
        "updatedAt" : "2020-04-21T05:15:02Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "160d227b-7560-449f-8d67-cdd87c4dee13",
        "parentId" : "9f9b349f-b028-4724-960e-779ffefd7abe",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "well, devices would get updated as a counted resource.",
        "createdAt" : "2020-04-21T05:10:31Z",
        "updatedAt" : "2020-04-21T05:15:03Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "be710257-da7e-4802-8f85-efaf946cdf5e",
        "parentId" : "9f9b349f-b028-4724-960e-779ffefd7abe",
        "authorId" : "b7c04289-77b0-4d05-90b7-3bde23e2b0bf",
        "body" : "Well, currently kubelet will report the new kubelet will report updated node resources once in a while (without being restarted), so if you pre-alloc more huge page memory, it will be updated on the fly. This fix only ensures that huge page sizes no longer supported on the node are removed. Today that dosen't happen, and the resource will just stay.\r\n\r\nAlso, this code is only executed on startup of kubelet, so it will run on both a kubelet restart and a node reboot.",
        "createdAt" : "2020-04-23T09:30:30Z",
        "updatedAt" : "2020-04-23T11:26:09Z",
        "lastEditedBy" : "b7c04289-77b0-4d05-90b7-3bde23e2b0bf",
        "tags" : [
        ]
      },
      {
        "id" : "4fb7875d-e545-485c-be1f-6af6562b983f",
        "parentId" : "9f9b349f-b028-4724-960e-779ffefd7abe",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "@odinuge ok, this makes sense.",
        "createdAt" : "2020-06-04T15:19:35Z",
        "updatedAt" : "2020-06-04T15:24:06Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "a233b9aab0d2ea595a08000d1c94564af8cd8e94",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +119,123 @@\trequiresUpdate = kl.updateDefaultLabels(node, existingNode) || requiresUpdate\n\trequiresUpdate = kl.reconcileExtendedResource(node, existingNode) || requiresUpdate\n\trequiresUpdate = kl.reconcileHugePageResource(node, existingNode) || requiresUpdate\n\tif requiresUpdate {\n\t\tif _, _, err := nodeutil.PatchNodeStatus(kl.kubeClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, existingNode); err != nil {"
  },
  {
    "id" : "b2e7a147-0799-4a01-b10d-1e2c9923952b",
    "prId" : 69753,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69753#pullrequestreview-172633447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "also check if labels changed",
        "createdAt" : "2018-11-06T06:55:06Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "19145467-b138-46cd-8978-2ee0a984c212",
        "parentId" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "authorId" : "4186ed58-9575-4126-b730-073268bc67cb",
        "body" : "`tryUpdateNodeStatus` does not touch label. I don't think we need to check labels here. Also confirmed with @yujuhong offline.",
        "createdAt" : "2018-11-06T19:17:57Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "4186ed58-9575-4126-b730-073268bc67cb",
        "tags" : [
        ]
      },
      {
        "id" : "33d02235-0bbb-4827-8e3f-3c626765b57d",
        "parentId" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "@verult where was the CSI label-updating going to hook in? I thought it folded into the node status update loop",
        "createdAt" : "2018-11-06T20:13:29Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "1f1cb9df-2617-4243-b2cb-ce9ff0967cc1",
        "parentId" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "authorId" : "4186ed58-9575-4126-b730-073268bc67cb",
        "body" : "As far as I can tell, label updates are separate from status updates. There are 2 functions touching labels, `initialNode()` and `updateDefaultLabels()`. And both results are sent to master in `tryRegisterWithAPIServer()` [here](https://github.com/kubernetes/kubernetes/blob/8dcdec0a6700b04beb9f73b66f5b125297c9acc7/pkg/kubelet/kubelet_node_status.go#L121).",
        "createdAt" : "2018-11-06T21:46:53Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "4186ed58-9575-4126-b730-073268bc67cb",
        "tags" : [
        ]
      },
      {
        "id" : "780264e6-dcc6-46f9-98d1-035ba90cde43",
        "parentId" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "authorId" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "body" : "@liggitt are you referring to the max volume attach limit, which is stored in `node.Status.Capacity` and `node.Status.Allocatable`? Does that logic need to be moved into this loop instead?\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/95014b26661862554778f00323f49f39d52e5ee6/pkg/volume/csi/nodeinfomanager/nodeinfomanager.go#L173",
        "createdAt" : "2018-11-06T23:25:28Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "34f61776-88da-4b26-aa20-3c4f92530d05",
        "tags" : [
        ]
      },
      {
        "id" : "7454c4f4-7b9b-412b-ac86-0ebe0f191b63",
        "parentId" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "authorId" : "4186ed58-9575-4126-b730-073268bc67cb",
        "body" : "Thanks for your reply, @verult! I see what it means now. IMO, it is fine to leave the CSI label-updating as is because:\r\n1. It is using `PatchNodeStatus` to update status/labels/annotations already, which is separate from kubelet's node status update.\r\n2. It is already checking if update is needed. That means, it only posts node status to master when there is some change, which is consistent with what we want to achieve with node lease feature.\r\n\r\nWDYT, @liggitt?",
        "createdAt" : "2018-11-06T23:41:34Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "4186ed58-9575-4126-b730-073268bc67cb",
        "tags" : [
        ]
      },
      {
        "id" : "dcbc47c4-9b76-4a2f-bf74-5dc7a96fb043",
        "parentId" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Kubelet itself updates the node labels in the registration function, which is separate from the node status update loop.\r\nhttps://github.com/kubernetes/kubernetes/blob/ed06cbe3e32370da07e9035f89a28521372c9392/pkg/kubelet/kubelet_node_status.go#L117-L125\r\n\r\nAs it stands, kubelet is not using this loop to update any node labels (unless I missed something). I don't think it's necessary to check the labels unless someone intends to use it.",
        "createdAt" : "2018-11-07T00:34:39Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "69658ae9-6f49-41ae-9e6d-39b7e38971ca",
        "parentId" : "b203ff27-ad5c-47a1-a618-b221bc8b67b0",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "> I don't think it's necessary to check the labels unless someone intends to use it.\r\n\r\n+1",
        "createdAt" : "2018-11-07T18:27:36Z",
        "updatedAt" : "2018-11-07T19:59:54Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "98fc4a107a8e6da008669b2792044a154f4a75a7",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +423,427 @@\tnow := kl.clock.Now()\n\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) && now.Before(kl.lastStatusReportTime.Add(kl.nodeStatusReportFrequency)) {\n\t\tif !podCIDRChanged && !nodeStatusHasChanged(&originalNode.Status, &node.Status) {\n\t\t\treturn nil\n\t\t}"
  },
  {
    "id" : "7933b5b2-885e-48b5-aa28-f58cd2ae486e",
    "prId" : 69753,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69753#pullrequestreview-197147827",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b565ea83-ba3e-4531-a73c-407d9c9f30d1",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "how does this work if additional conditions got added or removed in the node ?",
        "createdAt" : "2019-01-28T08:37:50Z",
        "updatedAt" : "2019-01-28T08:37:51Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "3512ba89-1b73-49ef-ac8e-717b79014c2c",
        "parentId" : "b565ea83-ba3e-4531-a73c-407d9c9f30d1",
        "authorId" : "4186ed58-9575-4126-b730-073268bc67cb",
        "body" : "It should just work, right?",
        "createdAt" : "2019-01-28T16:57:50Z",
        "updatedAt" : "2019-01-28T16:59:10Z",
        "lastEditedBy" : "4186ed58-9575-4126-b730-073268bc67cb",
        "tags" : [
        ]
      }
    ],
    "commit" : "98fc4a107a8e6da008669b2792044a154f4a75a7",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +615,619 @@\t\toriginalConditionsCopy[i].LastHeartbeatTime = replacedheartbeatTime\n\t\tconditionsCopy[i].LastHeartbeatTime = replacedheartbeatTime\n\t\tif !apiequality.Semantic.DeepEqual(&originalConditionsCopy[i], &conditionsCopy[i]) {\n\t\t\treturn true\n\t\t}"
  },
  {
    "id" : "0f928f67-1116-448b-a814-1e117e6517d3",
    "prId" : 64784,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64784#pullrequestreview-126163631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "495f0852-2132-43bc-9d73-7941904626be",
        "parentId" : null,
        "authorId" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "body" : "For sanity's sake, should this also set that resource's allocatable value to zero? Otherwise this could temporarily set capacity such that `allocatable > capacity`. As is, allocatable would be overwritten to zero on the next kubelet sync iteration.",
        "createdAt" : "2018-06-05T20:55:46Z",
        "updatedAt" : "2018-06-05T22:37:39Z",
        "lastEditedBy" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "tags" : [
        ]
      },
      {
        "id" : "536ab8c5-2669-4fe8-9b33-1d60cf595f17",
        "parentId" : "495f0852-2132-43bc-9d73-7941904626be",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "Done.",
        "createdAt" : "2018-06-05T21:39:17Z",
        "updatedAt" : "2018-06-05T22:37:39Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      }
    ],
    "commit" : "35efc4f96a0fafe50307b3eee13611358f2878a2",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +144,148 @@\tfor k := range node.Status.Capacity {\n\t\tif v1helper.IsExtendedResourceName(k) {\n\t\t\tnode.Status.Capacity[k] = *resource.NewQuantity(int64(0), resource.DecimalSI)\n\t\t\tnode.Status.Allocatable[k] = *resource.NewQuantity(int64(0), resource.DecimalSI)\n\t\t\trequiresUpdate = true"
  },
  {
    "id" : "202fa796-d79e-4493-9c21-0110e7603ed4",
    "prId" : 63955,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63955#pullrequestreview-139345153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "parentId" : null,
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "If the taint exists with a different effect, this will duplicate it. Granted, that's probably a bug anyway, but I think it's safer to just check for the key here (overwriting as necessary). Although actually, does the NodeRestriction controller allow the taint effect to be changed here?\r\n@liggitt ",
        "createdAt" : "2018-05-25T18:50:12Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "dc28b3cf-64a3-447e-984b-8c05092864d8",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "initialNode() is only used to create the node in initial registration. it does not update nodes.",
        "createdAt" : "2018-05-25T19:41:02Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "0d0c6102-b4d9-4135-ac18-d42673ce4472",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "> If the taint exists with a different effect, this will duplicate it. \r\n\r\nYes, that up to user's setting; it's an valid case that user also add the effect, we only need to handle `NoSchedule` by kubelet.\r\n\r\n> Although actually, does the NodeRestriction controller allow the taint effect to be changed here?\r\n\r\nAs liggitt@ said, `NodeRestriction controller` only reject node taints updates; this's accepted as creation :)",
        "createdAt" : "2018-05-28T01:17:39Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "684acd26-99f2-4b61-8fb6-db7575ced1d3",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "If that's the case, then why check `TaintExists` at all here?",
        "createdAt" : "2018-06-26T00:40:58Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "d5813e27-7c5b-47c3-8d2e-fc884c1ac564",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "They could've specified it explicitly in the --register-with-taints flag",
        "createdAt" : "2018-06-26T00:43:57Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "24ec512f-1de9-4b63-b740-fef1d08a1658",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "Sorry, I'm confused... then couldn't this duplicate the taint specified from --register-with-taints?",
        "createdAt" : "2018-07-09T21:45:28Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "e020d63e-489e-45ed-9da0-fb0f43cb8231",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "If it's duplicated, it'll not add it; that's why we check `TaintExists` :). And we only check against `unschedulableTaint`, which is handled by `NodeLifeCycleController`; the other customized taint will follow the default Taint/Toleration behaviour :)",
        "createdAt" : "2018-07-10T01:06:53Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "63521f56-df56-44cb-ab49-bf092cf2133a",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "To clarify:\r\n\r\n1. Kubelet is started with `--register-with-taints=node.kubernetes.io/unschedulable=true:PreferNoSchedule`\r\n2. This code checks for a taint with `node.kubernetes.io/unschedulable:NoSchedule`\r\n3. No match is found, and the kubelet ends up with the duplicate taint `node.kubernetes.io/unschedulable:PreferNoSchedule` and `node.kubernetes.io/unschedulable:NoSchedule`\r\n",
        "createdAt" : "2018-07-17T23:19:17Z",
        "updatedAt" : "2018-07-23T04:52:22Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "77ad8c87-0e2e-458f-9e1a-f6e9617d31ff",
        "parentId" : "4c0b6279-8cc7-40eb-828d-3ec4d6efc995",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "@tallclair , `node.kubernetes.io/unschedulable:PreferNoSchedule` is used for priorities, and `node.kubernetes.io/unschedulable:NoSchedule` is used for predicates; so I think they're not duplicated; and `node.kubernetes.io/unschedulable:NoSchedule` will be removed when `.spec.unschedulable` is updated to `false`.",
        "createdAt" : "2018-07-23T06:31:10Z",
        "updatedAt" : "2018-07-23T06:31:24Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "aac9f1cbaad3741688a8e3a221d420c63017d3d4",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +243,247 @@\tif utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition) {\n\t\tif node.Spec.Unschedulable &&\n\t\t\t!taintutil.TaintExists(nodeTaints, &unschedulableTaint) {\n\t\t\tnodeTaints = append(nodeTaints, unschedulableTaint)\n\t\t}"
  },
  {
    "id" : "540a6b2e-7dbd-42ec-a08d-9f37de42c603",
    "prId" : 63492,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63492#pullrequestreview-118106312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6631b59-1387-4710-9def-d219e968edc3",
        "parentId" : null,
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "Do we want to set onRepeatedHeartbeatFailure to nil or something? (once we invoke the method)",
        "createdAt" : "2018-05-07T18:34:09Z",
        "updatedAt" : "2018-05-07T19:07:12Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      },
      {
        "id" : "4369a650-751a-4387-b4a6-740722034904",
        "parentId" : "b6631b59-1387-4710-9def-d219e968edc3",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "No, that would mean the kubelet would hit the same issue if the network condition was encountered twice during a single process lifetime",
        "createdAt" : "2018-05-07T18:38:26Z",
        "updatedAt" : "2018-05-07T19:07:12Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "340c0617-e603-4759-b20b-b16338a2ffa8",
        "parentId" : "b6631b59-1387-4710-9def-d219e968edc3",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "Ack thanks!",
        "createdAt" : "2018-05-07T18:48:00Z",
        "updatedAt" : "2018-05-07T19:07:12Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "814b065928f86bb27f5e8ad973e4c0cfe8343d4a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +352,356 @@\t\tif err := kl.tryUpdateNodeStatus(i); err != nil {\n\t\t\tif i > 0 && kl.onRepeatedHeartbeatFailure != nil {\n\t\t\t\tkl.onRepeatedHeartbeatFailure()\n\t\t\t}\n\t\t\tglog.Errorf(\"Error updating node status, will retry: %v\", err)"
  },
  {
    "id" : "d069a3b1-4aa5-485a-bc28-a2d7a1b969d5",
    "prId" : 62543,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/62543#pullrequestreview-113684461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54314943-3aea-4a3f-ba37-5c3c7364659d",
        "parentId" : null,
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "Might put this at V(2) and change to \"timeout after %v trying to get instance information from cloud provider\"",
        "createdAt" : "2018-04-19T16:14:28Z",
        "updatedAt" : "2018-04-23T11:30:49Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "3818ca23-c3db-4903-9a2c-b0a64b95719c",
        "parentId" : "54314943-3aea-4a3f-ba37-5c3c7364659d",
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "```go\r\nglog.V(2).Infof(\"timeout after %v trying to get instance information from cloud provider\", kl.cloudproviderRequestTimeout)\r\nreturn nil\r\n```\r\n?",
        "createdAt" : "2018-04-19T16:24:14Z",
        "updatedAt" : "2018-04-23T11:30:49Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      },
      {
        "id" : "1af97847-4281-4457-a08c-962859441611",
        "parentId" : "54314943-3aea-4a3f-ba37-5c3c7364659d",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "looks good",
        "createdAt" : "2018-04-19T16:31:57Z",
        "updatedAt" : "2018-04-23T11:30:49Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "60b1b9e2-2b31-4033-9910-ce9f30d62100",
        "parentId" : "54314943-3aea-4a3f-ba37-5c3c7364659d",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "actually nevermind this whole thing.  i didn't see the block after this and that we were just setting err here, not returning it.",
        "createdAt" : "2018-04-19T16:34:28Z",
        "updatedAt" : "2018-04-23T11:30:49Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      }
    ],
    "commit" : "61efc29394ab42ac2b6771bf6e3e76cee9904001",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +460,464 @@\t\tcase <-kl.cloudproviderRequestSync:\n\t\tcase <-time.After(kl.cloudproviderRequestTimeout):\n\t\t\terr = fmt.Errorf(\"Timeout after %v\", kl.cloudproviderRequestTimeout)\n\t\t}\n"
  },
  {
    "id" : "da789672-1168-43e8-a1ef-ccc71d959d46",
    "prId" : 62242,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/62242#pullrequestreview-110277744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6fdb78f-4e3f-4541-b605-485a7c5496ec",
        "parentId" : null,
        "authorId" : "3c1422a0-6358-4857-8f56-961979171514",
        "body" : "Shall we move this check to method `updatePodCIDR`?",
        "createdAt" : "2018-04-08T06:02:45Z",
        "updatedAt" : "2018-04-27T03:15:29Z",
        "lastEditedBy" : "3c1422a0-6358-4857-8f56-961979171514",
        "tags" : [
        ]
      },
      {
        "id" : "9b61c6ad-7ecb-4469-aac5-73dca3592b32",
        "parentId" : "f6fdb78f-4e3f-4541-b605-485a7c5496ec",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "Both should be ok, but keep it outside is more clear",
        "createdAt" : "2018-04-08T07:33:09Z",
        "updatedAt" : "2018-04-27T03:15:29Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "91c6cfed2fe32af72291735b1ac4582a1232c24d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +383,387 @@\n\tif node.Spec.PodCIDR != \"\" {\n\t\tkl.updatePodCIDR(node.Spec.PodCIDR)\n\t}\n"
  },
  {
    "id" : "ea2b461f-34b4-4338-beca-bfe5789230e5",
    "prId" : 61183,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61183#pullrequestreview-103886485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f903007d-9e63-474a-ab67-dfa95c7dd85b",
        "parentId" : null,
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "The message should be better. The purpose is to report a given node status function is run without actually putting the info log into all of them.",
        "createdAt" : "2018-03-14T16:06:19Z",
        "updatedAt" : "2018-03-14T16:06:19Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d820d5a66a99f5251a1f9806102c5a19ee510ee",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1052,1056 @@func (kl *Kubelet) setNodeStatus(node *v1.Node) {\n\tfor i, f := range kl.setNodeStatusFuncs {\n\t\tglog.V(5).Infof(\"Setting node status at position %v\", i)\n\t\tif err := f(node); err != nil {\n\t\t\tglog.Warningf(\"Failed to set some node status fields: %s\", err)"
  },
  {
    "id" : "11cd8031-bcff-46af-8b86-79ce7cafd945",
    "prId" : 60332,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60332#pullrequestreview-99767420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24e72c55-25db-4089-993c-e6ed1e5a5498",
        "parentId" : null,
        "authorId" : "ed12fa4b-7790-4e88-b7d8-5d6a06b0b9f3",
        "body" : "@jiayingz, please take a look.",
        "createdAt" : "2018-02-27T00:24:49Z",
        "updatedAt" : "2018-02-28T01:25:59Z",
        "lastEditedBy" : "ed12fa4b-7790-4e88-b7d8-5d6a06b0b9f3",
        "tags" : [
        ]
      },
      {
        "id" : "ec8f2054-c08a-41eb-915a-281fa81c7c4d",
        "parentId" : "24e72c55-25db-4089-993c-e6ed1e5a5498",
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "should allocatable also be set to 0 ?",
        "createdAt" : "2018-02-27T06:51:52Z",
        "updatedAt" : "2018-02-28T01:25:59Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "af1bf27d-07b2-49de-9792-5d42119a8555",
        "parentId" : "24e72c55-25db-4089-993c-e6ed1e5a5498",
        "authorId" : "ed12fa4b-7790-4e88-b7d8-5d6a06b0b9f3",
        "body" : "Allocatable will be set to capacity at line 644.",
        "createdAt" : "2018-02-27T16:57:19Z",
        "updatedAt" : "2018-02-28T01:25:59Z",
        "lastEditedBy" : "ed12fa4b-7790-4e88-b7d8-5d6a06b0b9f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d880506fee441cdc742d61753a7a3c9215079a2",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +607,611 @@\n\t\tfor _, removedResource := range removedDevicePlugins {\n\t\t\tglog.V(2).Infof(\"Set capacity for %s to 0 on device removal\", removedResource)\n\t\t\t// Set the capacity of the removed resource to 0 instead of\n\t\t\t// removing the resource from the node status. This is to indicate"
  },
  {
    "id" : "25a08d1e-5782-4aca-bb89-c0368350b5b4",
    "prId" : 60159,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60159#pullrequestreview-100652802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a682649d-b712-44bc-a795-b92eaae86af8",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "I don't think `recordNodeScheduableEvent` should be one of `NodeStatusFuncs`.\r\nI'm fine with adding a lock here for now. But actually we should move `recordNodeScheduableEvent` out of `NodeStatusFuncs` and put it where it should be.\r\n\r\n@dashpole WDYT?",
        "createdAt" : "2018-03-02T01:22:47Z",
        "updatedAt" : "2018-03-02T23:10:20Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2e744c62050a6314e2b788bafe7f749e8ecd588",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1023,1027 @@\n// record if node schedulable change.\nfunc (kl *Kubelet) recordNodeSchedulableEvent(node *v1.Node) {\n\toldNodeUnschedulableLock.Lock()\n\tdefer oldNodeUnschedulableLock.Unlock()"
  },
  {
    "id" : "7f073527-d0c9-4ca8-adb6-95249fd85507",
    "prId" : 50103,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50103#pullrequestreview-54248443",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8bc8661-67f0-47bf-97c6-606de87fe8a2",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Why are we recording events on every node status update as well? Event delivery is not guaranteed, and this seems like an unnecessary duplication of information and lots of extra API/etcd writes\r\n\r\ncc @derekwaynecarr ",
        "createdAt" : "2017-08-03T23:09:31Z",
        "updatedAt" : "2017-08-03T23:09:31Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "9acda411-bacd-4091-94ef-bfbbe42e6d92",
        "parentId" : "a8bc8661-67f0-47bf-97c6-606de87fe8a2",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "It is not on every node status update, only those that change the node status.  This PR only reverts behavior back to what it was before #48846 in order to fix tests.",
        "createdAt" : "2017-08-03T23:12:37Z",
        "updatedAt" : "2017-08-03T23:12:37Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "6df8521b-eb45-4e0f-b507-a1ea679f7f71",
        "parentId" : "a8bc8661-67f0-47bf-97c6-606de87fe8a2",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "hmm, ok",
        "createdAt" : "2017-08-03T23:21:34Z",
        "updatedAt" : "2017-08-03T23:21:34Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "177d64213ce339a41a548b19211d96d104c6cdc4",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +874,878 @@\t\tnodeOODCondition.Reason = \"KubeletHasSufficientDisk\"\n\t\tnodeOODCondition.Message = \"kubelet has sufficient disk space available\"\n\t\tnodeOODCondition.LastTransitionTime = currentTime\n\t\tkl.recordNodeStatusEvent(v1.EventTypeNormal, \"NodeHasSufficientDisk\")\n\t}"
  },
  {
    "id" : "cb82af1f-14e0-4212-90c5-8a4c11d26b33",
    "prId" : 49202,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49202#pullrequestreview-60295764",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6b71f0d-f5c0-4096-a07c-dbd9a69c07ea",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this is preexisting, but while we're here, nothing would prevent a cloud provider from returning the IP as a type Hostname as well... if we double-set the hostname type, the update will fail.",
        "createdAt" : "2017-09-01T16:23:39Z",
        "updatedAt" : "2017-09-01T16:23:39Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "25a9d87b-7c76-4507-b3a3-e33a7a75f408",
        "parentId" : "f6b71f0d-f5c0-4096-a07c-dbd9a69c07ea",
        "authorId" : "a98c6d2e-ed95-4cf2-a946-1f35098ca647",
        "body" : "Right, I didn't want to change this part, as this is how it used to work previously.\r\nI can update the PR to only add a `NodeHostName` address if none is already defined or I can create a new PR specifically for this change, once this one is approved and applied.\r\n@liggitt What do you prefer ?",
        "createdAt" : "2017-09-02T23:16:18Z",
        "updatedAt" : "2017-09-02T23:16:18Z",
        "lastEditedBy" : "a98c6d2e-ed95-4cf2-a946-1f35098ca647",
        "tags" : [
        ]
      },
      {
        "id" : "7736302a-e801-491a-b9ce-a1e1506af648",
        "parentId" : "f6b71f0d-f5c0-4096-a07c-dbd9a69c07ea",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Separate PR is fine",
        "createdAt" : "2017-09-03T13:21:20Z",
        "updatedAt" : "2017-09-03T13:21:20Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b2a5c65007dfec38e31c3b64ad1c1a53412f593",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +477,481 @@\t\t\t}\n\t\t\tif len(enforcedNodeAddresses) > 0 {\n\t\t\t\tenforcedNodeAddresses = append(enforcedNodeAddresses, v1.NodeAddress{Type: v1.NodeHostName, Address: kl.GetHostname()})\n\t\t\t\tnode.Status.Addresses = enforcedNodeAddresses\n\t\t\t\treturn nil"
  },
  {
    "id" : "d34cfe11-045e-4d52-994d-cfba8f9c89df",
    "prId" : 45551,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45551#pullrequestreview-66237965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02cff38e-74e5-40d1-b373-42e1579c7e96",
        "parentId" : null,
        "authorId" : "59098d78-f4f5-4a00-90c5-b35bc66e6750",
        "body" : "I'm thinking about how we could unit test the logic below. We have unit tests for validateNodeIP, but not for the selection logic.\r\n\r\nOne suggestion may be to extract L521-531 into a function, and then a unit test can be run on it, checking cases like only V6, v6 before v4, v4 before v6, only v4.",
        "createdAt" : "2017-09-29T15:41:53Z",
        "updatedAt" : "2017-11-10T23:15:09Z",
        "lastEditedBy" : "59098d78-f4f5-4a00-90c5-b35bc66e6750",
        "tags" : [
        ]
      },
      {
        "id" : "ebd04443-b042-448f-8d28-c5aff7c1ef38",
        "parentId" : "02cff38e-74e5-40d1-b373-42e1579c7e96",
        "authorId" : "b6f0a138-d8c9-4579-8e5a-45064a09c78c",
        "body" : "My unit test follows the same pattern as the TestNodeStatusWithCloudProviderNodeIP test in kubelet_node_status_test.go.\r\n\r\nI agree that work can be done to not rely on system interface IP's and mock. However, I believe that is outside of the changes implemented in this PR and a follow-on Issue/PR should be used to address this concern.",
        "createdAt" : "2017-09-29T18:46:47Z",
        "updatedAt" : "2017-11-10T23:15:09Z",
        "lastEditedBy" : "b6f0a138-d8c9-4579-8e5a-45064a09c78c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ac6fe9c5dae02a543e9b84444cc74fef8501444",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +513,517 @@\t\t} else {\n\t\t\tvar addrs []net.IP\n\t\t\taddrs, _ = net.LookupIP(node.Name)\n\t\t\tfor _, addr := range addrs {\n\t\t\t\tif err = validateNodeIP(addr); err == nil {"
  },
  {
    "id" : "1a3e12de-315c-418f-8456-e7a04d71a6c2",
    "prId" : 44258,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/44258#pullrequestreview-32948169",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c3b152b-764f-4a79-88fa-afcf12acc26b",
        "parentId" : null,
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "is this better than doing `[]v1.Taint{}`?",
        "createdAt" : "2017-04-16T19:32:06Z",
        "updatedAt" : "2017-05-05T23:52:58Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "8666eaac539312131735933752254151522e8db2",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +203,207 @@\t\t},\n\t}\n\tnodeTaints := make([]v1.Taint, 0)\n\tif len(kl.kubeletConfiguration.RegisterWithTaints) > 0 {\n\t\ttaints := make([]v1.Taint, len(kl.kubeletConfiguration.RegisterWithTaints))"
  },
  {
    "id" : "bdcef982-7dca-4ac6-b0a0-29aaed262258",
    "prId" : 33123,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33123#pullrequestreview-1585324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5aab47e-e4c5-4136-9ffb-3338ec5568fe",
        "parentId" : null,
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "what if no nodeIP is passed from NodeAddress but I still want to include it?\n",
        "createdAt" : "2016-09-26T04:45:37Z",
        "updatedAt" : "2016-09-30T21:07:22Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "4e6981c3-b61c-4113-8064-61c0130c8a7f",
        "parentId" : "b5aab47e-e4c5-4136-9ffb-3338ec5568fe",
        "authorId" : "89888ad4-0dac-48c1-b890-5f4449a2de2b",
        "body" : "Would you just add this node-ip to the list of addresses even if its not part of the list?\nThe reason i am searching through the list of returned IPs is to make sure that the node-ip can be routable to the node.\n",
        "createdAt" : "2016-09-26T17:43:01Z",
        "updatedAt" : "2016-09-30T21:07:22Z",
        "lastEditedBy" : "89888ad4-0dac-48c1-b890-5f4449a2de2b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9123de9b473df7b5105708e791dad4ba3fa9992",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +384,388 @@\t\tif kl.nodeIP != nil {\n\t\t\tfor _, nodeAddress := range nodeAddresses {\n\t\t\t\tif nodeAddress.Address == kl.nodeIP.String() {\n\t\t\t\t\tnode.Status.Addresses = []api.NodeAddress{\n\t\t\t\t\t\t{Type: nodeAddress.Type, Address: nodeAddress.Address},"
  },
  {
    "id" : "3b42b792-a6ad-45e6-9bfd-7cbdbde333ca",
    "prId" : 33123,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33123#pullrequestreview-1585383",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d4e0e08-210f-49bc-87cc-8a6ec1e42c4d",
        "parentId" : null,
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "You have to give it a type?\n",
        "createdAt" : "2016-09-26T04:46:02Z",
        "updatedAt" : "2016-09-30T21:07:22Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "8c029843-cd1a-4b5d-aa3f-21dd26f01277",
        "parentId" : "2d4e0e08-210f-49bc-87cc-8a6ec1e42c4d",
        "authorId" : "89888ad4-0dac-48c1-b890-5f4449a2de2b",
        "body" : "I am providing a type in `Type: nodeAddress.Type,`\n",
        "createdAt" : "2016-09-26T17:43:20Z",
        "updatedAt" : "2016-09-30T21:07:22Z",
        "lastEditedBy" : "89888ad4-0dac-48c1-b890-5f4449a2de2b",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9123de9b473df7b5105708e791dad4ba3fa9992",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +386,390 @@\t\t\t\tif nodeAddress.Address == kl.nodeIP.String() {\n\t\t\t\t\tnode.Status.Addresses = []api.NodeAddress{\n\t\t\t\t\t\t{Type: nodeAddress.Type, Address: nodeAddress.Address},\n\t\t\t\t\t}\n\t\t\t\t\treturn nil"
  },
  {
    "id" : "63d01478-984e-44d3-a55b-2504feb69ffb",
    "prId" : 32914,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32914#pullrequestreview-590850",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6dec36ac-3ed1-4692-8ad9-d6d61499c683",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Why Digests and Tags? Are they both really legit names?\n",
        "createdAt" : "2016-09-16T23:32:31Z",
        "updatedAt" : "2016-09-16T23:32:31Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "d96aab64-9872-4217-a6d0-498ee3790e9b",
        "parentId" : "6dec36ac-3ed1-4692-8ad9-d6d61499c683",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "There is no actual \"name\" defined in docker images.. Digest is a content-addressable identifier which users can use to pull an image (which we also support in k8s), so it should be a legit \"name\" in our definition. I don't think there should be multiple digests per image (since it's sha256 hash based on the content), so I changed the order of the digests and tags to ensure that we always include the digest. \n",
        "createdAt" : "2016-09-19T16:55:40Z",
        "updatedAt" : "2016-09-19T16:55:41Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ada99181cdcf6bcbde72ddffe7d975c00444090",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +511,515 @@\n\t\tfor _, image := range containerImages {\n\t\t\tnames := append(image.RepoDigests, image.RepoTags...)\n\t\t\t// Report up to maxNamesPerImageInNodeStatus names per image.\n\t\t\tif len(names) > maxNamesPerImageInNodeStatus {"
  },
  {
    "id" : "877ef364-0592-4567-a38c-cc5f26ca0a4a",
    "prId" : 31730,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8faf0fb7-742b-479c-86f0-f74ca7402b0c",
        "parentId" : null,
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "@smarterclayton @deads2k do we have to worry about accidentally dropping new fields in an apiserver/kubelet version skew?\n",
        "createdAt" : "2016-08-30T20:01:46Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      },
      {
        "id" : "fd870da1-f4cb-4aed-aa93-3efdbf19c94b",
        "parentId" : "8faf0fb7-742b-479c-86f0-f74ca7402b0c",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "@ncdc I think that would be equally applicable to any resource Kubelet can update, wouldn't it?\n",
        "createdAt" : "2016-08-30T20:39:02Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "1bf399ff-fc22-4cb3-a375-b99807e74d74",
        "parentId" : "8faf0fb7-742b-479c-86f0-f74ca7402b0c",
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "Yeah, nevermind\n",
        "createdAt" : "2016-08-31T10:17:28Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1805d30b679108840c686990b612b656e5c9a01a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +108,112 @@\t\trequiresUpdate := kl.reconcileCMADAnnotationWithExistingNode(node, existingNode)\n\t\tif requiresUpdate {\n\t\t\tif _, err := kl.kubeClient.Core().Nodes().Update(existingNode); err != nil {\n\t\t\t\tglog.Errorf(\"Unable to reconcile node %q with API server: error updating node: %v\", kl.nodeName, err)\n\t\t\t\treturn false"
  },
  {
    "id" : "3a2e7439-7195-4123-9a17-86026f0d40c2",
    "prId" : 31730,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fa1add9-a2dc-4f37-8477-4f0446e7ebc0",
        "parentId" : null,
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "This doesn't seem like we have good separation of concerns here. What would you think about a separate method that's called in the startup flow that reconciles the annotation?\n",
        "createdAt" : "2016-08-30T20:44:38Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      },
      {
        "id" : "d763005d-c130-4ab1-ba74-052b5a32cada",
        "parentId" : "6fa1add9-a2dc-4f37-8477-4f0446e7ebc0",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I would prefer to hide all of this inside of `registerWithApiServer`, possibly renaming to `registerOrReconcileNodeWithApiServer`.\n",
        "createdAt" : "2016-08-30T21:06:37Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "598bd673-6eaf-427c-8385-923b38ed5a85",
        "parentId" : "6fa1add9-a2dc-4f37-8477-4f0446e7ebc0",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I'm not really sure that it's worth refactoring now -- this is a fix for a p1 and it's needed downstream as well.  Do you have an objection, besides the factoring, @ncdc?\n",
        "createdAt" : "2016-08-31T00:44:18Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "f700efd9-5eb4-40b6-93db-b17574c014b9",
        "parentId" : "6fa1add9-a2dc-4f37-8477-4f0446e7ebc0",
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "No, it's fine to proceed\n",
        "createdAt" : "2016-08-31T10:17:51Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1805d30b679108840c686990b612b656e5c9a01a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +74,78 @@// tryRegisterWithApiServer makes an attempt to register the given node with\n// the API server, returning a boolean indicating whether the attempt was\n// successful.  If a node with the same name already exists, it reconciles the\n// value of the annotation for controller-managed attach-detach of attachable\n// persistent volumes for the node.  If a node of the same name exists but has"
  },
  {
    "id" : "b86bfdf5-ccd2-4af1-a4a2-f9988acc198b",
    "prId" : 31730,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8601c239-98eb-452d-ba45-b27af4380ca8",
        "parentId" : null,
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "We're supposed to use `utilruntime.HandleError` instead of `glog.Error`, but looking at `pkg/kubelet`, I see that the latter happens much more than the former.\n",
        "createdAt" : "2016-08-31T10:32:08Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      },
      {
        "id" : "51284e0e-6af1-4dc9-8651-1785bbc65518",
        "parentId" : "8601c239-98eb-452d-ba45-b27af4380ca8",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "I'll use it here and make do a follow up after 1.5 opens to switch over throughout the kubelet\n",
        "createdAt" : "2016-08-31T14:47:36Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      },
      {
        "id" : "c021b285-cdec-4147-9253-f322fb19f780",
        "parentId" : "8601c239-98eb-452d-ba45-b27af4380ca8",
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "SGTM\n",
        "createdAt" : "2016-08-31T15:29:30Z",
        "updatedAt" : "2016-08-31T21:04:58Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1805d30b679108840c686990b612b656e5c9a01a",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +86,90 @@\n\tif !apierrors.IsAlreadyExists(err) {\n\t\tglog.Errorf(\"Unable to register node %q with API server: %v\", kl.nodeName, err)\n\t\treturn false\n\t}"
  },
  {
    "id" : "08aa0e22-1b1e-4f41-8965-bd43387a2711",
    "prId" : 31652,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/31652#pullrequestreview-4770701",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8af5902f-9062-49a4-94dd-c1cd4f25a50c",
        "parentId" : null,
        "authorId" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "body" : "umm I think there are edge cases here perhaps on over-provisioning. \n/cc @derekwaynecarr \n",
        "createdAt" : "2016-08-30T21:10:51Z",
        "updatedAt" : "2016-10-28T17:29:30Z",
        "lastEditedBy" : "f81960f6-a033-4403-bebf-c8ebb484e444",
        "tags" : [
        ]
      },
      {
        "id" : "4fe402e4-1244-4564-af10-d8cad579b4a5",
        "parentId" : "8af5902f-9062-49a4-94dd-c1cd4f25a50c",
        "authorId" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "body" : "Can you elaborate? Previously this function unconditionally clobbered `node.Status.Capacity`.\n",
        "createdAt" : "2016-09-01T21:14:54Z",
        "updatedAt" : "2016-10-28T17:29:30Z",
        "lastEditedBy" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "tags" : [
        ]
      },
      {
        "id" : "d9e731b8-2689-4c4f-a485-b71368e1733a",
        "parentId" : "8af5902f-9062-49a4-94dd-c1cd4f25a50c",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "the set call looks fine here, but I think it may be still possible that Kubelet will have node capacity cached elsewhere and will never observe the opaque resources were added.   I am not sure if that could result in failed admission checks.  I will need to vet that part of the code when not on phone\n",
        "createdAt" : "2016-09-23T04:01:17Z",
        "updatedAt" : "2016-10-28T17:29:30Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "c50d93c1-52cb-4832-8729-57cc61ce4132",
        "parentId" : "8af5902f-9062-49a4-94dd-c1cd4f25a50c",
        "authorId" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "body" : "@derekwaynecarr, do you still need to do some additional vetting of the affected Kubelet code? I checked for this during initial implementation, but If it would be helpful I could do an audit/writeup.\n",
        "createdAt" : "2016-10-17T16:53:28Z",
        "updatedAt" : "2016-10-28T17:29:30Z",
        "lastEditedBy" : "8b309230-89cb-4a04-bf89-29a323dad0d8",
        "tags" : [
        ]
      },
      {
        "id" : "7e8dc468-c00a-41fc-9777-ddcf57b246a9",
        "parentId" : "8af5902f-9062-49a4-94dd-c1cd4f25a50c",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "@ConnorDoyle -- apologies for the delay, too much going on.\n\nDuring kubelet admission, we call a registered set of admit handlers to do admission checks that are registered.\n\nsee: https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet.go#L724\n\nthis function ends up using `getNodeAnyway`, which falls through to this function you modified eventually, so I think we are ok for the area that would actually be impacted.\n",
        "createdAt" : "2016-10-18T22:09:46Z",
        "updatedAt" : "2016-10-28T17:29:30Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0421c1ba45d4313d40ce6d55e7b353e134426f8",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +440,444 @@\t//       resources are being advertised.\n\tif node.Status.Capacity == nil {\n\t\tnode.Status.Capacity = api.ResourceList{}\n\t}\n"
  },
  {
    "id" : "b4ad092d-16dd-4159-8c95-9d763f683caf",
    "prId" : 31647,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/31647#pullrequestreview-3192576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f7e8176-e1f6-40bc-a049-df8dc0112338",
        "parentId" : null,
        "authorId" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "body" : "Unless I am missing something, it seems to be overwriting the other annotations a node might have?\n",
        "createdAt" : "2016-10-06T20:00:23Z",
        "updatedAt" : "2016-12-06T18:33:09Z",
        "lastEditedBy" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "tags" : [
        ]
      },
      {
        "id" : "b0055b6e-a164-4e18-8c53-fc0ed44c7000",
        "parentId" : "0f7e8176-e1f6-40bc-a049-df8dc0112338",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "We just created the node above, on line 104. It doesn't have any annotations.\n",
        "createdAt" : "2016-10-06T20:03:43Z",
        "updatedAt" : "2016-12-06T18:33:09Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "6291bd5f-b9f9-432b-8228-dfcda01085cf",
        "parentId" : "0f7e8176-e1f6-40bc-a049-df8dc0112338",
        "authorId" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "body" : "Ahh, thanks.\n",
        "createdAt" : "2016-10-06T20:08:00Z",
        "updatedAt" : "2016-12-06T18:33:09Z",
        "lastEditedBy" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "tags" : [
        ]
      }
    ],
    "commit" : "3352fd161f2ff83fd9e333a9579898b31dce0a00",
    "line" : null,
    "diffHunk" : "@@ -1,1 +194,198 @@\t\t}\n\t\tannotations[v1.TaintsAnnotationKey] = string(b)\n\t\tnode.ObjectMeta.Annotations = annotations\n\n\t}"
  },
  {
    "id" : "b706f639-dd15-4876-9c52-f8289cce9518",
    "prId" : 25532,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/25532#pullrequestreview-5581873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ea58815-5cec-4f56-91ef-1a19897f3521",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this appears to filter the list of addresses from the cloud provider to ones that match the given IP. It seems like `api.NodeAddress{Type: api.NodeHostName, Address: kl.GetHostname()}` should still be included in the resulting list.\n",
        "createdAt" : "2016-10-25T04:44:43Z",
        "updatedAt" : "2016-11-01T00:25:04Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7880e7cd811dd1b5509c2c59863984cd1137c83",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +381,385 @@\t\t\treturn fmt.Errorf(\"failed to get node address from cloud provider: %v\", err)\n\t\t}\n\t\tif kl.nodeIP != nil {\n\t\t\tfor _, nodeAddress := range nodeAddresses {\n\t\t\t\tif nodeAddress.Address == kl.nodeIP.String() {"
  }
]