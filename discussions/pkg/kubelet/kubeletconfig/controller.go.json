[
  {
    "id" : "c2f2cfb0-5be4-4bd3-b7ce-444699da07b9",
    "prId" : 57488,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/57488#pullrequestreview-85213371",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da8561b5-da45-4996-8fc3-63120e5175ca",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Why empty line here?",
        "createdAt" : "2017-12-21T23:18:11Z",
        "updatedAt" : "2017-12-21T23:42:16Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "eb395032-119a-4e31-bf44-b5ef381e3818",
        "parentId" : "da8561b5-da45-4996-8fc3-63120e5175ca",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "I wanted to separate the general kubernetes imports from the imports that are specific to the controller (`pkg/kubelet/kubeletconfig`), since there are so many. Makes it a little easier to read.",
        "createdAt" : "2017-12-21T23:56:01Z",
        "updatedAt" : "2017-12-21T23:56:01Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "6ee191ab743d5f01cc4a135c183d5cfad98f20ff",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +29,33 @@\t\"k8s.io/kubernetes/pkg/kubelet/apis/kubeletconfig\"\n\t\"k8s.io/kubernetes/pkg/kubelet/apis/kubeletconfig/validation\"\n\n\t\"k8s.io/kubernetes/pkg/kubelet/kubeletconfig/checkpoint\"\n\t\"k8s.io/kubernetes/pkg/kubelet/kubeletconfig/checkpoint/store\""
  },
  {
    "id" : "a65da48f-912d-4c31-882a-e4ceb5c326c1",
    "prId" : 46254,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46254#pullrequestreview-54549041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b416c77f-5f64-4d12-8283-eeedbd3ebc90",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Here we try to decide if a given kubeletConfig is a bad one or not, but I think the logic is too simple:\r\n\r\n1) Kubelet might be crash after the config changed; but the cause of the crash might not be bad config. For example, a crash one day after a modification vs. a crash one minute after a modification, the high possibility of one day crash is not caused by a bad config here.\r\n\r\n2) On a node, there is a babysitter / watcher process to make sure Kubelet is running healthy. If not, that process might restart Kubelet. In this case, the restart is not caused by a bad configuration.\r\n\r\n3) Sometimes, there is a kernel panic, or node restarted issued by admin. Those Kubelet restarts due to node restart should be also considered here since they are not caused by a bad configure.\r\n\r\nIn general, we should make this logic more intelligent, and conservative. You don't want to claim a configuration as bad too aggressive; instead, we should assume by default those configuration is good, and use the logic as the last resort to recover Kubelet / node. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "createdAt" : "2017-08-02T23:32:35Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "4ff433be-5968-4623-ae57-e85cc35476b6",
        "parentId" : "b416c77f-5f64-4d12-8283-eeedbd3ebc90",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Ok, I saw you have a trial duration to cover first bullet I listed above.  ",
        "createdAt" : "2017-08-02T23:35:25Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "5649742e-4042-42b6-bf4f-b74caaee322b",
        "parentId" : "b416c77f-5f64-4d12-8283-eeedbd3ebc90",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "For (2), if the Kubelet is restarted due to \"poor health\" within the config trial period, isn't it possible (or even likely) that the \"poor health\" was caused by a bad configuration?\r\n\r\nFor (3), that's a good point, kernel panics need to be taken into account. We should probably consider Kubelet restarts since max(last node boot time, last config change time) instead of just the last config change time.",
        "createdAt" : "2017-08-03T18:28:24Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "dc6c8289-55d1-4f76-90fd-797a312966f4",
        "parentId" : "b416c77f-5f64-4d12-8283-eeedbd3ebc90",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "https://github.com/kubernetes/kubernetes/issues/50216",
        "createdAt" : "2017-08-07T00:54:23Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "378544362cd4fb1b35a940c400ce7c31d9ad1b76",
    "line" : 324,
    "diffHunk" : "@@ -1,1 +322,326 @@\n// crashLooping returns true if the number of startups since the last modification of the current config exceeds `threshold`, false otherwise\nfunc (cc *Controller) crashLooping(threshold int32) (bool, error) {\n\t// determine the last time the current config changed\n\tmodTime, err := cc.checkpointStore.CurrentModified()"
  },
  {
    "id" : "6370815d-f800-472f-8bbf-b97bd5e5d06e",
    "prId" : 46254,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46254#pullrequestreview-54549206",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61a40245-3a27-4c46-8726-34599ffa740f",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Can we have some small test cases to validate the controller for kubeletconfig including Bootstrap logic here? ",
        "createdAt" : "2017-08-03T00:14:06Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "4f380ce7-8fb7-4e2e-a210-f91609f16516",
        "parentId" : "61a40245-3a27-4c46-8726-34599ffa740f",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "If unittests / integration tests are too hard at this moment, can we have a set of node e2e tests to validate the controller? Basically I want to see both Bootstrap and StartSync  for both good and bad scenario. ",
        "createdAt" : "2017-08-03T00:25:38Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "fe0307cf-e7cd-4c46-89e3-14b818a7944e",
        "parentId" : "61a40245-3a27-4c46-8726-34599ffa740f",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "The most important things I want to see through node e2e tests:\r\n1) the kubelet is started from last-known-good-config file\r\n2) kubelet can download the new config properly to the node; and restart itself from the new config file\r\n3) kubelet can mark the config bad properly after several round of crashing caused the bad config\r\n4) Kubelet can roll back the last-known-good-config and restart itself from that good one\r\n5) kubelet can properly reports its configuration status\r\n",
        "createdAt" : "2017-08-03T01:24:10Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "7ce96443-16a1-4844-a6f2-6a06d4d63da6",
        "parentId" : "61a40245-3a27-4c46-8726-34599ffa740f",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "(2) is covered by existing e2e_node tests\r\n\r\n(1, 4, 5) were covered by the [manual test cases](https://docs.google.com/document/d/1L_bw_Dy4QgEOCSAIO_oxM8GxJVkWz84omZFLV-SWuk8/edit#), all of which I plan to turn into automated tests after this PR merges - but I'm trying not to increase the size of this PR any further\r\n\r\n(3) we need to figure out how to test this. I don't want to rely on a \"valid\" config that causes a crash loop - because the very existence of such a config should be viewed as a bug. I also don't want to include a \"please crash loop for our tests\" field in the config, because that's an ugly API. The best solution is not clear here.\r\n",
        "createdAt" : "2017-08-03T21:16:56Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "ced8a451-72a1-45d0-a8fa-d24f64cfb367",
        "parentId" : "61a40245-3a27-4c46-8726-34599ffa740f",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "https://github.com/kubernetes/kubernetes/issues/50217",
        "createdAt" : "2017-08-07T00:58:49Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "378544362cd4fb1b35a940c400ce7c31d9ad1b76",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +126,130 @@// Bootstrap attempts to return a valid KubeletConfiguration based on the configuration of the Controller,\n// or returns an error if no valid configuration could be produced. Bootstrap should be called synchronously before StartSync.\nfunc (cc *Controller) Bootstrap() (*componentconfig.KubeletConfiguration, error) {\n\tutillog.Infof(\"starting controller\")\n"
  },
  {
    "id" : "f0b54083-4eab-40c3-9163-2f2c7f67c5ac",
    "prId" : 46254,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46254#pullrequestreview-54179782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ae10e1e-fcff-4502-88d4-01ee5b539a78",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "One thing is unclear to me in this pr is if the validation of default and init config failed, should kubelet exit with an error? Or keep running if it can but reporting bad status? ",
        "createdAt" : "2017-08-03T01:00:39Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "18806aae-7d59-4e7d-a905-658b236f8500",
        "parentId" : "6ae10e1e-fcff-4502-88d4-01ee5b539a78",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "The Kubelet should exit with an error. It isn't at a point where it can construct an API client yet. In the future we can consider letting it run until it can make post a death status, then exit (not sure I like the extra complexity of this, though). But in any case it should refuse to run if the node was incorrectly provisioned. You must start with a correct foundation.",
        "createdAt" : "2017-08-03T18:17:44Z",
        "updatedAt" : "2017-08-08T19:21:50Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "378544362cd4fb1b35a940c400ce7c31d9ad1b76",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +133,137 @@\tutillog.Infof(\"validating combination of defaults and flags\")\n\tif err := validation.ValidateKubeletConfiguration(cc.defaultConfig); err != nil {\n\t\treturn nil, fmt.Errorf(\"combination of defaults and flags failed validation, error: %v\", err)\n\t}\n\t// only attempt to load and validate the init config if the user provided a path"
  }
]