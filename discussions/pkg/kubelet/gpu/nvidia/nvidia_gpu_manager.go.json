[
  {
    "id" : "7222d13d-db30-4477-9556-9cc42a33ce5e",
    "prId" : 46087,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46087#pullrequestreview-39507431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "087677cd-1c70-4710-b1f7-7b02b30608bc",
        "parentId" : null,
        "authorId" : "87ab1d19-ad39-40d0-a045-817039414280",
        "body" : "I'm -1 on this. If containers recover and continue using GPUs, then allocatable will be wrong for the node.",
        "createdAt" : "2017-05-19T18:51:36Z",
        "updatedAt" : "2017-05-19T18:52:47Z",
        "lastEditedBy" : "87ab1d19-ad39-40d0-a045-817039414280",
        "tags" : [
        ]
      },
      {
        "id" : "03e545d0-89a9-4f19-98f8-fbd7329f6708",
        "parentId" : "087677cd-1c70-4710-b1f7-7b02b30608bc",
        "authorId" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "body" : "but as the PR described, the number of activePods always remains 0 in the Start() function. And as the ngm.allocated has been initialized, it takes no chance to get the right data anymore.\r\nIf we don't erase this line from code, we have to call the Start() function late after the pods have been recovered so the activePods will not be 0 in the gpusInUse() function.",
        "createdAt" : "2017-05-20T02:27:31Z",
        "updatedAt" : "2017-05-20T02:27:31Z",
        "lastEditedBy" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "tags" : [
        ]
      },
      {
        "id" : "44e5dc94-53d9-4170-9a7a-fabf82c8fc97",
        "parentId" : "087677cd-1c70-4710-b1f7-7b02b30608bc",
        "authorId" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "body" : "I tried to figure out the right place to call Start() function, but seems pods' synchronization is done by the statusManager concurrently, which means there is no explicit checkpoint to tell if the pods have been recovered. So, if we can not get the right info of active pods in gpusInUse(), the initialization of ngm.allocated will never be right when kubelet restarts.",
        "createdAt" : "2017-05-22T02:52:13Z",
        "updatedAt" : "2017-05-22T02:52:13Z",
        "lastEditedBy" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "tags" : [
        ]
      },
      {
        "id" : "546adc7c-5df3-42ed-97ba-8dc70580bec6",
        "parentId" : "087677cd-1c70-4710-b1f7-7b02b30608bc",
        "authorId" : "87ab1d19-ad39-40d0-a045-817039414280",
        "body" : "Ok. I suppose things should suss themselves out in the unit & e2e tests. I think this should be helped a bit once checkpoints are added with https://github.com/kubernetes/kubernetes/issues/43240.",
        "createdAt" : "2017-05-22T15:20:41Z",
        "updatedAt" : "2017-05-22T15:20:41Z",
        "lastEditedBy" : "87ab1d19-ad39-40d0-a045-817039414280",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3b9874485718d8c4150b298fdd9bd7ad64e0e51",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +103,107 @@\t\treturn err\n\t}\n\n\t// We ignore errors when identifying allocated GPUs because it is possible that the runtime interfaces may be not be logically up.\n\treturn nil"
  },
  {
    "id" : "465cb6a6-9f7a-439e-81d2-b440f8899ac8",
    "prId" : 46087,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46087#pullrequestreview-40085866",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bcceb29-dd80-4704-b51f-ea307e32b532",
        "parentId" : null,
        "authorId" : "87ab1d19-ad39-40d0-a045-817039414280",
        "body" : "I'm very surprised that this would not happen by default. @yujuhong Is this a subtle bug from dockertools -> dockershim ?",
        "createdAt" : "2017-05-19T18:52:41Z",
        "updatedAt" : "2017-05-19T18:52:47Z",
        "lastEditedBy" : "87ab1d19-ad39-40d0-a045-817039414280",
        "tags" : [
        ]
      },
      {
        "id" : "00c287b9-f441-4add-b260-5560f71c145f",
        "parentId" : "0bcceb29-dd80-4704-b51f-ea307e32b532",
        "authorId" : "b208abb9-a197-491b-8925-71987861491b",
        "body" : "I wonder the same thing.",
        "createdAt" : "2017-05-22T09:48:05Z",
        "updatedAt" : "2017-05-22T09:48:35Z",
        "lastEditedBy" : "b208abb9-a197-491b-8925-71987861491b",
        "tags" : [
        ]
      },
      {
        "id" : "296ea340-a67b-4ab3-9a63-2a27d99e2eb3",
        "parentId" : "0bcceb29-dd80-4704-b51f-ea307e32b532",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "No, the code has always been like that. ",
        "createdAt" : "2017-05-24T16:52:09Z",
        "updatedAt" : "2017-05-24T16:52:09Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3b9874485718d8c4150b298fdd9bd7ad64e0e51",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +240,244 @@\t\tfor _, container := range pod.Status.ContainerStatuses {\n\t\t\tif containers.Has(container.Name) {\n\t\t\t\tcontainersToInspect = append(containersToInspect, containerIdentifier{strings.Replace(container.ContainerID, \"docker://\", \"\", 1), container.Name})\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "3ae5585e-ddc9-40ad-af9e-6c4fc31ebc8e",
    "prId" : 42942,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42942#pullrequestreview-196837654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8595f3c-edce-43b6-a790-998dfb6216e4",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Is it possible that devices is empty ?",
        "createdAt" : "2019-01-27T17:40:17Z",
        "updatedAt" : "2019-01-27T17:54:06Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad743a922aae18baba9a1907d4d1a7b37bf781af",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +146,150 @@\t// Check if GPUs have already been allocated. If so return them right away.\n\t// This can happen if a container restarts for example.\n\tif devices := ngm.allocated.getGPUs(string(pod.UID), container.Name); devices != nil {\n\t\tglog.V(2).Infof(\"Found pre-allocated GPUs for container %q in Pod %q: %v\", container.Name, pod.UID, devices.List())\n\t\treturn append(devices.List(), ngm.defaultDevices...), nil"
  },
  {
    "id" : "e65ae169-f2a2-40c0-bfc3-006d6b3f0855",
    "prId" : 42116,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42116#pullrequestreview-24084776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3eb67d74-1c53-4a1f-a38c-11cd7c2ff1cc",
        "parentId" : null,
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "Would love to have an issue linked here.  This is quite a lot of work to include in a comment.",
        "createdAt" : "2017-02-27T18:21:42Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "84653a5e-7e07-4c4b-918a-539225d47a11",
        "parentId" : "3eb67d74-1c53-4a1f-a38c-11cd7c2ff1cc",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "There isn't one. These are my thoughts. I can move them to an issue later. ",
        "createdAt" : "2017-02-27T20:20:53Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "13582a65aa165fc446a1b89cf90f8a785b33cc2f",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +130,134 @@// A GPU allocated to a container might be re-allocated to a subsequent container because the original container wasn't started quick enough.\n// The current algorithm scans containers only once and then uses a list of active pods to track GPU usage.\n// This is a sub-optimal solution and a better alternative would be that of using pod level cgroups instead.\n// GPUs allocated to containers should be reflected in pod level device cgroups before completing allocations.\n// The pod level cgroups will then serve as a checkpoint of GPUs in use."
  },
  {
    "id" : "e994a618-77e2-4126-8fe7-f5a40ce77c8a",
    "prId" : 42116,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42116#pullrequestreview-24127259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ffbf7d9c-acc5-4f13-93dc-bb808accf7ab",
        "parentId" : null,
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "This also seems like a large enough TODO to warrant a separate issue.",
        "createdAt" : "2017-02-27T18:29:32Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "9d8faf68-b99f-4705-9b0b-0a5aa1d5fd9e",
        "parentId" : "ffbf7d9c-acc5-4f13-93dc-bb808accf7ab",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Honestly, I don't fully grok it. @Hui-Zhi had added it in his original PR. I will open an issue once I find some time.",
        "createdAt" : "2017-02-27T23:10:05Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "b14e944e-ee90-4cf6-8a2c-449bd16543d7",
        "parentId" : "ffbf7d9c-acc5-4f13-93dc-bb808accf7ab",
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "@Hui-Zhi was saying that right now we can only discover devices that are found by the kernel drivers, but we can't tell if the device has issues, e.g. too many ECC errors, a busted BIOS, some kind of interrupt issues or a missing low-level library. Using NVML would catch all those. We can't tell the configuration, either, so we can't schedule jobs on a larger card if a pod needs one.",
        "createdAt" : "2017-02-27T23:53:22Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      }
    ],
    "commit" : "13582a65aa165fc446a1b89cf90f8a785b33cc2f",
    "line" : 195,
    "diffHunk" : "@@ -1,1 +193,197 @@\n// discoverGPUs identifies allGPUs NVIDIA GPU devices available on the local node by walking `/dev` directory.\n// TODO: Without NVML support we only can check whether there has GPU devices, but\n// could not give a health check or get more information like GPU cores, memory, or\n// family name. Need to support NVML in the future. But we do not need NVML until"
  },
  {
    "id" : "b9f0c1bd-50a2-4fa9-ad3b-02028bdeb20c",
    "prId" : 42116,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42116#pullrequestreview-24123740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6c86926-aadb-4faf-99e0-36ff3580cb99",
        "parentId" : null,
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "is this documented anywhere?  Specifying limits without requests would indicate to me that the GPU is 'optional' for that container, as it is limited to a GPU, but does not explicitly request it.",
        "createdAt" : "2017-02-27T18:34:49Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "ccf2ab8d-7474-4d2c-a5d6-4bcf1515e2af",
        "parentId" : "c6c86926-aadb-4faf-99e0-36ff3580cb99",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "There is QoS support for GPUs and so requests do not make sense. This entire experimental feature is undocumented. That something that needs to be resolved soon.",
        "createdAt" : "2017-02-27T23:10:49Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "b4c3c1b5-d7b9-4db7-aaa2-1a998d37a26f",
        "parentId" : "c6c86926-aadb-4faf-99e0-36ff3580cb99",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "ack",
        "createdAt" : "2017-02-27T23:22:12Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "ff46fc8b-0df7-41d8-976e-3a66d297856f",
        "parentId" : "c6c86926-aadb-4faf-99e0-36ff3580cb99",
        "authorId" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "body" : "It's not documented, but it's validated. Unless this PR removed that.",
        "createdAt" : "2017-02-27T23:37:46Z",
        "updatedAt" : "2017-02-28T21:42:20Z",
        "lastEditedBy" : "11725e10-43c9-4a8c-96d0-5118a3e67a6a",
        "tags" : [
        ]
      }
    ],
    "commit" : "13582a65aa165fc446a1b89cf90f8a785b33cc2f",
    "line" : 234,
    "diffHunk" : "@@ -1,1 +232,236 @@\t\tcontainers := sets.NewString()\n\t\tfor _, container := range pod.Spec.Containers {\n\t\t\t// GPUs are expected to be specified only in limits.\n\t\t\tif !container.Resources.Limits.NvidiaGPU().IsZero() {\n\t\t\t\tcontainers.Insert(container.Name)"
  }
]