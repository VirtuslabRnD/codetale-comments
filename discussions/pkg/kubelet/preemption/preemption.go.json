[
  {
    "id" : "31c04236-b99b-4ff7-9acd-8de25e4cbc5d",
    "prId" : 84120,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84120#pullrequestreview-304267542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8ac3857-3620-4b0a-9793-02633834ae35",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Can you share more about why only the first entry in the list is being used as a label?",
        "createdAt" : "2019-10-20T14:38:26Z",
        "updatedAt" : "2019-10-20T14:39:39Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "1a56b105-3606-44df-aaa2-836980f88986",
        "parentId" : "b8ac3857-3620-4b0a-9793-02633834ae35",
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Does it relate to your comment: `A preemption is only recorded for one resource`? Can you share more about that decision?",
        "createdAt" : "2019-10-20T14:39:09Z",
        "updatedAt" : "2019-10-20T14:39:39Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "40409527-28e0-455b-b259-f99dede26beb",
        "parentId" : "b8ac3857-3620-4b0a-9793-02633834ae35",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Mostly due to cardinality - otherwise we would have one potential label per type, and on contended nodes that could be a large number.  If we don’t allow that unbounded cardinality, then we can’t easily get an accurate count by summing across reasons.  So trying to include the signal without potentially going overboard.  I think if we wanted more granular preemption metrics we might design this slightly differently, but it might be best to do that from kube-state-metrics or the scheduler, which can also record more of the reasons why.",
        "createdAt" : "2019-10-20T16:53:34Z",
        "updatedAt" : "2019-10-20T16:53:34Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c44e11cfa9c9bf3116dd5e39310b1dacf42868f",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +114,118 @@\t\t}\n\t\tif len(insufficientResources) > 0 {\n\t\t\tmetrics.Preemptions.WithLabelValues(insufficientResources[0].resourceName.String()).Inc()\n\t\t} else {\n\t\t\tmetrics.Preemptions.WithLabelValues(\"\").Inc()"
  },
  {
    "id" : "5a32a19e-bbce-4f4e-ac1b-23d43f20a7cf",
    "prId" : 78493,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78493#pullrequestreview-243914423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14d61c93-98fd-4b97-a310-70879886470a",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Can you update this comment with what happens if the `kubelet` is never able to kill the pod it is stuck on? Or is there no chance of that happening?",
        "createdAt" : "2019-05-30T12:19:44Z",
        "updatedAt" : "2019-05-30T18:37:06Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "0e47f64f-0d6b-4ded-960e-bb073f1f76d0",
        "parentId" : "14d61c93-98fd-4b97-a310-70879886470a",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "The kubelet will continue to retry the deletion indefinitely.  It is obscure enough of a case where it probably doesn't belong in a comment here...\r\n\r\nFor example, we have encountered this in the past in cases where docker believes a container exists, but containerd doesn't.  In such cases, container deletions always fail, but docker continues to report that the container exists.",
        "createdAt" : "2019-05-30T17:09:37Z",
        "updatedAt" : "2019-05-30T18:37:06Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "132556e13aec79e35ae1500d2cd94921a1185666",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +109,113 @@\t\tif err != nil {\n\t\t\tklog.Warningf(\"preemption: pod %s failed to evict %v\", format.Pod(pod), err)\n\t\t\t// In future syncPod loops, the kubelet will retry the pod deletion steps that it was stuck on.\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "5c540740-164a-407e-ab53-dc947ac94d04",
    "prId" : 77686,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77686#pullrequestreview-236380014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f91b3098-7a05-4c5c-956b-6857a09b4b51",
        "parentId" : null,
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "I believe we can get rid of this `newQuantity > 0` check right? Because we would've already broken out of the loop if this was true?",
        "createdAt" : "2019-05-10T14:38:19Z",
        "updatedAt" : "2019-07-12T16:40:45Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "f11ff284-6130-4217-9dea-f5ba13bb320d",
        "parentId" : "f91b3098-7a05-4c5c-956b-6857a09b4b51",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "If newQuantity > 0 is true, the break on line 210 would not run.\r\nWe still need to check this condition here.",
        "createdAt" : "2019-05-10T16:00:36Z",
        "updatedAt" : "2019-07-12T16:40:45Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "3576eaa3-a756-48c8-82e8-4b9ee7aad2d0",
        "parentId" : "f91b3098-7a05-4c5c-956b-6857a09b4b51",
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Mhhm, yes you are 100% right :)",
        "createdAt" : "2019-05-11T14:44:12Z",
        "updatedAt" : "2019-07-12T16:40:45Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ec1f85b47a548f1fd1e84417d462c2c95d5a6f7",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +209,213 @@\t\t\t}\n\t\t}\n\t\tif newQuantity > 0 {\n\t\t\tnewList = append(newList, &admissionRequirement{\n\t\t\t\tresourceName: req.resourceName,"
  },
  {
    "id" : "ec223e89-92df-4309-b932-22612381730c",
    "prId" : 77686,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77686#pullrequestreview-623020045",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21c6d7d1-f6a1-42df-8f20-45b7b714ed62",
        "parentId" : null,
        "authorId" : "3aa105bc-7ffc-4cd6-8720-4f7e07137e73",
        "body" : "For a certain positive numbers why not remove math.Pow ?\r\n\r\n` dist += remainingRequest/float64(req.quantity) `",
        "createdAt" : "2021-03-29T09:15:02Z",
        "updatedAt" : "2021-03-29T09:15:02Z",
        "lastEditedBy" : "3aa105bc-7ffc-4cd6-8720-4f7e07137e73",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ec1f85b47a548f1fd1e84417d462c2c95d5a6f7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +191,195 @@\t\tremainingRequest := float64(req.quantity - resource.GetResourceRequest(pod, req.resourceName))\n\t\tif remainingRequest > 0 {\n\t\t\tdist += math.Pow(remainingRequest/float64(req.quantity), 2)\n\t\t}\n\t}"
  },
  {
    "id" : "e64b5289-2c54-4ffe-bd92-f109fc70aba5",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23574299",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0ec89b6-33f5-458a-ada0-9689feea3cec",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Is it an error if there exists no `burstable` or `besteffort` pods? IIUC, your logic will fail preemption under such scenarios.",
        "createdAt" : "2017-02-18T02:52:36Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "e8e260dd-0f68-4aad-8844-03855c184d70",
        "parentId" : "d0ec89b6-33f5-458a-ada0-9689feea3cec",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "If there are no burstable or besteffort pods, then the first call \r\n`guarateedToEvict, err := getPodsToPreemptByDistance(guaranteedPods, requirements.subtract(append(bestEffortPods, burstablePods...)...))`\r\nwill find guaranteed pods that fulfill the full set of requirements.  Then, when \"subtracting\" those guaranteed pods from the remaining resources, we will not have any remaining resource requirements, and these functions will return immediately.",
        "createdAt" : "2017-02-18T03:49:40Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "a447ea37-5440-47b4-9b6d-68838ba56a7c",
        "parentId" : "d0ec89b6-33f5-458a-ada0-9689feea3cec",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "Added some comments to clarify this in the latest version",
        "createdAt" : "2017-02-20T19:10:17Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "dc58a7f6-e86a-4558-a00f-7c0f0af6b999",
        "parentId" : "d0ec89b6-33f5-458a-ada0-9689feea3cec",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Hmm. Imagine that there are no Guaranteed pods. \r\nThe first call would be as follows\r\n```go\r\nguarateedToEvict, err := getPodsToPreemptByDistance(guaranteedPods, requirements.subtract(append(bestEffortPods, burstablePods...)...))\r\n```\r\nwhich would return with an error `no set of running pods found to reclaim resources`.  This will result in `getPodsToPreempt()` to return an error too.\r\n\r\nAm I still misreading the code?",
        "createdAt" : "2017-02-22T15:48:24Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "7bf800f0-e301-45b5-8d16-7db333e46f24",
        "parentId" : "d0ec89b6-33f5-458a-ada0-9689feea3cec",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "If there are no guaranteed pods, then one of two things is true:\r\n1. There are burstable pods that could be preempted to reclaim the missing resources, in which case `requirements.subtract(burstablePods)` is an empty list of resources, and returns immediately.\r\n2. There are not enough pods to preempt to reclaim resources, and an error should be returned.",
        "createdAt" : "2017-02-22T16:37:28Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "cd2a454e-c199-48e0-9a1e-2e2980f08353",
        "parentId" : "d0ec89b6-33f5-458a-ada0-9689feea3cec",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "ah ok :)",
        "createdAt" : "2017-02-23T20:08:22Z",
        "updatedAt" : "2017-02-23T20:08:22Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +155,159 @@\tfor len(requirements) > 0 {\n\t\tif len(pods) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"no set of running pods found to reclaim resources: %v\", requirements.toString())\n\t\t}\n\t\t// all distances must be less than len(requirements), because the max distance for a single requirement is 1"
  },
  {
    "id" : "2bba8e47-9c2f-4729-ad28-f12dd96c9938",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23238743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4d7bbd3-4731-40cc-b2a6-6ce9a9622bd5",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "On more thought, I like this algorithm :)",
        "createdAt" : "2017-02-22T15:35:59Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 146,
    "diffHunk" : "@@ -1,1 +144,148 @@}\n\n// finds the pods that have pod requests >= admission requirements.\n// Chooses pods that minimize \"distance\" to the requirements.\n// If more than one pod exists that fulfills the remaining requirements,"
  },
  {
    "id" : "b36ef071-1959-42a2-b524-3d5ed13f5c0c",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23280482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "929d6936-ef6f-42f5-948f-69a69d42440a",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Mention that you take into account `number of pods` resource as well.",
        "createdAt" : "2017-02-22T15:36:20Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "b7ddc620-d454-42da-b41a-8c534536d828",
        "parentId" : "929d6936-ef6f-42f5-948f-69a69d42440a",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "done.",
        "createdAt" : "2017-02-22T17:29:12Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +147,151 @@// Chooses pods that minimize \"distance\" to the requirements.\n// If more than one pod exists that fulfills the remaining requirements,\n// it chooses the pod that has the \"smaller resource request\"\n// This method, by repeatedly choosing the pod that fulfills as much of the requirements as possible,\n// attempts to minimize the number of pods returned."
  },
  {
    "id" : "5fa63f26-49d0-4ba0-9545-5d2a823c3bda",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23268284",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ac1f28d-d4b6-462f-8af6-0365a7a8416b",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Does this algorithm also evict a `besteffort` or a `burstable` pod all the time?\r\nIIUC, pod at index `0` will always get evicted even if it `useless` from a resource reclamation standpoint.",
        "createdAt" : "2017-02-22T16:10:28Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "23e2a7eb-8f92-4d8f-ba82-0fb8e605c6e5",
        "parentId" : "9ac1f28d-d4b6-462f-8af6-0365a7a8416b",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "No?  This wont always evict the pod at index 0.  The purpose of the \"distance\" function is to find the pod that is the most useful in terms of fulfilling requirements.  This portion of the algorithm has no concept of QoS.  Note that this function is called with []pod that are all the same QoS.",
        "createdAt" : "2017-02-22T16:43:01Z",
        "updatedAt" : "2017-02-23T18:31:30Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +169,173 @@\t\t\t}\n\t\t}\n\t\t// subtract the pod from requirements, and transfer the pod from input-pods to pods-to-evicted\n\t\trequirements = requirements.subtract(pods[bestPodIndex])\n\t\tpodsToEvict = append(podsToEvict, pods[bestPodIndex])"
  },
  {
    "id" : "1d9a8336-96f1-4d82-a1b2-a5aa33c6dff3",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23805945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "faa5b32e-c828-4c38-bff5-2d22987ba493",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "typo s/guarateedToEvict/guaranteedToEvict (and assuming this typo repeats elsewhere since the build succeeds)",
        "createdAt" : "2017-02-24T18:34:42Z",
        "updatedAt" : "2017-02-24T18:34:42Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "68a02d45-860e-450a-b6c5-d95e25f409b7",
        "parentId" : "faa5b32e-c828-4c38-bff5-2d22987ba493",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "will update after code freeze.  thanks for the review :)",
        "createdAt" : "2017-02-24T20:32:21Z",
        "updatedAt" : "2017-02-24T20:32:22Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +127,131 @@\t}\n\t// find the guaranteed pods we would need to evict if we already evicted ALL burstable and besteffort pods.\n\tguarateedToEvict, err := getPodsToPreemptByDistance(guaranteedPods, requirements.subtract(append(bestEffortPods, burstablePods...)...))\n\tif err != nil {\n\t\treturn nil, err"
  },
  {
    "id" : "af335ea1-a403-46bc-a195-82d17e784936",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23841421",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20669752-08ff-46e2-a621-6942908b5d77",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Maybe add a comment like \"guaranteedToEvict will be empty if required resources can be reclaimed from besteffort and burstable pods\". And additionally below, \"burstableToEvict will be empty if required resources can be reclaimed from besteffort pods.\"\r\n\r\nAnd if anything shows up in the guaranteedToEvict list, you can probably just prepend all the bestefforts and all the burstables and return immediately, since you know you'll have to evict all of those too.",
        "createdAt" : "2017-02-24T19:04:17Z",
        "updatedAt" : "2017-02-24T19:30:49Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "35657b64-fb4b-439d-a220-321f60c05407",
        "parentId" : "20669752-08ff-46e2-a621-6942908b5d77",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "We dont know we will have to evict all burstable and besteffort.  That is the point of finding the guaranteed we **have to** evict first.  That way, if we have to evict a large, guaranteed pod, we at least avoid evicting a whole bunch of burstable pods as well, and only evict the ones we need to.  \r\n\r\nIll revisit this and add a comment for the first part after code freeze.",
        "createdAt" : "2017-02-24T20:14:25Z",
        "updatedAt" : "2017-02-24T20:14:25Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "da1788ba-d3f6-4831-8276-1819e70ca80d",
        "parentId" : "20669752-08ff-46e2-a621-6942908b5d77",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Ah, that makes sense. I like this.",
        "createdAt" : "2017-02-25T00:34:21Z",
        "updatedAt" : "2017-02-25T00:34:21Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +127,131 @@\t}\n\t// find the guaranteed pods we would need to evict if we already evicted ALL burstable and besteffort pods.\n\tguarateedToEvict, err := getPodsToPreemptByDistance(guaranteedPods, requirements.subtract(append(bestEffortPods, burstablePods...)...))\n\tif err != nil {\n\t\treturn nil, err"
  },
  {
    "id" : "0c7a467a-4e21-439d-95aa-01ed44a6afcd",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23802813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb934bec-6e0c-434d-aa77-c00b9442f580",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Isn't distance actually the fraction of the requirement left *unsatisfied* by the Pod - e.g. a shorter distance means the pod is closer to the requirement?",
        "createdAt" : "2017-02-24T19:12:36Z",
        "updatedAt" : "2017-02-24T19:30:49Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "786161c0-0938-425c-933a-2c81243a24f9",
        "parentId" : "eb934bec-6e0c-434d-aa77-c00b9442f580",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "you are correct, good sir.  Ill update that in a pr after code freeze.",
        "createdAt" : "2017-02-24T20:15:23Z",
        "updatedAt" : "2017-02-24T20:15:23Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 188,
    "diffHunk" : "@@ -1,1 +186,190 @@\n// distance of the pods requests from the admissionRequirements.\n// distance is measured by the fraction of the requirement satisfied by the pod,\n// so that each requirement is weighted equally, regardless of absolute magnitude.\nfunc (a admissionRequirementList) distance(pod *v1.Pod) float64 {"
  },
  {
    "id" : "9df94fce-46c8-4197-a11f-111ca1f118f3",
    "prId" : 40952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40952#pullrequestreview-23866675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b38e708d-f978-4d9a-a3de-4e483d08b9d6",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Why are we squaring the fraction? If your goal is to weight each requirement equally in your resulting distance, shouldn't you divide each remaining request fraction by the number of requirements as you total them?\r\n\r\ne.g. if you have 3 requirements, and 1/2 of the first is left unsatisfied, 1/4 of the second is left unsatisfied, and 1/3 of the third is left unsatisfied, then you have (equally weighted) 1/6+1/12+1/9 left unsatisfied overall.\r\n\r\nOr am I just missing the point here?",
        "createdAt" : "2017-02-24T19:14:57Z",
        "updatedAt" : "2017-02-24T19:30:49Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "435fa01a-a944-4a0a-9ca5-09220d8ad142",
        "parentId" : "b38e708d-f978-4d9a-a3de-4e483d08b9d6",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "Vish asked the same question.  You seem to have two suggestions:\r\n1. Normalize distances, by dividing by the total number of requirements.\r\n2. Dont square distances.\r\n\r\n1 is not necessary, since they are being compared against each other.  Dividing everything by 3, if we have 3 requirements, still gives the same comparison results.\r\n\r\n2 Is up for debate.  The following is/was my reasoning:\r\nSay we have 2 requirements: 1000m cpu, and 1000Mi memory.\r\nSay we could pick one of two pods to evict next (we may evict others not in this group to fulfill remaining requirements afterwards)\r\nThe two options are:\r\n1.  Request = 500m cpu, 500Mi memory\r\n2. Request = 1000m cpu\r\n\r\nMy assumption, which may not be accurate, is that it would be better to evict pod (1), than pod (2).  This is because I believe we are more likely to find a pods that has a small amount of many different resources, than a large amount of a single resource.  It penalizes pods that dont fulfill a requirement at all, and encourages preempting pods that make some progress in a number of areas.\r\n\r\nAnother rationale, is that there may be very few pods that have a certain request.  Say only 2 pods have any cpu requests, and we would need to evict them both as part of preemption.  Since no other pods meet the cpu requirement, the two pods that do would be preferred for preemption.  This would be even more noticeable if critical pods used GPU (which they dont), since usually only a small number of pods would use it.  This prioritizes freeing resources that are \"scarcer\" on a node.",
        "createdAt" : "2017-02-24T20:29:40Z",
        "updatedAt" : "2017-02-24T20:32:58Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "ee1f0022-5e43-4571-9bb2-babb89cdd64c",
        "parentId" : "b38e708d-f978-4d9a-a3de-4e483d08b9d6",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "This is essentially a debate about using  L2 distance vs L1 distance.",
        "createdAt" : "2017-02-24T20:35:14Z",
        "updatedAt" : "2017-02-24T20:35:14Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "854c46e7-5fa7-49d1-9d06-573cd89460ae",
        "parentId" : "b38e708d-f978-4d9a-a3de-4e483d08b9d6",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "(2) is hard to know what's better without real data, and this will vary depending on people's workloads. I've heard some talk of having auto-tuning for resource requests (based on Pod profiling), if we had that then I think preemption behavior would be an interesting to auto-tune too. But I'll say that your intuition feels right to me as well.\r\n\r\n(1) Yeah I guess you're right. Since you add over the number of requirements normalizing ends up just multiplying a constant factor over the inequality when you compare two pods, and you get the same ordering either way.\r\n",
        "createdAt" : "2017-02-25T01:11:58Z",
        "updatedAt" : "2017-02-25T01:11:59Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "154cc302-7d4a-4cf6-9a1c-1d7afa274833",
        "parentId" : "b38e708d-f978-4d9a-a3de-4e483d08b9d6",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I would suggest to not over-think this. Once we have priority and preemption in the scheduler, which should be in the next release, the code in the kubelet should be invoked extremely rarely (if ever). The issues you are discussing here are good, but IMO should be transferred to the design of preemption in the scheduler. I would suggest to just do something simple in kubelet.",
        "createdAt" : "2017-02-25T20:49:45Z",
        "updatedAt" : "2017-02-25T20:49:45Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c58970e47ca3de6cc16a538432bc7f400fe9a84f",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +195,199 @@\t\t\tremainingRequest = 0\n\t\t}\n\t\tdist += math.Pow(remainingRequest/float64(req.quantity), 2)\n\t}\n\treturn dist"
  }
]