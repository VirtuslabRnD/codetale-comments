[
  {
    "id" : "13c6a040-138e-4d64-a582-6aded961b64d",
    "prId" : 97321,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97321#pullrequestreview-579738726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73f412a6-8c3e-4a09-b4b3-3fbb3acd4e5e",
        "parentId" : null,
        "authorId" : "5328b1c0-0dbd-4fd8-869d-e914880959c2",
        "body" : "Not seeing what this is fixing. Existing check looks fine.",
        "createdAt" : "2021-01-29T00:52:03Z",
        "updatedAt" : "2021-02-07T01:23:07Z",
        "lastEditedBy" : "5328b1c0-0dbd-4fd8-869d-e914880959c2",
        "tags" : [
        ]
      },
      {
        "id" : "490d721d-625a-4dbd-a531-55e4961a2f4c",
        "parentId" : "73f412a6-8c3e-4a09-b4b3-3fbb3acd4e5e",
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "https://github.com/kubernetes/kubernetes/blob/eb2b71c727b87f5c9ea4fe4e69dac320972b8799/pkg/kubelet/eviction/helpers.go#L233-L240\r\n\r\nI think the percentage here will be a float <= 1",
        "createdAt" : "2021-01-29T01:21:03Z",
        "updatedAt" : "2021-02-07T01:23:07Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "fa227445-0a1e-4373-8162-b2afee596d9a",
        "parentId" : "73f412a6-8c3e-4a09-b4b3-3fbb3acd4e5e",
        "authorId" : "1db0b401-b0b6-45c2-9a8d-33253f1b5f42",
        "body" : "Exactly, when I develop a webhook about eviction configuration, I used this part of the code directly and found this problem. The upper limit more than 100% should be wrong.",
        "createdAt" : "2021-01-29T01:32:25Z",
        "updatedAt" : "2021-02-07T01:23:07Z",
        "lastEditedBy" : "1db0b401-b0b6-45c2-9a8d-33253f1b5f42",
        "tags" : [
        ]
      },
      {
        "id" : "534cb7f1-370e-40d8-ba32-096d0d52ded3",
        "parentId" : "73f412a6-8c3e-4a09-b4b3-3fbb3acd4e5e",
        "authorId" : "5328b1c0-0dbd-4fd8-869d-e914880959c2",
        "body" : "Thanks! Can we either add a comment or change the variable name to reflect that? ",
        "createdAt" : "2021-01-29T17:20:47Z",
        "updatedAt" : "2021-02-07T01:23:07Z",
        "lastEditedBy" : "5328b1c0-0dbd-4fd8-869d-e914880959c2",
        "tags" : [
        ]
      },
      {
        "id" : "023dd632-9ec8-4873-91ae-0c7eac1e923b",
        "parentId" : "73f412a6-8c3e-4a09-b4b3-3fbb3acd4e5e",
        "authorId" : "1db0b401-b0b6-45c2-9a8d-33253f1b5f42",
        "body" : "for approve @mrunalp ",
        "createdAt" : "2021-01-30T07:52:34Z",
        "updatedAt" : "2021-02-07T01:23:07Z",
        "lastEditedBy" : "1db0b401-b0b6-45c2-9a8d-33253f1b5f42",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa8d07d3e1aa534601a90e527ed8d5321796fc6c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +205,209 @@\t\t}\n\t\t// percentage is a float and should not be greater than 1 (100%)\n\t\tif percentage > 1 {\n\t\t\treturn nil, fmt.Errorf(\"eviction percentage threshold %v must be <= 100%%: %s\", signal, val)\n\t\t}"
  },
  {
    "id" : "af197848-0745-4096-b648-2df6d2969b4e",
    "prId" : 79247,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79247#pullrequestreview-272914923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a89c79a-d59f-46cd-b3fc-46147d7672f8",
        "parentId" : null,
        "authorId" : "1ccd067e-9fba-4e11-a385-9b3c638dbd8a",
        "body" : "@tallclair - my apologies - it wasn't 100% clear to me if anything should be rectified in kubernetes/pkg/api/resource as well (the helpers.go looks the same as what's in v1, but out of date).  Some of the versioning API is still a bit magical to me; can you clarify?",
        "createdAt" : "2019-08-02T22:24:11Z",
        "updatedAt" : "2019-08-20T00:33:59Z",
        "lastEditedBy" : "1ccd067e-9fba-4e11-a385-9b3c638dbd8a",
        "tags" : [
        ]
      },
      {
        "id" : "9f4116dd-81f9-4827-9eff-ffcd2b60fa3d",
        "parentId" : "8a89c79a-d59f-46cd-b3fc-46147d7672f8",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "https://github.com/kubernetes/kubernetes/pull/81104 :smile: ",
        "createdAt" : "2019-08-07T23:54:53Z",
        "updatedAt" : "2019-08-20T00:33:59Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "d574be0e-9a8f-4cb3-abb2-02a453ed8c91",
        "parentId" : "8a89c79a-d59f-46cd-b3fc-46147d7672f8",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "This was very triggering to me. Look what you did: https://github.com/kubernetes/kubernetes/pull/81189 https://github.com/kubernetes/kubernetes/pull/81206",
        "createdAt" : "2019-08-09T01:57:57Z",
        "updatedAt" : "2019-08-20T00:33:59Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      }
    ],
    "commit" : "80ee072b85fb8616b5209cfcafc291ccbf868657",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +27,31 @@\t\"k8s.io/apimachinery/pkg/api/resource\"\n\t\"k8s.io/klog\"\n\tv1resource \"k8s.io/kubernetes/pkg/api/v1/resource\"\n\tstatsapi \"k8s.io/kubernetes/pkg/kubelet/apis/stats/v1alpha1\"\n\tevictionapi \"k8s.io/kubernetes/pkg/kubelet/eviction/api\""
  },
  {
    "id" : "1159d871-510d-466d-a46d-7c6bb1ab0b77",
    "prId" : 54316,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54316#pullrequestreview-73269701",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12981411-a4e6-4733-b5d4-94e69e8d1fae",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Do you plan to eventually add v1.ResourceDisk?",
        "createdAt" : "2017-10-31T17:44:39Z",
        "updatedAt" : "2017-11-21T18:21:44Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "1a12050e-9e84-4270-89df-92d522bd2a5e",
        "parentId" : "12981411-a4e6-4733-b5d4-94e69e8d1fae",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "I intend to move the entire eviction logic to use v1.EphemeralStorage in a seperate PR",
        "createdAt" : "2017-10-31T18:05:13Z",
        "updatedAt" : "2017-11-21T18:21:44Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b3bd5ae6033ffa7ff6b81b534031a6710bb8557",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +602,606 @@\t\tcase v1.ResourceMemory:\n\t\t\tcontainerValue.Add(*pod.Spec.Containers[i].Resources.Requests.Memory())\n\t\tcase resourceDisk:\n\t\t\tcontainerValue.Add(*pod.Spec.Containers[i].Resources.Requests.StorageEphemeral())\n\t\t}"
  },
  {
    "id" : "cf26afd7-1f65-42ec-a2a7-f6cb3117ba4a",
    "prId" : 53542,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53542#pullrequestreview-68758016",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31969458-455c-430a-ac2f-3f5d7b5246f1",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Should we reserve the previous behavior if the feature gate for PodPriority is false? Otherwise, there is a behavior change no matter the feature gate is on or off. ",
        "createdAt" : "2017-10-09T18:27:05Z",
        "updatedAt" : "2017-10-12T20:15:21Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "b1fe0b5a-dc1e-44b8-89e4-9c23481c91c6",
        "parentId" : "31969458-455c-430a-ac2f-3f5d7b5246f1",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "The behavior change is intentional.  Without priority enabled, the behavior is very similar to old behavior, but with a small difference.  The new behavior mimics ranking by QoS since Guaranteed pods always have usage < requests, and best-effort pods always have usage > requests.  The only different scenario is that a burstable pod consuming far above requests will be evicted before a best-effort pod that does not consume much.",
        "createdAt" : "2017-10-09T20:34:38Z",
        "updatedAt" : "2017-10-12T20:15:21Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "beafc1c4-fd21-4c4f-8c05-9bb6a56f2146",
        "parentId" : "31969458-455c-430a-ac2f-3f5d7b5246f1",
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Ok, I am ok with this small behavior changes on this. @derekwaynecarr any concerns?",
        "createdAt" : "2017-10-11T21:28:33Z",
        "updatedAt" : "2017-10-12T20:15:21Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      }
    ],
    "commit" : "539fddb49db1941bf9b9bd87eaa97a627ca83ab0",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +590,594 @@\n// priority compares pods by Priority, if priority is enabled.\nfunc priority(p1, p2 *v1.Pod) int {\n\tif !utilfeature.DefaultFeatureGate.Enabled(features.PodPriority) {\n\t\t// If priority is not enabled, all pods are equal."
  },
  {
    "id" : "066f26d8-0633-47af-8d59-b90b8699d3a3",
    "prId" : 53542,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53542#pullrequestreview-68100832",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8fe7f8b-e03f-482a-8513-c89074158e76",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Update the comment with more context on the ranking decision?",
        "createdAt" : "2017-10-09T18:35:42Z",
        "updatedAt" : "2017-10-12T20:15:21Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "d62c25d1-2905-4e70-93b6-6e64716e9704",
        "parentId" : "d8fe7f8b-e03f-482a-8513-c89074158e76",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "done",
        "createdAt" : "2017-10-09T20:37:28Z",
        "updatedAt" : "2017-10-12T20:15:21Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "539fddb49db1941bf9b9bd87eaa97a627ca83ab0",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +732,736 @@}\n\n// rankMemoryPressure orders the input pods for eviction in response to memory pressure.\n// It ranks by whether or not the pod's usage exceeds its requests, then by priority, and\n// finally by memory usage above requests."
  },
  {
    "id" : "9eb9d513-af19-4385-8f81-03595a4e6f99",
    "prId" : 53542,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53542#pullrequestreview-68100804",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f301017f-ac17-40eb-a97a-c5d4d48b1ea2",
        "parentId" : null,
        "authorId" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "body" : "Ditto, especially the ranking decision is based on a different policy than the memory.",
        "createdAt" : "2017-10-09T18:36:16Z",
        "updatedAt" : "2017-10-12T20:15:21Z",
        "lastEditedBy" : "a6409368-42e0-44a9-bf79-9d3042ac3b65",
        "tags" : [
        ]
      },
      {
        "id" : "cddb5a43-0474-46ba-86a5-041897625073",
        "parentId" : "f301017f-ac17-40eb-a97a-c5d4d48b1ea2",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "same as above.",
        "createdAt" : "2017-10-09T20:37:22Z",
        "updatedAt" : "2017-10-12T20:15:21Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "539fddb49db1941bf9b9bd87eaa97a627ca83ab0",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +739,743 @@}\n\n// rankDiskPressureFunc returns a rankFunc that measures the specified fs stats.\nfunc rankDiskPressureFunc(fsStatsToMeasure []fsStatsType, diskResource v1.ResourceName) rankFunc {\n\treturn func(pods []*v1.Pod, stats statsFunc) {"
  },
  {
    "id" : "b7e4b4bf-a063-410f-bc25-b403ad03818d",
    "prId" : 51490,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51490#pullrequestreview-59444837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98cfe0ee-7864-4cfa-a86a-e162ea561161",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "Since this function is very similar to podDiskUsage, I think you can reduce the code by using a common function to calculate container usage.\r\nOr just have one function podLocalStorageUsage (.... func volumeNames) to pass a function for checking the volumenames?",
        "createdAt" : "2017-08-29T17:15:34Z",
        "updatedAt" : "2017-08-30T05:54:53Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "5f0edb22-1ef0-4c5a-b45f-e8c8ab1be17f",
        "parentId" : "98cfe0ee-7864-4cfa-a86a-e162ea561161",
        "authorId" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "body" : "ok, fixed in accordance with the first option",
        "createdAt" : "2017-08-30T04:55:44Z",
        "updatedAt" : "2017-08-30T05:54:53Z",
        "lastEditedBy" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ca27417d9ee122c77ce6e13ddc482256dfadeeb",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +488,492 @@// podLocalEphemeralStorageUsage  aggregates pod local ephemeral storage usage and inode consumption for the specified stats to measure.\nfunc podLocalEphemeralStorageUsage(podStats statsapi.PodStats, pod *v1.Pod, statsToMeasure []fsStatsType) (v1.ResourceList, error) {\n\tdisk := resource.Quantity{Format: resource.BinarySI}\n\tinodes := resource.Quantity{Format: resource.BinarySI}\n"
  },
  {
    "id" : "c02bd67d-b280-48d8-9034-5b17018b6b1f",
    "prId" : 51490,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51490#pullrequestreview-59451654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ca4a815-7004-40dd-9e2d-d0554567a68e",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "actually this part of code could also use one common function by passing a list of volume names. But I am also ok with this, just the code is repeated in two functions..",
        "createdAt" : "2017-08-30T05:33:14Z",
        "updatedAt" : "2017-08-30T05:54:53Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "5d72679d-e01b-4813-b332-a1d90dd17c10",
        "parentId" : "9ca4a815-7004-40dd-9e2d-d0554567a68e",
        "authorId" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "body" : "Sorry for bothering again. fixed",
        "createdAt" : "2017-08-30T05:55:48Z",
        "updatedAt" : "2017-08-30T05:55:48Z",
        "lastEditedBy" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ca27417d9ee122c77ce6e13ddc482256dfadeeb",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +495,499 @@\tinodes.Add(containerUsageList[resourceInodes])\n\n\tif hasFsStatsType(statsToMeasure, fsStatsLocalVolumeSource) {\n\t\tvolumeNames := localEphemeralVolumeNames(pod)\n\t\tpodLocalVolumeUsageList := podLocalVolumeUsage(volumeNames, podStats)"
  },
  {
    "id" : "ea5f7b79-644a-4215-ae12-b2d61e100c37",
    "prId" : 42204,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42204#pullrequestreview-24634739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99225d4a-9921-4a81-932b-1c59a5d9e578",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "i would like a test case for this use case.",
        "createdAt" : "2017-02-28T22:56:43Z",
        "updatedAt" : "2017-03-02T15:36:53Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "6a0ea111-02be-4294-a866-8fe0feda93ae",
        "parentId" : "99225d4a-9921-4a81-932b-1c59a5d9e578",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "done.",
        "createdAt" : "2017-03-02T01:26:56Z",
        "updatedAt" : "2017-03-02T15:36:53Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac612eab8ec9fb466f135a3501079657e1aa2350",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +99,103 @@func ParseThresholdConfig(allocatableConfig []string, evictionHard, evictionSoft, evictionSoftGracePeriod, evictionMinimumReclaim string) ([]evictionapi.Threshold, error) {\n\tresults := []evictionapi.Threshold{}\n\tallocatableThresholds := getAllocatableThreshold(allocatableConfig)\n\tresults = append(results, allocatableThresholds...)\n"
  },
  {
    "id" : "e29d8701-e593-468c-931a-acaff94ada95",
    "prId" : 42204,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42204#pullrequestreview-24641389",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78d7b76a-55a7-4283-b4ca-441c948f46cc",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "I'd prefer exposing this logic as a containermanager method instead. why should evictions care about allocatable configuration excepting knowing if it needs to evict at allocatable level",
        "createdAt" : "2017-03-02T02:33:36Z",
        "updatedAt" : "2017-03-02T15:36:53Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac612eab8ec9fb466f135a3501079657e1aa2350",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +223,227 @@func getAllocatableThreshold(allocatableConfig []string) []evictionapi.Threshold {\n\tfor _, key := range allocatableConfig {\n\t\tif key == cm.NodeAllocatableEnforcementKey {\n\t\t\treturn []evictionapi.Threshold{\n\t\t\t\t{"
  },
  {
    "id" : "7ceea17d-912b-4a1a-b686-79c453efca07",
    "prId" : 32724,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32724#pullrequestreview-563711",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21fb23f3-581c-4d65-acc2-46cfbe582487",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "can you add a test case for this?\n",
        "createdAt" : "2016-09-19T14:43:16Z",
        "updatedAt" : "2016-10-06T16:39:14Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "98e97a475afc5058bbc0a626efae09f35b930ee6",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +670,674 @@}\n\nfunc thresholdsUpdatedStats(thresholds []Threshold, observations, lastObservations signalObservations) []Threshold {\n\tresults := []Threshold{}\n\tfor i := range thresholds {"
  },
  {
    "id" : "d9004374-7a81-4ac0-b99a-73205600b1cc",
    "prId" : 32724,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32724#pullrequestreview-563711",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbad0152-2232-4a33-8686-9e68633d7948",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "can you open an issue and add a TODO so we can have a time associated with `FsStats` ?\n",
        "createdAt" : "2016-09-19T14:45:09Z",
        "updatedAt" : "2016-10-06T16:39:14Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "98e97a475afc5058bbc0a626efae09f35b930ee6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +601,605 @@\t\t\tavailable: resource.NewQuantity(int64(*memory.AvailableBytes), resource.BinarySI),\n\t\t\tcapacity:  resource.NewQuantity(int64(*memory.AvailableBytes+*memory.WorkingSetBytes), resource.BinarySI),\n\t\t\ttime:      memory.Time,\n\t\t}\n\t}"
  },
  {
    "id" : "e2dd2620-6d56-4e76-8d00-e157916963f3",
    "prId" : 30400,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13d4f223-a604-40f3-965b-ad7e79b2972b",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "+1\n",
        "createdAt" : "2016-08-15T18:12:16Z",
        "updatedAt" : "2016-08-15T18:12:16Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2dd03aeccbe6d2fd4eda40f0faaaeb8cc81c497",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +573,577 @@\t\tresult[SignalMemoryAvailable] = signalObservation{\n\t\t\tavailable: resource.NewQuantity(int64(*memory.AvailableBytes), resource.BinarySI),\n\t\t\tcapacity:  resource.NewQuantity(int64(*memory.AvailableBytes+*memory.WorkingSetBytes), resource.BinarySI),\n\t\t}\n\t}"
  },
  {
    "id" : "d4c053b8-4eb0-42d4-b747-2e34f9773d0a",
    "prId" : 27199,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3353ea6d-f971-4d71-86a0-d638329164a6",
        "parentId" : null,
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "Could you split parseMinimumReclaims into a separate commit? \n",
        "createdAt" : "2016-07-08T18:04:25Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      }
    ],
    "commit" : "d37710f87b7a2497d918d8c5e51323e352dcce98",
    "line" : null,
    "diffHunk" : "@@ -1,1 +206,210 @@\n// parseMinimumReclaims parses the minimum reclaim statements\nfunc parseMinimumReclaims(expr string) (map[Signal]resource.Quantity, error) {\n\tif len(expr) == 0 {\n\t\treturn nil, nil"
  },
  {
    "id" : "bb640cfb-edc9-4fd4-a427-e7a0a6e2af05",
    "prId" : 27199,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1725b2d-0b5f-4aa2-abe9-cc1ff3529071",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "nit: mention that these volumes consume disk.\n\nIdeally, I'd argue that we should let the metrics summary API provide more information about what resources a volume consumes. For example, a tmpfs based empty dir consumes memory, and a regular empty dir consumes local disk and an nfs volume consumes remote storage, etc. Hard coding the list here is fine, but is probably not the cleanest solution. WDYT?\n",
        "createdAt" : "2016-07-19T00:26:27Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "ba9418e0-6afb-4f97-9f20-7f8ffe96e5a1",
        "parentId" : "b1725b2d-0b5f-4aa2-abe9-cc1ff3529071",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "i agree long term.  i will add a // TODO for follow-on.\n\nfor this PR, i will just ensure that empty dir is only included when medium is not memory.\n",
        "createdAt" : "2016-07-20T19:26:28Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "946ac965-1f5c-48e4-8746-63ce10fd77f0",
        "parentId" : "b1725b2d-0b5f-4aa2-abe9-cc1ff3529071",
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "Please add the TODO\n",
        "createdAt" : "2016-07-26T00:57:17Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      },
      {
        "id" : "0ecb94fe-2c90-4c9a-90ea-8e4bc21a9da6",
        "parentId" : "b1725b2d-0b5f-4aa2-abe9-cc1ff3529071",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "done\n",
        "createdAt" : "2016-07-26T02:21:29Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "d37710f87b7a2497d918d8c5e51323e352dcce98",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +255,259 @@}\n\n// localVolumeNames returns the set of volumes for the pod that are local\n// TODO: sumamry API should report what volumes consume local storage rather than hard-code here.\nfunc localVolumeNames(pod *api.Pod) []string {"
  },
  {
    "id" : "996ff505-54db-4ee9-a653-9fddeb4ea53e",
    "prId" : 27199,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2f004c5-b815-43a1-a8f9-a7f4ee164dea",
        "parentId" : null,
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "I think you can break after disk.add in line 287.\n",
        "createdAt" : "2016-07-26T00:57:19Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      },
      {
        "id" : "c1fddb24-4c00-4b9f-abb9-eefb883ae7f2",
        "parentId" : "e2f004c5-b815-43a1-a8f9-a7f4ee164dea",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "good point, fixed.\n",
        "createdAt" : "2016-07-26T02:22:32Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "d37710f87b7a2497d918d8c5e51323e352dcce98",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +286,290 @@\t\t\tfor _, volumeStats := range podStats.VolumeStats {\n\t\t\t\tif volumeStats.Name == volumeName {\n\t\t\t\t\tdisk.Add(*diskUsage(&volumeStats.FsStats))\n\t\t\t\t\tbreak\n\t\t\t\t}"
  },
  {
    "id" : "17bfbd8d-3861-4e4c-8247-2958f80d2be3",
    "prId" : 24750,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dda546b8-d124-4571-abf9-df417f292cc4",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Should we check for duplicates for hard and soft thresholds as well?\n",
        "createdAt" : "2016-04-25T22:39:21Z",
        "updatedAt" : "2016-05-06T16:06:13Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "6fd86d66-426c-47be-89db-20e0ac928cf9",
        "parentId" : "dda546b8-d124-4571-abf9-df417f292cc4",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I don't think so, we could check that if you have:\n\nhard=memory.available<X\nsoft=memory.available<Y\n\nWe could verify that X > Y, but not sure its worth the complication if we expose more operators.\n",
        "createdAt" : "2016-04-29T17:49:34Z",
        "updatedAt" : "2016-05-06T16:06:13Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "10adcc5a-44d7-42f5-a026-82bc0801e397",
        "parentId" : "dda546b8-d124-4571-abf9-df417f292cc4",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "What if the label includes multiple `hard=memory.available<X` sections?\n",
        "createdAt" : "2016-05-04T18:34:17Z",
        "updatedAt" : "2016-05-06T16:06:13Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "3955d61b-09d0-4aa8-b8ff-78a9d6b7671f",
        "parentId" : "dda546b8-d124-4571-abf9-df417f292cc4",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "see https://github.com/kubernetes/kubernetes/pull/24750/files#diff-d1674863b3df010b8b90a347f6bec9b2R68\n\nthis part of the code is handling grace periods, so the check you asked for is not in this part of the code.\n",
        "createdAt" : "2016-05-06T21:44:04Z",
        "updatedAt" : "2016-05-06T21:44:04Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "725af223aa38842373fb1703ffee0a76063da1ba",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +156,160 @@\n\t\t// check against duplicate statements\n\t\tif _, found := results[signal]; found {\n\t\t\treturn nil, fmt.Errorf(\"duplicate eviction grace period specified for %v\", signal)\n\t\t}"
  },
  {
    "id" : "075577fd-9255-4f73-8b75-f59f2a801029",
    "prId" : 21274,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28645ae3-4413-49f8-8c46-bf1b3bdb7172",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "What about usage of volumes?\n",
        "createdAt" : "2016-05-13T21:27:14Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "2335b820-bc53-421e-abbd-21bb4e4df327",
        "parentId" : "28645ae3-4413-49f8-8c46-bf1b3bdb7172",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I will leave a // TODO for volume tracking, we can handle that in a follow-on when we finish up disk.\n",
        "createdAt" : "2016-05-14T15:16:03Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "5058d84b-910f-4950-956b-0b0c1510abd2",
        "parentId" : "28645ae3-4413-49f8-8c46-bf1b3bdb7172",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "SGTM\n",
        "createdAt" : "2016-05-17T00:17:48Z",
        "updatedAt" : "2016-05-17T00:17:48Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "edc76f6d4fd40749aa5fc4545007043086790a75",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +211,215 @@\tmemory := resource.Quantity{Format: resource.BinarySI}\n\tfor _, container := range podStats.Containers {\n\t\t// disk usage (if known)\n\t\t// TODO: need to handle volumes\n\t\tfor _, fsStats := range []*statsapi.FsStats{container.Rootfs, container.Logs} {"
  },
  {
    "id" : "da7638ea-6c82-4993-bfde-8acf1269dde0",
    "prId" : 21274,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "777c0827-31dd-4210-8778-9de385d9cb2a",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Resource storage is pretty generic and weirdly it only applies to volumes as per API comments. Should `local-disk` be a first class resource? For the purposes of this PR, this can be a kubelet internal type too..\n",
        "createdAt" : "2016-05-13T21:40:03Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "374d579a-c649-4ed1-8d42-3d0354bfb63f",
        "parentId" : "777c0827-31dd-4210-8778-9de385d9cb2a",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "Yeah, I was unsure what resource to really use.  Fine with that.\n",
        "createdAt" : "2016-05-13T23:08:07Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "42cc4ece-3be1-4a84-94ca-37d7f1571fa3",
        "parentId" : "777c0827-31dd-4210-8778-9de385d9cb2a",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "added internal type \"disk\".\n",
        "createdAt" : "2016-05-14T15:21:26Z",
        "updatedAt" : "2016-05-14T15:34:53Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "edc76f6d4fd40749aa5fc4545007043086790a75",
    "line" : 253,
    "diffHunk" : "@@ -1,1 +393,397 @@\t\t// disk is best effort, so we don't measure relative to a request.\n\t\t// TODO: add disk as a guaranteed resource\n\t\tp1Disk := p1Usage[api.ResourceStorage]\n\t\tp2Disk := p2Usage[api.ResourceStorage]\n\t\t// if p2 is using more than p1, we want p2 first"
  }
]