[
  {
    "id" : "15da785d-5e2d-4744-bc2e-961fb94c905a",
    "prId" : 99095,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99095#pullrequestreview-633932768",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1e0543f-336a-4b90-b965-33a9dea245db",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "I think this change (`true` -> `false`) was introduced here: https://github.com/kubernetes/kubernetes/pull/59841/\r\n\r\nIt may have been the copy/paste error from this line (to include the call debug method):\r\nhttps://github.com/kubernetes/kubernetes/blob/d7355278b34f1eb83ae537506de3a816ac925226/pkg/kubelet/eviction/eviction_manager.go#L272 or intentional. As part of that PR, the comment about the min reclaim was removed:\r\n\r\n> `// evaluate all current thresholds to see if with adjusted observations, we think we have met min reclaim goals`\r\n\r\nCan you please review the mentioned PR and check if any of bugs fixed in that one may come back. If everything is OK, can you please add the comment about min reclaim back.\r\n\r\n(looking at code it feels that this is the right change. Cancelling lgtm to make sure that due diligence is made on the change source)\r\n\r\n/lgtm cancel\r\n\r\nCC: @dashpole ",
        "createdAt" : "2021-04-12T18:52:45Z",
        "updatedAt" : "2021-04-12T19:33:25Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "0ee3d61b-20a1-4fbe-9188-7a39a9ce8675",
        "parentId" : "e1e0543f-336a-4b90-b965-33a9dea245db",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "This change also looks correct to me.  I can't think of why I would have intentionally changed that.",
        "createdAt" : "2021-04-12T19:08:15Z",
        "updatedAt" : "2021-04-12T19:33:25Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "12dc2aa8-b15a-492f-b767-096183bf0a59",
        "parentId" : "e1e0543f-336a-4b90-b965-33a9dea245db",
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "/lgtm",
        "createdAt" : "2021-04-12T19:21:07Z",
        "updatedAt" : "2021-04-12T19:33:25Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "d83e3262-76cb-4cfc-8318-33419fbacd1b",
        "parentId" : "e1e0543f-336a-4b90-b965-33a9dea245db",
        "authorId" : "8759cd9e-2572-4eee-b566-07300d8ab2d4",
        "body" : "> It may have been the copy/paste error from this line \r\n\r\nThat's a pretty good theory. I couldn't understand why the grace period was mentioned in this context. I adapted the corresponding comment accordingly.\r\n\r\nI read PR [#59841](https://github.com/kubernetes/kubernetes/pull/59841) and its two referenced issues:\r\n* [#46789](https://github.com/kubernetes/kubernetes/issues/46789): Kubelet was not taking into account the amount of disk space freed by container garbage collection when doing the intermediary check, which could lead to unnecessary evictions\r\n* [#56573](https://github.com/kubernetes/kubernetes/issues/56573): Kubelet was not taking into account the amount of inodes freed by garbage collection when doing the intermediary check, which could lead to unnecessary evictions\r\n\r\nBoth are still solved by [#59841](https://github.com/kubernetes/kubernetes/pull/59841) which brings a more reliable way to measure how much resources Kubelet freed/has during this intermediary check, by doing a new observation. I went down the rabbit hole and followed a couple more issues and PRs. I should probably have stopped on the way since you added the lgtm label, and I realize @dashpole was involved in everything I read.\r\n\r\nAs far as I went ([#31362](https://github.com/kubernetes/kubernetes/issues/31362)), I understand the  sequences of changes I read and that led to where the code around this method is today and I really think this boolean value is just a mistake. \r\n\r\n",
        "createdAt" : "2021-04-12T20:21:53Z",
        "updatedAt" : "2021-04-12T20:30:38Z",
        "lastEditedBy" : "8759cd9e-2572-4eee-b566-07300d8ab2d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "63cba062eb8ce3927e2b44f4ba75ed881c30e299",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +432,436 @@\t\t// evaluate all thresholds independently of their grace period to see if with\n\t\t// the new observations, we think we have met min reclaim goals\n\t\tthresholds := thresholdsMet(m.config.Thresholds, observations, true)\n\t\tdebugLogThresholdsWithObservation(\"thresholds after resource reclaim - ignoring grace period\", thresholds, observations)\n"
  },
  {
    "id" : "44953aab-2e86-49e7-84c3-3deb18745ac5",
    "prId" : 99032,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99032#pullrequestreview-594737563",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93fc9de2-c52f-464d-a487-5046ae724527",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "is it expected that on line 168 the `klog.KObj(attrs.Pod)` used and here is `format.Pods` is used?",
        "createdAt" : "2021-02-17T20:38:46Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "2e38aec8-5285-46cf-9a4d-c22822444d45",
        "parentId" : "93fc9de2-c52f-464d-a487-5046ae724527",
        "authorId" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "body" : "I want to use klog.KObj(attrs.Pod), But I think evictedPods(multiple pods) is not work here,so I keep it not change.",
        "createdAt" : "2021-02-18T00:18:06Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "tags" : [
        ]
      },
      {
        "id" : "88ceacf3-364e-48f1-9a5f-862a68fe77fa",
        "parentId" : "93fc9de2-c52f-464d-a487-5046ae724527",
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "@serathius what should we be doing in the case of multiple pods like this? Do you think this is ok?",
        "createdAt" : "2021-02-19T18:36:26Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      },
      {
        "id" : "ab7ad095-c6a4-4c3c-8ace-0b3f729ae939",
        "parentId" : "93fc9de2-c52f-464d-a487-5046ae724527",
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "I think we should migrate it to `klog.KObj` to support Json and make changes in formatting easier. We didn't add specific methods for multiple pods as we weren't sure how often this case will came up. \r\n\r\nThis is the first case using, so for now let's just implement a simple for loop.",
        "createdAt" : "2021-02-20T10:11:52Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      }
    ],
    "commit" : "af0b4c9031bd26aa5ce6b2ef4fc66cae14e183dc",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +197,201 @@\t\tfor {\n\t\t\tif evictedPods := m.synchronize(diskInfoProvider, podFunc); evictedPods != nil {\n\t\t\t\tklog.InfoS(\"Eviction manager: pods evicted, waiting for pod to be cleaned up\", \"pods\", format.Pods(evictedPods))\n\t\t\t\tm.waitForPodsCleanup(podCleanedUpFunc, evictedPods)\n\t\t\t} else {"
  },
  {
    "id" : "6120c3a4-f60e-477e-a7ae-59cee5400035",
    "prId" : 99032,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99032#pullrequestreview-594653793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1cf9bbdc-1a46-4ac5-b584-a0e75ad9483b",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "why warning is replaced with Informational?",
        "createdAt" : "2021-02-17T20:39:49Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "3cfb7e6f-8e48-4f67-b9e3-70e0c5066b43",
        "parentId" : "1cf9bbdc-1a46-4ac5-b584-a0e75ad9483b",
        "authorId" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "body" : "I modified it according to the following rules\r\nhttps://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md",
        "createdAt" : "2021-02-18T00:20:31Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "tags" : [
        ]
      },
      {
        "id" : "54517172-d88a-4bdd-9aea-1cb497a0a0cd",
        "parentId" : "1cf9bbdc-1a46-4ac5-b584-a0e75ad9483b",
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "hm, interesting. thank you!",
        "createdAt" : "2021-02-19T23:43:38Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      }
    ],
    "commit" : "af0b4c9031bd26aa5ce6b2ef4fc66cae14e183dc",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +185,189 @@\t\t\t\tnotifier, err := NewMemoryThresholdNotifier(threshold, m.config.PodCgroupRoot, &CgroupNotifierFactory{}, thresholdHandler)\n\t\t\t\tif err != nil {\n\t\t\t\t\tklog.InfoS(\"Eviction manager: failed to create memory threshold notifier\", \"err\", err)\n\t\t\t\t} else {\n\t\t\t\t\tgo notifier.Start()"
  },
  {
    "id" : "c1a8f26d-61bd-466b-ae34-8a22914f6690",
    "prId" : 99032,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99032#pullrequestreview-594460133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9dd10361-3401-4c6a-ad9b-b61ab4995c18",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "warning replaced by Informational?",
        "createdAt" : "2021-02-17T20:40:40Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "e799a13d-f577-4a42-aa81-0fd51f6d7814",
        "parentId" : "9dd10361-3401-4c6a-ad9b-b61ab4995c18",
        "authorId" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "body" : "I modified it according to the following rules\r\nhttps://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md",
        "createdAt" : "2021-02-18T00:19:49Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "tags" : [
        ]
      },
      {
        "id" : "871aef6e-991e-469d-874e-da7a85f9b44f",
        "parentId" : "9dd10361-3401-4c6a-ad9b-b61ab4995c18",
        "authorId" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "body" : "This is correct---\"warning\" log messages are being deprecated.",
        "createdAt" : "2021-02-19T18:36:53Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "31a2ac00-6c67-4307-a4bc-bfd13f41ef27",
        "tags" : [
        ]
      }
    ],
    "commit" : "af0b4c9031bd26aa5ce6b2ef4fc66cae14e183dc",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +261,265 @@\t\tfor _, notifier := range m.thresholdNotifiers {\n\t\t\tif err := notifier.UpdateThreshold(summary); err != nil {\n\t\t\t\tklog.InfoS(\"Eviction manager: failed to update notifier\", \"notifier\", notifier.Description(), \"err\", err)\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "b9df42d9-0f9f-42cb-aab6-8213c1036bdb",
    "prId" : 99032,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99032#pullrequestreview-592739406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "028f6ed3-31d6-463f-89e2-aeae6b275e15",
        "parentId" : null,
        "authorId" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "body" : "how the `nodeConditions`  would be formatted? Will `%v` be applied by default?",
        "createdAt" : "2021-02-17T20:41:43Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "85d51570-e06e-4b3f-a869-f5f820e49119",
        "tags" : [
        ]
      },
      {
        "id" : "abe1a8b9-bfb7-4c73-a8ee-a6a45a85abd9",
        "parentId" : "028f6ed3-31d6-463f-89e2-aeae6b275e15",
        "authorId" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "body" : "I modified it according to the following rules\r\nhttps://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md",
        "createdAt" : "2021-02-18T00:21:26Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "c2702f95-b5ab-4472-9b9b-a3e60465f10f",
        "tags" : [
        ]
      }
    ],
    "commit" : "af0b4c9031bd26aa5ce6b2ef4fc66cae14e183dc",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +297,301 @@\tnodeConditions = nodeConditionsObservedSince(nodeConditionsLastObservedAt, m.config.PressureTransitionPeriod, now)\n\tif len(nodeConditions) > 0 {\n\t\tklog.V(3).InfoS(\"Eviction manager: node conditions - transition period not met\", \"nodeCondition\", nodeConditions)\n\t}\n"
  },
  {
    "id" : "d6104d23-23c0-4003-b2f9-4ded40f326fc",
    "prId" : 99032,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99032#pullrequestreview-594737788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b494620-5fd5-4319-b8b8-3453d3913d5f",
        "parentId" : null,
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "Same us above",
        "createdAt" : "2021-02-20T10:14:29Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      }
    ],
    "commit" : "af0b4c9031bd26aa5ce6b2ef4fc66cae14e183dc",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +366,370 @@\trank(activePods, statsFunc)\n\n\tklog.InfoS(\"Eviction manager: pods ranked for eviction\", \"pods\", format.Pods(activePods))\n\n\t//record age of metrics for met thresholds that we are using for evictions."
  },
  {
    "id" : "4ece224f-0ab6-4a7e-ba53-60266dd1723c",
    "prId" : 99032,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99032#pullrequestreview-594737810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e856205-eba5-413c-bc68-6ca8c6f9b24b",
        "parentId" : null,
        "authorId" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "body" : "ditto",
        "createdAt" : "2021-02-20T10:14:48Z",
        "updatedAt" : "2021-02-20T16:35:22Z",
        "lastEditedBy" : "6ea93d56-a0ec-4969-ac42-11a78c2085e6",
        "tags" : [
        ]
      }
    ],
    "commit" : "af0b4c9031bd26aa5ce6b2ef4fc66cae14e183dc",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +409,413 @@\t\t\t\t}\n\t\t\t\tif i == len(pods)-1 {\n\t\t\t\t\tklog.InfoS(\"Eviction manager: pods successfully cleaned up\", \"pods\", format.Pods(pods))\n\t\t\t\t\treturn\n\t\t\t\t}"
  },
  {
    "id" : "cf35275e-6a47-4d20-a5e0-a4c1b2c1c4ae",
    "prId" : 75144,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75144#pullrequestreview-213783136",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d9f7acb-8e77-4610-a285-cc2b2454b8f4",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Can you add some unit tests for this?",
        "createdAt" : "2019-03-13T06:35:47Z",
        "updatedAt" : "2019-03-13T06:40:32Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "ea9a7523-35da-4455-8289-819f7a8e57e5",
        "parentId" : "1d9f7acb-8e77-4610-a285-cc2b2454b8f4",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "TestCriticalPodsAreNotEvicted() already covers this. To make it still working, I added a simple `mirrorPodFunc()` implementation and pass to managerImpl.",
        "createdAt" : "2019-03-13T06:55:26Z",
        "updatedAt" : "2019-03-13T06:55:26Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "d67e7fd47fdd9fc5870d71ba083f67ab6195faf6",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +555,559 @@\t\t\t// skip only when it's a static and critical pod\n\t\t\tif kubelettypes.IsCriticalPod(mirrorPod) {\n\t\t\t\tklog.Errorf(\"eviction manager: cannot evict a critical static pod %s\", format.Pod(pod))\n\t\t\t\treturn false\n\t\t\t}"
  },
  {
    "id" : "7f2b6392-1b9b-4ba7-92d1-9786ad360565",
    "prId" : 75144,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75144#pullrequestreview-214854595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93cc71da-76d5-440c-8699-22a7ece05361",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "typo: statid",
        "createdAt" : "2019-03-15T03:32:12Z",
        "updatedAt" : "2019-03-15T03:33:51Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "d67e7fd47fdd9fc5870d71ba083f67ab6195faf6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +58,62 @@\t// the function to invoke to kill a pod\n\tkillPodFunc KillPodFunc\n\t// the function to get the mirror pod by a given statid pod\n\tmirrorPodFunc MirrorPodFunc\n\t// the interface that knows how to do image gc"
  },
  {
    "id" : "d1fecf9a-f2c3-4d24-a126-cf5872a6c173",
    "prId" : 75144,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75144#pullrequestreview-214854595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad5273f5-c668-4ec4-8518-5073b7ed3cfd",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "panic (since we won't hit this) ?",
        "createdAt" : "2019-03-15T03:33:06Z",
        "updatedAt" : "2019-03-15T03:33:51Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "d67e7fd47fdd9fc5870d71ba083f67ab6195faf6",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +559,563 @@\t\t\t}\n\t\t} else {\n\t\t\t// we should never hit this\n\t\t\tklog.Errorf(\"eviction manager: cannot get mirror pod from static pod %s, so cannot evict it\", format.Pod(pod))\n\t\t\treturn false"
  },
  {
    "id" : "38d9898e-5b20-4597-9a58-aacce388a5d9",
    "prId" : 50889,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50889#pullrequestreview-59075250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2074d7e6-0590-493a-baee-5c6385ee567b",
        "parentId" : null,
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "`fsStatsLocalVolumeSource` also contains `hostPath`, are we counting host path usage to Pod?  e.g. if we limit Pod usage to 500M and the pod mount a hostpath which is about 1Gi, are we gonna evict the pod?",
        "createdAt" : "2017-08-28T01:31:13Z",
        "updatedAt" : "2017-08-28T01:46:37Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "13ca2f14-2ad4-4d7f-9836-ebc66f59dfb5",
        "parentId" : "2074d7e6-0590-493a-baee-5c6385ee567b",
        "authorId" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "body" : "oh, my bad. I did not notice that `localVolumeNames` contains other volume sources\r\ncc @jingxu97 ",
        "createdAt" : "2017-08-28T02:22:26Z",
        "updatedAt" : "2017-08-28T02:22:40Z",
        "lastEditedBy" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "tags" : [
        ]
      },
      {
        "id" : "7e098c80-23c3-4198-9cb1-387e4119a7f5",
        "parentId" : "2074d7e6-0590-493a-baee-5c6385ee567b",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "Thinking further, if `hostPath` something outside rootfs, e.g. `/etc`, then do we calculate it at all?",
        "createdAt" : "2017-08-28T03:35:02Z",
        "updatedAt" : "2017-08-28T03:35:03Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "0a007a64-c49f-4715-bbfc-655ccf2c0ca0",
        "parentId" : "2074d7e6-0590-493a-baee-5c6385ee567b",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "Yes, I think hostapath should not be counted since it is considered as persistent storage. But secrets, configumap, gitrepo, downwardapi should be counted because they are basically a wrapper of emptydir. We might need to have a separate podLocalEphemeralStorageUsage function",
        "createdAt" : "2017-08-28T21:31:50Z",
        "updatedAt" : "2017-08-28T21:31:50Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9730e3d3029784e942719b6c3745c2c87cdce48a",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +517,521 @@\tfsStatsSet := []fsStatsType{}\n\tif *m.dedicatedImageFs {\n\t\tfsStatsSet = []fsStatsType{fsStatsLogs, fsStatsLocalVolumeSource}\n\t} else {\n\t\tfsStatsSet = []fsStatsType{fsStatsRoot, fsStatsLogs, fsStatsLocalVolumeSource}"
  },
  {
    "id" : "e9ebfac7-ad64-417f-92ef-c3aa58e376c0",
    "prId" : 50889,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50889#pullrequestreview-59074510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65b41ba6-b02a-4053-a43d-fab08a163ce0",
        "parentId" : null,
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "sorry if this is already discussed somewhere; i have two questions regarding dedicated image fs:\r\n- why do we still count `fsStatsLogs` if we have dedicated imagefs?  IIUC, it is yet to decide how to handle container logs, it's possible that imagefs provides storage for logs?\r\n- are we simply ignoring eviction for imagefs? e.g. if i have imagefs setup and a container keeps writing to writable layer, it can use up all storage in imagefs?",
        "createdAt" : "2017-08-28T01:43:19Z",
        "updatedAt" : "2017-08-28T01:46:37Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "feebacc7-5a4c-44b9-a437-54022374aec6",
        "parentId" : "65b41ba6-b02a-4053-a43d-fab08a163ce0",
        "authorId" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "body" : "We only manage one partition(root) now.  If we do not have imagefs partition, then writable layer is satisfied by root partition, we need to calculate its usage, if we have imagefs partition,  do not manage it now.",
        "createdAt" : "2017-08-28T01:58:11Z",
        "updatedAt" : "2017-08-28T01:58:11Z",
        "lastEditedBy" : "cf088828-7f69-4b6e-8956-1842c94daa02",
        "tags" : [
        ]
      },
      {
        "id" : "a18a71a3-4b14-4d94-8f6c-4459b7587893",
        "parentId" : "65b41ba6-b02a-4053-a43d-fab08a163ce0",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "which means this is true? `if i have imagefs setup and a container keeps writing to writable layer, it can use up all storage in imagefs?` \r\n> We only manage one partition(root) now. If we do not have imagefs partition, then writable layer is satisfied by root partition, we need to calculate its usage, if we have imagefs partition, do not manage it now.",
        "createdAt" : "2017-08-28T03:33:14Z",
        "updatedAt" : "2017-08-28T03:33:14Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "e1c3bff3-2939-4328-9ce2-a1b5f1bc4e00",
        "parentId" : "65b41ba6-b02a-4053-a43d-fab08a163ce0",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "@ddysher yes, that is true..",
        "createdAt" : "2017-08-28T21:28:39Z",
        "updatedAt" : "2017-08-28T21:28:40Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9730e3d3029784e942719b6c3745c2c87cdce48a",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +516,520 @@\tpodEphemeralStorageTotalUsage := &resource.Quantity{}\n\tfsStatsSet := []fsStatsType{}\n\tif *m.dedicatedImageFs {\n\t\tfsStatsSet = []fsStatsType{fsStatsLogs, fsStatsLocalVolumeSource}\n\t} else {"
  },
  {
    "id" : "5a108560-27ee-48cd-a844-3dd65bb743b2",
    "prId" : 45686,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45686#pullrequestreview-41402183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0044ffd5-be1e-47f4-92b7-bf54fedb061d",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "It's odd that the same pod might end up being appended multiple times. Is there any de-duping happening?",
        "createdAt" : "2017-06-01T00:31:04Z",
        "updatedAt" : "2017-06-05T19:07:20Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "fc158829-8b0e-4703-9e79-327d7db121c1",
        "parentId" : "0044ffd5-be1e-47f4-92b7-bf54fedb061d",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "fixed. If emptyDirLimitEviction returns true, will skip the containerOverlayLimitEviction check",
        "createdAt" : "2017-06-01T02:12:36Z",
        "updatedAt" : "2017-06-05T19:07:20Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b13aee0c0f9bd06eb323ea249db29547b66bc46",
    "line" : 104,
    "diffHunk" : "@@ -1,1 +470,474 @@\n\t\tif m.emptyDirLimitEviction(podStats, pod) {\n\t\t\tevicted = append(evicted, pod)\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "a3edcd4b-9232-4538-bac3-d6fb70c2a943",
    "prId" : 43590,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/43590#pullrequestreview-39327469",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13fb73ff-b935-4773-a1bb-8458894d898d",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Won't this result in a housekeeping loop that's longer than monitoring interval? \r\nFor example, if `waitForPodCleanup` took `9s`, then the next `synchronize` should happen within a second, assuming the `monitoringInterval` is `10s`.",
        "createdAt" : "2017-05-16T19:49:33Z",
        "updatedAt" : "2017-05-16T21:23:53Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "d3f0a75a-fd2a-4f74-aaf4-6341c05c6f21",
        "parentId" : "13fb73ff-b935-4773-a1bb-8458894d898d",
        "authorId" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "body" : "We either wait for cleanup or sleep for the monitoring interval, but never do both.  My thought was that if we evicted a pod, we would want to run synchronize again ASAP.  Right now, this wont matter much, as we will likely not be able to take action in the next loop because the stats wont be new.  This structure works really well once we have on-demand stats since we will essentially respond to pressure safely (since we always wait for pod cleanup), and quickly (since we start the next loop as soon after evicting the previous pod as possible).",
        "createdAt" : "2017-05-16T20:08:21Z",
        "updatedAt" : "2017-05-16T21:23:53Z",
        "lastEditedBy" : "d2b16581-e7e9-48b8-9f76-6f6bcb9ec300",
        "tags" : [
        ]
      },
      {
        "id" : "66b816d9-24be-4836-a99e-b69be0906173",
        "parentId" : "13fb73ff-b935-4773-a1bb-8458894d898d",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "This change makes sense to me.",
        "createdAt" : "2017-05-20T03:57:54Z",
        "updatedAt" : "2017-05-20T03:57:54Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "21fb487245aadce85a65c54ac07791110262f08e",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +149,153 @@\t\t\t\tm.waitForPodCleanup(podCleanedUpFunc, evictedPod)\n\t\t\t} else {\n\t\t\t\ttime.Sleep(monitoringInterval)\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "9c55c417-508c-4fdd-8baa-551b74906a26",
    "prId" : 40655,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40655#pullrequestreview-19844233",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4d34a09-5276-4a1a-ba5f-c4a855b6649d",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "@vishh -- so you are ok not providing support for this annotation on best effort pods?",
        "createdAt" : "2017-02-01T15:27:05Z",
        "updatedAt" : "2017-02-02T19:43:29Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "150326d2-9fb9-465b-bb05-6825d96f7422",
        "parentId" : "e4d34a09-5276-4a1a-ba5f-c4a855b6649d",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Yeah. I don't think it makes sense to run \"Critical\" BestEffort pods ",
        "createdAt" : "2017-02-02T17:34:26Z",
        "updatedAt" : "2017-02-02T19:43:29Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "77a88f7e8b99ed97a76821c9a19ccb87c06b73e6",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +113,117 @@\tif hasNodeCondition(m.nodeConditions, v1.NodeMemoryPressure) {\n\t\tnotBestEffort := v1.PodQOSBestEffort != qos.GetPodQOS(attrs.Pod)\n\t\tif notBestEffort {\n\t\t\treturn lifecycle.PodAdmitResult{Admit: true}\n\t\t}"
  },
  {
    "id" : "4c0c285c-6a55-46b1-9e47-0f55fbd1e97b",
    "prId" : 32577,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32577#pullrequestreview-758003",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "932f5987-f322-4ab6-b70e-e9b4c320cddf",
        "parentId" : null,
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "use `var _ Manager = &managerImpl{}` instead the comment?\n",
        "createdAt" : "2016-09-20T10:58:41Z",
        "updatedAt" : "2016-11-07T02:48:44Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      },
      {
        "id" : "3de7b952-4edc-45f4-8902-7e2718857633",
        "parentId" : "932f5987-f322-4ab6-b70e-e9b4c320cddf",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "yes, the type assertion is already below the struct def.  i was just fixing an inaccuracy in the in comment.\n",
        "createdAt" : "2016-09-20T14:20:41Z",
        "updatedAt" : "2016-11-07T02:48:44Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      },
      {
        "id" : "e53fe6c6-eaba-4a72-8ef8-0702fc9edca3",
        "parentId" : "932f5987-f322-4ab6-b70e-e9b4c320cddf",
        "authorId" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "body" : "First line that got hidden, no argue about that :)\n",
        "createdAt" : "2016-09-20T14:24:46Z",
        "updatedAt" : "2016-11-07T02:48:44Z",
        "lastEditedBy" : "c63e1ceb-64bd-4726-b8ef-e647d73dae0c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2583116f1a6e668500c86c9add88905e86e09c3e",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +36,40 @@)\n\n// managerImpl implements Manager\ntype managerImpl struct {\n\t//  used to track time"
  },
  {
    "id" : "cb883b1a-1e97-4c95-8ef4-e7e5388b211b",
    "prId" : 32577,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32577#pullrequestreview-3535352",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e87d29a-3b7b-4f3c-8062-8e272801c82d",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Why not retry again if it fails?\n",
        "createdAt" : "2016-10-07T19:11:39Z",
        "updatedAt" : "2016-11-07T02:48:44Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "cf92e7f8-57ff-4498-807b-8011c3c03261",
        "parentId" : "4e87d29a-3b7b-4f3c-8062-8e272801c82d",
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "Many of the error cases in `startMemoryThresholdNotifier()` are not transient.  If they fail once, they will likely fail every time and we don't want to fill the logs with errors on every `synchronize()`\n",
        "createdAt" : "2016-10-10T16:52:28Z",
        "updatedAt" : "2016-11-07T02:48:44Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      }
    ],
    "commit" : "2583116f1a6e668500c86c9add88905e86e09c3e",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +214,218 @@\t\t})\n\t\tif err != nil {\n\t\t\tglog.Warningf(\"eviction manager: failed to create hard memory threshold notifier: %v\", err)\n\t\t}\n\t\t// start hard memory notification"
  },
  {
    "id" : "3c3cdf10-b50b-48c9-9de5-4f9550dbcb8b",
    "prId" : 32577,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32577#pullrequestreview-3360694",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3da02b51-1428-4f44-b594-ad65c83880e3",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "note that in the future, we plan to support evictions both on root and on `--cgroups-root`. May be its worth adding a TODO here or even better file an issue for it. \n",
        "createdAt" : "2016-10-07T19:16:36Z",
        "updatedAt" : "2016-11-07T02:48:44Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "2583116f1a6e668500c86c9add88905e86e09c3e",
    "line" : null,
    "diffHunk" : "@@ -1,1 +158,162 @@\t\t}\n\t\t// TODO add support for eviction from --cgroup-root\n\t\tcgpath, found := cgroups.MountPoints[\"memory\"]\n\t\tif !found || len(cgpath) == 0 {\n\t\t\treturn fmt.Errorf(\"memory cgroup mount point not found\")"
  },
  {
    "id" : "29a84f26-2216-4cba-a8c5-329e49fe37bc",
    "prId" : 32577,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32577#pullrequestreview-6513016",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c45bcaa9-a906-4884-8d01-6e39ed8732be",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "nit: I'd prefer moving all cgroups logic to under `/pkg/kubelet/cm` to have better abstractions. I'm OK to perform that refactor in a separate PR though given that v1.5 FF is this week.\n",
        "createdAt" : "2016-10-31T19:57:57Z",
        "updatedAt" : "2016-11-07T02:48:44Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "2583116f1a6e668500c86c9add88905e86e09c3e",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +158,162 @@\t\t}\n\t\t// TODO add support for eviction from --cgroup-root\n\t\tcgpath, found := cgroups.MountPoints[\"memory\"]\n\t\tif !found || len(cgpath) == 0 {\n\t\t\treturn fmt.Errorf(\"memory cgroup mount point not found\")"
  },
  {
    "id" : "562f9ff0-b7a6-4d4a-8256-1cb957230919",
    "prId" : 31401,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b41e13c7-023c-48aa-895c-85192940a737",
        "parentId" : null,
        "authorId" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "body" : "Can we also fix the log above?\n",
        "createdAt" : "2016-08-25T04:07:37Z",
        "updatedAt" : "2016-08-25T21:11:29Z",
        "lastEditedBy" : "4e418bc8-21fb-4523-80c1-9c751c193126",
        "tags" : [
        ]
      },
      {
        "id" : "696b6ef9-ffd9-4980-add1-6a60d9b66043",
        "parentId" : "b41e13c7-023c-48aa-895c-85192940a737",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "I think dumping all the conditions is fine because a node could have multiple resources under pressure. We pick one resource as a reason to reject the pod, but we could still log all the conditions.\n",
        "createdAt" : "2016-08-25T18:11:47Z",
        "updatedAt" : "2016-08-25T21:11:29Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      },
      {
        "id" : "d997dc93-bccf-4daa-897a-f83f8c0a9d4d",
        "parentId" : "b41e13c7-023c-48aa-895c-85192940a737",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "Done.\n",
        "createdAt" : "2016-08-25T21:11:58Z",
        "updatedAt" : "2016-08-25T21:11:58Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "a072bda6fd598f2afdb5b30f571ae37e4700d70e",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +113,117 @@\t// reject pods when under memory pressure (if pod is best effort), or if under disk pressure.\n\tglog.Warningf(\"Failed to admit pod %q - node has conditions: %v\", format.Pod(attrs.Pod), m.nodeConditions)\n\treturn lifecycle.PodAdmitResult{\n\t\tAdmit:   false,\n\t\tReason:  reason,"
  },
  {
    "id" : "59a323b0-6cee-4962-b27f-be670d071982",
    "prId" : 29880,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10ad3e86-90c2-4cca-ac4a-859e018801ef",
        "parentId" : null,
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "Please group this section into a method.\n",
        "createdAt" : "2016-08-03T22:09:39Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      },
      {
        "id" : "7de48bfb-ee9a-4f02-af99-5d0e8084a90f",
        "parentId" : "10ad3e86-90c2-4cca-ac4a-859e018801ef",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "done\n",
        "createdAt" : "2016-08-04T20:42:52Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "68bc47ecc6740362e7580683cf1b26728f21fc14",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +217,221 @@\tm.recorder.Eventf(m.nodeRef, api.EventTypeWarning, \"EvictionThresholdMet\", \"Attempting to reclaim %s\", resourceToReclaim)\n\n\t// check if there are node-level resources we can reclaim to reduce pressure before evicting end-user pods.\n\tif m.reclaimNodeLevelResources(resourceToReclaim, observations) {\n\t\tglog.Infof(\"eviction manager: able to reduce %v pressure without evicting pods.\", resourceToReclaim)"
  },
  {
    "id" : "3ca23c46-c05a-42d5-9238-cf27a74cb383",
    "prId" : 29880,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a82faad9-9026-4133-ae50-4c9c49f763aa",
        "parentId" : null,
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "resourceToReclaim is the first entry in starvedResources. In the case when there are multiple starved resources, only one resource is reclaimed. Is this intended? Why not reclaiming all the starved resources?\n",
        "createdAt" : "2016-08-04T17:24:38Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      },
      {
        "id" : "e64af7b6-7718-4e42-a204-22a5c6b69a29",
        "parentId" : "a82faad9-9026-4133-ae50-4c9c49f763aa",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "this is intended.  we will prioritize reclaiming memory, and subsequent synchronization passes will reclaim the other resources.\n",
        "createdAt" : "2016-08-04T20:43:32Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "fe3df612-9091-4efe-915f-ca0b78218f39",
        "parentId" : "a82faad9-9026-4133-ae50-4c9c49f763aa",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "i see your point now after further reflection.\n\nin the case of pod eviction, we score pods by a primary resource, and we prioritize reclaiming memory over disk.  for node-level reclaim, i could see just reclaiming all starved resources.  in practice right now, we have no memory based reclaim.  for disk, i guess if both nodefs and imagefs were starved, we would get a benefit in calling the super set of functions.\n",
        "createdAt" : "2016-08-04T20:47:11Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "0167963d-1381-49c0-b022-f347f7481c31",
        "parentId" : "a82faad9-9026-4133-ae50-4c9c49f763aa",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "can we handle this in a follow-on?  the concern i have is it could make the checks if thresholds were resolved more complicated.  I will leave a TODO for now.\n",
        "createdAt" : "2016-08-04T21:07:31Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "f7b6eeed-7b1a-4ffd-8931-9ccf1820eb1a",
        "parentId" : "a82faad9-9026-4133-ae50-4c9c49f763aa",
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "Yeah that's what I am thinking.\n",
        "createdAt" : "2016-08-04T21:10:30Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      }
    ],
    "commit" : "68bc47ecc6740362e7580683cf1b26728f21fc14",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +218,222 @@\n\t// check if there are node-level resources we can reclaim to reduce pressure before evicting end-user pods.\n\tif m.reclaimNodeLevelResources(resourceToReclaim, observations) {\n\t\tglog.Infof(\"eviction manager: able to reduce %v pressure without evicting pods.\", resourceToReclaim)\n\t\treturn"
  },
  {
    "id" : "701f9ef5-4e8a-4294-adb4-983de2384bb6",
    "prId" : 29880,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1bc17d5-c8bf-4bc7-8048-c276e699d60d",
        "parentId" : null,
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "The existing reclaimResources() seems to collide with this method in terms of naming. How about naming the existing reclaimResources() to something like getStarvedResources()?\n",
        "createdAt" : "2016-08-04T17:24:40Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      },
      {
        "id" : "a31afde4-745d-4923-85d6-dbcdf78675fe",
        "parentId" : "f1bc17d5-c8bf-4bc7-8048-c276e699d60d",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "good point.\n",
        "createdAt" : "2016-08-04T20:52:39Z",
        "updatedAt" : "2016-08-04T21:13:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "68bc47ecc6740362e7580683cf1b26728f21fc14",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +272,276 @@\n// reclaimNodeLevelResources attempts to reclaim node level resources.  returns true if thresholds were satisfied and no pod eviction is required.\nfunc (m *managerImpl) reclaimNodeLevelResources(resourceToReclaim api.ResourceName, observations signalObservations) bool {\n\tnodeReclaimFuncs := m.resourceToNodeReclaimFuncs[resourceToReclaim]\n\tfor _, nodeReclaimFunc := range nodeReclaimFuncs {"
  },
  {
    "id" : "0dc8d036-61cf-407d-b417-ef7f4fadadf5",
    "prId" : 27199,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a590c50-48d6-4148-8c07-bece76ed28ca",
        "parentId" : null,
        "authorId" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "body" : "Where do you plan to invoke image GC when there's disk pressure?(https://github.com/kubernetes/kubernetes/blob/186c0c7b14dfc9403b131b68b70da08227c71a04/pkg/kubelet/image_manager.go#L51 provides the method to delete all unused images.)\n",
        "createdAt" : "2016-07-19T05:23:36Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "4a6c4c9f-42c7-4b89-b4ff-53e56b69cd54",
        "tags" : [
        ]
      },
      {
        "id" : "956695a0-20e8-4d38-a551-0097c9d6353b",
        "parentId" : "3a590c50-48d6-4148-8c07-bece76ed28ca",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "i was going to try and structure it as a series of actions, this call was not around when this PR was opened.  will think on the best way to structure it cleanly in a follow-on.\n",
        "createdAt" : "2016-07-20T19:23:06Z",
        "updatedAt" : "2016-07-28T20:20:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "d37710f87b7a2497d918d8c5e51323e352dcce98",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +90,94 @@\t\treturn lifecycle.PodAdmitResult{Admit: true}\n\t}\n\n\t// the node has memory pressure, admit if not best-effort\n\tif hasNodeCondition(m.nodeConditions, api.NodeMemoryPressure) {"
  }
]