[
  {
    "id" : "5135b70d-2a14-41d9-9e03-ae8ac8152552",
    "prId" : 77892,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77892#pullrequestreview-238219307",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e53eada2-7a7f-48c8-8a42-23c28f2cfee7",
        "parentId" : null,
        "authorId" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "body" : "Why do not use w.Stop? w.stopped chan seems not be closed?",
        "createdAt" : "2019-05-16T06:08:11Z",
        "updatedAt" : "2019-05-16T06:09:11Z",
        "lastEditedBy" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "tags" : [
        ]
      },
      {
        "id" : "eace4151-79f5-4b11-ba93-fddd6cb5c200",
        "parentId" : "e53eada2-7a7f-48c8-8a42-23c28f2cfee7",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "if we call Stop here it will block until the time out. chans don't need to be closed unless something is waiting on them. stopped will go out of scope and be garbage collected. https://groups.google.com/forum/m/#!msg/golang-nuts/pZwdYRGxCIk/qpbHxRRPJdUJ\r\n\r\n> it is only necessary to close a channel if the receiver is\r\n> looking for a close.  Closing the channel is a control signal on the\r\n> channel indicating that no more data follows.",
        "createdAt" : "2019-05-16T06:38:50Z",
        "updatedAt" : "2019-05-16T06:41:06Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "034ad04f-548a-4be5-8346-f1fdd2179ad3",
        "parentId" : "e53eada2-7a7f-48c8-8a42-23c28f2cfee7",
        "authorId" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "body" : "> if we call Stop here it will block until the time out. chans don't need to be closed unless something is waiting on them. stopped will go out of scope and be garbage collected. https://groups.google.com/forum/m/#!msg/golang-nuts/pZwdYRGxCIk/qpbHxRRPJdUJ\r\n> \r\n> > it is only necessary to close a channel if the receiver is\r\n> > looking for a close.  Closing the channel is a control signal on the\r\n> > channel indicating that no more data follows.\r\n\r\nThanks for your clarification!",
        "createdAt" : "2019-05-16T07:29:34Z",
        "updatedAt" : "2019-05-16T07:29:34Z",
        "lastEditedBy" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "tags" : [
        ]
      }
    ],
    "commit" : "531a50c776f149be1c9dd58272bd9ed1aee5aba8",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +113,117 @@\tif len(w.deprecatedPath) != 0 {\n\t\tif err := w.traversePluginDir(w.deprecatedPath); err != nil {\n\t\t\tw.fsWatcher.Close()\n\t\t\treturn fmt.Errorf(\"failed to traverse deprecated plugin socket path %q, err: %v\", w.deprecatedPath, err)\n\t\t}"
  },
  {
    "id" : "8dcf32a8-43a1-449f-8ca4-d9fe132a1ad2",
    "prId" : 77304,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77304#pullrequestreview-235634847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6affc4b-6dbc-4c17-a88e-769d5012216f",
        "parentId" : null,
        "authorId" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "body" : "@mattjmcnaughton Add sync.Once to keep close() would be called only once",
        "createdAt" : "2019-05-09T13:53:08Z",
        "updatedAt" : "2019-05-09T13:53:23Z",
        "lastEditedBy" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "tags" : [
        ]
      },
      {
        "id" : "57a1290c-5f9b-4e2e-858e-9002da3c6700",
        "parentId" : "d6affc4b-6dbc-4c17-a88e-769d5012216f",
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Thanks :)",
        "createdAt" : "2019-05-09T14:45:57Z",
        "updatedAt" : "2019-05-09T14:46:14Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      }
    ],
    "commit" : "a01f0b4e5eac9e3bb6b5e024f92df15bffe06c24",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +163,167 @@\tc := make(chan struct{})\n\tvar once sync.Once\n\tcloseFunc := func() { close(c) }\n\tgo func() {\n\t\tdefer once.Do(closeFunc)"
  },
  {
    "id" : "4942c580-6a56-4fb3-9195-e41633ddd94b",
    "prId" : 75110,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75110#pullrequestreview-231935628",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "291c6e71-aaf1-4257-b349-281582769529",
        "parentId" : null,
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "Looking at the implementation of handleCreateEvent, I think it's more direct to just do:\r\n```go\r\nif !w.containsBlacklistedDir(path) {\r\n  return w.traversePluginDir(path)\r\n}\r\n```",
        "createdAt" : "2019-04-23T17:43:01Z",
        "updatedAt" : "2019-04-23T20:24:53Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "310a352f-9ee7-4bbe-a9e2-796b04bf819a",
        "parentId" : "291c6e71-aaf1-4257-b349-281582769529",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Not sure if I follow, do you mean inside `handleCreateEvent`? If so, then it wouldn't call `handlePluginRegistration`... or am I missing somehting?",
        "createdAt" : "2019-04-23T19:40:48Z",
        "updatedAt" : "2019-04-23T20:24:53Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "578b4580-e2bb-4474-ad2b-edfb299d9eca",
        "parentId" : "291c6e71-aaf1-4257-b349-281582769529",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "I mean \"handle create event\" doesn't really seem like what this is doing, unless I'm not following the control flow. If you look at what handle create event is doing, the snippet I commented above is all that gets hit in that function, so I'm suggesting to just inline that piece here.",
        "createdAt" : "2019-04-29T17:02:58Z",
        "updatedAt" : "2019-04-29T17:02:58Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      },
      {
        "id" : "3f0fa5ac-98be-4907-bfe9-f9a2a99288c2",
        "parentId" : "291c6e71-aaf1-4257-b349-281582769529",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Ah, OK... I understand now...\r\n\r\nThis part of the code creates \"synthetic create events\" if we find a socket file. Then we \"handle\" this event right here (as opposed to sending it to a processing goroutine like it was before).\r\n\r\nSince we're dealing with a socket file, we  don't really want to `traversePluginDir()` it, but instead we want to register the plugin that's listening to this unix domain socket file (i.e., `handlePluginRegistration()`).\r\n\r\nHowever, we also need to make sure that we ignore black-listed directories (like you pointed in the snippet above) and files prefixed with a \".\"; this would do it:\r\n\r\n```go\r\n\t\t\tif !w.containsBlacklistedDir(path) {\r\n\t\t\t\tif !strings.HasPrefix(path, \".\") {\r\n\t\t\t\t\t// TODO: Handle errors by taking corrective measures\r\n\t\t\t\t\tif err := w.handlePluginRegistration(path); err != nil {\r\n\t\t\t\t\t\tklog.Errorf(\"error %v when handling create event for file: %s\", err, path)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n```\r\n\r\nIMO `handleCreateEvent()` looks like would be a better fit though (since we're almost re-implementing it in-line).\r\n\r\nDoes this make sense?",
        "createdAt" : "2019-04-29T21:30:36Z",
        "updatedAt" : "2019-04-29T21:30:36Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "90b5299e-3100-47ac-99d5-bd249bcaecee",
        "parentId" : "291c6e71-aaf1-4257-b349-281582769529",
        "authorId" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "body" : "Ah, sorry, I completely misread what `handleCreateEvent` was doing.",
        "createdAt" : "2019-04-29T22:41:38Z",
        "updatedAt" : "2019-04-29T22:41:38Z",
        "lastEditedBy" : "9f030d50-62db-4b00-a28c-847709b74d97",
        "tags" : [
        ]
      }
    ],
    "commit" : "f56455753b0e9054bfe756807c002f1a67704dcd",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +211,215 @@\t\t\t}\n\t\t\t//TODO: Handle errors by taking corrective measures\n\t\t\tif err := w.handleCreateEvent(event); err != nil {\n\t\t\t\tklog.Errorf(\"error %v when handling create event: %s\", err, event)\n\t\t\t}"
  },
  {
    "id" : "7af44894-c9ee-48e7-923d-a8a673c01f33",
    "prId" : 75110,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75110#pullrequestreview-231979037",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67d2632a-9318-4486-89d9-ff29055401c7",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Shouldn't the err be returned, in the same way error is returned for mode.IsDir() case ?\r\n\r\nI created PR #77244",
        "createdAt" : "2019-04-30T02:46:43Z",
        "updatedAt" : "2019-04-30T03:22:41Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "f56455753b0e9054bfe756807c002f1a67704dcd",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +211,215 @@\t\t\t}\n\t\t\t//TODO: Handle errors by taking corrective measures\n\t\t\tif err := w.handleCreateEvent(event); err != nil {\n\t\t\t\tklog.Errorf(\"error %v when handling create event: %s\", err, event)\n\t\t\t}"
  },
  {
    "id" : "faea7deb-5711-4859-afd8-4165447246bb",
    "prId" : 71440,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71440#pullrequestreview-179050820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "the goroutine inside traversePluginDir also makes order of events non-deterministic, especially if changes are occurring at the same time the initial scan is being done.",
        "createdAt" : "2018-11-27T15:52:46Z",
        "updatedAt" : "2018-11-27T15:52:46Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "5ef77055-2f3a-45e3-81d5-ab899bfe10a2",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "does the handleCreateEvent function verify the created path still exists?",
        "createdAt" : "2018-11-27T15:54:01Z",
        "updatedAt" : "2018-11-27T15:54:01Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "1a22a50f-fb51-451f-8283-7de2969f8e8a",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "The `goroutine`in `traversePluginDir` is used to place items on a non-buffered channel, `ws.fsWatcher.Events`.  If must be present to avoid deadlocks.",
        "createdAt" : "2018-11-27T17:18:24Z",
        "updatedAt" : "2018-11-27T17:18:24Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "b1270079-af00-42f8-a362-53499dbf948e",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "`handleCreateEvent` does not explicitly re-check existence of the dir right before the Driver is delegated to handle the registration: https://github.com/kubernetes/kubernetes/blob/e86bdc79895cfa5b744a3b44ee9aa59ed5cd7a31/pkg/kubelet/util/pluginwatcher/plugin_watcher.go#L250\r\n\r\nWith the changes in this PR, the fsnotify CREATE/DELETE operations should not occur out of sync.  If a dir existed right before Registration, it should not go away until a DELETE event comes right after it.",
        "createdAt" : "2018-11-27T17:26:05Z",
        "updatedAt" : "2018-11-27T17:26:05Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "c5ba9b9a-c996-47d6-84ed-05e69c7e98b3",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "> the fsnotify CREATE/DELETE operations should not occur out of sync\r\n\r\nThat is the expectation from the underlying library. Just to be safe, it will be worth catching any potential issue around delete after create operations and logging it.",
        "createdAt" : "2018-11-27T18:44:40Z",
        "updatedAt" : "2018-11-27T18:44:40Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "82c44f3f-5eca-464d-9b1d-199d92856ad4",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "> With the changes in this PR, the fsnotify CREATE/DELETE operations should not occur out of sync. If a dir existed right before Registration, it should not go away until a DELETE event comes right after it.\r\n\r\nnothing guarantees that, correct?\r\n\r\n1. traversePluginDir is called\r\n2. traversePluginDir adds a filesystem watch to a particular directory\r\n\r\n    https://github.com/kubernetes/kubernetes/blob/fad23990cea2f883c98684fc2f2b600a3c1bdbd8/pkg/kubelet/util/pluginwatcher/plugin_watcher.go#L205-L212\r\n\r\n3. traversePluginDir descends into the directory and adds synthetic Create events for the files found in the dir via a goroutine\r\n\r\n    https://github.com/kubernetes/kubernetes/blob/fad23990cea2f883c98684fc2f2b600a3c1bdbd8/pkg/kubelet/util/pluginwatcher/plugin_watcher.go#L215-L221\r\n\r\nbecause there is an active watcher registered in step 2 that can immediately start delivering events, and the synthetic create events in step 3 are delivered via a goroutine, they can interleave with actual observed filesystem events in non-deterministic ways. For example, if a driver is being deleted while this runs:\r\n\r\n1. filepath.Walk lists dir, sees driver socket file\r\n2. driver socket file is deleted\r\n3. filesystem delete event is observed and queued\r\n4. filepath.Walk queues synthetic create event\r\n\r\nbecause the delete is handled, then the synthetic create event, could we end up with a registered driver that doesn't actually exist any more?",
        "createdAt" : "2018-11-27T18:50:33Z",
        "updatedAt" : "2018-11-27T18:50:33Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "09fd33a0-d729-4807-a732-e95fac56cdcf",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "To guarantee consistency, shouldn't you be enqueuing existing sockets first prior to accepting fsnotify events from the kernel? Imagine a situation where a socket was identified by path traversal, but before `traversePluginDir` can enqueue a create event, the socket get's deleted and that event gets processed first before the creation event?",
        "createdAt" : "2018-11-27T19:03:50Z",
        "updatedAt" : "2018-11-27T19:03:50Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "31493917-707f-4d67-9dfb-0da08c82c374",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "> To guarantee consistency, shouldn't you be enqueuing existing sockets first prior to accepting fsnotify events from the kernel?\r\n\r\nthat's what I expected as well. registering watches on the dirs, processing the contents and enqueuing synthetic create events, then starting processing of the events from the registered watchers\r\n\r\n> Imagine a situation where a socket was identified by path traversal, but before traversePluginDir can enqueue a create event, the socket get's deleted and that event gets processed first before the creation event?\r\n\r\nyes, that's exactly the scenario described above",
        "createdAt" : "2018-11-27T19:13:13Z",
        "updatedAt" : "2018-11-27T19:13:49Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "d6f4538a-0afc-4e6c-97f9-82480ef7657c",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "@liggitt @vishh I think the code already has `HappensBefore` and `HappensAfter` serial properties that you are alluding to.  The code seems to have 1to1 parity between observed filesystem event and synthetic queued events.  To explain, let's further unpack the scenario that Jordan presented earlier:\r\n\r\n> 1. `filepath.Walk` lists dir, sees driver socket file\r\n\r\nSo let's look at some scenarios\r\n1. filepath.Walk hits dir, adds watcher for it, continue\r\n2.  filesystem creates socket file (from driver)\r\n  a. But, socket file is `immediately deleted` from filesystem\r\n  b. According to fswatcher, if the file is removed before it is observed, the Walk will generate an error\r\n3. filepath.Walk receives error because watcher is missing, returns\r\n\r\nScenario 2\r\n1. filepath.Walk hits dir, adds watcher for it, continue\r\n2.  filesystem creates socket file (from driver)\r\n3. filepath.Walk receives socket file info (prior to deletion)\r\n  a. queues synthetic create event\r\n4. Socket file is from filesystem\r\n5. filepath.Walk receives deleted file info (after deletion)\r\n  a. enqueues the observed delete event \r\n\r\nBecause there is a sequentiality between the creation and immediate deletion of the socket files, the observed events will have before/after relationships.  Therefore, the synthetic events are generated and placed on the internal event queue (fsWatcher.Events) should also inherit that sequentiality.\r\n\r\n",
        "createdAt" : "2018-11-27T22:04:55Z",
        "updatedAt" : "2018-11-27T22:04:55Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "e41121cd-a431-4bd5-ade7-ba03b4a7605b",
        "parentId" : "278976e4-ffb0-4f99-a26b-86c6cde20afd",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "the scenario described in https://github.com/kubernetes/kubernetes/pull/71440#discussion_r236798644 is still racy\r\n\r\nThe synthetic create events traversePluginDir sends to the channel (for socket files encountered by filepath.Walk) are independent of (and can race with) create/delete events sent to the channel by the registered filesystem watchers.\r\n\r\nThat said, if a synthetic create event *was* processed after an actual observed delete event, handleCreateEvent *does* verify the created path still exists:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/fad23990cea2f883c98684fc2f2b600a3c1bdbd8/pkg/kubelet/util/pluginwatcher/plugin_watcher.go#L240-L243\r\n\r\nI still think the raciness should be fixed in a follow up because it makes the event flow hard to understand and relies on compensation in the event handler, but in the context of this PR, it is not unsafe.",
        "createdAt" : "2018-11-27T22:54:12Z",
        "updatedAt" : "2018-11-27T22:54:12Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "e86bdc79895cfa5b744a3b44ee9aa59ed5cd7a31",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +112,116 @@\n\t\t\t\tw.wg.Add(1)\n\t\t\t\tfunc() {\n\t\t\t\t\tdefer w.wg.Done()\n"
  },
  {
    "id" : "1f2ac106-4e1f-4573-b746-c1ccab7dabab",
    "prId" : 70821,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70821#pullrequestreview-173223358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1eebe356-e22d-41de-8d26-152c49f205d1",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Will this cause all of plugin registration to fail or will it continue to walk?  What kind of traversal ordering does this do?",
        "createdAt" : "2018-11-08T21:07:53Z",
        "updatedAt" : "2018-11-09T00:20:42Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "25cdbbac-8405-4b0d-93fe-381b865314a5",
        "parentId" : "1eebe356-e22d-41de-8d26-152c49f205d1",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "> What kind of traversal ordering does this do?\r\n\r\nSee [filepath.Walk](https://golang.org/pkg/path/filepath/#Walk): \r\n> The files are walked in lexical order, which makes the output deterministic but means that for very large directories Walk can be inefficient. Walk does not follow symbolic links.\r\n\r\n> Will this cause all of plugin registration to fail or will it continue to walk?\r\n\r\nNo this will cause the traversePlugin function to ignore directories or files that can't be walked.\r\n",
        "createdAt" : "2018-11-08T22:30:53Z",
        "updatedAt" : "2018-11-09T00:20:42Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "35d4a0a8-f363-41a7-b3f0-c67484701fc9",
        "parentId" : "1eebe356-e22d-41de-8d26-152c49f205d1",
        "authorId" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "body" : "could we return a failure if it fail to access all dirs and files including root dir?",
        "createdAt" : "2018-11-08T23:15:04Z",
        "updatedAt" : "2018-11-09T00:20:42Z",
        "lastEditedBy" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "tags" : [
        ]
      },
      {
        "id" : "31b8c77e-07dd-4a51-8521-6ed6fae8ef8c",
        "parentId" : "1eebe356-e22d-41de-8d26-152c49f205d1",
        "authorId" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "body" : "we chat a little offline, i think we only need to return error when root dir access failed.",
        "createdAt" : "2018-11-08T23:43:16Z",
        "updatedAt" : "2018-11-09T00:20:42Z",
        "lastEditedBy" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "tags" : [
        ]
      },
      {
        "id" : "f4613eec-4881-4618-98a1-ab25c529e5fa",
        "parentId" : "1eebe356-e22d-41de-8d26-152c49f205d1",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Fixed",
        "createdAt" : "2018-11-08T23:44:54Z",
        "updatedAt" : "2018-11-09T00:20:42Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "11fef8ba3bae7a3b06f1c4aa4214fb94e3d11987",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +185,189 @@\t\t\t}\n\n\t\t\tglog.Errorf(\"error accessing path: %s error: %v\", path, err)\n\t\t\treturn nil\n\t\t}"
  },
  {
    "id" : "38d22a73-2710-47bf-9902-8574d996f8d2",
    "prId" : 64660,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64660#pullrequestreview-131831175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a62e1257-0807-4b84-a748-2cbbed74eaf5",
        "parentId" : null,
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "If the path is not a dir, is there further check that can be done? Is it assume if it's a file it's socket file ?",
        "createdAt" : "2018-06-20T20:28:57Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "bf570698-80bc-4c21-b5f6-134f32ae4a55",
        "parentId" : "a62e1257-0807-4b84-a748-2cbbed74eaf5",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "The only check that we've been doing is checking that it doesn't start with a '.'\r\nMore generally you won't know if it's a gRPC server until you try to connect to it",
        "createdAt" : "2018-06-20T22:06:22Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "3ffde856-1798-4c66-b71b-8c874e0735e7",
        "parentId" : "a62e1257-0807-4b84-a748-2cbbed74eaf5",
        "authorId" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "body" : "@RenaudWasTaken if it's a regular file, then you know you cannot connect it, right?",
        "createdAt" : "2018-06-23T01:34:33Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "tags" : [
        ]
      },
      {
        "id" : "eadcb793-1a76-4c28-934e-b6f2c10b9556",
        "parentId" : "a62e1257-0807-4b84-a748-2cbbed74eaf5",
        "authorId" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "body" : "i added file mode check in addition to Dir check, thanks\r\nhttps://golang.org/pkg/os/#FileMode",
        "createdAt" : "2018-06-26T00:36:14Z",
        "updatedAt" : "2018-06-26T00:36:14Z",
        "lastEditedBy" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "tags" : [
        ]
      }
    ],
    "commit" : "d04f5968293293668c675ef6c76439b37afe790f",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +177,181 @@\t}\n\n\tif !fi.IsDir() {\n\t\treturn w.registerPlugin(event.Name)\n\t}"
  },
  {
    "id" : "b62bc0c8-cab7-44f9-a583-4195b6971d5e",
    "prId" : 64660,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64660#pullrequestreview-132241556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb9a49eb-555a-4757-aebd-a5867a3f2069",
        "parentId" : null,
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "Use of this waitgroup is unclear (I know it's not related to this PR)",
        "createdAt" : "2018-06-20T20:46:11Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "124f14ec-a4d1-4490-8ba0-e3eb4466694e",
        "parentId" : "eb9a49eb-555a-4757-aebd-a5867a3f2069",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Do you mind explaining why this is unclear :) ?\r\nThis is a standard go patter where you add 1 before starting a goroutine",
        "createdAt" : "2018-06-20T22:09:29Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "4385c33b-b06d-454d-827d-5947e779c965",
        "parentId" : "eb9a49eb-555a-4757-aebd-a5867a3f2069",
        "authorId" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "body" : "it can‘t be more than 1 so technically it's not a group ;) as a barrier it just needs to be `chan`. ",
        "createdAt" : "2018-06-23T04:28:50Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "tags" : [
        ]
      },
      {
        "id" : "9fc72718-adf6-4535-8559-6fde1b230608",
        "parentId" : "eb9a49eb-555a-4757-aebd-a5867a3f2069",
        "authorId" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "body" : "I found in `Stop` function, this `wg` is translated to a chan, so it's really not necessary to use a waitgroup. ",
        "createdAt" : "2018-06-23T04:54:19Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "tags" : [
        ]
      },
      {
        "id" : "e4ecbd9c-6d79-4f2e-a501-54a4c51981d5",
        "parentId" : "eb9a49eb-555a-4757-aebd-a5867a3f2069",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "A waitgroup is a standard golang pattern, that is a lot more maintainable than having a channel.\r\n\r\nThat is, if in the future, this code has to handle a few more more goroutines, a waitgroup allows a simple barrier without having to change a lot of code or basically re-writing a waitgroup using channels.\r\n\r\nWhich, saying this makes me realize that we should add(1) each time we process an event.\r\n\r\n> I found in Stop function, this wg is translated to a chan, so it's really not necessary to use a waitgroup.\r\n\r\nThat's not the case, the Stop function is written in a way that after waiting 10 seconds on the `Done` call it raises a timeout error. It does not \"translate the wg to a channel\".",
        "createdAt" : "2018-06-23T21:20:42Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "21365a70-d1dd-44e6-98ea-82aae2bc045f",
        "parentId" : "eb9a49eb-555a-4757-aebd-a5867a3f2069",
        "authorId" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "body" : "@RenaudWasTaken it's typical when you have a *group* of events which you want to wait on. The group concept isn't useful here because `Start()` cannot be called more than once. The translation happens above what you pointed out in a goroutine. the goroutine is there because it needs a `chan` to select as you pointed out. ",
        "createdAt" : "2018-06-26T00:43:53Z",
        "updatedAt" : "2018-06-26T00:43:54Z",
        "lastEditedBy" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "tags" : [
        ]
      },
      {
        "id" : "41dc3e97-d679-4e86-89ec-ae0f227d4fdd",
        "parentId" : "eb9a49eb-555a-4757-aebd-a5867a3f2069",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "> @RenaudWasTaken it's typical when you have a group of events which you want to wait on. The group concept isn't useful here because Start() cannot be called more than once.\r\n\r\nIt is useful because it is a lot more maintainable. Just because we only have one cll to `Add` doesn't mean in the future this function won't have more. At that point it's a lot more useful and maintainable to have a waitgroup than to have to either add more `goroutineStopped` channels (i.e: rewriting a waitgroup) or replace the channel mechanism you are suggesting with a waitgroup.\r\n\r\nTo build on this point I was also suggesting to call `Add` for each event processed. This concrete example should show that it's a lot easier to always work with a waitgroup than to use a `goroutineStopped` channel per goroutine (or whatever pattern using channels).\r\n\r\nIndependently of maintainibility it's a lot easier to grasp the use of waitgroup than the use of having two or more channels.",
        "createdAt" : "2018-06-26T23:37:07Z",
        "updatedAt" : "2018-06-26T23:37:08Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d04f5968293293668c675ef6c76439b37afe790f",
    "line" : 230,
    "diffHunk" : "@@ -1,1 +210,214 @@\t}\n\n\tw.wg.Add(1)\n\tgo func(fsWatcher *fsnotify.Watcher) {\n\t\tdefer w.wg.Done()"
  },
  {
    "id" : "ce340f49-794f-4d05-af31-c42cc3b34eae",
    "prId" : 64660,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64660#pullrequestreview-131383748",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "404173fb-2c63-489b-8212-72f62f045e5f",
        "parentId" : null,
        "authorId" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "body" : "~`event.Op != fsnotify.Create`~ ignore this, original code is correct.",
        "createdAt" : "2018-06-23T04:13:07Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d04f5968293293668c675ef6c76439b37afe790f",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +168,172 @@// Handle filesystem notify event.\nfunc (w *Watcher) handleFsNotifyEvent(event fsnotify.Event) error {\n\tif event.Op&fsnotify.Create != fsnotify.Create {\n\t\treturn nil\n\t}"
  },
  {
    "id" : "313995a3-48de-4b24-a49e-c3a684ddeed3",
    "prId" : 64660,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64660#pullrequestreview-131383748",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28145318-0d9b-442d-bf86-4f227b5f54d6",
        "parentId" : null,
        "authorId" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "body" : "totally optional: could use `github.com/pkg/errors`.",
        "createdAt" : "2018-06-23T04:15:56Z",
        "updatedAt" : "2018-06-26T00:33:25Z",
        "lastEditedBy" : "fa37bbb4-59dd-42f0-8202-52d5717aff5e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d04f5968293293668c675ef6c76439b37afe790f",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +174,178 @@\tfi, err := os.Stat(event.Name)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"stat file %s failed: %v\", event.Name, err)\n\t}\n"
  },
  {
    "id" : "cd2701f7-dab5-4539-b584-200be9b5d117",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-148934055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bca8fd60-9be4-4ea4-9b8a-213ce39ed14e",
        "parentId" : null,
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "Couple of issues:\r\n\r\n- Is this really a `pluginPool`, with reusable plugins? Or, is it more like a cache, `pluginCache`?\r\n- I would consider using a named type for map[string]*sync.Mutex` for clarity and to convey what it is:\r\n```go\r\ntype pluginCacheItem map[string]*sync.Mutex\r\n...\r\ntype watcher struct {\r\n   ...\r\n   pluginsCache map[string]pluginCacheItem\r\n}",
        "createdAt" : "2018-08-20T20:32:50Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "dc0a747b-f448-4ca1-a939-a83c7ff1c238",
        "parentId" : "bca8fd60-9be4-4ea4-9b8a-213ce39ed14e",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "This map is a bit tricky, it's not really a \"reusable\" plugin pool nor a cache. It's a synchronization mechanism.\r\n\r\nThis is to handle the reregistration case:\r\n- When you have multiple plugins registering at the same time\r\n- These plugins have the same pluginType and pluginName hence the map of map , \r\n\r\nWhen you look at the State Machine you can see that you want to handle that event only during the `Register` state.\r\n\r\nGiven that we start a goroutine for each FSnotify event we would either need one goroutine per pluginType and pluginName or one mutex per pluginType and pluginName to handle that synchronization issue.\r\n\r\nI chose option 2 which basically stops all the goroutines from being continuing until the state machine reaches the `Register` state. \r\n\r\n\r\nFor your second suggestion, I'm happy to apply it, though I'm not sure that the `pluginCacheItem` fits. Do you have any idea?",
        "createdAt" : "2018-08-20T22:18:02Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "608803c3-cdd5-4da7-a995-9528493cce40",
        "parentId" : "bca8fd60-9be4-4ea4-9b8a-213ce39ed14e",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "Let me think about this.  Will get back",
        "createdAt" : "2018-08-23T14:19:52Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +46,50 @@\thandlers    map[string]PluginHandler\n\tplugins     map[string]pathInfo\n\tpluginsPool map[string]map[string]*sync.Mutex // map[pluginType][pluginName]\n}\n"
  },
  {
    "id" : "a20b7388-74f4-4338-aa62-084682c7f12c",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-151867691",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "parentId" : null,
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "The code is using two synchronization constructs, `workgroup` in `Start()` and `channel` in `Stop()`.  Why not use `wg.stopCh`  channel for synch between the `Start()` processing loop goroutine and `Stop()` ?",
        "createdAt" : "2018-08-20T20:49:40Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "196a9043-296a-4ee7-9907-662d24ed9e92",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "The current construct is:\r\n- The stopChan is used to signal all goroutine that they should stop.\r\n- The Waitgroup is used to wait for all the goroutines to stop\r\n\r\nMy experience has been that when you are trying to use a single channel to both signal a stop and wait for them to finish, you are basically re-implementing a workgroup usually using a \"custom protocol\" (sending ints, or constants, ...).\r\n\r\n",
        "createdAt" : "2018-08-20T22:02:23Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "99ba9e90-899c-40b4-bc70-1bc21c5b428f",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "In this context, it seems that once the processing loop (inside Start) is signaled to stop (via `<-w.stopCh`), there is no need for further coordination, there's nothing to wait for. Furthermore, any clean up can occur in fswatcher.Close().  ",
        "createdAt" : "2018-08-23T14:07:31Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "4bdb8d69-1d7b-465b-a164-6af7d7d83509",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "> In this context, it seems that once the processing loop (inside Start) is signaled to stop (via <-w.stopCh), there is no need for further coordination, there's nothing to wait for. \r\n\r\nHow would you catch goroutines that never return or resources that were never freed? This is particularly an issue when you are running tests.\r\n\r\nFor example if the goroutines that handle plugins of the previous tests are still running (whether you have a bug in your code or by sheer luck), they might interfere with your new test.\r\n\r\nIt seems like you feel the workgroup is unnecessary. Working in the kubernetes code base I've encountered a lot of issues due to the `Stop` function not freeing all the resources (gRPC being one of them).\r\nOn the other hand making sure that a module does indeed completely Stop when we call Stop managed to help catch a lot of bugs and made sure others don't happen.",
        "createdAt" : "2018-08-23T18:45:24Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "806557f2-eb92-4fd2-96ef-970c8c2ddfa2",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "Hi Renaud\r\nI am not arguing against workgroup and there are places, I am saying in this context I don't think you need it because there does not seem to be anything to wait for.  The processing loop in `Start()` has 3 `select` cases\r\n\r\n```go\r\nfunc Start() {...\r\n    for { select {\r\n       // #1\r\n       case event := <-fsWatcher.Events:\r\n           go func()...\r\n      // # 2\r\n       case err := <-fsWatcher.Errors:\r\n           continue\r\n       // # 3\r\n       case <-w.stopCh:\r\n           return\r\n    }}\r\n}\r\n```\r\n`Case 1` runs a new goroutine, and reading through the code, it has no coordination with its caller (no dependency on either channel or workgroup.  Meaning, whether it comes back or die, the caller does not care.\r\n`Case 2` is a simple continue.  There is no coordination needed here, the code does not have to wait for anything.\r\n`Case 3` is our signal watcher.  As soon as `w.stopCh` closes, it wins and ends  the loop and exits immediately.\r\n\r\n```go\r\nfunc Stop() {\r\n    close(w.stopCh)\r\n}\r\n```\r\nSo when `Stop()` closes the channel,  `Case 3` will occur and causes the processing loop to return.  At that point, I don't see why now `Stop()` has to wait for anything to complete from the loop above.  Maybe I am missing something...\r\n",
        "createdAt" : "2018-08-23T20:34:49Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "af90b693-564c-4949-bd0d-fc3f701fd5e1",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Looks like you caught a bug, I'm not increasing the count when starting a goroutine for the plugins.\r\n\r\n> I am saying in this context I don't think you need it because there does not seem to be anything to wait for. \r\n> it has no coordination with its caller (no dependency on either channel or workgroup. Meaning, whether it comes back or die, the caller does not care.\r\n\r\nYou are right in saying that the code in it's current state wouldn't be affected by removing this workgroup. I also agree that there isn't any coordination between the goroutines.\r\n\r\nThe problem the waitgroup is solving is catching the errors that will arise from changing the code. Making sure that all the gorountines returns protects developers from making mistakes that might not be caught otherwise.\r\nI believe this pattern increases maintainability of the code and provides strong guarantees to the developers using this module (i.e: All resources have been freed and all goroutines are stopped when `Stop()` returns).\r\n\r\nDo you think this piece of code decreases the overall readability? In which case is this an issue we can solve by better commenting the code?",
        "createdAt" : "2018-08-23T21:16:40Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "6012206f-1800-40b8-8d1f-c192d6df6dff",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "> The problem the waitgroup is solving is catching the errors that will arise from changing the code. Making sure that all the gorountines returns protects developers from making mistakes that might not be caught otherwise.\r\n\r\nThis is a fair point and I get that. It is always good idea to guard against rogue routines with timers to avoid indefinite blocking. \r\n\r\nBut using the 1 waitgroup and 2 channels still seems it can be done simply and still achieve the same. For instance, since you are creating a new channel, `c`, in `Stop()` to signal the end of wg.Wait(), how about moving that channel in `Watcher`, let's call it `waitCh` and remove the waitgroup altogether.  So the code becomes: \r\n\r\n```go\r\n\r\ntype Watcher struct {\r\n    <snip>\r\n    stopCh chan interface{}\r\n    waitCh chan interface{}\r\n}\r\n\r\n... \r\nfunc (w *Watcher) Start() error {\r\n    <snip>\r\n\r\n    w.stopCh = make(chan interface{})  // move in NewWatcher\r\n    w.waitCh = make(chan interface{})  // move in NewWatcher\r\n\r\n    ...\r\n    go func(fsWatcher *fsnotify.Watcher) {\r\n        defer close(w.waitCh)\r\n\r\n        for { select {\r\n        case event := <-fsWatcher.Events:\r\n           go func()...\r\n        case err := <-fsWatcher.Errors:\r\n           continue\r\n        case <-w.stopCh:\r\n           return\r\n        }}\r\n    }(fsWatcher)\r\n}\r\n\r\n...\r\nfunc (w *Watcher) Stop() error {\r\n\tclose(w.stopCh)\r\n\t\r\n\tselect {\r\n\tcase <-w.waitCh:\r\n\tcase <-time.After(10 * time.Second):\r\n\t\treturn fmt.Errorf(\"timeout on stopping watcher\")\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n```",
        "createdAt" : "2018-08-25T11:46:19Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      },
      {
        "id" : "57b30942-fb77-4e93-8cc2-788e3f9cc795",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "@vladimirvivien agree that prima facie waitgroup seems redundant but i am in favor of using waitgroup because we saw issues like this one in the past because of lack of proper clean up:\r\nhttps://github.com/kubernetes/kubernetes/issues/59488\r\nI think using waitgroup is a good pattern to follow, though seems redundant here.\r\n",
        "createdAt" : "2018-08-25T12:02:14Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "bc5482f1-bd7e-4449-948c-3b9689285a83",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "> Looks like you caught a bug, I'm not increasing the count when starting a goroutine for the plugins.\r\n\r\n\r\n+1",
        "createdAt" : "2018-08-25T12:11:41Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "796f7689-f5e4-4f39-a22b-ae0e9895f912",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "@vladimirvivien this pattern makes a lot of sense when you are only waiting for a single goroutine,\r\nIn this case, you pointed out that we weren't waiting for the plugins goroutine.\r\n\r\nThe way that I see this pattern scaling for multiple goroutines is the following two options:\r\n- You add more stop channels (maybe through a map)\r\n- You send a message in the stop channel for each goroutine\r\n\r\nEffectively in both case it seems to me that we are re-implementing a wait group.\r\n\r\nAppart from using a standard pattern, that's present in go's standard library, I believe that using a waitgroup even when you have a single goroutine is the better idea as it scales more amd is more maintainable.\r\n\r\nIt saves you the hassle of replacing the stopChan by a Waitgroup when/if you add more goroutines,\r\nIt also points the development effort in the right direction, i.e: Using a waitgroup rather than using a custom synchronisation mechanism.\r\n\r\n\r\nHow do you suggest we handle waiting for the multiple goroutines @vladimirvivien?",
        "createdAt" : "2018-08-26T15:22:56Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "4c206cf9-5ad7-49b5-b023-d6f8aa180589",
        "parentId" : "eedf2053-1097-4d2f-ad19-c806ce219448",
        "authorId" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "body" : "Sorry for delay.  I will answer this, but in there interest of time, let's keep moving with PR.\r\n\r\n> How do you suggest we handle waiting for the multiple goroutines @vladimirvivien?\r\n\r\nIn the case of work distribution with multiple goroutines, with a need to rendezvous at a certain execution point, a  workgroup certainly makes sense as a synchronization construct.  But the code, as-is in the `Start()` method, is not doing that.  It has 1 goroutine. I was pointing out that it can be simplified, deferring optimizing for multiple goroutines at a later time.",
        "createdAt" : "2018-09-03T19:50:54Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "54b8eb75-28dd-421d-a52b-63bf897147a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +98,102 @@\tw.fsWatcher = fsWatcher\n\n\tw.wg.Add(1)\n\tgo func(fsWatcher *fsnotify.Watcher) {\n\t\tdefer w.wg.Done()"
  },
  {
    "id" : "1efa48d7-4ee8-464c-964e-a8766ab1f371",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-149583003",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e259c91c-bc5c-4a51-918e-45f3c251bbc0",
        "parentId" : null,
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "just wondering how is it different from doing it before starting goroutine?",
        "createdAt" : "2018-08-25T10:43:17Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "96dd0b2b-8c21-4734-8d1d-6941cb798fe0",
        "parentId" : "e259c91c-bc5c-4a51-918e-45f3c251bbc0",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "TraversePluginDir will sequentially process each files and direcctories under the plugin path. For each these files it will start a goroutine.\r\n\r\nThese goroutines won't be processed until the main fsnotify goroutine is started. IMHO starting the processing goroutine seems like a pattern that will provide less failure and might prevent future errors.",
        "createdAt" : "2018-08-25T14:48:44Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "b618dc22-0014-42c1-9e10-04211f7cd909",
        "parentId" : "e259c91c-bc5c-4a51-918e-45f3c251bbc0",
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "> seems like a pattern that will provide less failure and might prevent future errors.\r\n\r\nagree. was just curious to know if you noticed any specific issue. Thanks!",
        "createdAt" : "2018-08-27T05:11:39Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +136,140 @@\n\t// Traverse plugin dir after starting the plugin processing goroutine\n\tif err := w.traversePluginDir(w.path); err != nil {\n\t\tw.Stop()\n\t\treturn fmt.Errorf(\"failed to traverse plugin socket path, err: %v\", err)"
  },
  {
    "id" : "74b52a45-7272-4872-a1a1-4e925254a1e1",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-151882588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e2de6a3-41ed-4de6-9f0d-5dc4151f68e5",
        "parentId" : null,
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "Is there a guideline behind using V(6) here and V(5) at L#189?",
        "createdAt" : "2018-08-25T10:44:42Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "fe1ae1cd-d1a1-41c7-9032-3d81037b9079",
        "parentId" : "9e2de6a3-41ed-4de6-9f0d-5dc4151f68e5",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Not really, what do you think is the right level?",
        "createdAt" : "2018-08-25T14:51:06Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "34f2b8c0-8c52-48e7-9a11-a1205bf64d51",
        "parentId" : "9e2de6a3-41ed-4de6-9f0d-5dc4151f68e5",
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "not sure either. However, lets keep same for sake of consistency.",
        "createdAt" : "2018-08-27T05:13:09Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "c0c190e8-36d2-47a7-9e29-435d871a409d",
        "parentId" : "9e2de6a3-41ed-4de6-9f0d-5dc4151f68e5",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Noting as a refactor followup",
        "createdAt" : "2018-09-03T23:51:06Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 185,
    "diffHunk" : "@@ -1,1 +204,208 @@// Handle filesystem notify event.\nfunc (w *Watcher) handleCreateEvent(event fsnotify.Event) error {\n\tglog.V(6).Infof(\"Handling create event: %v\", event)\n\n\tfi, err := os.Stat(event.Name)"
  },
  {
    "id" : "9693bd34-08bc-4e93-8244-0a39a82d3341",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-149512231",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "56eb95fe-34b2-4f91-bd82-6ea0ac079347",
        "parentId" : null,
        "authorId" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "body" : "not sure if 1 sec was enough or 10 sec is too long.",
        "createdAt" : "2018-08-25T11:01:09Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "51f59c69-efc0-451a-bd8f-d9fa2c281fd3",
        "tags" : [
        ]
      },
      {
        "id" : "952bd75e-b399-46b1-8167-d2000ec4e0fe",
        "parentId" : "56eb95fe-34b2-4f91-bd82-6ea0ac079347",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "I didn't change the timeout here, I only moved it from inside the dial to outside of it :)",
        "createdAt" : "2018-08-25T14:51:59Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 209,
    "diffHunk" : "@@ -1,1 +225,229 @@func (w *Watcher) handlePluginRegistration(socketPath string) error {\n\t//TODO: Implement rate limiting to mitigate any DOS kind of attacks.\n\tclient, conn, err := dial(socketPath, 10*time.Second)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"dial failed at socket %s, err: %v\", socketPath, err)"
  },
  {
    "id" : "9ddff55d-752a-45e3-9747-d3dbbd0f0ce2",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-151883545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1cd4c6b6-3e2e-4c06-874f-e88b77e3c6fb",
        "parentId" : null,
        "authorId" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "body" : "there are two type of mutex here:\r\n```\r\n\tmutex       sync.Mutex\r\n\tpluginsPool map[string]map[string]*sync.Mutex // map[pluginType][pluginName]\r\n```\r\nthe mutex is protecting the handler map.\r\nthe other is protecting re-registration/de-registration, \r\ni think both can be renamed that better reflecting the actual meaning.\r\nmy suggestion for those two names is: `handlerLock and registrationLocks`, but there could be other option.\r\n ",
        "createdAt" : "2018-09-03T23:32:22Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "tags" : [
        ]
      },
      {
        "id" : "8d8462ec-eb2c-45bc-8a08-15a8d9f8cffc",
        "parentId" : "1cd4c6b6-3e2e-4c06-874f-e88b77e3c6fb",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Would a followup PR be a solution that works for you (the code freeze is tomorrow)? ",
        "createdAt" : "2018-09-03T23:49:24Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      },
      {
        "id" : "6bd1b413-b501-49f2-b17e-cc6bbfd6b0ca",
        "parentId" : "1cd4c6b6-3e2e-4c06-874f-e88b77e3c6fb",
        "authorId" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "body" : "it is fine if you can follow up in a later PR, thanks",
        "createdAt" : "2018-09-04T00:08:58Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "bc94d261-7b05-4d36-85d9-895265d6df26",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +46,50 @@\thandlers    map[string]PluginHandler\n\tplugins     map[string]pathInfo\n\tpluginsPool map[string]map[string]*sync.Mutex // map[pluginType][pluginName]\n}\n"
  },
  {
    "id" : "e981811f-abb8-47d7-8bde-a01d618301e2",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-152914439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87f1b782-7d3d-47e3-beef-fd8c7c5ce61f",
        "parentId" : null,
        "authorId" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "body" : "does it have to backoff in case this error bumping too fast? if so, this would cause a high occupation of cpu.",
        "createdAt" : "2018-09-06T08:50:14Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "tags" : [
        ]
      },
      {
        "id" : "eabceaca-c58c-495c-b9d2-eb29d36a5034",
        "parentId" : "87f1b782-7d3d-47e3-beef-fd8c7c5ce61f",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "The error cases I've seen for this are mostly fsnotify having too many fds open, which usually causes failure everywhere in the kubelet.",
        "createdAt" : "2018-09-06T12:54:32Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +124,128 @@\t\t\t\t}()\n\t\t\t\tcontinue\n\t\t\tcase err := <-fsWatcher.Errors:\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Errorf(\"fsWatcher received error: %v\", err)"
  },
  {
    "id" : "be51fe4e-810f-40d9-9502-3493237174b9",
    "prId" : 64621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64621#pullrequestreview-152914593",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f358bc7-ab51-419a-bbbd-756513dbeba3",
        "parentId" : null,
        "authorId" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "body" : "I don't find that traversePluginDir() will raise this event, will it be called?",
        "createdAt" : "2018-09-06T08:57:37Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "7b10c627-72bd-4617-bd7a-2296b88861b7",
        "tags" : [
        ]
      },
      {
        "id" : "dea60763-7548-4dce-92ed-2422930a3842",
        "parentId" : "3f358bc7-ab51-419a-bbbd-756513dbeba3",
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "Yes fsnotify calls it",
        "createdAt" : "2018-09-06T12:54:55Z",
        "updatedAt" : "2018-09-06T13:35:15Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dd1d27c0345a3b288ce3e99cb47b2d00bd219a4",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +115,119 @@\t\t\t\t\t\t\tglog.Errorf(\"error %v when handling create event: %s\", err, event)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if event.Op&fsnotify.Remove == fsnotify.Remove {\n\t\t\t\t\t\terr := w.handleDeleteEvent(event)\n\t\t\t\t\t\tif err != nil {"
  },
  {
    "id" : "38740e76-af54-4d6f-99ce-fc8843532e88",
    "prId" : 63328,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63328#pullrequestreview-122947350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e7c34ef-10c5-4e6a-b496-74e358b71ec2",
        "parentId" : null,
        "authorId" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "body" : "It would be great to be able to not call the driver directly, because it kind of makes it a kubernetes specific and one of goals of CSI for the driver to be completely CO agnostic, but instead to use an interface, which can be generic definition but underneath with CSI or another plugin specific implementation.  Example, to be able to reply to GetInfo call, all CSI drivers will have to be aware of this type of request which is kind of kubernetes specific and it goes a bit against CSI approach. If registerPlugin is implemented as interface, and then each plugin uses plugin specific way to get required information, then this code is kept generic but we do not loose flexibility.\r\n@saad-ali @vladimirvivien Would you agree  with proposed modification?",
        "createdAt" : "2018-05-22T22:36:33Z",
        "updatedAt" : "2018-05-29T16:00:59Z",
        "lastEditedBy" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "tags" : [
        ]
      },
      {
        "id" : "05b8a2d3-4bd8-4ee7-929d-23b51ca236bf",
        "parentId" : "4e7c34ef-10c5-4e6a-b496-74e358b71ec2",
        "authorId" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "body" : "I thought CSI sidecar container will handle the interaction with kubelet, which would also include supporting this registerapi. Can some CSI folks double verify this?",
        "createdAt" : "2018-05-22T23:39:56Z",
        "updatedAt" : "2018-05-29T16:00:59Z",
        "lastEditedBy" : "611b3189-700b-4eda-8a2a-2c4280218d7c",
        "tags" : [
        ]
      },
      {
        "id" : "4299d552-47cb-49db-8c49-e17b6d9f7d21",
        "parentId" : "4e7c34ef-10c5-4e6a-b496-74e358b71ec2",
        "authorId" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "body" : "@jiayingz If side car handles `GetInfo` request then it would require two sockets, one opened by CSI driver for CSI related communication and one opened by sidecar to reply to GetInfo request,  how do you plan to deal now with two opened sockets? Where would sidecar create `GetInfo` socket? ",
        "createdAt" : "2018-05-23T00:12:12Z",
        "updatedAt" : "2018-05-29T16:00:59Z",
        "lastEditedBy" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "tags" : [
        ]
      },
      {
        "id" : "4d68a670-55c7-44be-819f-abf2cd5a5c65",
        "parentId" : "4e7c34ef-10c5-4e6a-b496-74e358b71ec2",
        "authorId" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "body" : "Defining registration interface, imho would be the cleanest solution.",
        "createdAt" : "2018-05-23T00:16:40Z",
        "updatedAt" : "2018-05-29T16:00:59Z",
        "lastEditedBy" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "tags" : [
        ]
      },
      {
        "id" : "e5616cf9-de7a-4753-870c-f473d9f4d8d9",
        "parentId" : "4e7c34ef-10c5-4e6a-b496-74e358b71ec2",
        "authorId" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "body" : "After discussion within CSI WG, please disregards these comments for now. In the interest of time, this PR should be merged and revisited later if required.",
        "createdAt" : "2018-05-24T11:18:58Z",
        "updatedAt" : "2018-05-29T16:00:59Z",
        "lastEditedBy" : "6c37c694-3f72-4ff3-ac3a-5fbddf4d5796",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a2e3bcc70ef9810b871db8bcf0599c4712f6bc2",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +112,116 @@\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\tdefer cancel()\n\tinfoResp, err := client.GetInfo(ctx, &registerapi.InfoRequest{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get plugin info using RPC GetInfo at socket %s, err: %v\", socketPath, err)"
  },
  {
    "id" : "223d526d-f163-4f82-b8f2-9795fd7b529b",
    "prId" : 63328,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63328#pullrequestreview-124394535",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e72afe49-62c0-4cea-8446-e645057cec26",
        "parentId" : null,
        "authorId" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "body" : "An interface model might be more explicit? i.e:\r\n```golang\r\ntype RegisterCallback interface {\r\n    Validate(...)\r\n    Register(...)\r\n}\r\n```",
        "createdAt" : "2018-05-30T13:10:40Z",
        "updatedAt" : "2018-05-30T13:17:36Z",
        "lastEditedBy" : "ac146833-f0d6-4680-968a-749269c0d55d",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a2e3bcc70ef9810b871db8bcf0599c4712f6bc2",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +35,39 @@\n// RegisterCallbackFn is the type of the callback function that handlers will provide\ntype RegisterCallbackFn func(pluginName string, endpoint string, versions []string, socketPath string) (error, chan bool)\n\n// Watcher is the plugin watcher"
  }
]