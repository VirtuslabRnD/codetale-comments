[
  {
    "id" : "187abd1e-f1e0-42ff-a13a-4e30e8c35d80",
    "prId" : 66165,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66165#pullrequestreview-137463073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38288622-0433-40f7-bb09-ee82bae2c817",
        "parentId" : null,
        "authorId" : "fa530650-5886-4415-a42f-0dee2e0e9ae3",
        "body" : "this is used at one more place:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/0fcc38425843d973538ce743a907e4c98095f8a0/pkg/apis/policy/validation/validation.go#L53-L55",
        "createdAt" : "2018-07-15T13:34:56Z",
        "updatedAt" : "2018-07-15T13:34:56Z",
        "lastEditedBy" : "fa530650-5886-4415-a42f-0dee2e0e9ae3",
        "tags" : [
        ]
      },
      {
        "id" : "f5c44340-71b2-4f60-bf4d-99f4e10d4b0a",
        "parentId" : "38288622-0433-40f7-bb09-ee82bae2c817",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "another sweep could be good, but I didn't see anything under that object that semantic equality would affect. I'd probably keep this PR focused on statefulset, since we want to backport it",
        "createdAt" : "2018-07-16T14:53:06Z",
        "updatedAt" : "2018-07-16T14:53:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "749484449aabf606a1dee389a2ffe78fd072411f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +150,154 @@\tstatefulSet.Spec.UpdateStrategy = oldStatefulSet.Spec.UpdateStrategy\n\n\tif !apiequality.Semantic.DeepEqual(statefulSet.Spec, oldStatefulSet.Spec) {\n\t\tallErrs = append(allErrs, field.Forbidden(field.NewPath(\"spec\"), \"updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden.\"))\n\t}"
  },
  {
    "id" : "88fb678b-2871-4c94-aa3d-4871b20f9405",
    "prId" : 46669,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46669#pullrequestreview-41861929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12288cba-2304-49fc-9df0-1668e9a642c4",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Are you intentionally allowing partition to be set for these strategies?  If so, add a comment.  If not... this is a significant source of pain for deployment patches when people change strategy types (validating sub fields and requiring them not to be set) because you can't just change the type.  We have discussed leaving them as allowable - copying @kargakis for opinion",
        "createdAt" : "2017-06-01T13:31:33Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "afcbed71-f27f-4615-a2c0-4bb8078043c6",
        "parentId" : "12288cba-2304-49fc-9df0-1668e9a642c4",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "I turned them off because it might be confusing. I'm open to suggestion about the best practice wrt to leaving the su- field allowable. I gave it some thought previously and thought that ignoring it might be the best course of action to allow easy type changes. After further thought it seems like it might be confusing. A user could get a SatefulSet and see a partition for rolling update and wonder why it isn't respected. During the design phase we kicked the idea around of applying a partition directly in the spec and we landed on using the structs. I'm leaning in favor of failing validation if the a partition is applied to a strategy that can not support it. It does not represent a valid API object based on the design.",
        "createdAt" : "2017-06-02T00:06:30Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "bdcf4f86-2a34-4dee-a793-ebea113692bd",
        "parentId" : "12288cba-2304-49fc-9df0-1668e9a642c4",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "@kow3ns does your comment hold true? I see you are validating now.",
        "createdAt" : "2017-06-02T16:21:34Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "9792a8d5-9460-42e0-be1f-fe7bc0e90dba",
        "parentId" : "12288cba-2304-49fc-9df0-1668e9a642c4",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "Yes I added the validation after @smarterclayton mentioned it and I thought it over. If it does become painful we can remove the validation without breaking the interface. We can't add it later without doing an API bump.",
        "createdAt" : "2017-06-02T19:17:08Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a784ef86ff94994e984753d885ffeff6e1772ac",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +79,83 @@\tcase \"\":\n\t\tallErrs = append(allErrs, field.Required(fldPath.Child(\"updateStrategy\"), \"\"))\n\tcase apps.OnDeleteStatefulSetStrategyType, apps.RollingUpdateStatefulSetStrategyType:\n\t\tif spec.UpdateStrategy.Partition != nil {\n\t\t\tallErrs = append("
  },
  {
    "id" : "7bd99004-c638-4b68-af25-dbf0a5c8b47e",
    "prId" : 46669,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46669#pullrequestreview-41946011",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I expect the approach of using a different strategy for partitioning to be more confusing and painful as opposed to this being an option for rollingUpdate. If I want to build an operator on top of StatefulSets, I am going to lose a lot of UX with the current approach. Assuming we add Conditions in the future and surface one once an update is complete, it is going to be much easier with a single strategy. I am thinking of post hooks. Now, you can't have a post-hook and re-use the StatefulSet controller to set a complete Condition just by switching Partition from 0 (which means all Pods have been updated) to nil. You have to make the operator set the Condition.\r\n\r\ncc: @tnozicka",
        "createdAt" : "2017-06-02T16:29:13Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "b38bf6d0-8cee-485d-afb5-4ed456248657",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "xref: https://github.com/kubernetes/community/pull/503#issuecomment-305850408",
        "createdAt" : "2017-06-03T11:20:38Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "9399caaf-781a-4afd-9419-10ab9c123a6b",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "I can get on board with the approach of adding  the partition to rolling update, but I think we can send the condition from the  controller in either case. In order to perform a staged rollout you can use patches like the one below while decrementing the ordinal.\r\n```json\r\n{\r\n\t\"updateStrategy\": {\r\n\t\t\"type\": \"Partitioned\",\r\n\t\t\"ordinal\": <ordinal>\r\n\t}\r\n}\r\n```\r\n\r\nSince the partitioned strategy never completes, no signal will be sent, via a Condition or otherwise, until the user switches the strategy back to rolling update or on delete. So the last applied patch will look like this.\r\n\r\n```json\r\n{\r\n\t\"updateStrategy\": {\r\n\t\t\"type\": \"RollingUpdate\"\r\n\t}\r\n}\r\n```\r\n\r\nAt this point the controller will complete the update and send any signals (including Conditions).\r\n\r\nWhat I like about this approach is that operators, human or otherwise, have an opportunity to validate the deployed product prior to triggering the controller to signal completion. Readiness checks are good for determining if the product is running, but this would allow for adding a smoke test prior completing the update.\r\n\r\nAgain, I don't feel super strongly about it ",
        "createdAt" : "2017-06-03T18:34:27Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "89525652-9633-4152-8401-af7b714e0b7b",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "> What I like about this approach is that operators, human or otherwise, have an opportunity to validate the deployed product prior to triggering the controller to signal completion. Readiness checks are good for determining if the product is running, but this would allow for adding a smoke test prior completing the update.\r\n\r\nNot sure why this is different in the case partitioning is an option of the rolling strategy. A smoke test prior to completing the update, or sending out an e-mail, or any kind of post-hook action could be possible to trigger by setting the ordinal to zero. At that point, the controller has moved all replicas to use the updated revision but it still waits for a higher-level orchestrator to set the ordinal to nil in order for the update to be considered as complete.\r\n\r\nAnother reason for moving this as an option vs a separate strategy is that in the future adding new strategies will make validation and UX even more complex and you will likely need to create an additional partition strategy for the new strategy. Take for example Deployments, we support Rolling and Recreate. Adding a single Partitioned strategy is not going to work for both, adding a PartitionedRolling and PartitionedRecreate will be a UX hit.",
        "createdAt" : "2017-06-04T00:01:31Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "550bd3e4-399e-4764-9b96-f20a4ea6faa8",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Would we have staged rollout marked as completed when we move partition inside the RollingUpdate struct ? I would like to see the same method of determining whether a Update completed or not , whether we are using staged or not. I would canary for 2 days or even 1 week, and having to determine the update completed based on looking at different fields is confusing not to say that a external tooling/portal showing the staged rollout as incomplete until the partitioning is removed and normal rolling update strategy is applied.",
        "createdAt" : "2017-06-04T00:55:00Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "f075ff1c-e58e-41a9-83b1-db85b5250360",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "@krmayankk No. That is not what @kargakis is suggesting. We would have a RollingUpdate that completes when all Pods are at a consistent user declared state indicated by both (in this most recent commit) the updateRevision and the currentRevision. We would remove the PartitionStrategy altogether.",
        "createdAt" : "2017-06-04T02:14:30Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "e653a208-7ee9-4f29-ad21-2bfba87d03f6",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "@kargakis, @smarterclayton  Do you feel strongly enough about this to gate LGTM? I'd be fine discussing this further offline and issuing PRs for removal in both the design and implementation after the fact, if that's where we land. If you think its a show stopper, I'd also be fine removing it in this PR provided we have general agreement. We can issue a PR to update the design concurrently.",
        "createdAt" : "2017-06-04T02:56:18Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "0f56c730-da58-428b-b98f-942762b99585",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I don't want to block this PR on this and I am fine with follow-ups if we decide to change. ",
        "createdAt" : "2017-06-04T09:52:32Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "6f1042b3-7026-4688-a1ec-d3dbae3b1c34",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I'd like to merge and then iterate.  I agree with the points @kargakis is making but once this lands we can do an offline discussion and reach closure in time for release cut.",
        "createdAt" : "2017-06-04T16:11:10Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "39f6445f-93f0-488a-a27a-a12782310a96",
        "parentId" : "18702541-e3b9-4a05-8199-19fe3572c217",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "@kow3ns please open an 1.7 issue so we can track",
        "createdAt" : "2017-06-04T17:01:15Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a784ef86ff94994e984753d885ffeff6e1772ac",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +89,93 @@\t\t}\n\tcase apps.PartitionStatefulSetStrategyType:\n\t\tif spec.UpdateStrategy.Partition == nil {\n\t\t\tallErrs = append(\n\t\t\t\tallErrs,"
  },
  {
    "id" : "0c40fe23-4e0e-44f4-992d-55b109ce2388",
    "prId" : 46669,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46669#pullrequestreview-41931042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "765768f4-5864-4f7a-84c5-4317fbc9bedf",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Is it only the volumeClaimTemplate that is not valid on updates now?",
        "createdAt" : "2017-06-02T16:34:09Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "dfceb3d5-2c1b-4c0c-8d71-deca9e9d92a9",
        "parentId" : "765768f4-5864-4f7a-84c5-4317fbc9bedf",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "So we made the PodTemplateSpec mutable, but nothing else. This means that labels and annotations still can't be added. I'm erring on the side of caution with respect to opening up mutation, but have been considering your previous comments that StatefulSet should be as mutable as DaemonSet an ReplicaSet. My thought is that we should reduce constraints over time rather than all at once  because once we make something mutable making it immutable again is a API breaking change.",
        "createdAt" : "2017-06-02T18:06:39Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "084be5a6-46f3-4593-95c8-ddb2224325f5",
        "parentId" : "765768f4-5864-4f7a-84c5-4317fbc9bedf",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "what about serviceName ?",
        "createdAt" : "2017-06-04T00:58:05Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "62e49678-3a71-4cc0-8363-a3e813ea9acd",
        "parentId" : "765768f4-5864-4f7a-84c5-4317fbc9bedf",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "Its mutable. You should look at the implementation of the main control loop. If you mutate it we will update the pod to conform in this code.\r\n```golang\r\n// updateIdentity updates pod's name, hostname, and subdomain to conform to set's name and headless service.\r\nfunc updateIdentity(set *apps.StatefulSet, pod *v1.Pod) {\r\n\tpod.Name = getPodName(set, getOrdinal(pod))\r\n\tpod.Namespace = set.Namespace\r\n\tpod.Spec.Hostname = pod.Name\r\n\tpod.Spec.Subdomain = set.Spec.ServiceName\r\n}\r\n```\r\nIf you are consuming it via the downward API you will see the change. You can get yourself into trouble playing with it, but you can get yourself into even more trouble playing with the Selector of DaemonSets, ReplicaSets, and Deployments, and, as there are, at least for Deployments and ReplicaSets, valid use cases for that mutation we allow it.\r\n",
        "createdAt" : "2017-06-04T01:20:30Z",
        "updatedAt" : "2017-06-06T20:47:39Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a784ef86ff94994e984753d885ffeff6e1772ac",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +158,162 @@\n\tif !reflect.DeepEqual(statefulSet.Spec, oldStatefulSet.Spec) {\n\t\tallErrs = append(allErrs, field.Forbidden(field.NewPath(\"spec\"), \"updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden.\"))\n\t}\n\tstatefulSet.Spec.Replicas = restoreReplicas"
  },
  {
    "id" : "aba8f9a6-1068-4a63-9419-7430469bdb60",
    "prId" : 45867,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45867#pullrequestreview-40242238",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Can this be zero?",
        "createdAt" : "2017-05-24T20:04:34Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "509689d2-36e8-4b43-b691-770cac207b62",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "yes \r\n```golang\r\n// Validates that given value is not negative.\r\nfunc ValidateNonnegativeField(value int64, fldPath *field.Path) field.ErrorList {\r\n\tallErrs := field.ErrorList{}\r\n\tif value < 0 {\r\n\t\tallErrs = append(allErrs, field.Invalid(fldPath, value, IsNegativeErrorMsg))\r\n\t}\r\n\treturn allErrs\r\n}\r\n```",
        "createdAt" : "2017-05-24T20:43:48Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "f4a3bca6-1bf2-4f63-a845-683297455076",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Sorry, I am asking if it should be valid for a revision to equal 0. Should all controllers start setting from zero? Do we care if some users will start from one?",
        "createdAt" : "2017-05-24T21:00:20Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "dbe4a9ac-cadc-44c0-9489-fa1c73fb717e",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Please consider not making it 0. In `kubectl rollout undo`, --to-revision=0 means rolling back to last revision without specifying a specific revision number. Deployment rollback (server side) also uses \"rollback to revision 0\" to roll back to the last revision.",
        "createdAt" : "2017-05-24T22:10:23Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "a465f586-034a-4aa4-9ab2-7bd54fe69ea2",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "I think that a ControllerRevision with a `.Revision` of 0 is a valid object. It doesn't enforce that the end user ever creates such an object, but implementing validation based on a convention for a kubectl flag default seems to be mixing concerns imo. What if the flag is defaulted to -1 when non-present in the future, or if a nil pointer is used to indicate its absence. I don't see a strong reason to say that a ControllerRevision with `.Revision` == 0, is not a valid object.",
        "createdAt" : "2017-05-24T23:30:58Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "71d7729f-6176-4e1a-a928-73efb12ff446",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Use -1 would work. Need to maintain backward compatibility when we 1) change `--to-revision` default value and 2) change what `--to-revision=0` means. \r\n\r\nWe can make both `--to-revision=-1` and `=0` work for Deployment (compatible with 1.6 server). But can we change the `--to-revision=0` behavior in 1.7?",
        "createdAt" : "2017-05-24T23:56:27Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "02dd27ba-1b87-4412-b8e6-59d40ac43886",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "I'm not saying we should change it, I'm just saying we shouldn't constrain validation of the API Object based on it. StatefulSet and DaemonSet are both going to use 1 for the first revision, but I don't think that necessitates constraining validation of the object.",
        "createdAt" : "2017-05-25T01:48:40Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "429a3d8a-439f-4e36-8a35-b8a2dfdc78ab",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Note that kubectl flags and their defaults are API and we are treating them like that.",
        "createdAt" : "2017-05-25T08:34:22Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "00626671-6d24-4095-890b-161596179bae",
        "parentId" : "b5cf51c3-c61f-4db5-856b-0e61b8d4f930",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "We also use 0 as \"rollback to the last revision\" in the Deployment API and not just kubectl: \r\nhttps://github.com/kubernetes/kubernetes/blob/4def5add114b651c26fe576b7315f8029bfce46a/pkg/apis/apps/v1beta1/types.go#L262\r\nhttps://github.com/kubernetes/kubernetes/blob/4def5add114b651c26fe576b7315f8029bfce46a/pkg/controller/deployment/rollback.go#L39\r\n\r\nBy validating zero here, we enforce the convention that rolling back to 0 means rolling back to the previous revision. I don't feel strong about it but then again I see we can have a solution that works the same across all controllers.",
        "createdAt" : "2017-05-25T09:27:09Z",
        "updatedAt" : "2017-05-25T18:39:15Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba128e6e417a8a2af062de0a81b4171e3a54a6fc",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +158,162 @@\t\terrs = append(errs, field.Required(field.NewPath(\"data\"), \"data is mandatory\"))\n\t}\n\terrs = append(errs, apivalidation.ValidateNonnegativeField(revision.Revision, field.NewPath(\"revision\"))...)\n\treturn errs\n}"
  },
  {
    "id" : "b2c39cd3-4110-47ac-ad3b-b3629f6884bb",
    "prId" : 24912,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74763064-86cd-4d5a-809a-ff634bd54c44",
        "parentId" : null,
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "Using `reflect.DeepEqual` is anti-pattern to compare structs.\nCould we use some sort of hash value to compare two petsets?\n",
        "createdAt" : "2016-04-29T04:48:38Z",
        "updatedAt" : "2016-05-05T01:39:30Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      },
      {
        "id" : "c0522081-c237-42c1-ac90-c3cab1467824",
        "parentId" : "74763064-86cd-4d5a-809a-ff634bd54c44",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "yeah but we use it all over the place in our validation (https://github.com/kubernetes/kubernetes/blob/master/pkg/api/validation/validation.go#L1383). It's leaves less room for error than a hash.\n",
        "createdAt" : "2016-04-29T05:19:10Z",
        "updatedAt" : "2016-05-05T01:39:30Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "007f2d08-82f4-4ebf-9569-f73f4042a8a6",
        "parentId" : "74763064-86cd-4d5a-809a-ff634bd54c44",
        "authorId" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "body" : "That doesn't look good. But I would leave it as it.\n",
        "createdAt" : "2016-04-29T05:49:16Z",
        "updatedAt" : "2016-05-05T01:39:30Z",
        "lastEditedBy" : "bb4cf218-381a-40ad-ac0c-0c2c66685cd4",
        "tags" : [
        ]
      }
    ],
    "commit" : "6bc3052551550aa2bd7306daed44c8b293221ff0",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +112,116 @@\tpetSet.Generation = oldPetSet.Generation\n\n\tif !reflect.DeepEqual(petSet, oldPetSet) {\n\t\tallErrs = append(allErrs, field.Forbidden(field.NewPath(\"spec\"), \"updates to petset spec for fields other than 'replicas' are forbidden.\"))\n\t}"
  }
]