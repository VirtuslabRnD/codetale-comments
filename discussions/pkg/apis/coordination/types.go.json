[
  {
    "id" : "4afa5179-3450-453e-9f8b-0579a925a099",
    "prId" : 64246,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64246#pullrequestreview-127673662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e9252b0-c583-45ce-b0be-cf22dfe0387a",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "If we don't have `Status`, do we need to nest in `Spec` (many API objects without `Status` choose not to explicitly nest their config in `Spec`)?",
        "createdAt" : "2018-06-06T18:42:52Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "962d3815-c95b-4d8a-a099-d16dc3f89020",
        "parentId" : "0e9252b0-c583-45ce-b0be-cf22dfe0387a",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Is there anything different than Secret and ConfigMap?\r\nI think that was actually a bad decision - we have convention that objects should have Spec and Status (if they are conceptually non-empty).",
        "createdAt" : "2018-06-07T09:56:59Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "53600462-b57d-4fb4-9e8d-ff8220d3ce54",
        "parentId" : "0e9252b0-c583-45ce-b0be-cf22dfe0387a",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "In this case, it's plausible we could add status in the future.",
        "createdAt" : "2018-06-11T16:29:08Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "552526d2-5e3a-4c40-bdcd-3351630f1b28",
        "parentId" : "0e9252b0-c583-45ce-b0be-cf22dfe0387a",
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Sounds good, we'll keep it as `Spec`",
        "createdAt" : "2018-06-11T17:53:33Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "0950084137a5abf5cd731bbda46ccb441499801d",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +32,36 @@\t// Specification of the Lease.\n\t// +optional\n\tSpec LeaseSpec\n}\n"
  },
  {
    "id" : "c523a036-67f4-4958-9d15-b994576010f0",
    "prId" : 64246,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64246#pullrequestreview-127655234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "024a56a0-fe00-4b47-ad74-c270f502c6e5",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "Have we considered something like `/renew` and `/reassign` subresources, I wonder if these would be more convenient to implement against, vs having to manually set the timestamps and transition count in a client?\r\nAlso thoughts on clock skew between clients and the API server?",
        "createdAt" : "2018-06-06T19:21:22Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      },
      {
        "id" : "1522aaa8-2719-42c4-a8b1-348a87469327",
        "parentId" : "024a56a0-fe00-4b47-ad74-c270f502c6e5",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I think this may be done on top of that PR - it's an addition, not a change to this PR.",
        "createdAt" : "2018-06-07T09:58:57Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "37cb3fae-4ac3-4d24-a90e-6f2b99e1011d",
        "parentId" : "024a56a0-fe00-4b47-ad74-c270f502c6e5",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "So in other words - those might be useful, but:\r\n1. our current leader election library already does that manually (so we're not introducing a new problem)\r\n2. we are already saying that clock skew can't be significant in your cluster (e.g. node controller is already looking into current time and comparing them with timestamps send by kubelets). \r\n\r\nSo let's make this PR purely API PR and make possible extensions in a separate one if needed.",
        "createdAt" : "2018-06-11T07:17:19Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "0f63c263-73fd-4813-94a8-c3064408000b",
        "parentId" : "024a56a0-fe00-4b47-ad74-c270f502c6e5",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "@mtaufen This type of issue was already discussed during the KEP itself:\r\nhttps://github.com/kubernetes/community/pull/2090\r\n\r\nThough perhaps we should expand the alternatives section.\r\n\r\nRe. timestamps: Clients really shouldn't be using the persisted timestamps. Components instead should treat observed time changes as triggers and use their own timers locally. In the case of crash, restart, etc., components should be conservative and restart their timers.\r\n\r\nRe. custom verbs: I understand the sentiment, but there are a number of challenges with it.\r\n1. I want to avoid business logic in kube-apiserver. It could be performed in admission control, I suppose, but I'd prefer admission control be used for validation and policy enforcement only.\r\n2. I want to avoid custom verbs. We're trying to move towards more consistent CRUD.\r\n3. I want to avoid synchronous verbs. The typical verb would just be `/acquire`, blocking while already leased by another. It could be implemented as an asynchronous verb, in which case it would fail if leased by another. However, in the latter case, the client would need to determine reasonable retry intervals, which suggests it would need to reason about the time-related fields. Additionally, in this case, we want kubelet to acquire the lease as soon as it is able, and we want node controller to observe whether the lease is held, but not to acquire the lease itself. So clients need to reason about most of the fields (and need to properly use resourceVersion).\r\n4. We have a purely client-based implementation, and such an implementation is simpler in aggregate for the system for our expected use cases, which are by Kubernetes components, including by the controller manager (for HA leader election). Due to the support for optimistic concurrency, effectively any resource can act as a lease. For a long time, this functionality has been implemented using annotations.\r\n\r\nSimilarly, the question of whether to use the etcd lease API was raised, which would analogous to the suggested `/reassign` (grant) and `/renew` (keep alive) verbs:\r\nhttps://coreos.com/etcd/docs/latest/learning/api.html#lease-api\r\n\r\nIn addition to creating a new, undesirable dependency on the etcd API, that API doesn't expose enough information for intended use cases, such as the holder of the lease. (I'm also not sure what happens in the case of a crash and restart.)\r\n\r\nNote that AcquireTime and LeaseTransitions are really just for debugging.",
        "createdAt" : "2018-06-11T17:08:37Z",
        "updatedAt" : "2018-06-27T11:31:21Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "0950084137a5abf5cd731bbda46ccb441499801d",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +25,29 @@\n// Lease defines a lease concept.\ntype Lease struct {\n\tmetav1.TypeMeta\n\t// +optional"
  }
]