[
  {
    "id" : "8fbd20e8-3565-44e0-a578-cfce88b335f5",
    "prId" : 83104,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83104#pullrequestreview-294109171",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "694110d0-a625-4e06-b1a0-0dd2819fb6bd",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "the comment just above says it can be hostname or IP, yet this line expects only IP. So what is correct here?",
        "createdAt" : "2019-09-26T10:13:56Z",
        "updatedAt" : "2019-10-02T00:55:50Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "46b3723e-aed1-4f98-aec3-5f8bc79274a2",
        "parentId" : "694110d0-a625-4e06-b1a0-0dd2819fb6bd",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Yeah, that comment is wrong. Because we can not fill hostname to endpoint address field. It should be a valid IP address.",
        "createdAt" : "2019-09-27T04:29:07Z",
        "updatedAt" : "2019-10-02T00:55:50Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "599fa5583eddad2176cb6075b53b4c9a0a6a7694",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +971,975 @@\t\tipaddr := dstrings.Join(nodeInfo.NodeAddRequest.Hostnames.Storage, \"\")\n\t\t// IP validates if a string is a valid IP address.\n\t\tip := net.ParseIP(ipaddr)\n\t\tif ip == nil {\n\t\t\treturn nil, fmt.Errorf(\"glusterfs server node ip address %s must be a valid IP address, (e.g. 10.9.8.7)\", ipaddr)"
  },
  {
    "id" : "2788457b-76c4-47f5-b8a1-c0ca1f70e6ee",
    "prId" : 76983,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76983#pullrequestreview-232189813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d9daa97-c77f-42a7-9229-388694189ec0",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "How does it interact with `backup-volfile-servers`? The comment above suggests that it does not really matter which address is used as mount source.",
        "createdAt" : "2019-04-30T13:45:08Z",
        "updatedAt" : "2019-04-30T13:45:09Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "e66b296b-4013-46d4-989e-e38dd6ab1a97",
        "parentId" : "3d9daa97-c77f-42a7-9229-388694189ec0",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@jsafrane  True.. The effect on `backup-volfile-servers` is negligible,  the reason being `backup-volfile-servers` used as  a list/iteration to try out if one/attempted mount server is not available.  Without randomization most of the mount happens on single/same server ( as its available mostly) and this patch should avoid such huge number of mounts on the same server.",
        "createdAt" : "2019-04-30T14:04:19Z",
        "updatedAt" : "2019-04-30T14:04:19Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "f30b14a408f70e63107c5688c3c67b36f8f1b7ff",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +396,400 @@\n\tif (len(addrlist) > 0) && (addrlist[0] != \"\") {\n\t\tip := addrlist[rand.Intn(len(addrlist))]\n\t\terrs = b.mounter.Mount(ip+\":\"+b.path, dir, \"glusterfs\", mountOptions)\n\t\tif errs == nil {"
  },
  {
    "id" : "caefb62f-e38b-4581-82b7-9bcd7cb989b4",
    "prId" : 76178,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76178#pullrequestreview-223215963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4793005e-8ab1-48e6-87a5-39e92fecc27d",
        "parentId" : null,
        "authorId" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "body" : "shouldn't the error printing be left to the caller?\r\n\r\n\r\nbut instead of doing the same error twice you can also do:\r\n\r\n```\r\nerr := fmt.Errorf(\"....\")\r\nklog.Error(err.Error())\r\nreturn .... err\r\n```",
        "createdAt" : "2019-04-05T11:06:38Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "2ce2b44c-9841-49e7-983e-fb7696974908",
        "tags" : [
        ]
      }
    ],
    "commit" : "e47a7897ea608c85715cc647b777d0dc276eafbf",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +824,828 @@\tif err != nil {\n\t\tklog.Errorf(\"failed to create endpoint/service %v/%v: %v\", epNamespace, epServiceName, err)\n\t\treturn nil, 0, \"\", fmt.Errorf(\"failed to create endpoint/service %v/%v: %v\", epNamespace, epServiceName, err)\n\t}\n\tklog.V(3).Infof(\"dynamic endpoint %v and service %v \", endpoint, service)"
  },
  {
    "id" : "4366462a-8e95-48ec-925f-83f2fc1d9ab5",
    "prId" : 76178,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76178#pullrequestreview-228947991",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53df9902-752f-4368-bc00-ab85a6ff664f",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "is this re-entrant, or if the update fails, will we be left with a non-functional volume and service/endpoint?",
        "createdAt" : "2019-04-15T15:09:11Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "8528b0ce-4fd1-451d-9a98-08eef61ce2be",
        "parentId" : "53df9902-752f-4368-bc00-ab85a6ff664f",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "what happens today if the service/endpoint create fails? is the function re-entrant?",
        "createdAt" : "2019-04-15T15:10:00Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "e7febc42-84d4-400e-8a90-76c7da1dea34",
        "parentId" : "53df9902-752f-4368-bc00-ab85a6ff664f",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@liggitt if the update failed, we have to cleanup the volume and endpoint. I have updated the patchset with this cleanup operation. Thanks !! ",
        "createdAt" : "2019-04-16T06:26:24Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "d15302f4-57b2-44a6-b35f-f85646e47c77",
        "parentId" : "53df9902-752f-4368-bc00-ab85a6ff664f",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "if this fails, does CreateVolume get recalled for the same PV later? if so, can it reuse the work already done, rather than trying to clean up every time? attempting to delete (like we did before with the volume, and like we're doing in this PR with the service/endpoint) does not mean we are re-entrant... network problems or process exit could leave us stranded before we have the ability to delete.\r\n\r\nThe three things this method is doing are:\r\n* create volume\r\n* create service\r\n* create endpoint with volume IP\r\n\r\nThere are multiple possibilities for each of those operations:\r\n* succeeds\r\n* fails because it already exists\r\n* fails for some other reason\r\n\r\nIf it already exists, are we responsible for reacting to the actual state of the already-existing thing? What does  that look like for each of the object types?\r\n\r\nIf it fails for some other reason, are we expected to fully clean up after ourselves (hint: that's not possible to guarantee... see comment about process exit or network errors), or simply return an error and get called again in the future to retry?",
        "createdAt" : "2019-04-17T15:19:20Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "1b309500-40c0-4430-a808-bd386d6ca78f",
        "parentId" : "53df9902-752f-4368-bc00-ab85a6ff664f",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@liggitt , previously the order was, \r\n\r\n- create volume\r\n- create ep/svc \r\n\r\nIn above workflow, we came across a limitation where eventhough the volume creation succeeded, svc creation failed due to quota setting in a customer setup . Considering volume creation and deletion is heavy compared to ep/svc creation this patch got introduced. Now, if the ep/svc creation failed or if volume creation failed , the cleanup is really light ie nothing but cleanup svc. The additional improvements what we can think of here is, can we make use of already created volume if there is a retry (ex:(  if ep update failed).  iic,  provision() from controller is on different go routine till it satisfies the PVC and the request is based on pvc UID. Unless user deleted and recreated the pvc object with same manifest the UID should remain same. But , the caveat here is that, heketi create a volume for each request with a different dynamically generated UID or volume name. To avoid that or to reuse what heketi already created in any of the previous iteration, we should map the volid between PVC UID and gluster/heketi vol ID ( somewhere) , such mapping does not exist. Also, as you mentioned, there are different combinations exist and we have to think through all of these. Till then, I think its better to track each transaction differently. I will also think through the possibilities of improving the workflow based on the scenarios you mentioned above, may be in a future followup design PR. :), does it sound fine ?",
        "createdAt" : "2019-04-22T03:31:54Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "e47a7897ea608c85715cc647b777d0dc276eafbf",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +882,886 @@\t\taddrlist[i].IP = v\n\t}\n\tsubset := make([]v1.EndpointSubset, 1)\n\tports := []v1.EndpointPort{{Port: 1, Protocol: \"TCP\"}}\n"
  },
  {
    "id" : "2bf8ddea-3065-4bfb-8362-2fa220e912c7",
    "prId" : 76178,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76178#pullrequestreview-233084541",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2021202f-c8c1-4732-933d-491b384f543d",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "what if other aspects of a pre-existing service or endpoints object are incorrect (e.g. labels are wrong, service ports are wrong)?",
        "createdAt" : "2019-05-02T13:17:12Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "4a288ff1-0a90-4da6-84d5-443f5c4450e0",
        "parentId" : "2021202f-c8c1-4732-933d-491b384f543d",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Except `name` of ep/svc and `addrlist` field in ep object, all are  irrelevant/dummy. So, we are safe here.",
        "createdAt" : "2019-05-02T15:39:39Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "e47a7897ea608c85715cc647b777d0dc276eafbf",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +887,891 @@\tendpoint.Subsets = subset\n\tendpoint.Subsets[0].Addresses = addrlist\n\tendpoint.Subsets[0].Ports = ports\n\n\t_, err = kubeClient.CoreV1().Endpoints(epNamespace).Update(endpoint)"
  },
  {
    "id" : "cd973423-d3be-411a-adb3-66a40c6dec21",
    "prId" : 76178,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76178#pullrequestreview-237027351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ea16b5a-e2e2-4c58-bdc5-5605732c9653",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "do we also have to delete the endpoints object?",
        "createdAt" : "2019-05-09T17:36:35Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "df09c703-1688-4e0d-a01f-6c54eb7f12b3",
        "parentId" : "7ea16b5a-e2e2-4c58-bdc5-5605732c9653",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "The service deletion should also wipe endpoint associated with it. ",
        "createdAt" : "2019-05-14T05:02:19Z",
        "updatedAt" : "2019-05-14T05:02:19Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "e47a7897ea608c85715cc647b777d0dc276eafbf",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +898,902 @@\t\tklog.V(3).Infof(\"failed to update endpoint, deleting %s\", endpoint)\n\n\t\terr = kubeClient.CoreV1().Services(epNamespace).Delete(epServiceName, nil)\n\n\t\tif err != nil && errors.IsNotFound(err) {"
  },
  {
    "id" : "f35b51a9-de20-4c5a-82d7-17f586c91400",
    "prId" : 76178,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76178#pullrequestreview-237027391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7d15aaf-2754-406e-b4f8-888a53175365",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "avoid logging if the error is a NotFound error",
        "createdAt" : "2019-05-09T17:36:54Z",
        "updatedAt" : "2019-05-14T05:01:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "acea7cbb-a33c-4c97-93aa-e81de30281ca",
        "parentId" : "b7d15aaf-2754-406e-b4f8-888a53175365",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Taken care in the latest patch set.",
        "createdAt" : "2019-05-14T05:02:33Z",
        "updatedAt" : "2019-05-14T05:02:33Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "e47a7897ea608c85715cc647b777d0dc276eafbf",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +904,908 @@\t\t\terr = nil\n\t\t}\n\t\tif err != nil {\n\t\t\tklog.Errorf(\"failed to delete service %s/%s: %v\", epNamespace, epServiceName, err)\n\t\t}"
  },
  {
    "id" : "a947a970-a39e-4a38-84e3-6ba484e04313",
    "prId" : 70286,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70286#pullrequestreview-169006386",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13cc70b2-ff63-4a99-a9d5-6cdc8c4286a5",
        "parentId" : null,
        "authorId" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "body" : "This is correct but actually not related to the topic of this patch.\r\nSo that comment fix might be a separate patch...",
        "createdAt" : "2018-10-26T14:00:19Z",
        "updatedAt" : "2018-10-26T14:32:04Z",
        "lastEditedBy" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "tags" : [
        ]
      },
      {
        "id" : "3722c88b-0fe3-4b6a-89c3-09d1d0ae3ff4",
        "parentId" : "13cc70b2-ff63-4a99-a9d5-6cdc8c4286a5",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Put it as  a seperate commit.",
        "createdAt" : "2018-10-26T14:32:32Z",
        "updatedAt" : "2018-10-26T14:32:33Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "2f568605-7c48-415a-aa83-d286eb1148b8",
        "parentId" : "13cc70b2-ff63-4a99-a9d5-6cdc8c4286a5",
        "authorId" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "body" : "Thanks @humblec . I noticed there is another error in the line: `it create` --> `it creates`. Maybe we just skip this hunk from the PR and consider the separate PR #70306 for comment cleanup?",
        "createdAt" : "2018-10-26T21:28:41Z",
        "updatedAt" : "2018-10-26T21:29:58Z",
        "lastEditedBy" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6759a7b22c8577f72b64ea0a3797ae15e552633",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +825,829 @@\t// The 'endpointname' is created in form of 'glusterfs-dynamic-<PVC UID>'.\n\t// createEndpointService() checks for this 'endpoint' existence in PVC's namespace and\n\t// if not found, it create an endpoint and service using the IPs we dynamically picked at time\n\t// of volume creation.\n\tepServiceName := dynamicEpSvcPrefix + string(p.options.PVC.UID)"
  },
  {
    "id" : "3c04f9c3-2def-46e8-bee7-445e52966203",
    "prId" : 66675,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66675#pullrequestreview-142879319",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "524f743d-2ce2-4348-b507-a510a4f99fa9",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "What if someone wants also custom log level, e.g. to debug something?",
        "createdAt" : "2018-07-30T13:34:48Z",
        "updatedAt" : "2018-08-02T16:40:41Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "a41e9fc0-4ac8-4d6f-a262-1b9e907a90d6",
        "parentId" : "524f743d-2ce2-4348-b507-a510a4f99fa9",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "true, possible and I was thinking to address it in another PR, Is it fine ? or do I need to address `log-level` as well in this PR ?",
        "createdAt" : "2018-07-30T15:02:25Z",
        "updatedAt" : "2018-08-02T16:40:41Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "89581dad-0477-4ec9-a4b9-c5be84639446",
        "parentId" : "524f743d-2ce2-4348-b507-a510a4f99fa9",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "The main reason for  a validation of `=` or `empty` was to fall back to default or existing way of creating log file. I dont know, failing if log-file option not provided properly or falling back to default log-file is preferred. I was thinking latter is better, but look like you prefer to fail if option is not provided properly. Please confirm.",
        "createdAt" : "2018-07-30T15:59:59Z",
        "updatedAt" : "2018-08-02T16:40:41Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "08c27c53-f21b-4817-b04a-6bf131eeaef1",
        "parentId" : "524f743d-2ce2-4348-b507-a510a4f99fa9",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "> I was thinking to address it in another PR, Is it fine ?\r\n\r\nYes, it's fine.",
        "createdAt" : "2018-08-01T08:15:10Z",
        "updatedAt" : "2018-08-02T16:40:41Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "7332e5cd-6905-4b59-b590-24bc78ee477a",
        "parentId" : "524f743d-2ce2-4348-b507-a510a4f99fa9",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Thanks!",
        "createdAt" : "2018-08-02T16:41:31Z",
        "updatedAt" : "2018-08-02T16:41:37Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e29ab985b70ca76cddcdabfadfba70d392d52c8",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +324,328 @@\t// Use derived/provided log file in gluster fuse mount\n\toptions = append(options, \"log-file=\"+log)\n\toptions = append(options, \"log-level=ERROR\")\n\n\tvar addrlist []string"
  },
  {
    "id" : "b57d0d5a-b7fc-4496-93f3-170dfb947985",
    "prId" : 60195,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60195#pullrequestreview-171589393",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b4175c4-1674-42fd-b7fd-306a2946a5ae",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "do you plan to adjust the provisioner to allow specifying the endpoint namespace as an option, so it doesn't have to be exposed to the PVC user?",
        "createdAt" : "2018-08-08T01:59:39Z",
        "updatedAt" : "2018-11-06T10:23:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "055fbcb2-2ffb-4838-b841-451cff0b87c4",
        "parentId" : "3b4175c4-1674-42fd-b7fd-306a2946a5ae",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "As of now, getting the option from storage class is not planned, I can avoid exposing it to pvc user while bringing the change. Is it fine?",
        "createdAt" : "2018-10-25T15:48:00Z",
        "updatedAt" : "2018-11-06T10:23:29Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "c5d353c9-28cc-480a-b8af-dc6a459bd8fd",
        "parentId" : "3b4175c4-1674-42fd-b7fd-306a2946a5ae",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "follow up is fine, though it would be ideal to give the option in the same release the PV capability is added to avoid confusion about availability",
        "createdAt" : "2018-10-26T16:32:19Z",
        "updatedAt" : "2018-11-06T10:23:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "2a855adf-648b-4271-9228-2d5b10273fd1",
        "parentId" : "3b4175c4-1674-42fd-b7fd-306a2946a5ae",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "@humblec external provisioner too?",
        "createdAt" : "2018-11-05T14:50:33Z",
        "updatedAt" : "2018-11-06T10:23:29Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      }
    ],
    "commit" : "bdb051c72d836af2ab477dd23c4ff3fa06bbc1f4",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +881,885 @@\treturn &v1.GlusterfsPersistentVolumeSource{\n\t\tEndpointsName:      endpoint.Name,\n\t\tEndpointsNamespace: &epNamespace,\n\t\tPath:               volume.Name,\n\t\tReadOnly:           false,"
  },
  {
    "id" : "b40cb580-e9a1-45da-8776-7b5676b8ed7b",
    "prId" : 60195,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60195#pullrequestreview-169319599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05f6f1d0-9f82-4227-b4c9-37ed8b725cec",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "doc what the return values are (path and readOnly) as well",
        "createdAt" : "2018-10-29T13:21:39Z",
        "updatedAt" : "2018-11-06T10:23:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "fa4cc623-e21e-4032-a317-8b2572dd5be2",
        "parentId" : "05f6f1d0-9f82-4227-b4c9-37ed8b725cec",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "sure, done.",
        "createdAt" : "2018-10-29T14:11:32Z",
        "updatedAt" : "2018-11-06T10:23:29Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "bdb051c72d836af2ab477dd23c4ff3fa06bbc1f4",
    "line" : 115,
    "diffHunk" : "@@ -1,1 +448,452 @@\n//getVolumeInfo returns 'path' and 'readonly' field values from the provided glusterfs spec.\nfunc getVolumeInfo(spec *volume.Spec) (string, bool, error) {\n\tif spec.Volume != nil && spec.Volume.Glusterfs != nil {\n\t\treturn spec.Volume.Glusterfs.Path, spec.Volume.Glusterfs.ReadOnly, nil"
  },
  {
    "id" : "9231bf00-0d25-4658-bad4-e3b7e87e1fbd",
    "prId" : 58626,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58626#pullrequestreview-90606815",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8aa2131b-dc8b-46d9-bfce-0b7daf7213c7",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "So, I would assume this never worked or was the annotation changed some point in time?",
        "createdAt" : "2018-01-22T19:43:35Z",
        "updatedAt" : "2018-01-22T19:43:35Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "31442791-eef9-48f8-abdd-355d88348661",
        "parentId" : "8aa2131b-dc8b-46d9-bfce-0b7daf7213c7",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@gnufied annotation changed just before the last patch which I corrected now.",
        "createdAt" : "2018-01-22T19:46:51Z",
        "updatedAt" : "2018-01-22T19:46:51Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "4590962c-8ff9-4128-8137-d3b8b0306238",
        "parentId" : "8aa2131b-dc8b-46d9-bfce-0b7daf7213c7",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Can you link the patch where annotation was changed? I am wondering about backward compatibility.",
        "createdAt" : "2018-01-22T19:50:54Z",
        "updatedAt" : "2018-01-22T19:50:54Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "d0d9e193-6490-4647-856c-698244d785e2",
        "parentId" : "8aa2131b-dc8b-46d9-bfce-0b7daf7213c7",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "https://github.com/kubernetes/kubernetes/pull/56315 this patch brought that annotation, till then I was using `volid` as the annotation. Reg#backward compatibility I have tested and verified it works. @gnufied thanks!",
        "createdAt" : "2018-01-22T20:00:36Z",
        "updatedAt" : "2018-01-22T20:00:37Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e6b5acbb5282bca5103d16dd3c78ae3ca383c9e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1086,1090 @@\t// Get volID from pvspec if available, else fill it from volumename.\n\tif pv != nil {\n\t\tif pv.Annotations[heketiVolIDAnn] != \"\" {\n\t\t\tvolumeID = pv.Annotations[heketiVolIDAnn]\n\t\t} else {"
  },
  {
    "id" : "57cfac0a-38ae-4c68-8774-214ac75aa141",
    "prId" : 56823,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56823#pullrequestreview-83720444",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8ee17fb-0626-461e-ab6a-acfe186ee7a8",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "What if there is no prefix? Should it fail?",
        "createdAt" : "2017-12-13T16:49:22Z",
        "updatedAt" : "2018-01-10T13:01:28Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "59b2ff89-1982-4fda-8f36-613d0b710d4f",
        "parentId" : "f8ee17fb-0626-461e-ab6a-acfe186ee7a8",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Volumes were always created with prefix. If its changed with custom volume name it should have an entry in pv spec. What I am trying to say here is  it should fall into any of these conditions.",
        "createdAt" : "2017-12-14T13:21:08Z",
        "updatedAt" : "2018-01-10T13:01:28Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "6f0a16d9-d038-4ad0-ae4d-2b5ae31d4f3f",
        "parentId" : "f8ee17fb-0626-461e-ab6a-acfe186ee7a8",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "How will the error look like when someone modifies the spec not to include `vol_` or remove annotations? `TrimPrefix` won't do anything, it goes to heketi, heketi says this volume does not exist. Is the error message clear enough for the user to spot the error?",
        "createdAt" : "2017-12-14T13:35:14Z",
        "updatedAt" : "2018-01-10T13:01:28Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "f1105d39-d134-418e-8052-8db839639f7b",
        "parentId" : "f8ee17fb-0626-461e-ab6a-acfe186ee7a8",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Yes, in that case heketi returns proper error which states the volume does not exist. ",
        "createdAt" : "2017-12-15T05:40:42Z",
        "updatedAt" : "2018-01-10T13:01:28Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc6443ce2c3a2b36b9f7dbf2388055751b612fdd",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1063,1067 @@\t\t\tvolumeID = pv.Annotations[\"VolID\"]\n\t\t} else {\n\t\t\tvolumeID = dstrings.TrimPrefix(volumeName, volPrefix)\n\t\t}\n\t} else {"
  },
  {
    "id" : "86250580-7ad7-4804-b17f-c5dbe6cc9844",
    "prId" : 56581,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56581#pullrequestreview-81062201",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2e6b0b3-64af-4bed-9e6d-7ffe472b09e5",
        "parentId" : null,
        "authorId" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "body" : "Oh, one thing that just struck me: Since you introduce `RoundUpToGiB()`, why don't you use it?",
        "createdAt" : "2017-11-30T14:57:47Z",
        "updatedAt" : "2017-11-30T14:58:02Z",
        "lastEditedBy" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "tags" : [
        ]
      },
      {
        "id" : "a86f0976-25c4-484f-90d6-f5f30790d812",
        "parentId" : "a2e6b0b3-64af-4bed-9e6d-7ffe472b09e5",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Because, we have to calculate difference and then use `RoundUpToGib` and `RoundUpToGiB` takes `resource.Quantity` as arguments..",
        "createdAt" : "2017-12-05T03:34:45Z",
        "updatedAt" : "2017-12-05T03:34:45Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "fcfca65a54e53f5e49693c2c599e63a0dab8661e",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +1077,1081 @@\t// Find out delta size\n\texpansionSize := (newSize.Value() - oldSize.Value())\n\texpansionSizeGiB := int(volume.RoundUpSize(expansionSize, volume.GIB))\n\n\t// Make volume expansion request"
  },
  {
    "id" : "74fd1af8-6167-402b-ab5e-7906857c3c37",
    "prId" : 49727,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49727#pullrequestreview-59332971",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b54df2c2-b7f2-4b80-8109-d182c570f805",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Is fetching storageClass here required? It looks like it is only used for logging. In that case, if you fail to fetch SC, should that maybe be a warning instead of a terminating error?",
        "createdAt" : "2017-08-28T17:43:42Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "4bf932fe-aecd-425b-9cf7-b97a4fa76ee6",
        "parentId" : "b54df2c2-b7f2-4b80-8109-d182c570f805",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "It is also used for extracting heketi url etc from storageClass.",
        "createdAt" : "2017-08-28T19:49:12Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "c8ef457e-7d62-431e-8aea-67b028a9478b",
        "parentId" : "b54df2c2-b7f2-4b80-8109-d182c570f805",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "its required @saad-ali. We need the REST url from SC of the PV at time of expansion. ",
        "createdAt" : "2017-08-29T18:13:01Z",
        "updatedAt" : "2017-09-04T07:08:19Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "84029c2c1a798bab26781c0219186b278ad69af7",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1060,1064 @@\tvolumeID := dstrings.TrimPrefix(volumeName, volPrefix)\n\n\t//Get details of SC.\n\tclass, err := volutil.GetClassForVolume(plugin.host.GetKubeClient(), spec.PersistentVolume)\n\tif err != nil {"
  },
  {
    "id" : "c9f46ff9-1dcd-473a-8cd4-b48c62f59be6",
    "prId" : 45995,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45995#pullrequestreview-39120659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b53dc34-c9d5-4d83-a66f-5edd71f069ee",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "There already is `gciGlusterMountBinariesPath = \"/sbin/mount.glusterfs\"` in glusterfs.go",
        "createdAt" : "2017-05-18T12:10:50Z",
        "updatedAt" : "2017-05-22T18:21:22Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "da8d09d1-18e4-4615-ab11-ddebc041e30b",
        "parentId" : "3b53dc34-c9d5-4d83-a66f-5edd71f069ee",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@jsafrane Yes, there is one and while following the PR which introduced this constant, I got an impression that the `path` mentioned with `gciGlusterMountBinary` is very much specific to GCI.  so, if I depend on that path, it may cause problems in future.  I am also trying to put that path as `gci-linux` specific via PR https://github.com/kubernetes/kubernetes/pull/45938 .",
        "createdAt" : "2017-05-18T13:36:19Z",
        "updatedAt" : "2017-05-22T18:21:22Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "71c6f202-aab7-4344-8181-3d91ada105df",
        "parentId" : "3b53dc34-c9d5-4d83-a66f-5edd71f069ee",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "for the same change, please either make this change into #45938 or close that one",
        "createdAt" : "2017-05-18T17:29:08Z",
        "updatedAt" : "2017-05-22T18:21:22Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "0fd66b70-2fe6-4165-b736-a9dc04370d46",
        "parentId" : "3b53dc34-c9d5-4d83-a66f-5edd71f069ee",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "I will close that and get the changes here.",
        "createdAt" : "2017-05-19T06:57:10Z",
        "updatedAt" : "2017-05-22T18:21:22Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "04bf95a5d1b2251eba5df159a6b222fd99333c7a",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +84,88 @@\tglusterTypeAnn          = \"gluster.org/type\"\n\tglusterDescAnn          = \"Gluster: Dynamically provisioned PV\"\n\tlinuxGlusterMountBinary = \"mount.glusterfs\"\n\tautoUnmountBinaryVer    = \"3.11\"\n)"
  },
  {
    "id" : "cbeaae7b-a10c-4ab2-926d-a8d2ea50ad5b",
    "prId" : 45995,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45995#pullrequestreview-41607446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92ca6ab5-e0f8-4fb3-b04e-6e7d97e06f30",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "why does provisioner set this option for every pv?",
        "createdAt" : "2017-05-22T14:32:11Z",
        "updatedAt" : "2017-05-22T18:21:22Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "e3a6a81d-8775-45de-bcb4-07cc9b40fd53",
        "parentId" : "92ca6ab5-e0f8-4fb3-b04e-6e7d97e06f30",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "This option is introduced as a fix in gluster fuse bridge, so we would like to enable it for every upcoming version, so the annotation set in the pv and filtering out clients < 3.11 to make sure gluster mount works for existing clients till they upgrade. ",
        "createdAt" : "2017-05-22T16:15:32Z",
        "updatedAt" : "2017-05-22T18:21:22Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "ea70f6ad-3930-4b70-8db6-8c86e98bd333",
        "parentId" : "92ca6ab5-e0f8-4fb3-b04e-6e7d97e06f30",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "thanks for the info.\r\n\r\n@jsafrane @gnufied any comment on setting a default mount option annotation? The context of this option is to address https://bugzilla.redhat.com/show_bug.cgi?id=1424680",
        "createdAt" : "2017-05-22T16:28:10Z",
        "updatedAt" : "2017-05-22T18:21:22Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "f32f3bc3-d128-4a58-81bb-321dba3b8879",
        "parentId" : "92ca6ab5-e0f8-4fb3-b04e-6e7d97e06f30",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Hmm, so what happens if we add support for specifying mount-options in storageclasses and then this value gets overridden? \r\n\r\nI think, we should have added this at mount time rather than as a annotation here.\r\n",
        "createdAt" : "2017-06-01T19:04:33Z",
        "updatedAt" : "2017-06-01T19:04:33Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "04bf95a5d1b2251eba5df159a6b222fd99333c7a",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +867,871 @@\t\tglusterTypeAnn:                      \"file\",\n\t\t\"Description\":                       glusterDescAnn,\n\t\tv1.MountOptionAnnotation:            \"auto_unmount\",\n\t}\n"
  },
  {
    "id" : "46c5acba-094c-425a-ae27-6e2abf88c9a1",
    "prId" : 45528,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45528#pullrequestreview-38293863",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c38d355-3a81-4296-8878-07a7bc119801",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "a bit more paranoid but validate that `epName` is created using the same schema that provision uses.",
        "createdAt" : "2017-05-15T19:12:46Z",
        "updatedAt" : "2017-05-16T14:22:46Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "4bf810e6-c2bd-4e0e-b08b-400c93b4adaf",
        "parentId" : "0c38d355-3a81-4296-8878-07a7bc119801",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Done.",
        "createdAt" : "2017-05-16T05:26:39Z",
        "updatedAt" : "2017-05-16T14:22:46Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "936c81ddfbd5370fe15e9a5bc7cfc934dcd348fe",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +147,151 @@\n\tep, err := kubeClient.Core().Endpoints(ns).Get(epName, metav1.GetOptions{})\n\tif err != nil && errors.IsNotFound(err) {\n\t\tclaim := spec.PersistentVolume.Spec.ClaimRef.Name\n\t\tcheckEpName := dynamicEpSvcPrefix + claim"
  },
  {
    "id" : "221a15ba-413e-4acb-bbde-d90f563f0e4c",
    "prId" : 45528,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45528#pullrequestreview-44579360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf272d45-5d48-4a9a-9dad-8926502706db",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "nodes cannot look up arbitrary secrets based on values in storage class parameters...",
        "createdAt" : "2017-06-16T14:29:41Z",
        "updatedAt" : "2017-06-16T14:29:41Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "f158c0e4-c04d-41a9-89b9-e1f94e1b1ea9",
        "parentId" : "cf272d45-5d48-4a9a-9dad-8926502706db",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "I also thought only the create/delete, attach/detach phases were supposed to resolve storage class info... isn't mount only supposed to use info in the PV?",
        "createdAt" : "2017-06-16T14:44:57Z",
        "updatedAt" : "2017-06-16T14:44:57Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "936c81ddfbd5370fe15e9a5bc7cfc934dcd348fe",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +155,159 @@\t\tglog.Errorf(\"glusterfs: failed to get endpoint %s[%v]\", epName, err)\n\t\tif spec != nil && spec.PersistentVolume.Annotations[\"kubernetes.io/createdby\"] == heketiAnn {\n\t\t\tclass, err := volutil.GetClassForVolume(plugin.host.GetKubeClient(), spec.PersistentVolume)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"glusterfs: failed to get storageclass, error: %v\", err)"
  },
  {
    "id" : "47841e36-49a9-456b-a740-b225e31d4236",
    "prId" : 45528,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45528#pullrequestreview-44575316",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab8a75f8-0fe8-4dcd-9197-20b775618d4e",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "nodes do not have permission to create endpoints or services... this will fail on any cluster running RBAC",
        "createdAt" : "2017-06-16T14:30:28Z",
        "updatedAt" : "2017-06-16T14:30:28Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "936c81ddfbd5370fe15e9a5bc7cfc934dcd348fe",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +184,188 @@\t\t\t// Give an attempt to recreate endpoint/service.\n\n\t\t\t_, _, err = plugin.createEndpointService(ns, epName, endpointIPs, claim)\n\n\t\t\tif err != nil && !errors.IsAlreadyExists(err) {"
  },
  {
    "id" : "265f5e2c-5c1c-4ed3-83d6-9f4d4ec5a7e6",
    "prId" : 44174,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/44174#pullrequestreview-41153057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c3bf54f-5568-4205-b22c-c165b00ca375",
        "parentId" : null,
        "authorId" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "body" : "Thinking ahead to when we (likely) move in-tree plugins out-of-tree, why should kubernetes be parsing these options? Would it be better to pass this []string as is to the provider?",
        "createdAt" : "2017-05-18T16:29:55Z",
        "updatedAt" : "2017-07-18T14:16:08Z",
        "lastEditedBy" : "65c676d6-aec8-4761-943f-80e1f66d400b",
        "tags" : [
        ]
      },
      {
        "id" : "5d4933d5-d0c5-4308-8c4f-2a4b54e636c4",
        "parentId" : "3c3bf54f-5568-4205-b22c-c165b00ca375",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "We are not doing parsing any more. The parsing logic was introduced when we want to do some kind of wrapping in the plugin. By wrapping I mean,  user only know about the string 'encryption', but internally we will have 'cleint.ssl on` , `server.ssl on` options tied to this string \"encryption\". I deviated from this path and we have agreed to move with direct passthrough of options. Thanks.",
        "createdAt" : "2017-05-31T07:37:56Z",
        "updatedAt" : "2017-07-18T14:16:08Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b432854f1706d28f93d754adad4852365bfec7db",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1045,1049 @@\t}\n\n\tif len(parseVolumeOptions) != 0 {\n\t\tvolOptions := dstrings.Split(parseVolumeOptions, \",\")\n\t\tif len(volOptions) == 0 {"
  },
  {
    "id" : "b2f50425-5a2e-421d-835a-c028fad45bcc",
    "prId" : 44174,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/44174#pullrequestreview-49965193",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cb19bde-520b-453b-9335-1ddac3b08c62",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "please validate the options",
        "createdAt" : "2017-07-12T17:01:22Z",
        "updatedAt" : "2017-07-18T14:16:08Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "b3a17f90-0452-4766-85f7-a6fc1d7f5439",
        "parentId" : "0cb19bde-520b-453b-9335-1ddac3b08c62",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@rootfs we are not validating any options from provisioner or in heketi. It has been passed to glusterd and validation will only happen on volume creation, because there are atleast 256 volume options available to set on gluster volume.  Validation in any of the above layers will be difficult for the same reason. ",
        "createdAt" : "2017-07-13T08:15:15Z",
        "updatedAt" : "2017-07-18T14:16:08Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "6638758b-b2df-4d21-9d5b-d0fec015978d",
        "parentId" : "0cb19bde-520b-453b-9335-1ddac3b08c62",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@rootfs I have added the link to the GUIDE. Thanks.",
        "createdAt" : "2017-07-13T09:54:27Z",
        "updatedAt" : "2017-07-18T14:16:08Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "dbc1490c-92ff-4240-854c-079a26cc704f",
        "parentId" : "0cb19bde-520b-453b-9335-1ddac3b08c62",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "This is unfortunate. Could there be a case a wrong option cause a service disruption? Volumes are created and space allocated but unusable due to unsupported options. If this happens, we need options printed in messages and logs.",
        "createdAt" : "2017-07-13T13:58:38Z",
        "updatedAt" : "2017-07-18T14:16:08Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "476ec105-56b0-4ec1-b908-b8ab45976a1b",
        "parentId" : "0cb19bde-520b-453b-9335-1ddac3b08c62",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@rootfs no. If the option specified is wrong, the volume creation will fail and it will be logged as like any other volume creation failure from glusterd, due to the fact that volume options are tried to set as part of volume creation code in heketi.",
        "createdAt" : "2017-07-14T05:12:22Z",
        "updatedAt" : "2017-07-18T14:16:08Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b432854f1706d28f93d754adad4852365bfec7db",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +971,975 @@\t\tcase \"volumeoptions\":\n\t\t\tif len(v) != 0 {\n\t\t\t\tparseVolumeOptions = v\n\t\t\t}\n"
  },
  {
    "id" : "b2bb8825-f5c5-478f-8aa7-a3b672537081",
    "prId" : 41628,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41628#pullrequestreview-23277623",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21084fd3-9125-46ef-98d1-388be1ec93b6",
        "parentId" : null,
        "authorId" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "body" : "empty line after the funtion is missing.",
        "createdAt" : "2017-02-22T12:29:31Z",
        "updatedAt" : "2017-02-22T17:16:06Z",
        "lastEditedBy" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "tags" : [
        ]
      },
      {
        "id" : "4c732b19-9644-43d3-80a4-15f99f9e08c3",
        "parentId" : "21084fd3-9125-46ef-98d1-388be1ec93b6",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "Done.",
        "createdAt" : "2017-02-22T17:17:25Z",
        "updatedAt" : "2017-02-22T17:17:25Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ade29ff7314b4b70190414e28862f3b9e2053d1",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +716,720 @@\t}\n\treturn dynamicHostIps, nil\n}\n\nfunc (p *glusterfsVolumeProvisioner) CreateVolume(gid int) (r *v1.GlusterfsVolumeSource, size int, err error) {"
  },
  {
    "id" : "0e98b806-7e05-46f3-9122-e792413e44cf",
    "prId" : 40726,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40726#pullrequestreview-19374496",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2048fcfd-7e4b-4407-881d-d37ce84f7778",
        "parentId" : null,
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Sorry, but could you please clarify why need this modification?",
        "createdAt" : "2017-01-31T14:05:42Z",
        "updatedAt" : "2017-01-31T17:35:18Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "3e6d6857-0401-4f08-90b9-813829d79a2e",
        "parentId" : "2048fcfd-7e4b-4407-881d-d37ce84f7778",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@resouer just as a better name for the struct. ",
        "createdAt" : "2017-01-31T17:32:05Z",
        "updatedAt" : "2017-01-31T17:35:18Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c7c2dcd208981d38e4618204ba509191f6b17e7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +369,373 @@}\n\ntype provisionerConfig struct {\n\turl             string\n\tuser            string"
  },
  {
    "id" : "e4a59345-19b0-41e0-8d2a-76350072f315",
    "prId" : 37886,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/37886#pullrequestreview-11269682",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20c44920-1844-489c-95ba-7063bb53911d",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "make convertGid return an error that doesn't need to be wrapped, then just `return convertGid(gidStr)`",
        "createdAt" : "2016-12-02T14:33:12Z",
        "updatedAt" : "2016-12-03T04:27:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "5043b239-233d-4618-9e09-816b4b6f0079",
        "parentId" : "20c44920-1844-489c-95ba-7063bb53911d",
        "authorId" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "body" : "sorry, not entirely sure what it means \"wrapping the error\".",
        "createdAt" : "2016-12-02T14:50:49Z",
        "updatedAt" : "2016-12-03T04:27:29Z",
        "lastEditedBy" : "5d45f6c8-157c-4f55-a9f4-31d702648fd2",
        "tags" : [
        ]
      },
      {
        "id" : "1cf3a956-7898-4067-ba1d-d573c488d29d",
        "parentId" : "20c44920-1844-489c-95ba-7063bb53911d",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "if the annotation is malformed, the code currently logs about three times\r\n\r\nconvertGid(), line 408:\r\nglog.Errorf(\"glusterfs: failed to parse gid %v \", inputGid)\r\n\r\ngetGid(), line 555:\r\nglog.Errorf(\"glusterfs: failed to parse gid %v\", gid)\r\n\r\nDelete(), line 582:\r\nglog.Errorf(\"glusterfs: failed to parse the gid\")\r\n\r\ninstead, convertGid() should return an error like this, without logging:\r\n```\r\nreturn 0, fmt.Errorf(\"glusterfs: failed to parse gid %v: %v\", inputGid, err)\r\n```\r\n\r\ngetGid() can then just do this, also without logging:\r\n```\r\ngid, err := convertGid(gidStr)\r\nreturn gid, true, err\r\n```\r\n\r\nand Delete() can log the result without wrapping the error:\r\n```\r\nif gid, exists, err := d.getGid(); err != nil {\r\n  // being unable to get the gid from the PV should not stop deletion of it\r\n  glog.Error(err)\r\n} else if exists {\r\n  gidTable, err := d.plugin.getGidTable(class.Name, cfg.gidMin, cfg.gidMax)\r\n  if err != nil {\r\n    return err\r\n  }\r\n  if err := gidTable.Release(int(gid)); if err != nil {\r\n    return err\r\n  }\r\n}\r\n```",
        "createdAt" : "2016-12-02T18:23:48Z",
        "updatedAt" : "2016-12-03T04:27:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "347c8d7a-13cf-4e4d-b775-9201b653ed96",
        "parentId" : "20c44920-1844-489c-95ba-7063bb53911d",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "still outstanding",
        "createdAt" : "2016-12-03T02:37:32Z",
        "updatedAt" : "2016-12-03T04:27:29Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "06ad835e48c635decbfa375b20fe56c940c2296e",
    "line" : null,
    "diffHunk" : "@@ -1,1 +552,556 @@\t}\n\n\tgid, err := convertGid(gidStr)\n\n\treturn gid, true, err"
  },
  {
    "id" : "ff92e9a5-5cc2-40ed-94f5-675d300b8576",
    "prId" : 35460,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35460#pullrequestreview-8442791",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a52b5d0-87ed-4100-8ee1-b72b43086e00",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "will all callers be happy with a randomized gid? is this a breaking change?\nalso, is it normal to seed a new rand source per-use?\n",
        "createdAt" : "2016-11-14T16:02:07Z",
        "updatedAt" : "2016-11-16T14:22:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "ed5912b7-69cd-47f2-a565-74e6b36d0d97",
        "parentId" : "3a52b5d0-87ed-4100-8ee1-b72b43086e00",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@liggitt as discussed here https://github.com/kubernetes/kubernetes/pull/35460#issuecomment-257071419 , a random GID should satisfy the requirement and iic, the new rand source looks to be a common practice we follow in k8s code base. \n",
        "createdAt" : "2016-11-14T17:20:16Z",
        "updatedAt" : "2016-11-16T14:22:02Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a0d219d12d7754ad41d2b4c6559749207005600",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +489,493 @@\tvar err error\n\tvar reqGid int64\n\tgidRandomizer := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tif r.options.PVC.Spec.Selector != nil {\n\t\tglog.V(4).Infof(\"glusterfs: not able to parse your claim Selector\")"
  },
  {
    "id" : "67a76d82-719b-4deb-a653-1034d28ccdd6",
    "prId" : 35460,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35460#pullrequestreview-8445274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "826086cf-bd62-4620-83cf-a47b90a6eeb0",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "why not make gid part of the options spec\n",
        "createdAt" : "2016-11-14T16:03:21Z",
        "updatedAt" : "2016-11-16T14:22:02Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "68cfc66a-bb52-4275-b8f5-dabfc3a12ee8",
        "parentId" : "826086cf-bd62-4620-83cf-a47b90a6eeb0",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "If u are referring `VolumeOptions->parameters`, afaict, it get  filled from SC. The reqGid/Gid is not taken/filled in as SC parameter.  \n",
        "createdAt" : "2016-11-14T17:32:47Z",
        "updatedAt" : "2016-11-16T14:22:02Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a0d219d12d7754ad41d2b4c6559749207005600",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +523,527 @@}\n\nfunc (p *glusterfsVolumeProvisioner) CreateVolume(reqGid int64) (r *api.GlusterfsVolumeSource, size int, err error) {\n\tcapacity := p.options.PVC.Spec.Resources.Requests[api.ResourceName(api.ResourceStorage)]\n\tvolSizeBytes := capacity.Value()"
  },
  {
    "id" : "27d77059-50aa-4017-9705-ea6a9c18e390",
    "prId" : 35285,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35285#pullrequestreview-5281369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7434dc6d-59dc-4299-bae9-23a834b16f78",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "why volumeDelete will get stale endpoint?\n",
        "createdAt" : "2016-10-21T15:38:46Z",
        "updatedAt" : "2016-10-21T15:38:46Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "41f4c329-736f-48c9-90c7-4612006991f4",
        "parentId" : "7434dc6d-59dc-4299-bae9-23a834b16f78",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@rootfs  no.. its other way around, on volume creation code path, if it created volume and then while trying to create an endpoint if it fails, previously created volume will exist in the backend. When next provisioning request comes it will create one more volume .. so on.  This patch delete the volume it created if there is a failure in endpoint/svc creation.\n",
        "createdAt" : "2016-10-21T15:41:04Z",
        "updatedAt" : "2016-10-21T15:41:04Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "90263476d5c5cb06b8d7d07d545e54771bdd6a53",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +553,557 @@\t\terr = cli.VolumeDelete(volume.Id)\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"glusterfs: error when deleting the volume :%v , manual deletion required\", err)\n\t\t}\n\t\treturn nil, 0, fmt.Errorf(\"failed to create endpoint/service %v\", err)"
  },
  {
    "id" : "c7255796-7385-482f-9d46-cc0320a7bd27",
    "prId" : 35022,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35022#pullrequestreview-4827241",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b72f41cf-057b-4405-90c8-9e0fe1c8dc2b",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Can we add a unit test for this bit of code? \n",
        "createdAt" : "2016-10-18T15:56:50Z",
        "updatedAt" : "2016-10-19T08:53:21Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "a45ebc00-67d0-47d8-be6a-fd67a008252f",
        "parentId" : "b72f41cf-057b-4405-90c8-9e0fe1c8dc2b",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "testcase added\n",
        "createdAt" : "2016-10-19T08:53:51Z",
        "updatedAt" : "2016-10-19T08:53:51Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b2508ba15649884555c2fcb5f9fd2c741938971",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +516,520 @@// parseClassParameters parses StorageClass.Parameters\nfunc parseClassParameters(params map[string]string, kubeClient clientset.Interface) (*provisioningConfig, error) {\n\tvar cfg provisioningConfig\n\tvar err error\n"
  },
  {
    "id" : "1b13309c-b50e-43d7-8e7d-eb607ffceef1",
    "prId" : 34705,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34705#pullrequestreview-4215501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74703349-3a8d-42c0-914f-4654ddcf36f5",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "don't reset err, create a new one for service. \nlog a message is endpoint exists\n",
        "createdAt" : "2016-10-13T19:17:27Z",
        "updatedAt" : "2016-10-19T19:02:43Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "f0d18229-a837-4120-b619-e9e19306946f",
        "parentId" : "74703349-3a8d-42c0-914f-4654ddcf36f5",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "I have logged the message. The error has been reset for catching all the errors except `IsAlreadyExists`.\n",
        "createdAt" : "2016-10-14T06:53:46Z",
        "updatedAt" : "2016-10-19T19:02:44Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d080f986d8cd5b257cdf1afe1fb5c1100883d17",
    "line" : null,
    "diffHunk" : "@@ -1,1 +583,587 @@\tif err != nil && errors.IsAlreadyExists(err) {\n\t\tglog.V(1).Infof(\"glusterfs: endpoint [%s] already exist in namespace [%s]\", endpoint, namespace)\n\t\terr = nil\n\t}\n\tif err != nil {"
  },
  {
    "id" : "4cc5f949-3005-404d-8a38-b4b5666eb940",
    "prId" : 34705,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34705#pullrequestreview-4215219",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d18babe-eeb7-427d-9204-e40e22f66c97",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "i don't see Name in api.Endpoints\n",
        "createdAt" : "2016-10-13T19:30:27Z",
        "updatedAt" : "2016-10-19T19:02:43Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "2ca35e94-83df-45e0-afd1-bd5f097a0e5d",
        "parentId" : "0d18babe-eeb7-427d-9204-e40e22f66c97",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@rootfs  it resolves from ObjectMeta.  For ex:https://github.com/kubernetes/kubernetes/pull/34705/commits/61917ffda884a33b46e1a0e61e8bd9936bacb5e6#diff-e97253dd603331ffca81131a4b67264fR626\n",
        "createdAt" : "2016-10-14T06:51:44Z",
        "updatedAt" : "2016-10-19T19:02:43Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d080f986d8cd5b257cdf1afe1fb5c1100883d17",
    "line" : null,
    "diffHunk" : "@@ -1,1 +555,559 @@\tglog.V(3).Infof(\"glusterfs: dynamic ep %v and svc : %v \", endpoint, service)\n\treturn &api.GlusterfsVolumeSource{\n\t\tEndpointsName: endpoint.Name,\n\t\tPath:          volume.Name,\n\t\tReadOnly:      false,"
  },
  {
    "id" : "ed4ec08c-ff74-4983-9eba-356504e53a5c",
    "prId" : 34705,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34705#pullrequestreview-4260188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa4c1473-6055-4341-b119-02c9c50a24bc",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "you do not need extra `else` branch after `return` above, you ca save yourself one indentation level.\n",
        "createdAt" : "2016-10-14T12:42:46Z",
        "updatedAt" : "2016-10-19T19:02:44Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d080f986d8cd5b257cdf1afe1fb5c1100883d17",
    "line" : null,
    "diffHunk" : "@@ -1,1 +302,306 @@\tif b.hosts == nil {\n\t\treturn fmt.Errorf(\"glusterfs: endpoint is nil\")\n\t} else {\n\t\taddr := make(map[string]struct{})\n\t\tif b.hosts.Subsets != nil {"
  },
  {
    "id" : "9fa1dd67-80fc-4502-8879-4155c7e8e2f7",
    "prId" : 34705,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34705#pullrequestreview-4260188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9ae95b2-9a66-4eb7-a5d9-0e9d0b333729",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "please check if pvSpec.ClaimRef is `nil` (and throw an error if it is) and throw an error when `ClaimRef.Namespace` is empty\n",
        "createdAt" : "2016-10-14T12:45:43Z",
        "updatedAt" : "2016-10-19T19:02:44Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d080f986d8cd5b257cdf1afe1fb5c1100883d17",
    "line" : null,
    "diffHunk" : "@@ -1,1 +449,453 @@\t\tglog.Errorf(\"glusterfs: namespace is nil\")\n\t\treturn fmt.Errorf(\"glusterfs: namespace is nil\")\n\t}\n\tdynamicNamespace = pvSpec.ClaimRef.Namespace\n\tif pvSpec.Glusterfs.EndpointsName != \"\" {"
  },
  {
    "id" : "27049bf2-2a48-4432-b321-394ccccfa7e4",
    "prId" : 34705,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34705#pullrequestreview-4943659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9a2eb83-acf6-46d2-abdb-3b9a8460c995",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "shouldn't we delete also the endpoint here? I know, it may be garbage collected automatically, however we created it, we should delete it.\n",
        "createdAt" : "2016-10-19T18:46:58Z",
        "updatedAt" : "2016-10-19T19:02:44Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "44563e21-e723-4e47-9cd6-864f73dab0d3",
        "parentId" : "a9a2eb83-acf6-46d2-abdb-3b9a8460c995",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@jsafrane at earlier patch set, I deleted ep followed with svc deletion. Then @rootfs suggested to remove service first. If we do so, endpoint is gone at the same moment. so only deleting svc now. \n",
        "createdAt" : "2016-10-19T18:51:31Z",
        "updatedAt" : "2016-10-19T19:02:44Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      },
      {
        "id" : "4ab168b1-05e2-4b71-8898-70641c5d820c",
        "parentId" : "a9a2eb83-acf6-46d2-abdb-3b9a8460c995",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "ok\n",
        "createdAt" : "2016-10-19T19:06:57Z",
        "updatedAt" : "2016-10-19T19:06:57Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "2646a24a-9291-4381-9bbb-24e8ee6edc72",
        "parentId" : "a9a2eb83-acf6-46d2-abdb-3b9a8460c995",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "@humblec @jsafrane removing service first can survive a sudden crash without leaking endpoint, since hanging endpoint is going to be cleaned up. But I would suggest removing endpoint as well - in case next time the endpoint is to be created and provisioner hit the line and prematurely exit without creating service.\n\n```\n    _, err = p.plugin.host.GetKubeClient().Core().Endpoints(namespace).Create(endpoint)\n    if err != nil && errors.IsAlreadyExists(err) \n```\n",
        "createdAt" : "2016-10-19T19:13:42Z",
        "updatedAt" : "2016-10-19T19:13:43Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "082512a3-36e7-4492-b60c-d6fda9c0f5e0",
        "parentId" : "a9a2eb83-acf6-46d2-abdb-3b9a8460c995",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "@rootfs if we delete the svc first, the ep is gone at the same moment and the very next line which is delete endpoint give error saying non existence of ep. so I kept only svc deletion. Also when we delete svc from commandline, the ep is deleted internally. afaict, we only need to delete svc. \n",
        "createdAt" : "2016-10-19T19:27:13Z",
        "updatedAt" : "2016-10-19T19:36:30Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d080f986d8cd5b257cdf1afe1fb5c1100883d17",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +618,622 @@\t\treturn fmt.Errorf(\"error deleting service %v in namespace [%s]\", err, namespace)\n\t}\n\tglog.V(1).Infof(\"glusterfs: service/endpoint [%s] in namespace [%s] deleted successfully\", epServiceName, namespace)\n\treturn nil\n}"
  },
  {
    "id" : "45ebbc1c-4e20-4f9f-8037-d9984bf7ac96",
    "prId" : 31869,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83194aee-eec0-4233-8631-8c6e5b2360b9",
        "parentId" : null,
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "As the provisioningConfig has secretvalue in plaintext, do we need to log it ?\n",
        "createdAt" : "2016-09-01T16:14:38Z",
        "updatedAt" : "2016-09-21T12:58:25Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "1adf8567350caefa4bb5771907f8b5ed9174fa10",
    "line" : 241,
    "diffHunk" : "@@ -1,1 +519,523 @@\tvolSizeBytes := p.options.Capacity.Value()\n\tsz := int(volume.RoundUpSize(volSizeBytes, 1024*1024*1024))\n\tglog.V(2).Infof(\"glusterfs: create volume of size: %d bytes and configuration %+v\", volSizeBytes, p.provisioningConfig)\n\tif p.url == \"\" {\n\t\tglog.Errorf(\"glusterfs : rest server endpoint is empty\")"
  },
  {
    "id" : "9e8ae1c9-9f17-4d93-901b-f549a37230f0",
    "prId" : 30888,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46735fa9-61af-46aa-af73-d5108123bddd",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "does this has be part of plugin?\n",
        "createdAt" : "2016-08-18T20:22:53Z",
        "updatedAt" : "2016-08-19T17:34:18Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "f81a0ae2-95b0-4b1a-af30-e22fd7f98da9",
        "parentId" : "46735fa9-61af-46aa-af73-d5108123bddd",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "afaict, yes\n",
        "createdAt" : "2016-08-19T03:10:05Z",
        "updatedAt" : "2016-08-19T17:34:18Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "836ac6e4037c2ff0c9994ced9526b6a18a451486",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +43,47 @@\thost        volume.VolumeHost\n\texe         exec.Interface\n\tclusterconf *glusterfsClusterConf\n}\n"
  },
  {
    "id" : "83a0cdc9-433b-4c7c-85af-0f985c63d37f",
    "prId" : 30888,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f532f689-e056-4ee8-8db5-b7236e6221eb",
        "parentId" : null,
        "authorId" : "93379813-c6cb-43b0-9742-0424ecb8da56",
        "body" : "This should not be done, in case that Heketi changes its volume name.  Instead when the volume is created, the Volname to ID mapping should be saved by the client.  Most likely in this client, it should be saved in etcd.\n",
        "createdAt" : "2016-08-18T21:34:02Z",
        "updatedAt" : "2016-08-19T17:34:18Z",
        "lastEditedBy" : "93379813-c6cb-43b0-9742-0424ecb8da56",
        "tags" : [
        ]
      },
      {
        "id" : "67921439-9601-4669-b38f-cb9ceab29cb4",
        "parentId" : "f532f689-e056-4ee8-8db5-b7236e6221eb",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "The code is based on a particular commit or how it works now. If there are changes in future on the dependency, this  also has to be adjusted. I will book mark this for later enhancement though.\n",
        "createdAt" : "2016-08-19T03:23:17Z",
        "updatedAt" : "2016-08-19T17:34:18Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "836ac6e4037c2ff0c9994ced9526b6a18a451486",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +393,397 @@\tvolumetodel := d.glusterfsMounter.path\n\td.glusterfsClusterConf = d.plugin.clusterconf\n\tnewvolumetodel := dstrings.TrimPrefix(volumetodel, volprefix)\n\tcli := gcli.NewClient(d.glusterRestUrl, d.glusterRestUser, d.glusterRestUserKey)\n\tif cli == nil {"
  },
  {
    "id" : "95f9f399-3e8d-4adb-96f8-225a18d15183",
    "prId" : 30888,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93bab272-5e13-49bc-abe2-d1a18c5a003e",
        "parentId" : null,
        "authorId" : "93379813-c6cb-43b0-9742-0424ecb8da56",
        "body" : "Here volume has the ID.  Save the ID mapped to the volume.Name so that it can be retrieved later\n",
        "createdAt" : "2016-08-18T21:41:10Z",
        "updatedAt" : "2016-08-19T17:34:18Z",
        "lastEditedBy" : "93379813-c6cb-43b0-9742-0424ecb8da56",
        "tags" : [
        ]
      },
      {
        "id" : "4b7264ec-41a1-4863-81b6-9349bd7a04f3",
        "parentId" : "93bab272-5e13-49bc-abe2-d1a18c5a003e",
        "authorId" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "body" : "same as above.\n",
        "createdAt" : "2016-08-19T03:24:36Z",
        "updatedAt" : "2016-08-19T17:34:18Z",
        "lastEditedBy" : "3eb39df2-c27a-40ed-9fab-cabae1c6353d",
        "tags" : [
        ]
      }
    ],
    "commit" : "836ac6e4037c2ff0c9994ced9526b6a18a451486",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +469,473 @@\t}\n\tvolumeReq := &gapi.VolumeCreateRequest{Size: sz}\n\tvolume, err := cli.VolumeCreate(volumeReq)\n\tif err != nil {\n\t\tglog.Errorf(\"glusterfs: error creating volume %s \", err)"
  },
  {
    "id" : "2a32bd0d-ce48-4d8f-87c3-9abf84bcb3b1",
    "prId" : 27970,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9baff29-a996-4854-930e-982d2af51333",
        "parentId" : null,
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Verify `volumeName` is correct for these fields. If not, leave them unset.\n",
        "createdAt" : "2016-08-06T03:55:10Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      },
      {
        "id" : "893b5f2e-49ad-4e02-80e8-a25459b41688",
        "parentId" : "a9baff29-a996-4854-930e-982d2af51333",
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "These fields such as EndpointsName and Path was used for constructing UniqueVolumeName (returned by plugin.GetVolumeName()). If leaving it unset, the returned GetVolumeName() will be empty and the returned GetUniqueVolumeName() will only has pluginName which will cause name collision. But after Paul's change, for nonattachable volume, these device fields are not used for constructing UniquevVolumeName, so leaving them unset seems ok. My only question is in api.types comments, it saids some of these fields are requirement, is it ok to leave them unset?\n",
        "createdAt" : "2016-08-07T05:10:41Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      },
      {
        "id" : "b2ce0cb7-9ab2-4ca3-b5e8-a09705159ede",
        "parentId" : "a9baff29-a996-4854-930e-982d2af51333",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "Ack. Yes, that's fine then. And yes it is, since they get set to default values.\n",
        "createdAt" : "2016-08-09T18:20:50Z",
        "updatedAt" : "2016-08-15T18:29:44Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "f19a1148db1b7584be6b6b60abaf8c0bd1503ed3",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +152,156 @@\t\t\tGlusterfs: &api.GlusterfsVolumeSource{\n\t\t\t\tEndpointsName: volumeName,\n\t\t\t\tPath:          volumeName,\n\t\t\t},\n\t\t},"
  },
  {
    "id" : "0c0eaa4b-7a01-42b5-a1a2-b538135df145",
    "prId" : 24808,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8da55294-6441-437f-9b98-e1a8e6080915",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "if no `/sbin/mount.glusterfs` is on your node, mount will fail (as reported by `errs`) and `readGlusterLog` will return an error (failed to open log file), but since `errs` is not returned, the real cause is not reported .\n",
        "createdAt" : "2016-04-27T19:10:14Z",
        "updatedAt" : "2016-06-02T13:09:39Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "23ff975b-a5b4-4072-ada9-69b5fe5acad9",
        "parentId" : "8da55294-6441-437f-9b98-e1a8e6080915",
        "authorId" : "e15ef128-90ac-4a14-8795-b5be15e790ce",
        "body" : "good point...added back in @rootfs and upated new describe event messages above\n",
        "createdAt" : "2016-04-28T17:13:31Z",
        "updatedAt" : "2016-06-02T13:09:39Z",
        "lastEditedBy" : "e15ef128-90ac-4a14-8795-b5be15e790ce",
        "tags" : [
        ]
      },
      {
        "id" : "a9a246e3-d565-4672-857c-5eef9c89e07b",
        "parentId" : "8da55294-6441-437f-9b98-e1a8e6080915",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "It's a convention in go to re-use an error in a case like this where a list of errors is being accumulated.\n\nAlso, I think the gluster log has to be a best-effort attempt here.  There are legitimate reasons that this fail might not be present when this fails.\n\n@thockin @saad-ali any thoughts on this?\n",
        "createdAt" : "2016-04-28T17:27:21Z",
        "updatedAt" : "2016-06-02T13:09:39Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "a36cd3d55b8a6ccb2c5ec5224fd3985f30fda259",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +270,274 @@\t// it all goes in a log file, we will read the log file\n\tlogerror := readGlusterLog(log, b.pod.Name)\n\tif logerror != nil {\n\t\t// return fmt.Errorf(\"glusterfs: mount failed: %v\", logerror)\n\t\treturn fmt.Errorf(\"glusterfs: mount failed: %v the following error information was pulled from the glusterfs log to help diagnose this issue: %v\", errs, logerror)"
  },
  {
    "id" : "ea8ef51d-20c5-47fe-93ae-2ef48452814b",
    "prId" : 15352,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10761bc4-ae3f-4b21-9979-3dcfda0b47e2",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "Glusterfs mount is based on FUSE. Potentially, we can make use of FUSE allow_other to allow non-root users to access files. \n",
        "createdAt" : "2015-10-16T15:16:11Z",
        "updatedAt" : "2015-10-22T20:41:20Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "8d5a13f8-8803-44de-b505-274cd610eca6",
        "parentId" : "10761bc4-ae3f-4b21-9979-3dcfda0b47e2",
        "authorId" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "body" : "@rootfs Meaning the client could do the `chmod` and `chown` calls without escalating privileges on the server?\n",
        "createdAt" : "2015-10-16T17:53:58Z",
        "updatedAt" : "2015-10-22T20:41:20Z",
        "lastEditedBy" : "498aade9-b8f0-4e29-8055-89afa6f5fcc8",
        "tags" : [
        ]
      }
    ],
    "commit" : "3cd12f5e0580e330a77412a6d722fd51d9dfe57f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +136,140 @@var _ volume.Builder = &glusterfsBuilder{}\n\nfunc (_ *glusterfsBuilder) SupportsOwnershipManagement() bool {\n\treturn false\n}"
  },
  {
    "id" : "6ea08804-bc1f-477a-bb35-9028de541d3a",
    "prId" : 15236,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44a31218-afdc-4766-bcff-e3c704c848e4",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Add a comment here that this needs a ls because the plugin container may be on a filesystem that is not visible to the volume plugin process.\n",
        "createdAt" : "2015-10-18T14:16:56Z",
        "updatedAt" : "2015-10-20T18:27:42Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "1665ba4d3370f6d748d27c629539fb59b68266ce",
    "line" : null,
    "diffHunk" : "@@ -1,1 +64,68 @@\t// this needs a ls because the plugin container may be on a filesystem\n\t// that is not visible to the volume plugin process.\n\t_, err := plugin.execCommand(\"ls\", []string{\"/sbin/mount.glusterfs\"})\n\tif err == nil {\n\t\treturn true"
  },
  {
    "id" : "2afde1ce-c058-4d91-b3ba-6a10a604727f",
    "prId" : 13807,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7beb47e6-41a4-465d-afbf-82beceb0c38b",
        "parentId" : null,
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "this is rare, but I prefer not exiting so early. How about if `MkdirAll` succeeds, then append `log-file` option. \n",
        "createdAt" : "2015-09-11T13:48:23Z",
        "updatedAt" : "2015-09-11T17:29:33Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "5f6e9aa1-21a1-4892-ad38-df2af9374f08",
        "parentId" : "7beb47e6-41a4-465d-afbf-82beceb0c38b",
        "authorId" : "9ed4efe3-ac64-4100-9b5d-a68d6c37d018",
        "body" : "I had mistakenly used `os.Mkdir` initially which was failing due to parent directories being missing. The code then silently continued without the mount option and defaulted back to the original behaviour which I was trying to track down so I changed the logic which then made my error obvious.\n",
        "createdAt" : "2015-09-11T13:56:54Z",
        "updatedAt" : "2015-09-11T17:29:33Z",
        "lastEditedBy" : "9ed4efe3-ac64-4100-9b5d-a68d6c37d018",
        "tags" : [
        ]
      },
      {
        "id" : "e1a67c77-6f05-42d5-ad26-8c5c61a07799",
        "parentId" : "7beb47e6-41a4-465d-afbf-82beceb0c38b",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "that's a nice debug trick :)\n",
        "createdAt" : "2015-09-11T14:06:21Z",
        "updatedAt" : "2015-09-11T17:29:33Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "38900dd7-2412-417b-b0c8-690beea8178c",
        "parentId" : "7beb47e6-41a4-465d-afbf-82beceb0c38b",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "@bodgit have you signed cla? The label is still red\n",
        "createdAt" : "2015-09-11T14:09:17Z",
        "updatedAt" : "2015-09-11T17:29:33Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      }
    ],
    "commit" : "8bbc86d83cb8009023fc58270040f14eefb95023",
    "line" : null,
    "diffHunk" : "@@ -1,1 +224,228 @@\tp := path.Join(b.glusterfs.plugin.host.GetPluginDir(glusterfsPluginName), b.glusterfs.volName)\n\tif err := os.MkdirAll(p, 0750); err != nil {\n\t\treturn err\n\t}\n\tlog := path.Join(p, \"glusterfs.log\")"
  },
  {
    "id" : "a36f06d6-a4ad-4177-915c-4c82756f66ff",
    "prId" : 6174,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "parentId" : null,
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Is this plugin really a `PersistentVolumePlugin`? Persistent Volumes follow the claim and use model. This plugin seems like a `VolumePlugin`\n",
        "createdAt" : "2015-03-30T22:22:40Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "b7010ea5-9369-493a-ba2d-220ac19dac0f",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "0ac5d6cd-da87-46a2-9b30-0d21db1063a2",
        "body" : ">  This plugin seems like a VolumePlugin\n\nAgreed.\n",
        "createdAt" : "2015-03-31T17:21:23Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "0ac5d6cd-da87-46a2-9b30-0d21db1063a2",
        "tags" : [
        ]
      },
      {
        "id" : "d0e8eb33-4c77-404b-a543-f57b97fbb5b7",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "loop in @markturansky \nthere is a followup on the persistent volume front - once gluster is made to be able to handle it.\n",
        "createdAt" : "2015-03-31T17:29:38Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "63813bc0-2f4c-4eac-943e-30c773b67d3a",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "727fc82d-d969-41a4-a614-7fefce94f9a6",
        "body" : "All volumes are VolumePlugins.  Some are PersistentVolumePlugins if they are going to be provisioned by the admin and claimed by the user.\n\nIs there a use case where Gluster is best for pod authors who also know it is Gluster?   As opposed to asking for a persistent volume and getting one (might be Gluster, might be EBS, only the Admin knows).\n",
        "createdAt" : "2015-03-31T17:33:03Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "727fc82d-d969-41a4-a614-7fefce94f9a6",
        "tags" : [
        ]
      },
      {
        "id" : "fd62dba2-f61c-4997-9453-64a34f201b82",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "Since none of the existing examples treat Gluster as a Persistent Volume, shall we treat it as a VolumePlugin in this PR?\n",
        "createdAt" : "2015-04-01T21:30:37Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "0815209a-b1dd-4eb9-8675-6cb6ca1fb7fa",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "@rootfs: Can you address this comment?\n",
        "createdAt" : "2015-04-06T15:33:20Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "f730b6e4-bedc-4ed5-a404-11c0bc7525d8",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "the gluster is going to be provisioned by the admin and claimed by the user. \n",
        "createdAt" : "2015-04-06T15:36:18Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "c7f7d676-0130-4bae-9573-9d91ebd10687",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "body" : "This PR seems to implement Gluster as a node volume. Once gluster becomes a persistent volume, can we add this part?\n",
        "createdAt" : "2015-04-06T15:40:14Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "c4b970b3-3b9c-4773-bc9b-f8d005b15fd1",
        "tags" : [
        ]
      },
      {
        "id" : "09554e49-204c-42b7-9b3d-29001a3fd4d0",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "there will be a followup PR once persistent volume is completed.\n",
        "createdAt" : "2015-04-06T15:42:52Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      },
      {
        "id" : "b2a42117-e6c2-4ad8-bb72-0227cf5e4921",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "727fc82d-d969-41a4-a614-7fefce94f9a6",
        "body" : "GetAccessModes as a persistent volume function is already merged and @rootfs implemented and tested it correctly.\n\nThe only thing needed to truly make it a persistent volume is to add a pointer to the PersistentVolumeSource struct in types.  That bit along with examples of persistent usage make a good follow-on PR, IMO.\n",
        "createdAt" : "2015-04-06T15:46:45Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "727fc82d-d969-41a4-a614-7fefce94f9a6",
        "tags" : [
        ]
      },
      {
        "id" : "0b861231-8a76-4b0a-8b13-b65020375613",
        "parentId" : "64fe3c6d-6c23-40c2-9013-e8c64bcec116",
        "authorId" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "body" : "thanks @markturansky\n",
        "createdAt" : "2015-04-06T15:50:30Z",
        "updatedAt" : "2015-04-07T13:15:33Z",
        "lastEditedBy" : "a7f673a6-4b23-4df6-aa10-f123fa9dcd5f",
        "tags" : [
        ]
      }
    ],
    "commit" : "a278ceeb0ab17c9472164ee4cf78645c8c1ccec4",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +61,65 @@}\n\nfunc (plugin *glusterfsPlugin) GetAccessModes() []api.AccessModeType {\n\treturn []api.AccessModeType{\n\t\tapi.ReadWriteOnce,"
  }
]