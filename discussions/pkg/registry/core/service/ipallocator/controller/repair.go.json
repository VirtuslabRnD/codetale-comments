[
  {
    "id" : "66966b8f-bfb1-4a93-bfe3-91db370744a2",
    "prId" : 79386,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79386#pullrequestreview-276727923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6ef15e2-0dc3-491c-8715-16fc7df26779",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "This is OK, and I don't see anythign egregiously wrong in here, but it seems complicated and fiddly.  2 thoughts:\r\n\r\nWould it be easier to just run 2 repair loops, 1 for each family?  That way the delta here becomes \"if the IP is not from my family, ignore it\" ?\r\n\r\nWould it be easier just loop over a list of {range, registry} ignoring family, and only complaining if we get to the end of the list and find no matching CIDR?",
        "createdAt" : "2019-08-12T22:14:43Z",
        "updatedAt" : "2019-08-28T18:42:36Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "8ff88150-2292-4740-8d76-83772985c6a2",
        "parentId" : "f6ef15e2-0dc3-491c-8715-16fc7df26779",
        "authorId" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "body" : "I would not say easier. testing two distinct `repairers` in a dualstack would be a lot more fiddly. At the runtime each repairer will have to enumerate the entire data set, knowing that it may ignore the entire data set (user choose all services to be one family). The caller (that owns the ref to repairer, now has to handle one type of failure across all types of families supported by the cluster). If we created two repairer will force the caller to handle distinct error cases for each repairer.  ",
        "createdAt" : "2019-08-19T18:12:04Z",
        "updatedAt" : "2019-08-28T18:42:36Z",
        "lastEditedBy" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "tags" : [
        ]
      }
    ],
    "commit" : "c27e0b029d328552cc3ef0661f16a5ad3c422fb8",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +62,66 @@\tnetwork          *net.IPNet\n\talloc            rangeallocation.RangeRegistry\n\tsecondaryNetwork *net.IPNet\n\tsecondaryAlloc   rangeallocation.RangeRegistry\n"
  },
  {
    "id" : "49f66954-e941-4b42-8d52-4def5301f039",
    "prId" : 54304,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54304#pullrequestreview-78933673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b176945b-244d-4631-8f42-26c7ff17e361",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@gmarek - in the context of new event API, what is now the policy of adding new events?\r\nI think those are added in this PR are generally useful, but I'm not sure if we should add them in the old API or maybe wait for the new one?",
        "createdAt" : "2017-11-20T11:25:43Z",
        "updatedAt" : "2017-11-24T18:10:56Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "720a1df5-e888-4ea2-8e68-55e5d89ccb23",
        "parentId" : "b176945b-244d-4631-8f42-26c7ff17e361",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "We add them with the old API, as new one doesn't have proper client library implemented yet.",
        "createdAt" : "2017-11-24T14:50:43Z",
        "updatedAt" : "2017-11-24T18:10:56Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ed0bc12504b05bac0f1f4334ba26708207d9987",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +146,150 @@\t\tif ip == nil {\n\t\t\t// cluster IP is corrupt\n\t\t\tc.recorder.Eventf(&svc, v1.EventTypeWarning, \"ClusterIPNotValid\", \"Cluster IP %s is not a valid IP; please recreate service\", svc.Spec.ClusterIP)\n\t\t\truntime.HandleError(fmt.Errorf(\"the cluster IP %s for service %s/%s is not a valid IP; please recreate\", svc.Spec.ClusterIP, svc.Name, svc.Namespace))\n\t\t\tcontinue"
  },
  {
    "id" : "9ab7cce5-5ef2-423a-8fce-b2d66c0505d0",
    "prId" : 39206,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39206#pullrequestreview-14325510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6cfc659-03ab-4ea6-a654-bf9c336ebbad",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "So violent",
        "createdAt" : "2016-12-23T17:55:17Z",
        "updatedAt" : "2016-12-27T05:59:40Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "64f5b050a1defb3ea5b09b8e2582ac51796ca7f2",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +187,191 @@\t})\n\n\t// Blast the rebuilt state into storage.\n\tif err := rebuilt.Snapshot(snapshot); err != nil {\n\t\treturn fmt.Errorf(\"unable to snapshot the updated service IP allocations: %v\", err)"
  },
  {
    "id" : "eb18eab1-cb86-4ea8-a441-c443d0dc2470",
    "prId" : 39206,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39206#pullrequestreview-14344992",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f98c74da-b4b2-49a1-98f9-3c5345f8f07c",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "If we are full and have leaks, should we do something immediately?\r\n\r\nScenario: full, restart, expect to free some up, nothing happens.  Have to wait to see the free.\r\n\r\nIs it worth being slightly more clever here?",
        "createdAt" : "2016-12-23T17:56:54Z",
        "updatedAt" : "2016-12-27T05:59:40Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "8687461f-1c4b-4c02-8f42-1de98fb620a1",
        "parentId" : "f98c74da-b4b2-49a1-98f9-3c5345f8f07c",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "We *can't* do it in a single cycle.  Truthfully, probably 2 loops is sufficient.  Or we can increase the frequency of this loop running.  Or we can detect \"full\" and fire off an extra shorter-duration timer in hopes of cleaning up leaks...\r\n\r\nAll sound premature to me, except maybe changing the repair frequency",
        "createdAt" : "2016-12-23T23:30:11Z",
        "updatedAt" : "2016-12-27T05:59:40Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "64f5b050a1defb3ea5b09b8e2582ac51796ca7f2",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +160,164 @@\t\t\t// TODO: send event\n\t\t\t// somehow we are out of IPs\n\t\t\treturn fmt.Errorf(\"the service CIDR %v is full; you must widen the CIDR in order to create new services\", rebuilt)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unable to allocate cluster IP %s for service %s/%s due to an unknown error, exiting: %v\", ip, svc.Name, svc.Namespace, err)"
  }
]