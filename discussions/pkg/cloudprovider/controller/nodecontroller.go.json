[
  {
    "id" : "1ba86781-b659-434a-b53c-fd393e23bacc",
    "prId" : 6561,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "505036d5-e94f-4c05-9046-0d58e743cca5",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "IIUC the argument to kubeClient.Events() is the namespace? I guess \"\" is fine but I was just curious why it wasn't \"default\" or something like that.\n",
        "createdAt" : "2015-04-08T20:52:09Z",
        "updatedAt" : "2015-04-09T08:39:20Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "290807de-f6bf-4349-a86f-a8dd44557ea4",
        "parentId" : "505036d5-e94f-4c05-9046-0d58e743cca5",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I thought we agreed that node events would go in a namespace associated with the cluster-id . \n\n@thockin @bgrant0607 am I mis-remembering the outcome of that discussion?\n",
        "createdAt" : "2015-04-08T20:55:45Z",
        "updatedAt" : "2015-04-09T08:39:20Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "db2c8708-ccc2-4554-9728-920022889fa1",
        "parentId" : "505036d5-e94f-4c05-9046-0d58e743cca5",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "If we're going to do that (which I don't necessarily object to), we should move everything to use cluster-id at the same time. All of our other event generation stuff currently seems to be using empty namespace, so for this PR it's probably best to leave as-is.\n\nExamples:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/894fbc1e8a0522a63ea4880bbc29f1439de638e8/plugin/cmd/kube-scheduler/app/server.go#L100\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/8599564c96083aa221ebce070b67b22d5bbd9b1e/cmd/kubelet/app/server.go#L404\n",
        "createdAt" : "2015-04-08T21:02:30Z",
        "updatedAt" : "2015-04-09T08:39:20Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "e25e79f6-b7b8-4b9b-802e-6e59b4aed0e5",
        "parentId" : "505036d5-e94f-4c05-9046-0d58e743cca5",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Leaving empty as it was before.\n",
        "createdAt" : "2015-04-09T08:14:08Z",
        "updatedAt" : "2015-04-09T08:39:20Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "c1dd881f484378e88363d6b5b78a453964c94084",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +112,116 @@\tif kubeClient != nil {\n\t\tglog.Infof(\"Sending events to api server.\")\n\t\teventBroadcaster.StartRecordingToSink(kubeClient.Events(\"\"))\n\t} else {\n\t\tglog.Infof(\"No api server defined - no events will be sent to API server.\")"
  },
  {
    "id" : "fe6146a7-9a27-45f4-9fbb-692954e83726",
    "prId" : 6292,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7b186f5-e357-4e76-a254-d9b3988b5c07",
        "parentId" : null,
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "In the case that this fails, it looks like it won't be retried at all until the next time a node is added or removed from the cluster. This could be quite a long time in many circumstances.\n\nIn the case that this means we're not sending traffic to one of the nodes of the cluster, it isn't a huge deal due to the second layer of load balancing in the kube-proxy, but if it means that we're sending traffic to a node that's no longer part of the cluster, it may be a bigger problem.\n",
        "createdAt" : "2015-04-03T17:38:39Z",
        "updatedAt" : "2015-04-10T14:17:42Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "e4ca7fb7-b271-4d0b-a8c4-cd2868657f40",
        "parentId" : "f7b186f5-e357-4e76-a254-d9b3988b5c07",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "with luck the cloud load balancer itself will skip when it can't connect, but I'm not positive it does that... \n\n(but I agree that a more complete reconciler that isn't edge triggered would be preferable)\n",
        "createdAt" : "2015-04-03T21:34:32Z",
        "updatedAt" : "2015-04-10T14:17:42Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "9269d49f-2b65-42ef-a55a-cb3035177453",
        "parentId" : "f7b186f5-e357-4e76-a254-d9b3988b5c07",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Can you address this comment too?\n",
        "createdAt" : "2015-04-07T18:53:24Z",
        "updatedAt" : "2015-04-10T14:17:42Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "a2d33fe1-47a8-4648-9690-550dc7711d33",
        "parentId" : "f7b186f5-e357-4e76-a254-d9b3988b5c07",
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "Done.\n",
        "createdAt" : "2015-04-08T11:54:48Z",
        "updatedAt" : "2015-04-10T14:17:42Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      },
      {
        "id" : "66957505-41a0-49f3-a022-9177c1e08160",
        "parentId" : "f7b186f5-e357-4e76-a254-d9b3988b5c07",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I;m wondering if in case of failure we should wait for the next \"sync\" (which currently is 5 seconds) or we should retry immediately. Maybe waiting 5 second is fine, but at least please add a comment.\n",
        "createdAt" : "2015-04-10T12:23:58Z",
        "updatedAt" : "2015-04-10T14:17:42Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "540d9fd7-04c1-4481-b837-7371e62160e6",
        "parentId" : "f7b186f5-e357-4e76-a254-d9b3988b5c07",
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "I think it is better to wait a while (e.g.: 5 seconds) before the retry. If we assume that the error is transient, it is better not to spam cloud provider with our requests, but wait and give some time, so the error condition is over.\n\nI've added an error log with info about the re-transmission. \n",
        "createdAt" : "2015-04-10T14:46:27Z",
        "updatedAt" : "2015-04-10T14:46:27Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      }
    ],
    "commit" : "1c042208c771f14fadb447d669aa510acf3c7a5b",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +273,277 @@\t\t\terr := balancer.UpdateTCPLoadBalancer(name, zone.Region, hosts)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"External error while updating TCP load balancer: %v.\", err)\n\t\t\t\tshouldRetry = true\n\t\t\t}"
  },
  {
    "id" : "85e20a4d-24b8-4f4f-9649-4074a5ce03c8",
    "prId" : 6217,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff35ae77-aa3f-4153-811d-37c90fea7099",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "I think its cleaner if you return here too (easier to understand the flow)\n\n(while you're at it, it's weird that the error comes first in the return signature, mind fixing that while you're in here?)\n",
        "createdAt" : "2015-04-02T03:21:13Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "bb890743-9c99-4a9a-bc2d-1d6e47d53776",
        "parentId" : "ff35ae77-aa3f-4153-811d-37c90fea7099",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done. Sorry for the order of return types, I just had to add grace period and ready condition later, and it ended like this. For the record: I think that returning error as first value would be better, as it would strongly suggest that error should be checked before doing anything (it would be impossible to \"forget\" that there's a possibility of getting an error).\n",
        "createdAt" : "2015-04-02T06:22:03Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "751d1d25ff08946060e53426bbd02e7e13c4cf34",
    "line" : null,
    "diffHunk" : "@@ -1,1 +563,567 @@\t\tif !api.Semantic.DeepEqual(nc.getCondition(&node.Status, api.NodeReady), lastReadyCondition) {\n\t\t\tif _, err = nc.kubeClient.Nodes().Update(node); err != nil {\n\t\t\t\tglog.Errorf(\"Error updating node %s: %v\", node.Name, err)\n\t\t\t\treturn gracePeriod, lastReadyCondition, readyCondition, err\n\t\t\t} else {"
  },
  {
    "id" : "817bade7-96d1-421d-aec5-ea32e030453b",
    "prId" : 6217,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76a37fc1-8754-468a-873f-77d46d0dd24e",
        "parentId" : null,
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "orthogonal to the PR, but go prefers 'early return', so it would be better to \nif err == nil {\n   break\n}\nname := node.Name\nxxx\n",
        "createdAt" : "2015-04-02T20:47:48Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "6fc088d2-a8f4-48b4-9971-a60f1811de2c",
        "parentId" : "76a37fc1-8754-468a-873f-77d46d0dd24e",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2015-04-02T22:29:57Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "751d1d25ff08946060e53426bbd02e7e13c4cf34",
    "line" : null,
    "diffHunk" : "@@ -1,1 +599,603 @@\t\t\tname := node.Name\n\t\t\tnode, err = nc.kubeClient.Nodes().Get(name)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Failed while getting a Node to retry updating NodeStatus. Probably Node %s was deleted.\", name)\n\t\t\t\tbreak"
  },
  {
    "id" : "df3a75a3-358a-4acc-8e8d-e25327f62855",
    "prId" : 6217,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49d47e97-6a49-4ac4-a965-2aaf778e9073",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Why do you have this else clause? Don't you always want to set transitionTime = nc.now() when savedCondition.LastProbeTime != observedCondition.LastProbeTime, even if the condition stayed the same? In the comment you said \"both saved and current statuses have Ready Conditions, they have different LastProbeTimes, but the same Ready Condition State - everything's in order, we update probeTimestamp\"\n",
        "createdAt" : "2015-04-03T07:21:14Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "8be02e46-f906-460e-bd5e-62a3fe5705d0",
        "parentId" : "49d47e97-6a49-4ac4-a965-2aaf778e9073",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "IIUC \"TransitionTime\" is the last time Condition has changed, so if Condition Status is the same as previously we assume that no transition occurred. OTOH ProbeTime is the last time we heard from Node, so it's updated if anything in NodeStatus has changed (as a proxy we use ProbeTime from status, as it is updated during any change in NodeStatus.\n",
        "createdAt" : "2015-04-03T07:32:11Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "80be7973-a918-4856-9d2a-740e52cd339e",
        "parentId" : "49d47e97-6a49-4ac4-a965-2aaf778e9073",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Ah, sorry, I got confused. What you have seems OK but would it be slightly better to change the \"if\" from\nsavedCondition.Status != observedCondition.Status\nto\nsavedCondition.LastTransitionTime != observedCondition.LastTransitionTime\n?\n",
        "createdAt" : "2015-04-03T07:54:04Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "b1fcf940-efb1-4fa0-97ac-4acdcf4798c4",
        "parentId" : "49d47e97-6a49-4ac4-a965-2aaf778e9073",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Sure. Done.\n",
        "createdAt" : "2015-04-03T08:00:22Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "751d1d25ff08946060e53426bbd02e7e13c4cf34",
    "line" : null,
    "diffHunk" : "@@ -1,1 +526,530 @@\n\t\t\ttransitionTime = nc.now()\n\t\t} else {\n\t\t\ttransitionTime = savedNodeStatus.readyTransitionTimestamp\n\t\t}"
  },
  {
    "id" : "634adf86-151a-44f4-92d2-bf9e3bb0a0b8",
    "prId" : 6217,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e166ec5-920a-49b7-914c-86137683c8c8",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "just to confirm, this == savedNodeStatus.probeTimestamp, right? (no need to change, just want to make sure I'm not missing omething)\n",
        "createdAt" : "2015-04-03T09:57:25Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "87a35a86-958f-4e8a-b791-df4c19524955",
        "parentId" : "0e166ec5-920a-49b7-914c-86137683c8c8",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Right.\n",
        "createdAt" : "2015-04-03T10:01:40Z",
        "updatedAt" : "2015-04-07T07:21:04Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "751d1d25ff08946060e53426bbd02e7e13c4cf34",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +568,572 @@\t\t\t\tnc.nodeStatusMap[node.Name] = NodeStatusData{\n\t\t\t\t\tstatus:                   node.Status,\n\t\t\t\t\tprobeTimestamp:           nc.nodeStatusMap[node.Name].probeTimestamp,\n\t\t\t\t\treadyTransitionTimestamp: nc.now(),\n\t\t\t\t}"
  },
  {
    "id" : "acf562bc-5eda-4db7-bca5-4f542985c2be",
    "prId" : 6161,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c2f6759-1139-46e0-8378-d4fd185b675c",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Please add a comment saying that Nodes for which we fail populating address are skipped. \n",
        "createdAt" : "2015-03-31T16:13:13Z",
        "updatedAt" : "2015-03-31T19:58:44Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "72ee90494ecf83c1ac0d1f6022dd244be41f98a4",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +229,233 @@\t\t\tnodeList.Items = []api.Node{node}\n\t\t\t_, err = nc.PopulateAddresses(nodeList)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Error fetching addresses for new node %s: %v\", node.Name, err)\n\t\t\t\tcontinue"
  },
  {
    "id" : "a954462a-2b58-403f-9a01-a0013b65e5be",
    "prId" : 6156,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4636aca8-0e7b-44c5-aec1-d668d4ef36c9",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "probably you should also break in this case. if the Get failed, calling tryUpdateNodeStatus() again with the old node seems pointless. \n",
        "createdAt" : "2015-03-31T07:45:05Z",
        "updatedAt" : "2015-03-31T09:22:14Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "4c812245-9898-45fe-bcbb-362ec77f02f1",
        "parentId" : "4636aca8-0e7b-44c5-aec1-d668d4ef36c9",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Right. Thanks for the catch.\n",
        "createdAt" : "2015-03-31T08:51:09Z",
        "updatedAt" : "2015-03-31T09:22:14Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa6d5e259fc02e77d15bbcba28eb3b2c3a1b7006",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +528,532 @@\t\t\t\tnode, err = nc.kubeClient.Nodes().Get(name)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Errorf(\"Failed while getting a Node to retry updating NodeStatus. Probably Node %s was deleted.\", name)\n\t\t\t\t\tbreak\n\t\t\t\t}"
  },
  {
    "id" : "872bb34f-e0c7-446f-a323-47684b139e47",
    "prId" : 5399,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "773325f2-7f70-48f8-a36c-e3d3dc11fd1f",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Could you just set this to something like nodeMonitorGracePeriod/3 instead of making it be a flag?\n",
        "createdAt" : "2015-03-20T08:20:07Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "51429c50-e0e9-44e1-8e6d-fe4ce5376cb5",
        "parentId" : "773325f2-7f70-48f8-a36c-e3d3dc11fd1f",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "Well, it controls the rate nodecontroller contacts apiserver, so I think it warrants adding as a constant.\n\nDo you have compelling reasons not using a flag?  BTW, it's a constant, not a flag, so I think it's fine here :)\n",
        "createdAt" : "2015-03-20T16:57:36Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "7f93038b-2df1-4896-8feb-c13ea0933169",
        "parentId" : "773325f2-7f70-48f8-a36c-e3d3dc11fd1f",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Sorry, never mind, I was confused.\n",
        "createdAt" : "2015-03-20T23:27:04Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "862118dc-0152-4592-ae9b-126d03b0d9fc",
        "parentId" : "773325f2-7f70-48f8-a36c-e3d3dc11fd1f",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Why it's not a flag? It seems like a value that's likely to change when configuring the cluster (optimal value is probably a function of master machine resources, network performance and number of nodes).\n",
        "createdAt" : "2015-03-24T14:03:24Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5675b8924ace47762289bb4e6f1545ebca625f4",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +58,62 @@\t// Theoretically, this value should be lower than nodeMonitorGracePeriod.\n\t// TODO: Change node status monitor to watch based.\n\tnodeMonitorPeriod = 5 * time.Second\n)\n"
  },
  {
    "id" : "6991b80f-d302-406f-861d-d425388443fb",
    "prId" : 5399,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b98dc666-4604-4382-8a3c-4a0b9b4ae90c",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Also mention that is must be < podEvictionTimeout\n",
        "createdAt" : "2015-03-23T06:28:24Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "a81b9ec0-26ef-4abf-bbe6-4d6d73c03f19",
        "parentId" : "b98dc666-4604-4382-8a3c-4a0b9b4ae90c",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "Comment added; but podEvictionTimeout is controlled by user, so we'll never be able to say the value is less than podEvictionTimeout.\n",
        "createdAt" : "2015-03-23T18:34:36Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5675b8924ace47762289bb4e6f1545ebca625f4",
    "line" : null,
    "diffHunk" : "@@ -1,1 +43,47 @@\t// Note: be cautious when changing the constant, it must work with nodeStatusUpdateFrequency\n\t// in kubelet. There are several constraints:\n\t// 1. nodeMonitorGracePeriod must be N times more than nodeStatusUpdateFrequency, where\n\t//    N means number of retries allowed for kubelet to post node status. It is pointless\n\t//    to make nodeMonitorGracePeriod be less than nodeStatusUpdateFrequency, since there"
  },
  {
    "id" : "6160b21d-825c-413c-9c0e-f76a71a36c9a",
    "prId" : 5399,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bdd299f-a5dc-4e9c-b385-0fd0cf78669e",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Say something like \"similar concerns as for setting nodeMonitorGracePeriod apply\" ?\n",
        "createdAt" : "2015-03-23T06:28:28Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "afb15482-d7f9-4a6e-842d-0f9baf86728a",
        "parentId" : "6bdd299f-a5dc-4e9c-b385-0fd0cf78669e",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "I don't think this startup grace period has the same concern, it is basically for node/cluster startup; e.g. master is started before kubelet; kubelet is starting cadvisor, etc.  It's just one-off thing, why do you think the same concerns will apply?\n",
        "createdAt" : "2015-03-23T18:34:42Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5675b8924ace47762289bb4e6f1545ebca625f4",
    "line" : null,
    "diffHunk" : "@@ -1,1 +53,57 @@\t// The constant is used if sync_nodes_status=False, only for node startup. When node\n\t// is just created, e.g. cluster bootstrap or node creation, we give a longer grace period.\n\tnodeStartupGracePeriod = 30 * time.Second\n\t// The constant is used if sync_nodes_status=False. It controls NodeController monitoring\n\t// period, i.e. how often does NodeController check node status posted from kubelet."
  },
  {
    "id" : "b8556a01-3654-44a3-8ea2-9cf9a5cc0b47",
    "prId" : 5399,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ab0aa53-1499-42f3-9406-681c2556b243",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "This is definitely better, but I think there is still one corner case that is problematic. What if a node flips back and forth between ConditionNone and ConditionUnknown, staying in each state for < podEvictionTimeout-gracePeriod seconds. (For example, node reports not ready, then node becomes unreachable, then node becomes reachable and reports not ready, then node becomes unreachable, etc. etc.) Then I think we will never evict the pod.\n\nI think the logic needs to be:\nif ReadyCondition is ConditionNone && now > LastTransitionTime + podEvictionTimeout { deletePods }\nif ReadyCondition is ConditionUnknown && now > LastProbeTime + (podEvictionTimeout-gracePeriod) { deletePods }\n\nIn other words -- if you have been in ConditionNone for podEvictionTimeout OR the last time you heard from the Kubelet was > podEvictionTimeout-gracePeriod, then evict.\n\nWhat do you think?\n\nOne thing this requires is that we don't \"refresh\" LastProbeTime except when we actually hear from Kubelet. I didn't understand why you were \"refreshing\" LastProbeTime anyway. Maybe you can explain.\n",
        "createdAt" : "2015-03-23T06:30:47Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "e9f898a8-7951-475a-b096-6745b8886096",
        "parentId" : "8ab0aa53-1499-42f3-9406-681c2556b243",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "Ok, the above comments are all related here; so let's discuss here.\n\nI think the tradeoff here is refreshing LastProbeTime and a better logic.  I was leaning towards an up-to-date status from enduser's point; it is always good to see fresh status, especially when kubelet updates NodeReady=Full periodically.  Another reason is that, the status we are posting, the Unknown status, it is updated by nodecontroller, and its LastProbeTime and LastTransitionTime is updated at _now_, not the previous value.  IMO, we are hacking the status to fulfill the better logic.\n\nThe two are the primary reasons I'm refreshing LastProbeTime.  I did realize the flipping problem, but that's rare and is one of the reasons I'm introducing the grace period.\n\nCombined the two, I think node controller still needs to set LastTransitionTime to now() at least, and LastProbeTime set to previous time.  The suggested logic can be used here.  How's that sound?\n",
        "createdAt" : "2015-03-23T18:34:49Z",
        "updatedAt" : "2015-03-24T18:27:23Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5675b8924ace47762289bb4e6f1545ebca625f4",
    "line" : 550,
    "diffHunk" : "@@ -1,1 +482,486 @@\t\tif readyCondition != nil {\n\t\t\t// Check eviction timeout.\n\t\t\tif lastReadyCondition.Status == api.ConditionNone &&\n\t\t\t\tnc.now().After(lastReadyCondition.LastTransitionTime.Add(nc.podEvictionTimeout)) {\n\t\t\t\t// Node stays in not ready for at least 'podEvictionTimeout' - evict all pods on the unhealthy node."
  },
  {
    "id" : "730d57be-aafb-4d3f-9035-7fc8392a5537",
    "prId" : 5116,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b144a0b3-5ebc-4fcc-9541-b008b7581914",
        "parentId" : null,
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "We can override the address here:  node.Status.Addresses = []api.Nodeaddress{address}\n",
        "createdAt" : "2015-03-06T16:40:56Z",
        "updatedAt" : "2015-03-06T16:40:56Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf031fb0822d996e66301708c8a364a945e22b55",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +279,283 @@\t\t\t} else {\n\t\t\t\taddress := api.NodeAddress{Type: api.NodeLegacyHostIP, Address: hostIP.String()}\n\t\t\t\tapi.AddToNodeAddresses(&node.Status.Addresses, address)\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "d3f50a21-b264-4cda-bb17-7f527b19dacd",
    "prId" : 4585,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fcb59785-c814-4bf0-9b31-b53b01a1ad74",
        "parentId" : null,
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "Do we want to set node to unschedulable if node is not ready?\n",
        "createdAt" : "2015-02-25T01:06:59Z",
        "updatedAt" : "2015-03-12T21:30:31Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "bce7835e-2c81-4ff4-ad55-4ab0d211e92f",
        "parentId" : "fcb59785-c814-4bf0-9b31-b53b01a1ad74",
        "authorId" : "0fac9b44-2025-4030-871c-d6ff0e1b299e",
        "body" : "I think node status should reflect the current status and it may have multiple node conditions in failed state. Subset of the node conditions may impact other components in the system.\n",
        "createdAt" : "2015-02-25T01:45:21Z",
        "updatedAt" : "2015-03-12T21:30:31Z",
        "lastEditedBy" : "0fac9b44-2025-4030-871c-d6ff0e1b299e",
        "tags" : [
        ]
      },
      {
        "id" : "9e7f5770-8cc4-4b39-bcd0-43ea25ace3b8",
        "parentId" : "fcb59785-c814-4bf0-9b31-b53b01a1ad74",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "What does it mean if a node is schedulable but not ready?  A schedulable node should be ready. We are supporting plugin scheduler, so we need to make sure each condition is sematically right.\n",
        "createdAt" : "2015-02-27T02:54:41Z",
        "updatedAt" : "2015-03-12T21:30:31Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      },
      {
        "id" : "e2309724-2fcb-44f8-aa6b-ce9fa4204597",
        "parentId" : "fcb59785-c814-4bf0-9b31-b53b01a1ad74",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Our choice is to combine conditions in the scheduler, here:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/plugin/pkg/scheduler/factory/factory.go#L182\n\nOr to combine them here. \n\nGiven that I foresee handling eviction in the node controller, I agree that it makes sense to do it here.\n",
        "createdAt" : "2015-02-27T04:05:31Z",
        "updatedAt" : "2015-03-12T21:30:31Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0efb7a061a617695235ef887719ea87696847c7",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +353,357 @@\t// Check Condition: NodeSchedulable\n\toldSchedulableCondition := s.getCondition(node, api.NodeSchedulable)\n\tnewSchedulableCondition := s.checkNodeSchedulable(node)\n\ts.updateLastTransitionTime(oldSchedulableCondition, newSchedulableCondition)\n\tconditions = append(conditions, *newSchedulableCondition)"
  },
  {
    "id" : "c7eb8c54-d902-42df-b3a5-ce5406f0b15d",
    "prId" : 4585,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84171610-3352-45a8-b31b-9dc1c693e521",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "I'm fine with this behavior.\n",
        "createdAt" : "2015-03-06T01:03:47Z",
        "updatedAt" : "2015-03-12T21:30:31Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0efb7a061a617695235ef887719ea87696847c7",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +373,377 @@\n// checkNodeSchedulable checks node schedulable condition, without transition timestamp set.\nfunc (s *NodeController) checkNodeSchedulable(node *api.Node) *api.NodeCondition {\n\tif node.Spec.Unschedulable {\n\t\treturn &api.NodeCondition{"
  },
  {
    "id" : "02d80406-23d4-475c-863b-180e66cc2d7f",
    "prId" : 4241,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73f8db0a-5529-45a9-975a-51e8e35e0651",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "The scheduler watches for pods that don't have the Host field set, so that field is supposed as a selector. Could you use that?  See:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/registry/pod/rest.go#L108\n",
        "createdAt" : "2015-02-09T17:25:09Z",
        "updatedAt" : "2015-02-09T23:10:58Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "1c2fcb04-451a-4422-adaa-cb348a0ff181",
        "parentId" : "73f8db0a-5529-45a9-975a-51e8e35e0651",
        "authorId" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "body" : "I think the link from your comment is to convert pod status to selectable fields, i.e. if we list Pod.Host == \"\", then we need to have all other fields to match against. It looks like an apiserver thing.\n\nScheduler seems to do it's own listing:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/6d9845361f05eaddb350f7df59efe94ca9d63ec1/pkg/client/cache/listwatch.go#L41\n\nBut our standard client only have label select\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/6d9845361f05eaddb350f7df59efe94ca9d63ec1/pkg/client/pods.go#L58\n",
        "createdAt" : "2015-02-09T18:13:25Z",
        "updatedAt" : "2015-02-09T23:10:58Z",
        "lastEditedBy" : "7116d1ae-39f7-4e5d-81a9-1bcb75ebd909",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c6f6e0efc441b2cda0b359e900ee6723d922cf8",
    "line" : 179,
    "diffHunk" : "@@ -1,1 +330,334 @@func (s *NodeController) deletePods(nodeID string) error {\n\tglog.V(2).Infof(\"Delete all pods from %v\", nodeID)\n\t// TODO: We don't yet have field selectors from client, see issue #1362.\n\tpods, err := s.kubeClient.Pods(api.NamespaceAll).List(labels.Everything())\n\tif err != nil {"
  }
]