[
  {
    "id" : "0f423569-231e-49bb-9d95-a77893db268e",
    "prId" : 9105,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54f12186-8fd2-49be-aa2b-9726906f992d",
        "parentId" : null,
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "Is it possible for this to be a not-found error, or can monitors not be deleted while still associated with a pool?\n",
        "createdAt" : "2015-06-03T05:52:44Z",
        "updatedAt" : "2015-06-05T06:28:35Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "2e8130e1-5dc2-491c-b34d-1a6e80b90260",
        "parentId" : "54f12186-8fd2-49be-aa2b-9726906f992d",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "They can't be deleted while associated to a pool, but it is possible that another routine much like this one is racing with us and has already disassociated/deleted various portions of the lbaas setup (So you will get some other error, not not-found).\nI figured at this point exiting out and restarting the entire delete process was a fine response.\n",
        "createdAt" : "2015-06-03T07:05:14Z",
        "updatedAt" : "2015-06-05T06:28:35Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "75f49b331aa64b441c1de92fbd137bd0eb221468",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +733,737 @@\t\tfor _, monId := range pool.MonitorIDs {\n\t\t\t_, err = pools.DisassociateMonitor(lb.network, pool.ID, monId).Extract()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}"
  },
  {
    "id" : "d62e55e4-3c33-4f20-bafe-a096dfaaf6e2",
    "prId" : 7852,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cee63250-20e1-4a9f-9b35-b0310b0af5cf",
        "parentId" : null,
        "authorId" : null,
        "body" : "Same nit as above.\n",
        "createdAt" : "2015-05-06T20:24:19Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      }
    ],
    "commit" : "dbf224475da7d69540e3ca543741ededc99d9349",
    "line" : null,
    "diffHunk" : "@@ -1,1 +654,658 @@\t\t\treturn poolErr\n\t\t}\n\t}\n\tpoolExists := (poolErr == nil)\n"
  },
  {
    "id" : "f15a7678-c114-4114-9e1f-6ee477fe4535",
    "prId" : 7852,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "parentId" : null,
        "authorId" : null,
        "body" : "I think that you need to return an error here if the pool deletion fails, so that you will retry again later?\nI'm pretty sure that given the eventual consistency of the underlying GCE, a significant number of these pool deletions are going to fail because the monitor disassociation will not have completed under the hood yet. \n",
        "createdAt" : "2015-05-06T20:28:01Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "dab3fa1e-d689-4a06-9ed7-6043d95345ac",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "I'd think so as well, but I don't know why @anguslees originally wrote it this way, so I'm hesitant to change it, given that it looks to have been an intentional choice. It'd be nice if he could chime in, but otherwise I don't want to mess with it given that I have no way of testing it (since I don't have an openstack cluster).\n",
        "createdAt" : "2015-05-06T22:33:06Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "dc72f851-a65a-4f8d-a511-7da51be613b8",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : null,
        "body" : "My apologies - I'd overlooked the fact that this was openstack and not GCE.\n",
        "createdAt" : "2015-05-06T23:11:53Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : null,
        "tags" : [
        ]
      },
      {
        "id" : "1db23221-c624-4b7d-8783-20aa30e252b5",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "We need to delete the vip before the pool (otherwise the pool is still in use), and once the vip is deleted GetTCPLoadBalancerExists will indicate \"does not exist\", and we could potentially recreate another vip with the same name (and a new pool, also with the same name).  My intention (and the current code) was just to leak the pool object during partial failures (the \"ignore errors following here\" comment could have been clearer).\n\nNow that we're looking at retrying the delete operation, we should definitely disassociate (and delete!) the monitors before attempting to remove the vip (partial failures will lead to missing health checks, but if DeleteLoadBalancer is retried then eventually we might delete everything successfully).  If the disassociate succeeds and the monitor delete fails, we're still leaking the monitor.\n\nThe pool is still awkward.  It is possible to lookup the pool by name (rather than ID), but the name is not necessarily unique - if we have already recreated a new LoadBalancer with the same name then we could delete the wrong pool.  With a bit more clarity around how k8s will interpret \"Get\" and \"Create\" when \"Delete\" fails we could assume the pool name is unique and use that to retry deleting the pool.\n\nLess desirable alternatives might include trying to \"unwind\" and recreate the vip if there were failures deleting the pool (yuck); using OpenStack metadata to associate pool+monitor to original LB (I don't modify metadata anywhere else); or modifying k8s to allow cloud providers to store resource metadata somewhere within k8s (bigger change, but could also lead to performance improvements with object lookup).\n",
        "createdAt" : "2015-05-07T01:09:56Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "43d0a717-242d-4356-8fd4-0cfc9910cf16",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "Since a couple weeks ago, load balancers have been named using their service's UID, so there will never be name conflicts that enable reusing old pools. Given that, would you be ok with switching over to getting the pool by name? This would make the OpenStack behavior almost identical to the GCE behavior, meaning it'll be less likely to break in weird ways in the future.\n\nIn the meantime, I can have this function gracefully return nil if the vip doesn't exist. Once you've switched the pool lookups to be by name, we can go back to trying to delete the pool even if the vip has already been deleted.\n\nHow does that sound? I'll open up an issue for doing this work and for cleaning up the monitors that aren't currently being deleted.\n",
        "createdAt" : "2015-05-15T22:49:18Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "3a96eb64-8dd9-4b63-9538-ef36773903e8",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "> Since a couple weeks ago, load balancers have been named using their service's UID, so there will never be name conflicts that enable reusing old pools.\n\n(thinking)\n\nWhat about:\n- Create() creates pool successfully, fails to create vip, fails to delete new pool during unwind\n- Create() now gets called again.\n\nDo we now have a duplicate pool?  Or will Create() call Delete repeatedly in some way before reattempting the Create?\n",
        "createdAt" : "2015-05-16T18:20:48Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "e36bea29-a870-489e-88f6-f613688aa9c0",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "The model I've used in the past (and now in the GCE implementation) is to ensure idempotency of each individual step. If we attempt to create a pool and one already exists with that name, we consider it a success and continue to the next step. That way, the service controller can keep retrying until the creation succeeds.\n\nEffectively, we'd just have to check the return code from the pool creation attempt, and if it's an already-exists error, consider it a success.\n",
        "createdAt" : "2015-05-18T06:34:04Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      },
      {
        "id" : "710557f5-475a-4cae-b40a-6a8b8b55182f",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "(I'm really not trying to be awkward here)\n\nOpenStack resource names are not unique, so we won't get an already-exists error at the underlying level.  We will have to search first, and then create - which is easy enough, but does leave a small race window that could result in duplicate pools.\n\nI'm ok with all this btw.  I think there's still some hairy corner cases involving k8s restarts/etc that will result in odd/leaked resources, but I agree that the proposed change is better than the existing logic.\n",
        "createdAt" : "2015-05-18T17:02:24Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "350b5fba-543d-409a-ac0c-ee41204779a8",
        "parentId" : "b03c25e2-f798-4a77-aa31-8817afc0ea83",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "Ah, I didn't realize that about Openstack. It does make this strategy a little less effective. I'm definitely open to alternative implementations, I just want to reduce the likelihood of leaked resources.\n\nSo long as having duplicate pools around doesn't break functionality, I'd expect we could also avoid stranding resources even in that rare case by deleting all pools with the UID-derived name when deleting the LB.\n",
        "createdAt" : "2015-05-18T18:18:23Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbf224475da7d69540e3ca543741ededc99d9349",
    "line" : null,
    "diffHunk" : "@@ -1,1 +673,677 @@\t\t\tpools.DisassociateMonitor(lb.network, pool.ID, monId)\n\t\t}\n\t\tpools.Delete(lb.network, pool.ID)\n\t}\n"
  },
  {
    "id" : "91ddb9f7-5672-45b1-9095-a5bd295b504f",
    "prId" : 7852,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d73fb315-c7d3-497a-88b6-a75e58184d79",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "If the above returned ErrNotFound, then vip.PoolID will be undefined here.\n",
        "createdAt" : "2015-05-07T00:37:37Z",
        "updatedAt" : "2015-05-19T17:07:02Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbf224475da7d69540e3ca543741ededc99d9349",
    "line" : null,
    "diffHunk" : "@@ -1,1 +648,652 @@\t// It's ok if the pool doesn't exist, as we may still need to delete the vip\n\t// (although I don't believe the system should ever be in that state).\n\tpool, poolErr := pools.Get(lb.network, vip.PoolID).Extract()\n\tif poolErr != nil {\n\t\tdetailedErr, ok := poolErr.(*gophercloud.UnexpectedResponseCodeError)"
  }
]