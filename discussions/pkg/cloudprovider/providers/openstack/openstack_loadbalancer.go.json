[
  {
    "id" : "6a8ca0fc-b96f-45bd-8963-1f1ecf94790f",
    "prId" : 53714,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53714#pullrequestreview-68809418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb5a7077-e4f4-4921-a3c9-af3989d0f56c",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Another alternative would be to use the service UID, which is also unique over time (but less readable).\r\n\r\nIf I delete a service and then quickly create a new one with the same namespace/name - what do we want to have happen to the securityGroup?\r\n\r\nI think we _want_ the securityGroup to not overlap in this case (so the old one can be cleaned up in parallel with the new one being created, without conflicts).\r\n... So I think this means that this function should return\r\n```go\r\nfmt.Sprintf(\"lb-sg-%s\", service.UID)\r\n```",
        "createdAt" : "2017-10-12T02:44:13Z",
        "updatedAt" : "2017-10-12T02:47:11Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "33956481-8632-42fc-92a6-08f4db23fc27",
        "parentId" : "cb5a7077-e4f4-4921-a3c9-af3989d0f56c",
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : ">If I delete a service and then quickly create a new one with the same namespace/name - what do we want to have happen to the securityGroup?\r\n\r\nThat make sense. The service.UID is less readable.\r\nHow about \"fmt.Sprintf(\"lb-sg-%s-%s-%s\", clusterName, service.Namespace, service.Name, service.UID)\"?",
        "createdAt" : "2017-10-12T03:00:20Z",
        "updatedAt" : "2017-10-12T03:00:20Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      },
      {
        "id" : "0f18b2ef-dbf7-43b3-8937-60620c9d32a5",
        "parentId" : "cb5a7077-e4f4-4921-a3c9-af3989d0f56c",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "That's now very long - is there any length limit that we need to worry about here?  A quick look at the code seems to imply that the name is limited to 255 chars, so I think we're ok on length.\r\n\r\nAnother (better?) option would be to use the securityGroup \"description\" field (also 255 chars) rather than trying to mash all our user-friendly text in the name field.  That way we can have spaces, etc and change the specific text over time without worrying about wider impact.\r\n\r\nPersonally, I think the user is going to have a pretty good idea of which cluster a securityGroup is related to, and will be able to quickly find the relevant Service based on context, ports referred to, etc without needing any additional help.  I agree that the UID is less readable though - so I agree that either your long-name version or the above name+description version is better than my original short-name-only version.",
        "createdAt" : "2017-10-12T03:15:28Z",
        "updatedAt" : "2017-10-12T03:19:31Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "4816ce26-7ba8-449a-9037-e0c828091f11",
        "parentId" : "cb5a7077-e4f4-4921-a3c9-af3989d0f56c",
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "That's right, I will check the size of its name.",
        "createdAt" : "2017-10-12T03:40:23Z",
        "updatedAt" : "2017-10-12T03:40:23Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      }
    ],
    "commit" : "5af6a0b4de938df1b6ce2c8ac65f33152a502ddf",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +373,377 @@\nfunc getSecurityGroupName(clusterName string, service *v1.Service) string {\n\treturn fmt.Sprintf(\"lb-sg-%s-%s-%s\", clusterName, service.Namespace, service.Name)\n}\n"
  },
  {
    "id" : "51b2058c-b7dd-4002-a3d9-98110c0b16be",
    "prId" : 52609,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/52609#pullrequestreview-63540438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69ab318b-5e01-4189-9cb1-181732e954fa",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Aside: I see this code is structured ever-so-slightly differently to the LbaasV2 version (with different corner cases around whether the floating IP already exists), which is unfortunate.  It would be nice if there was less duplication/variation here - otoh the LbaasV1 version could probably also be declared obsolete and just removed...",
        "createdAt" : "2017-09-19T02:40:23Z",
        "updatedAt" : "2017-09-19T02:41:34Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "61cdd68c-843c-4918-98a3-23124ba50da9",
        "parentId" : "69ab318b-5e01-4189-9cb1-181732e954fa",
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "Yeah, there are many different places Between LbaasV1 and LbaasV2, LbaasV1 has been  deprecated on OpenStack, so it is better to remove LbaasV1. ",
        "createdAt" : "2017-09-19T03:06:33Z",
        "updatedAt" : "2017-09-19T03:06:35Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      }
    ],
    "commit" : "70a0f443c84918f4a12f726486bc559c45d3258f",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1471,1475 @@\t\t}\n\n\t\tstatus.Ingress = []v1.LoadBalancerIngress{{IP: floatIP.FloatingIP}}\n\t} else {\n\t\tstatus.Ingress = []v1.LoadBalancerIngress{{IP: vip.Address}}"
  },
  {
    "id" : "f4ba07b5-75d4-4131-8984-650bd3f64c4a",
    "prId" : 51795,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51795#pullrequestreview-60322204",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecb90273-01e9-4dba-a6ea-e815b1008d92",
        "parentId" : null,
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "Nice changes.",
        "createdAt" : "2017-09-04T04:31:53Z",
        "updatedAt" : "2017-09-04T04:31:53Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      }
    ],
    "commit" : "74a3d89ad4ff1508a16bacb0c333a1f795c31470",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +995,999 @@\n\t\tportID := loadbalancer.VipPortID\n\t\tupdate_opts := neutronports.UpdateOpts{SecurityGroups: &[]string{lbSecGroup.ID}}\n\t\tres := neutronports.Update(lbaas.network, portID, update_opts)\n\t\tif res.Err != nil {"
  },
  {
    "id" : "f035872e-6dc8-4144-ad55-598587b51e2c",
    "prId" : 50836,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50836#pullrequestreview-68171490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a867a63-738a-4f1d-9394-b22a0c92aa13",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "need to include port/protocol/direction",
        "createdAt" : "2017-10-10T04:26:46Z",
        "updatedAt" : "2017-10-10T07:09:30Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "daadccfa-bd93-4b59-993b-026eeaa801a1",
        "parentId" : "3a867a63-738a-4f1d-9394-b22a0c92aa13",
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "Done.",
        "createdAt" : "2017-10-10T06:49:21Z",
        "updatedAt" : "2017-10-10T07:09:30Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      }
    ],
    "commit" : "04dbfe67d62b8a4f5a75b2920d81d88b3a04b4a8",
    "line" : 315,
    "diffHunk" : "@@ -1,1 +1315,1319 @@\n\t\tfor _, nodeSecurityGroupID := range lbaas.opts.NodeSecurityGroupIDs {\n\t\t\topts := rules.ListOpts{\n\t\t\t\tDirection:     string(rules.DirIngress),\n\t\t\t\tSecGroupID:    nodeSecurityGroupID,"
  },
  {
    "id" : "0fb49ece-169c-4682-b101-23cbbb51c07b",
    "prId" : 49697,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49697#pullrequestreview-53863674",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7274666a-ff5a-4138-b716-9af3adabd68c",
        "parentId" : null,
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "Do we want to check if `annotationValue` is empty?",
        "createdAt" : "2017-08-02T13:56:15Z",
        "updatedAt" : "2017-08-03T05:47:07Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      },
      {
        "id" : "2640315b-cc59-489c-8e38-d196a1e6a378",
        "parentId" : "7274666a-ff5a-4138-b716-9af3adabd68c",
        "authorId" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "body" : "imo it is good that we do not check that. It allows creating lbaas resources without external floating ip. One use-case for instance is that we have other servers outside kubernetes in same openstack subnet and we want expose services there.\r\n\r\n```\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  labels:\r\n    run: web-nofloating\r\n  name: web-nofloating\r\n  annotations:\r\n    loadbalancer.openstack.org/floating-network-id: \"\"\r\n  namespace: default\r\nspec:\r\n  selector:\r\n    run: web\r\n  ports:\r\n  - port: 80\r\n    name: https\r\n    protocol: TCP\r\n    targetPort: 80\r\n  type: LoadBalancer\r\n```\r\n\r\nthe result is:\r\n```\r\n% kubectl get svc -o wide\r\nNAME             CLUSTER-IP       EXTERNAL-IP                   PORT(S)        AGE       SELECTOR\r\nkubernetes       10.254.0.1       <none>                        443/TCP        3h        <none>\r\nweb-ext          10.254.45.205    192.168.1.86,193.xx.xxx.225   80:31277/TCP   3h        run=web\r\nweb-int          10.254.59.195    192.168.1.88,10.222.130.102   80:31338/TCP   3h        run=web\r\nweb-nofloating   10.254.107.195   192.168.1.89                  80:30913/TCP   27s       run=web\r\n```",
        "createdAt" : "2017-08-02T16:47:02Z",
        "updatedAt" : "2017-08-03T05:47:07Z",
        "lastEditedBy" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "tags" : [
        ]
      },
      {
        "id" : "d0f9a19a-6389-449f-8c70-66c11768f6e8",
        "parentId" : "7274666a-ff5a-4138-b716-9af3adabd68c",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "@zetaab cool, then let's make it clear with comments that this is by design. So folks who come later don't try to \"fix\" things and break this scenarion. I'll `/approve` immediately after. thanks for your patience!",
        "createdAt" : "2017-08-02T16:55:58Z",
        "updatedAt" : "2017-08-03T05:47:07Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef0015a9938a691fe9dea675ba9b274b62a638cc",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +587,591 @@func getStringFromServiceAnnotation(service *v1.Service, annotationKey string, defaultSetting string) string {\n\tglog.V(4).Infof(\"getStringFromServiceAnnotation(%v, %v, %v)\", service, annotationKey, defaultSetting)\n\tif annotationValue, ok := service.Annotations[annotationKey]; ok {\n\t\t//if there is an annotation for this setting, set the \"setting\" var to it\n\t\t// annotationValue can be empty, it is working as designed"
  },
  {
    "id" : "bee15fd4-4300-4797-b3ab-096b103cf985",
    "prId" : 44387,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/44387#pullrequestreview-33975015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf19ec52-09ce-4ca0-9de2-fd123c222d38",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "I remember why I hate working on this code :(\r\n\r\nhttps://github.com/gophercloud/gophercloud/pull/20 suggests that VipPortID will be undefined when `loadbalancer` is the result of the \"create\" branch way above.  Can we confirm that this actually works in the \"LB doesn't already exist\" case on a few recent openstack releases - or perhaps change the create case way above to do a regular \"get\" after the \"create\"?",
        "createdAt" : "2017-04-20T04:04:04Z",
        "updatedAt" : "2017-04-20T04:04:26Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "16898cd2-7508-4ff9-8aa4-e87eee5216fb",
        "parentId" : "cf19ec52-09ce-4ca0-9de2-fd123c222d38",
        "authorId" : "bdc500eb-fd23-4d89-81ce-2986aa831dfa",
        "body" : "It's included in an [integration test](https://github.com/openstack/neutron-lbaas/blob/f99f4f8a6dd5893703bbc66fdbb29d26d73fc23b/neutron_lbaas/tests/unit/db/loadbalancer/test_db_loadbalancerv2.py#L767) so I'm pretty sure `vip_port_id` is returned in the create response. We've also been running this in a dev env for over a week, and things are fixed. \r\n\r\n@coreypobrien are we doing create ops?",
        "createdAt" : "2017-04-20T09:15:08Z",
        "updatedAt" : "2017-04-20T09:15:17Z",
        "lastEditedBy" : "bdc500eb-fd23-4d89-81ce-2986aa831dfa",
        "tags" : [
        ]
      },
      {
        "id" : "8f9df1b6-7361-4d47-94ca-276bfe30bfd4",
        "parentId" : "cf19ec52-09ce-4ca0-9de2-fd123c222d38",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "The change looks good otherwise, so I'm happy to let this go ahead and let it get more widespread testing.  This section of code has been unusually full of dragons, and most of it not caused by us.\r\n\r\n/lgtm",
        "createdAt" : "2017-04-21T06:36:01Z",
        "updatedAt" : "2017-04-24T00:26:45Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "93ec2392-0612-4e67-913b-3b04870be68c",
        "parentId" : "cf19ec52-09ce-4ca0-9de2-fd123c222d38",
        "authorId" : "bdc500eb-fd23-4d89-81ce-2986aa831dfa",
        "body" : "Thanks. Would you mind commenting on the main PR thread, I don't think the merge bot tracks `/lgtm` in inline comments :) ",
        "createdAt" : "2017-04-21T09:06:10Z",
        "updatedAt" : "2017-04-21T09:06:10Z",
        "lastEditedBy" : "bdc500eb-fd23-4d89-81ce-2986aa831dfa",
        "tags" : [
        ]
      }
    ],
    "commit" : "622c69c1e5e01a47b1728457048e78c82d2a8fd0",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +829,833 @@\tstatus.Ingress = []v1.LoadBalancerIngress{{IP: loadbalancer.VipAddress}}\n\n\tportID := loadbalancer.VipPortID\n\tfloatIP, err := getFloatingIPByPortID(lbaas.network, portID)\n\tif err != nil && err != ErrNotFound {"
  },
  {
    "id" : "78f3a1fe-e636-41f9-8d6d-baa3c2893def",
    "prId" : 38959,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38959#pullrequestreview-29720833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1e99ab3-b33f-4ec7-993f-89a9bbb40bc5",
        "parentId" : null,
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "I think this change should also update UpdateLoadBalancer(), like #43056.",
        "createdAt" : "2017-03-21T02:08:57Z",
        "updatedAt" : "2017-05-05T09:36:12Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      },
      {
        "id" : "8a6e929c-ef5c-4960-a8b6-e7ac3044bac2",
        "parentId" : "f1e99ab3-b33f-4ec7-993f-89a9bbb40bc5",
        "authorId" : "5ddb4aea-598c-4eed-998d-444335720d68",
        "body" : "Thank you for your comment @FengyunPan, I'll change UpdateLoadBalancer as soon as posible 👍 ",
        "createdAt" : "2017-03-29T13:30:03Z",
        "updatedAt" : "2017-05-05T09:36:12Z",
        "lastEditedBy" : "5ddb4aea-598c-4eed-998d-444335720d68",
        "tags" : [
        ]
      }
    ],
    "commit" : "a87d34ce40caa085650f0191c74213057fb7c1a7",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +1060,1064 @@\n\t// get all listeners associated with this loadbalancer\n\tlistenerList, err := getListenersByLoadBalancerID(lbaas.network, loadbalancer.ID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error getting LB %s listeners: %v\", loadbalancer.ID, err)"
  },
  {
    "id" : "5713b343-3246-4640-b886-d95706158937",
    "prId" : 38959,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38959#pullrequestreview-28009549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b06a574-d03c-4db3-9dba-353d50d72ba9",
        "parentId" : null,
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "same here, like #43056.",
        "createdAt" : "2017-03-21T02:11:39Z",
        "updatedAt" : "2017-05-05T09:36:12Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      }
    ],
    "commit" : "a87d34ce40caa085650f0191c74213057fb7c1a7",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +1069,1073 @@\tvar monitorIDs []string\n\tfor _, listener := range listenerList {\n\t\tpool, err := getPoolByListenerID(lbaas.network, loadbalancer.ID, listener.ID)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Error getting pool for listener %s: %v\", listener.ID, err)"
  },
  {
    "id" : "2c04818a-aa10-4694-ae88-d9b42ebada3e",
    "prId" : 38959,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38959#pullrequestreview-28009596",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "147ac112-a013-4b8e-a787-93652e0c7a68",
        "parentId" : null,
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "same here, like #43056.",
        "createdAt" : "2017-03-21T02:12:14Z",
        "updatedAt" : "2017-05-05T09:36:12Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      }
    ],
    "commit" : "a87d34ce40caa085650f0191c74213057fb7c1a7",
    "line" : 203,
    "diffHunk" : "@@ -1,1 +1080,1084 @@\tvar memberIDs []string\n\tfor _, pool := range poolIDs {\n\t\tmembersList, err := getMembersByPoolID(lbaas.network, pool)\n\t\tif err != nil && !isNotFound(err) {\n\t\t\treturn fmt.Errorf(\"Error getting pool members %s: %v\", pool, err)"
  },
  {
    "id" : "3e912f6f-2f30-498a-9851-ccc761759ca9",
    "prId" : 33768,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33768#pullrequestreview-4816023",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1800f0f-c3d6-4e58-9c33-73cb3e395a8d",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "`break`\n",
        "createdAt" : "2016-10-11T04:13:27Z",
        "updatedAt" : "2016-11-01T17:31:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "69ed2cd3-eec5-45d4-9503-dd958ea6800c",
        "parentId" : "a1800f0f-c3d6-4e58-9c33-73cb3e395a8d",
        "authorId" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "body" : "Done\n",
        "createdAt" : "2016-10-19T07:37:16Z",
        "updatedAt" : "2016-11-01T17:31:04Z",
        "lastEditedBy" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "tags" : [
        ]
      }
    ],
    "commit" : "b73485b990a6644fdbb7cb624b18e375c1783cc6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +977,981 @@\t\t\t\t\tlbListeners[key] = l\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "4b79301b-a3e1-45ae-b589-24a74ecc1fcd",
    "prId" : 33768,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33768#pullrequestreview-4816027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "205be423-32e8-4669-90f9-006758fb2b43",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "`break`\n",
        "createdAt" : "2016-10-11T04:14:31Z",
        "updatedAt" : "2016-11-01T17:31:04Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "7cff430b-c4b4-4982-880c-2ddd2f9bbfcc",
        "parentId" : "205be423-32e8-4669-90f9-006758fb2b43",
        "authorId" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "body" : "Done\n",
        "createdAt" : "2016-10-19T07:37:20Z",
        "updatedAt" : "2016-11-01T17:31:04Z",
        "lastEditedBy" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "tags" : [
        ]
      }
    ],
    "commit" : "b73485b990a6644fdbb7cb624b18e375c1783cc6",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +999,1003 @@\t\t\t\tfor _, val := range lbListeners {\n\t\t\t\t\tif val.ID == l.ID {\n\t\t\t\t\t\tlbPools[l.ID] = p\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}"
  },
  {
    "id" : "ea76b50a-8515-4dd9-9d1e-cf081f05d55c",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1309048",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c03189ee-0df9-420f-b897-c06c96b8e835",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "It sucks, but you need to `waitLoadbalancerActiveProvisioningStatus(lbaas.network, loadbalancer.ID)` every time you modify the LB, like here.\n",
        "createdAt" : "2016-09-23T03:48:18Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "37ec0c6c-56a7-4e0b-9f00-1d11aa340bec",
        "parentId" : "c03189ee-0df9-420f-b897-c06c96b8e835",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "createLoadBalancer calls it. do you mean something else?\n",
        "createdAt" : "2016-09-23T09:59:13Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      },
      {
        "id" : "82a448cb-c3d0-403e-9364-11059ae5bf52",
        "parentId" : "c03189ee-0df9-420f-b897-c06c96b8e835",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "aha.  Nope, I just mis-read which function you were calling.  Nothing to see here, carry on ;)\n",
        "createdAt" : "2016-09-23T10:55:34Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +527,531 @@\t\t\t// Unknown error, retry later\n\t\t\treturn nil, fmt.Errorf(\"Error creating loadbalancer %s: %v\", name, err)\n\t\t}\n\t} else {\n\t\tglog.V(2).Infof(\"LoadBalancer %s already exists\", name)"
  },
  {
    "id" : "029c236b-00b0-4c65-9a19-95d48bded6c8",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1821869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c907723c-6dfc-42c9-bdb2-a8a7da047645",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Need to clean up (at least) what you have created.\n",
        "createdAt" : "2016-09-23T03:50:56Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "303123cf-e0bc-47ab-94ce-8939df23b5a9",
        "parentId" : "c907723c-6dfc-42c9-bdb2-a8a7da047645",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "any reason why we are not cleaning up between what was here prior to calling ensure lb and what we have created so far?\n",
        "createdAt" : "2016-09-27T20:47:35Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 271,
    "diffHunk" : "@@ -1,1 +555,559 @@\t\t\tif err != nil {\n\t\t\t\t// Unknown error, retry later\n\t\t\t\treturn nil, fmt.Errorf(\"Error creating LB listener: %v\", err)\n\t\t\t}\n\t\t\twaitLoadbalancerActiveProvisioningStatus(lbaas.network, loadbalancer.ID)"
  },
  {
    "id" : "9bebca70-1cf7-442c-94db-287c1cb1b862",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1821869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96e93b70-752d-4383-b5e1-673a117efee5",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Ditto cleanup (and ever other early-return below)\n",
        "createdAt" : "2016-09-23T03:52:56Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "65f0de33-d8ab-41ce-b32b-54e6f3f091b7",
        "parentId" : "96e93b70-752d-4383-b5e1-673a117efee5",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "same here\n",
        "createdAt" : "2016-09-27T20:47:44Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 296,
    "diffHunk" : "@@ -1,1 +568,572 @@\t\tif err != nil && err != ErrNotFound {\n\t\t\t// Unknown error, retry later\n\t\t\treturn nil, fmt.Errorf(\"Error getting pool for listener %s: %v\", listener.ID, err)\n\t\t}\n\t\tif pool == nil {"
  },
  {
    "id" : "ed934d20-236c-4f89-afbc-01cc9ad3d786",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1320594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ca306d9-cd06-46ae-923e-a7a24e6a9d91",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "No need to do a separate lookup:\n\n``` go\noldListeners := loadbalancer.Listeners\n```\n",
        "createdAt" : "2016-09-23T03:56:00Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "9818c391-dbd8-48ad-af63-4ac4cae94dad",
        "parentId" : "5ca306d9-cd06-46ae-923e-a7a24e6a9d91",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "loadbalancer.Listeners does not return correct information about listeners.\n",
        "createdAt" : "2016-09-23T12:38:57Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +539,543 @@\t}\n\n\toldListeners, err := getListenersByLoadBalancerID(lbaas.network, loadbalancer.ID)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error getting LB %s listeners: %v\", name, err)"
  },
  {
    "id" : "ca52bce0-725a-40e9-8024-eb2a47a4964f",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1499838",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d7bdf7f-5f81-4406-b44f-9ec86d2e8067",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "No need for separate network lookup:\n\n``` go\npool := listener.Pools[0]\n// with some error handling for len() == 0 and len() > 1\n```\n",
        "createdAt" : "2016-09-23T03:58:23Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "56fd0de1-4a7f-4ecf-b4bb-944321dc1466",
        "parentId" : "7d7bdf7f-5f81-4406-b44f-9ec86d2e8067",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "Listener.Pools is always empty.\n",
        "createdAt" : "2016-09-26T09:22:20Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 293,
    "diffHunk" : "@@ -1,1 +565,569 @@\t\t// Pop valid listeners.\n\t\toldListeners = popListener(oldListeners, listener.ID)\n\t\tpool, err := getPoolByListenerID(lbaas.network, loadbalancer.ID, listener.ID)\n\t\tif err != nil && err != ErrNotFound {\n\t\t\t// Unknown error, retry later"
  },
  {
    "id" : "4a25da2f-fef7-4a53-8d34-08bc8729242a",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1540057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6f5987a-21a1-4296-ab0c-fd2d3d710d7a",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "``` go\nmembers := pool.Members\n```\n",
        "createdAt" : "2016-09-23T04:01:27Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "69aa3006-07e2-4d80-a547-c0c065c23a8b",
        "parentId" : "a6f5987a-21a1-4296-ab0c-fd2d3d710d7a",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "pool.Members: [{Name: Weight:0 AdminStateUp:false TenantID: SubnetID: PoolID: Address: ProtocolPort:0 ID:54c42604-d6f3-4161-9216-07f4ddb73b8a}]\n\ngetMembersByPoolID: [{Name: Weight:1 AdminStateUp:true TenantID:6b582e30f65f4c05875851b4fc4ed3f1 SubnetID:b18da2d5-3901-422d-acdb-a219ab4c8b28 PoolID: Address:192.168.13.10 ProtocolPort:30461 ID:54c42604-d6f3-4161-9216-07f4ddb73b8a}]\n",
        "createdAt" : "2016-09-26T09:25:50Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      },
      {
        "id" : "e884ae5c-852f-4273-a25c-6fb707886c60",
        "parentId" : "a6f5987a-21a1-4296-ab0c-fd2d3d710d7a",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "ah :(\n",
        "createdAt" : "2016-09-26T14:00:46Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +587,591 @@\n\t\tglog.V(4).Infof(\"Pool for listener %s: %s\", listener.ID, pool.ID)\n\t\tmembers, err := getMembersByPoolID(lbaas.network, pool.ID)\n\t\tif err != nil && !isNotFound(err) {\n\t\t\treturn nil, fmt.Errorf(\"Error getting pool members %s: %v\", pool.ID, err)"
  },
  {
    "id" : "023e4ed6-ac57-4a09-ae62-d9034d10d10a",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1897785",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45ad359c-26f1-45e9-ac23-82f800edcdf4",
        "parentId" : null,
        "authorId" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "body" : "may loss ':'.\n",
        "createdAt" : "2016-09-28T07:38:46Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "5fe5bdb9-52d3-4f2d-8554-f12673e65198",
        "tags" : [
        ]
      },
      {
        "id" : "0f5d8844-2f6d-415e-9905-67b029a85c98",
        "parentId" : "45ad359c-26f1-45e9-ac23-82f800edcdf4",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "What do you mean by this?\n",
        "createdAt" : "2016-09-28T09:30:59Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 477,
    "diffHunk" : "@@ -1,1 +721,725 @@\t\t\tPortID:            portID,\n\t\t}\n\t\tfloatIP, err = floatingips.Create(lbaas.network, floatIPOpts).Extract()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Error creating LB floatingip %+v: %v\", floatIPOpts, err)"
  },
  {
    "id" : "b1d956df-8805-4771-a6b9-3c736875b2df",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-2083848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b19c0f45-a990-4466-85d8-4d00f59a6120",
        "parentId" : null,
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "what happens if the lb already exists but is in degraded state?  If we are not creating a new lb.  We need to ensure this lb operating_status is online and provisioning_status is ACTIVE.\n",
        "createdAt" : "2016-09-28T18:03:48Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      },
      {
        "id" : "afa631cc-ae18-44ab-8770-3fba01b940e9",
        "parentId" : "b19c0f45-a990-4466-85d8-4d00f59a6120",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "I don't think we need to be concerned about operating_status. To ensure provisioning_status, might move waitLoadbalancerActiveProvisioningStatus call out of createLoadBalancer and call in after the else section, that is indeed a good point.\n",
        "createdAt" : "2016-09-29T07:10:31Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      },
      {
        "id" : "64076b53-cdf8-410f-91ba-84552b75e83d",
        "parentId" : "b19c0f45-a990-4466-85d8-4d00f59a6120",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "Implemented.\n",
        "createdAt" : "2016-09-29T07:24:04Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +529,533 @@\t\t}\n\t} else {\n\t\tglog.V(2).Infof(\"LoadBalancer %s already exists\", name)\n\t}\n"
  },
  {
    "id" : "cacd5c04-06e8-487e-a250-3abd34a19695",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-1995157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "594c99c0-d43e-4fdb-84ec-6fa3b5d8933d",
        "parentId" : null,
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "again, we should clean up\n",
        "createdAt" : "2016-09-28T18:04:18Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 213,
    "diffHunk" : "@@ -1,1 +526,530 @@\t\tif err != nil {\n\t\t\t// Unknown error, retry later\n\t\t\treturn nil, fmt.Errorf(\"Error creating loadbalancer %s: %v\", name, err)\n\t\t}\n\t} else {"
  },
  {
    "id" : "a5ecd20c-23fd-4685-a501-c5bd897cfc11",
    "prId" : 33276,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33276#pullrequestreview-2321776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbfeaa39-c42b-4713-9d0e-01ac3584bacb",
        "parentId" : null,
        "authorId" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "body" : "We've just discovered an preexisting issue with the LBaaS v2 API+K8. It is not possible (at least in Liberty) to filter the pool list by loadbalancer, or listener, ID. Neutron will silently ignore the parameter and return all pools from all listeners. \n\nUsing this PR, Liberty OpenStack:\n- Create 2 LB services.\n- Wait for LB's to up+ready\n- Inspect the LBaaS Pool list - Verify there are 2 pools, 1 for each LB.\n- Delete 1 of the LB services.\n- Inspect the LBaaS Pool list - Verify there 0 pools - pools for both loadbalancers have been deleted.\n\n(Don't you just love Neutron's insistence on ignoring unexpected parameters)\n",
        "createdAt" : "2016-09-29T12:06:49Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "tags" : [
        ]
      },
      {
        "id" : "6329e389-c742-46f8-a849-7b7152a9146c",
        "parentId" : "dbfeaa39-c42b-4713-9d0e-01ac3584bacb",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "Did you notice that the pools are filtered by listener ID in the code to append only those with correct listener ID? The function ensures you only get pools that belong to a listener. I'm aware that there are many issues that exist in the current 1.4 code; for example it always deletes all monitors in the project (previously tenant) when EnsureLoadBalancerDeleted is called.\n",
        "createdAt" : "2016-09-29T13:06:30Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      },
      {
        "id" : "627501b1-030d-4e48-8770-f1b66c81c07d",
        "parentId" : "dbfeaa39-c42b-4713-9d0e-01ac3584bacb",
        "authorId" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "body" : "I did, my original (unsubmitted) comment even said this should be OK as it's caught below, but then I tested it and found it continued to delete all the LBaaS pools upon deletion of a single service.\n",
        "createdAt" : "2016-09-29T13:11:53Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "tags" : [
        ]
      },
      {
        "id" : "c1813107-3557-44a9-91bd-2701fe6b06aa",
        "parentId" : "dbfeaa39-c42b-4713-9d0e-01ac3584bacb",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "@kiall now I understand you better. So basically, you're talking about this code in EnsureLoadBalancerDeleted:\n\n```\n// get all pools (and health monitors) associated with this loadbalancer\n    var poolIDs []string\n    var monitorIDs []string\n    err = v2_pools.List(lbaas.network, v2_pools.ListOpts{LoadbalancerID: loadbalancer.ID}).EachPage(func(page pagination.Page) (bool, error) {\n        poolsList, err := v2_pools.ExtractPools(page)\n        if err != nil {\n            return false, err\n        }\n\n        for _, pool := range poolsList {\n            poolIDs = append(poolIDs, pool.ID)\n            monitorIDs = append(monitorIDs, pool.MonitorID)\n        }\n\n        return true, nil\n    })\n    if err != nil {\n        return err\n    }\n```\n\nI will investigate, we are also running liberty. I can include the fix in this PR if confirmed.\n",
        "createdAt" : "2016-09-29T13:14:00Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      },
      {
        "id" : "cbfbac22-fb5d-4f52-8504-de8179c363a1",
        "parentId" : "dbfeaa39-c42b-4713-9d0e-01ac3584bacb",
        "authorId" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "body" : "Possibly - I'm unsure exactly what codepath triggered the deletion, but something did :)\n",
        "createdAt" : "2016-09-29T13:16:14Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "tags" : [
        ]
      },
      {
        "id" : "bbc715a6-42af-4140-8500-41d998b49a54",
        "parentId" : "dbfeaa39-c42b-4713-9d0e-01ac3584bacb",
        "authorId" : "2fbdd721-f744-4998-90ab-86689913771c",
        "body" : "@kiall: I must correct myself, we're currently running mitaka. Can't reproduce the behavior you described. Your problem could be easily solved by adding similar explicit verification for loadbalancer id that getPoolByListenerID function has for listeners. Maybe it should be a separate PR though.\n",
        "createdAt" : "2016-09-29T14:36:04Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "2fbdd721-f744-4998-90ab-86689913771c",
        "tags" : [
        ]
      },
      {
        "id" : "6a121abc-b247-4ed1-afb1-b59d54becab6",
        "parentId" : "dbfeaa39-c42b-4713-9d0e-01ac3584bacb",
        "authorId" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "body" : "Fixed in https://github.com/kubernetes/kubernetes/pull/33768\n",
        "createdAt" : "2016-09-30T13:06:12Z",
        "updatedAt" : "2016-10-02T09:30:09Z",
        "lastEditedBy" : "d8ef36ae-ed7e-4f09-a5d3-3267c3e5a930",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1068c9a495e99913f6f6d26765bee00f3db426d",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +274,278 @@func getPoolByListenerID(client *gophercloud.ServiceClient, loadbalancerID string, listenerID string) (*v2_pools.Pool, error) {\n\tlistenerPools := make([]v2_pools.Pool, 0, 1)\n\terr := v2_pools.List(client, v2_pools.ListOpts{LoadbalancerID: loadbalancerID}).EachPage(func(page pagination.Page) (bool, error) {\n\t\tpoolsList, err := v2_pools.ExtractPools(page)\n\t\tif err != nil {"
  },
  {
    "id" : "d43fbfab-f23f-4ff8-bcd4-c913bd31a8ff",
    "prId" : 31921,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "115e6a06-4bf0-4607-a742-9365ae611209",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "This, and all other early returns below should unwind and cleanup the loadbalancer.\n\n``` go\n_ = lbaas.EnsureLoadBalancerDeleted(clusterName, apiService)\nreturn nil, err\n```\n",
        "createdAt" : "2016-09-07T03:34:04Z",
        "updatedAt" : "2016-10-13T22:42:06Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "9e9567ce-a9b2-46ce-89e2-84aa3cdcc6e5",
        "parentId" : "115e6a06-4bf0-4607-a742-9365ae611209",
        "authorId" : "1a9f8098-2f0f-424f-963d-bdb20ced3db5",
        "body" : "Done\n",
        "createdAt" : "2016-09-07T23:15:54Z",
        "updatedAt" : "2016-10-13T22:42:06Z",
        "lastEditedBy" : "1a9f8098-2f0f-424f-963d-bdb20ced3db5",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac205183d4ccec7020f846432d04893910e5cbfa",
    "line" : null,
    "diffHunk" : "@@ -1,1 +817,821 @@\t\t\t// cleanup what was created so far\n\t\t\t_ = lbaas.EnsureLoadBalancerDeleted(clusterName, apiService)\n\t\t\treturn nil, err\n\t\t}\n"
  },
  {
    "id" : "a6910a67-d9cf-4ec8-aa6d-4a13b76de5ec",
    "prId" : 31921,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c9e0132-72ec-46cd-97db-06dae3afaa61",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Please create a duplicate rule to allow IPv6 too.  We make no effort to filter to only IPv4 addresses in `NodeAddresses` and it's quite possible that traffic goes IPv4 to the LB and then IPv6 to the backend node.\n",
        "createdAt" : "2016-09-07T04:13:27Z",
        "updatedAt" : "2016-10-13T22:42:06Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac205183d4ccec7020f846432d04893910e5cbfa",
    "line" : null,
    "diffHunk" : "@@ -1,1 +488,492 @@\t\tRemoteGroupID: lbSecGroup,\n\t\tSecGroupID:    nodeSecurityGroupID,\n\t\tEtherType:     \"IPv4\",\n\t}\n"
  },
  {
    "id" : "f0a14ea8-ae4b-43fa-b373-402d237058e1",
    "prId" : 31921,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7aacca63-362b-4411-bc66-b9616591fcb8",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Somewhere we need to add a rule to this secgroup to allow (at least) ICMP frag-needed (and perhaps equivalent for IPv6), otherwise PMTUD will be broken.\n",
        "createdAt" : "2016-09-08T00:20:09Z",
        "updatedAt" : "2016-10-13T22:42:06Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac205183d4ccec7020f846432d04893910e5cbfa",
    "line" : 217,
    "diffHunk" : "@@ -1,1 +812,816 @@\t\t}\n\n\t\tlbSecGroup, err := groups.Create(lbaas.network, lbSecGroupCreateOpts).Extract()\n\n\t\tif err != nil {"
  },
  {
    "id" : "6e2cd61d-4823-47e5-8ab8-1cd838310dea",
    "prId" : 31321,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/31321#pullrequestreview-7765258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00056d88-1925-4f7e-b464-3c55bd246a41",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Do you want to filter these on Ready and Schedulable nodes? \n",
        "createdAt" : "2016-10-04T16:39:27Z",
        "updatedAt" : "2016-11-30T22:54:40Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "181e937f-2e99-41f5-b226-3cceea08035f",
        "parentId" : "00056d88-1925-4f7e-b464-3c55bd246a41",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "I don't think so?  I'm given the list of nodes as an argument, and I want to configure the LB with the whole lot and have the LB healthchecks react to \"dynamic\" readyness.\n(If the `UpdateLoadBalancer` API is intended to mean otherwise, please let me know)\n",
        "createdAt" : "2016-10-05T15:03:19Z",
        "updatedAt" : "2016-11-30T22:54:40Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "0cd1d16e-99e9-476d-b6af-b9ff4819e931",
        "parentId" : "00056d88-1925-4f7e-b464-3c55bd246a41",
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "I don't think we do... is GCE filtering them @bprashanth ?\n\nIn any case, that should be a separate issue / PR, not snuck into a refactor PR :-)\n",
        "createdAt" : "2016-10-08T03:19:14Z",
        "updatedAt" : "2016-11-30T22:54:40Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "40839249-5fa2-4bcf-b192-df53e31a0379",
        "parentId" : "00056d88-1925-4f7e-b464-3c55bd246a41",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "This pr replaces the function that checks for unschedulable (hostsFromNodeList) with one that doesn't, I think. Servicecontroller is responsible for understand node readiness/schedulability, which it currently does at listing time via  `getNodeConditionPredicate`. Can you confirm that we aren't regressing anything? I don't want to push the burden of schedulable/unscheduleable/taints/readieness etc onto each cloudprovider module. \n",
        "createdAt" : "2016-10-24T16:25:26Z",
        "updatedAt" : "2016-11-30T22:54:40Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "9224be32-fd03-4bf9-ab74-36930064b338",
        "parentId" : "00056d88-1925-4f7e-b464-3c55bd246a41",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "The PR I had replaced `hostsFromNodeList` (which checks for unschedulable) with `getNodeConditionPredicate` (which checks for unschedulable and ready).  I needed to rewrite/remove `hostsFromNodeList` (because it uses node names) and I felt this was a correct change (in particular it makes `createLoadBalancer` and `nodeSyncLoop` use the same logic) - but you're right, I shouldn't have included that change in logic in this PR.\n\nI've reverted this portion of the code to use the original logic (unschedulable only).\n",
        "createdAt" : "2016-11-09T07:16:11Z",
        "updatedAt" : "2016-11-30T22:54:40Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d479f948adeabf97ba9e6e4f771fb50e1d1935e",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +1032,1036 @@\taddrs := map[string]empty{}\n\tfor _, node := range nodes {\n\t\taddr, err := nodeAddressForLB(node)\n\t\tif err != nil {\n\t\t\treturn err"
  },
  {
    "id" : "20431b31-7363-418a-919f-dec63a07adb2",
    "prId" : 30649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ffcb104-c057-4ce6-8257-c62a6191441a",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "I think using an `addrs` structure shared across all pools/listeners like this fails when the pools get out of sync with each other for some reason.  I think it would be better to have a single loop over `ports` much higher up this function, and do the entire membership update separately for each port/listener/pool.\n\nSo: maybe move this \"add new members\" logic up to the end of the \"foreach pool\" list at L549-550\n",
        "createdAt" : "2016-08-17T02:22:05Z",
        "updatedAt" : "2016-08-23T17:17:45Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "f31b215a-9e64-473a-bca9-0b36c68fc04a",
        "parentId" : "2ffcb104-c057-4ce6-8257-c62a6191441a",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "My understanding is that the UpdateLoadBalancer will ensure the set of members (hosts) for the existing load balancer are up to date.  Since there is one listener and one pool per load balancing port, this will not change in update since it is not intended to once a service has been created.  \n\nThe way this is implemented, the update will work as intended.  It is divided into two parts, the first removing members that are not required anymore and the second part adding new members in the correct listener/pool set.\n\nI will see if this can be optimized some more.\n",
        "createdAt" : "2016-08-17T17:18:17Z",
        "updatedAt" : "2016-08-23T17:17:45Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      },
      {
        "id" : "04a84db9-737b-41cb-811c-f02cd3b4f338",
        "parentId" : "2ffcb104-c057-4ce6-8257-c62a6191441a",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "the update has been cleaned up.  I still have the remove non-required members and add new members as separate pieces.  Reason being when removing members, we don't care which port that member was for, therefore there is no reason to loop through ports then.  When adding new members, since new members are added in respective pools with port information, we need to iterate over the set of ports this lb is serving. \n",
        "createdAt" : "2016-08-18T00:06:09Z",
        "updatedAt" : "2016-08-23T17:17:45Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "97fab82552348d73f9b2e5a97c08d1f24651a5f6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +579,583 @@\n\t\t// Add any new members for this port\n\t\tfor addr := range addrs {\n\t\t\tif _, ok := members[addr]; ok {\n\t\t\t\t// Already exists, do not create member"
  },
  {
    "id" : "7a2762e3-5bb6-4085-8179-4b05a5c7c082",
    "prId" : 30649,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dce98f41-04ad-42ac-9ea5-19fc540416e4",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Sorry if I'm still being dense and misunderstanding the current code flow.\n\nThe scenario I'm imagining is that `UpdateLoadBalancer` is invoked, adds the member to one pool, and then encounters some sort of transient error anywhere in this `foreach ports` loop while working on a later port.  Now, from what I can see, this code will see that this member exists in one pool, set `addrs[addr]` to `false` up on line 542, and then this skip here will ensure we never attempt to add the member to later ports/pools (and thus recover from the earlier failure).\n\nI feel like this `ports` loop and the earlier `poolsLists` loop need to be merged, so that we assess each pool's members individually.\n",
        "createdAt" : "2016-08-19T01:40:07Z",
        "updatedAt" : "2016-08-23T17:17:45Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "11c47d2f-d928-4abc-a56d-cccb4966d0a9",
        "parentId" : "dce98f41-04ad-42ac-9ea5-19fc540416e4",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "we can't do the port loop earlier without having to go over listeners as well.  Members are tied to ports which are tied to pools which are tied to a listener.  I'm not sure I understand the transient error, if such an error occurs the current update will not complete.  This is the same as it was implemented before and the way `v1/UpdateLoadBalancer` is implemented.  \n\nIf you feel this port loop should go earlier, can you propose a solution as I am not understanding your view point.  \n",
        "createdAt" : "2016-08-19T19:01:36Z",
        "updatedAt" : "2016-08-23T17:17:45Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      },
      {
        "id" : "e4a5f18e-26d7-48fe-a6b8-059f93ce2e9f",
        "parentId" : "dce98f41-04ad-42ac-9ea5-19fc540416e4",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Previously we only had one pool, so we were naturally only checking/updating the same pool.\n\nYou can see in [servicecontroller.go](https://github.com/kubernetes/kubernetes/blob/5645ca749b8dde4779422371c04eab6d18d93263/pkg/controller/service/servicecontroller.go#L621) that `UpdateLoadBalancer` is basically called repeatedly and is expected to eventually update the list of hosts correctly - so a transient error will cause the current update to return early, but then the function will be called again (and again..) and expected to still do the right thing with this half-failed state.\n\nI'll send a PR against your branch..\n",
        "createdAt" : "2016-08-22T02:05:10Z",
        "updatedAt" : "2016-08-23T17:17:45Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "f3c9ee69-a628-410c-9bff-b8943b1b4151",
        "parentId" : "dce98f41-04ad-42ac-9ea5-19fc540416e4",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "Thank you for explaining with PR.  I like the adding new members for each port before removing not-required members.  Through testing in OS, the first solution still worked as intended even if a failure happened while looking up existing members since state on members that have been removed/added is not kept.  I will take your recommendation for keeping adding/removing members in the same overall port loop.\n",
        "createdAt" : "2016-08-22T20:36:54Z",
        "updatedAt" : "2016-08-23T17:17:45Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "97fab82552348d73f9b2e5a97c08d1f24651a5f6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +582,586 @@\t\t\tif _, ok := members[addr]; ok {\n\t\t\t\t// Already exists, do not create member\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t_, err := v2_pools.CreateAssociateMember(lbaas.network, pool.ID, v2_pools.MemberCreateOpts{"
  },
  {
    "id" : "d28bbea9-63f8-4d57-9082-bc373a59619f",
    "prId" : 25987,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a92c914b-48ec-43ef-bf98-adfd94d27e91",
        "parentId" : null,
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "Are all these `waitLoadbalancerActiveProvisioningStatus` calls required before proceeding to each next step?  If so, that's a shame :(\n",
        "createdAt" : "2016-05-24T05:10:12Z",
        "updatedAt" : "2016-06-09T01:17:53Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      },
      {
        "id" : "0ac4bcf8-43ed-4c98-9ab0-cce5ef65e97b",
        "parentId" : "a92c914b-48ec-43ef-bf98-adfd94d27e91",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "yeah, unfortunately anytime a change is made to a component, the respective load balancer changes state to \"... pending\".  During this time, any changes made will return with failure bc the load balancer state is not ACTIVE\n",
        "createdAt" : "2016-05-24T07:51:14Z",
        "updatedAt" : "2016-06-09T01:17:53Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      },
      {
        "id" : "a0c39dc7-1b62-4547-ac8a-5535af49ed87",
        "parentId" : "a92c914b-48ec-43ef-bf98-adfd94d27e91",
        "authorId" : "6650fb44-6602-4c80-b0e4-e86b286c410c",
        "body" : "Does the underlying library have a equivalent of `--wait` that does this under the covers?\n",
        "createdAt" : "2016-05-31T23:21:10Z",
        "updatedAt" : "2016-06-09T01:17:53Z",
        "lastEditedBy" : "6650fb44-6602-4c80-b0e4-e86b286c410c",
        "tags" : [
        ]
      },
      {
        "id" : "b1abf43e-0944-47ab-97a9-ba05193178cc",
        "parentId" : "a92c914b-48ec-43ef-bf98-adfd94d27e91",
        "authorId" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "body" : "There is https://godoc.org/github.com/rackspace/gophercloud#WaitFor, but it doesn't do anything more magic than repeatedly polling until a provided predicate returns true (so very similar to `waitLoadbalancerActiveProvisioningStatus`)\n",
        "createdAt" : "2016-06-01T06:12:37Z",
        "updatedAt" : "2016-06-09T01:17:53Z",
        "lastEditedBy" : "58cf89ce-9cc3-4dce-b99b-49ae3682cc9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb1ae12906c7096e69e1140dbdcc67df2f24fc58",
    "line" : 359,
    "diffHunk" : "@@ -1,1 +357,361 @@\t}\n\n\twaitLoadbalancerActiveProvisioningStatus(lbaas.network, loadbalancer.ID)\n\n\tlistener, err := listeners.Create(lbaas.network, listeners.CreateOpts{"
  },
  {
    "id" : "51701817-9bc6-4519-82b7-8f520cf274dd",
    "prId" : 25987,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6f381e3-9652-4d67-b078-56e6caa2e21a",
        "parentId" : null,
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "there is no `name` for pool members according to https://wiki.openstack.org/wiki/Neutron/LBaaS/API_2.0#Add_a_New_Member_to_a_Pool, I got the error \"create failed (client error): Unrecognized attribute(s) 'name'\" when creating a service.\n\nI had to remove this line to be able to successfully create a service with lbv2 on openstack liberty\n",
        "createdAt" : "2016-06-21T01:06:45Z",
        "updatedAt" : "2016-06-21T01:06:45Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      },
      {
        "id" : "14baf931-ef08-4af2-8eef-17f06cfb3a53",
        "parentId" : "d6f381e3-9652-4d67-b078-56e6caa2e21a",
        "authorId" : "89888ad4-0dac-48c1-b890-5f4449a2de2b",
        "body" : "Perhaps member's name is supported in Mitaka and beyond. Can you  verify this?\n",
        "createdAt" : "2016-06-21T22:10:19Z",
        "updatedAt" : "2016-06-21T22:10:19Z",
        "lastEditedBy" : "89888ad4-0dac-48c1-b890-5f4449a2de2b",
        "tags" : [
        ]
      },
      {
        "id" : "2a3a650b-66a3-4bf7-994b-ffb62af5fffa",
        "parentId" : "d6f381e3-9652-4d67-b078-56e6caa2e21a",
        "authorId" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "body" : "This works for latest/Mitaka but will remove it so this will work for liberty as well.\n",
        "createdAt" : "2016-06-21T22:23:06Z",
        "updatedAt" : "2016-06-21T22:23:06Z",
        "lastEditedBy" : "87633202-0ae3-4a2c-8d60-a7c3854b3b8e",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb1ae12906c7096e69e1140dbdcc67df2f24fc58",
    "line" : 397,
    "diffHunk" : "@@ -1,1 +395,399 @@\n\t\t_, err = v2_pools.CreateAssociateMember(lbaas.network, pool.ID, v2_pools.MemberCreateOpts{\n\t\t\tName:         name,\n\t\t\tProtocolPort: int(ports[0].NodePort), //TODO: need to handle multi-port\n\t\t\tAddress:      addr,"
  }
]