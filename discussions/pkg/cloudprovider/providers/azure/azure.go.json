[
  {
    "id" : "6dfdade7-4456-431d-825a-6523f6a5d1bc",
    "prId" : 75282,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75282#pullrequestreview-213735579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68a73e83-b391-4ab1-bf1c-3b023ce4fb83",
        "parentId" : null,
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "this fixed another issue?",
        "createdAt" : "2019-03-12T09:02:37Z",
        "updatedAt" : "2019-03-13T02:30:06Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "4e579b17-966d-42a8-b64f-5d44f95092b0",
        "parentId" : "68a73e83-b391-4ab1-bf1c-3b023ce4fb83",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "No, it's just moving the original condition within standard sku. ExcludeMasterFromStandardLB is only applied for standard LB.",
        "createdAt" : "2019-03-13T02:27:53Z",
        "updatedAt" : "2019-03-13T02:30:06Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "84617c8b510981b3264dd351ea8e92fb1cbc541a",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +330,334 @@\t\t// Do not add master nodes to standard LB by default.\n\t\tif config.ExcludeMasterFromStandardLB == nil {\n\t\t\tconfig.ExcludeMasterFromStandardLB = &defaultExcludeMasterFromStandardLB\n\t\t}\n"
  },
  {
    "id" : "b383dde4-f976-4774-8539-dce3b4ec168d",
    "prId" : 72621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72621#pullrequestreview-240385182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "738d3b50-0a2a-4270-93b2-67a4d2f10101",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Is this one to one mapping with a K8s service of type LoadBalancer ?",
        "createdAt" : "2019-01-07T05:39:44Z",
        "updatedAt" : "2019-01-07T05:39:44Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "1cb95479-ab48-479f-a918-a7d574849b33",
        "parentId" : "738d3b50-0a2a-4270-93b2-67a4d2f10101",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "one to one, or one to many. e.g. if service tags or loadBalancerSourceRanges are set, multiple rules would be created for the service",
        "createdAt" : "2019-01-07T05:55:49Z",
        "updatedAt" : "2019-01-07T05:55:49Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      },
      {
        "id" : "1f851128-1061-4506-9f3b-259a03adf6f2",
        "parentId" : "738d3b50-0a2a-4270-93b2-67a4d2f10101",
        "authorId" : "7257349e-6423-4e10-93c3-2f1e37bc97f5",
        "body" : "Hi Feiskyer,\r\n\r\nMy name's Vinicius Silva from Brazil and I'm following your commits on Kubernetes Respository, thank your for works to make better every day the Kubernetes. Today our company is a  Microsoft partner and we are heaving user of AKS service. Yesterday we found a limit of LoadBalancer to use in Aks (Actual number 250) and we have a little question about.\r\n\r\nYou can help us to understand these limit? Follow the little code part about the limits of loadbalancers:\r\n\r\nFile: azure.go\r\nline: 50:                    maximumLoadBalancerRuleCount = 250\r\nline: 405\r\nif az.MaximumLoadBalancerRuleCount == 0 {\r\naz.MaximumLoadBalancerRuleCount = maximumLoadBalancerRuleCount\r\n}\r\n\r\nIt's possible we use this value on flag for example? Other question is, the LoadBalancer basic are documented on Microsoft docs the limit is 250, if we use Standard version, we can use the 1500 (Rules per resouce) for example?\r\n\r\nImage 1: Load Balancer Documentation (https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits#load-balancer)\r\n",
        "createdAt" : "2019-05-21T16:36:42Z",
        "updatedAt" : "2019-05-21T16:36:42Z",
        "lastEditedBy" : "7257349e-6423-4e10-93c3-2f1e37bc97f5",
        "tags" : [
        ]
      },
      {
        "id" : "19df9723-9ff4-4856-b917-2636ac200f9e",
        "parentId" : "738d3b50-0a2a-4270-93b2-67a4d2f10101",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "yep, it could be configured in azure cloud configure file. just set `maximumLoadBalancerRuleCount` to the new value there.",
        "createdAt" : "2019-05-22T02:38:59Z",
        "updatedAt" : "2019-05-22T02:39:00Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "654ab75bec72bbc1ee9cdb1dbd8ca01e7ab786c7",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +56,60 @@\tbackoffJitterDefault   = 1.0\n\t// According to https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits#load-balancer.\n\tmaximumLoadBalancerRuleCount = 250\n\n\tvmTypeVMSS     = \"vmss\""
  },
  {
    "id" : "62792586-0e57-46a3-ab5e-cb189713c971",
    "prId" : 70866,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70866#pullrequestreview-185883451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08cbf854-fab6-4300-8749-9ab7adfaaf4d",
        "parentId" : null,
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "I would suggest to use sdk retry flavor by default, we have 3 months to test this retry change, and if we don't change the original retry mechanism, I doubt whether aks or acs-engine will use sdk retry flavor, the new retry machanism will never be used.",
        "createdAt" : "2018-11-19T08:44:59Z",
        "updatedAt" : "2018-12-04T03:23:24Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "6c4097f0-8e5b-43e9-b72c-97831470c17c",
        "parentId" : "08cbf854-fab6-4300-8749-9ab7adfaaf4d",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "@brendandburns @khenidak What do you think about the default backoff mode?",
        "createdAt" : "2018-11-19T09:00:08Z",
        "updatedAt" : "2018-12-04T03:23:24Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      },
      {
        "id" : "ab11387e-9bcb-4ec3-b39d-771cebb3d0a4",
        "parentId" : "08cbf854-fab6-4300-8749-9ab7adfaaf4d",
        "authorId" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "body" : "I don't think we should maintain two modes. Do you see a usecase for maintaining the old mode?",
        "createdAt" : "2018-12-17T20:12:21Z",
        "updatedAt" : "2018-12-17T20:19:38Z",
        "lastEditedBy" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "tags" : [
        ]
      },
      {
        "id" : "82bd0a8c-a747-469e-8155-05a44b6af2a1",
        "parentId" : "08cbf854-fab6-4300-8749-9ab7adfaaf4d",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "per @brendandburns suggestion [here](https://github.com/kubernetes/kubernetes/pull/70866#issuecomment-438447463). \r\n\r\n> I'm a little bit worried about taking out the manual configuration knobs and deferring entirely to the SDK.\r\n\r\n> I think that we should both maintain the existing behavior, and implement the new one, and switch between them based on a flag.\r\n\r\n> Since this is a big change, we need to maintain backward/rollback as an option for clusters without reverting the Kubernetes version.\r\n",
        "createdAt" : "2018-12-18T02:20:47Z",
        "updatedAt" : "2018-12-18T02:20:48Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ef7ef8c6eeb4339a4a8111f9c0d2c819e0def55",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +123,127 @@\t// * v2 means only backoff in the Azure SDK is used. In such mode, CloudProviderBackoffDuration and\n\t//   CloudProviderBackoffJitter are omitted.\n\t// \"default\" will be used if not specified.\n\tCloudProviderBackoffMode string `json:\"cloudProviderBackoffMode\" yaml:\"cloudProviderBackoffMode\"`\n\t// Enable rate limiting"
  },
  {
    "id" : "dbcc96fa-ca31-4e5c-8e46-750ef4e46d49",
    "prId" : 68212,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/68212#pullrequestreview-154966867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f002707-6a80-4148-a321-782e69c9041d",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "cc @tallclair @deads2k \r\nfor more cloud provider clients\r\n\r\nI know we don't want roles for these getting added to bootstrap policy. what client should they use to speak to the API?",
        "createdAt" : "2018-09-05T02:57:47Z",
        "updatedAt" : "2018-09-14T02:57:34Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "6b6743c2-ec5c-469a-9388-fe30f7289aed",
        "parentId" : "8f002707-6a80-4148-a321-782e69c9041d",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "@liggitt Didn't get the response yet from @tallclair @deads2k. Any suggestion of candidate role should be used for such cases?",
        "createdAt" : "2018-09-13T07:59:19Z",
        "updatedAt" : "2018-09-14T02:57:34Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "de9c127c9411cb222a2a932638ba1a9f761bcda9",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +393,397 @@// Initialize passes a Kubernetes clientBuilder interface to the cloud provider\nfunc (az *Cloud) Initialize(clientBuilder controller.ControllerClientBuilder) {\n\taz.kubeClient = clientBuilder.ClientOrDie(\"azure-cloud-provider\")\n\taz.eventBroadcaster = record.NewBroadcaster()\n\taz.eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: az.kubeClient.CoreV1().Events(\"\")})"
  },
  {
    "id" : "c29847f1-60b8-4ccc-a91f-f182d3ee43df",
    "prId" : 67604,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67604#pullrequestreview-148358459",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a664e7f6-d8ce-417d-be97-5b34e8fc5ffd",
        "parentId" : null,
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "add more comments here, it locks: nodeZones, nodeResourceGroups, unmanagedNodes",
        "createdAt" : "2018-08-22T06:28:26Z",
        "updatedAt" : "2018-08-23T02:38:58Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "8cb6af8b-98ac-4091-a10a-a19d465e91c6",
        "parentId" : "a664e7f6-d8ce-417d-be97-5b34e8fc5ffd",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "ACK",
        "createdAt" : "2018-08-22T07:22:56Z",
        "updatedAt" : "2018-08-23T02:38:58Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b5101b5630d25f7f996b969ec48e0849b49136a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +160,164 @@\tvmSet                   VMSet\n\n\t// Lock for access to node caches\n\tnodeCachesLock sync.Mutex\n\t// nodeZones is a mapping from Zone to a sets.String of Node's names in the Zone"
  },
  {
    "id" : "03c71114-7ce6-428f-be80-025f723c770f",
    "prId" : 67604,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67604#pullrequestreview-148358880",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d75f788e-1a31-4f65-9754-7daf8c90ffd2",
        "parentId" : null,
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "it's a get operation, is it necessary to put a lock here? I am worried about too many lock here",
        "createdAt" : "2018-08-22T06:32:26Z",
        "updatedAt" : "2018-08-23T02:38:58Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "4d7d2c09-cd74-4dd7-9a13-11ed6fad1315",
        "parentId" : "d75f788e-1a31-4f65-9754-7daf8c90ffd2",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "Yep, map is not thread safe in go",
        "createdAt" : "2018-08-22T07:24:23Z",
        "updatedAt" : "2018-08-23T02:38:58Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b5101b5630d25f7f996b969ec48e0849b49136a",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +578,582 @@\t}\n\n\taz.nodeCachesLock.Lock()\n\tdefer az.nodeCachesLock.Unlock()\n\tif !az.nodeInformerSynced() {"
  },
  {
    "id" : "0c3956b9-7a8a-4293-a052-5d5957ec91a0",
    "prId" : 67604,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67604#pullrequestreview-148742840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d73a7eb5-2494-4428-b27e-4e1c05a5396c",
        "parentId" : null,
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "is it necessary to lock here since unmanagedNodes is a set, not map",
        "createdAt" : "2018-08-23T02:47:54Z",
        "updatedAt" : "2018-08-23T02:54:03Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "caa350cc-38e8-46b8-83d9-7ebd7e9bdc8e",
        "parentId" : "d73a7eb5-2494-4428-b27e-4e1c05a5396c",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "It is still based on map.",
        "createdAt" : "2018-08-23T02:58:55Z",
        "updatedAt" : "2018-08-23T02:58:55Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b5101b5630d25f7f996b969ec48e0849b49136a",
    "line" : 204,
    "diffHunk" : "@@ -1,1 +621,625 @@\t}\n\n\taz.nodeCachesLock.Lock()\n\tdefer az.nodeCachesLock.Unlock()\n\tif !az.nodeInformerSynced() {"
  },
  {
    "id" : "3c32cde6-5eff-4627-a03d-ea43498e78c0",
    "prId" : 66553,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66553#pullrequestreview-141372418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64647cbc-0842-4730-b4b2-3d4ecc61b7dd",
        "parentId" : null,
        "authorId" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "body" : "if the goal is to get the zones used by the cluster for disks then we shouldn't need a watch for that. We can use the cached VM or we can even call `get nodes` every time we need to. Please remove the this watch ",
        "createdAt" : "2018-07-26T00:36:18Z",
        "updatedAt" : "2018-07-30T05:34:22Z",
        "lastEditedBy" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "tags" : [
        ]
      },
      {
        "id" : "88b39e16-ea92-4533-ab2d-1126cbea2242",
        "parentId" : "64647cbc-0842-4730-b4b2-3d4ecc61b7dd",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "This is used to get active zones from nodes. This is recommended way because\r\n\r\n- We don't cache every VM in the cluster (remember we have removed list cache when adding that)\r\n- Informers is a common pattern to get nodes information in kubernetes, it is used in many controllers and stable enough for our cases. Get nodes every time isn't a good idea, it should be cached.\r\n",
        "createdAt" : "2018-07-26T01:50:41Z",
        "updatedAt" : "2018-07-30T05:34:22Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      },
      {
        "id" : "a213e776-6ba1-4f8d-a2b7-8beed785612a",
        "parentId" : "64647cbc-0842-4730-b4b2-3d4ecc61b7dd",
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "@khenidak I think we could use this in general. While I think we should add more logs in this func, node add/update/delete is not a frequent action, we could add more logs for diagnostics.",
        "createdAt" : "2018-07-26T03:17:21Z",
        "updatedAt" : "2018-07-30T05:34:22Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "11c86da7-0b50-4b62-9876-0920959b551e",
        "parentId" : "64647cbc-0842-4730-b4b2-3d4ecc61b7dd",
        "authorId" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "body" : "I don't understand why the code is done elsewhere this way. Having it done this way somewhere else does not make it correct and/or the best way for us. And things might be different elsewhere. I am more concerned with this watch, in this context.\r\n\r\nWatches are nice, but they are expensive. And there are way to many of them currently in the ecosystem. You are essentially establishing additional long running connection to api-server and caching data locally. For an object that does not change that frequently. However due to the node heart beat, this channel will be `needlessly` always busy with updates. My argument is. Having a `current` list of zones, does not require watch. a simple `list-node` will suffice with less code, less cpu cycle and easier to maintain.\r\n\r\nPlease change. ",
        "createdAt" : "2018-07-26T15:33:05Z",
        "updatedAt" : "2018-07-30T05:34:22Z",
        "lastEditedBy" : "0c76e20f-41a5-4725-b3c3-d5b6cae89641",
        "tags" : [
        ]
      },
      {
        "id" : "b8d205d0-4091-45eb-bcdc-52013cf47b3b",
        "parentId" : "64647cbc-0842-4730-b4b2-3d4ecc61b7dd",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "Talked with KAL offline, we agreed to use informers to fetch node's information.",
        "createdAt" : "2018-07-30T01:47:11Z",
        "updatedAt" : "2018-07-30T05:34:22Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "6bfd2be2eaee5f1841dbca2242125ae35bd29450",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +444,448 @@\n// SetInformers sets informers for Azure cloud provider.\nfunc (az *Cloud) SetInformers(informerFactory informers.SharedInformerFactory) {\n\tglog.Infof(\"Setting up informers for Azure cloud provider\")\n\tnodeInformer := informerFactory.Core().V1().Nodes().Informer()"
  },
  {
    "id" : "6f4f257b-593b-4015-909b-bf0031541de5",
    "prId" : 57875,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/57875#pullrequestreview-87053813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0949912-5edc-483d-a7e5-069564dadc5d",
        "parentId" : null,
        "authorId" : "dd7ae991-dbbc-4a29-896b-4334bdee748b",
        "body" : "already passed through 'NewAccountsClientWithBaseURI'? also line338",
        "createdAt" : "2018-01-05T09:52:32Z",
        "updatedAt" : "2018-01-05T09:52:32Z",
        "lastEditedBy" : "dd7ae991-dbbc-4a29-896b-4334bdee748b",
        "tags" : [
        ]
      },
      {
        "id" : "9b525586-1f5d-44b3-a411-58c45f41602d",
        "parentId" : "b0949912-5edc-483d-a7e5-069564dadc5d",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "oops, didn't notice that. thanks",
        "createdAt" : "2018-01-05T23:54:09Z",
        "updatedAt" : "2018-01-05T23:54:09Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "61d6084c97188532eb2e1d2bb964ae7f850b638d",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +329,333 @@\n\tstorageAccountClient := storage.NewAccountsClientWithBaseURI(az.Environment.ResourceManagerEndpoint, az.SubscriptionID)\n\tstorageAccountClient.BaseURI = az.Environment.ResourceManagerEndpoint\n\tstorageAccountClient.Authorizer = autorest.NewBearerAuthorizer(servicePrincipalToken)\n\tstorageAccountClient.PollingDelay = 5 * time.Second"
  },
  {
    "id" : "cfdfa143-01db-4360-9b4e-e3a3ed9d17e3",
    "prId" : 55833,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55833#pullrequestreview-77034475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfb3cc22-cc2c-4a27-9e0a-fb5f3ede2635",
        "parentId" : null,
        "authorId" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "body" : "what is VirtualMachineScaleSetsClient used for?",
        "createdAt" : "2017-11-16T07:34:39Z",
        "updatedAt" : "2017-11-21T07:14:39Z",
        "lastEditedBy" : "e0218e0a-9e55-43f5-8929-04673eea9015",
        "tags" : [
        ]
      },
      {
        "id" : "75527327-8a91-4890-93f7-4c41c453b09d",
        "parentId" : "bfb3cc22-cc2c-4a27-9e0a-fb5f3ede2635",
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "It is not used yet in this PR, but will be used in next one.",
        "createdAt" : "2017-11-16T08:13:24Z",
        "updatedAt" : "2017-11-21T07:14:39Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "924f9a45f317ca45d84a0f3613b850bc27e40ddf",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +201,205 @@\n\t// Clients for vmss.\n\tVirtualMachineScaleSetsClient   compute.VirtualMachineScaleSetsClient\n\tVirtualMachineScaleSetVMsClient compute.VirtualMachineScaleSetVMsClient\n"
  },
  {
    "id" : "eb17650f-b14c-47bd-8f6a-9455664d466e",
    "prId" : 46660,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46660#pullrequestreview-42706457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "604f0ef8-d247-4482-ace5-747fd52418e9",
        "parentId" : null,
        "authorId" : "405e6d07-e29d-4d39-a352-7a5b887f25b5",
        "body" : "read quota is much higher than write, I don't know if you want to differentiate in this change. that potentially give you higher quota. we can improve this later too.",
        "createdAt" : "2017-06-07T19:24:04Z",
        "updatedAt" : "2017-06-07T19:26:03Z",
        "lastEditedBy" : "405e6d07-e29d-4d39-a352-7a5b887f25b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "acb65170f3cf3169090af68532e968fc5f8e30e3",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +41,45 @@\t// CloudProviderName is the value used for the --cloud-provider flag\n\tCloudProviderName      = \"azure\"\n\trateLimitQPSDefault    = 1.0\n\trateLimitBucketDefault = 5\n\tbackoffRetriesDefault  = 6"
  }
]