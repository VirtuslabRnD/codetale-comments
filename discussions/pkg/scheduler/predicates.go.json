[
  {
    "id" : "988ed13a-b252-4501-86f8-30007ef6c17c",
    "prId" : 3043,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ba8793a-37df-4edd-b445-4049b5a33bce",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Go ahead and add this to the default predicates:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/plugin/pkg/scheduler/algorithmprovider/defaults/defaults.go#L31\n",
        "createdAt" : "2014-12-19T00:39:03Z",
        "updatedAt" : "2014-12-19T04:39:41Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "dd924fda-b146-4f87-8a55-f84a150aaf50",
        "parentId" : "4ba8793a-37df-4edd-b445-4049b5a33bce",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done.\n",
        "createdAt" : "2014-12-19T04:21:04Z",
        "updatedAt" : "2014-12-19T04:39:41Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "a8b5598d-654f-4f67-8c52-440b8c634fd9",
        "parentId" : "4ba8793a-37df-4edd-b445-4049b5a33bce",
        "authorId" : "727fc82d-d969-41a4-a614-7fefce94f9a6",
        "body" : "@bgrant0607 would this be a good way to format newly attached GCEPDs/EBS volumes?  In #2609 I suggested having kubelet format a newly attached disk before mounting the filesystem.  I can see a per-node controller performing this action instead.  It could effectively throttle the number of concurrent formatting operations and prevent the host from getting crushed.\n\nYour feedback and advice on #2609 would be invaluable :)\n\nSetting the host in Spec would also easily accommodate the hostDir example laid out in #1515.\n",
        "createdAt" : "2014-12-19T04:30:05Z",
        "updatedAt" : "2014-12-19T04:39:41Z",
        "lastEditedBy" : "727fc82d-d969-41a4-a614-7fefce94f9a6",
        "tags" : [
        ]
      },
      {
        "id" : "37c21a6c-e84c-4f50-9cc7-26101b0ddbfb",
        "parentId" : "4ba8793a-37df-4edd-b445-4049b5a33bce",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "@markturansky I'll take a look at the other PRs.\n",
        "createdAt" : "2014-12-19T04:42:26Z",
        "updatedAt" : "2014-12-19T04:42:26Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e17193161342069d2b818d49ea620a498f1e6e6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +163,167 @@}\n\nfunc PodFitsHost(pod api.Pod, existingPods []api.Pod, node string) (bool, error) {\n\tif len(pod.Spec.Host) == 0 {\n\t\treturn true, nil"
  },
  {
    "id" : "7eee6761-626e-432f-94d8-681ded7ae2c9",
    "prId" : 2906,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ddcb6c2-2392-44ad-a943-0ce882b61875",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Can this be unnested?\n",
        "createdAt" : "2014-12-23T08:27:15Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "bcc7a747-b48d-4a1e-8d70-2f8ac6c10634",
        "parentId" : "7ddcb6c2-2392-44ad-a943-0ce882b61875",
        "authorId" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "body" : "Couldn't come up with a quick solution. I could move the nested code into a function,  but that just improves readability and nothing else. Is that what you were looking for?\n",
        "createdAt" : "2015-01-05T23:01:24Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbac18a909b09f32bbee792fc748a7f97a4079f6",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +281,285 @@\t\t\t}\n\t\t}\n\t}\n\n\t// if there are no existing pods in the service, consider all minions"
  },
  {
    "id" : "5e285e3b-63db-40b5-a327-610e033e8e3f",
    "prId" : 2906,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5a7b785-5951-47ad-904f-505ca9352148",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Should ServiceAffinity ever be a predicate? It doesn't seem like something we should block scheduling to a node for.\n",
        "createdAt" : "2014-12-23T15:47:25Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "b99eac35-6e88-4f1e-b9d7-e3e12537e3d5",
        "parentId" : "f5a7b785-5951-47ad-904f-505ca9352148",
        "authorId" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "body" : "The idea is that pods belonging to the same service should be hosted on minions based on affinity rules specified. For instance, you could specify that all service pods should be hosted on minions belonging to the same \"zone\". Here the \"zone\" definition is left upto the K8s admin/installer and nodes have the \"zone\" label to identify the zone that they belong to. This can be achieved by specifying a specific \"zone\" as NodeRequirement in the pod spec itself. However, it is often desired to pick a zone at random (based on other considerations) and then ensure that all subsequent service pods are scheduled on minions within the same \"zone\".\n",
        "createdAt" : "2015-01-05T23:00:06Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "tags" : [
        ]
      },
      {
        "id" : "1c821721-4bc2-402b-bfcd-d8108ec68a85",
        "parentId" : "f5a7b785-5951-47ad-904f-505ca9352148",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I got a bit confused what you're trying to do here. I guess what you're trying to do is add an implicit selector requiring some particular value V for label L to a pod which does not have any selector for L, if L is listed in the ServiceAffinity object that is passed into the function and some other pod from the same service is already scheduled onto a minion that has value V for label L? If so, please use that as the comment. If not, please explain more clearly.\n\nAs for whether this is useful as a predicate rather than as a priority function, it's hard to say. I suspect a priority function is more useful, since it allows you to say \"spread across as few zones as possible\" and thus can handle the case where you have more pods than you have resources in a single zone. However, I can also see the usefulness of what you implemented here, so I would leave it for now.\n",
        "createdAt" : "2015-01-06T08:28:31Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "347b5e94-3176-45b8-b49d-caddbed68529",
        "parentId" : "f5a7b785-5951-47ad-904f-505ca9352148",
        "authorId" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "body" : "Use Case\nWe would like to ensure affinity for all pods within the same service. This can be achieved using the \"MatchNodeSelector\" predicate and by specifying the label value for pod.Spec.NodeSelector. However, for our use case, what region (as an example) a pod goes in is not relevant, and hence a user may not specify/select a region upfront. What is important is that every pod within the service goes into the same region, ensuring region affinity. Hence, this predicate allows the first pod for the service to be scheduled on a minion within any region (assuming a region is not spefcified in the pod.Spec.NodeSelector) and ensures that any subsequent pods within the service are hosted on minions within the same region as the first pod. Another use case is that you can have affinity at multiple levels. So, you could specify that you would like all pods for a service to be scheduled on minions within the same region and within the same zone (under the region). This is the reason to allow a list of labels to be specified in the ServiceAffinity struct.\n\nImplementation: \nThe levels (region/zone/rack/etc) at which the affinity is applied is governed by the ServiceAffinity struct \"labels\" field. By looking at the minion for already scheduled pods of the same service, we are eliminating the need to specify a region/zone upfront at the time of creating the pod (or replication controller).\n\nNote:\nThere is nothing special about the terms -- region/zone/rack. These are just some terms used commonly within datacenters and hence have been used for ease of understanding. There is nothing preventing an admin from using city/building/room as labels for their infrastructure topological levels. \n\nAlso, I will modify the code to first look at pod.Spec.NodeSelector labels instead of pod.Labels. This allows consistency with the other predicate and avoids confusion/conflcts.\n",
        "createdAt" : "2015-01-07T00:26:59Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "tags" : [
        ]
      },
      {
        "id" : "4bf72243-9738-437c-bfe3-916fdff9f26b",
        "parentId" : "f5a7b785-5951-47ad-904f-505ca9352148",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Yeah, that's consistent with how I understand it, but can you confirm that the slightly more formal way I described the algorithm is accurate? If so, I think it would be good to put that into the comment.\n",
        "createdAt" : "2015-01-07T06:20:16Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "829d80a6-a952-4946-826a-8fc1ca8789cb",
        "parentId" : "f5a7b785-5951-47ad-904f-505ca9352148",
        "authorId" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "body" : "Oh, yes. I will do that as part of my commit - which I haven't submitted yet. Should be in tonight or tomorrow.\n",
        "createdAt" : "2015-01-07T16:47:48Z",
        "updatedAt" : "2015-01-13T18:09:43Z",
        "lastEditedBy" : "4ae12efd-7e98-4c8b-9218-1ed35d1a2df1",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbac18a909b09f32bbee792fc748a7f97a4079f6",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +218,222 @@}\n\nfunc NewServiceAffinityPredicate(podLister PodLister, serviceLister ServiceLister, nodeInfo NodeInfo, labels []string) FitPredicate {\n\taffinity := &ServiceAffinity{\n\t\tpodLister:     podLister,"
  },
  {
    "id" : "c6b3b5ac-bd35-4d34-a52f-b44daea775b7",
    "prId" : 1754,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "edd0d615-6264-4f70-b163-b4f75566aca3",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Can you add some godoc comment explaining why this check is needed/useful?\n",
        "createdAt" : "2014-10-13T18:24:49Z",
        "updatedAt" : "2014-10-13T22:38:28Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "c6b658a2-267c-4a3e-82dc-f5f05a84dd7b",
        "parentId" : "edd0d615-6264-4f70-b163-b4f75566aca3",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done.\n",
        "createdAt" : "2014-10-13T22:38:08Z",
        "updatedAt" : "2014-10-13T22:38:28Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "47c4b8fc98339525a9ec8abdfb214a46912ff566",
    "line" : null,
    "diffHunk" : "@@ -1,1 +73,77 @@// there. This is GCE specific for now.\n// TODO: migrate this into some per-volume specific code?\nfunc NoDiskConflict(pod api.Pod, existingPods []api.Pod, node string) (bool, error) {\n\tmanifest := &(pod.DesiredState.Manifest)\n\tfor ix := range manifest.Volumes {"
  },
  {
    "id" : "9ef2db8e-374c-4b9e-94af-687a30275411",
    "prId" : 1754,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c0400c3-6097-48e0-9595-2b6da79e54ed",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I'm concerned about the precedent of adding scheduler logic for specific disk types here. Ideally, this predicate would be located along with the other GCE code, and there'd be some way to register this predicate for the GCE disk type. Can you add a TODO to this effect if you don't feel like actually producing that mechanism?\n",
        "createdAt" : "2014-10-13T18:30:08Z",
        "updatedAt" : "2014-10-13T22:38:28Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "42508a58-6158-4d96-9067-208be7694f43",
        "parentId" : "8c0400c3-6097-48e0-9595-2b6da79e54ed",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "done.\n",
        "createdAt" : "2014-10-13T22:38:15Z",
        "updatedAt" : "2014-10-13T22:38:28Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "47c4b8fc98339525a9ec8abdfb214a46912ff566",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +56,60 @@\t\treturn false\n\t}\n\tpdName := volume.Source.GCEPersistentDisk.PDName\n\n\tmanifest := &(pod.DesiredState.Manifest)"
  },
  {
    "id" : "838eb252-c645-4800-b3cd-bdbf9094c129",
    "prId" : 1474,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0aa70d29-1dd4-41df-b35b-b472cabaaee9",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Should a pod that requests an unknown resource have that resource ignored, or be unschedulable?\nOr did you plan to handle unknown resources in validation in a future PR?\n",
        "createdAt" : "2014-09-30T22:12:37Z",
        "updatedAt" : "2014-10-01T19:38:20Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "32275371-9068-45b8-abfa-102d40d53163",
        "parentId" : "0aa70d29-1dd4-41df-b35b-b472cabaaee9",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "For now, since pods only have memory and cpu, there are no unknown resources.\n",
        "createdAt" : "2014-09-30T23:44:24Z",
        "updatedAt" : "2014-10-01T19:38:20Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      },
      {
        "id" : "8b0e916b-33b0-4973-b9b7-3d5411c52f13",
        "parentId" : "0aa70d29-1dd4-41df-b35b-b472cabaaee9",
        "authorId" : "0b810b86-bf29-41c9-8254-136b101781dd",
        "body" : "But when other resource types _are_ defined, \"Kubernetes schedulers should ensure that the sum of the resources allocated (requested and granted) to its pods never exceeds the usable capacity of the node.\"  (from resources.md)  This means that a request for a non-zero amount of an unknown (user-defined?) resource type from a node that doesn't have any of those resources will/should fail.\n",
        "createdAt" : "2014-10-01T00:18:41Z",
        "updatedAt" : "2014-10-01T19:38:20Z",
        "lastEditedBy" : "0b810b86-bf29-41c9-8254-136b101781dd",
        "tags" : [
        ]
      }
    ],
    "commit" : "39d03948e78eaeda5bfa96208cd1f8e5972c1826",
    "line" : null,
    "diffHunk" : "@@ -1,1 +43,47 @@\t\tresult.milliCPU += pod.DesiredState.Manifest.Containers[ix].CPU\n\t}\n\treturn result\n}\n"
  }
]