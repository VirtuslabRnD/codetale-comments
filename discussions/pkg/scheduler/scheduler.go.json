[
  {
    "id" : "f69950dd-7cec-4437-9ede-550d965e230a",
    "prId" : 103383,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/103383#pullrequestreview-697686718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d80d75dd-ad55-425a-8dfb-257d58de1995",
        "parentId" : null,
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "why this logic is done two times? since both the `Permit` plugin and `PostBind` plugin are called in a scheduling cycle, cannot it just be done in one place?",
        "createdAt" : "2021-07-01T09:56:20Z",
        "updatedAt" : "2021-07-01T10:31:54Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "9f55a345-7378-4078-b3ff-b847ced6321c",
        "parentId" : "d80d75dd-ad55-425a-8dfb-257d58de1995",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "`PostBind` is done in the binding cycle, i.e., in another goroutine.",
        "createdAt" : "2021-07-01T22:02:59Z",
        "updatedAt" : "2021-07-01T22:03:00Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb9cafc99be94a73d9b92545164dbf336bbd230a",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +679,683 @@\t\t\tfwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)\n\n\t\t\t// At the end of a successful binding cycle, move up Pods if needed.\n\t\t\tif len(podsToActivate.Map) != 0 {\n\t\t\t\tsched.SchedulingQueue.Activate(podsToActivate.Map)"
  },
  {
    "id" : "b5165414-4e22-4850-9a57-250e8dc5716e",
    "prId" : 103383,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/103383#pullrequestreview-700587141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4541a7c1-d517-4cbb-80ae-eb93631bbe7c",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "shouldn't we run this even if we were not successful in scheduling the pod, not only on success? the failed to schedule pod may want another pod to scheduler first and so it may want to activate it.",
        "createdAt" : "2021-07-06T22:39:33Z",
        "updatedAt" : "2021-07-06T22:39:33Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "77cf18df-fab1-48c6-99a4-521b8dcbefb9",
        "parentId" : "4541a7c1-d517-4cbb-80ae-eb93631bbe7c",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "It's debatable but my option is that \"allowing the failed pod to activate pods\" makes the mechanics complicated/chaotic and thus hard to use/debug, sometimes may generate a cycling dependency.\r\n\r\nTake a group of pods as an example, some are \"critical path\" and some are not. I'd like the \"critical\" pod (e.g., a spark driver) to trigger the non-critical (executors) pods' activating; instead of the other way around. In terms of gang scheduling, although the implementation differs, an efficient way to do this is to let the Nth pod enters into waiting, while also activating other pods.\r\n\r\nIn other words, for a group of pods, I'd assume them to be inter-dependent in a unidirectional manner (instead of bidirectional).",
        "createdAt" : "2021-07-07T01:31:43Z",
        "updatedAt" : "2021-07-07T01:31:43Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "a606f8a8-0210-4910-9351-dada77586bdf",
        "parentId" : "4541a7c1-d517-4cbb-80ae-eb93631bbe7c",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Ok, sounds a bit ad-hoc either way. In any case, lets clarify those semantics, I left a comment about that.",
        "createdAt" : "2021-07-07T05:19:31Z",
        "updatedAt" : "2021-07-07T05:19:32Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb9cafc99be94a73d9b92545164dbf336bbd230a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +613,617 @@\n\t// At the end of a successful scheduling cycle, pop and move up Pods if needed.\n\tif len(podsToActivate.Map) != 0 {\n\t\tsched.SchedulingQueue.Activate(podsToActivate.Map)\n\t\t// Clear the entries after activation."
  },
  {
    "id" : "528add77-45e0-4720-96ac-95506ab50007",
    "prId" : 101394,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/101394#pullrequestreview-645264349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "is there any overhead (specifically memory and traffic between the api-server and scheduler) to instantiating this informer? if so, we should have an explicit enable flag in component config that is false by default.",
        "createdAt" : "2021-04-26T12:48:58Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "42d9b251-9d0b-457b-b657-b32814c135aa",
        "parentId" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "In terms of default scheduler, there is no plugin registering CR events, so IMO instantiating a `dynInformerFactory` won't do any concrete work.",
        "createdAt" : "2021-04-26T17:12:00Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "775bb505-e330-467a-9bce-15bac8219518",
        "parentId" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "so the dynamic informer cache is populated only if an event is registered?",
        "createdAt" : "2021-04-26T17:47:44Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "f8123895-8a08-4b86-a376-746ac677bfdc",
        "parentId" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "yes, just like coreInformerFactory, by default the embedded informers are empty.",
        "createdAt" : "2021-04-26T18:08:25Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "0004b42b-9a74-4be4-a5f9-27ca59a0632d",
        "parentId" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "hmm, so someone needs to initiate an informer then, so that makes it even more necessary to pass this factory via framework handler to the plugins that will use it, right?",
        "createdAt" : "2021-04-26T18:58:16Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "ed44614a-a6d9-4e35-81d3-7afec002d038",
        "parentId" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Yes, that's your point at https://github.com/kubernetes/kubernetes/pull/101394#discussion_r620264539. For now, plugin developers instantiate **typed** informer and register event handlers accordingly.",
        "createdAt" : "2021-04-26T19:35:45Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "8d6b1524-d23c-46f9-9736-0b42aaadffb2",
        "parentId" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "It is related, but might be different. If you remember, an informer is initiated only if it is explicitly accessed before starting the the informer. Is this the case here (I don't have experience with dynamic informer, so asking just to make sure)? ",
        "createdAt" : "2021-04-26T23:37:16Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "ffbc14fd-b813-4db2-a40d-9ac014234cd3",
        "parentId" : "f7094870-fa86-458c-884e-4d17c72ad092",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> If you remember, an informer is initiated only if it is explicitly accessed before starting the the informer.\r\n\r\nYes, similar here. For dynamic informer factory, an informer for CR xyz is only initiated when we fetch the gvr of xyz, and call its informer() function. In this PR, it's in the `default` case - i.e., only triggered when a custom plugin implemented EventsToRegister() to show its interests in CR xyz.\r\n\r\nHowever, the framework doesn't really care about the cache for CRs - the out-of-tree plugins are responsible to build their own cache, if needed. The hook setup here is fairly simple: just to guarantee that upon some CR events, partial unschedulable Pods can be moved properly.\r\n\r\n(BTW: I rethink the necessity of offering the dynInformerFactory in framework handle. See my other comment https://github.com/kubernetes/kubernetes/pull/101394#discussion_r620773413)",
        "createdAt" : "2021-04-27T01:31:59Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b3a124ba6049f91175ac2f2b141720af1601ffc",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +292,296 @@\tif options.kubeConfig != nil {\n\t\tdynClient := dynamic.NewForConfigOrDie(options.kubeConfig)\n\t\tdynInformerFactory = dynamicinformer.NewFilteredDynamicSharedInformerFactory(dynClient, 0, v1.NamespaceAll, nil)\n\t}\n"
  },
  {
    "id" : "0c2c0d38-f2bf-4091-b904-f99ca1796201",
    "prId" : 101394,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/101394#pullrequestreview-663563857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41dbe1bd-65a6-4ca1-b119-01c330d7d5e3",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "if plugins want to access CRD resources, it is better if they do through the same informer factory used for registering the events, so we may want to plumb this to the framework handler just like we did for shared informer factory.",
        "createdAt" : "2021-04-26T12:50:14Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "3bc3cbec-e906-4f27-a5de-e0ecc88fe1d0",
        "parentId" : "41dbe1bd-65a6-4ca1-b119-01c330d7d5e3",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Good point. I need to think about it. I'm a bit concerned that would add extra burden for plugin developers, such as fetching the GVR before add event handlers, etc. Probably need to compare it with how controller-runtime obstacles similar problems, and come up with a better abstraction.",
        "createdAt" : "2021-04-26T17:19:07Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "c4ad89f3-751c-4cd3-83a3-4aad10be4ecd",
        "parentId" : "41dbe1bd-65a6-4ca1-b119-01c330d7d5e3",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I rethink it a bit. It's better not to enforce out-of-tree plugins to use dynamic informers as typed ones are more applicable and easy to manipulate, in both core logic and tests.",
        "createdAt" : "2021-04-27T01:30:56Z",
        "updatedAt" : "2021-04-27T16:51:11Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "ed3cece2-91d8-4967-b882-972a600d1822",
        "parentId" : "41dbe1bd-65a6-4ca1-b119-01c330d7d5e3",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "We are not forcing them to use it, but I think it is useful to make it available to them since the cache is already populated for the types they registered events for. If they create there own informer for the same type, aren't we wasting memory because we will have two informerFactor caches?",
        "createdAt" : "2021-05-19T18:51:13Z",
        "updatedAt" : "2021-05-19T18:51:13Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "a042447c-1bb6-4032-aec4-e4cbdd1fa8ef",
        "parentId" : "41dbe1bd-65a6-4ca1-b119-01c330d7d5e3",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> but I think it is useful to make it available to them since the cache is already populated for the types they registered events for\r\n\r\nNot quite sure the plugin developers would really use it, as the typed informer and other generated stuff are way easier to manipulate and performant.\r\n\r\n> If they create their own informer for the same type, aren't we wasting memory because we will have two informerFactory caches?\r\n\r\nYes, this is true. But AFAIK, using dynamic client/informer all the way down can impose overhead as it stores objects as interfaces under the hood, and hence that cause a lot of reflection operations.",
        "createdAt" : "2021-05-19T19:02:43Z",
        "updatedAt" : "2021-05-19T19:05:28Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b3a124ba6049f91175ac2f2b141720af1601ffc",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +288,292 @@\n\t// Build dynamic client and dynamic informer factory\n\tvar dynInformerFactory dynamicinformer.DynamicSharedInformerFactory\n\t// options.kubeConfig can be nil in tests.\n\tif options.kubeConfig != nil {"
  },
  {
    "id" : "b76cc7ea-eb9d-4fb3-a32c-3209ed49b113",
    "prId" : 99644,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99644#pullrequestreview-602033109",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94859c68-a7ea-421c-bf9e-a9ea325ad9af",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "where is this used?",
        "createdAt" : "2021-03-02T15:55:18Z",
        "updatedAt" : "2021-03-09T15:26:14Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "850fdaf0-4916-4bf4-94d2-10f8f5e126b2",
        "parentId" : "94859c68-a7ea-421c-bf9e-a9ea325ad9af",
        "authorId" : "628143cf-73c8-4c51-bd38-d4079089c756",
        "body" : "Updated. cmd will call `WithParallelism` for this parameter.",
        "createdAt" : "2021-03-02T16:43:23Z",
        "updatedAt" : "2021-03-09T15:26:14Z",
        "lastEditedBy" : "628143cf-73c8-4c51-bd38-d4079089c756",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2ceb21a3ef625744284ff6c1fd658c393581e9f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +99,103 @@\textenders                  []schedulerapi.Extender\n\tframeworkCapturer          FrameworkCapturer\n\tparallelism                int32\n}\n"
  },
  {
    "id" : "dce96acb-df87-4e56-a05a-8c188ed0e87d",
    "prId" : 94636,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/94636#pullrequestreview-519032641",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "023d1627-dad0-4cdd-9a10-052efc0e9c8e",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "add a `TODO(name or issue)` here to move to schedulerOptions and stop calling the global setter here",
        "createdAt" : "2020-10-28T13:13:06Z",
        "updatedAt" : "2020-10-29T06:46:42Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "347a82e5-7267-40bc-9b58-0c42be54a2b1",
        "parentId" : "023d1627-dad0-4cdd-9a10-052efc0e9c8e",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "```\r\nTODO(#95952): Remove global setter in favor of a struct that holds the configuration.\r\n```",
        "createdAt" : "2020-10-28T17:20:11Z",
        "updatedAt" : "2020-10-29T06:46:42Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "489ebd4f-a607-4e56-b7e6-c7c5458d83d0",
        "parentId" : "023d1627-dad0-4cdd-9a10-052efc0e9c8e",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Still not solved",
        "createdAt" : "2020-10-28T19:55:00Z",
        "updatedAt" : "2020-10-29T06:46:42Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "1763688d71ba18b333e9e3d190b0f03a513be57f",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +115,119 @@func WithParallelism(threads int32) Option {\n\treturn func(o *schedulerOptions) {\n\t\tparallelize.SetParallelism(int(threads))\n\t}\n}"
  },
  {
    "id" : "4d110a76-c561-40e6-997f-f959a2902d68",
    "prId" : 93831,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93831#pullrequestreview-463871372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49058326-81f6-41ea-be25-09b896b93964",
        "parentId" : null,
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "- put `recordSchedulingFailure` after the `Forget` because if the re-enqueuing & pop-out of that failed Pod happens super fast, the pod update event may still be skipped (see https://github.com/kubernetes/kubernetes/pull/93511#issuecomment-665316132)\r\n- put `ForgetPod` after `RunReservePluginsUnreserve` for consistency (assume -> reserve -> unreserve -> unassume)",
        "createdAt" : "2020-08-09T12:47:18Z",
        "updatedAt" : "2020-08-11T13:28:58Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1176ef9c7d776390c1cbdb8062de1618b00b07c3",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +520,524 @@\t\t\tklog.Errorf(\"scheduler cache ForgetPod failed: %v\", forgetErr)\n\t\t}\n\t\tsched.recordSchedulingFailure(prof, assumedPodInfo, sts.AsError(), SchedulerError, \"\")\n\t\treturn\n\t}"
  },
  {
    "id" : "d1ffd17b-4938-4eef-99fc-89e58bd3a68b",
    "prId" : 93831,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93831#pullrequestreview-464715835",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fddacf99-6290-419f-b7fe-9cd26ca8217d",
        "parentId" : null,
        "authorId" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "body" : "L513: we should probably make `metrics.PodScheduleError` calls be consistently ordered too while we're at it? It's metrics -> recordSchedulingFailure everywhere else except L513.",
        "createdAt" : "2020-08-10T13:36:41Z",
        "updatedAt" : "2020-08-11T13:28:58Z",
        "lastEditedBy" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "tags" : [
        ]
      },
      {
        "id" : "35d6a234-6a08-43c0-9e3f-fccbcf325ed8",
        "parentId" : "fddacf99-6290-419f-b7fe-9cd26ca8217d",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "SGTM",
        "createdAt" : "2020-08-10T13:38:48Z",
        "updatedAt" : "2020-08-11T13:28:58Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      },
      {
        "id" : "4cc457b2-160c-46e6-a7eb-1d67fc580675",
        "parentId" : "fddacf99-6290-419f-b7fe-9cd26ca8217d",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "done",
        "createdAt" : "2020-08-11T02:58:14Z",
        "updatedAt" : "2020-08-11T13:28:58Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1176ef9c7d776390c1cbdb8062de1618b00b07c3",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +511,515 @@\t\treturn\n\t}\n\n\t// Run the Reserve method of reserve plugins.\n\tif sts := prof.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {"
  },
  {
    "id" : "2cd460c1-f9d9-4d7c-bf22-4f537b576604",
    "prId" : 93831,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93831#pullrequestreview-464716467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d5c1dac-ab7f-454b-a255-80fe07000f39",
        "parentId" : null,
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "move `ForgetPod` from `finishBinding` here to follow the function call order on failure (unreserve -> unassume -> recordSchedulingFailure) , see explanation https://github.com/kubernetes/kubernetes/pull/93511#issuecomment-666019546",
        "createdAt" : "2020-08-11T03:00:20Z",
        "updatedAt" : "2020-08-11T13:28:58Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1176ef9c7d776390c1cbdb8062de1618b00b07c3",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +590,594 @@\t\t\tif err := sched.SchedulerCache.ForgetPod(assumedPod); err != nil {\n\t\t\t\tklog.Errorf(\"scheduler cache ForgetPod failed: %v\", err)\n\t\t\t}\n\t\t\tsched.recordSchedulingFailure(prof, assumedPodInfo, fmt.Errorf(\"Binding rejected: %v\", err), SchedulerError, \"\")\n\t\t} else {"
  },
  {
    "id" : "b59df44a-c860-49d0-8e11-a1632d4e19f0",
    "prId" : 93831,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93831#pullrequestreview-464717268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4d3b3587-cb26-46ac-9689-c41bcd5d1e75",
        "parentId" : null,
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "suggested by @adtac, call metrics function before all other functions for consistency. this aligns with other places.",
        "createdAt" : "2020-08-11T03:03:19Z",
        "updatedAt" : "2020-08-11T13:28:58Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1176ef9c7d776390c1cbdb8062de1618b00b07c3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +502,506 @@\terr = sched.assume(assumedPod, scheduleResult.SuggestedHost)\n\tif err != nil {\n\t\tmetrics.PodScheduleError(prof.Name, metrics.SinceInSeconds(start))\n\t\t// This is most probably result of a BUG in retrying logic.\n\t\t// We report an error here so that pod scheduling can be retried."
  },
  {
    "id" : "521e3952-8546-4d19-8775-cd2ff52631c0",
    "prId" : 92892,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92892#pullrequestreview-444978742",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cbfe36c7-2805-4855-8b11-47a5fbde23b6",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "This is preexisting, and maybe I'm misunderstanding, but this makes it seem like you have to enable postfilter plugins for preemption to work. That's pretty surprising to me.",
        "createdAt" : "2020-07-08T17:15:49Z",
        "updatedAt" : "2020-07-08T17:15:49Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "eaead41a-fe5e-4c27-903b-b6b5029a9660",
        "parentId" : "cbfe36c7-2805-4855-8b11-47a5fbde23b6",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> this makes it seem like you have to enable postfilter plugins for preemption to work.\r\n\r\nTrue, it's enabled by default. The background is that scheduler have almost \"pluginize\" all hard-coded logic into plugins. In 1.19 the defaultpreemption plugin follows that direction.\r\n\r\nSo for a default scheduler configuration, preemption functions the same as before. But users can still disable preemption by tweaking the config, just like what they did by specifying `DisablePreemption: false`.",
        "createdAt" : "2020-07-08T17:23:48Z",
        "updatedAt" : "2020-07-08T17:23:48Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "d65a97848e83e21bde6a76feefd385afaea9c1d7",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +469,473 @@\t\tif fitError, ok := err.(*core.FitError); ok {\n\t\t\tif !prof.HasPostFilterPlugins() {\n\t\t\t\tklog.V(3).Infof(\"No PostFilter plugins are registered, so no preemption will be performed.\")\n\t\t\t} else {\n\t\t\t\t// Run PostFilter plugins to try to make the pod schedulable in a future scheduling cycle."
  },
  {
    "id" : "89708235-6d5a-4dde-a319-914d572e976f",
    "prId" : 92391,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92391#pullrequestreview-438471153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d88b0124-c6e4-4b10-b8e4-5d46424fe2c6",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "We need to document that this is called in the interface for ReservePlugin",
        "createdAt" : "2020-06-26T16:34:18Z",
        "updatedAt" : "2020-06-26T20:41:39Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "cd3ae004-8d77-4d5e-a152-440962bda84b",
        "parentId" : "d88b0124-c6e4-4b10-b8e4-5d46424fe2c6",
        "authorId" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "body" : "Good point. Done.",
        "createdAt" : "2020-06-26T17:26:45Z",
        "updatedAt" : "2020-06-26T20:41:39Z",
        "lastEditedBy" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b223b861a3f1eba214b9a609de04235ec70fe0f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +522,526 @@\t\tmetrics.PodScheduleError(prof.Name, metrics.SinceInSeconds(start))\n\t\t// trigger un-reserve to clean up state associated with the reserved Pod\n\t\tprof.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)\n\t\treturn\n\t}"
  },
  {
    "id" : "3daf7aeb-ef68-4949-9055-6a8bf9d08fd5",
    "prId" : 92200,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92200#pullrequestreview-433718724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e3a20bd-63a9-4368-913b-68fc60310213",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Here we should call Unreserve #83557\r\n\r\nWe will need a test for it.",
        "createdAt" : "2020-06-16T21:29:42Z",
        "updatedAt" : "2020-06-24T21:11:11Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "5a23ca1e-40cd-4885-ada8-3fd00f2367ec",
        "parentId" : "9e3a20bd-63a9-4368-913b-68fc60310213",
        "authorId" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "body" : "I'd like to implement the merge of the two plugins first and then handle the case of failing reserves later in a separate PR to keep this PR small, atomic, and contained to one issue.",
        "createdAt" : "2020-06-16T22:49:36Z",
        "updatedAt" : "2020-06-24T21:11:11Z",
        "lastEditedBy" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "tags" : [
        ]
      },
      {
        "id" : "162af8fe-7145-4fc1-acfc-8bc9e2cf776b",
        "parentId" : "9e3a20bd-63a9-4368-913b-68fc60310213",
        "authorId" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "body" : "Could I recreate https://github.com/kubernetes/kubernetes/pull/83557 after this PR will be merged?",
        "createdAt" : "2020-06-17T00:02:12Z",
        "updatedAt" : "2020-06-24T21:11:11Z",
        "lastEditedBy" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "tags" : [
        ]
      },
      {
        "id" : "201cf90d-5334-44ac-97a3-ef54211b657c",
        "parentId" : "9e3a20bd-63a9-4368-913b-68fc60310213",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "@adtac could take it, since he is already modifying the code here. Unless you @mrkm4ntr  have some specific need?",
        "createdAt" : "2020-06-17T14:32:25Z",
        "updatedAt" : "2020-06-24T21:11:11Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "e36a1d08-c359-4233-a337-649f397c2d5d",
        "parentId" : "9e3a20bd-63a9-4368-913b-68fc60310213",
        "authorId" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "body" : "I'd like to see this feature to completion, but if you'd strongly like to work on that feature after this PR, I don't mind :)",
        "createdAt" : "2020-06-17T14:51:48Z",
        "updatedAt" : "2020-06-24T21:11:11Z",
        "lastEditedBy" : "521b6fc2-4b36-4594-8b5b-9d1e5bcc8759",
        "tags" : [
        ]
      },
      {
        "id" : "8c690f40-b0b4-4e11-a6ea-c1a780856170",
        "parentId" : "9e3a20bd-63a9-4368-913b-68fc60310213",
        "authorId" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "body" : "Oh, I misunderstood. I have nothing to do :)",
        "createdAt" : "2020-06-18T23:51:42Z",
        "updatedAt" : "2020-06-24T21:11:11Z",
        "lastEditedBy" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec83143342817f3d6c1a993261adb66010031d34",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +519,523 @@\t// Run the Reserve method of reserve plugins.\n\tif sts := prof.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {\n\t\tsched.recordSchedulingFailure(prof, assumedPodInfo, sts.AsError(), SchedulerError, \"\")\n\t\tmetrics.PodScheduleError(prof.Name, metrics.SinceInSeconds(start))\n\t\treturn"
  },
  {
    "id" : "9629b40b-855a-4bde-bd43-2057acb84b9c",
    "prId" : 91535,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91535#pullrequestreview-420378417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79e6e840-6367-4458-ba3e-586763b3a0f0",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "@ahg-g can you hold this PR until my preemption work is complete? This preempt() method may no longer needed in new codebase.",
        "createdAt" : "2020-05-28T16:23:33Z",
        "updatedAt" : "2020-05-29T00:11:34Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "4df25dbe-7685-413a-9404-64d9f1c09768",
        "parentId" : "79e6e840-6367-4458-ba3e-586763b3a0f0",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "But the concept is the same, the idea is to set the nominated name and the condition in the same patch request. Lets do this in the existing code and see how things will look like when this refactored, what do you think?",
        "createdAt" : "2020-05-28T16:54:36Z",
        "updatedAt" : "2020-05-29T00:11:34Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "b7cd6960-ef09-4e96-a85e-a34208859cf9",
        "parentId" : "79e6e840-6367-4458-ba3e-586763b3a0f0",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "SG. Let's finish this first.",
        "createdAt" : "2020-05-28T20:23:09Z",
        "updatedAt" : "2020-05-29T00:11:34Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "27caa6e7271fc14c7e45270e75835be0bd5f6aa1",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +557,561 @@\t\t\t} else {\n\t\t\t\tpreemptionStartTime := time.Now()\n\t\t\t\tnominatedNode, _ = sched.preempt(schedulingCycleCtx, prof, state, pod, fitError)\n\t\t\t\tmetrics.PreemptionAttempts.Inc()\n\t\t\t\tmetrics.SchedulingAlgorithmPreemptionEvaluationDuration.Observe(metrics.SinceInSeconds(preemptionStartTime))"
  },
  {
    "id" : "9e0b0b51-99a1-4f58-a250-ebf563e90b15",
    "prId" : 91535,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91535#pullrequestreview-420518713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7cf0470a-3f3a-4930-8a27-2304469e69fb",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "are we loosing any context here by removing \"AssumePod failed\"?",
        "createdAt" : "2020-05-28T21:31:22Z",
        "updatedAt" : "2020-05-29T00:11:34Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "97bf53d4-f326-4d26-ae50-fba62fb63e21",
        "parentId" : "7cf0470a-3f3a-4930-8a27-2304469e69fb",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "we are not, the error returned by assume already set the context: https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/scheduler.go#L454",
        "createdAt" : "2020-05-28T21:56:18Z",
        "updatedAt" : "2020-05-29T00:11:34Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "27caa6e7271fc14c7e45270e75835be0bd5f6aa1",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +597,601 @@\t\t// to a node and if so will not add it back to the unscheduled pods queue\n\t\t// (otherwise this would cause an infinite loop).\n\t\tsched.recordSchedulingFailure(prof, assumedPodInfo, err, SchedulerError, \"\")\n\t\tmetrics.PodScheduleErrors.Inc()\n\t\t// trigger un-reserve plugins to clean up state associated with the reserved Pod"
  },
  {
    "id" : "d09f4d43-b4f3-464e-b687-88192ca5594e",
    "prId" : 91535,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91535#pullrequestreview-420573287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1363d3d-37a1-4fe7-a635-b85db6a93e1f",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Although now `updatePod` is much less likely to fail, but if it does, should we revert setting `nominatedNode` here?\r\n\r\n```go\r\nsched.SchedulingQueue.DeleteNominatedPodIfExists(podInfo.Pod)\r\n```",
        "createdAt" : "2020-05-28T23:08:20Z",
        "updatedAt" : "2020-05-29T00:11:34Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "5adb1414-43d6-4581-8e40-947c6d8673ec",
        "parentId" : "c1363d3d-37a1-4fe7-a635-b85db6a93e1f",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I think it is fine, the next update should clear it if something wrong happens.",
        "createdAt" : "2020-05-29T00:10:59Z",
        "updatedAt" : "2020-05-29T00:11:34Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "27caa6e7271fc14c7e45270e75835be0bd5f6aa1",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +378,382 @@\t\tMessage: err.Error(),\n\t}, nominatedNode); err != nil {\n\t\tklog.Errorf(\"Error updating pod %s/%s: %v\", pod.Namespace, pod.Name, err)\n\t}\n}"
  },
  {
    "id" : "638008ee-7e5d-4f94-907c-c3d7d827a8b6",
    "prId" : 90978,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90978#pullrequestreview-411246732",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73bf7299-0efb-4ea0-b5c4-9cc8af99c733",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "what cases would fail that PATCH? I assume only when the version on the server had a different pod condition than the base version we used for the patch, right? meaning if pod.Spec.NodeName was modified, this would still succeed, correct?",
        "createdAt" : "2020-05-12T03:30:02Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "35a1df6e-4b0b-4326-88d6-2168dd9914d1",
        "parentId" : "73bf7299-0efb-4ea0-b5c4-9cc8af99c733",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> if pod.Spec.NodeName was modified, this would still succeed, correct?\r\n\r\nYes. However, this Patch is called only by `recordSchedulingFailure()`, so I don't think .spec.nodeName will be set elsewhere.",
        "createdAt" : "2020-05-12T07:07:20Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "e31b5159-b067-47e5-9c72-f41a9ddeba37",
        "parentId" : "73bf7299-0efb-4ea0-b5c4-9cc8af99c733",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Are we saying that if this fails (there was another change), we still don't cancel the cycle?\r\n\r\nI'm referring to https://github.com/kubernetes/kubernetes/blob/a15168288792daa23c993e551894229161dabdd0/pkg/scheduler/scheduler.go#L452-L456",
        "createdAt" : "2020-05-12T13:55:08Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "c2117e3c-b4ba-4b82-a9af-5eac6ce64cee",
        "parentId" : "73bf7299-0efb-4ea0-b5c4-9cc8af99c733",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "The question above hasn't been resolved yet.",
        "createdAt" : "2020-05-12T20:47:39Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "c1016b87-da2d-46c5-8e9b-a670f7139685",
        "parentId" : "73bf7299-0efb-4ea0-b5c4-9cc8af99c733",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "Using patch, I'm not sure what would cause an error in this case other than some kind of network/infrastructure error. \r\n\r\nA conflict won't occur anymore if something else in the Status has been changed, so the request should no longer ever fail with a 409 status code. \r\n\r\nI'll defer to those who have more experience with the scheduler, but in my opinion if an error here was ignored/logged before, we can just keep ignoring/logging the error.\r\n\r\nBesides, if you wanted to handle this error and cancel the cycle at this point, it would mean undoing everything that has occurred up to that point in the preemption, which I'm not sure would even be reliably possible.\r\n",
        "createdAt" : "2020-05-13T16:39:56Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "b843d2a6-21f6-43c4-a77b-efd013cffd22",
        "parentId" : "73bf7299-0efb-4ea0-b5c4-9cc8af99c733",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> Are we saying that if this fails (there was another change), we still don't cancel the cycle?\r\n\r\nYes. It's like the operation is half-completed - preemptor get its NominatedNodeName set, but the pods that has lower priority and also had NominatedNodeName failed on clearing their NominatedNodeName field. Which is the root cause of this flake.",
        "createdAt" : "2020-05-13T19:47:44Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eb8e7a6d6b90225999092287e4f1a9c996ba894",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +737,741 @@\t\treturn fmt.Errorf(\"failed to create merge patch for pod %q/%q: %v\", pod.Namespace, pod.Name, err)\n\t}\n\t_, err = p.Client.CoreV1().Pods(pod.Namespace).Patch(context.TODO(), pod.Name, types.StrategicMergePatchType, patchBytes, metav1.PatchOptions{}, \"status\")\n\treturn err\n}"
  },
  {
    "id" : "20408125-4191-4cfd-851a-1608662a32c3",
    "prId" : 90978,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90978#pullrequestreview-411141825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "any idea how expensive it is to marshal two objects and create a patch compared to making a PUT on the status directly?",
        "createdAt" : "2020-05-12T03:32:47Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "a4779f8b-acf5-4d57-85fa-44cea757efae",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Can you share a sample of this output? I wonder if we can Marshal smaller portions (like directly Status). Also, we should not use json.Marshal, but get an Encoder from apimachinery packages.",
        "createdAt" : "2020-05-12T13:51:13Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "46c004a9-49f9-4b43-9e7f-a9eba79a58b0",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "Sure, I added some logging and ran the tests to capture the patch content...\r\n\r\nSetting the nominated node:\r\n```\r\n{\"status\":{\"nominatedNodeName\":\"node1\"}}\r\n```\r\n\r\nClearing the nominated node:\r\n```\r\n{\"status\":{\"nominatedNodeName\":null}}\r\n```\r\n\r\nUpdating the pod conditions:\r\n```\r\n{\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2020-05-12T16:18:14Z\",\"message\":\"0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory.\",\"reason\":\"Unschedulable\",\"status\":\"False\",\"type\":\"PodScheduled\"}]}}\r\n```\r\n\r\n---\r\n\r\n> Also, we should not use json.Marshal, but get an Encoder from apimachinery packages.\r\n\r\nAlmost all of the places I looked where `CreateTwoWayMergePatch` is being used seem to be using `json.Marshal` which is why I used it. If you think it is better to use the Encoder from apimachinery, I can change it though.",
        "createdAt" : "2020-05-12T16:25:38Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "b25c5d95-5dde-4362-809a-aaf248e2c6c0",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "I think I see what you're saying though about marshaling smaller portions, can we just do something like this...\r\n```\r\n\toldData, err := json.Marshal(podCopy.Status)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\tpodCopy.Status.NominatedNodeName = nominatedNodeName\r\n\tnewData, err := json.Marshal(podCopy.Status)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\tpatchBytes, err := strategicpatch.CreateTwoWayMergePatch(oldData, newData, &v1.PodStatus{})\r\n```\r\n\r\nLet me give it a try...\r\n\r\nEDIT: This doesn't really work because it gives just `{\"nominatedNodeName\":\"node1\"}`",
        "createdAt" : "2020-05-12T16:27:57Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "a11acf4f-9872-4f1f-bdd2-5422f6e63a08",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "The other thing is we know exactly what is changing, so do we even need to marshal at all?  Can we just construct `{\"status\":{\"nominatedNodeName\":\"node1\"}}` directly?  ",
        "createdAt" : "2020-05-12T16:30:39Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "d228806e-73cf-428e-b73d-d5d7e69ca42e",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "Feels like a premature optimization to me, but I don't know how sensitive scheduler is to performance",
        "createdAt" : "2020-05-12T16:32:19Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "e1ee365d-d803-4a2a-8a2e-f021fa1b9ea1",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "I was reading the documentation of `CreateTwoWayMergePatch`, which is supposed to accept Precondition functions. Are we sure this fails if the initial `nominatedNodeName` doesn't match the original we have?",
        "createdAt" : "2020-05-12T20:44:06Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "9b32e3f4-c831-46da-9876-ef9ac483b280",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "The preconditions are optional and most callers don't use it.  The only one I found that used it was in [editoptions](https://github.com/kubernetes/kubernetes/blob/50c8f73a4b23b18ec0f87a45748a063c740055c4/staging/src/k8s.io/kubectl/pkg/cmd/util/editor/editoptions.go#L617).\r\n\r\nYou're right though, we should make sure the behavior is exactly what is intended. It is using a \"strategic merge\" which I believe does what is described here:\r\nhttps://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#notes-on-the-strategic-merge-patch\r\n\r\nI'll look into this a little more to make sure this will update things as expected, maybe write an integration test around that, or update an existing integration test\r\n",
        "createdAt" : "2020-05-12T21:12:18Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "28a183c8-eb79-4cdb-8e63-3684968be2d1",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "FWIW, the Conditions metadata says this uses the \"merge\" patch strategy, keying on `type`... so I think that means, replacing any existing condition of the same type, but leaving all other existing conditions alone, but I can check to make sure.\r\n\r\n```\r\nConditions []PodCondition `json:\"conditions,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"type\" protobuf:\"bytes,2,rep,name=conditions\"`\r\n```",
        "createdAt" : "2020-05-12T21:15:37Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "5feee8bb-a693-4ec4-9f5b-b343047be858",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Oh.... I see. What about `nominatedNodeName`?",
        "createdAt" : "2020-05-12T21:55:24Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "81c95831-9750-4270-b02f-729890d51502",
        "parentId" : "07ae0a79-a64b-49dd-a875-de9e5a0a6cf1",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "nominatedNodeName doesn't need a patch strategy, since it is not an array... it just replaces it.",
        "createdAt" : "2020-05-13T17:23:53Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eb8e7a6d6b90225999092287e4f1a9c996ba894",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +733,737 @@\t\treturn err\n\t}\n\tpatchBytes, err := strategicpatch.CreateTwoWayMergePatch(oldData, newData, &v1.Pod{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create merge patch for pod %q/%q: %v\", pod.Namespace, pod.Name, err)"
  },
  {
    "id" : "a146baed-f394-4672-9dac-959ce81dc3ae",
    "prId" : 90978,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90978#pullrequestreview-409755372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99ec08f2-d38f-4284-aee7-6a3fd2f00eb9",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "@Huang-Wei  in the original code we returned an error in case the status update failed (i.e., the case that we hit in the test), which means the scheduling cycle of the the high priority pod failed in the test, and later got scheduled again and either evicted the medium pod if there were no enough resources, or simply coexisted with medium pod? This seems to me a reasonable behavior, and that the problem is with the test itself (asserting that NominatedNodeName field getting cleared), not the code, what do you think?",
        "createdAt" : "2020-05-12T03:39:22Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "8956d2c8-d53d-43e7-bacf-54f2f8142969",
        "parentId" : "99ec08f2-d38f-4284-aee7-6a3fd2f00eb9",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "well, the high priority pod is already failed to schedule, that is why we are here in the first place, alas I don't think we are fixing a bug here, I think the test is making a strict assumption, and so, unless the new code is not more expensive than the original one, we should modify the test not the code.",
        "createdAt" : "2020-05-12T03:50:02Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "55eb490a-c10b-49f6-8db8-fc3a3262c017",
        "parentId" : "99ec08f2-d38f-4284-aee7-6a3fd2f00eb9",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "See my comment https://github.com/kubernetes/kubernetes/pull/90978#issuecomment-627153537.",
        "createdAt" : "2020-05-12T07:08:11Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eb8e7a6d6b90225999092287e4f1a9c996ba894",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +773,777 @@\t}\n\t_, err = p.Client.CoreV1().Pods(pod.Namespace).Patch(context.TODO(), pod.Name, types.StrategicMergePatchType, patchBytes, metav1.PatchOptions{}, \"status\")\n\treturn err\n}\n"
  },
  {
    "id" : "1d5082df-1482-423b-a898-57cc8c06cedc",
    "prId" : 90978,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90978#pullrequestreview-411096671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63fce93a-7239-4516-8c79-86f265fcc42e",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "I think we need a unit test for this struct",
        "createdAt" : "2020-05-12T20:50:08Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "5552e2b7-e705-4dc8-90ef-9b4886be7e1a",
        "parentId" : "63fce93a-7239-4516-8c79-86f265fcc42e",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "Definitely agree... I'll add a test around this and also for podPreempterImpl.  I'll wait until we run the integration tests 10 times to make sure this will fix the flake and then add the unit tests",
        "createdAt" : "2020-05-12T20:54:25Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      },
      {
        "id" : "f2fb2ab5-341a-45d9-ab1d-c3b405447fd8",
        "parentId" : "63fce93a-7239-4516-8c79-86f265fcc42e",
        "authorId" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "body" : "Tests added",
        "createdAt" : "2020-05-13T16:27:03Z",
        "updatedAt" : "2020-05-14T19:18:17Z",
        "lastEditedBy" : "5ba8956e-ec1f-452c-959a-ae0daad08298",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eb8e7a6d6b90225999092287e4f1a9c996ba894",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +720,724 @@}\n\nfunc (p *podConditionUpdaterImpl) update(pod *v1.Pod, condition *v1.PodCondition) error {\n\tklog.V(3).Infof(\"Updating pod condition for %s/%s to (%s==%s, Reason=%s)\", pod.Namespace, pod.Name, condition.Type, condition.Status, condition.Reason)\n\toldData, err := json.Marshal(pod)"
  },
  {
    "id" : "8aed6bfd-dd3a-4384-8067-681bff163c25",
    "prId" : 88285,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88285#pullrequestreview-362135073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39dbb68d-4d38-47da-8d56-072389c7b6b8",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Any difference with:\r\n\r\n```go\r\nprofiles: options.profiles,\r\n```\r\n?",
        "createdAt" : "2020-02-20T08:57:41Z",
        "updatedAt" : "2020-02-25T17:40:49Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "d184ca8d-8f18-45ae-bb4e-dcbbc45f07d3",
        "parentId" : "39dbb68d-4d38-47da-8d56-072389c7b6b8",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Yes. This was actually a source of an initial bug I had. The configurator overrides `profiles` with the processed configuration coming from the provider or policy. If we don't do a copy, the `defaultSchedulerOptions` is overwritten.",
        "createdAt" : "2020-02-20T19:04:56Z",
        "updatedAt" : "2020-02-25T17:40:49Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0488584712fe37372f35236d2d6dd0e9f630e26",
    "line" : 131,
    "diffHunk" : "@@ -1,1 +262,266 @@\t\tpodMaxBackoffSeconds:     options.podMaxBackoffSeconds,\n\t\tenableNonPreempting:      utilfeature.DefaultFeatureGate.Enabled(kubefeatures.NonPreemptingPriority),\n\t\tprofiles:                 append([]schedulerapi.KubeSchedulerProfile(nil), options.profiles...),\n\t\tregistry:                 registry,\n\t\tnodeInfoSnapshot:         snapshot,"
  },
  {
    "id" : "4328836d-f65d-446c-9cd9-53ffb791679c",
    "prId" : 88285,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88285#pullrequestreview-364327381",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86c064de-d2ba-4422-ad2b-bc908939f88a",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Add a comment clarifying that the default provider plugins is used by default",
        "createdAt" : "2020-02-25T16:00:55Z",
        "updatedAt" : "2020-02-25T17:40:49Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "66126178-fa7a-4d88-a6be-22e16a02ddea",
        "parentId" : "86c064de-d2ba-4422-ad2b-bc908939f88a",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Done",
        "createdAt" : "2020-02-25T17:44:34Z",
        "updatedAt" : "2020-02-25T17:44:35Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0488584712fe37372f35236d2d6dd0e9f630e26",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +200,204 @@\tprofiles: []schedulerapi.KubeSchedulerProfile{\n\t\t// Profiles' default plugins are set from the algorithm provider.\n\t\t{SchedulerName: v1.DefaultSchedulerName},\n\t},\n\tschedulerAlgorithmSource: schedulerapi.SchedulerAlgorithmSource{"
  },
  {
    "id" : "4852989a-60ef-4691-b35a-aa27c2b80270",
    "prId" : 88199,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/88199#pullrequestreview-360013434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63c42fa8-f398-426a-a2ab-60d90d90df02",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "There is quite a bit of repetition and inconsistency throughout the file in how we handle the errors. I think we should modify recordSchedulingFailure to accommodate all cases (e.g., which metric to increment, run unreserve or not, invoke ForgetPod or not etc.). I will do that in a follow up PR.",
        "createdAt" : "2020-02-18T00:28:43Z",
        "updatedAt" : "2020-02-18T18:04:35Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "d221d82eaf3756a759530bdd332af9ba755151dd",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +657,661 @@\t\t// One of the plugins returned status different than success or wait.\n\t\tfwk.RunUnreservePlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)\n\t\tsched.recordSchedulingFailure(assumedPodInfo, runPermitStatus.AsError(), reason, runPermitStatus.Message())\n\t\treturn\n\t}"
  },
  {
    "id" : "32774aba-94d6-4945-8295-182a662e0177",
    "prId" : 87692,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87692#pullrequestreview-351240420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a4b0b98-6c70-433c-a364-aba1c1e31dec",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "why moving this to here makes a difference?",
        "createdAt" : "2020-01-30T16:46:13Z",
        "updatedAt" : "2020-01-31T08:07:23Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "f8ca1058-bb5a-4dae-8cb2-4447140f114f",
        "parentId" : "6a4b0b98-6c70-433c-a364-aba1c1e31dec",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "oh, because it has to be done before initializing the scheduling_queue",
        "createdAt" : "2020-01-30T16:55:12Z",
        "updatedAt" : "2020-01-31T08:07:23Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "ff075188-d22a-4bbb-afc9-0c31578d491a",
        "parentId" : "6a4b0b98-6c70-433c-a364-aba1c1e31dec",
        "authorId" : "60cf1937-b446-4873-9cbc-7c2ea3ae0a27",
        "body" : "Yes, it is the reason.  I wanted to find a more essential & safe way to `Register` metric instances, though.",
        "createdAt" : "2020-01-31T00:56:57Z",
        "updatedAt" : "2020-01-31T08:07:23Z",
        "lastEditedBy" : "60cf1937-b446-4873-9cbc-7c2ea3ae0a27",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c4be66d3ed697f36d4eb6c20c069a1fdcc6495",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +292,296 @@\t}\n\n\tmetrics.Register()\n\n\tvar sched *Scheduler"
  },
  {
    "id" : "48f69eae-07da-4e7e-b054-ab361f091bd7",
    "prId" : 87297,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87297#pullrequestreview-345493033",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b50707a3-3db2-4d9f-8d1a-c29cc98ea146",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "This gives precedence to the extenders to bind the pod, the original code checked the framework plugins first. I think this makes sense, but it would be good to have a comment clarifying the order.",
        "createdAt" : "2020-01-20T18:36:33Z",
        "updatedAt" : "2020-01-20T19:10:57Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "fea3d63d-4088-42fd-aed3-49c46ba8386d",
        "parentId" : "b50707a3-3db2-4d9f-8d1a-c29cc98ea146",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Indeed. Once we add the default plugin, we have to check extenders first for backwards compatibility.",
        "createdAt" : "2020-01-20T19:11:05Z",
        "updatedAt" : "2020-01-20T19:11:05Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb265bc7db7f11245533e80ce71e674f3f0f86c2",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +512,516 @@// The precedence for binding is: (1) extenders, (2) plugins and (3) default binding.\n// We expect this to run asynchronously, so we handle binding metrics internally.\nfunc (sched *Scheduler) bind(ctx context.Context, assumed *v1.Pod, targetNode string, state *framework.CycleState) (err error) {\n\tstart := time.Now()\n\tdefer func() {"
  },
  {
    "id" : "cc0f74e5-b896-44ae-8277-c3c6eae86106",
    "prId" : 86230,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86230#pullrequestreview-331668004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7abe71ae-4391-495b-b366-a56b56ba52ec",
        "parentId" : null,
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "We can explain in the comments that two cases we could skip scheduling the pod: \r\n\r\n1. deleting pod; \r\n2. pod updates could be skipped.",
        "createdAt" : "2019-12-13T02:21:36Z",
        "updatedAt" : "2019-12-31T06:22:21Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "05784dac-6d90-4c66-a3f2-9fd5f9da069e",
        "parentId" : "7abe71ae-4391-495b-b366-a56b56ba52ec",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "Thanks! I make these cases more clear by updating comments(`Case XXX`). Then we do not need to update function doc when we add more cases. Is it OK?",
        "createdAt" : "2019-12-13T02:57:14Z",
        "updatedAt" : "2019-12-31T06:22:21Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "65a3861f-defa-4fbc-b59c-e4116c8f6059",
        "parentId" : "7abe71ae-4391-495b-b366-a56b56ba52ec",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Yes. SGTM.",
        "createdAt" : "2019-12-13T05:10:47Z",
        "updatedAt" : "2019-12-31T06:22:21Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "706e90a0339fae592a3854849548a5c2010b90ab",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +763,767 @@}\n\n// skipPodSchedule returns true if we could skip scheduling the pod for specified cases.\nfunc (sched *Scheduler) skipPodSchedule(pod *v1.Pod) bool {\n\t// Case 1: pod is being deleted."
  },
  {
    "id" : "0725f987-8b11-41c1-a433-1ec7312bf689",
    "prId" : 86230,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86230#pullrequestreview-337326236",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2ea64b5-c64e-4d61-abb4-315ba95927e4",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "This looks strange because ```sched.skipPodUpdate(pod)``` is checking if the pod is ```DeepEqual``` with itself (which will always be true).\r\n\r\nAs you mention in the comment, this is basically checking if the pod assumed or not, which negates the logic in eventhandlers.go for accepting pod update events: here you are skipping all assumed pods, but the update event handler skips assumed pods _that didn't change_.\r\n\r\nI think the main issue here is that skipPodUpdate does not skip updates for unassumed pods even if they didn't change. So the question that we need to answer is this: can we skip updates for unchanged pods even if they were not assumed? are there cases that prevents us from doing so?",
        "createdAt" : "2019-12-30T20:35:37Z",
        "updatedAt" : "2019-12-31T06:22:21Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "d187e260-de65-40f1-8118-870f7557e72e",
        "parentId" : "f2ea64b5-c64e-4d61-abb4-315ba95927e4",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "> This looks strange because sched.skipPodUpdate(pod) is checking if the pod is DeepEqual with itself (which will always be true).\r\n\r\n`sched.skipPodUpdate` is defined as following. https://github.com/kubernetes/kubernetes/blob/26d491f413e4fe992209e8d68b40d0048ebb2698/pkg/scheduler/eventhandlers.go#L286-L291\r\n\r\nThe pod is compared with the assumed pod which is cached in `schedulerCache.assumedPods`, so it is not checking pods with itself.\r\n\r\n> As you mention in the comment, this is basically checking if the pod assumed or not, which negates the logic in eventhandlers.go for accepting pod update events: here you are skipping all assumed pods, but the update event handler skips assumed pods that didn't change.\r\n\r\nIt does not skip all assumed pods as said above, and eventhandlers.go also calls `sched.skipPodUpdate(pod)` to skip unneeded updates at [updatePodInSchedulingQueue](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/eventhandlers.go#L176).\r\n\r\n> I think the main issue here is that skipPodUpdate does not skip updates for unassumed pods even if they didn't change. So the question that we need to answer is this: can we skip updates for unchanged pods even if they were not assumed? are there cases that prevents us from doing so?\r\n\r\nActually the pod's annotation is changed(which is described at https://github.com/kubernetes/kubernetes/issues/86198), howerver if the pod has been assumed(this case has been resolved in https://github.com/kubernetes/kubernetes/issues/52914) or is being scheduled(I want to address it in this PR), scheduler could skip scheduling it again.  Why we do not skip these updates if the pod is not in above cases? I guess the annotation change might be used by some scheduler plugins, so we do not.",
        "createdAt" : "2019-12-31T02:12:22Z",
        "updatedAt" : "2019-12-31T06:22:21Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "bcc1f3ad-f2f4-486f-98a8-22c399333273",
        "parentId" : "f2ea64b5-c64e-4d61-abb4-315ba95927e4",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "> > This looks strange because sched.skipPodUpdate(pod) is checking if the pod is DeepEqual with itself (which will always be true).\r\n> \r\n> `sched.skipPodUpdate` is defined as following.\r\n> \r\n> https://github.com/kubernetes/kubernetes/blob/26d491f413e4fe992209e8d68b40d0048ebb2698/pkg/scheduler/eventhandlers.go#L286-L291\r\n> \r\n> The pod is compared with the assumed pod which is cached in `schedulerCache.assumedPods`, so it is not checking pods with itself.\r\n> \r\n> > As you mention in the comment, this is basically checking if the pod assumed or not, which negates the logic in eventhandlers.go for accepting pod update events: here you are skipping all assumed pods, but the update event handler skips assumed pods that didn't change.\r\n> \r\n> It does not skip all assumed pods as said above, and eventhandlers.go also calls `sched.skipPodUpdate(pod)` to skip unneeded updates at [updatePodInSchedulingQueue](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/eventhandlers.go#L176).\r\n> \r\n\r\nRight, I missed that we make a deep copy of the pod instance on assume.\r\n\r\n> > I think the main issue here is that skipPodUpdate does not skip updates for unassumed pods even if they didn't change. So the question that we need to answer is this: can we skip updates for unchanged pods even if they were not assumed? are there cases that prevents us from doing so?\r\n> \r\n> Actually the pod's annotation is changed(which is described at #86198), howerver if the pod has been assumed(this case has been resolved in #52914) or is being scheduled(I want to address it in this PR), scheduler could skip scheduling it again. Why we do not skip these updates if the pod is not in above cases? I guess the annotation change might be used by some scheduler plugins, so we do not.\r\n\r\nWhat I was trying to suggest is to find a way to prevent the pod from being added to the queue in the first place for the issue described in  #86198 (e.g., not add a pod that is currently being scheduled to the queue), but it might not be straight forward to achieve.\r\n\r\nIn any case, can you add a comment explaining when we hit this case: \"An assumed pod can be added again to the scheduling queue if it got an update event during its previous scheduling cycle but before getting assumed.\" \r\n\r\n",
        "createdAt" : "2019-12-31T05:59:00Z",
        "updatedAt" : "2019-12-31T06:22:21Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "0914383a-a5bf-447d-87ec-1adc6182869a",
        "parentId" : "f2ea64b5-c64e-4d61-abb4-315ba95927e4",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "> What I was trying to suggest is to find a way to prevent the pod from being added to the queue in the first place for the issue described in #86198 (e.g., not add a pod that is currently being scheduled to the queue), but it might not be straight forward to achieve.\r\n\r\nIt will be great to avoid it in the first place :) I tried to get/record the pod that is being scheduled, howerver as you said, it is not straight forward, e.g. we could add an field that records the pod being scheduled, howerver it does not look good.",
        "createdAt" : "2019-12-31T06:20:01Z",
        "updatedAt" : "2019-12-31T06:22:21Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "f52cf1bc-cae0-47c2-ae6f-fb4338475c5c",
        "parentId" : "f2ea64b5-c64e-4d61-abb4-315ba95927e4",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "> In any case, can you add a comment explaining when we hit this case: \"An assumed pod can be added again to the scheduling queue if it got an update event during its previous scheduling cycle but before getting assumed.\"\r\n\r\nAddressed. PTAL",
        "createdAt" : "2019-12-31T06:22:43Z",
        "updatedAt" : "2019-12-31T06:22:44Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "5c22886d-73d5-4885-9842-4b0f3f3f7621",
        "parentId" : "f2ea64b5-c64e-4d61-abb4-315ba95927e4",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "On second thought, it is not easy to deal with it in the first place. When it is being scheduled, there might not feasible nodes for it, and it will not get assumed, then we do not need to skip the upate.\r\nSo I think here is the right place to address it.",
        "createdAt" : "2019-12-31T08:56:57Z",
        "updatedAt" : "2019-12-31T08:56:57Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      }
    ],
    "commit" : "706e90a0339fae592a3854849548a5c2010b90ab",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +775,779 @@\t// An assumed pod can be added again to the scheduling queue if it got an update event\n\t// during its previous scheduling cycle but before getting assumed.\n\tif sched.skipPodUpdate(pod) {\n\t\treturn true\n\t}"
  },
  {
    "id" : "6b65c623-037c-4ccd-a2c1-2ae129eb2c8b",
    "prId" : 85590,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85590#pullrequestreview-322763871",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2491fe0-752f-4b8b-b31d-190ee3475d50",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "this seem like a valid error. Could you please add a test case in `TestSchedulerCreation`?\r\nOr should it be compatibility test @ahg-g?",
        "createdAt" : "2019-11-25T15:48:50Z",
        "updatedAt" : "2019-12-04T01:22:10Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "29f753ef-f094-4d04-aa4e-0dc036dbef65",
        "parentId" : "d2491fe0-752f-4b8b-b31d-190ee3475d50",
        "authorId" : "ac20bfc0-048b-4f7c-8f40-c004ff5299e1",
        "body" : "> this seem like a valid error. Could you please add a test case in `TestSchedulerCreation`?\r\n> Or should it be compatibility test @ahg-g?\r\n\r\ni add a test case in `TestSchedulerCreation`, please have a review on this, thanks.",
        "createdAt" : "2019-11-26T07:05:47Z",
        "updatedAt" : "2019-12-04T01:22:10Z",
        "lastEditedBy" : "ac20bfc0-048b-4f7c-8f40-c004ff5299e1",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a9db0045121a7d11ed3390e640586ee35ee152c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +304,308 @@\t\t})\n\t}\n\tif err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {\n\t\treturn nil, err\n\t}"
  },
  {
    "id" : "5e71fd2f-5ee5-4feb-aec2-6b4f24aa0c50",
    "prId" : 84859,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84859#pullrequestreview-314723131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfab284a-224d-47ab-9a3b-0a39d5b619ed",
        "parentId" : null,
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "comment need updated.",
        "createdAt" : "2019-11-11T04:28:28Z",
        "updatedAt" : "2019-11-11T04:31:20Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "e6a12966-2e07-489e-b900-328a5a2d8760",
        "parentId" : "dfab284a-224d-47ab-9a3b-0a39d5b619ed",
        "authorId" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "body" : "Thanks @wgliang ^_^   I didn't change the order between `bindVolumes` and `bindPod`. I just mv `bindVolumes` after `permit` plugin because I find it is useless to bind Volumes if the `permit` rejects the pod.",
        "createdAt" : "2019-11-11T07:02:33Z",
        "updatedAt" : "2019-11-11T07:02:33Z",
        "lastEditedBy" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9fda7aac057a4a87efd5091b053821acfc72c86",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +716,720 @@\t\t}\n\n\t\t// Bind volumes first before Pod\n\t\tif !allBound {\n\t\t\terr := sched.bindVolumes(assumedPod)"
  },
  {
    "id" : "14ff5623-4cbb-4cd7-9eb3-135db68e397c",
    "prId" : 84337,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84337#pullrequestreview-307075212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c29b9de-0be5-4292-8fad-b961eee2bbc0",
        "parentId" : null,
        "authorId" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "body" : "is it better to keep using `ctx` instead of `schedulingCycleCtx` for reducing code changes?  \r\n```go\r\nctx, cancel := context.WithCancel(ctx)\r\n```",
        "createdAt" : "2019-10-25T08:52:43Z",
        "updatedAt" : "2019-10-29T14:38:20Z",
        "lastEditedBy" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "tags" : [
        ]
      },
      {
        "id" : "8b2156bc-757a-4e9a-891f-2a5356ee88d1",
        "parentId" : "3c29b9de-0be5-4292-8fad-b961eee2bbc0",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "After schedule cycle finishes, it will call `cancel`.  We are running plugins of binding cycle in another goroutine, they have not finished when schedule cycle finishes. If we use this new `ctx`(or create a new context that uses this new `ctx` as parent context) in binding cycle, it will be also cancelled even it has not finished. So we need keep original `ctx` to be used by binding cycle.",
        "createdAt" : "2019-10-25T09:08:57Z",
        "updatedAt" : "2019-10-29T14:38:20Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      }
    ],
    "commit" : "81b705960f24416e8d560f05b3cc24a6bf370e56",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +615,619 @@\tstart := time.Now()\n\tstate := framework.NewCycleState()\n\tschedulingCycleCtx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tscheduleResult, err := sched.Algorithm.Schedule(schedulingCycleCtx, state, pod)"
  },
  {
    "id" : "97404740-c141-4a40-8be7-e340187643aa",
    "prId" : 83674,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83674#pullrequestreview-300330866",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8f59bd1-97c0-47e6-b7dd-37343fad3b49",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "the klog.Errorf on the previous line is redundant, we already log info on an error inside ```bindVolumes```, perhaps delete this and change that to klog.Error?",
        "createdAt" : "2019-10-10T13:23:22Z",
        "updatedAt" : "2019-10-10T20:59:05Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "6d46aed5-8fd7-485a-82aa-e0c18f0befe4",
        "parentId" : "c8f59bd1-97c0-47e6-b7dd-37343fad3b49",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Removed err from log. Added pod ns/name.",
        "createdAt" : "2019-10-10T15:02:44Z",
        "updatedAt" : "2019-10-10T20:59:05Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "bb3e5fd0-046f-483d-8bc1-d87b0ed3816d",
        "parentId" : "c8f59bd1-97c0-47e6-b7dd-37343fad3b49",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "why do we need to log two lines? lets just do it once here it is easier to grep if it is a single line.",
        "createdAt" : "2019-10-10T20:11:02Z",
        "updatedAt" : "2019-10-10T20:59:05Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "19299eb3-dfe0-4f20-bd35-cce9bbf23c51",
        "parentId" : "c8f59bd1-97c0-47e6-b7dd-37343fad3b49",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "ditto to the one above as well.",
        "createdAt" : "2019-10-10T20:11:15Z",
        "updatedAt" : "2019-10-10T20:59:05Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "2210d95b-d185-4ae0-975a-895d2c8b2b9b",
        "parentId" : "c8f59bd1-97c0-47e6-b7dd-37343fad3b49",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Sorry I didn't realize that the error reason is already provided to recordSchedulingFailure. Removed redundant log lines. And I found an existing redundant log which is removed as well!",
        "createdAt" : "2019-10-10T20:26:41Z",
        "updatedAt" : "2019-10-10T20:59:05Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "085852160a65f123f3697cf5968add822a91a8dd",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +649,653 @@\t\t\terr := sched.bindVolumes(assumedPod)\n\t\t\tif err != nil {\n\t\t\t\tsched.recordSchedulingFailure(assumedPodInfo, err, \"VolumeBindingFailed\", err.Error())\n\t\t\t\tmetrics.PodScheduleErrors.Inc()\n\t\t\t\t// trigger un-reserve plugins to clean up state associated with the reserved Pod"
  },
  {
    "id" : "a67cccf4-37a0-4053-9373-4d980a99a9f2",
    "prId" : 83674,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83674#pullrequestreview-300048256",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62865479-12b9-4d73-85ba-14ea6a1e868f",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "the klog.Error is redundant, we already log error inside assume",
        "createdAt" : "2019-10-10T13:25:17Z",
        "updatedAt" : "2019-10-10T20:59:05Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "085852160a65f123f3697cf5968add822a91a8dd",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +632,636 @@\terr = sched.assume(assumedPod, scheduleResult.SuggestedHost)\n\tif err != nil {\n\t\t// This is most probably result of a BUG in retrying logic.\n\t\t// We report an error here so that pod scheduling can be retried.\n\t\t// This relies on the fact that Error will check if the pod has been bound"
  },
  {
    "id" : "d57a075c-b37f-4e46-be36-ddfe3ab54952",
    "prId" : 83573,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83573#pullrequestreview-298380063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db4248aa-787e-4c4b-abeb-e48748ffae1b",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "can this be unexported already?",
        "createdAt" : "2019-10-07T17:09:54Z",
        "updatedAt" : "2019-10-12T21:04:42Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "af539890-c546-4411-b78c-291ad1bd05e9",
        "parentId" : "db4248aa-787e-4c4b-abeb-e48748ffae1b",
        "authorId" : "690cad9d-7e11-4aad-9d0e-fccf78563a4f",
        "body" : "the only blocker now is the call in cluster-autoscaler.",
        "createdAt" : "2019-10-07T17:23:32Z",
        "updatedAt" : "2019-10-12T21:04:42Z",
        "lastEditedBy" : "690cad9d-7e11-4aad-9d0e-fccf78563a4f",
        "tags" : [
        ]
      },
      {
        "id" : "53812957-04d6-426a-bf87-7a4360d6a406",
        "parentId" : "db4248aa-787e-4c4b-abeb-e48748ffae1b",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Since they are locked to an old version, and we already have an alternative for when they decide to upgrade, I would say this is not a blocker.",
        "createdAt" : "2019-10-07T17:33:49Z",
        "updatedAt" : "2019-10-12T21:04:42Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "8750b706-20c5-4565-9d2f-99f5d76532c1",
        "parentId" : "db4248aa-787e-4c4b-abeb-e48748ffae1b",
        "authorId" : "690cad9d-7e11-4aad-9d0e-fccf78563a4f",
        "body" : "Okay, could be addressed in a separate PR. WDYT?",
        "createdAt" : "2019-10-07T20:04:05Z",
        "updatedAt" : "2019-10-12T21:04:42Z",
        "lastEditedBy" : "690cad9d-7e11-4aad-9d0e-fccf78563a4f",
        "tags" : [
        ]
      },
      {
        "id" : "e1777a08-f778-43f3-97ec-e2685076245c",
        "parentId" : "db4248aa-787e-4c4b-abeb-e48748ffae1b",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "sg",
        "createdAt" : "2019-10-07T20:09:59Z",
        "updatedAt" : "2019-10-12T21:04:42Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "4448a1cea98a3b519ac158347c3c0322cd32577b",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +398,402 @@\n// NewFromConfig returns a new scheduler using the provided Config.\nfunc NewFromConfig(config *Config) *Scheduler {\n\tmetrics.Register()\n\treturn &Scheduler{"
  },
  {
    "id" : "ca2fadeb-5be5-47b1-860b-f5ff651340e0",
    "prId" : 83535,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83535#pullrequestreview-297874869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b799697-99ae-44d7-896b-c4c20fade803",
        "parentId" : null,
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "We create a lot of goroutines in the following methods\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/00458855e487b36ec7356038c6dc7e10a5354018/pkg/scheduler/core/generic_scheduler.go#L681-L689",
        "createdAt" : "2019-10-06T01:59:09Z",
        "updatedAt" : "2019-10-21T08:38:33Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      },
      {
        "id" : "4badf84b-a7b3-4264-aca5-2c6ad014af7d",
        "parentId" : "6b799697-99ae-44d7-896b-c4c20fade803",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Yes, I am more willing to add more stages than just binding.",
        "createdAt" : "2019-10-07T01:19:57Z",
        "updatedAt" : "2019-10-21T08:38:33Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d173852c14f0e8efb0d67db2c38b8dcfa45b31b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +676,680 @@\t// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).\n\tgo func() {\n\t\tmetrics.SchedulerGoroutines.WithLabelValues(\"binding\").Inc()\n\t\tdefer metrics.SchedulerGoroutines.WithLabelValues(\"binding\").Dec()\n"
  },
  {
    "id" : "a470c58d-7e99-440c-a863-236358c105f6",
    "prId" : 83389,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83389#pullrequestreview-296245164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51c66aec-1ade-4706-9c88-ec8bfb744d2d",
        "parentId" : null,
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "if the interface no longer needs to be exported, do the methods?",
        "createdAt" : "2019-10-02T13:39:59Z",
        "updatedAt" : "2019-10-11T20:58:24Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      }
    ],
    "commit" : "6bbc607d7251088f454aee941e4e3bee45f7e8f0",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +64,68 @@// field of the preemptor pod.\n// TODO (ahmad-diaa): Remove type and replace it with scheduler methods\ntype podPreemptor interface {\n\tgetUpdatedPod(pod *v1.Pod) (*v1.Pod, error)\n\tdeletePod(pod *v1.Pod) error"
  },
  {
    "id" : "701c0745-8203-4b0f-9452-6026ad476286",
    "prId" : 83342,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83342#pullrequestreview-296969657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2b27513-11b2-4fa7-bda8-8b8fd08ebb92",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "can't this be a private function of the Scheduler? why do we need the ```PodConditionUpdater``` interface? tests seem to use one that returns always ok.",
        "createdAt" : "2019-10-03T03:19:49Z",
        "updatedAt" : "2019-10-03T03:20:34Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "95e08b02-43d1-40a9-842c-c2936ab806f7",
        "parentId" : "d2b27513-11b2-4fa7-bda8-8b8fd08ebb92",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "We lose the clientset somewhere in the pipeline. I think we can revisit them once we get rid of the Configurator and we can pass a fake clientset to the scheduler.",
        "createdAt" : "2019-10-03T14:37:35Z",
        "updatedAt" : "2019-10-03T14:37:35Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "0bc21040-a209-46c2-92bb-2fcc0907060a",
        "parentId" : "d2b27513-11b2-4fa7-bda8-8b8fd08ebb92",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "We can add clientSet to the Scheduler struct, and use it for the \"pod preemptor\" as well since that interface also seems unnecessary.",
        "createdAt" : "2019-10-03T14:44:54Z",
        "updatedAt" : "2019-10-03T14:45:35Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "10cec3c6-a8d7-42be-bb92-e12b77b3d2fc",
        "parentId" : "d2b27513-11b2-4fa7-bda8-8b8fd08ebb92",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "sgtm",
        "createdAt" : "2019-10-03T15:45:43Z",
        "updatedAt" : "2019-10-03T15:45:43Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c75e1baa255307eb49bf9dae1faccd4c8bee14b",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +681,685 @@}\n\nfunc (p *podConditionUpdaterImpl) update(pod *v1.Pod, condition *v1.PodCondition) error {\n\tklog.V(3).Infof(\"Updating pod condition for %s/%s to (%s==%s, Reason=%s)\", pod.Namespace, pod.Name, condition.Type, condition.Status, condition.Reason)\n\tif podutil.UpdatePodCondition(&pod.Status, condition) {"
  },
  {
    "id" : "f3c954da-f0ec-41bc-ad0a-aa04942f17e2",
    "prId" : 81876,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81876#pullrequestreview-286844392",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86986fa4-13de-468b-a86f-4440aa92300b",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "the framework should be available in `sched.Framework`. Pass `pluginContext` as second argument to for consistency with `schedule`",
        "createdAt" : "2019-08-26T12:48:24Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "c6caab76-a695-4deb-9f37-7bcd5b6f0592",
        "parentId" : "86986fa4-13de-468b-a86f-4440aa92300b",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Like the reply above, I actually prefer to update pass pluginContext as first argument with schedule.",
        "createdAt" : "2019-08-27T02:23:38Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "31daa2f9-67b6-4b61-a3f1-c34b201cdb16",
        "parentId" : "86986fa4-13de-468b-a86f-4440aa92300b",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "I don't disagree, but please do it in a separate PR.",
        "createdAt" : "2019-08-27T12:43:16Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "52df48c4-6600-4b4e-b167-9401e5ff4540",
        "parentId" : "86986fa4-13de-468b-a86f-4440aa92300b",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Sure :)",
        "createdAt" : "2019-08-27T13:13:22Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "270f893f-5718-42e9-879e-a7af876812a2",
        "parentId" : "86986fa4-13de-468b-a86f-4440aa92300b",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "FYI, this is being handled in #82072",
        "createdAt" : "2019-09-11T14:13:40Z",
        "updatedAt" : "2019-09-11T14:13:40Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "5ba890c8-e7d9-4ecf-9959-c8537fb33c17",
        "parentId" : "86986fa4-13de-468b-a86f-4440aa92300b",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "We should do this in a separate small PR because we don't know if #82072 will move forward or not, and if yes in what form, so it may take a little while to merge.",
        "createdAt" : "2019-09-11T14:42:16Z",
        "updatedAt" : "2019-09-11T14:42:16Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "d84a75c1405b8ba0f9e6770b4220bc444a823f5a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +350,354 @@// If it succeeds, it adds the name of the node where preemption has happened to the pod spec.\n// It returns the node name and an error if any.\nfunc (sched *Scheduler) preempt(pluginContext *framework.PluginContext, fwk framework.Framework, preemptor *v1.Pod, scheduleErr error) (string, error) {\n\tpreemptor, err := sched.PodPreemptor.GetUpdatedPod(preemptor)\n\tif err != nil {"
  },
  {
    "id" : "a1101405-e3cd-4a75-adf6-d07be90140ef",
    "prId" : 80811,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80811#pullrequestreview-270477696",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d7c1c9a-d90e-43e9-a1f6-bd59b90ef33a",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "`|| n.Status == nil`?",
        "createdAt" : "2019-08-02T17:26:22Z",
        "updatedAt" : "2019-08-07T20:20:54Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "7ff3c18f-30a1-4e9a-887a-dd4342eb56b6",
        "parentId" : "8d7c1c9a-d90e-43e9-a1f6-bd59b90ef33a",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Status is a struct object who will have default values, see [code](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/core/v1/types.go#L4557)",
        "createdAt" : "2019-08-02T20:31:55Z",
        "updatedAt" : "2019-08-07T20:20:54Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "903da19d-eb34-44a9-b634-f7f1dd8361ec",
        "parentId" : "8d7c1c9a-d90e-43e9-a1f6-bd59b90ef33a",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "oh right, my bad.",
        "createdAt" : "2019-08-02T20:55:32Z",
        "updatedAt" : "2019-08-07T20:20:54Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "18656842-48ca-413c-803e-31678bd058ec",
        "parentId" : "8d7c1c9a-d90e-43e9-a1f6-bd59b90ef33a",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "No worries :)",
        "createdAt" : "2019-08-04T01:45:26Z",
        "updatedAt" : "2019-08-07T20:20:54Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0ade56c4894208133963d816cb66630b6c7413",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +622,626 @@// nodeResourceString returns a string representation of node resources.\nfunc nodeResourceString(n *v1.Node) string {\n\tif n == nil {\n\t\treturn \"N/A\"\n\t}"
  },
  {
    "id" : "419ee4ef-ea0c-4141-a5da-780479a5b8a6",
    "prId" : 80736,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80736#pullrequestreview-268762025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "591809b8-9e8e-4f00-bbe1-dd6c7260e0e3",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "LGTM. I'm curious how often does it happen in your env?",
        "createdAt" : "2019-07-31T02:39:00Z",
        "updatedAt" : "2019-07-31T02:39:01Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "745f17d6-0411-47f5-bc0f-e747b68a95d6",
        "parentId" : "591809b8-9e8e-4f00-bbe1-dd6c7260e0e3",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "sometimes because of lack of the status update permission (customized scheduler deployment)",
        "createdAt" : "2019-07-31T02:52:15Z",
        "updatedAt" : "2019-07-31T02:52:15Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "00afede30d53b3b9bbd656445768550499d07b15",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +273,277 @@\tsched.config.Error(pod, err)\n\tsched.config.Recorder.Eventf(pod, nil, v1.EventTypeWarning, \"FailedScheduling\", \"Scheduling\", message)\n\tif err := sched.config.PodConditionUpdater.Update(pod, &v1.PodCondition{\n\t\tType:    v1.PodScheduled,\n\t\tStatus:  v1.ConditionFalse,"
  },
  {
    "id" : "73e99f42-175c-40b5-bff8-1e7a346296a6",
    "prId" : 80416,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80416#pullrequestreview-264644416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19488428-5358-4b93-9b0f-3b2807c4cbee",
        "parentId" : null,
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "NOTE:\r\n\r\n> \t// Unschedulable is used when a plugin finds a pod unschedulable.\r\n\t// The accompanying status message should explain why the pod is unschedulable.\r\n\tUnschedulable\r\n\r\n> // PodScheduleFailures counts how many pods could not be scheduled.\r\n\r\nGiven the comments, this change looks good.",
        "createdAt" : "2019-07-22T07:42:02Z",
        "updatedAt" : "2019-07-22T07:47:48Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "169537499c3a66f4892d147e217898eb9323c5c5",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +560,564 @@\t\tif !permitStatus.IsSuccess() {\n\t\t\tvar reason string\n\t\t\tif permitStatus.Code() == framework.Unschedulable {\n\t\t\t\tmetrics.PodScheduleFailures.Inc()\n\t\t\t\treason = v1.PodReasonUnschedulable"
  },
  {
    "id" : "b38bbdd4-c009-4f45-b1cd-9608d6a9048d",
    "prId" : 80416,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80416#pullrequestreview-286975691",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fd75c92-0237-49bf-a27b-44c34aa6e593",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "This doesn't look correct. For each scheduling cycle, we should at most log one failure - which is conditionally performed at:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/19e85a9092399b065309aa5b4aec71a5b8101bab/pkg/scheduler/scheduler.go#L504\r\n\r\nWe don't and shouldn't log another failure here.",
        "createdAt" : "2019-08-08T07:10:40Z",
        "updatedAt" : "2019-08-08T07:10:41Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "9f3fae18-fd1b-4c25-8a62-abbbfb8cbe03",
        "parentId" : "4fd75c92-0237-49bf-a27b-44c34aa6e593",
        "authorId" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "body" : "Thank you for your review.\r\n\r\n> For each scheduling cycle, we should at most log one failure - which is conditionally performed at: https://github.com/kubernetes/kubernetes/blob/19e85a9092399b065309aa5b4aec71a5b8101bab/pkg/scheduler/scheduler.go#L504\r\n\r\nBut that path immediately returns, so each scheduling cycle logs at most once. According to the definition of metrics, I think we should increment `scheduleAttempts` with any label in one scheduling cycle.\r\nhttps://github.com/kubernetes/kubernetes/blob/22cf3ca0a536b402a00ca771960702c811a3ca5c/pkg/scheduler/metrics/metrics.go#L58-L63\r\n\r\nIn this case, I think `unschedulable` label is appropriate.",
        "createdAt" : "2019-08-08T09:34:21Z",
        "updatedAt" : "2019-08-09T01:40:28Z",
        "lastEditedBy" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "tags" : [
        ]
      },
      {
        "id" : "3c30381d-a340-49a7-b68d-66999ea149dd",
        "parentId" : "4fd75c92-0237-49bf-a27b-44c34aa6e593",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Hmm... I'm a little surprised we allow/expect Permit plugin to return \"Unschedulable\" b/c Permit/Bind plugins belong to \"Binding Cycle\" which shouldn't be considered as a \"schedule failure\", in my opinion.\r\n\r\nBut based on current codebase, your PR is correct, we should increase PodScheduleFailures metric by 1 in the `if` block. Thanks.\r\n\r\nI will follow up with @ahg-g to see if we should eliminate the logic related with tolerating/expecting Premit/Bind plugins to return \"unschedulable\" status.",
        "createdAt" : "2019-08-09T08:19:45Z",
        "updatedAt" : "2019-08-09T08:19:45Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "b8a81a04-f590-4dcb-8222-dd872d339ea4",
        "parentId" : "4fd75c92-0237-49bf-a27b-44c34aa6e593",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Yeah, Permit unfortunately is different. When a pod is rejected (by other pod, or timeout) this is not considered a scheduling Error, and so we used unschedulable. Do you have other suggestions we might be able to handle this case?",
        "createdAt" : "2019-09-11T18:07:32Z",
        "updatedAt" : "2019-09-11T18:07:39Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "169537499c3a66f4892d147e217898eb9323c5c5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +561,565 @@\t\t\tvar reason string\n\t\t\tif permitStatus.Code() == framework.Unschedulable {\n\t\t\t\tmetrics.PodScheduleFailures.Inc()\n\t\t\t\treason = v1.PodReasonUnschedulable\n\t\t\t} else {"
  },
  {
    "id" : "0c967157-9131-4402-a5a1-a3b30c6e7afc",
    "prId" : 80254,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80254#pullrequestreview-265144539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "557c9805-9f69-4db1-a530-3fc6642bb377",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "It does not really make a difference technically, but it make more sense to first reject the pod and then delete it, this is semantically the right thing to do.",
        "createdAt" : "2019-07-21T11:08:52Z",
        "updatedAt" : "2019-07-23T02:25:26Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "754fb4dc-2a0a-4e54-b123-2dbeb01cc223",
        "parentId" : "557c9805-9f69-4db1-a530-3fc6642bb377",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "Two pros for it from my perspective:\r\n\r\n*  Let the permit plugin returns early after it receives `Reject` message, instead of waiting timeout.   \r\n\r\n* When writing a [coscheduling](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/34-20180703-coscheduling.md) plugin which will permit pods until the number of pods reaches a certain number. Suppose that it requires 3 pods to run together, the permit plugin will return `Wait` for the first 2 pods, when it tries to permit the 3rd pod, it will find that there have been 2 pods in the waiting pod queue, then it will [`Allow`](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/framework/v1alpha1/waiting_pods_map.go#L91) them all. It does not know whether or not they have been preempted. We need to add some check for it. With the PR, we could avoid these extra check.",
        "createdAt" : "2019-07-21T13:14:38Z",
        "updatedAt" : "2019-07-23T02:25:26Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "c3c8c2fa-f2cc-414e-b601-5790744f1bbf",
        "parentId" : "557c9805-9f69-4db1-a530-3fc6642bb377",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "sorry, I guess my comment was not clear, I agree we should do this, I am just suggesting to do the rejection  before deleting the pod (i.e., move it up to line 327).",
        "createdAt" : "2019-07-21T13:34:14Z",
        "updatedAt" : "2019-07-23T02:25:26Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "505da4ad-df75-4fb5-a3b3-5a9bc8559de8",
        "parentId" : "557c9805-9f69-4db1-a530-3fc6642bb377",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : ":) It might fail to delete the pod(`sched.config.PodPreemptor.DeletePod()` might return an error), then we do not need to reject it, so I think we could reject after deleting successfully? ",
        "createdAt" : "2019-07-22T01:17:01Z",
        "updatedAt" : "2019-07-23T02:25:26Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "700847e4-79a4-454f-96d1-e5956252a76e",
        "parentId" : "557c9805-9f69-4db1-a530-3fc6642bb377",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "right, it is just that doing it in this ordered will look strange in the logs, we will see something like: \r\n- pod x deleted \r\n- pod x rejected\r\n\r\nI don't have strong opinion on this though, so I am fine with this implementation.\r\n\r\n",
        "createdAt" : "2019-07-22T16:36:56Z",
        "updatedAt" : "2019-07-23T02:25:26Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "d6cc8f8c-8dfe-4ee4-b800-92170aeaf2f3",
        "parentId" : "557c9805-9f69-4db1-a530-3fc6642bb377",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "Yes, it will be a little strange if there are logs. There is no reject log, so it is not a problem now. :)",
        "createdAt" : "2019-07-23T02:29:31Z",
        "updatedAt" : "2019-07-23T02:29:31Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      }
    ],
    "commit" : "df14adf474f2b5b1277f88f7cdacae271dd1b423",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +329,333 @@\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\t// If the victim is a WaitingPod, send a reject message to the PermitPlugin\n\t\t\tif waitingPod := fwk.GetWaitingPod(victim.UID); waitingPod != nil {\n\t\t\t\twaitingPod.Reject(\"preempted\")"
  },
  {
    "id" : "7c1c7bb1-71ab-4f03-9f10-e90c4b5272f2",
    "prId" : 80254,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80254#pullrequestreview-364938996",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "412e28ae-f2db-4c74-9aad-d89d8a3e9d57",
        "parentId" : null,
        "authorId" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "body" : "We are implementing the preemption interfaces of scheduling framework and I have a question.\r\nWhy we delete waiting pods? IMO, we may just need reject them.",
        "createdAt" : "2020-02-26T14:24:50Z",
        "updatedAt" : "2020-02-26T14:24:50Z",
        "lastEditedBy" : "c36d7654-ffec-4c9c-964e-b4b2e70085d0",
        "tags" : [
        ]
      }
    ],
    "commit" : "df14adf474f2b5b1277f88f7cdacae271dd1b423",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +330,334 @@\t\t\t}\n\t\t\t// If the victim is a WaitingPod, send a reject message to the PermitPlugin\n\t\t\tif waitingPod := fwk.GetWaitingPod(victim.UID); waitingPod != nil {\n\t\t\t\twaitingPod.Reject(\"preempted\")\n\t\t\t}"
  },
  {
    "id" : "69fed2d1-1832-48b5-9639-f4b463c63caa",
    "prId" : 78513,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78513#pullrequestreview-244105312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc8a42f2-1875-44f9-ab34-707833381da0",
        "parentId" : null,
        "authorId" : "e9ccec4a-f0c0-4820-a947-6050b5de65a5",
        "body" : "I think this will happen error when ``bindstatus==nil``.",
        "createdAt" : "2019-05-30T12:29:54Z",
        "updatedAt" : "2019-06-20T07:57:03Z",
        "lastEditedBy" : "e9ccec4a-f0c0-4820-a947-6050b5de65a5",
        "tags" : [
        ]
      },
      {
        "id" : "cf8c5c22-a2d8-47ff-affa-d3f1b92d5fce",
        "parentId" : "cc8a42f2-1875-44f9-ab34-707833381da0",
        "authorId" : "38428f84-e217-4cb8-8916-cc4e674102d6",
        "body" : "I don't think so. `IsSuccess` already checked whether its pointer receiver is nil.",
        "createdAt" : "2019-05-31T01:50:15Z",
        "updatedAt" : "2019-06-20T07:57:03Z",
        "lastEditedBy" : "38428f84-e217-4cb8-8916-cc4e674102d6",
        "tags" : [
        ]
      },
      {
        "id" : "2b568503-25ac-4c22-bcb0-e8b392ea4dcb",
        "parentId" : "cc8a42f2-1875-44f9-ab34-707833381da0",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "> I don't think so. `IsSuccess` already checked whether its pointer receiver is nil.\r\n\r\nYes, here is the `IsSucess` method:\r\n\r\n```go\r\n// IsSuccess returns true if and only if \"Status\" is nil or Code is \"Success\".\r\nfunc (s *Status) IsSuccess() bool {\r\n\tif s == nil || s.code == Success {\r\n\t\treturn true\r\n\t}\r\n\treturn false\r\n}\r\n```\r\n\r\n",
        "createdAt" : "2019-05-31T01:58:55Z",
        "updatedAt" : "2019-06-20T07:57:03Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "b339c0a8bf75ff246440fbd84a3cf93b1f4418bd",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +429,433 @@\t\t\t},\n\t\t})\n\t} else if !bindStatus.IsSuccess() {\n\t\terr = fmt.Errorf(\"scheduler RunBindPlugins failed for pod %v/%v: code %d, err %v\", assumed.Namespace, assumed.Name, bindStatus.Code(), err)\n\t}"
  },
  {
    "id" : "fca54bcf-c24b-4a5d-8e85-c1ab913f1e87",
    "prId" : 78461,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78461#pullrequestreview-298812792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5cd5b63-b5c6-41bb-9858-ce640f8ebce1",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "that is not a bug, err is going to be nil here anyways, right?",
        "createdAt" : "2019-05-29T19:46:04Z",
        "updatedAt" : "2019-05-29T19:46:19Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "9ed195d6-ee87-48d4-9b0e-d7edb8ead29b",
        "parentId" : "a5cd5b63-b5c6-41bb-9858-ce640f8ebce1",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "/remove-kind bug\r\n/kind cleanup\r\n/lgtm",
        "createdAt" : "2019-10-08T14:37:29Z",
        "updatedAt" : "2019-10-08T14:37:54Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a380ebce9a3bbc9d561d0de2d0185755080b6ba",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +288,292 @@\t\treturn core.ScheduleResult{}, err\n\t}\n\treturn result, nil\n}\n"
  },
  {
    "id" : "7a694249-97cd-45c1-b93d-1e3b79867918",
    "prId" : 77559,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77559#pullrequestreview-235054433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce403c41-fbb9-4619-b925-ea8f90e23704",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "We must ensure that `Unreserve` plugins are called here. The PR for Unreserve plugins is not merged yet.",
        "createdAt" : "2019-05-07T22:47:04Z",
        "updatedAt" : "2019-05-10T16:42:17Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "127c8532-9ca7-4850-a2d9-2484e1a48b93",
        "parentId" : "ce403c41-fbb9-4619-b925-ea8f90e23704",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Ack, once that one is submitted, I will rebase and call Unreserve.",
        "createdAt" : "2019-05-08T13:26:23Z",
        "updatedAt" : "2019-05-10T16:42:17Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "98de316436503f88204bb8e3eb49e685973d7cbe",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +547,551 @@\t\t\t\tklog.Errorf(\"scheduler cache ForgetPod failed: %v\", forgetErr)\n\t\t\t}\n\t\t\tsched.recordSchedulingFailure(assumedPod, permitStatus.AsError(), reason, permitStatus.Message())\n\t\t\t// trigger un-reserve plugins to clean up state associated with the reserved Pod\n\t\t\tfwk.RunUnreservePlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)"
  },
  {
    "id" : "6fa37980-b078-4928-8e46-9ca2c2c24e9d",
    "prId" : 75848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75848#pullrequestreview-231907731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b57ca2de-f610-43ec-8c31-2d076b0a8667",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "should this be happening before the call to assume on line 514?",
        "createdAt" : "2019-04-26T20:53:57Z",
        "updatedAt" : "2019-04-29T23:41:08Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "1a4b9d90-af55-4018-bd42-67e8b956b83b",
        "parentId" : "b57ca2de-f610-43ec-8c31-2d076b0a8667",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Yes. These are the reserve plugins that could block reservation of the pod.",
        "createdAt" : "2019-04-29T21:54:01Z",
        "updatedAt" : "2019-04-29T23:41:08Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "83828bcb2df27f3111837226ce87258d76533090",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +505,509 @@\n\t// Run \"reserve\" plugins.\n\tif sts := fwk.RunReservePlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {\n\t\tsched.recordSchedulingFailure(assumedPod, sts.AsError(), SchedulerError, sts.Message())\n\t\tmetrics.PodScheduleErrors.Inc()"
  },
  {
    "id" : "018f8ac9-85e3-4e0b-9056-dd11c1d0d9ed",
    "prId" : 75848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75848#pullrequestreview-231907731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb06ca40-7a92-4f50-9f66-e5795a56302d",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "This particular check will probably become common and need to be refactored into a helper. But let's not worry about that yet.",
        "createdAt" : "2019-04-26T20:56:22Z",
        "updatedAt" : "2019-04-29T23:41:08Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "4bcfe859-be91-4c33-aedb-bb016d497050",
        "parentId" : "cb06ca40-7a92-4f50-9f66-e5795a56302d",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I agree",
        "createdAt" : "2019-04-29T21:59:35Z",
        "updatedAt" : "2019-04-29T23:41:08Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "83828bcb2df27f3111837226ce87258d76533090",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +534,538 @@\t\tif !prebindStatus.IsSuccess() {\n\t\t\tvar reason string\n\t\t\tif prebindStatus.Code() == framework.Unschedulable {\n\t\t\t\treason = v1.PodReasonUnschedulable\n\t\t\t} else {"
  },
  {
    "id" : "74294bdb-9e14-4025-99ee-ee96d3a2b94f",
    "prId" : 72619,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72619#pullrequestreview-190112131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "894444e5-5b49-4adc-8600-5fa447a0468d",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Since the active queue takes into account lastProbeTime, and if a pod keeps getting unschedulable , which means its lastProbeTime will always be latest, it will be be farthest behind in the queue and hence least likely to be considered for scheduling ?  Is that the general idea ?\r\n\r\nAfter being marked as unschedulable, how often is it tried again ? \r\n\r\nIf an unschedulable pod eventually becomes schedulable, the LastTransitionTime will still update for this condition , hence this will still be least in prioirty to get schedulable compared to other pods. I am wondering if that will cause starvation ?\r\n",
        "createdAt" : "2019-01-07T05:37:05Z",
        "updatedAt" : "2019-01-07T05:37:05Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "186e3926-1707-4871-bedc-246429ed1305",
        "parentId" : "894444e5-5b49-4adc-8600-5fa447a0468d",
        "authorId" : "60cf1937-b446-4873-9cbc-7c2ea3ae0a27",
        "body" : ">  Is that the general idea ?\r\n\r\nThank you for your clear explanation 🙇  Yes, it is.  That's the idea.\r\n\r\n> After being marked as unschedulable, how often is it tried again ?\r\n\r\nI think it depends on cluster status.  `MoveAllToActiveQueue()` moves unschedulable pods from unschedulable queue to active queue with backoff per pod.  The method are called generally when scheduler detected pods/nodes status changed.\r\n\r\n> If an unschedulable pod eventually becomes schedulable, the LastTransitionTime will still update for this condition\r\n\r\n`LastTransitionTime` should be updated only when the condition status is changed by [definition](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions).  This means, once the pod is marked as unschedulable, which creates `PodScheduled` condition with `Status=False` on the pod status, `LastTransitionTime` shoudn't be updated until condition status will become `True`.  It is because I added code updating `LastProbeTime` when `schedule()`  failed.\r\n\r\nI think`kubelet` is responsible for updating with `PodScheduled.Status = True` of `PodScheduled` condition in under the current implementation.",
        "createdAt" : "2019-01-07T07:32:18Z",
        "updatedAt" : "2019-01-07T08:17:46Z",
        "lastEditedBy" : "60cf1937-b446-4873-9cbc-7c2ea3ae0a27",
        "tags" : [
        ]
      },
      {
        "id" : "fe1ddf73-2fb1-4270-b20f-1094532c0df8",
        "parentId" : "894444e5-5b49-4adc-8600-5fa447a0468d",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "What  I was trying to say is that this change optimizes for the cases where there are lot of unschedulable pods and favors other pods scheduling in that case.  Does it make the recovery of  a pod which has been unschedulable for a while and just became schedulable slower compare to previously @everpeace @bsalamat  because its been keep pushing to end because of constant updating of lastprobetime ?",
        "createdAt" : "2019-01-08T02:42:51Z",
        "updatedAt" : "2019-01-08T02:42:51Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "d0ea4a7e-b635-42c6-ba29-afe9a5796e43",
        "parentId" : "894444e5-5b49-4adc-8600-5fa447a0468d",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "@krmayankk I don't think so. If an unschedulable pod has higher priority, it will still get to the head of the queue even after this change. When it has the same priority as other pods, it is fair to put it behind other pods with the same priority after the scheduler has tried it and determined that it is unschedulable.",
        "createdAt" : "2019-01-08T06:01:37Z",
        "updatedAt" : "2019-01-08T06:01:37Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "273873ea-f3c5-458f-8d19-f8e99d558503",
        "parentId" : "894444e5-5b49-4adc-8600-5fa447a0468d",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "@bsalamat  i was talking about the case when there is no priority involved. All pods are same priroity or default priority. In that case its trying to avoid starvation for regular pods when lot of unschedulable pods are present. How does it affect the recovery of the unschedulable pods which finally become shcedulable. Does this behavior change when compared to without this change ? \r\nNote: Just trying to understand, the answer may be no change. It depends on how the active queue is implemented ",
        "createdAt" : "2019-01-08T06:19:28Z",
        "updatedAt" : "2019-01-08T06:19:29Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "f51df728-ba2b-4ef9-b0be-b619622527d5",
        "parentId" : "894444e5-5b49-4adc-8600-5fa447a0468d",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "With this change (and somewhat similarly after #71488), a pod that is determined unschedulable goes behind other similar priority pods in the scheduling queue. Once pods become schedulable they are processed by their order in the scheduling queue. So, depending on their location in the queue, they may get scheduled before or after same priority pods.\r\nIn short, we don't expect further delays in scheduling unschedulable pods after this change.",
        "createdAt" : "2019-01-08T07:03:04Z",
        "updatedAt" : "2019-01-08T07:03:05Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "22079a79d45ffb2e478e275731b9e5ee2b6bf99d",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +285,289 @@\t\tType:          v1.PodScheduled,\n\t\tStatus:        v1.ConditionFalse,\n\t\tLastProbeTime: metav1.Now(),\n\t\tReason:        reason,\n\t\tMessage:       err.Error(),"
  },
  {
    "id" : "ef7bcf88-3ac4-4d0b-9c1b-6f1cdd9077cf",
    "prId" : 71212,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71212#pullrequestreview-177081377",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03525fb0-9607-429f-872a-2f29c4bd81e9",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "is there a test we can set up that would exercise the bug this fixes?",
        "createdAt" : "2018-11-19T14:35:41Z",
        "updatedAt" : "2018-11-21T03:25:43Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "da472c34-0681-43be-8520-2ff330c909a1",
        "parentId" : "03525fb0-9607-429f-872a-2f29c4bd81e9",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "hard to set up a test case to cover this, probably we can add a stress test, it's easier to reproduce this issue in a few runs locally",
        "createdAt" : "2018-11-19T15:45:13Z",
        "updatedAt" : "2018-11-21T03:25:43Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      },
      {
        "id" : "51e7de05-c870-428e-bc2c-e4a255bd87bc",
        "parentId" : "03525fb0-9607-429f-872a-2f29c4bd81e9",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "+1 to async \"stress\" test... this looks like it should be fairly quick to trigger",
        "createdAt" : "2018-11-19T15:50:48Z",
        "updatedAt" : "2018-11-21T03:25:43Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "363308d9-e7cd-40a5-9fab-284b1c947ee9",
        "parentId" : "03525fb0-9607-429f-872a-2f29c4bd81e9",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "We do have some existing stress integration tests but I think they don't handle prepound PVs.  It shouldn't be too difficult to randomly make a fraction of the PVs prebound",
        "createdAt" : "2018-11-19T16:23:49Z",
        "updatedAt" : "2018-11-21T03:25:43Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "88f6d2f2-3a5f-4bf8-9607-1a0c20a84c92",
        "parentId" : "03525fb0-9607-429f-872a-2f29c4bd81e9",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "I've created a issue for this: https://github.com/kubernetes/kubernetes/issues/71301",
        "createdAt" : "2018-11-21T05:27:59Z",
        "updatedAt" : "2018-11-21T05:27:59Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fc00ebda63d1ab16a9ab029404b2c2a9868e757",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +403,407 @@\t\t// Volumes may be bound by PV controller asynchronously, we must clear\n\t\t// stale pod binding cache.\n\t\tsched.config.VolumeBinder.DeletePodBindings(assumed)\n\n\t\treason = \"VolumeBindingFailed\""
  },
  {
    "id" : "ac051a21-f4b4-40b3-87a2-5d56a432245d",
    "prId" : 71118,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71118#pullrequestreview-180082482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0366870-02f0-46ca-90d4-91c7b22989b4",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "preemption attempts must be incremented regardless of the error. It has been an attempt after all. Some attempts may fail.",
        "createdAt" : "2018-11-29T22:20:29Z",
        "updatedAt" : "2018-12-10T00:41:26Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "63ce4091-f67a-4b91-9998-bc827eb170c4",
        "parentId" : "d0366870-02f0-46ca-90d4-91c7b22989b4",
        "authorId" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "body" : "Oh, I see, so we should leave the !util.PodPriorityEnabled() || sched.config.DisablePreemption judgment outside of preempt, at this time, preemption did not occur. \r\nThanks for your explanation, I have revised it.",
        "createdAt" : "2018-11-30T00:46:32Z",
        "updatedAt" : "2018-12-10T00:41:26Z",
        "lastEditedBy" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "tags" : [
        ]
      }
    ],
    "commit" : "98f852a4412d58cad636eded0b4932fbd120c2df",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +500,504 @@\t\t\t\tpreemptionStartTime := time.Now()\n\t\t\t\tsched.preempt(pod, fitError)\n\t\t\t\tmetrics.PreemptionAttempts.Inc()\n\t\t\t\tmetrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime))\n\t\t\t\tmetrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime))"
  },
  {
    "id" : "0f533498-c2df-44cf-96dd-ce59811d1781",
    "prId" : 71118,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71118#pullrequestreview-183004940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23932c31-e9ef-4acf-a4b0-ce93b7b0bc2d",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "What is the benefit of moving the check here?\r\nI would prefer the logic of this function be simpler and easier to read and give the responsibility of the checking whether preemption is enabled or disabled to the \"preempt\" function.",
        "createdAt" : "2018-12-04T18:53:30Z",
        "updatedAt" : "2018-12-10T00:41:26Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "8c253752-a83b-462e-bbba-7e37ead021b7",
        "parentId" : "23932c31-e9ef-4acf-a4b0-ce93b7b0bc2d",
        "authorId" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "body" : "but when preemption is disabled, preemption did not occur, at this time I think metrics data shouldn't be count.",
        "createdAt" : "2018-12-05T00:49:03Z",
        "updatedAt" : "2018-12-10T00:41:26Z",
        "lastEditedBy" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "tags" : [
        ]
      },
      {
        "id" : "9ae815fe-961b-4b0e-8024-4ac5747918dc",
        "parentId" : "23932c31-e9ef-4acf-a4b0-ce93b7b0bc2d",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I see. You are right. Please move `preemptionStartTime := time.Now()` from line 496 to the `else` block.",
        "createdAt" : "2018-12-09T19:59:30Z",
        "updatedAt" : "2018-12-10T00:41:26Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "5a43bf13-d1a3-4a84-8f77-436cb30eabc2",
        "parentId" : "23932c31-e9ef-4acf-a4b0-ce93b7b0bc2d",
        "authorId" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "body" : "I have moved it, thanks.",
        "createdAt" : "2018-12-10T00:45:04Z",
        "updatedAt" : "2018-12-10T00:45:04Z",
        "lastEditedBy" : "61a9a744-a5c3-4fab-b291-e721679fb5fd",
        "tags" : [
        ]
      }
    ],
    "commit" : "98f852a4412d58cad636eded0b4932fbd120c2df",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +494,498 @@\t\t// into the resources that were preempted, but this is harmless.\n\t\tif fitError, ok := err.(*core.FitError); ok {\n\t\t\tif !util.PodPriorityEnabled() || sched.config.DisablePreemption {\n\t\t\t\tklog.V(3).Infof(\"Pod priority feature is not enabled or preemption is disabled by scheduler configuration.\" +\n\t\t\t\t\t\" No preemption is performed.\")"
  },
  {
    "id" : "d0b27810-a49f-4d47-800f-4c0407de73cc",
    "prId" : 70227,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70227#pullrequestreview-169575642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5f975ee-8496-4e73-925a-71fd9b98dcd1",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I'm curious that are we going to replace current \"assuming\" logic as a `ReservePlugin`, and provided by default? Or, keep current logic as is.",
        "createdAt" : "2018-10-29T19:01:39Z",
        "updatedAt" : "2018-12-01T00:10:13Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "5c2c0889-81f6-4cb5-8ba1-3989ade3634d",
        "parentId" : "e5f975ee-8496-4e73-925a-71fd9b98dcd1",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "The assumption logic which updates the scheduler's cache remains as a part of core.",
        "createdAt" : "2018-10-30T00:47:29Z",
        "updatedAt" : "2018-12-01T00:10:13Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "679d88c1a0e4edf82e0b94de2e4903db557ec2a8",
    "line" : 160,
    "diffHunk" : "@@ -1,1 +543,547 @@\t}\n\n\t// Run \"reserve\" plugins.\n\tfor _, pl := range plugins.ReservePlugins() {\n\t\tif err := pl.Reserve(plugins, assumedPod, suggestedHost); err != nil {"
  },
  {
    "id" : "83628342-77c2-4b9f-b62f-11016e5eac18",
    "prId" : 70227,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70227#pullrequestreview-170027340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f299f642-f89c-41c0-b9f7-9befa0176575",
        "parentId" : null,
        "authorId" : "e2ca6907-6765-444e-8bf6-1452233150d6",
        "body" : "Same as the comment for the `pluginSet`: rely on a method that says if the it is empty or not.",
        "createdAt" : "2018-10-30T22:35:15Z",
        "updatedAt" : "2018-12-01T00:10:13Z",
        "lastEditedBy" : "e2ca6907-6765-444e-8bf6-1452233150d6",
        "tags" : [
        ]
      }
    ],
    "commit" : "679d88c1a0e4edf82e0b94de2e4903db557ec2a8",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +483,487 @@\tplugins := sched.config.PluginSet\n\t// Remove all plugin context data at the beginning of a scheduling cycle.\n\tif plugins.Data().Ctx != nil {\n\t\tplugins.Data().Ctx.Reset()\n\t}"
  },
  {
    "id" : "94722f38-b2df-427d-8643-e22c29c128ca",
    "prId" : 70227,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70227#pullrequestreview-178114210",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5da7cbdd-e0a9-4725-8a04-7b0b1aa2d815",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "What happened if first plugin successfully, but second one failed; the plugin may updates some data in it; similar to other phase. IMO, we may give a callback to plugin on failures.",
        "createdAt" : "2018-11-24T03:37:25Z",
        "updatedAt" : "2018-12-01T00:10:13Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "1470bee4-8084-4c1f-9e04-3fcf7c6c7981",
        "parentId" : "5da7cbdd-e0a9-4725-8a04-7b0b1aa2d815",
        "authorId" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "body" : "From the design [doc](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduling-framework.md#extension-points), we can revert all reserve operations in the `Reject` extension point if any `Reserve` plugins failed.\r\n\r\nFor the `Prebind` extension point, we indeed don't have a revert mechanism. I think this problem also exists in current implementation: In the bind phase, scheduler invokes `bindVolumes` and `bind` in order. If `bindVolumes` succeeded but `bind` failed, we do not revert the volume binding operation.\r\n\r\nBTW, if all `Reserve` plugins succeeded but the bind phase failed, need we to revert all reserve operations? In the current implementation, we invoke `sched.config.SchedulerCache.ForgetPod` to revert assumed pods in the cache. Maybe we need this mechanism for `Reserve` plugins too.\r\n\r\ncc @bsalamat ",
        "createdAt" : "2018-11-25T04:15:48Z",
        "updatedAt" : "2018-12-01T00:10:13Z",
        "lastEditedBy" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "tags" : [
        ]
      },
      {
        "id" : "73bdf52a-3a4f-41c5-bbb2-001d110c6d2c",
        "parentId" : "5da7cbdd-e0a9-4725-8a04-7b0b1aa2d815",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "IMO, to simplify, just give an `Event` to plugins on failure at any point :) Plugins should handle those Event accordingly and scheduler handle failure on framework scope.",
        "createdAt" : "2018-11-26T02:08:13Z",
        "updatedAt" : "2018-12-01T00:10:13Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "679d88c1a0e4edf82e0b94de2e4903db557ec2a8",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +549,553 @@\t\t\tsched.recordSchedulingFailure(assumedPod, err, SchedulerError,\n\t\t\t\tfmt.Sprintf(\"reserve plugin %v failed\", pl.Name()))\n\t\t\tmetrics.PodScheduleErrors.Inc()\n\t\t\treturn\n\t\t}"
  },
  {
    "id" : "c4bee85d-966a-4442-ad75-592c783bca61",
    "prId" : 69057,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69057#pullrequestreview-162567171",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cda7269-dfce-41f6-a252-6c34cadfe4bb",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "We will eventually need to rework the unit tests in this package to cover this constructor instead of the other ones. It would be nice to see some test coverage in this PR. At minimum it would be nice to cover `CreateFromProvider`",
        "createdAt" : "2018-10-05T00:58:08Z",
        "updatedAt" : "2018-10-10T09:15:58Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "db0aa089-4fcc-4b25-8887-e02b55024c92",
        "parentId" : "3cda7269-dfce-41f6-a252-6c34cadfe4bb",
        "authorId" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "body" : "I have add test for this constructor. Maybe you can review it and Is it what you want? please comment your advice~ thank you",
        "createdAt" : "2018-10-08T17:21:04Z",
        "updatedAt" : "2018-10-10T09:15:58Z",
        "lastEditedBy" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "tags" : [
        ]
      }
    ],
    "commit" : "608911d5ac056247831f3bffcada4d714c9234d8",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +135,139 @@\n// New returns a Scheduler\nfunc New(client clientset.Interface,\n\tnodeInformer coreinformers.NodeInformer,\n\tpodInformer coreinformers.PodInformer,"
  },
  {
    "id" : "86b9048f-873d-49e6-8e1a-3b9c3bad72cd",
    "prId" : 67556,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67556#pullrequestreview-151215323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71c3cefa-ff97-4219-87bd-856cd449c6a5",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "@bsalamat this has the potential to take minutes.  Would it make sense to add a scheduler metric for this?",
        "createdAt" : "2018-08-24T01:22:32Z",
        "updatedAt" : "2018-09-05T03:37:28Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "b3836f2f-c59b-4675-95fd-8723c661140b",
        "parentId" : "71c3cefa-ff97-4219-87bd-856cd449c6a5",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Yes. That's a good idea (perhaps necessary) to add a scheduler metric for this. It doesn't have to be in this PR though.",
        "createdAt" : "2018-08-30T23:16:29Z",
        "updatedAt" : "2018-09-05T03:37:28Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "e124159990c6d22e9ca662ef35de5eeaef5d8956",
    "line" : 186,
    "diffHunk" : "@@ -1,1 +446,450 @@\t\t// Bind volumes first before Pod\n\t\tif !allBound {\n\t\t\terr = sched.bindVolumes(assumedPod)\n\t\t\tif err != nil {\n\t\t\t\treturn"
  },
  {
    "id" : "6bc644e7-3025-44f7-b2a7-b11421303e92",
    "prId" : 67556,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67556#pullrequestreview-152292031",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67c3a0f7-1fc8-4146-a145-30b8b2475e85",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I assume you didn't use glog.Errorf here because this is a recoverable error.",
        "createdAt" : "2018-09-04T23:22:25Z",
        "updatedAt" : "2018-09-05T03:37:28Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "6a2e838a-ccfd-445b-affd-b015b2625936",
        "parentId" : "67c3a0f7-1fc8-4146-a145-30b8b2475e85",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I used V(1).Infof primarily because that's what `bind()` does.  But yes, this is on a retry, this may succeed.",
        "createdAt" : "2018-09-04T23:43:01Z",
        "updatedAt" : "2018-09-05T03:37:28Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e124159990c6d22e9ca662ef35de5eeaef5d8956",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +299,303 @@\terr := sched.config.VolumeBinder.Binder.BindPodVolumes(assumed)\n\tif err != nil {\n\t\tglog.V(1).Infof(\"Failed to bind volumes for pod \\\"%v/%v\\\": %v\", assumed.Namespace, assumed.Name, err)\n\n\t\t// Unassume the Pod and retry scheduling"
  },
  {
    "id" : "d7e906f1-d3c8-4ea0-ab80-f1de439ad39b",
    "prId" : 67308,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67308#pullrequestreview-156633591",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "936d01fe-d06f-478c-8a79-ad1933573e7e",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I don't think this comment applies here.",
        "createdAt" : "2018-09-18T21:47:13Z",
        "updatedAt" : "2018-09-22T04:10:05Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "8805a1a8-b3ee-4b54-bc12-909d5b558053",
        "parentId" : "936d01fe-d06f-478c-8a79-ad1933573e7e",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "After assuming pod into scheduler cache, it will invalidate affected predicates in equivalence cache.\r\nhttps://github.com/kubernetes/kubernetes/blob/8ece5d624ded4815655104d8a12712cfe6d8743e/pkg/scheduler/scheduler.go#L355-L363",
        "createdAt" : "2018-09-19T02:05:25Z",
        "updatedAt" : "2018-09-22T04:10:05Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3f1e1200b47b1404f64a0743ece9e1a505677d9",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +330,334 @@\t// immediately.\n\tassumed.Spec.NodeName = host\n\t// NOTE: Updates must be written to scheduler cache before invalidating\n\t// equivalence cache, because we could snapshot equivalence cache after the\n\t// invalidation and then snapshot the cache itself. If the cache is"
  },
  {
    "id" : "c89bdddb-0479-4049-a212-4e1c6426caa4",
    "prId" : 61089,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61089#pullrequestreview-103660291",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12cb6bf2-5b54-4492-90f2-4ad437c06565",
        "parentId" : null,
        "authorId" : "b41ac763-2378-4c8a-a9bf-7a3dd4082826",
        "body" : "The Event objet itself is namespaced?",
        "createdAt" : "2018-03-13T12:06:18Z",
        "updatedAt" : "2018-03-13T12:06:18Z",
        "lastEditedBy" : "b41ac763-2378-4c8a-a9bf-7a3dd4082826",
        "tags" : [
        ]
      },
      {
        "id" : "bfea7bc6-3cbc-4f8b-886e-4c4fc299af0a",
        "parentId" : "12cb6bf2-5b54-4492-90f2-4ad437c06565",
        "authorId" : "358f43af-04a8-4d07-80a4-4a643693fc62",
        "body" : "the event contains its namespaces, but other event also recorded the namspaces in reason, so I did it .",
        "createdAt" : "2018-03-14T00:31:42Z",
        "updatedAt" : "2018-03-14T00:31:42Z",
        "lastEditedBy" : "358f43af-04a8-4d07-80a4-4a643693fc62",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e57ae73a883e97c618d0150d2933e472e9a6242",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +424,428 @@\n\tmetrics.BindingLatency.Observe(metrics.SinceInMicroseconds(bindingStart))\n\tsched.config.Recorder.Eventf(assumed, v1.EventTypeNormal, \"Scheduled\", \"Successfully assigned %v/%v to %v\", assumed.Namespace, assumed.Name, b.Target.Name)\n\treturn nil\n}"
  },
  {
    "id" : "47d2c436-20f3-439d-8c74-08e5a63e55b5",
    "prId" : 58192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58192#pullrequestreview-88563378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74f1dfe0-f596-4563-9b8c-fea4a9c8925c",
        "parentId" : null,
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "The original issue is about: Add metrics for **number** of preemptions and **number** of victims.` Not the time it consumed (also the preemption time does not matter this much).\r\n\r\nYou may want to rework the PR :smiley:",
        "createdAt" : "2018-01-12T07:52:02Z",
        "updatedAt" : "2018-01-13T06:57:28Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "cc07e111-99d4-471c-b58e-31aab793c764",
        "parentId" : "74f1dfe0-f596-4563-9b8c-fea4a9c8925c",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Sure @resouer - Thanks for pointing it, I saw it after I submitted my PR. I have already started working on it. But I think preemption time is important as it has certain computation involved and we need to measure this as well if the cluster is fully loaded and frequent preemptions are happening.",
        "createdAt" : "2018-01-12T11:10:28Z",
        "updatedAt" : "2018-01-13T06:57:28Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "790d6c5c-e777-46ee-af90-6db7cc1e9296",
        "parentId" : "74f1dfe0-f596-4563-9b8c-fea4a9c8925c",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Sure, I am totally positive with this as long as we are now on the right track",
        "createdAt" : "2018-01-12T18:23:06Z",
        "updatedAt" : "2018-01-13T06:57:28Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8aebf3554c7534300f08a7646748a1afe91b1812",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +456,460 @@\t\treturn\n\t}\n\tmetrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInMicroseconds(start))\n\t// Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet.\n\t// This allows us to keep scheduling without waiting on binding to occur."
  }
]