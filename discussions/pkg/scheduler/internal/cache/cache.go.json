[
  {
    "id" : "13894e07-5461-45da-aa58-1657563be3f5",
    "prId" : 100049,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/100049#pullrequestreview-609165197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "177e0bca-4223-4ab8-b08b-48165c6b2dd8",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Isn't this clone expensive? I don't think we need to clone, as we don't modify it for checks.",
        "createdAt" : "2021-03-10T14:44:33Z",
        "updatedAt" : "2021-03-11T20:32:06Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "56b0a5cf-150d-4176-8555-468bd158d643",
        "parentId" : "177e0bca-4223-4ab8-b08b-48165c6b2dd8",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "we need to clone because the node could be updated while we are doing the checks. But I think the caller should do the clone not here.",
        "createdAt" : "2021-03-10T19:35:07Z",
        "updatedAt" : "2021-03-11T20:32:06Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "0863ac70-c25b-4e19-9502-581fcd3d2606",
        "parentId" : "177e0bca-4223-4ab8-b08b-48165c6b2dd8",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> Isn't this clone expensive?\r\n\r\nYes, but we need the clone to be thread-safe.\r\n\r\n> But I think the caller should do the clone not here.\r\n\r\nProbably not, if we do clone in the caller side, we will have to do another lock/unlock.",
        "createdAt" : "2021-03-10T20:16:05Z",
        "updatedAt" : "2021-03-11T20:32:06Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "04a72995-f27f-4cf2-abef-f5b53d1973b5",
        "parentId" : "177e0bca-4223-4ab8-b08b-48165c6b2dd8",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "> Probably not, if we do clone in the caller side, we will have to do another lock/unlock.\r\n\r\ntrue.\r\n",
        "createdAt" : "2021-03-10T20:18:26Z",
        "updatedAt" : "2021-03-11T20:32:06Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "2f815752-dbd4-4a04-acd5-b5fe848e02c9",
        "parentId" : "177e0bca-4223-4ab8-b08b-48165c6b2dd8",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> we will have to do another lock/unlock.\r\n\r\nPrecisely, a read lock/unlock. Let me know which way you prefer.",
        "createdAt" : "2021-03-10T20:26:52Z",
        "updatedAt" : "2021-03-11T20:32:06Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "8cb82d4b-cb8b-4250-9f0d-cd361062ae88",
        "parentId" : "177e0bca-4223-4ab8-b08b-48165c6b2dd8",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "existing logic make sense to me now.",
        "createdAt" : "2021-03-10T21:04:37Z",
        "updatedAt" : "2021-03-11T20:32:06Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "6384f397b4702fa6d5543a5947ed0a3f67cf115d",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +610,614 @@\tcache.addNodeImageStates(node, n.info)\n\tn.info.SetNode(node)\n\treturn n.info.Clone()\n}\n"
  },
  {
    "id" : "3148684b-5db5-441d-af3c-8b548dbeba89",
    "prId" : 95130,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/95130#pullrequestreview-497925739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4d500fe9-9a9b-4c32-b43f-1b1c27df8cc5",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "can you add a comment explaining that deleted nodes get removed from the tree but are kept in the cache until the pods are deleted, and hence here we check against the number of nodes in the tree not the cache.",
        "createdAt" : "2020-09-28T21:14:37Z",
        "updatedAt" : "2020-09-28T21:51:04Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "a8ccbb49-5144-4074-8afb-132a035649ab",
        "parentId" : "4d500fe9-9a9b-4c32-b43f-1b1c27df8cc5",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "also, document that the snapshot (both map and lists) only include nodes that are not deleted as of the time the snapshot was taken, and so nodeinfo.Node() is guaranteed not to be nil. ",
        "createdAt" : "2020-09-28T21:17:20Z",
        "updatedAt" : "2020-09-28T21:51:04Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "0c3b1f93-f23b-43a1-ada7-603ac8855ed0",
        "parentId" : "4d500fe9-9a9b-4c32-b43f-1b1c27df8cc5",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Done. I amended.",
        "createdAt" : "2020-09-28T21:51:10Z",
        "updatedAt" : "2020-09-28T21:51:10Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6f09f7dfb51cbed389e5bacbe4c83dfad3e9b97",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +262,266 @@\t// Deleted nodes get removed from the tree, but they might remain in the nodes map\n\t// if they still have non-deleted Pods.\n\tif len(nodeSnapshot.nodeInfoMap) > cache.nodeTree.numNodes {\n\t\tcache.removeDeletedNodesFromSnapshot(nodeSnapshot)\n\t\tupdateAllLists = true"
  },
  {
    "id" : "173a5070-e260-43b3-a5f3-e3d6cd2ed324",
    "prId" : 93938,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93938#pullrequestreview-467102931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b91b1949-3eff-4a5b-8fe2-8267724c2933",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I recalled the original logic returned an error?",
        "createdAt" : "2020-08-13T19:29:59Z",
        "updatedAt" : "2020-08-13T19:30:00Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "42d68719-b3dd-4f97-9779-ce0369f93275",
        "parentId" : "b91b1949-3eff-4a5b-8fe2-8267724c2933",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "It did. But returning nil is actually safer in the case of extraneous update events that might arrive before a node is created, and after the original node was completely removed.",
        "createdAt" : "2020-08-13T19:32:51Z",
        "updatedAt" : "2020-08-13T19:32:52Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "f3d5048a-f86a-43cf-9348-37925395fee6",
        "parentId" : "b91b1949-3eff-4a5b-8fe2-8267724c2933",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "+1 to returning nil and just logging an error.",
        "createdAt" : "2020-08-13T19:41:38Z",
        "updatedAt" : "2020-08-13T19:41:38Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "953d6f16-0ea5-4f0d-906c-ee167cdb53e5",
        "parentId" : "b91b1949-3eff-4a5b-8fe2-8267724c2933",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I checked the usage of `removePod()`, there are still a number of callers rely on the returned value. So I'd suggest to revert to the original state.",
        "createdAt" : "2020-08-13T19:41:41Z",
        "updatedAt" : "2020-08-13T19:41:41Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "de46cec5-2f80-49c3-ad42-1a1f009a5427",
        "parentId" : "b91b1949-3eff-4a5b-8fe2-8267724c2933",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Detail for each caller:\r\n\r\n- ForgetPod: We actually want to proceed and clear the assumedPods and podStates.\r\n- expirePod: Same as above.\r\n- AddPod: it just logs the error returned, so same effect.\r\n- RemovePod: We want to clear podStates.\r\n- updatePod: This is the case where we want to prevent losing information.",
        "createdAt" : "2020-08-13T20:02:01Z",
        "updatedAt" : "2020-08-13T20:02:01Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "6e17725d-d9df-4c27-8124-2e9f8c2a144d",
        "parentId" : "b91b1949-3eff-4a5b-8fe2-8267724c2933",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "That said, for expirePod and ForgetPod, the node shouldn't have been removed because it still had pods assigned.",
        "createdAt" : "2020-08-13T20:03:24Z",
        "updatedAt" : "2020-08-13T20:03:40Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "c566237a-816c-4051-a659-0351779df1e8",
        "parentId" : "b91b1949-3eff-4a5b-8fe2-8267724c2933",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Thanks. That's fair.",
        "createdAt" : "2020-08-13T20:23:17Z",
        "updatedAt" : "2020-08-13T20:23:17Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfe9e413d9fab9037a4c8b62ce3bdddd7b0d58da",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +435,439 @@\tn, ok := cache.nodes[pod.Spec.NodeName]\n\tif !ok {\n\t\tklog.Errorf(\"node %v not found when trying to remove pod %v\", pod.Spec.NodeName, pod.Name)\n\t\treturn nil\n\t}"
  },
  {
    "id" : "b897f481-7a36-4528-8d3f-339015d909cc",
    "prId" : 93473,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93473#pullrequestreview-457352257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "857a4a1e-6ce4-4b32-a4c9-c103dcef608b",
        "parentId" : null,
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "whether the loop against `nodesList` here is needed when you get a error from `cache.nodeTree.list()`?",
        "createdAt" : "2020-07-29T08:28:07Z",
        "updatedAt" : "2020-08-18T08:35:42Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "e1edb56b-e1ef-4366-951b-5ed20f5117bf",
        "parentId" : "857a4a1e-6ce4-4b32-a4c9-c103dcef608b",
        "authorId" : "270a0c70-71d3-4103-8c71-0612c0396f58",
        "body" : "The idea was that an error in listing the node would not fully prevent the scheduler to work properly, it would work with whatever partial list of nodes it got, and log an error to not let it go unnoticed. Is that problematic ?",
        "createdAt" : "2020-07-29T09:16:48Z",
        "updatedAt" : "2020-08-18T08:35:42Z",
        "lastEditedBy" : "270a0c70-71d3-4103-8c71-0612c0396f58",
        "tags" : [
        ]
      },
      {
        "id" : "ac5f96fc-d97b-453f-bfe3-c8cb90d36297",
        "parentId" : "857a4a1e-6ce4-4b32-a4c9-c103dcef608b",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "sound good, thank for the response.",
        "createdAt" : "2020-07-29T09:31:07Z",
        "updatedAt" : "2020-08-18T08:35:42Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc5edb8c12aa7c158972f6a2c8e64c1014567650",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +284,288 @@\t\t\tklog.Error(err)\n\t\t}\n\t\tfor _, nodeName := range nodesList {\n\t\t\tif n := snapshot.nodeInfoMap[nodeName]; n != nil {\n\t\t\t\tsnapshot.nodeInfoList = append(snapshot.nodeInfoList, n)"
  },
  {
    "id" : "44120c8c-9a69-4ea1-97ce-ce89b1a953ae",
    "prId" : 93355,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93355#pullrequestreview-453720363",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30e671b5-676d-40bf-b1e8-f5f65495a787",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Do you have a unit test for this?\r\n\r\ni.e. a set of steps that cause `nt.resetExhausted()` to not be called inside `next()`",
        "createdAt" : "2020-07-22T21:53:17Z",
        "updatedAt" : "2020-07-24T06:44:01Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "e241388f-704e-44a7-b83c-a2d9b1336989",
        "parentId" : "30e671b5-676d-40bf-b1e8-f5f65495a787",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Oh I see, the test you shared it covers it https://github.com/kubernetes/kubernetes/issues/91601#issuecomment-662663090\r\n\r\nBut I would appreciate if you translate the test into cache_test.go",
        "createdAt" : "2020-07-22T22:09:27Z",
        "updatedAt" : "2020-07-24T06:44:01Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2ec8bedbcc9a675d0c4c0317e8be559bc5e3f63",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +281,285 @@\t\t// Take a snapshot of the nodes order in the tree\n\t\tsnapshot.nodeInfoList = make([]*framework.NodeInfo, 0, cache.nodeTree.numNodes)\n\t\tcache.nodeTree.resetExhausted()\n\t\tfor i := 0; i < cache.nodeTree.numNodes; i++ {\n\t\t\tnodeName := cache.nodeTree.next()"
  },
  {
    "id" : "c46c03bf-3909-4379-964a-75f3fedf0eb0",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324762207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84aacb6f-f289-498c-9306-a383e65ed750",
        "parentId" : null,
        "authorId" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "body" : "Then we can change it here:\r\n```go\r\nnodeSnapshot.HavePodsWithAffinityNodeInfoList = make([]*schedulernodeinfo.NodeInfo, 0, cache.nodeTree.numNodes)\r\nif updateAll {\r\n    ....\r\n} else {\r\n    .... // just update HavePodsWithAffinityNodeInfoList\r\n}",
        "createdAt" : "2019-11-29T07:25:54Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "tags" : [
        ]
      },
      {
        "id" : "2c4ac94d-fa9e-4cf3-a89b-35f69b97c440",
        "parentId" : "84aacb6f-f289-498c-9306-a383e65ed750",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "addressed.",
        "createdAt" : "2019-11-29T15:04:57Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +283,287 @@\t\t// Take a snapshot of the nodes order in the tree\n\t\tnodeSnapshot.NodeInfoList = make([]*schedulernodeinfo.NodeInfo, 0, cache.nodeTree.numNodes)\n\t\tfor i := 0; i < cache.nodeTree.numNodes; i++ {\n\t\t\tnodeName := cache.nodeTree.next()\n\t\t\tif n := nodeSnapshot.NodeInfoMap[nodeName]; n != nil {"
  },
  {
    "id" : "0a94b4df-7f0e-444c-8923-b9b9755c098e",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324837153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccb0510d-49d7-4b7d-941a-69323156ee92",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "`HavePodsWithAffinityNodeInfoList` is not applicable here.",
        "createdAt" : "2019-11-29T15:10:16Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "17af1480-f6ba-4ff8-80c5-7196c96aa744",
        "parentId" : "ccb0510d-49d7-4b7d-941a-69323156ee92",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "it is, we have to update both.",
        "createdAt" : "2019-11-29T17:21:00Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "11c38a68-2308-4ecc-bae0-a148625f2b93",
        "parentId" : "ccb0510d-49d7-4b7d-941a-69323156ee92",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "I think `HavePodsWithAffinityNodeInfoList` is covered by the  `updateNodesHavePodsWithAffinity` flag below. ",
        "createdAt" : "2019-11-29T17:37:14Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "6ae2bd2f-c29e-4deb-950a-c2b83949331e",
        "parentId" : "ccb0510d-49d7-4b7d-941a-69323156ee92",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "sure, but the sentence here is certainly correct: we need to recreate all lists when a node is added or deleted.",
        "createdAt" : "2019-11-29T17:44:52Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "5e2282ef-7d2e-4449-950c-94125933d708",
        "parentId" : "ccb0510d-49d7-4b7d-941a-69323156ee92",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Right... in the current implementation.\r\nBut can we skip recreating  `HavePodsWithAffinityNodeInfoList`  if no updates to nodes with affinity pods?",
        "createdAt" : "2019-11-29T18:35:05Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "aa21793e-856f-4b35-ac2b-e967504aa159",
        "parentId" : "ccb0510d-49d7-4b7d-941a-69323156ee92",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "recreating HavePodsWithAffinityNodeInfoList is not expensive to do if we are recreating the NodeInfoList. The main overhead is the map lookup.",
        "createdAt" : "2019-11-29T18:59:21Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "3886594e-cc2e-43c8-aaeb-c2aabc4c965e",
        "parentId" : "ccb0510d-49d7-4b7d-941a-69323156ee92",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "makes sense to me",
        "createdAt" : "2019-11-29T19:19:20Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +211,215 @@\n\t// NodeInfoList and HavePodsWithAffinityNodeInfoList must be re-created if a node was added\n\t// or removed from the cache.\n\tupdateAllLists := false\n\t// HavePodsWithAffinityNodeInfoList must be re-created if a node changed its"
  },
  {
    "id" : "85f88fbe-5631-4b9e-81fa-da3708b054a6",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324818490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d285cdd-1408-4d40-b4c8-0c7df5059cce",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "nit: `return` is better but `break` has the same effect.",
        "createdAt" : "2019-11-29T15:14:24Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "92c24925-458f-4e8d-918b-26329fede409",
        "parentId" : "0d285cdd-1408-4d40-b4c8-0c7df5059cce",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I prefer to return in one place.",
        "createdAt" : "2019-11-29T17:20:48Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "d695fa87-b07b-4ba5-874d-b32ca9760c6a",
        "parentId" : "0d285cdd-1408-4d40-b4c8-0c7df5059cce",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "according to my previous comment, this might not be accurate.",
        "createdAt" : "2019-11-29T18:15:43Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +308,312 @@\tfor name := range nodeSnapshot.NodeInfoMap {\n\t\tif toDelete <= 0 {\n\t\t\tbreak\n\t\t}\n\t\tif _, ok := cache.nodes[name]; !ok {"
  },
  {
    "id" : "d19e0327-8fed-4239-9959-99b30e4ae89b",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324813319",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0f5bbdc-7641-4478-bacf-31a6413f662e",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "So this was a bug before?",
        "createdAt" : "2019-11-29T15:15:44Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "50bfc96a-d60d-4827-a3a1-bcb7f7a971b4",
        "parentId" : "f0f5bbdc-7641-4478-bacf-31a6413f662e",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "probably only in the tests since in a real scenario we don't update an non-existent node.",
        "createdAt" : "2019-11-29T17:20:35Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +605,609 @@\t\tn = newNodeInfoListItem(schedulernodeinfo.NewNodeInfo())\n\t\tcache.nodes[newNode.Name] = n\n\t\tcache.nodeTree.addNode(newNode)\n\t} else {\n\t\tcache.removeNodeImageStates(n.info.Node())"
  },
  {
    "id" : "332fa9fa-a208-4853-9578-25409362e76b",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324817323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cae52d4-55b9-4a2e-a6fd-558afb741f6d",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "I think we can optimize further by tracking 1)the list of nodes that are added/removed and 2) list of nodes with affinity pods changed. And in `updateNodeInfoSnapshotList`, we only update the updated nodes instead of recreating the entire node list(s).\r\n\r\nBut in order to maintain the same order as the node tree, this approach may require the snapshot to maintain a node tree structure. And the node list need to be a linked list for faster insertion/removal.\r\n\r\nNeed some more thoughts on this. ",
        "createdAt" : "2019-11-29T16:10:37Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "9500436b-0127-4747-a17a-981ebf4fc1b0",
        "parentId" : "4cae52d4-55b9-4a2e-a6fd-558afb741f6d",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Just to clarify: I am not suggesting to explore this idea in this PR. This PR is good enough!",
        "createdAt" : "2019-11-29T16:36:14Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "c5c75e91-c829-4cf6-b489-3028198902a7",
        "parentId" : "4cae52d4-55b9-4a2e-a6fd-558afb741f6d",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Not clear to me how much we would gain by adding this complexity, certainly nothing in the benchmark test we have (the profile shows that this now is insignificant).",
        "createdAt" : "2019-11-29T17:18:58Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "19fb807d-f1bc-48e8-9306-021e7b12ee0f",
        "parentId" : "4cae52d4-55b9-4a2e-a6fd-558afb741f6d",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "it could be useful for clusters that constantly change size, but I think the overhead of bringing up/down nodes is more significant than this.",
        "createdAt" : "2019-11-29T17:19:49Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "8439bd49-e236-479e-9f4f-34524063c34f",
        "parentId" : "4cae52d4-55b9-4a2e-a6fd-558afb741f6d",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "I agree that maintaining nodes added/removed may not be very important. But I think it certainly can help with the cluster where affinity is used a lot. ",
        "createdAt" : "2019-11-29T17:39:11Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +258,262 @@\t}\n\n\tif updateAllLists || updateNodesHavePodsWithAffinity {\n\t\tcache.updateNodeInfoSnapshotList(nodeSnapshot, updateAllLists)\n\t}"
  },
  {
    "id" : "22eeece8-f7d2-4d9c-b822-4be9daab1354",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324841871",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "\"likely\" or definitely?\r\nIf we can recover, I don't think we should fail the current scheduling cycle. We should log the error message and go on.",
        "createdAt" : "2019-11-29T16:12:06Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "f35c262d-acae-490e-a643-fb310229799d",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I want to make this more visible than just a log because this is a major error.",
        "createdAt" : "2019-11-29T17:22:49Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "de5c5204-7caf-43f3-8de5-671710060ffb",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "I don't think returning an error makes it \"more visible\". We just record a metric and retry.\r\nIf you think the metric makes it more visible, we can just add a metric here.\r\n\r\nSince we can recover, I don't think we should return an error because by definition there is no error any more.",
        "createdAt" : "2019-11-29T17:41:16Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "ee419e14-e1a7-46bc-8898-02161f3d7533",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "yes, I was thinking about the failure metrics. I don't think we should update the metric here, it should be updated in one place where we test whether or not the scheduling cycle succeeded.",
        "createdAt" : "2019-11-29T17:47:41Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "671c85a4-2171-4766-b355-a640bcd11e03",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "I agree, better return it. But ideally we should just have more test cases.",
        "createdAt" : "2019-11-29T17:54:56Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "0f185a1f-e800-44bc-ad6d-ecd18e3800b8",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "If metrics is the way to make it visibly, let's create a new metric for this. Otherwise it's hard to tell what caused the existing schedule_attempt error metric.\r\n\r\nI still disagree with returning error - first there is no error any more, second it makes scheduler to waste another cycle. I think what we want here is to make this visible, and I don't think returning an error is the best way to do it.\r\n\r\nFor this type of unexpected errors, I have seen people either using a metric or have a log scan that alerts when there are too many error logs.",
        "createdAt" : "2019-11-29T18:41:12Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "84c2fb36-afe6-494e-a088-2f399f0f0e8d",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "> If metrics is the way to make it visibly, let's create a new metric for this. \r\n\r\nI don't think this is worth a separate metric, it is just another fault, do we want to create a separate metric for every fault we encounter?\r\n\r\n> Otherwise it's hard to tell what caused the existing schedule_attempt error metric.\r\n\r\nAdmins usually look for patterns, if this is happening consistently for a specific scheduler, then an uptick in the schedule_attempt error will prompt admins to look at the logs.\r\n\r\n> I still disagree with returning error - first there is no error any more, \r\n\r\nThere was an error, and we are trying to surface it as best we can.\r\n\r\n> second it makes scheduler to waste another cycle. I think what we want here is to make this visible, and I don't think returning an error is the best way to do it.\r\n\r\nI don't see wasting a cycle in a potentially unhealthy scheduler is an issue.\r\n\r\n> For this type of unexpected errors, I have seen people either using a metric or have a log scan that alerts when there are too many error logs.\r\n\r\nSure, but this is not a binary deployed in one place that we control how it is monitored. So the more signals we send the better in my opinion.\r\n\r\nI added an error message in addition to returning an error.\r\n",
        "createdAt" : "2019-11-29T19:17:26Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "01c61ad5-fc2a-4d1f-b370-685b431c0b8b",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "I am not in favor of creating a metric. I am just arguing that the only effect of returning the error is to increment an error counter. If that's the case, why not just increment the error metric here and move on? \r\nIf we retry, the pod is added back to the queue and has to wait until it's picked up again (which may take some time). I think it's not \"fair\" for the pod.\r\n\r\nIt's weird to me that an error is properly handled and then returned. I think we should log the error.",
        "createdAt" : "2019-11-29T19:22:57Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "1243d515-a7d0-4903-b8cc-929cc2c9fa91",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "> If that's the case, why not just increment the error metric here and move on?\r\n\r\nThis was addressed above, do we want to increment a failure metric everywhere we fail, or just in one place? I think just in one place.\r\n\r\n> If we retry, the pod is added back to the queue and has to wait until it's picked up again (which may take some time). I think it's not \"fair\" for the pod.\r\n\r\nThis is a weird \"fair\" argument :)\r\n\r\n> \r\n> It's weird to me that an error is properly handled and then returned. I think we should log the error.\r\n\r\nIt is not guaranteed that we actually recovered. Both options have pros and cons, there is clearly no optimal solution here. I added a log message.\r\n",
        "createdAt" : "2019-11-29T19:33:53Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "095011a0-a975-42d5-91a5-52c62b83c0c8",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "> This was addressed above, do we want to increment a failure metric everywhere we fail, or just in one place? I think just in one place.\r\n\r\nThis is different. I actually think this error can be properly recovered. If I am wrong, then yeah let's do a best effort recovery and return the error.\r\n\r\n> It is not guaranteed that we actually recovered. Both options have pros and cons, there is clearly no optimal solution here. I added a log message.\r\n\r\nI believe by recreating the snapshot we can actually guarantee a proper recover. If this is not the case, yeah then I agree to return the error.",
        "createdAt" : "2019-11-29T19:40:12Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "a65ac958-caab-488c-a71a-79fba8b1eca4",
        "parentId" : "b54b2b1d-9fe5-4f88-8203-72e602da5dc9",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "SG, then as I mentioned in the other comment, we don't have the logic to properly recover, so we can keep this as is, and perhaps in a separate PR we can try to come up with an approach to test for inconsistent state in the map and properly recover.",
        "createdAt" : "2019-11-29T19:46:46Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +270,274 @@\t\tklog.Error(errMsg)\n\t\t// We will try to recover by re-creating the lists for the next scheduling cycle, but still return an\n\t\t// error to surface the problem, the error will likely cause a failure to the current scheduling cycle.\n\t\tcache.updateNodeInfoSnapshotList(nodeSnapshot, true)\n\t\treturn fmt.Errorf(errMsg)"
  },
  {
    "id" : "f7156e8a-9eaa-4207-9fad-c018b8eedeba",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324838688",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a269b07-e7ee-45a7-8ad4-e22e1aff82e4",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Is this a guaranteed recovery? Or should we recreate the snapshot entirely?",
        "createdAt" : "2019-11-29T16:14:14Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "5d24d6f4-c511-4b7f-9024-e3c1fb82ff60",
        "parentId" : "4a269b07-e7ee-45a7-8ad4-e22e1aff82e4",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "from the lists perspective, the map is the source of truth, so I think this is enough for now (or at least for the purpose of what we added). I don't think we have a function to recreate the map.",
        "createdAt" : "2019-11-29T17:50:53Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "38d47411-4052-4440-b8b5-77f7b335b947",
        "parentId" : "4a269b07-e7ee-45a7-8ad4-e22e1aff82e4",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "I am afraid if there is a possibility to run into infinite error loop. I think it's safer to recreate the snapshot entirely. Maybe I am just over thinking.",
        "createdAt" : "2019-11-29T18:45:51Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "bccf58be-fe97-4943-b260-3a8be36b3eff",
        "parentId" : "4a269b07-e7ee-45a7-8ad4-e22e1aff82e4",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "This PR does not impact the snapshot map at all, so if we want to recreate it, then we need a better test for checking when it is consistent and when it is not.",
        "createdAt" : "2019-11-29T18:55:36Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "e045e2b2-6531-4315-a4ed-053533f44865",
        "parentId" : "4a269b07-e7ee-45a7-8ad4-e22e1aff82e4",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Yeah I know this PR doesn't change the map. I am OK with the current version.\r\n\r\nThis is a tricky problem and I really don't know if it could happen or not, and what's the best approach to handle it if it happens. The best approach I can think of is either to recover properly or to panic..",
        "createdAt" : "2019-11-29T19:28:05Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +271,275 @@\t\t// We will try to recover by re-creating the lists for the next scheduling cycle, but still return an\n\t\t// error to surface the problem, the error will likely cause a failure to the current scheduling cycle.\n\t\tcache.updateNodeInfoSnapshotList(nodeSnapshot, true)\n\t\treturn fmt.Errorf(errMsg)\n\t}"
  },
  {
    "id" : "cf668be7-b1ca-4027-a2e5-0cd002d76994",
    "prId" : 85738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85738#pullrequestreview-324834954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0869dd92-b3ec-4f63-8a30-390da38fdea7",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "can a node be added and another one removed within cycles? If so, let's have a unit test for that.",
        "createdAt" : "2019-11-29T17:50:44Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "b4f6e655-7349-4ab2-bf2e-0fe92a9a2a88",
        "parentId" : "0869dd92-b3ec-4f63-8a30-390da38fdea7",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "It's definitely possible. But I think this is covered in the tests such as \"Add a few nodes, and remove some of them\"",
        "createdAt" : "2019-11-29T18:37:55Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "33f56acd-1ac9-42a9-ad6c-ce68377ebf95",
        "parentId" : "0869dd92-b3ec-4f63-8a30-390da38fdea7",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "is it accounting for having the same size?",
        "createdAt" : "2019-11-29T18:53:56Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "717d3d7d-5493-48d1-b0df-5aa1951341dd",
        "parentId" : "0869dd92-b3ec-4f63-8a30-390da38fdea7",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I added a test to cover that use case, take a look.",
        "createdAt" : "2019-11-29T19:07:44Z",
        "updatedAt" : "2019-11-29T20:06:27Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ea43f20cb74fe43011b747fb94d8d3d53d42e42",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +254,258 @@\n\tif len(nodeSnapshot.NodeInfoMap) > len(cache.nodes) {\n\t\tcache.removeDeletedNodesFromSnapshot(nodeSnapshot)\n\t\tupdateAllLists = true\n\t}"
  },
  {
    "id" : "00b2e78e-e76e-4f1d-9e0e-431da7d8d96d",
    "prId" : 84014,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84014#pullrequestreview-304153946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cec5348c-ad05-41ea-a5df-e72641aceed5",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "The loop seems to be both memory and time consuming, can we have a benchmark testing the whole scheduling cycle?",
        "createdAt" : "2019-10-18T21:34:13Z",
        "updatedAt" : "2019-10-18T21:36:36Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "f6b8a802-9ba3-4222-a309-767d1f5ea55a",
        "parentId" : "cec5348c-ad05-41ea-a5df-e72641aceed5",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "It is not really, as described in the issue description, we gain about 3%, as for memory, this is just an array of pointers, so even for cluster with 5k nodes for example, the overhead is negligible. \r\n\r\nNote that we do something similar in the predicates metadata, here are examples:\r\nhttps://github.com/kubernetes/kubernetes/blob/de9a7d863dada47d5be112b4ed03eccfe97a0471/pkg/scheduler/algorithm/predicates/metadata.go#L407\r\nhttps://github.com/kubernetes/kubernetes/blob/de9a7d863dada47d5be112b4ed03eccfe97a0471/pkg/scheduler/algorithm/predicates/metadata.go#L731",
        "createdAt" : "2019-10-18T21:51:52Z",
        "updatedAt" : "2019-10-18T22:02:35Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "63d7733e988776355306e21947c43cb2f4fb8896",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +242,246 @@\t// Take a snapshot of the nodes order in the tree\n\tnodeSnapshot.NodeInfoList = make([]*schedulernodeinfo.NodeInfo, 0, cache.nodeTree.numNodes)\n\tfor i := 0; i < cache.nodeTree.numNodes; i++ {\n\t\tnodeName := cache.nodeTree.next()\n\t\tif n := nodeSnapshot.NodeInfoMap[nodeName]; n != nil {"
  },
  {
    "id" : "031ecc94-6835-4a00-9f53-b8ea90d7a5c6",
    "prId" : 83508,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83508#pullrequestreview-308303528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb178719-04c1-4fd9-84ed-c6d825698aa0",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "It looks like not all cache size change events are captured, e.g,, Line #400 in original file where `delete(cache.assumedPods, key)`. \r\n\r\nI think the best way to capture everything is to go through all references of the 3 maps `assumedPods`, `podStates` and `nodes`, and `Inc/Dec` the metric when `add/delete` happens. There might be an opportunity to refactor the code as well, e.g., if we make all `add/delete` operation go through a single set of helper functions, then we only need to `Inc/Dec` in those helper functions.",
        "createdAt" : "2019-10-04T18:56:25Z",
        "updatedAt" : "2019-10-30T19:33:35Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "7d5021f1-8a93-4ef2-9418-2e9aaa408ac0",
        "parentId" : "eb178719-04c1-4fd9-84ed-c6d825698aa0",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "That's a good point, I also think there is a chance to refactor some of the cache code here. Will work more on it",
        "createdAt" : "2019-10-07T14:59:11Z",
        "updatedAt" : "2019-10-30T19:33:35Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "a30bc6dc-8be7-40e9-87f1-d16c704f492a",
        "parentId" : "eb178719-04c1-4fd9-84ed-c6d825698aa0",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "it seems error prone to Inc/Dec because I am pretty sure we will forget a place and the counter will diverge from the actual cache size and can never recover, how about we have a function that ```Set``` the value of the metrics to ```len(map)``` of each map, instead of doing inc/dec",
        "createdAt" : "2019-10-29T07:22:55Z",
        "updatedAt" : "2019-10-30T19:33:35Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "828d6622a96548a0d69da4b722f5e88960e22bb6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +30,34 @@\t\"k8s.io/kubernetes/pkg/features\"\n\tschedulerlisters \"k8s.io/kubernetes/pkg/scheduler/listers\"\n\t\"k8s.io/kubernetes/pkg/scheduler/metrics\"\n\tschedulernodeinfo \"k8s.io/kubernetes/pkg/scheduler/nodeinfo\"\n\tnodeinfosnapshot \"k8s.io/kubernetes/pkg/scheduler/nodeinfo/snapshot\""
  },
  {
    "id" : "880fc9ff-b127-4b0b-83a7-02c817a7dec2",
    "prId" : 83508,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83508#pullrequestreview-309467882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34a10803-3dfb-42c4-8073-949e9664bf54",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "nit: To make it more explicit, I would rename(or comment) `cleanupExpiredAssumedPods` to `cleanupExpiredAssumedPodsAndUpdateMetrics`. ",
        "createdAt" : "2019-10-30T19:22:07Z",
        "updatedAt" : "2019-10-30T19:33:35Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "10da9849-e2fc-442d-bb08-b176b6f86a75",
        "parentId" : "34a10803-3dfb-42c4-8073-949e9664bf54",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "I don't think it's necessary to make it explicit in the function name that it updates the metrics, I know there are many other functions that report metrics as a side-effect that don't have it in the name. but I'll update the comment so it's documented",
        "createdAt" : "2019-10-30T19:32:30Z",
        "updatedAt" : "2019-10-30T19:33:35Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      }
    ],
    "commit" : "828d6622a96548a0d69da4b722f5e88960e22bb6",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +641,645 @@\tcache.mu.Lock()\n\tdefer cache.mu.Unlock()\n\tdefer cache.updateMetrics()\n\n\t// The size of assumedPods should be small"
  },
  {
    "id" : "6b2f83c1-3c0c-4c7f-874d-99c11c07e6cc",
    "prId" : 80220,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80220#pullrequestreview-262993224",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "578512a0-e5bb-487b-aadf-a2252cc92412",
        "parentId" : null,
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "%q -> %s/%v is ok",
        "createdAt" : "2019-07-17T01:57:56Z",
        "updatedAt" : "2019-07-24T02:11:01Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "063542ae-a615-4b76-8a7d-e0c0fdc1327a",
        "parentId" : "578512a0-e5bb-487b-aadf-a2252cc92412",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "any reason? quoted might be more readable.",
        "createdAt" : "2019-07-17T12:32:17Z",
        "updatedAt" : "2019-07-24T02:11:01Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "f58abdf96690cb70cc526ca87d35733122ff2210",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +716,720 @@\tn, ok := cache.nodes[nodeName]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"node %q not found in cache\", nodeName)\n\t}\n"
  },
  {
    "id" : "d91639db-07da-4651-b578-7232b9bb31f6",
    "prId" : 80084,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80084#pullrequestreview-268354374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I like this approach better, but there is one important missing part. The scheduler takes a snapshot of its cache and works with that snapshot for scheduling a pod. The snapshot remains unchanged until a scheduling decision is made. This helps avoid race conditions between a scheduling cycle and the informer events that update the scheduler cache in parallel. I think a similar snapshotting mechanism is needed for CSINodes.",
        "createdAt" : "2019-07-19T01:32:59Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "0c27c19d-2efb-4ba7-86cf-504828633bca",
        "parentId" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "What kind of races does the snapshotting avoid?  I don't expect CSINode to be updated frequently in the cluster.",
        "createdAt" : "2019-07-19T01:47:03Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "7db6b20f-c89e-42ed-9093-33f322ea071c",
        "parentId" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Imagine a function that checks certain things for CSINode object. The first check passes, the object is updated in the cache, the second check runs. Often times our logic is not designed to handle such scenarios. ",
        "createdAt" : "2019-07-19T22:10:25Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "cd17e4eb-4a20-4bf0-989b-d83e503fd887",
        "parentId" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "We don't currently cache anything related to CSINode except the API objects themselves and they are not modified.",
        "createdAt" : "2019-07-19T22:25:22Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "d61def91-b452-4221-8a93-3f8df885ba3b",
        "parentId" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "If they are never modified, why do we have an [`update` function](https://github.com/kubernetes/kubernetes/pull/80084/files#diff-f4a894ca5e905aa5f613269fc967fe2cR587) for them? Also deletion of an object from the map could potentially cause problems. For example, a function may check that an object is in the map once and then index the map later on without further checks. If the object is deleted from the map, the map will return nil and could potentially cause nil pointer dereference. ",
        "createdAt" : "2019-07-19T22:42:00Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "b87c5f69-758c-4639-b268-440e45f11052",
        "parentId" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "The API objects themselves are updated, however what I meant is that we do not cache any extra state based on those objects, and we don't modify those API objects in-memory to store temporary state.\r\n\r\nMy understanding of the snapshot capability is so that you don't have corner cases where the scheduler logic reads the value multiple times and gets different values.  I don't think that is an issue for this predicate. We only read and use the CSINode value once per node per plugin.  The CSINode object is also not expected to be frequently updated. It only changes when a driver is installed or upgraded in the cluster.\r\n\r\nIf the value we use in the predicate changes while we're still processing the predicate, that's ok. It's a similar scenario where the value might change after we have already made a scheduling decision.",
        "createdAt" : "2019-07-22T22:36:08Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "3ff3449f-0e91-4154-b851-485d644a1056",
        "parentId" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "> My understanding of the snapshot capability is so that you don't have corner cases where the scheduler logic reads the value multiple times and gets different values. I don't think that is an issue for this predicate. We only read and use the CSINode value once per node per plugin.\r\n\r\nIf that's the case, I agree that snapshotting CSINodes is not necessary.",
        "createdAt" : "2019-07-29T23:00:10Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "41c6c1ab-fa16-4c50-aa82-a717b0d04ced",
        "parentId" : "1e8a08f2-5a2c-4cc2-9be4-b38f7b5a39c7",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "@bsalamat, @msau42: thanks for looking into this. I made some small adjustments and I think this PR is ready for review again.",
        "createdAt" : "2019-07-30T12:29:17Z",
        "updatedAt" : "2019-08-01T08:16:53Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5d9af2bda5a628b1a4123b4bfff6029312c32cc",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +71,75 @@\tpodStates map[string]*podState\n\tnodes     map[string]*nodeInfoListItem\n\tcsiNodes  map[string]*storagev1beta1.CSINode\n\t// headNode points to the most recently updated NodeInfo in \"nodes\". It is the\n\t// head of the linked list."
  },
  {
    "id" : "8f8408b5-5f98-42a3-b39f-3a9a800dca18",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-254417200",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c859c59-16be-4b85-94d7-ee12e9cade9c",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "If n, the NodeInfoListItem, was created on line 579, shouldn't cache.removeNodeInfoFromList be called ?",
        "createdAt" : "2019-06-26T07:36:39Z",
        "updatedAt" : "2019-06-26T07:42:36Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +607,611 @@\t\treturn fmt.Errorf(\"node %v is not found\", csiNode.Name)\n\t}\n\tn.info.SetCSINode(nil)\n\tcache.moveNodeInfoToHead(csiNode.Name)\n\treturn nil"
  },
  {
    "id" : "61fe082b-1c2a-42f7-8b6f-8dc194379635",
    "prId" : 75848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75848#pullrequestreview-225273294",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbc5c13f-d295-4ff1-9946-2dd0de8053ac",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "Did you change this because golang doesn't let you take the address of a returned value?",
        "createdAt" : "2019-04-10T23:53:40Z",
        "updatedAt" : "2019-04-29T23:41:08Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      }
    ],
    "commit" : "83828bcb2df27f3111837226ce87258d76533090",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +123,127 @@\n// NewNodeInfoSnapshot initializes a NodeInfoSnapshot struct and returns it.\nfunc NewNodeInfoSnapshot() *NodeInfoSnapshot {\n\treturn &NodeInfoSnapshot{\n\t\tNodeInfoMap: make(map[string]*schedulernodeinfo.NodeInfo),"
  },
  {
    "id" : "ec725b91-3b04-4583-8896-c8a5fae7d10e",
    "prId" : 74041,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74041#pullrequestreview-204040631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3dad3ad9-f5b5-49cb-88be-50fe2b324ce0",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "If we're going to shorten the variable name for type `NodeInfo` I'd rather see `node` than `info`. After all, info is short for information, and don't all variables carry information? I am not worried that someone will get confused between `NodeInfo` and `v1.Node` in this context.",
        "createdAt" : "2019-02-14T23:31:47Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "1e012043-4b4c-4061-a007-2426d1e36206",
        "parentId" : "3dad3ad9-f5b5-49cb-88be-50fe2b324ce0",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "The reason that I went with `info` is that the map structure that uses this struct is called `nodes`. So, with this field called info, we have code like `node.info` and `n.info`. If this is called `node`, then we would have `node.node` and `n.node` which would look a bit odd.",
        "createdAt" : "2019-02-15T00:45:48Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "337cb7036cc260264ebc8e4ad2925040de75ba65",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +52,56 @@// The items closer to the head are the most recently updated items.\ntype nodeInfoListItem struct {\n\tinfo *schedulernodeinfo.NodeInfo\n\tnext *nodeInfoListItem\n\tprev *nodeInfoListItem"
  },
  {
    "id" : "6ac1ee20-b79a-4cfa-b793-765cecccef4d",
    "prId" : 74041,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74041#pullrequestreview-204040631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e056f821-d570-45e2-b033-f162ab6283d7",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "should we return here? is this covered by test?",
        "createdAt" : "2019-02-14T23:33:36Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "d69be811-5b44-4926-87fe-fe03680a0277",
        "parentId" : "e056f821-d570-45e2-b033-f162ab6283d7",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Added a return. I have added tests for nodes that don't exists, but the caller of this function check existence of the node in the cache and return before calling this function.",
        "createdAt" : "2019-02-15T00:50:01Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "337cb7036cc260264ebc8e4ad2925040de75ba65",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +135,139 @@\tni, ok := cache.nodes[name]\n\tif !ok {\n\t\tklog.Errorf(\"No NodeInfo with name %v found in the cache\", name)\n\t\treturn\n\t}"
  },
  {
    "id" : "71fc64b9-e8e3-48ca-88fe-b29ac611ff47",
    "prId" : 74041,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74041#pullrequestreview-205515370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61a50f9a-cec7-48cb-8cf6-96a49202d487",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "I know you're not changing this logic, but I don't like that it is here and I will look into it later.",
        "createdAt" : "2019-02-14T23:38:33Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "7a411825-7cce-4438-95d6-2e7b42e181a8",
        "parentId" : "61a50f9a-cec7-48cb-8cf6-96a49202d487",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Yes, we should revisit this and move it out of here.",
        "createdAt" : "2019-02-19T23:56:29Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "337cb7036cc260264ebc8e4ad2925040de75ba65",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +225,229 @@\t\tif balancedVolumesEnabled && node.info.TransientInfo != nil {\n\t\t\t// Transient scheduler info is reset here.\n\t\t\tnode.info.TransientInfo.ResetTransientSchedulerInfo()\n\t\t}\n\t\tif np := node.info.Node(); np != nil {"
  }
]