[
  {
    "id" : "9e60f7fd-a9ef-45a5-8a17-f6ac78bda7cc",
    "prId" : 66733,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66733#pullrequestreview-141709359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed71b135-54d9-4d77-bdb3-5975f4dc8fe5",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "I think we should catch error and then return it here.",
        "createdAt" : "2018-07-29T01:17:25Z",
        "updatedAt" : "2018-08-17T18:19:03Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "73e3fc81-31ae-4be3-acf0-9d550efe1d59",
        "parentId" : "ed71b135-54d9-4d77-bdb3-5975f4dc8fe5",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Returning an error here would be a change of behavior. A caller might assume that when an error is returned, the node is not removed from the cache, while the node is already removed in the above lines. Given this and the fact that nodeTree.RemoveNode returns an error only when the node is not found in the tree, I don't think we should return an error here.",
        "createdAt" : "2018-07-29T03:49:54Z",
        "updatedAt" : "2018-08-17T18:19:03Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "693258df-d956-4e61-8163-01ad424654b7",
        "parentId" : "ed71b135-54d9-4d77-bdb3-5975f4dc8fe5",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Ohh, you are right, then shouldn't we reverse the order, first we would remove the node from the tree and then remove it from cache or atleast log it instead of supressing it?",
        "createdAt" : "2018-07-29T14:56:06Z",
        "updatedAt" : "2018-08-17T18:19:03Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "fb2ec68e-0619-4414-ab9e-a0b769cc7c42",
        "parentId" : "ed71b135-54d9-4d77-bdb3-5975f4dc8fe5",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Not sure about the reverse order. Given that NodeTree is a part of cache, I would prefer to keep it in sync with other cache structures. So, if a node is in the cached NodeInfo struct, I would prefer it to be kept in NodeTree as well.\r\nAdded logging to NodeTree.Remove.",
        "createdAt" : "2018-07-30T20:46:19Z",
        "updatedAt" : "2018-08-17T18:19:03Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "2860743c869f62d448a83469b7748c9758caca7c",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +467,471 @@\t}\n\n\tcache.nodeTree.RemoveNode(node)\n\tcache.removeNodeImageStates(node)\n\treturn nil"
  },
  {
    "id" : "6e222f90-1448-40bf-b471-57816ee66322",
    "prId" : 66224,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66224#pullrequestreview-141290545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af6861a4-0654-49e5-8316-60a6f4e7eb07",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "I think previous line doesn't make much sense now -`state.nodes.Delete(node.Name)` when len(state.nodes) == 0, you can change the order. I'd to do a `!ok` pattern, atleast log something in the !ok block and then in else we can handle this condition.",
        "createdAt" : "2018-07-16T12:06:23Z",
        "updatedAt" : "2018-07-16T12:06:42Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "8ed3426e-a5ed-45ac-bb4f-1c762587bc46",
        "parentId" : "af6861a4-0654-49e5-8316-60a6f4e7eb07",
        "authorId" : "fa530650-5886-4415-a42f-0dee2e0e9ae3",
        "body" : "I think there are still some nits and tests to be fixed as a follow-up for https://github.com/kubernetes/kubernetes/pull/65745. Maybe this can be incorporated into that, or I'll fix it as a follow-up later. :+1: ",
        "createdAt" : "2018-07-16T13:38:11Z",
        "updatedAt" : "2018-07-16T13:38:12Z",
        "lastEditedBy" : "fa530650-5886-4415-a42f-0dee2e0e9ae3",
        "tags" : [
        ]
      },
      {
        "id" : "2d91eb43-7321-44fa-9843-bd81196920bd",
        "parentId" : "af6861a4-0654-49e5-8316-60a6f4e7eb07",
        "authorId" : "02c0526d-01bd-4cb0-97b2-08ed2933051b",
        "body" : "@nikhita Thank you for the fix! I agree more tests are needed - we are working on integration and e2e test at https://github.com/kubernetes/kubernetes/pull/64662 - pls let me know if you find any potential cases we might need to cover.",
        "createdAt" : "2018-07-27T22:55:22Z",
        "updatedAt" : "2018-07-27T22:55:22Z",
        "lastEditedBy" : "02c0526d-01bd-4cb0-97b2-08ed2933051b",
        "tags" : [
        ]
      }
    ],
    "commit" : "c166743272c7e703be62060f63181b7dfb829593",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +507,511 @@\t\t\tif ok {\n\t\t\t\tstate.nodes.Delete(node.Name)\n\t\t\t\tif len(state.nodes) == 0 {\n\t\t\t\t\t// Remove the unused image to make sure the length of\n\t\t\t\t\t// imageStates represents the total number of different"
  },
  {
    "id" : "2d204944-5539-45f3-b5bb-a0fb1405cc0c",
    "prId" : 65745,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65745#pullrequestreview-136878277",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3338aadd-1108-44bc-9874-a21febc1f80c",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Please remove this blank line",
        "createdAt" : "2018-07-13T01:01:17Z",
        "updatedAt" : "2018-07-13T01:01:17Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2003a0db97d421a3e30ac29bf1b1e0484659622d",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +141,145 @@\tcache.mu.Lock()\n\tdefer cache.mu.Unlock()\n\n\tfor name, info := range cache.nodes {\n\t\tif utilfeature.DefaultFeatureGate.Enabled(features.BalanceAttachedNodeVolumes) && info.TransientInfo != nil {"
  },
  {
    "id" : "5c1f1277-6d00-467c-ae81-89caa63b5027",
    "prId" : 65745,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65745#pullrequestreview-136879170",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d7079bd-5c43-45c8-baae-0c7e403a357a",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "I think you could just pass node.Status.Images here.",
        "createdAt" : "2018-07-13T01:08:12Z",
        "updatedAt" : "2018-07-13T01:08:12Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2003a0db97d421a3e30ac29bf1b1e0484659622d",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +469,473 @@// addNodeImageStates adds states of the images on given node to the given nodeInfo and update the imageStates in\n// scheduler cache. This function assumes the lock to scheduler cache has been acquired.\nfunc (cache *schedulerCache) addNodeImageStates(node *v1.Node, nodeInfo *NodeInfo) {\n\tnewSum := make(map[string]*ImageStateSummary)\n"
  },
  {
    "id" : "a59f0ea1-bfc7-437f-9ce5-93c44925f165",
    "prId" : 65745,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65745#pullrequestreview-136881891",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe62d281-868c-4d3a-9108-c78d8f65639e",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "I think SetNode could return an error rendering this cache addition invalid.",
        "createdAt" : "2018-07-13T01:30:46Z",
        "updatedAt" : "2018-07-13T01:30:46Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2003a0db97d421a3e30ac29bf1b1e0484659622d",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +427,431 @@\t}\n\n\tcache.addNodeImageStates(node, n)\n\treturn n.SetNode(node)\n}"
  },
  {
    "id" : "a1b3ff07-db61-411c-a4aa-928fab1385de",
    "prId" : 65714,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65714#pullrequestreview-136281518",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da980b34-4a70-40e8-bbc6-57bc4b3b4324",
        "parentId" : null,
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Comment for reviewer: this change is to improve `cache.IsUpToDate`",
        "createdAt" : "2018-07-03T07:41:34Z",
        "updatedAt" : "2018-07-18T07:12:43Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "2909fe6c-6aaa-4d57-96ed-1b8186b995f8",
        "parentId" : "da980b34-4a70-40e8-bbc6-57bc4b3b4324",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "Did we forget to approve/merge the PR where someone else made this change? /=",
        "createdAt" : "2018-07-09T17:30:59Z",
        "updatedAt" : "2018-07-18T07:12:43Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "d8d96bb6-73a4-4650-baed-9eb8fe5a2830",
        "parentId" : "da980b34-4a70-40e8-bbc6-57bc4b3b4324",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Nope, that previous PR only improved `ecache`, not `cache` :D",
        "createdAt" : "2018-07-10T12:58:32Z",
        "updatedAt" : "2018-07-18T07:12:43Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "5d58dd97-368f-4011-b525-83c402983038",
        "parentId" : "da980b34-4a70-40e8-bbc6-57bc4b3b4324",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "So, the way I see it, we are making 2 changes as part of this PR,\r\n\r\n- Changing scheduler cache's lock from Mutex to RWMutex\r\n- Changing ecache to be 2 level cache.\r\n\r\nSo, the flame graphs attached, could be result of both the changes? Am I missing something?",
        "createdAt" : "2018-07-11T11:29:07Z",
        "updatedAt" : "2018-07-18T07:12:43Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "2bb71f7e-f70e-4168-afe6-b1757eaba51d",
        "parentId" : "da980b34-4a70-40e8-bbc6-57bc4b3b4324",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "The scheduler cache refactoring is just a bonus :D Performance improvement is brought by 2 level cache which is the core idea of this patch.",
        "createdAt" : "2018-07-11T14:59:24Z",
        "updatedAt" : "2018-07-18T07:12:44Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5a7a4caf71ac63651a1abfada3850fb37ea44a8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@\n\t// This mutex guards all fields within this cache struct.\n\tmu sync.RWMutex\n\t// a set of assumed pod keys.\n\t// The key could further be used to get an entry in podStates."
  },
  {
    "id" : "b848e64e-d7e8-49c8-85ba-abf7a0b0f10a",
    "prId" : 65714,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65714#pullrequestreview-138126820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aed0d988-8870-41c9-b15c-989fee71fd8e",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "The lock in the above function (ListPDBs) should also be RLock.",
        "createdAt" : "2018-07-17T20:27:29Z",
        "updatedAt" : "2018-07-18T07:12:44Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "5968b55e-9ba4-45b0-b739-2ed48142d87a",
        "parentId" : "aed0d988-8870-41c9-b15c-989fee71fd8e",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Oh, nice catch, just fixed",
        "createdAt" : "2018-07-18T07:12:58Z",
        "updatedAt" : "2018-07-18T07:12:58Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5a7a4caf71ac63651a1abfada3850fb37ea44a8",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +552,556 @@\nfunc (cache *schedulerCache) IsUpToDate(n *NodeInfo) bool {\n\tcache.mu.RLock()\n\tdefer cache.mu.RUnlock()\n\tnode, ok := cache.nodes[n.Node().Name]"
  },
  {
    "id" : "b26c1478-8522-460c-95f9-a66a5b7f7210",
    "prId" : 64692,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64692#pullrequestreview-126219521",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51bb7dcd-2116-4602-ad60-c50113ad859c",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Would you share the case that this PR handled? And as we're changed to use pod's UID as key, that may make cache in-consistent.",
        "createdAt" : "2018-06-04T13:54:06Z",
        "updatedAt" : "2018-06-06T13:13:38Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "4f86481b-03d7-495b-8a08-586c17de7f9e",
        "parentId" : "51bb7dcd-2116-4602-ad60-c50113ad859c",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "oh, sorry for confusion: the UID did not change when updating pod. so the fix seems ok. Let me check it today :)",
        "createdAt" : "2018-06-04T22:47:39Z",
        "updatedAt" : "2018-06-06T13:13:38Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "d0337207-04ef-4145-9009-531e0048f0bb",
        "parentId" : "51bb7dcd-2116-4602-ad60-c50113ad859c",
        "authorId" : "9eb1241d-3dca-4a34-a85d-a880ba615f8c",
        "body" : "@k82cn I think this PR just makes sure the cache always keep consistent. We won't have any problem now if we don't update the podStates map, but if pod resources could be updated(like in-place pod resource update), this will bring problem.",
        "createdAt" : "2018-06-05T05:41:43Z",
        "updatedAt" : "2018-06-06T13:13:38Z",
        "lastEditedBy" : "9eb1241d-3dca-4a34-a85d-a880ba615f8c",
        "tags" : [
        ]
      },
      {
        "id" : "2fc8a9af-95e3-4fe5-8f75-1168e979598a",
        "parentId" : "51bb7dcd-2116-4602-ad60-c50113ad859c",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Yes, resource update is one case. Another case is `GetPod`, which may return 'old' pod; can you help to add a test for it? For assumed pod, `GetPod` will return the correct value; but for assigned pods, it may return old pod (although no one uses this feature right now).",
        "createdAt" : "2018-06-06T02:37:20Z",
        "updatedAt" : "2018-06-06T13:13:38Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "bf17ba72-f181-4800-aa0f-955b95ca7b3e",
        "parentId" : "51bb7dcd-2116-4602-ad60-c50113ad859c",
        "authorId" : "9eb1241d-3dca-4a34-a85d-a880ba615f8c",
        "body" : "sure I will add a test case asap.",
        "createdAt" : "2018-06-06T03:01:32Z",
        "updatedAt" : "2018-06-06T13:13:38Z",
        "lastEditedBy" : "9eb1241d-3dca-4a34-a85d-a880ba615f8c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6116c64f64271046425874d569da1e754b22eca0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +318,322 @@\t\t\treturn err\n\t\t}\n\t\tcurrState.pod = newPod\n\tdefault:\n\t\treturn fmt.Errorf(\"pod %v is not added to scheduler cache, so cannot be updated\", key)"
  }
]