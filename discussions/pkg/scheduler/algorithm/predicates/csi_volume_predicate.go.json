[
  {
    "id" : "45231508-29ce-446a-aed8-b65d69ca236a",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245097473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e994f05-1cf4-4cf9-9df2-5fd1257a919a",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "So this code will not prevent scheduling of pods to a node which does not have CSI driver installed yet. The KEP states that - for migrated plugins we should immediately stop scheduling of pods to nodes where driver is not installed and for other (non-migrated drivers) after 2 releases we are going to stop scheduling of pods to the node where driver is not installed.",
        "createdAt" : "2019-05-31T14:39:30Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "0d834a34-a8b2-4fc4-bc0c-7d9a3eba0dd9",
        "parentId" : "1e994f05-1cf4-4cf9-9df2-5fd1257a919a",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Same as https://github.com/kubernetes/kubernetes/pull/77595/files#r289505445\r\n\r\nAdded check",
        "createdAt" : "2019-06-03T08:24:27Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "f2e51617-4430-4978-b933-243ba0850a3e",
        "parentId" : "1e994f05-1cf4-4cf9-9df2-5fd1257a919a",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I am probably missing something but where did you add that check? AFAICT - we are still not preventing pod scheduling if migration is enabled for a CSI volume and driver is not installed on the node. \r\n",
        "createdAt" : "2019-06-03T20:08:28Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +143,147 @@\t\tvolumeUniqueName := fmt.Sprintf(\"%s/%s\", driverName, volumeHandle)\n\t\tvolumeLimitKey := volumeutil.GetCSIAttachLimitKey(driverName)\n\t\tresult[volumeUniqueName] = volumeLimitKey\n\t}\n\treturn nil"
  },
  {
    "id" : "18efc329-b538-47f7-b004-fe136819c834",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-244724433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33e679f7-8fd7-4d9d-a3b2-d687864e34ef",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Do we have a scenario where both CSI and in-tree driver can be in-use on a node? If yes - this code will not count them correctly. cc @davidz627 @jsafrane ",
        "createdAt" : "2019-05-31T14:56:39Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "75eac98f-925e-4cc3-a0d5-a098a253c5a7",
        "parentId" : "33e679f7-8fd7-4d9d-a3b2-d687864e34ef",
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "Do you mean when migration is off and we have the in-tree plugin and its \"migration\" CSI Driver both be in use on the node?\r\n\r\nThis is possible.",
        "createdAt" : "2019-05-31T18:21:53Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "f3ae094d-a518-420b-a686-5f5b313458fd",
        "parentId" : "33e679f7-8fd7-4d9d-a3b2-d687864e34ef",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "The scenario that @davidz627 described should be covered: if migration is off, in-tree and CSI volumes will be counted separately.\r\n\r\nAlso, if migration is on, in-tree and CSI volumes from the same storage backend will be counted together, since they are both supposed to be provided by the CSI driver.",
        "createdAt" : "2019-06-03T08:35:28Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +199,203 @@\t\t}\n\n\t\tcsiSource = csiPV.Spec.PersistentVolumeSource.CSI\n\t}\n"
  },
  {
    "id" : "52c94bb1-9459-4028-bf9f-7508105751f0",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245263690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e091dc2a-107f-4bee-a8fd-12e71e250408",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Add a comment describing the function and its return values",
        "createdAt" : "2019-06-03T14:15:53Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "2e6646ed-699e-431b-a744-c081060a2d60",
        "parentId" : "e091dc2a-107f-4bee-a8fd-12e71e250408",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Added",
        "createdAt" : "2019-06-04T07:37:06Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +151,155 @@// If the PVC is from a migrated in-tree plugin, this function will return\n// the information of the CSI driver that the plugin has been migrated to.\nfunc (c *CSIMaxVolumeLimitChecker) getCSIDriverInfo(csiNode *storagev1beta1.CSINode, pvc *v1.PersistentVolumeClaim) (string, string) {\n\tpvName := pvc.Spec.VolumeName\n\tnamespace := pvc.Namespace"
  },
  {
    "id" : "95830d09-4f9f-4770-b8c7-4f31f9445ee1",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245279174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bf58e45-3b98-46b3-b098-53697a4a07de",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I'm unsure if this is case we need to handle.   I believe volumeID must be unique, so we may need to fix mock driver if it's not following this",
        "createdAt" : "2019-06-03T14:25:14Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "3c48f3ff-76b1-4e92-bad4-e3d82b0e00d5",
        "parentId" : "1bf58e45-3b98-46b3-b098-53697a4a07de",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "In theory, different CSI drivers may use the same VolumeID for different volumes.",
        "createdAt" : "2019-06-04T07:44:53Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +141,145 @@\t\t}\n\n\t\tvolumeUniqueName := fmt.Sprintf(\"%s/%s\", driverName, volumeHandle)\n\t\tvolumeLimitKey := volumeutil.GetCSIAttachLimitKey(driverName)\n\t\tresult[volumeUniqueName] = volumeLimitKey"
  },
  {
    "id" : "b4157e46-8e43-444a-a7b7-338b3e010da5",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245970749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cae7738c-9de8-49dd-8d52-c7f244333b79",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "We're missing a check (probably somewhere around L108) that limit should == 0 if an entry doesn't exist.\r\n\r\nWe may be able to make future optimizations around this area too to not have to count volumes if no CSI drivers are installed on the node.",
        "createdAt" : "2019-06-03T14:33:55Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "d7f06773-733a-4e7e-81d7-ea4e5cb226c5",
        "parentId" : "cae7738c-9de8-49dd-8d52-c7f244333b79",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Actually there's two cases we need to handle:\r\n- csi driver not installed: limit is 0\r\n- csi driver installed but doesn't report limits: limit is infinite",
        "createdAt" : "2019-06-03T20:17:35Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "75b71e26-077d-4a55-8e3a-fb7eedeae87e",
        "parentId" : "cae7738c-9de8-49dd-8d52-c7f244333b79",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Can you take a look if https://github.com/kubernetes/kubernetes/pull/77595/commits/8912fa28ea7564a5f01b7acd4066d099fcfea5ed addresses this?\r\n\r\nI'm assuming that a `0` limit means no limit. The specs says that the CO should decide how many volumes can be published if the driver reports `.",
        "createdAt" : "2019-06-04T16:31:24Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "8ce85878-fe33-4168-9240-4eb404067867",
        "parentId" : "cae7738c-9de8-49dd-8d52-c7f244333b79",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Sorry, new commit: https://github.com/kubernetes/kubernetes/pull/77595/commits/8912fa28ea7564a5f01b7acd4066d099fcfea5ed",
        "createdAt" : "2019-06-04T17:14:10Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "09a080b8-34a8-412a-a2d5-bdd357885f1e",
        "parentId" : "cae7738c-9de8-49dd-8d52-c7f244333b79",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "CC @gnufied ",
        "createdAt" : "2019-06-04T17:27:16Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "33b28a8b-ae12-4d93-81f5-ea92894ef6d1",
        "parentId" : "cae7738c-9de8-49dd-8d52-c7f244333b79",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "This should be fixed now.",
        "createdAt" : "2019-06-05T12:24:37Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +64,68 @@\n\tnewVolumes := make(map[string]string)\n\tif err := c.filterAttachableVolumes(nodeInfo, pod.Spec.Volumes, pod.Namespace, newVolumes); err != nil {\n\t\treturn false, nil, err\n\t}"
  },
  {
    "id" : "90aff1d4-f2d7-4a9a-b9f3-1a6352a20409",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245384722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1add3726-30c1-4944-9d22-939bd181f8e0",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Are we also handling inline (non-pv) migration?",
        "createdAt" : "2019-06-04T01:50:43Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "9045cfc4-1616-4ef6-8f15-fd3e8466c09c",
        "parentId" : "1add3726-30c1-4944-9d22-939bd181f8e0",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "No, migration of in-line in-tree volumes was merged last Friday, so there was no time to adopt it.\r\n\r\nIMO it'd be better to handle that in a subsequent PR (i.e., in the next cycle), if that's possible.",
        "createdAt" : "2019-06-04T08:20:37Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "fd449f0a-4eb7-4fec-adf9-8f8c03657673",
        "parentId" : "1add3726-30c1-4944-9d22-939bd181f8e0",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "That's fine. Migration is still alpha this release. I think most important is to convert existing CSI limits to use the new csinode field.",
        "createdAt" : "2019-06-04T11:27:17Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +188,192 @@\t\t}\n\n\t\tcsiPV, err := csilib.TranslateInTreePVToCSI(pv)\n\t\tif err != nil {\n\t\t\tklog.V(5).Infof(\"Unable to translate in-tree volume to CSI: %v\", err)"
  },
  {
    "id" : "fdd0a408-fafa-4650-9e3a-1694bf4cc9cd",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245634287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a174c67-cbde-4795-b993-aba744dc7d5c",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "please remove `v1` alias:\r\n\r\n```suggestion\r\n\t\"k8s.io/api/core/v1\"\r\n```",
        "createdAt" : "2019-06-04T04:06:46Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "1498b457-096c-4b40-9f5d-2f06ad187784",
        "parentId" : "6a174c67-cbde-4795-b993-aba744dc7d5c",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "This is autoadded by goimports. It's not worth fighting the tool, in my opinion ",
        "createdAt" : "2019-06-04T04:10:59Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "91760045-5dda-4587-bcab-0e336dbc73a9",
        "parentId" : "6a174c67-cbde-4795-b993-aba744dc7d5c",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Yes, but it's not worth introducing a dummy alias. I'm now turning to `gofmt`, and I have a long post at https://groups.google.com/d/msg/kubernetes-dev/Y2ShVRrU4xM/LKfaiFqAAwAJ explaining how to configure in vscode.",
        "createdAt" : "2019-06-04T04:16:38Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "9a1eb5f5-7878-490d-a671-7233bf852656",
        "parentId" : "6a174c67-cbde-4795-b993-aba744dc7d5c",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "I agree that it's annoying, but it changes nothing else in the code. I saw it in this PR and decided not to mention it since that's just extra work for the author and us the reviewers.\r\n\r\nIncidentally I recall sending feedback on this behavior and getting a \"working as intended\" response, but I couldn't find that response when I looked for it. I think the tool assumes that our module is actually named \"core\" and uses semver so that major versions have different import paths. Therefore, the `v1` alias is an indication that no, actually our module is named \"v1\".\r\n\r\nI know it's convention not to alias this import, but I think if we started aliasing it as `core` that things like `core.Pod` would make more sense.",
        "createdAt" : "2019-06-04T17:26:55Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "f9069609-dffe-4651-94ca-5790d9d568cc",
        "parentId" : "6a174c67-cbde-4795-b993-aba744dc7d5c",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "https://github.com/golang/go/issues/30051",
        "createdAt" : "2019-06-04T18:48:08Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +20,24 @@\t\"fmt\"\n\n\tv1 \"k8s.io/api/core/v1\"\n\tstoragev1beta1 \"k8s.io/api/storage/v1beta1\"\n\t\"k8s.io/apimachinery/pkg/util/rand\""
  },
  {
    "id" : "dd0fddf4-4621-421f-92b8-ab069b0f210e",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245545127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58c58ada-ae42-46ef-ac0b-ee6beb1828b1",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "I think it's better to check if nodeInfo is nil here as well before passing on the pointer.",
        "createdAt" : "2019-06-04T15:58:42Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +63,67 @@\t}\n\n\tnewVolumes := make(map[string]string)\n\tif err := c.filterAttachableVolumes(nodeInfo, pod.Spec.Volumes, pod.Namespace, newVolumes); err != nil {\n\t\treturn false, nil, err"
  },
  {
    "id" : "0a827e69-f46e-440b-ba05-9b5bf5ee39a9",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245612370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "063f0791-72c9-45f1-9e79-e1194267cfc4",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Why can't we have this check earlier and exit earlier?",
        "createdAt" : "2019-06-04T16:34:16Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "6e996c68-db19-48d6-a87e-2f0098f7e707",
        "parentId" : "063f0791-72c9-45f1-9e79-e1194267cfc4",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "I'm using this map to verify if a potential CSI driver is installed on the node and report the appropriate error.",
        "createdAt" : "2019-06-04T18:05:40Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +74,78 @@\n\t// If the node doesn't have volume limits, the predicate will always be true\n\tnodeVolumeLimits := nodeInfo.VolumeLimits()\n\tif len(nodeVolumeLimits) == 0 {\n\t\treturn true, nil, nil"
  },
  {
    "id" : "31e21573-10dc-4d11-8dd8-375f69ebfdd7",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245630078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a5de347-a075-402f-9d06-ff22665aa792",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "What if we return \"\", \"\"  but at https://github.com/kubernetes/kubernetes/pull/77595/commits/396b38f9bd2a6bd494112d4451134976bb598afc#diff-950de94621e8191fe9ca3368eef89318R208, shouldn't we log here?",
        "createdAt" : "2019-06-04T17:57:17Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "0b6c9336-6674-4991-8d50-937f5883cfa4",
        "parentId" : "1a5de347-a075-402f-9d06-ff22665aa792",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Added",
        "createdAt" : "2019-06-04T18:40:37Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +135,139 @@\n\t\tcsiNode := nodeInfo.CSINode()\n\t\tdriverName, volumeHandle := c.getCSIDriverInfo(csiNode, pvc)\n\t\tif driverName == \"\" || volumeHandle == \"\" {\n\t\t\tklog.V(5).Infof(\"Could not find a CSI driver name or volume handle, not counting volume\")"
  },
  {
    "id" : "8de74f63-5045-4f7d-a0ba-984742001dd9",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-250927881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f54b96d0-a8a7-40c3-9647-5ebe790e00ef",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can you add a comment here to say don't count multiple pods sharing the same volume twice.",
        "createdAt" : "2019-06-17T22:45:30Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "f2e8c7b2-3e18-46c1-8e82-609719363c45",
        "parentId" : "f54b96d0-a8a7-40c3-9647-5ebe790e00ef",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Added comment",
        "createdAt" : "2019-06-18T08:16:25Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +90,94 @@\t\tif _, ok := newVolumes[volumeUniqueName]; ok {\n\t\t\t// Don't count single volume used in multiple pods more than once\n\t\t\tdelete(newVolumes, volumeUniqueName)\n\t\t}\n\t\tattachedVolumeCount[volumeLimitKey]++"
  },
  {
    "id" : "809942e5-4974-4e44-83b6-da61ab6912b5",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-250927881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99f21954-1f6a-46db-b775-ab00c9e16a16",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "nil checks for each field that's dereferenced?",
        "createdAt" : "2019-06-17T22:51:26Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "075b3c8b-43e4-4e4d-ae3f-ed4e849c2a6e",
        "parentId" : "99f21954-1f6a-46db-b775-ab00c9e16a16",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "`CSI` is the only pointer",
        "createdAt" : "2019-06-18T08:16:58Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +194,198 @@\t\t}\n\n\t\tif csiPV.Spec.PersistentVolumeSource.CSI == nil {\n\t\t\tklog.V(5).Infof(\"Unable to get a valid volume source for translated PV %s\", pvName)\n\t\t\treturn \"\", \"\""
  },
  {
    "id" : "e2807d8a-b84b-454e-9344-0e28bf3cc8db",
    "prId" : 73863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73863#pullrequestreview-206644661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "256ee76b-da8a-4b2c-9c91-7acb453ad62d",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can you add a comment here that this is an error condition where either:\r\n* User prebound the PVC to a non-existing PV\r\n* PV object got deleted erroneously\r\n\r\nShould we fallback to SC.provisionername?",
        "createdAt" : "2019-02-15T01:10:05Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "0b40bce1-6a3f-4540-b783-8feb51275646",
        "parentId" : "256ee76b-da8a-4b2c-9c91-7acb453ad62d",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Don't we have a pv proection controller to prevent things like PV object getting deleted erroneously? First one is more likely perhaps - but then it seems like no pod can start using such a PVC. ",
        "createdAt" : "2019-02-15T03:34:36Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "47fbced2-2ffa-4cf9-9b8c-3885c9dfc07b",
        "parentId" : "256ee76b-da8a-4b2c-9c91-7acb453ad62d",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I'm trying to think of the case where someone bypasses the scheduler by directly setting Pod.nodename and they have prebound the PVC and the PV gets bound in the middle.  Then we won't count it.  But the same issue also exists if they have an empty or non-existent storageclass.  So I'm not sure if we'll be able to fully solve this race.",
        "createdAt" : "2019-02-16T02:37:56Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "173476f3-49db-44c5-b863-42c26be13ea8",
        "parentId" : "256ee76b-da8a-4b2c-9c91-7acb453ad62d",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "So if they prebound the PVC then most likely it will have missing SC too right? Although likely-hood of binding happening in middle for prebound PVCs is small. I am for leaving this code as it is. I think this is \"good middleground\". ",
        "createdAt" : "2019-02-19T22:23:26Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "98a378f0-584f-491b-b93a-d3861826f9b3",
        "parentId" : "256ee76b-da8a-4b2c-9c91-7acb453ad62d",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can we require that users create a StorageClass object with correct provisioner name even for prebinding PVC/PV, in order for the volume counting to work correctly?  ",
        "createdAt" : "2019-02-19T23:51:04Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "4b95285b-7f13-4e9f-b208-969ef8c4a59d",
        "parentId" : "256ee76b-da8a-4b2c-9c91-7acb453ad62d",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I am not sure. So if a PVC is not bound and does not have a storageclass with delayed binding (because user is trying to submit pv/pvc/pod template at once) - won't the scheduler not schedule the pod until PVC is bound?  That seems to be my experience and in which case - for pre-bound PVCs this will be much less of a problem. Am I missing something?",
        "createdAt" : "2019-02-20T03:09:17Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "034967a3-1a34-4e25-809f-e9aefa563162",
        "parentId" : "256ee76b-da8a-4b2c-9c91-7acb453ad62d",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "added fallback. ",
        "createdAt" : "2019-02-22T03:25:05Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3a93c560b4d3ce97d3327cc476aca7349bee36e",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +161,165 @@\n\tif err != nil {\n\t\tklog.V(4).Infof(\"Unable to look up PV info for PVC %s/%s and PV %s\", namespace, pvcName, pvName)\n\t\t// If we can't fetch PV associated with PVC, may be it got deleted\n\t\t// or PVC was prebound to a PVC that hasn't been created yet."
  },
  {
    "id" : "f641529d-2647-4ca5-b889-0efe6e9a5b08",
    "prId" : 73863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73863#pullrequestreview-214764760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7655542-96eb-4df8-b9b7-ddb393c77183",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "i forget, why is the prefix needed?",
        "createdAt" : "2019-02-15T01:13:34Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "5c120784-bb73-423c-ac71-f0d22db6a01c",
        "parentId" : "f7655542-96eb-4df8-b9b7-ddb393c77183",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "hmm if it's not bound, we use pvc name as the volume handle. If it's bound, we use the volumeID.  \r\n\r\nThis inconsistency might have races where the PVC got bound in the middle of the predicate.\r\n\r\nBut if we only use PVC name, then this may not work when we support inline volumes, where the same volume can be specified as a PVC and as an inline volume.  We could also just say we don't support that case...",
        "createdAt" : "2019-02-15T01:18:00Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "b5a18551-d853-4929-a906-01774c4cdeac",
        "parentId" : "f7655542-96eb-4df8-b9b7-ddb393c77183",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "The prefix is used to avoid conflict with volume-ids (however unlikely).\r\n\r\nWhat race you are thinking of? Even if PVC gets bound in the middle of the predicate and may be there is a pod that uses same volume inline(on same node), in worst case we end up overcounting the volume?",
        "createdAt" : "2019-02-15T03:30:13Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "2cafad1d-7214-4479-8e53-63b6f925827e",
        "parentId" : "f7655542-96eb-4df8-b9b7-ddb393c77183",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Ah yes you're right, it would end up overcounting in that case, which is better than undercounting.",
        "createdAt" : "2019-02-16T02:32:10Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "34f7bebb-51aa-4c7f-b020-3e182b43c8bb",
        "parentId" : "f7655542-96eb-4df8-b9b7-ddb393c77183",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "we'll need to consider potential ramifications of overcounting if we decide to cache counts in the future. can you add this possible race condition as a comment?\r\n\r\nfyi persistent inline volume support got dropped so we could potentially use pvc name throughout.  although we may need to think about if we want to do any sort of counting for ephemeral volumes (not attach)",
        "createdAt" : "2019-02-22T21:54:11Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "202f3c44-709f-4c49-927f-155cfca3f05f",
        "parentId" : "f7655542-96eb-4df8-b9b7-ddb393c77183",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I have added a comment about potential overcounting of volumes here. ",
        "createdAt" : "2019-03-14T21:08:54Z",
        "updatedAt" : "2019-03-14T21:08:54Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3a93c560b4d3ce97d3327cc476aca7349bee36e",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +199,203 @@\t// predicate and there is another pod(on same node) that uses same volume then we will overcount\n\t// the volume and consider both volumes as different.\n\tvolumeHandle := fmt.Sprintf(\"%s-%s/%s\", c.randomVolumeIDPrefix, namespace, pvcName)\n\treturn storageClass.Provisioner, volumeHandle\n}"
  },
  {
    "id" : "83b007d3-e121-407f-99fd-c4855eae911d",
    "prId" : 73863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73863#pullrequestreview-214764592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2cb7514e-2924-466d-97d0-bda63961ef7a",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "or PVC was prebound to a PV that hasn't been created yet",
        "createdAt" : "2019-02-22T21:31:01Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "1f153ea5-8397-4091-b15e-d7c489546f46",
        "parentId" : "2cb7514e-2924-466d-97d0-bda63961ef7a",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "fixed",
        "createdAt" : "2019-03-14T21:08:33Z",
        "updatedAt" : "2019-03-14T21:08:33Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3a93c560b4d3ce97d3327cc476aca7349bee36e",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +162,166 @@\tif err != nil {\n\t\tklog.V(4).Infof(\"Unable to look up PV info for PVC %s/%s and PV %s\", namespace, pvcName, pvName)\n\t\t// If we can't fetch PV associated with PVC, may be it got deleted\n\t\t// or PVC was prebound to a PVC that hasn't been created yet.\n\t\t// fallback to using StorageClass for volume counting"
  },
  {
    "id" : "cb161a96-0ac7-4bab-8f3e-888716f15e8a",
    "prId" : 73863,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73863#pullrequestreview-214764640",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39dcc26a-ee72-438a-8f8e-0f5b7f3b57e6",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Add a comment here, if storage class is not set or found, then it is immediate binding mode, which requires that the pvc is bound before scheduling, so we can safely not count it.",
        "createdAt" : "2019-02-22T21:36:56Z",
        "updatedAt" : "2019-03-14T21:07:55Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "ed208010-df65-4bf0-9e91-cbd596cc6519",
        "parentId" : "39dcc26a-ee72-438a-8f8e-0f5b7f3b57e6",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "fixed",
        "createdAt" : "2019-03-14T21:08:39Z",
        "updatedAt" : "2019-03-14T21:08:39Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3a93c560b4d3ce97d3327cc476aca7349bee36e",
    "line" : 104,
    "diffHunk" : "@@ -1,1 +183,187 @@\tplaceHolderCSIDriver := \"\"\n\tplaceHolderHandle := \"\"\n\tif scName == nil {\n\t\t// if StorageClass is not set or found, then PVC must be using immediate binding mode\n\t\t// and hence it must be bound before scheduling. So it is safe to not count it."
  },
  {
    "id" : "ca14f020-16f7-47fa-9858-6d3cfb99cb6d",
    "prId" : 67731,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67731#pullrequestreview-152278748",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf7d7c2c-fe06-42e1-a2f0-8da5e7efc83f",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Could we not have reused the existing MaxPD predicate and just define a new filter for CSI?",
        "createdAt" : "2018-08-24T01:11:00Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "da330cfd-78ba-4071-b9b1-2e9ab3cf3191",
        "parentId" : "bf7d7c2c-fe06-42e1-a2f0-8da5e7efc83f",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "tests?",
        "createdAt" : "2018-08-24T01:11:15Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "e7c61263-5411-4fbb-ae65-b65da858c4ad",
        "parentId" : "bf7d7c2c-fe06-42e1-a2f0-8da5e7efc83f",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "May be - we can, but it will be ugly because underlying data structures are different. The existing MaxPD* predicates filter volumes based on hardcoded type. For example - \"how many EBS volumes are attached to the node\", but since there can be multiple CSI volumes on a node, the CSI predicate has to count - \"How many volumes of each type are attached to the node\" and same thing applies to uniqueness check and etc.\r\n\r\nAt some point - I would like to consolidate all the predicates in one when `AttachLimit` feature becomes GA, because then all the volume limits will come from node itself and we can remove these older and hardcoded predicates.",
        "createdAt" : "2018-09-04T18:46:07Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "3b539db4-5e3a-48b7-b712-6998346b4bbf",
        "parentId" : "bf7d7c2c-fe06-42e1-a2f0-8da5e7efc83f",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Thanks for the explanation, it makes sense to keep them separate for now",
        "createdAt" : "2018-09-04T18:48:36Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "5fd63827-45d7-48c8-8248-b2ad52d33c8a",
        "parentId" : "bf7d7c2c-fe06-42e1-a2f0-8da5e7efc83f",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "added some tests too btw",
        "createdAt" : "2018-09-04T22:34:37Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc61620db53a9a2ac91de032ea935f8392f64268",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +45,49 @@}\n\nfunc (c *CSIMaxVolumeLimitChecker) attachableLimitPredicate(\n\tpod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {\n"
  },
  {
    "id" : "09783440-23f5-47f4-b171-1bb0733fa652",
    "prId" : 67731,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67731#pullrequestreview-152293539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4a6b52c-d8c9-4e9a-a27b-79b76b93cbc3",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Do we need to revisit this for inline csi volumes?",
        "createdAt" : "2018-09-04T22:50:39Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "e784088c-e896-48dd-ad6d-d771ba4d48c6",
        "parentId" : "f4a6b52c-d8c9-4e9a-a27b-79b76b93cbc3",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I am not 100% sure yet but I thought plan was to keep inline CSI volumes non-attachable. cc @vladimirvivien is that still true?",
        "createdAt" : "2018-09-04T23:03:21Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "61a6b595-f7cd-4ce3-9f4b-c745dd7fcc59",
        "parentId" : "f4a6b52c-d8c9-4e9a-a27b-79b76b93cbc3",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "discussed this offline. The handling of inline CSI volumes is out of scope for this PR. Currently inline CSI volumes are being proposed as an alpha feature. ",
        "createdAt" : "2018-09-04T23:51:11Z",
        "updatedAt" : "2018-09-05T16:29:41Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc61620db53a9a2ac91de032ea935f8392f64268",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +114,118 @@\n\tfor _, vol := range volumes {\n\t\t// CSI volumes can only be used as persistent volumes\n\t\tif vol.PersistentVolumeClaim == nil {\n\t\t\tcontinue"
  }
]