[
  {
    "id" : "b5c70625-6e13-4a03-b001-2c82488e0efc",
    "prId" : 85106,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85106#pullrequestreview-315577401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcea381b-3c38-4cd6-bc37-1bed37c24cbf",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "space at the beginning of the comment above.",
        "createdAt" : "2019-11-11T23:19:28Z",
        "updatedAt" : "2019-11-12T16:02:00Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "f4b7027b-5aa8-45cb-9ffd-cedf2f1a8788",
        "parentId" : "dcea381b-3c38-4cd6-bc37-1bed37c24cbf",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Done",
        "createdAt" : "2019-11-12T14:34:11Z",
        "updatedAt" : "2019-11-12T16:02:00Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "c35fe2c80124f038d295a1a7a15f8d8176260fef",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +206,210 @@\t}\n\t// Pods matched namespace,selector on current node.\n\tvar selector labels.Selector\n\tif firstServiceSelector != nil {\n\t\tselector = firstServiceSelector"
  },
  {
    "id" : "475f7fc0-7c37-478c-9492-25c1573b09fa",
    "prId" : 84738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84738#pullrequestreview-312576101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca473822-c660-4ffc-9045-592fee94e9ae",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "s/host/node everywhere in the file (host is the old name of nodes).",
        "createdAt" : "2019-11-06T15:37:52Z",
        "updatedAt" : "2019-11-08T03:05:00Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "9ed1628c-645f-4774-8e15-87836f98665d",
        "parentId" : "ca473822-c660-4ffc-9045-592fee94e9ae",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "let's not do this since this file is going away soon?",
        "createdAt" : "2019-11-06T16:36:30Z",
        "updatedAt" : "2019-11-08T03:05:00Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4389707302ef38669191a8d1efd379ee1e3b8ec",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +266,270 @@\tmaxPriorityFloat64 := float64(framework.MaxNodeScore)\n\n\tfor _, hostPriority := range mapResult {\n\t\tnumServicePods += hostPriority.Score\n\t\tnodeInfo, err := sharedLister.NodeInfos().Get(hostPriority.Name)"
  },
  {
    "id" : "9263642f-97e5-4493-822a-74a7627ebbe3",
    "prId" : 84738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84738#pullrequestreview-312525920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3d86ce4-6c7c-4e08-81a8-66cafb6361d7",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Say also that we compensate that by artificially multiplying the weight by the number of labels.",
        "createdAt" : "2019-11-06T15:42:51Z",
        "updatedAt" : "2019-11-08T03:05:00Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4389707302ef38669191a8d1efd379ee1e3b8ec",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +293,297 @@\t\t\tfScore = maxPriorityFloat64 * (float64(numServicePods-podCounts[labelValue]) / float64(numServicePods))\n\t\t}\n\t\t// The score of current label only accounts for 1/len(s.labels) of the total score.\n\t\t// The policy API definition only allows a single label to be configured, associated with a weight.\n\t\t// This is compensated by the fact that the total weight is the sum of all weights configured"
  },
  {
    "id" : "f30f485f-126f-46cb-a662-8e0543d7f3b4",
    "prId" : 84738,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84738#pullrequestreview-312576101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ddec684-df7a-45a6-86b0-b6a2d1fe1ac5",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "We need good documentation explaining why we have this awkward implementation: that we are trying to workaround a v1 API that allows defining only a single label, but multiple custom priorities etc.",
        "createdAt" : "2019-11-06T15:46:36Z",
        "updatedAt" : "2019-11-08T03:05:00Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "2a3ef38d-5871-48a2-a3a0-fbb29426b4e0",
        "parentId" : "4ddec684-df7a-45a6-86b0-b6a2d1fe1ac5",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Added something, let me know if it look good to you.",
        "createdAt" : "2019-11-06T16:43:15Z",
        "updatedAt" : "2019-11-08T03:05:00Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4389707302ef38669191a8d1efd379ee1e3b8ec",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +236,240 @@// The label to be considered is provided to the struct (ServiceAntiAffinity).\nfunc (s *ServiceAntiAffinity) CalculateAntiAffinityPriorityReduce(pod *v1.Pod, meta interface{}, sharedLister schedulerlisters.SharedLister, result framework.NodeScoreList) error {\n\treduceResult := make([]float64, len(result))\n\tfor _, label := range s.labels {\n\t\tif err := s.updateNodeScoresForLabel(sharedLister, result, reduceResult, label); err != nil {"
  },
  {
    "id" : "ed8eb78c-ea61-4c0e-9051-086ce82d4c43",
    "prId" : 61548,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61548#pullrequestreview-131385559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "723aac16-5104-4201-affc-1e4b7907ddca",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Does this affect the default scheduling based on region and zone labels (failure-domain.beta.kubernetes.io/region)  ?\r\nDoes this mean that without this fix , if a pod is currently in terminating condition , it would count as a pod on the node and hence additional pods may not be placed on that node ? @aveshagarwal @bsalamat  ",
        "createdAt" : "2018-06-23T05:28:23Z",
        "updatedAt" : "2018-06-23T05:28:23Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f79d75b7a585b376247a0af98f4f8bf0eaefaa6",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +214,218 @@\t\t// Ignore pods being deleted for spreading purposes\n\t\t// Similar to how it is done for SelectorSpreadPriority\n\t\tif namespace == pod.Namespace && pod.DeletionTimestamp == nil && selector.Matches(labels.Set(pod.Labels)) {\n\t\t\tpods = append(pods, pod)\n\t\t}"
  }
]