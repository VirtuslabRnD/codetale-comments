[
  {
    "id" : "cc9193d2-31bd-443b-a945-4ff7937e870d",
    "prId" : 78319,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78319#pullrequestreview-250807648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "285158c0-28c6-417f-abc1-227ce51e43f4",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "ditto",
        "createdAt" : "2019-06-18T00:32:08Z",
        "updatedAt" : "2019-07-16T22:03:48Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "9babbf8bd7f009b4b1738efbf524e90b1c340e85",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +617,621 @@\n\t// If Overhead is being utilized, add to the total requests for the pod\n\tif pod.Spec.Overhead != nil && utilfeature.DefaultFeatureGate.Enabled(features.PodOverhead) {\n\t\tresPtr.Add(pod.Spec.Overhead)\n"
  },
  {
    "id" : "62fc69db-c602-435d-b09a-96141226f8e1",
    "prId" : 78263,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78263#pullrequestreview-244322853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1490d9e0-65bd-4aae-aaa9-7b43b4268a50",
        "parentId" : null,
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "Would it be better to add an explanation to minimum value: `Total requested resources of all pods on this node with a default value(DefaultMilliCPURequest/DefaultMemoryRequest) applied to each container's CPU and memory requests if they are empty.`",
        "createdAt" : "2019-05-31T13:58:48Z",
        "updatedAt" : "2019-05-31T13:58:48Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      }
    ],
    "commit" : "66d359eb23f5cc89bd3f7fea16e9be66ab8fbabd",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +56,60 @@\t// pods, which scheduler has sent for binding, but may not be scheduled yet.\n\trequestedResource *Resource\n\t// Total requested resources of all pods on this node with a minimum value\n\t// applied to each container's CPU and memory requests. This does not reflect\n\t// the actual resource requests for this node, but is used to avoid scheduling"
  },
  {
    "id" : "e1d1f9af-1642-411b-8fa1-5622afbabdde",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-242502910",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ea9ccb8-79d0-4dac-92cd-eebd6a8a2c45",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "This function is not used.",
        "createdAt" : "2019-05-27T15:00:42Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "1a12e443-1ac9-4329-936c-be4c22696bc9",
        "parentId" : "3ea9ccb8-79d0-4dac-92cd-eebd6a8a2c45",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "I use it in the predicate: https://github.com/bertinatto/kubernetes/blob/760e8a8d276a842c636b7f490fd7c4170feeaebf/pkg/scheduler/algorithm/predicates/csi_volume_predicate.go#L163",
        "createdAt" : "2019-05-28T07:59:20Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +289,293 @@\n// CSINode returns overall CSI-related information about this node.\nfunc (n *NodeInfo) CSINode() *storagev1beta1.CSINode {\n\tif n == nil {\n\t\treturn nil"
  },
  {
    "id" : "20add3b9-92d2-4049-bf71-b5e6415f99dc",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-244345564",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1587b124-dfbb-447e-b3dd-f3a7b4dbe8a8",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "This is kinda tricky. This will mean that, we may not be able to distinguish between if driver is installed on the node or volume limit is coming from `Node` object.",
        "createdAt" : "2019-05-31T14:58:53Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +498,502 @@\t\t\t\tvolumeLimits[k] = int64(*d.Allocatable.Count)\n\t\t\t}\n\t\t}\n\t}\n"
  },
  {
    "id" : "7943ff2f-8615-4dcb-bf7b-ecf0794b6c4b",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245626174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38dcc861-ef10-4a0d-8fb9-20a73abaf453",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "As a relatively low-hanging optimization fruit, volume limits for each node can be pre-calculated in informer callbacks (`AddCSINode` / `UpdateCSINode` + their `Node` equivalents). \r\nProfiler says that `VolumeLimits()` call itself takes 3s out of 8s of whole `attachableLimitPredicate()`.\r\n\r\nFurthermore, list of migrated plugins can be pre-parsed for each node there.\r\n\r\nI will defer to sig-scheduling to judge if current performance is OK or it needs to be fixed as part of this PR.",
        "createdAt" : "2019-06-03T11:46:24Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "be82e8d0-1025-4759-818f-5a4412f8ef6a",
        "parentId" : "38dcc861-ef10-4a0d-8fb9-20a73abaf453",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I'm surprised. The number of objects being iterated over per call should not be very large.  I would have suspected the pod iteration to take the most cycles.",
        "createdAt" : "2019-06-03T14:11:15Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "0a62d46e-d1be-4b39-ac4a-4d8801da50a7",
        "parentId" : "38dcc861-ef10-4a0d-8fb9-20a73abaf453",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "@jsafrane Is the perf overhead applied in regular case (without PV/PVC involved)?",
        "createdAt" : "2019-06-04T04:18:02Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "c9e7fa75-b331-4d76-b2b1-096ed80fb538",
        "parentId" : "38dcc861-ef10-4a0d-8fb9-20a73abaf453",
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "> I'm surprised. The number of objects being iterated over per call should not be very large\r\n\r\nIt is not large, but it is called for every combination of Node and Pod, which is lot of calls. Even simple feature gate check there takes ~0.5s out of this 8 seconds increase.\r\n\r\n> Is the perf overhead applied in regular case (without PV/PVC involved)?\r\n\r\nGood question! Yes, **all pods with volumes (incl. Secrets) gets such overhead, if CSINode object exists**.\r\n\r\nIn theory, any volume can be migrated to CSI, so we must know limits reported by the driver. In practice, we won't migrate ephemeral volumes like Secrets, ConfigMap, EmptyDir and so on, some (dirty?) shortcuts are possible. For example, caching of `VolumeLimits` value in `NodeInfo` as I suggested above.\r\n",
        "createdAt" : "2019-06-04T10:47:42Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "65b659f2-5da2-4259-ad13-11a1284e1441",
        "parentId" : "38dcc861-ef10-4a0d-8fb9-20a73abaf453",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : ">  Profiler says that VolumeLimits() call itself takes 3s out of 8s of whole attachableLimitPredicate().\r\n\r\nTo be clear, 8s is avg time taken by `VolumeLimits()` per scheduling of a pod?\r\n\r\n> In theory, any volume can be migrated to CSI, so we must know limits reported by the driver. In practice, we won't migrate ephemeral volumes like Secrets, ConfigMap, EmptyDir and so on, some (dirty?) shortcuts are possible. For example, caching of VolumeLimits value in NodeInfo as I suggested above.\r\n\r\nCan we include excluding those ephemeral volumes as part of this PR, if it proves to improve performance?",
        "createdAt" : "2019-06-04T18:32:53Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +481,485 @@\n// VolumeLimits returns volume limits associated with the node\nfunc (n *NodeInfo) VolumeLimits() map[v1.ResourceName]int64 {\n\tvolumeLimits := map[v1.ResourceName]int64{}\n"
  },
  {
    "id" : "7a532f4c-5248-45f1-8ffa-7aeea139285b",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-245419709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8209b7bd-988a-4e3f-b73b-7fe6afea4160",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Should we be calling GetCSIAttachLimitKey()? It does the hashing thing when driver name is too big, which the new design should handle.  We should also make sure that non-migration CSI drivers will use the correct resource name.",
        "createdAt" : "2019-06-03T14:08:56Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "53a841ec-1176-4512-a8b5-0ea2b4eeaa44",
        "parentId" : "8209b7bd-988a-4e3f-b73b-7fe6afea4160",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "I think this is to ensure that, both new driver names coming from `CSINode` and old names coming from `Node` object are consolidated with unique keys in the `volumeLimits` map. Otherwise, we will end up with 2 values for migrated plugins. \r\n\r\nAs a interim step I think it is fine until we can stop recognizing limits coming from node objects..",
        "createdAt" : "2019-06-03T20:00:17Z",
        "updatedAt" : "2019-06-25T14:31:23Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "02cf5141-0b75-4cca-95a9-0bfbec788090",
        "parentId" : "8209b7bd-988a-4e3f-b73b-7fe6afea4160",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "K let's just add a comment to clean this up later",
        "createdAt" : "2019-06-04T11:29:22Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "3d328ecc-183f-4358-8622-4adbb61d61c8",
        "parentId" : "8209b7bd-988a-4e3f-b73b-7fe6afea4160",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Added TODO",
        "createdAt" : "2019-06-04T12:44:34Z",
        "updatedAt" : "2019-06-25T14:31:24Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +495,499 @@\t\t\tif d.Allocatable != nil && d.Allocatable.Count != nil {\n\t\t\t\t// TODO: drop GetCSIAttachLimitKey once we don't get values from Node object\n\t\t\t\tk := v1.ResourceName(volumeutil.GetCSIAttachLimitKey(d.Name))\n\t\t\t\tvolumeLimits[k] = int64(*d.Allocatable.Count)\n\t\t\t}"
  },
  {
    "id" : "f5bb86f5-4c32-41ef-9b72-ec934847a1ca",
    "prId" : 77595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77595#pullrequestreview-288480906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "125ed857-aadc-43f3-a7b8-3f9f1e6498ae",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Can we establish map from ResourceName to int64 once (inside NodeInfo#SetCSINode) so that we don't perform the calculation here every time ?\r\n",
        "createdAt" : "2019-06-26T08:24:19Z",
        "updatedAt" : "2019-06-26T09:41:19Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "a1f8a94d-17f3-4d54-b535-c62de78dfffd",
        "parentId" : "125ed857-aadc-43f3-a7b8-3f9f1e6498ae",
        "authorId" : "74808d33-32a9-4db9-8265-aadfe78d94ab",
        "body" : "What if we may need the ResourceName other place?",
        "createdAt" : "2019-09-16T08:40:29Z",
        "updatedAt" : "2019-09-16T08:40:29Z",
        "lastEditedBy" : "74808d33-32a9-4db9-8265-aadfe78d94ab",
        "tags" : [
        ]
      }
    ],
    "commit" : "6abc04d059b42ffdc1b68f5b847f18e57ac0680d",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +495,499 @@\t\t\tif d.Allocatable != nil && d.Allocatable.Count != nil {\n\t\t\t\t// TODO: drop GetCSIAttachLimitKey once we don't get values from Node object\n\t\t\t\tk := v1.ResourceName(volumeutil.GetCSIAttachLimitKey(d.Name))\n\t\t\t\tvolumeLimits[k] = int64(*d.Allocatable.Count)\n\t\t\t}"
  }
]