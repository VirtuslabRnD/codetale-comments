[
  {
    "id" : "71ff8056-ea1d-40a9-8908-7a220d0c59bc",
    "prId" : 87692,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87692#pullrequestreview-350985218",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31abdc84-b794-475b-b111-8edefce8d03e",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "instead of doing this, perhaps we should follow the pendingPods metric approach, define functions that return the metric (see lines 275-292)",
        "createdAt" : "2020-01-30T16:49:00Z",
        "updatedAt" : "2020-01-31T08:07:23Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "b2a2d645-65f8-4836-be78-c620ca2bceca",
        "parentId" : "31abdc84-b794-475b-b111-8edefce8d03e",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "ok, I guess we access those handlers directly in the scheduler, ok I guess this is fine.",
        "createdAt" : "2020-01-30T16:57:54Z",
        "updatedAt" : "2020-01-31T08:07:23Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c4be66d3ed697f36d4eb6c20c069a1fdcc6495",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +266,270 @@\t\t}\n\t\tvolumeschedulingmetrics.RegisterVolumeSchedulingMetrics()\n\t\tPodScheduleSuccesses = scheduleAttempts.With(metrics.Labels{\"result\": \"scheduled\"})\n\t\tPodScheduleFailures = scheduleAttempts.With(metrics.Labels{\"result\": \"unschedulable\"})\n\t\tPodScheduleErrors = scheduleAttempts.With(metrics.Labels{\"result\": \"error\"})"
  },
  {
    "id" : "bb055624-6cf3-4617-891e-f87c738a98a8",
    "prId" : 83603,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83603#pullrequestreview-299794762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "711a88d8-7421-4638-867b-c6e679e440e4",
        "parentId" : null,
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "BTW: do we break the original metrics by changing this to the histogram? This is in the alpha stage, so it implies we don't guarantee the compatibility of the metrics, right? \r\n\r\nBut I think we do need a release note for this.",
        "createdAt" : "2019-10-08T15:50:37Z",
        "updatedAt" : "2019-10-09T22:28:41Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      },
      {
        "id" : "99d22970-5f46-4339-bf7e-2a54d6d95a62",
        "parentId" : "711a88d8-7421-4638-867b-c6e679e440e4",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "cc/ @ahg-g @liu-cong ",
        "createdAt" : "2019-10-08T15:53:41Z",
        "updatedAt" : "2019-10-09T22:28:41Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      },
      {
        "id" : "71c5fe0a-ba1c-404f-bbff-7d5b14ce23bd",
        "parentId" : "711a88d8-7421-4638-867b-c6e679e440e4",
        "authorId" : "add996dd-2e15-4a18-8761-e2a63995771e",
        "body" : "@draveness Thanks for the review. I have included a release note and squashed the commits.",
        "createdAt" : "2019-10-08T16:29:51Z",
        "updatedAt" : "2019-10-09T22:28:41Z",
        "lastEditedBy" : "add996dd-2e15-4a18-8761-e2a63995771e",
        "tags" : [
        ]
      },
      {
        "id" : "aaae2609-4975-4475-837c-5d119407f493",
        "parentId" : "711a88d8-7421-4638-867b-c6e679e440e4",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "> BTW: do we break the original metrics by changing this to the histogram? This is in the alpha stage, so it implies we don't guarantee the compatibility of the metrics, right?\r\n\r\nThat's my understanding as well. @piosz, can you give us some feedback on this?\r\n",
        "createdAt" : "2019-10-08T16:41:10Z",
        "updatedAt" : "2019-10-10T13:51:04Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "f376e053-1585-49a0-8519-0209b490b586",
        "parentId" : "711a88d8-7421-4638-867b-c6e679e440e4",
        "authorId" : "ae15cfb8-5436-4398-94e0-d443e413b257",
        "body" : "According to [stability-classes](https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/20190404-kubernetes-control-plane-metrics-stability.md#stability-classes),\r\n`Alpha metrics have no stability guarantees; as such they can be modified or deleted at any time. `.\r\n\r\nI agree with @draveness release note is necessary and it's enough, even though a little rough.",
        "createdAt" : "2019-10-10T02:48:35Z",
        "updatedAt" : "2019-10-10T02:48:35Z",
        "lastEditedBy" : "ae15cfb8-5436-4398-94e0-d443e413b257",
        "tags" : [
        ]
      }
    ],
    "commit" : "451a535401856ba917e0da8fb1fa5bd94c75b3a4",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +196,200 @@\t\t},\n\t)\n\tPreemptionVictims = metrics.NewHistogram(\n\t\t&metrics.HistogramOpts{\n\t\t\tSubsystem: SchedulerSubsystem,"
  },
  {
    "id" : "2345f6a9-1ef1-4e01-b873-b74480d2e317",
    "prId" : 75501,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75501#pullrequestreview-220772102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed8dd804-2bab-4ea0-80af-463e358482f6",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "In the issue, I also asked for adding a metric for the number of pods scheduled. We can add that in a separate PR. In that case, this PR shouldn't close  #75267.",
        "createdAt" : "2019-03-20T18:30:47Z",
        "updatedAt" : "2019-04-09T00:52:47Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "ea40632f-884e-42e2-a7c9-6ca7656fbe9d",
        "parentId" : "ed8dd804-2bab-4ea0-80af-463e358482f6",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "oh, I mentioned the reason in the PR description: \"BTW: metrics of scheduled pods has been represented by `scheduler_schedule_attempts_total{result=\"scheduled\"}`.\"",
        "createdAt" : "2019-03-26T00:43:03Z",
        "updatedAt" : "2019-04-09T00:52:47Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "3a7ea96e-2995-458a-9a6c-5ee1c745de0e",
        "parentId" : "ed8dd804-2bab-4ea0-80af-463e358482f6",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "sorry, I missed it. Thanks!",
        "createdAt" : "2019-03-29T21:55:01Z",
        "updatedAt" : "2019-04-09T00:52:47Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "63c3a617cda9e66a89ca7eb7e807185191f6edc0",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +221,225 @@\t\tPreemptionVictims,\n\t\tPreemptionAttempts,\n\t\tpendingPods,\n\t}\n)"
  },
  {
    "id" : "08130150-a232-4a4c-b472-c24f02716ad8",
    "prId" : 75501,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75501#pullrequestreview-225104234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d638a149-d882-4820-8c04-fb010d487579",
        "parentId" : null,
        "authorId" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "body" : "The total suffix is meant for counter according to the metrics instrumentation guidelines. This should just be “pending_pods”",
        "createdAt" : "2019-04-10T17:00:51Z",
        "updatedAt" : "2019-04-10T17:03:18Z",
        "lastEditedBy" : "4108cff4-d61c-4717-862b-6c3be3b73be2",
        "tags" : [
        ]
      },
      {
        "id" : "d2f56712-d600-4169-818b-b6efd87efd1b",
        "parentId" : "d638a149-d882-4820-8c04-fb010d487579",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Sure. I will raise a follow up PR shortly. We do need sig-instrumentation 's advice on best practices, conventions on this kind of PR :-)\r\n\r\nThanks @brancz .",
        "createdAt" : "2019-04-10T17:09:49Z",
        "updatedAt" : "2019-04-10T17:09:49Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "63c3a617cda9e66a89ca7eb7e807185191f6edc0",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +196,200 @@\t\tprometheus.GaugeOpts{\n\t\t\tSubsystem: SchedulerSubsystem,\n\t\t\tName:      \"pending_pods_total\",\n\t\t\tHelp:      \"Number of pending pods, by the queue type. 'active' means number of pods in activeQ; 'backoff' means number of pods in backoffQ; 'unschedulable' means number of pods in unschedulableQ.\",\n\t\t}, []string{\"queue\"})"
  },
  {
    "id" : "a12e96a1-ca4a-42c7-8b3e-b5bf76596eff",
    "prId" : 64838,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64838#pullrequestreview-126798448",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc6ba257-f4b1-4e99-ba0c-cddc6255d044",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "In the [naming](https://prometheus.io/docs/practices/naming/) section of the prometheus guide, they mention that,\r\n\r\n> either the sum() or the avg() over all dimensions of a given metric should be meaningful.\r\n\r\nWhen you define a label with values for \"binding\", \"algorithm\", and \"e2e\", then sum and avg are both meaningless.\r\n\r\nFor counters, it's clear to see how the \"total\" would be represented by the sum over each label value. I am not sure if histograms work the same way, but this smells odd to me.",
        "createdAt" : "2018-06-06T17:01:34Z",
        "updatedAt" : "2018-06-14T13:05:41Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "bacc28cd-e573-4b3f-af09-585ab7abe424",
        "parentId" : "dc6ba257-f4b1-4e99-ba0c-cddc6255d044",
        "authorId" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "body" : "Ok. I think I understand your point.\r\nI've removed the \"e2e\" label. This \"e2e\" is equivalent to the sum of \"binding\" and \"algorithm\".",
        "createdAt" : "2018-06-07T14:11:44Z",
        "updatedAt" : "2018-06-14T13:05:41Z",
        "lastEditedBy" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "tags" : [
        ]
      }
    ],
    "commit" : "e32910a5441a30b25dbe04aaafb48e154b2e3953",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +31,35 @@\n\t// OperationLabel - operation label name\n\tOperationLabel = \"operation\"\n\t// Binding - binding operation label value\n\tBinding = \"binding\""
  },
  {
    "id" : "c02296fe-31aa-475b-ad1b-121aa149a596",
    "prId" : 64838,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64838#pullrequestreview-126797716",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "497eaafa-dd0b-46f5-9598-a8f8194bfe13",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "I would prefer that we don't use `scheduling_algorithm` as a bucket name. \"scheduling\" adds no meaning in this context, as the subsystem name and metric name already contain it. \"algorithm\" is vague to anyone not familiar with our internal implementation details.\r\n\r\nMaybe `select_node` instead?\r\n",
        "createdAt" : "2018-06-06T17:19:05Z",
        "updatedAt" : "2018-06-14T13:05:41Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "55df7844-921c-4c9a-a486-a813075385a2",
        "parentId" : "497eaafa-dd0b-46f5-9598-a8f8194bfe13",
        "authorId" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "body" : "Renamed to \"selecting_node\"",
        "createdAt" : "2018-06-07T14:10:12Z",
        "updatedAt" : "2018-06-14T13:05:41Z",
        "lastEditedBy" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "tags" : [
        ]
      }
    ],
    "commit" : "e32910a5441a30b25dbe04aaafb48e154b2e3953",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +31,35 @@\n\t// OperationLabel - operation label name\n\tOperationLabel = \"operation\"\n\t// Binding - binding operation label value\n\tBinding = \"binding\""
  },
  {
    "id" : "dc09a6ce-7ba3-48f4-a900-353a1c69c254",
    "prId" : 64838,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64838#pullrequestreview-129189524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "378219d2-d066-4fa5-895f-de0806de9ed8",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Looks like this metric is now deprecated and replaced by `Binding`. Could you please add a comment to mark it deprecated?",
        "createdAt" : "2018-06-14T20:58:03Z",
        "updatedAt" : "2018-06-14T20:58:43Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "85f374be-a308-4f15-9e7a-99d15e2d49bc",
        "parentId" : "378219d2-d066-4fa5-895f-de0806de9ed8",
        "authorId" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "body" : "Imo, this metric shouldn't be deprecated - histogram metric provides different data than summary metric. I think both metrics are useful and could be used to measure different aspects.",
        "createdAt" : "2018-06-15T10:30:42Z",
        "updatedAt" : "2018-06-15T10:30:42Z",
        "lastEditedBy" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "tags" : [
        ]
      },
      {
        "id" : "5388bcda-0cd7-443e-ad05-978ad678f926",
        "parentId" : "378219d2-d066-4fa5-895f-de0806de9ed8",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "Bobby - this one is just re-adding the earlier removed metric. Deprecating could mean backward compatibility problem for users who rely on this metric being a histogram (the newer one is summary as Krzysiek pointed).",
        "createdAt" : "2018-06-15T14:26:28Z",
        "updatedAt" : "2018-06-15T14:26:28Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      }
    ],
    "commit" : "e32910a5441a30b25dbe04aaafb48e154b2e3953",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +92,96 @@\t\t},\n\t)\n\tBindingLatency = prometheus.NewHistogram(\n\t\tprometheus.HistogramOpts{\n\t\t\tSubsystem: SchedulerSubsystem,"
  },
  {
    "id" : "79435731-242f-4633-97b1-132d110093a0",
    "prId" : 64316,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64316#pullrequestreview-124669316",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "542e3183-4de1-4639-9337-7147ae8d078e",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Renaming metrics is a breaking change. It is not backward compatible and is going to break monitoring systems that rely on these metrics. This PR should be reverted.",
        "createdAt" : "2018-05-30T18:52:28Z",
        "updatedAt" : "2018-05-30T18:53:00Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "49a9e5f6-7e24-47df-8bd5-ab4c61194f94",
        "parentId" : "542e3183-4de1-4639-9337-7147ae8d078e",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "Yeah, that's a valid concern and also the option I was initially suggesting in https://github.com/kubernetes/kubernetes/issues/63493#issuecomment-389904406. I've cc'd there sig-scheduling and you - following lazy consensus since there weren't any objections :)\r\n\r\nSo, what do you think about re-adding the old metric alongside the new one (instead of replacing it)?",
        "createdAt" : "2018-05-30T19:23:42Z",
        "updatedAt" : "2018-05-30T19:23:42Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "5190b0fa-039b-4d0e-8f11-00590a96d1ce",
        "parentId" : "542e3183-4de1-4639-9337-7147ae8d078e",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Wasn't this metric broken anyway?",
        "createdAt" : "2018-05-30T19:39:25Z",
        "updatedAt" : "2018-05-30T19:39:26Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "5146a528-d329-4ac7-bf0d-22a7b7535ec9",
        "parentId" : "542e3183-4de1-4639-9337-7147ae8d078e",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "I don't think so.. IIUC we were just not capturing the values properly in our test framework (which expects summary instead of histogram).",
        "createdAt" : "2018-05-30T19:42:23Z",
        "updatedAt" : "2018-05-30T19:42:23Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "827c2cdc-411c-4003-9d6e-a05cc639d3ea",
        "parentId" : "542e3183-4de1-4639-9337-7147ae8d078e",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "If we're going to add \"aliases\" for metrics, I think we should make more of a push toward prometheus best practices. The various histograms in this file and their relationships are a bit confusing.",
        "createdAt" : "2018-05-30T19:54:44Z",
        "updatedAt" : "2018-05-30T19:54:44Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "1d1b3946-5476-4d32-89b5-92760d122da8",
        "parentId" : "542e3183-4de1-4639-9337-7147ae8d078e",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I agree with @misterikkit. Some of these names were not following Prometheus best practices. The new ones introduces in this PR are not following those best practices either. We should definitely keep the old ones for backward compatibility and maybe add new aliases based on the best practices. ",
        "createdAt" : "2018-05-31T04:45:16Z",
        "updatedAt" : "2018-05-31T04:45:17Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e833bfc83f8356b7bfe257393583732572d34d7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +26,30 @@const (\n\t// SchedulerSubsystem - subsystem name used by scheduler\n\tSchedulerSubsystem = \"scheduler\"\n\t// SchedulingLatencyName - scheduler latency metric name\n\tSchedulingLatencyName = \"scheduling_latencies_summary\""
  },
  {
    "id" : "2720984a-40d8-4028-987e-79a5ef85b6bd",
    "prId" : 63260,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63260#pullrequestreview-121251437",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "420e10f4-ad69-46b2-bac9-fd0ae790e9bf",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Not sure, if it makes sense but how about adding metrics which provides how much time we have spent for ecache operations. I think locking may cause some performance impact especially when updating ecache even after hit happens. ",
        "createdAt" : "2018-04-28T03:18:01Z",
        "updatedAt" : "2018-08-15T22:51:46Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "38a7ba68-c634-44d2-b0de-d6986c1ea0dd",
        "parentId" : "420e10f4-ad69-46b2-bac9-fd0ae790e9bf",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Are you suggesting to add metrics for `scheduling with ecache` process, or just ecache operations (update & invalidate)? I think the former make sense.",
        "createdAt" : "2018-04-30T09:05:08Z",
        "updatedAt" : "2018-08-15T22:51:46Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "85256024-39e5-47dd-b738-0a6f66c10fb3",
        "parentId" : "420e10f4-ad69-46b2-bac9-fd0ae790e9bf",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "I think that the \"latency\" and \"duration\" metrics above are capturing this information. I guess if the scenario is that scheduler throughput declines gradually over several hours or days, then we want to know if eCache is the culprit?",
        "createdAt" : "2018-05-03T06:39:16Z",
        "updatedAt" : "2018-08-15T22:51:46Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "6fb75570-7a59-4335-a1c9-33312a36c25b",
        "parentId" : "420e10f4-ad69-46b2-bac9-fd0ae790e9bf",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "> I guess if the scenario is that scheduler throughput declines gradually over several hours or days, then we want to know if eCache is the culprit?\r\n\r\nThat is correct.",
        "createdAt" : "2018-05-04T00:23:16Z",
        "updatedAt" : "2018-08-15T22:51:46Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "8451ebef-2651-4c20-9207-94c43db2c593",
        "parentId" : "420e10f4-ad69-46b2-bac9-fd0ae790e9bf",
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "Hi, sorry I haven't responded to this recently. In looking at adding further equiv. cache metrics, I worry that latency may not measure the right thing. In particular, the hash value is computed separately from EquivalenceCache.RunPredicate. Performance concerns have been raised about hash computation, but not about the map lookups.\r\n\r\nI think there is plenty of opportunity to improve scheduler metrics (there is no counter for pods scheduled today) so I would like to push additional metrics into other PRs.",
        "createdAt" : "2018-05-16T21:10:46Z",
        "updatedAt" : "2018-08-15T22:51:46Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "82704a1a-bfc0-47e8-85f7-70d79f39e816",
        "parentId" : "420e10f4-ad69-46b2-bac9-fd0ae790e9bf",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "> In particular, the hash value is computed separately from EquivalenceCache.RunPredicate. \r\n\r\nThanks. I was just curious about locking & unlocking happening at https://github.com/kubernetes/kubernetes/blob/b71966aceaa3c38040236bc0decc6fad36eeb762/pkg/scheduler/core/equivalence_cache.go#L74\r\n\r\nBut if there are no concerns around it, I am fine. My past experiences with RWLocks have not been good when it comes to scaling but I think we are not going to use them.\r\n\r\n> Performance concerns have been raised about hash computation, but not about the map lookups.\r\n\r\nSure. I agree that hash computation could take time to avoid collisions. We can get that information from benchmarking too.\r\n\r\n> I think there is plenty of opportunity to improve scheduler metrics (there is no counter for pods scheduled today) so I would like to push additional metrics into other PRs.\r\n\r\nFair enough. ",
        "createdAt" : "2018-05-17T03:30:34Z",
        "updatedAt" : "2018-08-15T22:51:46Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "a0e30aa6-74e7-45a8-8335-6cbb507e5b04",
        "parentId" : "420e10f4-ad69-46b2-bac9-fd0ae790e9bf",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "xref the upcoming optimization of lock here: https://github.com/kubernetes/kubernetes/issues/63784#issuecomment-389345703\r\n\r\n",
        "createdAt" : "2018-05-17T23:36:38Z",
        "updatedAt" : "2018-08-15T22:51:46Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b874d2789b11e21b3f91840a9ea0f6b0b90f54d5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +118,122 @@\t\t\tHelp:      \"Total preemption attempts in the cluster till now\",\n\t\t})\n\n\tequivalenceCacheLookups = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{"
  },
  {
    "id" : "45379606-9603-45f5-8be4-ad41910d7c4d",
    "prId" : 59529,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59529#pullrequestreview-164353416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f643089-4056-4bb8-bc0d-b61864eea29b",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Any chance we could separate the metrics in a different package and import only the metrics package instead of the whole persistent volume controller?",
        "createdAt" : "2018-07-31T23:43:57Z",
        "updatedAt" : "2018-10-23T13:01:13Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "ed416da6-de84-4485-b27c-c5f737858c2c",
        "parentId" : "2f643089-4056-4bb8-bc0d-b61864eea29b",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "@wackxu would you be able to look into this?",
        "createdAt" : "2018-08-14T15:05:45Z",
        "updatedAt" : "2018-10-23T13:01:13Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "3cd93bc3-3505-4080-9697-3da8ba4258d1",
        "parentId" : "2f643089-4056-4bb8-bc0d-b61864eea29b",
        "authorId" : "f71497cc-dd85-47eb-bbed-996bde0d13a4",
        "body" : "We have already import `k8s.io/kubernetes/pkg/controller/volume/persistentvolume` in the scheduler and the metric is used only for scheduler like scheduler_binder files. I think we can extract a separate dir for those files in a follow pr",
        "createdAt" : "2018-10-12T09:25:20Z",
        "updatedAt" : "2018-10-23T13:01:13Z",
        "lastEditedBy" : "f71497cc-dd85-47eb-bbed-996bde0d13a4",
        "tags" : [
        ]
      },
      {
        "id" : "cfa2b0ac-32ee-4677-b0ce-ede6d14324ed",
        "parentId" : "2f643089-4056-4bb8-bc0d-b61864eea29b",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "That sounds reasonable.",
        "createdAt" : "2018-10-12T18:11:02Z",
        "updatedAt" : "2018-10-23T13:01:13Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5edcd3dc33b1ae5db8f7b87775860a9addc5a8a",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +22,26 @@\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"k8s.io/kubernetes/pkg/controller/volume/persistentvolume\"\n)\n"
  },
  {
    "id" : "960e6f21-fc0c-4c6d-bee0-0afa2e57e1ee",
    "prId" : 58192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58192#pullrequestreview-88556228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3571d63b-20e3-4b17-8412-d0a1e993455e",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Thanks for adding this. This is useful.",
        "createdAt" : "2018-01-12T18:00:15Z",
        "updatedAt" : "2018-01-13T06:57:28Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "8aebf3554c7534300f08a7646748a1afe91b1812",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +60,64 @@\t\t},\n\t)\n\tSchedulingAlgorithmPremptionEvaluationDuration = prometheus.NewHistogram(\n\t\tprometheus.HistogramOpts{\n\t\t\tSubsystem: schedulerSubsystem,"
  },
  {
    "id" : "0c18933f-0d7a-440c-9092-e2171099d10e",
    "prId" : 58192,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/58192#pullrequestreview-88568139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3686736a-a2b5-4ac7-8b5e-b2e206fbb8b3",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I am not sure if this is useful when we have `pod_preempted`. These two should be almost always the same value. It would be good if we have the number of times that preemption is attempted. In other words, the number of times that we call `preempt`. So, I'd replace this with the number of preemption attempts.",
        "createdAt" : "2018-01-12T18:11:08Z",
        "updatedAt" : "2018-01-13T06:57:28Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "05c1a73c-5fab-4560-9055-5451e629e125",
        "parentId" : "3686736a-a2b5-4ac7-8b5e-b2e206fbb8b3",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "> I am not sure if this is useful when we have pod_preempted. These two should be almost always the same value.\r\n\r\nYou are right but I wasn't quite sure about terminology used in issue(no of preemptions, I misunderstood it as preempted pods) so added, I will add a metric for no of preemptions attempted now.",
        "createdAt" : "2018-01-12T18:41:40Z",
        "updatedAt" : "2018-01-13T06:57:28Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "8aebf3554c7534300f08a7646748a1afe91b1812",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +76,80 @@\t\t},\n\t)\n\tPreemptionVictims = prometheus.NewGauge(\n\t\tprometheus.GaugeOpts{\n\t\t\tSubsystem: schedulerSubsystem,"
  },
  {
    "id" : "2e809571-67d1-442d-991b-f5d54dc2ce06",
    "prId" : 46245,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46245#pullrequestreview-88377626",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4366ece0-f6d3-482c-9fcb-748a50c03ceb",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "It would be good to add a comment here and below to mention that our smallest bucket is 1 mili-second.\r\nOtherwise, LGTM.",
        "createdAt" : "2018-01-11T21:56:26Z",
        "updatedAt" : "2018-01-12T04:25:39Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "ba399e44-1ffd-4b30-a40e-7c06c4e61d3a",
        "parentId" : "4366ece0-f6d3-482c-9fcb-748a50c03ceb",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Done @bsalamat - I have added a top level comment for all the histogram based buckets.",
        "createdAt" : "2018-01-12T04:27:08Z",
        "updatedAt" : "2018-01-12T04:27:08Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3c57a880ce42222ede01878df2cb595f0aad571",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +49,53 @@\t\t\tName:      \"scheduling_algorithm_predicate_evaluation\",\n\t\t\tHelp:      \"Scheduling algorithm predicate evaluation duration\",\n\t\t\tBuckets:   prometheus.ExponentialBuckets(1000, 2, 15),\n\t\t},\n\t)"
  }
]