[
  {
    "id" : "1160ee09-a5a1-479a-bddf-10ff9fd57f10",
    "prId" : 99411,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99411#pullrequestreview-604576318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62eecb40-2a0d-4e70-8841-226f0ffdf73a",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "please rename j to something more meaningful. I think it refers to plugin here?",
        "createdAt" : "2021-02-24T21:38:11Z",
        "updatedAt" : "2021-03-02T19:02:12Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "58f18e3a-d04a-4022-bd17-8224eef34592",
        "parentId" : "62eecb40-2a0d-4e70-8841-226f0ffdf73a",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Come to think that this is not a very efficient implementation of nested loops, for 2 reasons:\r\n\r\n1. we can save lookups by doing:\r\n    ```\r\n    for plugin, nodeScores := range scoresMap\r\n    ```\r\n2. Iterating over the map in the outer loop and nodes in the inner loop would keep the slices closer to faster caches in the CPU.\r\n\r\nCan you fix this in a separate PR? It would be nice if it shows in any performance benchmarks, but plugins are probably way slower to run. Maybe it will show in some profiles.",
        "createdAt" : "2021-02-24T21:49:44Z",
        "updatedAt" : "2021-03-02T19:02:12Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "6f4189fd-b91a-4065-b435-cef48ccb63e7",
        "parentId" : "62eecb40-2a0d-4e70-8841-226f0ffdf73a",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "Yeah, I'll open another PR for that",
        "createdAt" : "2021-02-25T17:23:57Z",
        "updatedAt" : "2021-03-02T19:02:12Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "37daafa0-6fec-4a7e-802a-83872ed0669c",
        "parentId" : "62eecb40-2a0d-4e70-8841-226f0ffdf73a",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Don't forget this please :)\r\n\r\n/lgtm",
        "createdAt" : "2021-03-04T20:20:19Z",
        "updatedAt" : "2021-03-04T20:20:19Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "97ac14db-a04f-440f-9175-ee51cdb40982",
        "parentId" : "62eecb40-2a0d-4e70-8841-226f0ffdf73a",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "Opened here: https://github.com/kubernetes/kubernetes/pull/99807",
        "createdAt" : "2021-03-04T21:27:44Z",
        "updatedAt" : "2021-03-04T21:27:44Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      }
    ],
    "commit" : "d09a841246f4874873e24699cce0697f1bfa4e3d",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +446,450 @@\tfor i := range nodes {\n\t\tresult = append(result, framework.NodeScore{Name: nodes[i].Name, Score: 0})\n\t\tfor j := range scoresMap {\n\t\t\tresult[i].Score += scoresMap[j][i].Score\n\t\t}"
  },
  {
    "id" : "4431e30a-1f77-4993-85f4-cb39f53248b6",
    "prId" : 99411,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99411#pullrequestreview-602155881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce73bae2-d556-4d04-9a85-b72d616dc21c",
        "parentId" : null,
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "why change this?",
        "createdAt" : "2021-02-25T10:23:56Z",
        "updatedAt" : "2021-03-02T19:02:12Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "e788a4c6-8daf-44f6-a1da-198a6033d7ed",
        "parentId" : "ce73bae2-d556-4d04-9a85-b72d616dc21c",
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "I think this information matches well with the info that I'm exposing in this PR. The highest scoring plugins themselves are a direct contributor to the total score for that node, so I'm lowering it from `V(10)`, which is the level at which we log every score for every plugin.",
        "createdAt" : "2021-02-25T17:20:00Z",
        "updatedAt" : "2021-03-02T19:02:12Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "cb4101c7-3a43-498b-9c60-f13dd84fdb4a",
        "parentId" : "ce73bae2-d556-4d04-9a85-b72d616dc21c",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "sgtm",
        "createdAt" : "2021-03-02T18:52:57Z",
        "updatedAt" : "2021-03-02T19:02:12Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "d09a841246f4874873e24699cce0697f1bfa4e3d",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +495,499 @@\t}\n\n\tif klog.V(4).Enabled() {\n\t\tfor i := range result {\n\t\t\tklog.InfoS(\"Calculated node's final score for pod\", \"pod\", klog.KObj(pod), \"node\", result[i].Name, \"score\", result[i].Score)"
  },
  {
    "id" : "f789ef63-0539-4d14-950d-897b4287728e",
    "prId" : 96929,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/96929#pullrequestreview-551382342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf6cb613-c75c-4c0a-940f-f7020dfbdfb5",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "line 313 below is not necessary",
        "createdAt" : "2020-12-14T13:37:36Z",
        "updatedAt" : "2020-12-19T16:19:06Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "0b4750ec-7156-4656-950c-5f5fafdbd03c",
        "parentId" : "bf6cb613-c75c-4c0a-940f-f7020dfbdfb5",
        "authorId" : "628143cf-73c8-4c51-bd38-d4079089c756",
        "body" : "good catch",
        "createdAt" : "2020-12-14T13:49:48Z",
        "updatedAt" : "2020-12-19T16:19:06Z",
        "lastEditedBy" : "628143cf-73c8-4c51-bd38-d4079089c756",
        "tags" : [
        ]
      }
    ],
    "commit" : "070773c3996e12c62455c9a920914945a4efe724",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +302,306 @@\t\t\treturn\n\t\t}\n\t\tif status.IsSuccess() {\n\t\t\tlength := atomic.AddInt32(&feasibleNodesLen, 1)\n\t\t\tif length > numNodesToFind {"
  },
  {
    "id" : "36c9af38-3f99-4738-aac3-445574e835cf",
    "prId" : 93179,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93179#pullrequestreview-572281554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87a62987-ff25-4c32-bd37-40839c78b399",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Is there any unit test we can add for the changes in this file?",
        "createdAt" : "2021-01-19T14:51:39Z",
        "updatedAt" : "2021-01-26T01:59:45Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "e4554ab6-5606-4b99-92f8-a92c2d2710fb",
        "parentId" : "87a62987-ff25-4c32-bd37-40839c78b399",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "The overall logic has been verified by testcase `TestPreferNominatedNode` created in \"test/integration/scheduler/preemption_test.go\". \r\n- If the nominated passed all the filters,  the nominated node will be return, otherwise,\r\n- other nodes will be evaluated and return a suitable candidate.\r\n\r\n\"TestPreferNominatedNode\" is truly has not verify the logic of preemption, if you think it's better to test in \"generic_scheduler_test.go\" or there is any specific change haven't covered by any testcase, please just let me know, I will take care it.",
        "createdAt" : "2021-01-20T05:24:17Z",
        "updatedAt" : "2021-01-26T01:59:45Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "bfe2bc94-a5e2-4f9e-b553-d917dfd2d674",
        "parentId" : "87a62987-ff25-4c32-bd37-40839c78b399",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "hmm, that's integration test, as to the unit test for the change like this, output should be the same before and after the change, it is hard to see which aspect should be tested in the unit test.",
        "createdAt" : "2021-01-20T07:47:11Z",
        "updatedAt" : "2021-01-26T01:59:45Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "8618e74f-c26f-4e57-8629-b7f53c94e4de",
        "parentId" : "87a62987-ff25-4c32-bd37-40839c78b399",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "I like the filter call counts :)",
        "createdAt" : "2021-01-20T14:39:41Z",
        "updatedAt" : "2021-01-26T01:59:45Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d800ffe0704c58865ec2a1cb611209f6d5731d8",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +226,230 @@\t// Run \"prefilter\" plugins.\n\ts := fwk.RunPreFilterPlugins(ctx, state, pod)\n\tallNodes, err := g.nodeInfoSnapshot.NodeInfos().List()\n\tif err != nil {\n\t\treturn nil, nil, err"
  },
  {
    "id" : "6f1eb998-278c-40c7-b05d-595b4c78a647",
    "prId" : 93179,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93179#pullrequestreview-574001389",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83cabebb-13bf-4f15-9dbb-2587b52b2b2b",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Lets not return, just fall back to the old logic. This will be easier to do once this logic is encapsulated in a separate function.",
        "createdAt" : "2021-01-21T15:17:33Z",
        "updatedAt" : "2021-01-26T01:59:45Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "e3ecdc91-a221-4e92-9669-b92a68422e57",
        "parentId" : "83cabebb-13bf-4f15-9dbb-2587b52b2b2b",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "Done",
        "createdAt" : "2021-01-22T07:16:52Z",
        "updatedAt" : "2021-01-26T01:59:45Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d800ffe0704c58865ec2a1cb611209f6d5731d8",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +246,250 @@\tif len(pod.Status.NominatedNodeName) > 0 && feature.DefaultFeatureGate.Enabled(features.PreferNominatedNode) {\n\t\tfeasibleNodes, err := g.evaluateNominatedNode(ctx, pod, fwk, state, filteredNodesStatuses)\n\t\tif err != nil {\n\t\t\tklog.ErrorS(err, \"Evaluation failed on nominated node\", \"pod\", klog.KObj(pod), \"node\", pod.Status.NominatedNodeName)\n\t\t}"
  },
  {
    "id" : "ccfa4e07-e3b2-465d-a2b3-a9372dcf8e90",
    "prId" : 93179,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93179#pullrequestreview-602651759",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "526afa77-697f-493b-b9f4-5585ea38939a",
        "parentId" : null,
        "authorId" : "41c4c4f1-58a5-4719-8860-1fbf4fe8b073",
        "body" : "@chendave Just curious.  Could we remove nominatedNode from `allNodes`  quickly if this result is empty? After that, this node should not go Filters/Extenders twice.\r\n",
        "createdAt" : "2021-03-03T06:53:13Z",
        "updatedAt" : "2021-03-03T06:53:20Z",
        "lastEditedBy" : "41c4c4f1-58a5-4719-8860-1fbf4fe8b073",
        "tags" : [
        ]
      },
      {
        "id" : "b073a465-66b2-4354-91b2-40021089fb84",
        "parentId" : "526afa77-697f-493b-b9f4-5585ea38939a",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "the original version was removing this node from the node list to avoid the evaluation again later, but this require some slice manipulation, which might be cost more than just evaluate it again.",
        "createdAt" : "2021-03-03T08:50:59Z",
        "updatedAt" : "2021-03-03T08:51:00Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      },
      {
        "id" : "c855b714-1c92-43f7-9db4-6fbc4c6872e7",
        "parentId" : "526afa77-697f-493b-b9f4-5585ea38939a",
        "authorId" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "body" : "this is true today, as we don't have too much default filtering list, and cost of the filtering is not dramatic.",
        "createdAt" : "2021-03-03T08:54:49Z",
        "updatedAt" : "2021-03-03T08:54:49Z",
        "lastEditedBy" : "1fde46ca-8fae-4b82-9978-f266fdae6ffe",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d800ffe0704c58865ec2a1cb611209f6d5731d8",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +250,254 @@\t\t}\n\t\t// Nominated node passes all the filters, scheduler is good to assign this node to the pod.\n\t\tif len(feasibleNodes) != 0 {\n\t\t\treturn feasibleNodes, filteredNodesStatuses, nil\n\t\t}"
  },
  {
    "id" : "ce552504-ad38-46fd-b11b-745dd1d2ebcc",
    "prId" : 92866,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92866#pullrequestreview-564928105",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca3ada53-c192-42f8-861f-1e5f872fd32c",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "would this happen?\r\nI can only think of the case where the extender returns both states for the same Node.",
        "createdAt" : "2021-01-07T22:05:13Z",
        "updatedAt" : "2021-01-29T03:33:29Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "bbe1edba-97ac-496b-83dd-2b36924672fb",
        "parentId" : "ca3ada53-c192-42f8-861f-1e5f872fd32c",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "yes, ignore the nodes in `failedMap` if they are already in `failedAndUnresolvableMap`",
        "createdAt" : "2021-01-08T01:44:50Z",
        "updatedAt" : "2021-01-29T03:33:29Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      },
      {
        "id" : "ca3b30e8-b888-4747-a75f-16fcd489f383",
        "parentId" : "ca3ada53-c192-42f8-861f-1e5f872fd32c",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "please add a comment that this only happens if the extender returns it in both",
        "createdAt" : "2021-01-08T14:19:45Z",
        "updatedAt" : "2021-01-29T03:33:29Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "7ee9300f-348f-4f6c-8b72-fbd263402f10",
        "parentId" : "ca3ada53-c192-42f8-861f-1e5f872fd32c",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "added, PTAL",
        "createdAt" : "2021-01-11T01:58:26Z",
        "updatedAt" : "2021-01-29T03:33:29Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a2713503a3cc1727010796628757fe7e6398803",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +381,385 @@\n\t\tfor failedNodeName, failedMsg := range failedMap {\n\t\t\tif _, found := failedAndUnresolvableMap[failedNodeName]; found {\n\t\t\t\t// failedAndUnresolvableMap takes precedence over failedMap\n\t\t\t\t// note that this only happens if the extender returns the node in both maps"
  },
  {
    "id" : "ae7f107c-8044-4892-a079-5530d42fe3be",
    "prId" : 92797,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92797#pullrequestreview-445763674",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a8f4b24-f3d4-46f3-b07f-4bd3717c8ab1",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "I rethink it a bit.\r\n\r\nThe primary benefit of returning a FitError (instead of Error) is to continue preemption. Given that current preemption logic doesn't proceed upon `UnschedulableAndUnresolvable` status:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/21953d15ea48972f20a8de29d58bd5ce6d913914/pkg/scheduler/framework/plugins/defaultpreemption/default_preemption.go#L224-L228\r\n\r\n(but we can argue that custom preemption plugin may proceed even with `UnschedulableAndUnresolvable` status)\r\n\r\nSo maybe we can optimize the logic here:\r\n\r\n```go\r\nif s.Code() == UnschedulableAndUnresolvable {\r\n```\r\n\r\n",
        "createdAt" : "2020-07-09T00:57:32Z",
        "updatedAt" : "2020-07-09T00:57:32Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "75a677d4-7a3a-4e68-a67c-c372381ae71c",
        "parentId" : "1a8f4b24-f3d4-46f3-b07f-4bd3717c8ab1",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Not only to execute preemption but also proper metric reporting, so I think we should keep it this way and optimize later the fact that all nodes will have the same status.",
        "createdAt" : "2020-07-09T02:23:44Z",
        "updatedAt" : "2020-07-09T02:23:44Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "eb56efcb-54f7-4958-bccf-fff2b835dc9a",
        "parentId" : "1a8f4b24-f3d4-46f3-b07f-4bd3717c8ab1",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "The question here is whether PreFilter plugins could return `Unschedulable`, as opposed to only `UnschedulableAndUnresolvable`.\r\n\r\nI don't think we need to limit them.",
        "createdAt" : "2020-07-09T13:47:56Z",
        "updatedAt" : "2020-07-09T13:47:56Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "9fdf7976-833b-4997-8fae-52afc392abb9",
        "parentId" : "1a8f4b24-f3d4-46f3-b07f-4bd3717c8ab1",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "> so I think we should keep it this way and optimize later the fact that all nodes will have the same status.\r\n\r\nThat for sure has an improvement room, like simply using a `*` to represent all nodes.\r\n\r\n> I don't think we need to limit them.\r\n\r\n+1. I believe it's possible that some PreFilter plugin may return `Unschedulable` as well.",
        "createdAt" : "2020-07-09T16:08:05Z",
        "updatedAt" : "2020-07-09T16:08:05Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "c98dee49458c861958c3a1e4a32c2abbd054fdf7",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +259,263 @@\ts := prof.RunPreFilterPlugins(ctx, state, pod)\n\tif !s.IsSuccess() {\n\t\tif !s.IsUnschedulable() {\n\t\t\treturn nil, nil, s.AsError()\n\t\t}"
  },
  {
    "id" : "b6d8a956-90d6-46ad-8b49-1057d7ba6a5d",
    "prId" : 92476,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92476#pullrequestreview-437680220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c37b945d-4e9d-4c2f-8e37-031c6c2a61a4",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "wouldn't this make the value go into the minus? I guess it doesn't matter if that happens, right?",
        "createdAt" : "2020-06-25T13:37:12Z",
        "updatedAt" : "2020-06-25T13:40:49Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "5420a309-c48a-4e16-9c96-7a4e213d3a05",
        "parentId" : "c37b945d-4e9d-4c2f-8e37-031c6c2a61a4",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Yes, pdbsAllowed[i] can be negative now.\r\n\r\nAnother style is like this:\r\n\r\n```go\r\nif pdbsAllowed[i] <= 0 {\r\n    pdbForPodIsViolated = true\r\n} else {\r\n    pdbsAllowed[i]--\r\n}\r\n```\r\n\r\nNot quite make a big difference I think :)",
        "createdAt" : "2020-06-25T16:51:17Z",
        "updatedAt" : "2020-06-25T16:51:17Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "82ab6db94b9e636ac338fd4add18018af1aad600",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +945,949 @@\t\t\t\t// Only decrement the matched pdb when it's not in its <DisruptedPods>;\n\t\t\t\t// otherwise we may over-decrement the budget number.\n\t\t\t\tpdbsAllowed[i]--\n\t\t\t\t// We have found a matching PDB.\n\t\t\t\tif pdbsAllowed[i] < 0 {"
  },
  {
    "id" : "9dab0fa9-6cea-447a-bbd2-3fcce6612420",
    "prId" : 92108,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92108#pullrequestreview-436903197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24fd3767-d8a1-41ca-8fdb-e42f3fb22b03",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "what is the plan for this function? I don't think we want to keep the dependency form the plugin to generic_scheduler.go. ",
        "createdAt" : "2020-06-24T04:17:39Z",
        "updatedAt" : "2020-06-25T19:34:10Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "51b79df7-4ac9-423e-80df-bcb877c4b3a4",
        "parentId" : "24fd3767-d8a1-41ca-8fdb-e42f3fb22b03",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "In addition to the preemption path, this function is called by the regular Filter path (`findNodesThatPassFilters()`). I think the destination should be the runtime package (pkg/scheduler/framework/runtime).\r\n\r\nIf we do so, the last dependency is `core.ErrNoNodesAvailable`, which is easier to sort out.",
        "createdAt" : "2020-06-24T05:52:08Z",
        "updatedAt" : "2020-06-25T19:34:10Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "7aa5d14e-33e3-47a7-bb7e-ce73b9ded54a",
        "parentId" : "24fd3767-d8a1-41ca-8fdb-e42f3fb22b03",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "hmm, we don't want plugins to have dependencies on the runtime pkg either. We should probably have an interface to invoke it to avoid that dependency.",
        "createdAt" : "2020-06-24T13:26:05Z",
        "updatedAt" : "2020-06-25T19:34:10Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "cb918c47-f745-47c9-a0db-ff0e06b71c99",
        "parentId" : "24fd3767-d8a1-41ca-8fdb-e42f3fb22b03",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Let me put a TODO here for further brainstorming.",
        "createdAt" : "2020-06-24T18:30:05Z",
        "updatedAt" : "2020-06-25T19:34:10Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "058e3d425812cf5559a1e4c6a90fcd9de2427633",
    "line" : 250,
    "diffHunk" : "@@ -1,1 +417,421 @@// NodeInfo before calling this function.\n// TODO: move this out so that plugins don't need to depend on <core> pkg.\nfunc PodPassesFiltersOnNode(\n\tctx context.Context,\n\tph framework.PreemptHandle,"
  },
  {
    "id" : "588a512a-5c35-4260-99fd-609752f32ff1",
    "prId" : 92108,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92108#pullrequestreview-436343043",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "787b3881-fca4-44a5-9ccb-9174117d1961",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "ditto, what is the plan for this function?",
        "createdAt" : "2020-06-24T04:19:10Z",
        "updatedAt" : "2020-06-25T19:34:10Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "8d4ae0f4-71dc-4629-8fd3-7b507f58d447",
        "parentId" : "787b3881-fca4-44a5-9ccb-9174117d1961",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Per https://github.com/kubernetes/kubernetes/pull/92108/files#r444659410, it should be moved along with `PodPassesFiltersOnNode()`, and remain private.",
        "createdAt" : "2020-06-24T05:54:29Z",
        "updatedAt" : "2020-06-25T19:34:10Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "058e3d425812cf5559a1e4c6a90fcd9de2427633",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +381,385 @@// to run on the node. It returns 1) whether any pod was added, 2) augmented cycleState,\n// 3) augmented nodeInfo.\nfunc addNominatedPods(ctx context.Context, ph framework.PreemptHandle, pod *v1.Pod, state *framework.CycleState, nodeInfo *framework.NodeInfo) (bool, *framework.CycleState, *framework.NodeInfo, error) {\n\tif ph == nil || nodeInfo == nil || nodeInfo.Node() == nil {\n\t\t// This may happen only in tests."
  },
  {
    "id" : "746c217e-fcd6-4d38-9036-e1e7e545fc71",
    "prId" : 92012,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92012#pullrequestreview-431101988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2fd389b-6b86-4454-846b-bbc86a79d00c",
        "parentId" : null,
        "authorId" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "body" : "Could we make `selectNodesForPreemption` as public method so that user's custom preemption plugins can reuse this method?",
        "createdAt" : "2020-06-15T06:40:10Z",
        "updatedAt" : "2020-06-19T16:14:11Z",
        "lastEditedBy" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "tags" : [
        ]
      },
      {
        "id" : "08cea865-0088-405a-9146-a3cb53e365a5",
        "parentId" : "e2fd389b-6b86-4454-846b-bbc86a79d00c",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "It will be made public in a followup PR. This PR just keep the old interfaces as is.",
        "createdAt" : "2020-06-15T07:57:36Z",
        "updatedAt" : "2020-06-19T16:14:11Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "784a3821-0373-4e2b-99fb-1ecac791921d",
        "parentId" : "e2fd389b-6b86-4454-846b-bbc86a79d00c",
        "authorId" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "body" : "Got it, thanks!",
        "createdAt" : "2020-06-16T01:41:07Z",
        "updatedAt" : "2020-06-19T16:14:11Z",
        "lastEditedBy" : "ea65316b-7fdf-4fe0-99b0-2d437bf2580e",
        "tags" : [
        ]
      }
    ],
    "commit" : "196056d7fe2c0cf8e955615d67759362cc5bec4e",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +286,290 @@\t\treturn \"\", err\n\t}\n\tnodeNameToVictims, err := selectNodesForPreemption(ctx, fh.PreemptHandle(), fh.PreemptHandle(), state, pod, potentialNodes, pdbs)\n\tif err != nil {\n\t\treturn \"\", err"
  },
  {
    "id" : "0285d626-4cb4-4d8c-9670-04dea59b2eb4",
    "prId" : 89370,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89370#pullrequestreview-379882409",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2eeca367-9440-46f1-ae88-4a5a4e62c54b",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "nit: it seems `state` doesn't need to be passed in? as there is no place instantiating earlier than RunPreFilterPlugins?",
        "createdAt" : "2020-03-23T19:29:09Z",
        "updatedAt" : "2020-03-23T19:29:09Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "5c5afda1-c950-4939-9914-5d53d803e739",
        "parentId" : "2eeca367-9440-46f1-ae88-4a5a4e62c54b",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "we need to pass it, this is where the prefilters will write the precomputed state.",
        "createdAt" : "2020-03-23T22:24:45Z",
        "updatedAt" : "2020-03-23T22:24:52Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "a14a84b4-ef00-4db4-b47a-ee865a8d70c2",
        "parentId" : "2eeca367-9440-46f1-ae88-4a5a4e62c54b",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "sg",
        "createdAt" : "2020-03-23T22:52:47Z",
        "updatedAt" : "2020-03-23T22:52:47Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "24fe5a2f726ba203bfe8b07a68961e89f0e3b1f3",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +400,404 @@// filter plugins and filter extenders.\nfunc (g *genericScheduler) findNodesThatFitPod(ctx context.Context, prof *profile.Profile, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.NodeToStatusMap, error) {\n\ts := prof.RunPreFilterPlugins(ctx, state, pod)\n\tif !s.IsSuccess() {\n\t\treturn nil, nil, s.AsError()"
  },
  {
    "id" : "5f2446a4-ba52-4190-8194-179e75d93e68",
    "prId" : 89224,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/89224#pullrequestreview-376970892",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c04ad59f-f684-45da-8a8a-3a7c70540a4b",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Please add a comment on why this could happen.",
        "createdAt" : "2020-03-18T15:22:34Z",
        "updatedAt" : "2020-03-18T15:41:49Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "eceaf499638318d84ad00f9c98762c2f597ebf03",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +639,643 @@\t// In the case where a node delete event is received before all pods on the node are deleted, a NodeInfo may have\n\t// a nil Node(). This should not be an issue in 1.18 and later, but we need to check it here. See https://github.com/kubernetes/kubernetes/issues/89006\n\tif info.Node() == nil {\n\t\treturn false, []predicates.PredicateFailureReason{}, framework.NewStatus(framework.UnschedulableAndUnresolvable, \"node being deleted\"), nil\n\t}"
  },
  {
    "id" : "4eeb2db2-fa21-403a-a7f1-74a5a009b02d",
    "prId" : 87458,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/87458#pullrequestreview-346725096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f6f5ef8-04f1-431b-949b-8be673375b11",
        "parentId" : null,
        "authorId" : "41c25afd-5561-4611-9b3a-7df68582aa10",
        "body" : "s/because/because of/",
        "createdAt" : "2020-01-22T16:08:14Z",
        "updatedAt" : "2020-01-22T18:46:53Z",
        "lastEditedBy" : "41c25afd-5561-4611-9b3a-7df68582aa10",
        "tags" : [
        ]
      },
      {
        "id" : "3bc86fe2-4b58-4fa8-8328-aea8d67cee47",
        "parentId" : "0f6f5ef8-04f1-431b-949b-8be673375b11",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "that is not right.",
        "createdAt" : "2020-01-22T16:27:52Z",
        "updatedAt" : "2020-01-22T18:46:53Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0aeb4d6a36ac3ce4806d05abc57e89986c23f14",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +494,498 @@\tstatusCode := framework.Success\n\tdefer func() {\n\t\t// We record Filter extension point latency here instead of in framework.go because framework.RunFilterPlugins\n\t\t// function is called for each node, whereas we want to have an overall latency for all nodes per scheduling cycle.\n\t\t// Note that this latency also includes latency for `addNominatedPods`, which calls framework.RunPreFilterAddPod."
  },
  {
    "id" : "febfde7b-c4da-449c-a2d1-ee79b24933ee",
    "prId" : 86133,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/86133#pullrequestreview-330241123",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd76df13-b65e-49a8-a889-b24ed545ae39",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "It looks like this is the only usage of `addNominatedPods`. Remove the return value.",
        "createdAt" : "2019-12-10T19:48:28Z",
        "updatedAt" : "2019-12-11T03:14:59Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "b68cd675-aa83-40d6-a3a3-2be95d55dc31",
        "parentId" : "bd76df13-b65e-49a8-a889-b24ed545ae39",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Let's get rid of it in the metadata cleanup PR.",
        "createdAt" : "2019-12-10T23:57:46Z",
        "updatedAt" : "2019-12-11T03:14:59Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "3dff8681-5d1b-4ac8-bb8a-9d9621553653",
        "parentId" : "bd76df13-b65e-49a8-a889-b24ed545ae39",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "sg, TODO maybe?",
        "createdAt" : "2019-12-11T00:02:17Z",
        "updatedAt" : "2019-12-11T03:14:59Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "e9f15ae1-09c8-459c-8c81-d5555ed6d33c",
        "parentId" : "bd76df13-b65e-49a8-a889-b24ed545ae39",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Yeap, added at L587:\r\n\r\n```\r\n// TODO(Huang-Wei): remove 'meta predicates.Metadata' from the signature.\r\n```",
        "createdAt" : "2019-12-11T00:51:25Z",
        "updatedAt" : "2019-12-11T03:14:59Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc3d1bd238ef460339946185713b59675819ab3f",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +668,672 @@\t\tif i == 0 {\n\t\t\tvar err error\n\t\t\tpodsAdded, _, stateToUse, nodeInfoToUse, err = g.addNominatedPods(ctx, pod, meta, state, info)\n\t\t\tif err != nil {\n\t\t\t\treturn false, []predicates.PredicateFailureReason{}, nil, err"
  },
  {
    "id" : "30058ef0-42e8-407b-8722-7f9832d641ed",
    "prId" : 84873,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84873#pullrequestreview-312787640",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52f5d413-69cc-4f6e-a7a3-b2d5d6d3a9df",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "nit: `for _, node := range nodes`\r\n\r\nDo we even have to do this? What if we just return an empty list and then select a random node in `selectHost`? \r\nhttps://github.com/kubernetes/kubernetes/blob/c33f217a5d62d062c069cd702bb50ff8d120a168/pkg/scheduler/core/generic_scheduler.go#L288-L293",
        "createdAt" : "2019-11-06T22:06:55Z",
        "updatedAt" : "2019-11-06T22:18:44Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "83180783-bc1a-41b7-99f8-9e16201f0737",
        "parentId" : "52f5d413-69cc-4f6e-a7a3-b2d5d6d3a9df",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "never mind, it's not important at all. We can probably do some cleanup once we completely migrate to the framework.",
        "createdAt" : "2019-11-06T22:27:31Z",
        "updatedAt" : "2019-11-06T22:27:31Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3453a1842a01d9b1ae61a2a525c6fb3f10d7497",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +723,727 @@\tif len(g.prioritizers) == 0 && len(g.extenders) == 0 && !g.framework.HasScorePlugins() {\n\t\tresult := make(framework.NodeScoreList, 0, len(nodes))\n\t\tfor i := range nodes {\n\t\t\tresult = append(result, framework.NodeScore{\n\t\t\t\tName:  nodes[i].Name,"
  },
  {
    "id" : "399559da-1ef0-41b5-92f1-efea5ea87f88",
    "prId" : 84449,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84449#pullrequestreview-308817380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a8131f0-bfa2-4303-baeb-22803d537088",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "We can make the parameter as `sharedLister schedulerlisters.SharedLister`.",
        "createdAt" : "2019-10-29T19:14:48Z",
        "updatedAt" : "2019-10-29T20:22:08Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "a7960f39-1d0b-42e3-9cb1-cbb52c4bfd25",
        "parentId" : "4a8131f0-bfa2-4303-baeb-22803d537088",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "```genericScheduler``` is aware of  ```Snapshot```, it is actually the one that updates it, so I think it is a little confusing to use ```Snapshot``` in some part of the code here and ```SharedLister``` in others. I think a potential useful refactoring is to make ```PrioritizeNodes``` a member function of ```genericScheduler```, I can do that in a followup PR.",
        "createdAt" : "2019-10-29T20:37:39Z",
        "updatedAt" : "2019-10-29T20:37:39Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "c6baa263a3761c8c8025d547b142de6638d65719",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +705,709 @@\tctx context.Context,\n\tpod *v1.Pod,\n\tsnapshot *nodeinfosnapshot.Snapshot,\n\tmeta interface{},\n\tpriorityConfigs []priorities.PriorityConfig,"
  },
  {
    "id" : "6780ab93-08c6-42dd-94ef-8984e20feb58",
    "prId" : 84054,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84054#pullrequestreview-303502116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9194b693-a5ce-43cc-844e-582ada9cf5b6",
        "parentId" : null,
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "I don't fully understand why we run predicates twice here. But looking at the code, when this branch is reached, the status can only be Success or Unschedulable. So this change is to break if status is Unschedulable? ",
        "createdAt" : "2019-10-17T17:05:36Z",
        "updatedAt" : "2019-10-17T17:14:19Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      },
      {
        "id" : "1b116a2c-71f0-4b32-9775-4c43c4c7c480",
        "parentId" : "9194b693-a5ce-43cc-844e-582ada9cf5b6",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "The [comment](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/core/generic_scheduler.go#L626) at the top tries to explain this, do you have questions about that?",
        "createdAt" : "2019-10-17T17:23:26Z",
        "updatedAt" : "2019-10-17T17:23:26Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "89e9b436-b2a7-4bae-8cef-c45bb94e3324",
        "parentId" : "9194b693-a5ce-43cc-844e-582ada9cf5b6",
        "authorId" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "body" : "Now I understand it. Discussed offline, we are going to keep it for now and I am gonna do a followup cleanup.",
        "createdAt" : "2019-10-17T19:46:00Z",
        "updatedAt" : "2019-10-17T19:46:00Z",
        "lastEditedBy" : "a650878f-0c10-41c7-b0fc-033031305d77",
        "tags" : [
        ]
      }
    ],
    "commit" : "517116921b8a120f2c92351d080ae15ecbe783a4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +652,656 @@\t\t\t\treturn false, []predicates.PredicateFailureReason{}, nil, err\n\t\t\t}\n\t\t} else if !podsAdded || len(failedPredicates) != 0 || !status.IsSuccess() {\n\t\t\tbreak\n\t\t}"
  },
  {
    "id" : "414da55f-b6b3-43a2-beea-c91c6ff9d48b",
    "prId" : 84028,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84028#pullrequestreview-302987842",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2091257-93a5-46fb-949b-6b14c58481a4",
        "parentId" : null,
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "Will there be any default filter plugins if not configured?",
        "createdAt" : "2019-10-17T03:05:28Z",
        "updatedAt" : "2019-10-17T03:05:28Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "0bba23f8-d86b-4696-b939-51d82e04205c",
        "parentId" : "e2091257-93a5-46fb-949b-6b14c58481a4",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I am not sure what you mean, but the else clause should be executed if there is at least one filter or predicate to evaluate.",
        "createdAt" : "2019-10-17T03:18:17Z",
        "updatedAt" : "2019-10-17T03:18:17Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "dc975aa4-6e19-412c-b908-446fe91eb92a",
        "parentId" : "e2091257-93a5-46fb-949b-6b14c58481a4",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "There will be default plugins configured, but how is this relevant to this line?",
        "createdAt" : "2019-10-17T03:19:41Z",
        "updatedAt" : "2019-10-17T03:19:41Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "061de713-7674-476a-8a2a-f2d74fa560c7",
        "parentId" : "e2091257-93a5-46fb-949b-6b14c58481a4",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "Sorry, I checked the code again, it works.",
        "createdAt" : "2019-10-17T03:39:01Z",
        "updatedAt" : "2019-10-17T03:39:02Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      }
    ],
    "commit" : "17a6a7914c7c22e805694adcb1e099e8f3c17f48",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +459,463 @@\tfilteredNodesStatuses := framework.NodeToStatusMap{}\n\n\tif len(g.predicates) == 0 && !g.framework.HasFilterPlugins() {\n\t\tfiltered = g.nodeInfoSnapshot.ListNodes()\n\t} else {"
  },
  {
    "id" : "080a3660-4724-4eff-980c-69baccdb2d28",
    "prId" : 84014,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84014#pullrequestreview-304161759",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49e9492b-64a7-4b5d-9440-54d791991396",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "This doesn't look right, we used to return all predicates errors to the users, but here it changes to a single error.",
        "createdAt" : "2019-10-18T21:23:50Z",
        "updatedAt" : "2019-10-18T21:36:36Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "aef9e5ca-86d6-4dd9-8db3-50a3cec3604f",
        "parentId" : "49e9492b-64a7-4b5d-9440-54d791991396",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "This was collecting errors from all **nodes**, not predicates. Why is it useful to continue to examine all nodes and end up existing anyways? remember that this is error, not predicate failure, so it is likely that something internal went wrong and caused the error, so again keeping iterating over all nodes does not seem useful to me.",
        "createdAt" : "2019-10-18T21:58:21Z",
        "updatedAt" : "2019-10-18T22:01:22Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "23ac3e33-2d5d-48b8-8ff5-ad86d5d4284f",
        "parentId" : "49e9492b-64a7-4b5d-9440-54d791991396",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Ah I misread, `err` is for unexpected internal error, not PredicateFailure. Nevermind then.",
        "createdAt" : "2019-10-18T22:19:03Z",
        "updatedAt" : "2019-10-18T22:19:04Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "63d7733e988776355306e21947c43cb2f4fb8896",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +517,521 @@\n\t\tfiltered = filtered[:filteredLen]\n\t\tif err := errCh.ReceiveError(); err != nil {\n\t\t\treturn []*v1.Node{}, FailedPredicateMap{}, framework.NodeToStatusMap{}, err\n\t\t}"
  },
  {
    "id" : "312fc680-00af-4896-b34d-12436e8824d9",
    "prId" : 84014,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84014#pullrequestreview-305748188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c3e5063-638c-451d-a8cd-d3177da8e7b3",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This broke scalability tests: https://github.com/kubernetes/kubernetes/issues/84151\r\n\r\nTL;DR; this breaks spreading of pods in large clusters.\r\n\r\nWhat exactly has happened:\r\n1, in large enough clusters, we are using the features of finding only N feasible nodes and scoring only those:\r\nhttps://github.com/kubernetes/kubernetes/blob/9d173852c14f0e8efb0d67db2c38b8dcfa45b31b/pkg/scheduler/core/generic_scheduler.go#L464\r\n\r\n2. Before this change, we were looking for nodes starting at the point where we previously stopped (because Next() was done at the level of the original tree).\r\n3. With this change, we're always starting from 0.\r\n\r\nSo assume, you have 5k nodes, all are feasible, and numFeasible is choosing 250.\r\n- previously, you first chose nodes [1..250], then [251..500], ... [4751..5000], [1..250], ...\r\n- with this change, we will chose [1..250], [1..250], [1..250], ... until one of those nodes become unfeasible and we will choose something different.\r\n\r\nWith this PR, next() is called only in UpdateNodeSnapshotInfo:\r\nhttps://github.com/kubernetes/kubernetes/pull/84014/files#diff-f4a894ca5e905aa5f613269fc967fe2cR206\r\nand if the set of nodes doesn't change, we will pretty much always be generating the same set of nodes.\r\n\r\nThis kind of breaks the fact that scheduler is scheduling in the whole cluster. While it's not documented feature per-se, I think this isn't the right thing to do.\r\n\r\nI'm going to open a revert of this PR to to fix scalability tests (or the half of them, because we seem to have two different regressions), but will wait for your explicit approval.\r\nWe can discuss how to fix that later.\r\n",
        "createdAt" : "2019-10-23T09:14:02Z",
        "updatedAt" : "2019-10-23T09:14:02Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "888a8648-4975-4f8c-ac44-9962174e6635",
        "parentId" : "9c3e5063-638c-451d-a8cd-d3177da8e7b3",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "heh... it's no longer possible to autorevert it..",
        "createdAt" : "2019-10-23T09:15:06Z",
        "updatedAt" : "2019-10-23T09:15:07Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "3a0f7281-a204-4df3-ba18-bddaced6fe84",
        "parentId" : "9c3e5063-638c-451d-a8cd-d3177da8e7b3",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Fortunately the conflicts were trivial - opened https://github.com/kubernetes/kubernetes/pull/84222",
        "createdAt" : "2019-10-23T09:23:53Z",
        "updatedAt" : "2019-10-23T09:23:54Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "63d7733e988776355306e21947c43cb2f4fb8896",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +479,483 @@\n\t\tcheckNode := func(i int) {\n\t\t\tnodeInfo := g.nodeInfoSnapshot.NodeInfoList[i]\n\t\t\tfits, failedPredicates, status, err := g.podFitsOnNode(\n\t\t\t\tctx,"
  },
  {
    "id" : "09fbb0ae-8654-41a8-b5c8-17f1f6ce10eb",
    "prId" : 83558,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83558#pullrequestreview-300036199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "372f6f26-1f62-4e6e-a37b-685c39eee7b7",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "change ```MoreImportantPod``` to accept ```*v1.Pod``` instead of ```interface{}```",
        "createdAt" : "2019-10-10T12:52:45Z",
        "updatedAt" : "2019-10-12T16:27:50Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "589656108eff41abe5bf9e5653c59340e1318aba",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +1162,1166 @@\tvar victims []*v1.Pod\n\tnumViolatingVictim := 0\n\tsort.Slice(potentialVictims, func(i, j int) bool { return util.MoreImportantPod(potentialVictims[i], potentialVictims[j]) })\n\t// Try to reprieve as many pods as possible. We first try to reprieve the PDB\n\t// violating victims and then other non-violating ones. In both cases, we start"
  },
  {
    "id" : "e0fc53fb-34a4-46cb-b59f-8d4098f9908f",
    "prId" : 83537,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83537#pullrequestreview-298996134",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b44406f-678a-450b-8981-4f5b407d32b3",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "The comment needs to be updated too regarding the return values",
        "createdAt" : "2019-10-08T19:28:19Z",
        "updatedAt" : "2019-10-08T20:13:20Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1751c251d828d6573aeaa3708294f967fdde845a",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +558,562 @@// to run on the node given in nodeInfo to meta and nodeInfo. It returns 1) whether\n// any pod was added, 2) augmented metadata, 3) augmented CycleState 4) augmented nodeInfo.\nfunc (g *genericScheduler) addNominatedPods(pod *v1.Pod, meta predicates.PredicateMetadata, state *framework.CycleState,\n\tnodeInfo *schedulernodeinfo.NodeInfo, queue internalqueue.SchedulingQueue) (bool, predicates.PredicateMetadata,\n\t*framework.CycleState, *schedulernodeinfo.NodeInfo, error) {"
  },
  {
    "id" : "6275c9ff-483c-4985-a47b-fdfbb853171c",
    "prId" : 83537,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83537#pullrequestreview-299031068",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86ba22dd-2ce7-4a3f-b5d6-c8655e6c28e0",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Why are we removing `nominatedPods == nil` check?",
        "createdAt" : "2019-10-08T19:29:12Z",
        "updatedAt" : "2019-10-08T20:13:20Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "e89a6558-03f6-44a2-ac9c-98908f33fa94",
        "parentId" : "86ba22dd-2ce7-4a3f-b5d6-c8655e6c28e0",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "len(nominatedPods) is zero for a nil nominatedPods, so the check is redundant.",
        "createdAt" : "2019-10-08T20:07:06Z",
        "updatedAt" : "2019-10-08T20:13:20Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "25ee47e8-96f2-4f20-92ee-48c385cbb6e8",
        "parentId" : "86ba22dd-2ce7-4a3f-b5d6-c8655e6c28e0",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "Ok, thanks for explanation",
        "createdAt" : "2019-10-08T20:31:54Z",
        "updatedAt" : "2019-10-08T20:31:54Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1751c251d828d6573aeaa3708294f967fdde845a",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +566,570 @@\t}\n\tnominatedPods := queue.NominatedPodsForNode(nodeInfo.Node().Name)\n\tif len(nominatedPods) == 0 {\n\t\treturn false, meta, state, nodeInfo, nil\n\t}"
  },
  {
    "id" : "602c1eb0-9292-4896-8eaa-33c727686907",
    "prId" : 83537,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83537#pullrequestreview-299028407",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "95501cdb-5519-4f3d-aa1c-d412bacd4a00",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Why can this happen?",
        "createdAt" : "2019-10-08T19:46:19Z",
        "updatedAt" : "2019-10-08T20:13:20Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "f5c8a973-46d0-42fc-a49b-99b9348e2a7f",
        "parentId" : "95501cdb-5519-4f3d-aa1c-d412bacd4a00",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "moved it as is from where it was.",
        "createdAt" : "2019-10-08T20:27:37Z",
        "updatedAt" : "2019-10-08T20:35:19Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "1751c251d828d6573aeaa3708294f967fdde845a",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +1017,1021 @@\tcheckNode := func(i int) {\n\t\tnodeName := potentialNodes[i].Name\n\t\tif nodeNameToInfo[nodeName] == nil {\n\t\t\treturn\n\t\t}"
  },
  {
    "id" : "ac17cbc7-a20a-473f-b037-01a63c666cae",
    "prId" : 83537,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83537#pullrequestreview-299036020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c6743a2-2a1e-4462-acf1-8080f53cdeff",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "why was it not necessary to clone before?",
        "createdAt" : "2019-10-08T19:46:54Z",
        "updatedAt" : "2019-10-08T20:13:20Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "ff56ea92-b9ff-4337-8430-9ae55894da7a",
        "parentId" : "3c6743a2-2a1e-4462-acf1-8080f53cdeff",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "You mean why move it? or why copy at all? if the former, I wanted to make sure all instances of copy (which is nodeinfo/meta/state) are done in the same place, if the latter, this is needed because nodeInfo gets modified during eviction evaluation inside selectVictimsOnNode",
        "createdAt" : "2019-10-08T20:27:08Z",
        "updatedAt" : "2019-10-08T20:35:19Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "2e60bfd1-dc62-455b-9049-530600a0b436",
        "parentId" : "3c6743a2-2a1e-4462-acf1-8080f53cdeff",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "Ok, I realize now that this was moved from inside the other function. Sg.",
        "createdAt" : "2019-10-08T20:40:59Z",
        "updatedAt" : "2019-10-08T20:40:59Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "1751c251d828d6573aeaa3708294f967fdde845a",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +1020,1024 @@\t\t\treturn\n\t\t}\n\t\tnodeInfoCopy := nodeNameToInfo[nodeName].Clone()\n\t\tvar metaCopy predicates.PredicateMetadata\n\t\tif meta != nil {"
  },
  {
    "id" : "ee5e6b9f-b535-4f8c-a671-03f177d0ce9e",
    "prId" : 83537,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83537#pullrequestreview-299040929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8ec4849-5981-40a9-b2c6-79d9974f5e7c",
        "parentId" : null,
        "authorId" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "body" : "Does this need to fail here or could it aggregate the errors from nominated pods and return that?",
        "createdAt" : "2019-10-08T20:38:41Z",
        "updatedAt" : "2019-10-08T20:38:42Z",
        "lastEditedBy" : "0e2b7889-1224-444e-a36d-475f9edd0703",
        "tags" : [
        ]
      },
      {
        "id" : "86566f44-cff3-4fd9-b32e-a6e72d9af4bc",
        "parentId" : "c8ec4849-5981-40a9-b2c6-79d9974f5e7c",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I don't think it is useful to do that, if this happen, I don't it matters which nominated pod caused it.",
        "createdAt" : "2019-10-08T20:49:41Z",
        "updatedAt" : "2019-10-08T20:49:41Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "1751c251d828d6573aeaa3708294f967fdde845a",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +582,586 @@\t\t\tif metaOut != nil {\n\t\t\t\tif err := metaOut.AddPod(p, nodeInfoOut.Node()); err != nil {\n\t\t\t\t\treturn false, meta, state, nodeInfo, err\n\t\t\t\t}\n\t\t\t}"
  },
  {
    "id" : "5e4fd148-68cd-4ca0-b66c-66a0379e0347",
    "prId" : 83490,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83490#pullrequestreview-297361963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "427ef408-af07-45c1-9978-f1c46baaa08f",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Please add a comment saying that: \"MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore, therefore we need to scale the score returned by extenders to the score range used by the scheduler\".",
        "createdAt" : "2019-10-04T03:00:49Z",
        "updatedAt" : "2019-10-04T09:32:51Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "c990111b-9bce-4714-af4d-42bfd8474cdd",
        "parentId" : "427ef408-af07-45c1-9978-f1c46baaa08f",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "done",
        "createdAt" : "2019-10-04T09:32:41Z",
        "updatedAt" : "2019-10-04T09:32:51Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "27326e4f9a44a8222a5734454c37f6acf41de5a1",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +843,847 @@\t\t\t// MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore,\n\t\t\t// therefore we need to scale the score returned by extenders to the score range used by the scheduler.\n\t\t\tresult[i].Score += combinedScores[result[i].Name] * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)\n\t\t}\n\t}"
  },
  {
    "id" : "504d3303-320e-4e5d-8491-9cb96fd50742",
    "prId" : 82912,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82912#pullrequestreview-292502177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f67da2d-f6e7-4d5a-831d-748e01fc585c",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "why not do the copy in `selectVictimsNode`, given that pluginContextClone isn't used outside of it?\r\n\r\nAlso, prefer shorter names for local variables. We could start using `pCtx` instead of `pluginContext`",
        "createdAt" : "2019-09-24T14:14:25Z",
        "updatedAt" : "2019-09-24T17:02:02Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "1784d443-1167-41fc-86ab-dad55b3847c1",
        "parentId" : "8f67da2d-f6e7-4d5a-831d-748e01fc585c",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "> why not do the copy in `selectVictimsNode`, given that pluginContextClone isn't used outside of it?\r\n> \r\n\r\nTo be consistent with how \"meta\" is treated.\r\n\r\n> Also, prefer shorter names for local variables. We could start using `pCtx` instead of `pluginContext`\r\n\r\nwe already use pluginContext everywhere.\r\n",
        "createdAt" : "2019-09-24T14:29:23Z",
        "updatedAt" : "2019-09-24T17:02:02Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "b6847586-e472-4a92-be33-9c17d219047a",
        "parentId" : "8f67da2d-f6e7-4d5a-831d-748e01fc585c",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "sg, they are just nits.",
        "createdAt" : "2019-09-24T15:15:58Z",
        "updatedAt" : "2019-09-24T17:02:02Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      }
    ],
    "commit" : "37b9e6d1eadbf801b3dae4bdf7738b5eae39bf44",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1024,1028 @@\t\t\tmetaCopy = meta.ShallowCopy()\n\t\t}\n\t\tpluginContextClone := pluginContext.Clone()\n\t\tpods, numPDBViolations, fits := g.selectVictimsOnNode(\n\t\t\tpluginContextClone, pod, metaCopy, nodeNameToInfo[nodeName], fitPredicates, queue, pdbs)"
  },
  {
    "id" : "7f3743b8-16bb-4c21-813e-86e861ceb2a3",
    "prId" : 82235,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82235#pullrequestreview-294365577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48f8e3f0-4cde-47e2-bd8d-4c186c4764ff",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I suggest adding a ```continue``` to reduce indentation.\r\n\r\n```\r\nif len(pod.Labels) == 0 {\r\n  continue\r\n}\r\n```",
        "createdAt" : "2019-09-27T02:03:48Z",
        "updatedAt" : "2020-01-23T01:15:50Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "fd4a57c4-8184-4e3e-b0dc-aa9c1a465605",
        "parentId" : "48f8e3f0-4cde-47e2-bd8d-4c186c4764ff",
        "authorId" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "body" : "if `len(pod.Labels) == 0`, we have to call not only continue, but also `nonViolatingPods = append(nonViolatingPods, pod)`. Even if addressed this, the code won't be so clear.",
        "createdAt" : "2019-09-27T14:32:41Z",
        "updatedAt" : "2020-01-23T01:15:50Z",
        "lastEditedBy" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "tags" : [
        ]
      }
    ],
    "commit" : "577b2dcdacd679b81479c4adcf947d47a8c9c161",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +904,908 @@\t\tpdbForPodIsViolated := false\n\t\t// A pod with no labels will not match any PDB. So, no need to check.\n\t\tif len(pod.Labels) != 0 {\n\t\t\tfor i, pdb := range pdbs {\n\t\t\t\tif pdb.Namespace != pod.Namespace {"
  },
  {
    "id" : "ef2593fc-bc31-4b5e-bcbc-9031675e8a16",
    "prId" : 82209,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82209#pullrequestreview-284573806",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94e4d23c-eeea-4986-87e0-0915e65be36f",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "We should log the error here.",
        "createdAt" : "2019-09-05T22:23:15Z",
        "updatedAt" : "2019-09-06T23:25:38Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "89a70fa407b10329e5e71de35d94616e8d444b2d",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +1169,1173 @@\t\tif fits, err := reprievePod(p); err != nil {\n\t\t\tklog.Warningf(\"Failed to reprieve pod %q: %v\", p.Name, err)\n\t\t\treturn nil, 0, false\n\t\t} else if !fits {\n\t\t\tnumViolatingVictim++"
  },
  {
    "id" : "526fd88d-0129-42dd-98bb-642dd35017c6",
    "prId" : 82209,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/82209#pullrequestreview-284573821",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1970b3c-89d5-488c-aa3a-63e6717dcc79",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "We should log the error here.",
        "createdAt" : "2019-09-05T22:23:18Z",
        "updatedAt" : "2019-09-06T23:25:38Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "89a70fa407b10329e5e71de35d94616e8d444b2d",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +1178,1182 @@\t\tif _, err := reprievePod(p); err != nil {\n\t\t\tklog.Warningf(\"Failed to reprieve pod %q: %v\", p.Name, err)\n\t\t\treturn nil, 0, false\n\t\t}\n\t}"
  },
  {
    "id" : "5f08326d-a3e4-4d3a-b62a-1d0119e039bc",
    "prId" : 81876,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81876#pullrequestreview-284622182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "556c0441-84b1-4651-a8a9-fbba42b2a62b",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "pod and then pluginContext? I prefer we are consistent with `Schedule`",
        "createdAt" : "2019-08-26T12:26:38Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "c01f7d8f-005c-4053-8985-598b0ba9e4e6",
        "parentId" : "556c0441-84b1-4651-a8a9-fbba42b2a62b",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "I prefer the keep the context as the first argument, there is a chance it would conform `context.Context`.",
        "createdAt" : "2019-08-26T17:48:37Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      },
      {
        "id" : "5ccf9e1c-8f8b-40c8-be5d-fd6d93092c19",
        "parentId" : "556c0441-84b1-4651-a8a9-fbba42b2a62b",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "@alculquicondor \r\nFYI:https://golang.org/pkg/context/\r\n_The Context should be the first parameter_",
        "createdAt" : "2019-08-27T02:19:49Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "2afafd91-223f-4ab7-8618-0f3fe7bf2fda",
        "parentId" : "556c0441-84b1-4651-a8a9-fbba42b2a62b",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "This is not currently an implementation of context.Context, so it doesn't apply. That said, we should consider rearranging the parameters in Schedule along with Preempt. But that shouldn't be in this PR.",
        "createdAt" : "2019-08-27T12:39:34Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "5123f0e8-9d90-4eed-bf40-00459aab78ca",
        "parentId" : "556c0441-84b1-4651-a8a9-fbba42b2a62b",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "+1 to Aldo's suggestion regarding rearranging the Schedule to be consistent with Preempt in a separate PR.",
        "createdAt" : "2019-08-27T14:02:36Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "4b74c035-9103-4381-883d-e6f3475b3eef",
        "parentId" : "556c0441-84b1-4651-a8a9-fbba42b2a62b",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Ok. I will do that. :)",
        "createdAt" : "2019-08-28T09:48:54Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "523a3b0a-bd21-4743-be78-3af0d8467f8f",
        "parentId" : "556c0441-84b1-4651-a8a9-fbba42b2a62b",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "IMO we could accept this since we have already reached a consensus on conforming the Context in https://github.com/kubernetes/kubernetes/issues/81433 and https://github.com/kubernetes/kubernetes/pull/82072 is making the change.",
        "createdAt" : "2019-09-06T01:54:16Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "d84a75c1405b8ba0f9e6770b4220bc444a823f5a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +138,142 @@\t// It returns the node where preemption happened, a list of preempted pods, a\n\t// list of pods whose nominated node name should be removed, and error if any.\n\tPreempt(*framework.PluginContext, *v1.Pod, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error)\n\t// Predicates() returns a pointer to a map of predicate functions. This is\n\t// exposed for testing."
  },
  {
    "id" : "3f8c0bd4-218f-479f-a22c-d7059fc37a73",
    "prId" : 81876,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81876#pullrequestreview-280714483",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8eec310-2a5d-4749-b344-81d8dbfdf05c",
        "parentId" : null,
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "what about status? That could be an error as well.",
        "createdAt" : "2019-08-26T12:40:30Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "fc7ae6ed-865e-42fd-b92e-1dc8bc91b57f",
        "parentId" : "b8eec310-2a5d-4749-b344-81d8dbfdf05c",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "DONE",
        "createdAt" : "2019-08-28T09:51:35Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d84a75c1405b8ba0f9e6770b4220bc444a823f5a",
    "line" : 151,
    "diffHunk" : "@@ -1,1 +1127,1131 @@\t// priority pods is not a recommended configuration anyway.\n\tif fits, _, _, err := g.podFitsOnNode(pluginContext, pod, meta, nodeInfoCopy, fitPredicates, queue, false); !fits {\n\t\tif err != nil {\n\t\t\tklog.Warningf(\"Encountered error while selecting victims on node %v: %v\", nodeInfo.Node().Name, err)\n\t\t}"
  },
  {
    "id" : "c3805b94-762d-4d31-aed5-d819f9251c9e",
    "prId" : 81876,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81876#pullrequestreview-281807129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "242ea567-368e-4b9b-a9f1-041723bdc706",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "we should add both:\r\n```\r\nif !status.IsSuccess() {\r\n  filteredNodesStatuses[nodeName] = status\r\n} \r\nif len(failedPredicates) {\r\n  failedPredicateMap[nodeName] = failedPredicates\r\n}\r\n```",
        "createdAt" : "2019-08-29T14:47:21Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "c7a01a98-44cc-4699-8a53-3aa8dd6e2c21",
        "parentId" : "242ea567-368e-4b9b-a9f1-041723bdc706",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Fixed",
        "createdAt" : "2019-08-30T02:47:19Z",
        "updatedAt" : "2019-09-06T01:59:18Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d84a75c1405b8ba0f9e6770b4220bc444a823f5a",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +520,524 @@\t\t\t\tif len(failedPredicates) != 0 {\n\t\t\t\t\tfailedPredicateMap[nodeName] = failedPredicates\n\t\t\t\t}\n\t\t\t\tpredicateResultLock.Unlock()\n\t\t\t}"
  },
  {
    "id" : "f3e189cb-c2a0-405a-bd5b-f1cdba5d68df",
    "prId" : 81346,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81346#pullrequestreview-278309473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed245d85-8c5b-4d37-9320-ca74b3bdb8a8",
        "parentId" : null,
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Can you add any unit tests for this change?",
        "createdAt" : "2019-08-13T09:20:59Z",
        "updatedAt" : "2019-08-22T08:21:16Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "128555b2-3e04-41d7-a40c-ba85a17b9d36",
        "parentId" : "ed245d85-8c5b-4d37-9320-ca74b3bdb8a8",
        "authorId" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "body" : "This change is for performance improvement. In k8s, is such change a target of unit tests?",
        "createdAt" : "2019-08-13T09:41:23Z",
        "updatedAt" : "2019-08-22T08:21:16Z",
        "lastEditedBy" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "tags" : [
        ]
      },
      {
        "id" : "02025811-04be-434a-9124-47ee2c136208",
        "parentId" : "ed245d85-8c5b-4d37-9320-ca74b3bdb8a8",
        "authorId" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "body" : "@wgliang I added unit tests for this change. Review please.",
        "createdAt" : "2019-08-22T09:51:18Z",
        "updatedAt" : "2019-08-22T09:51:18Z",
        "lastEditedBy" : "e1ba72c9-3be8-432b-b345-ac2d180a8eab",
        "tags" : [
        ]
      }
    ],
    "commit" : "acd82613efe26e766964546849a2fa698ff383fa",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +601,605 @@\t\t}\n\t}\n\treturn podsAdded, metaOut, nodeInfoOut\n}\n"
  },
  {
    "id" : "d94e987a-fcce-48c3-bf6f-59db55224101",
    "prId" : 80901,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80901#pullrequestreview-274315990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fa47b5e-38c5-4524-aa3c-0ce1fd662d2c",
        "parentId" : null,
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I suspect that the original code will be faster because it has better cache locality when reading scores from the scoresMap. ",
        "createdAt" : "2019-08-09T19:21:38Z",
        "updatedAt" : "2019-08-09T19:23:26Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "f18cae98-1b74-44fc-b179-da3bec6a91a9",
        "parentId" : "4fa47b5e-38c5-4524-aa3c-0ce1fd662d2c",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "We need more evidence on this, before that I prefer to keep it this way.",
        "createdAt" : "2019-08-12T08:11:20Z",
        "updatedAt" : "2019-08-12T08:11:20Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      },
      {
        "id" : "a6345e3f-3cf9-4265-a87d-3e1ed57adf0c",
        "parentId" : "4fa47b5e-38c5-4524-aa3c-0ce1fd662d2c",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "In theory sequential (or strided) reads are the fastest form of memory access patterns because of prefetching and cache locality. \r\n\r\nThe other thing in this nested loop that could make it slower is that it makes n*m map lookups (n=number of nodes, m = number of priority functions/score plugins), while if we make the node loop inside, the number of scoresMap lookups will be zero. So it might be even faster (and easier to read) to flip the whole thing like this:\r\n\r\n```\r\nfor i := range nodes {\r\n\t\tresult = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0})\r\n}\r\n\r\nfor j, priorityList := range priorityConfigs {\r\n\t\tfor i := range nodes {\r\n\t\t\tresult[i].Score += priorityList[i].Score * priorityConfigs[j].Weight\r\n\t\t}\r\n}\r\n\r\nfor _, scoreList := range scoresMap {\r\n\t\tfor i := range nodes {\r\n\t\t\tresult[i].Score += scoreList[i].Score\r\n\t\t}\r\n}\r\n```\r\n\r\n> We need more evidence on this, before that I prefer to keep it this way.\r\nThe theory says this change makes things slower, so if you don't have evidence to the contrary, then we shouldn't make the change, especially that it is not related to the purpose of the PR \"using named array in normalize scores\".\r\n\r\n\r\n",
        "createdAt" : "2019-08-13T05:33:23Z",
        "updatedAt" : "2019-08-13T05:33:23Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "1065d178-5488-4bb1-af55-304500de4b35",
        "parentId" : "4fa47b5e-38c5-4524-aa3c-0ce1fd662d2c",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "> In theory sequential (or strided) reads are the fastest form of memory access patterns because of prefetching and cache locality.\r\n\r\nI know the theory of the cache locality and here are the benchmark numbers:\r\n\r\n```go\r\npackage main\r\n\r\nimport \"testing\"\r\n\r\nfunc locality(result []int, scoresMap map[int][]int, anotherScoresMap map[int][]int) {\r\n\tfor _, scores := range scoresMap {\r\n\t\tfor i := range scores {\r\n\t\t\tresult[i] = result[i] + scores[i]\r\n\t\t}\r\n\t}\r\n\r\n\tfor _, scores := range anotherScoresMap {\r\n\t\tfor i := range scores {\r\n\t\t\tresult[i] = result[i] + scores[i]\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc nonlocality(result []int, scoresMap map[int][]int, anotherScoresMap map[int][]int) {\r\n\tfor i := range result {\r\n\t\tfor j := range scoresMap {\r\n\t\t\tresult[i] = result[i] + scoresMap[j][i]\r\n\t\t}\r\n\r\n\t\tfor j := range anotherScoresMap {\r\n\t\t\tresult[i] = result[i] + anotherScoresMap[j][i]\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc BenchmarkLocality(b *testing.B) {\r\n\tresult := make([]int, 5000)\r\n\tscores := make(map[int][]int, 10)\r\n\tanotherScores := make(map[int][]int, 10)\r\n\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tscores[i] = make([]int, 5000)\r\n\t\tanotherScores[i] = make([]int, 5000)\r\n\t\tfor j := 0; j < 5000; j++ {\r\n\t\t\tscores[i][j] = j\r\n\t\t\tanotherScores[i][j] = j\r\n\t\t}\r\n\t}\r\n\r\n\tb.ResetTimer()\r\n\r\n\tfor i := 0; i < b.N; i++ {\r\n\t\tlocality(result, scores, anotherScores)\r\n\t}\r\n}\r\n\r\nfunc BenchmarkNonLocality(b *testing.B) {\r\n\tresult := make([]int, 5000)\r\n\tscores := make(map[int][]int, 10)\r\n\tanotherScores := make(map[int][]int, 10)\r\n\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tscores[i] = make([]int, 5000)\r\n\t\tanotherScores[i] = make([]int, 5000)\r\n\t\tfor j := 0; j < 5000; j++ {\r\n\t\t\tscores[i][j] = j\r\n\t\t\tanotherScores[i][j] = j\r\n\t\t}\r\n\t}\r\n\r\n\tb.ResetTimer()\r\n\r\n\tfor i := 0; i < b.N; i++ {\r\n\t\tlocality(result, scores, anotherScores)\r\n\t}\r\n}\r\n\r\n$ go test -bench=.\r\ngoos: darwin\r\ngoarch: amd64\r\npkg: github.com/golang\r\nBenchmarkLocality-8      \t   30000\t     44607 ns/op\r\nBenchmarkNonLocality-8   \t   30000\t     44245 ns/op\r\nPASS\r\n```\r\n\r\nAccording to the benchmarks, I prefer to keep it this way.",
        "createdAt" : "2019-08-13T06:12:04Z",
        "updatedAt" : "2019-08-13T06:19:28Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      },
      {
        "id" : "bac8f5de-189f-46e5-ba26-a4c46d5ef32a",
        "parentId" : "4fa47b5e-38c5-4524-aa3c-0ce1fd662d2c",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Strange, it is either golang's compiler is so good, or new architectures have such powerful memory controllers. I would like to do the same test in C++ to see if I get different results.\r\n\r\nDo you have a theory why they are the same?",
        "createdAt" : "2019-08-13T07:30:08Z",
        "updatedAt" : "2019-08-13T07:30:09Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "3a1cefc6-9ace-4586-8ea3-a9ccc30108bf",
        "parentId" : "4fa47b5e-38c5-4524-aa3c-0ce1fd662d2c",
        "authorId" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "body" : "The thing is that the number of priorities (`j` values) is very small, and so is the number of nodes. We are only talking about ~200kb, which can fit in the caches pretty easily.\r\n\r\nMinor note in the posted benchmark: it should be `map[string][]int`. but again, we are only doing lookups among 10 values.\r\n\r\nOn the other hand, I don't understand the motivation to do the change. I'm not going to hold the review.",
        "createdAt" : "2019-08-13T14:05:44Z",
        "updatedAt" : "2019-08-13T14:06:40Z",
        "lastEditedBy" : "31fbce73-ef64-43f8-9faa-047479d8fc32",
        "tags" : [
        ]
      },
      {
        "id" : "9dd6b07e-05c4-4fcf-8d97-27d7a5b29b7b",
        "parentId" : "4fa47b5e-38c5-4524-aa3c-0ce1fd662d2c",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "I re ran the test and set the sizes to two orders of magnitude bigger, no difference.\r\n\r\nThe issue about cache is not only about size, but also what you move to the cache (how much of the cache line being moved from memory is actually used), so it is about effective memory bandwidth.",
        "createdAt" : "2019-08-13T14:18:01Z",
        "updatedAt" : "2019-08-13T14:18:02Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa5f9fda52d0171e45682254e0d37b16f58ae6fc",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +807,811 @@\n\t\tfor j := range scoresMap {\n\t\t\tresult[i].Score += scoresMap[j][i].Score\n\t\t}\n\t}"
  },
  {
    "id" : "6792a5c6-603d-492b-9c19-09f01646d2bb",
    "prId" : 79109,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79109#pullrequestreview-257755207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b328aa04-8a3a-48d3-960a-12de107fdc0a",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Similar to the filter plugins, can we keep the parallelize code here and pass a single node to `RunScorePlugins`?",
        "createdAt" : "2019-07-02T00:19:45Z",
        "updatedAt" : "2019-07-16T14:25:03Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "c02f9acf-7264-44d3-83ad-32937df6f3ba",
        "parentId" : "b328aa04-8a3a-48d3-960a-12de107fdc0a",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "We can, but it will not be as easy to follow as this implementation.  Note that we can't really escape having parallelism inside the framework: the implementation of NormalizeScore will have to employ parallelism across plugins. One argument could be that we should maintain consistency in the place where parallelism is implemented for Score and NormalizeScore, but I don't have strong opinion on this. What do you think?",
        "createdAt" : "2019-07-02T01:01:22Z",
        "updatedAt" : "2019-07-16T14:25:03Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "bf2d2c98-87eb-430d-872c-dda2287e56aa",
        "parentId" : "b328aa04-8a3a-48d3-960a-12de107fdc0a",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "For NormalizeScore, we need to pass all node scores to the plugins that do normalization. Depending on the kind of normalization algorithm we use, we may have parallelism or not. For this particular plugins, I feel if we have the parallelism in one place, it will be easier to follow the code. Of course, this is something we can change in the future. These are implementation details and are not part of our public API.",
        "createdAt" : "2019-07-02T19:45:43Z",
        "updatedAt" : "2019-07-16T14:25:03Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "bfdb7f7f-3366-40f9-95f7-a266801d232c",
        "parentId" : "b328aa04-8a3a-48d3-960a-12de107fdc0a",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "NormalizeScore could parallelize across the plugins similar to what Reduce currently do, but l guess we can leave this discussion to the NormalizeScore PR.\r\n\r\nFor this one, I prefer to keep it as is for now, I tried to workout a way to move parallelism out, but it is not going to be pretty.",
        "createdAt" : "2019-07-02T21:06:04Z",
        "updatedAt" : "2019-07-16T14:25:03Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "0407ab94-d09a-4ea2-aaa6-0699c0f84b1a",
        "parentId" : "b328aa04-8a3a-48d3-960a-12de107fdc0a",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "That's fine. As I said, we can change this in the future if needed.",
        "createdAt" : "2019-07-03T22:21:27Z",
        "updatedAt" : "2019-07-16T14:25:03Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "a80425bd23282ecf37e01d35d901aecdb9704964",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +765,769 @@\n\t// Run the Score plugins.\n\tscoresMap, scoreStatus := framework.RunScorePlugins(pluginContext, pod, nodes)\n\tif !scoreStatus.IsSuccess() {\n\t\treturn schedulerapi.HostPriorityList{}, scoreStatus.AsError()"
  },
  {
    "id" : "13a3f4b7-65ac-4cf6-96b6-cd3e814d5afe",
    "prId" : 78782,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/78782#pullrequestreview-247340331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "052ac9a7-5488-4c54-a483-59e715f7b0e8",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "I assume holding lock over the course of checkNode execution would defeat the purpose of the original improvement.",
        "createdAt" : "2019-06-07T23:35:54Z",
        "updatedAt" : "2019-06-07T23:35:55Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "ece3e3cdba4985215d76c82ba713a778fe978aaf",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +462,466 @@\t\tfiltered = nodes\n\t} else {\n\t\tallNodes := int32(g.cache.NodeTree().NumNodes())\n\t\tnumNodesToFind := g.numFeasibleNodesToFind(allNodes)\n"
  },
  {
    "id" : "7a3aa312-7f4b-49a5-b2ac-c4f8989d2f89",
    "prId" : 77509,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77509#pullrequestreview-234783897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ab49721-081c-4462-a664-e6f6c32f4f51",
        "parentId" : null,
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "I have a question, how do we deal with the reduction in the number of Nodes using the index? Is it possible to change position(skipped a node) or index out of range?",
        "createdAt" : "2019-05-06T23:07:12Z",
        "updatedAt" : "2019-05-08T13:50:50Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "ac7ae2d9-fd0a-4b78-89c0-a1fec786cee8",
        "parentId" : "9ab49721-081c-4462-a664-e6f6c32f4f51",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "The modulo should handle this, it ensures that g.lastIndex is always between 0 and \"number_of_nodes - 1\". Please let me know if I didn't fully answer the question.",
        "createdAt" : "2019-05-07T01:03:19Z",
        "updatedAt" : "2019-05-08T13:50:50Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "ffed463f-9e68-4719-ab33-60a0f2232c44",
        "parentId" : "9ab49721-081c-4462-a664-e6f6c32f4f51",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Suppose the current number of nodes is 100, g. lastIndex is 90, but in the next scheduling loop, the number of nodes is reduced to 80, at this time g. lastIndex value does not exist.",
        "createdAt" : "2019-05-07T01:51:46Z",
        "updatedAt" : "2019-05-08T13:50:50Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "7ca237bc-4a7d-415f-85db-19788aca2462",
        "parentId" : "9ab49721-081c-4462-a664-e6f6c32f4f51",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "right, but we are taking the modulo: (90 + whatever) % 80 = a number between 0 and 79",
        "createdAt" : "2019-05-07T02:51:11Z",
        "updatedAt" : "2019-05-08T13:50:50Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "d86cef86-54ae-4c01-a5e4-26c90cdfd955",
        "parentId" : "9ab49721-081c-4462-a664-e6f6c32f4f51",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "Ok, then will this break the fairness of each node being chosen? Because we can't guarantee which node nodes are deleted, we can't find the last time we left by indexing.",
        "createdAt" : "2019-05-07T03:45:07Z",
        "updatedAt" : "2019-05-08T13:50:50Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      },
      {
        "id" : "1dc0b52d-7077-41ca-b72e-66673ff62125",
        "parentId" : "9ab49721-081c-4462-a664-e6f6c32f4f51",
        "authorId" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "body" : "Not really, this is pretty much the same semantics of the original code (if not better).\r\n\r\nIn the original code, we were relying on next to pick the node. nodeArray.next function resets to zero if the index was larger than the length of the node array, and so it has the exact same issue you are describing here: if the index was 90, and the next scheduling loop the number of nodes were reduced to 80, then the next node that will be picked up is always the one at index 0 irrespective of which nodes were removed.\r\n\r\nThe only difference with the new logic is that we don't reset to zero, we loop back, so in the example above, the next nodes will be picked starting from index 10. So in a way we improve fairness in that we don't always restart at the same place (zero).",
        "createdAt" : "2019-05-07T14:06:37Z",
        "updatedAt" : "2019-05-08T13:50:50Z",
        "lastEditedBy" : "570b631b-84a0-4888-a815-ca0e7934e412",
        "tags" : [
        ]
      },
      {
        "id" : "dd463bbd-d973-4cfc-b5a3-e0abff899b18",
        "parentId" : "9ab49721-081c-4462-a664-e6f6c32f4f51",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "This is not necessarily better than the old code. In the old version of the code, fairness after adding/removing nodes was being kept by NodeTree. Now, the client of NodeTree preserves fairness. So, I think the final outcome is similar.",
        "createdAt" : "2019-05-07T22:03:47Z",
        "updatedAt" : "2019-05-08T13:50:50Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "e660e84459b990239ed1a6f65627cd08617d21f6",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +517,521 @@\t\t// are found.\n\t\tworkqueue.ParallelizeUntil(ctx, 16, len(allNodes), checkNode)\n\t\tg.lastIndex = (g.lastIndex + int(processedNodes)) % len(allNodes)\n\n\t\tfiltered = filtered[:filteredLen]"
  },
  {
    "id" : "ac67f8c8-e75e-4b9f-bffa-cdf605974479",
    "prId" : 75848,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/75848#pullrequestreview-225845702",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d2517b1-209d-4b94-b3ab-61150613eff1",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "Why did you change this to a pointer? the snapshot wraps a map, which is another reference type.",
        "createdAt" : "2019-04-10T23:54:25Z",
        "updatedAt" : "2019-04-29T23:41:08Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "13768059-abab-447e-9f8f-1d0ac0d1ccad",
        "parentId" : "5d2517b1-209d-4b94-b3ab-61150613eff1",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Other functions that take a snapshot, take a pointer of the snapshot. I didn't want to change all of those.",
        "createdAt" : "2019-04-11T23:49:03Z",
        "updatedAt" : "2019-04-29T23:41:08Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "83828bcb2df27f3111837226ce87258d76533090",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +163,167 @@\tlastNodeIndex            uint64\n\talwaysCheckAllPredicates bool\n\tnodeInfoSnapshot         *internalcache.NodeInfoSnapshot\n\tvolumeBinder             *volumebinder.VolumeBinder\n\tpvcLister                corelisters.PersistentVolumeClaimLister"
  },
  {
    "id" : "f523d348-efd3-413e-88ba-8c28d7e9fae1",
    "prId" : 74974,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74974#pullrequestreview-217064004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Should we check the second node's start time (in this case) ?",
        "createdAt" : "2019-03-20T15:34:38Z",
        "updatedAt" : "2019-03-20T15:36:03Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "606af9ab-44ba-4c32-8ce2-1dfea239df31",
        "parentId" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "authorId" : "740aa7e1-09de-43aa-afe4-e89f80f4efe2",
        "body" : "i think this is a case which should not be reached, the log has `Should not reach here`",
        "createdAt" : "2019-03-20T15:46:02Z",
        "updatedAt" : "2019-03-20T15:46:02Z",
        "lastEditedBy" : "740aa7e1-09de-43aa-afe4-e89f80f4efe2",
        "tags" : [
        ]
      },
      {
        "id" : "c7a11c42-f258-4bf6-b6d6-ebe66ff846a9",
        "parentId" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Later on line 925:\r\n```\r\n                        klog.Errorf(\"earliestStartTime is nil for node %s. Should not reach here.\", node)\r\n                        continue\r\n```\r\nI think using a loop for finding start time around line 913 would make the code more consistent.",
        "createdAt" : "2019-03-20T15:56:20Z",
        "updatedAt" : "2019-03-20T15:56:20Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "abb69ac3-8751-4909-a8a8-705515260efb",
        "parentId" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "@tedyu `latestStartTime` should never be nil. If for some unexpected reason it becomes nil, we prefer to see an error message and investigate the issue instead of tolerating the error.",
        "createdAt" : "2019-03-20T16:54:06Z",
        "updatedAt" : "2019-03-20T16:54:06Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "08523349-1dd1-455a-825b-8d06fc4182a9",
        "parentId" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "In that case, panic'ing would be proper choice in this scenario.",
        "createdAt" : "2019-03-20T17:01:28Z",
        "updatedAt" : "2019-03-20T17:01:29Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      },
      {
        "id" : "084f5ac6-62cf-428e-aa80-8414cb0a9dbb",
        "parentId" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "authorId" : "740aa7e1-09de-43aa-afe4-e89f80f4efe2",
        "body" : "Hi @tedyu , IMO, a nil for timestamp is unlikely to happen. and if it happens, we can still fall back by returning minNodes2[0], which is the same as before where we don't take \"start time of pods\" into account.",
        "createdAt" : "2019-03-20T17:10:54Z",
        "updatedAt" : "2019-03-20T17:10:54Z",
        "lastEditedBy" : "740aa7e1-09de-43aa-afe4-e89f80f4efe2",
        "tags" : [
        ]
      },
      {
        "id" : "89967e8a-da9a-4308-907e-52577d669d88",
        "parentId" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "authorId" : "df8dc16d-08c7-457c-8593-619395912000",
        "body" : "> In that case, panic'ing would be proper choice in this scenario.\r\n\r\nSome test cases will panic because they perhaps do not set `StartTime` :(",
        "createdAt" : "2019-03-21T02:51:45Z",
        "updatedAt" : "2019-03-21T02:51:45Z",
        "lastEditedBy" : "df8dc16d-08c7-457c-8593-619395912000",
        "tags" : [
        ]
      },
      {
        "id" : "5f3358d1-54c6-45e3-b243-db708ec34649",
        "parentId" : "bb095334-7dcd-4e90-9358-c3b022cbb705",
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "Interesting.\r\nIn another popular Apache project, my test encounters NPE due to null being passed by test(s) to configure() (in production, it wouldn't be null).\r\nI am modifying the test not to do that.\r\n\r\nFor k8s, we should do the same - tests should mimic what happens in production.",
        "createdAt" : "2019-03-21T02:57:41Z",
        "updatedAt" : "2019-03-21T02:57:41Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d991e6ee22400d187b53ca4699ad99489e8bbc2",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +914,918 @@\tif latestStartTime == nil {\n\t\t// If the earliest start time of all pods on the 1st node is nil, just return it,\n\t\t// which is not expected to happen.\n\t\tklog.Errorf(\"earliestStartTime is nil for node %s. Should not reach here.\", minNodes2[0])\n\t\treturn minNodes2[0]"
  },
  {
    "id" : "27037e18-4813-40b1-bf2e-2b5f27cd526b",
    "prId" : 74614,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74614#pullrequestreview-226985069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "384c980b-01c0-4183-add5-9335651dcc8a",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "We should probably create a struct for all of these configuration related parameters and pass that struct as an argument to `NewGenericScheduler`, but not in this PR. We can do that in a follow-up PR.",
        "createdAt" : "2019-04-16T01:08:40Z",
        "updatedAt" : "2019-05-31T04:42:34Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "94017f08-4540-4ae2-beff-3c0891f9e7d5",
        "parentId" : "384c980b-01c0-4183-add5-9335651dcc8a",
        "authorId" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "body" : "+1 ",
        "createdAt" : "2019-04-16T05:02:46Z",
        "updatedAt" : "2019-05-31T04:42:34Z",
        "lastEditedBy" : "89bff7d0-c420-41e1-9e5e-db63c4cccd93",
        "tags" : [
        ]
      }
    ],
    "commit" : "52f3380ef3a5d5de4d75ff92defd1c9af9d775bc",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1226,1230 @@\tdisablePreemption bool,\n\tpercentageOfNodesToScore int32,\n\tenableNonPreempting bool,\n) ScheduleAlgorithm {\n\treturn &genericScheduler{"
  },
  {
    "id" : "6d7eab44-45f6-4463-84fe-13d907c9d3be",
    "prId" : 74041,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74041#pullrequestreview-204039408",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12ac699e-e55d-4e9a-87c5-95fcad9f5207",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "not really within scope of this PR, but do we really need these long import aliases for our own internal packages?",
        "createdAt" : "2019-02-14T23:28:38Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      },
      {
        "id" : "14867c8a-b8cc-4541-8868-85c78727f03e",
        "parentId" : "12ac699e-e55d-4e9a-87c5-95fcad9f5207",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Probably not, but as you noted, I don't want to include unrelated cleanups in this PR.",
        "createdAt" : "2019-02-15T00:40:07Z",
        "updatedAt" : "2019-02-20T21:55:44Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "337cb7036cc260264ebc8e4ad2925040de75ba65",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +142,146 @@\tlastNodeIndex            uint64\n\talwaysCheckAllPredicates bool\n\tnodeInfoSnapshot         schedulerinternalcache.NodeInfoSnapshot\n\tvolumeBinder             *volumebinder.VolumeBinder\n\tpvcLister                corelisters.PersistentVolumeClaimLister"
  },
  {
    "id" : "656497f9-6485-43c1-a1ca-187a0953d48e",
    "prId" : 72332,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72332#pullrequestreview-187983990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "576aff83-d69a-4b97-87aa-c576c82dc0b8",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Are you saying replacing \"microseconds\" with \"seconds\" is a common pattern? If yes, can you show that where it's documented?",
        "createdAt" : "2018-12-26T18:31:40Z",
        "updatedAt" : "2019-01-08T05:08:50Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "e93b972a-4db2-43a9-968d-c62321a44c1a",
        "parentId" : "576aff83-d69a-4b97-87aa-c576c82dc0b8",
        "authorId" : "a472aa4a-c4f7-4692-bc21-708257a1dacc",
        "body" : "Correct.\r\nFrom the KEP [kubernetes-metrics-overhaul](https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/0031-kubernetes-metrics-overhaul.md), Kubernetes metrics should follow Prometheus best practices.\r\nThe `seconds` is the [suggested base unit](https://prometheus.io/docs/practices/naming/#base-units) for time type metrics. And `seconds` already been widely used in other Kubernetes metrics.\r\nSo we should change these metrics to let them more consistent with others.",
        "createdAt" : "2018-12-27T01:34:35Z",
        "updatedAt" : "2019-01-08T05:08:50Z",
        "lastEditedBy" : "a472aa4a-c4f7-4692-bc21-708257a1dacc",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f516f751d04e6cc6d0fca371b23fa0f8fc47e0e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +193,197 @@\t\t}\n\t}\n\tmetrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInSeconds(startPredicateEvalTime))\n\tmetrics.DeprecatedSchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime))\n\tmetrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime))"
  },
  {
    "id" : "2a22da3c-73c7-4472-a011-8aa7fe1c3dcc",
    "prId" : 72140,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72140#pullrequestreview-186822631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f708bec6-25d8-4aea-811b-25dabb370f81",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Now that this is a bit more complex, could you please write a unit-test for this function?",
        "createdAt" : "2018-12-20T00:45:59Z",
        "updatedAt" : "2018-12-20T03:20:16Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "2b21ec48-dc7b-40b5-9a36-53e6af944c7c",
        "parentId" : "f708bec6-25d8-4aea-811b-25dabb370f81",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "> NOTE: I will add some test case about the percentage of nodes if #71926 update the scheduling result type.\r\n\r\nYes, in fact, I originally planned to wait for #71926 to merge and add unit test cases for this because it need to use `ScheduleResult` type.\r\n",
        "createdAt" : "2018-12-20T01:52:02Z",
        "updatedAt" : "2018-12-20T03:20:16Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6515c4e09b37c530df38f55e1f02912398c5bff1",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +381,385 @@// numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops\n// its search for more feasible nodes.\nfunc (g *genericScheduler) numFeasibleNodesToFind(numAllNodes int32) (numNodes int32) {\n\tif numAllNodes < minFeasibleNodesToFind || g.percentageOfNodesToScore >= 100 {\n\t\treturn numAllNodes"
  },
  {
    "id" : "4065701c-0224-4789-a4e1-d40fba255c22",
    "prId" : 72140,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72140#pullrequestreview-187148319",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "794f057f-9c33-4c90-8279-3f47eea733cd",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "doc doesn't mention negative percentages",
        "createdAt" : "2018-12-20T19:24:09Z",
        "updatedAt" : "2018-12-20T19:24:09Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "6515c4e09b37c530df38f55e1f02912398c5bff1",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +387,391 @@\n\tadaptivePercentage := g.percentageOfNodesToScore\n\tif adaptivePercentage <= 0 {\n\t\tadaptivePercentage = schedulerapi.DefaultPercentageOfNodesToScore - numAllNodes/125\n\t\tif adaptivePercentage < minFeasibleNodesPercentageToFind {"
  },
  {
    "id" : "dadd9807-6fa0-4c53-8f00-432a1f2c2cd7",
    "prId" : 72140,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72140#pullrequestreview-187166193",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f813d120-9808-4496-9f66-58505c89d368",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this is hard to visualize... curious what the graph looks like as nodes range from 1-5000",
        "createdAt" : "2018-12-20T19:26:06Z",
        "updatedAt" : "2018-12-20T19:26:06Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "11a10f3f-419a-4936-8dcc-8232e28b319f",
        "parentId" : "f813d120-9808-4496-9f66-58505c89d368",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "This is actually a linear formula which produces 50 for 100 node clusters and 10 for 5000 node clusters. However, we have a lower bound of 100 nodes to score. So, we score all nodes of the clusters for clusters of 100 nodes or smaller.",
        "createdAt" : "2018-12-20T19:39:36Z",
        "updatedAt" : "2018-12-20T19:39:36Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "a401fe25-9af8-4bd2-8223-6d2337c2d527",
        "parentId" : "f813d120-9808-4496-9f66-58505c89d368",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Putting that in a comment would be helpful",
        "createdAt" : "2018-12-20T20:13:07Z",
        "updatedAt" : "2018-12-20T20:13:08Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "6515c4e09b37c530df38f55e1f02912398c5bff1",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +388,392 @@\tadaptivePercentage := g.percentageOfNodesToScore\n\tif adaptivePercentage <= 0 {\n\t\tadaptivePercentage = schedulerapi.DefaultPercentageOfNodesToScore - numAllNodes/125\n\t\tif adaptivePercentage < minFeasibleNodesPercentageToFind {\n\t\t\tadaptivePercentage = minFeasibleNodesPercentageToFind"
  },
  {
    "id" : "4da10224-50ff-4301-ac72-a7d2ba0aacaa",
    "prId" : 71722,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/71722#pullrequestreview-181912163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c02fc142-8c32-48f6-b02f-b30d46ebc85a",
        "parentId" : null,
        "authorId" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "body" : "Just out of curiosity. There will never be a situation when Function != nil and Reduce != nil, right?",
        "createdAt" : "2018-12-05T12:49:34Z",
        "updatedAt" : "2018-12-05T12:49:34Z",
        "lastEditedBy" : "e25d86e1-9b84-46f1-8d7f-a44328cd9bb4",
        "tags" : [
        ]
      },
      {
        "id" : "e3480987-0ed8-4438-997d-8a8b16aad756",
        "parentId" : "c02fc142-8c32-48f6-b02f-b30d46ebc85a",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "That's right. The Map/Reduce functions are the new style for priority functions. The old-style was using \"Function\". No priority function is supposed to mix the two and none of our priority functions do that.",
        "createdAt" : "2018-12-05T18:43:11Z",
        "updatedAt" : "2018-12-05T18:43:11Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "ece8f42ea2558bbc19560d3bb805900ef2f34c39",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +691,695 @@\n\tfor i := range priorityConfigs {\n\t\tif priorityConfigs[i].Reduce == nil {\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "2bfea995-77f8-4181-ba57-1f3ddf3e2419",
    "prId" : 70898,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70898#pullrequestreview-174153124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57451a01-ae1e-4f67-9378-27dc953e6774",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "checking UID is necessary b/c right now `pod` itself exists in nominatedPods.",
        "createdAt" : "2018-11-13T00:46:33Z",
        "updatedAt" : "2018-11-16T22:24:26Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4fd11512ac3cce6e7932a08db77db798167af1b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +512,516 @@\tnodeInfoOut := nodeInfo.Clone()\n\tfor _, p := range nominatedPods {\n\t\tif util.GetPodPriority(p) >= util.GetPodPriority(pod) && p.UID != pod.UID {\n\t\t\tnodeInfoOut.AddPod(p)\n\t\t\tif metaOut != nil {"
  },
  {
    "id" : "6a33a9ff-ac47-4675-a041-4317f4ff08fd",
    "prId" : 70892,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70892#pullrequestreview-173646534",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "457d58e0-9a0b-49f7-bd8e-b0f27e9813e9",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "One nit: we can remove L656~L658, and move L657 here:\r\n\r\n```suggestion\r\n\t\t\tresults[i] = make(schedulerapi.HostPriorityList, len(nodes))\r\n\t\t\treturn\r\n```",
        "createdAt" : "2018-11-09T23:06:50Z",
        "updatedAt" : "2018-11-09T23:21:42Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "c67ef3f7-1cbe-4dcd-90ee-24492aa686ec",
        "parentId" : "457d58e0-9a0b-49f7-bd8e-b0f27e9813e9",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Done.",
        "createdAt" : "2018-11-09T23:21:58Z",
        "updatedAt" : "2018-11-09T23:21:58Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "a2c669d8-966d-47e0-bdbc-cb08b378676f",
        "parentId" : "457d58e0-9a0b-49f7-bd8e-b0f27e9813e9",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "No, we cannot. This is inside \"ParallelizeUntil\". This runs many times.",
        "createdAt" : "2018-11-09T23:25:50Z",
        "updatedAt" : "2018-11-09T23:25:50Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "09eeade1-b5cd-4684-bef5-1cd45edf8c07",
        "parentId" : "457d58e0-9a0b-49f7-bd8e-b0f27e9813e9",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "NVM, this ParallelizeUntil runs for each priorityConfig, not for each node. Confused it with the next one.",
        "createdAt" : "2018-11-09T23:33:39Z",
        "updatedAt" : "2018-11-09T23:33:39Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "3dc4e380-9042-4d14-854d-7a029c68df83",
        "parentId" : "457d58e0-9a0b-49f7-bd8e-b0f27e9813e9",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "But for a particular `i` (the `i` here is for each `priorityConfig`, not `node`), it only runs once, isn't it?\r\n\r\nUnlike the second `ParallelizeUntil` below, it's just iterating `priorityConfigs`.",
        "createdAt" : "2018-11-09T23:33:47Z",
        "updatedAt" : "2018-11-09T23:33:47Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "386ede13-7c0b-4fd9-811e-f562d8a2ddc2",
        "parentId" : "457d58e0-9a0b-49f7-bd8e-b0f27e9813e9",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Correct. That's what I said in my second comment as well.",
        "createdAt" : "2018-11-10T02:26:51Z",
        "updatedAt" : "2018-11-10T02:26:51Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "79289dcb-3cc4-4f20-89cf-c5b26f08a4bd",
        "parentId" : "457d58e0-9a0b-49f7-bd8e-b0f27e9813e9",
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Yeap, we commented exactly at the same time :)",
        "createdAt" : "2018-11-10T02:28:40Z",
        "updatedAt" : "2018-11-10T02:28:40Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      }
    ],
    "commit" : "62c3ec969d7a59a69fe7c45039e4379096870891",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +660,664 @@\t\tif priorityConfig.Function == nil {\n\t\t\tresults[i] = make(schedulerapi.HostPriorityList, len(nodes))\n\t\t\treturn\n\t\t}\n"
  },
  {
    "id" : "ad5f0076-0c14-46a2-b862-966bc3a083a3",
    "prId" : 70274,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70274#pullrequestreview-169531170",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "618de57c-7aa4-4ded-a089-a75159c422b3",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Thanks, @zhangmingld for your PR, but this change is not equivalent to the original code. In the original code, each old-style priority function is called once for all the nodes (line 663 of the original code).\r\nThe `processNode` function at line 673 of the original code is called once per node. By moving the logic inside `processNode`, you would make the priority functions be unnecessarily called once per node.",
        "createdAt" : "2018-10-26T23:04:36Z",
        "updatedAt" : "2018-10-31T00:54:10Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "9fe48d18-863c-48db-b747-d62a63ef9110",
        "parentId" : "618de57c-7aa4-4ded-a089-a75159c422b3",
        "authorId" : "74dd05f8-9ac6-4855-8f4c-d7bb01717295",
        "body" : "Thankyou for you review~.I think i had done this, the line \"if results[i][0].Host == \"\"\", this will make sure that the priority runs once  . For example , we have node1 and node3, in the beginning,results[i] is empty ,after node1 completes, then the results[i] will have some value , When node3 starts , it will found  results[i] is not empty ,so it will not calculate again. Though node1 and node3 will run in Parallelize, but in fact ,they won`t start in the same time, so i think it will work ",
        "createdAt" : "2018-10-29T02:39:47Z",
        "updatedAt" : "2018-10-31T00:54:10Z",
        "lastEditedBy" : "74dd05f8-9ac6-4855-8f4c-d7bb01717295",
        "tags" : [
        ]
      },
      {
        "id" : "f720c2e6-3a58-4cfb-9b06-c5b9e31bd137",
        "parentId" : "618de57c-7aa4-4ded-a089-a75159c422b3",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I see what you have done now. At the very least, you should add a comment explaining this.",
        "createdAt" : "2018-10-29T21:41:54Z",
        "updatedAt" : "2018-10-31T00:54:10Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "7164967662c5e99cf3095a00b97a49a976572723",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +661,665 @@\t\tnodeInfo := nodeNameToInfo[nodes[index].Name]\n\t\tvar err error\n\t\tfor i, priorityConfig := range priorityConfigs {\n\t\t\t// DEPRECATED when ALL priorityConfigs have Map-Reduce pattern.\n\t\t\tif priorityConfigs[i].Function != nil {"
  },
  {
    "id" : "fdfb3252-c0db-4cc2-869d-da10ace22609",
    "prId" : 70274,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70274#pullrequestreview-169601527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c800dac2-534a-4f2b-8a57-1a8df1489402",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "You should check errors and append them when they occur.",
        "createdAt" : "2018-10-29T21:45:41Z",
        "updatedAt" : "2018-10-31T00:54:10Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "c6d7c424-a754-47e9-a285-dbf9228c0a87",
        "parentId" : "c800dac2-534a-4f2b-8a57-1a8df1489402",
        "authorId" : "74dd05f8-9ac6-4855-8f4c-d7bb01717295",
        "body" : "ok, i will do it later~",
        "createdAt" : "2018-10-30T03:35:29Z",
        "updatedAt" : "2018-10-31T00:54:10Z",
        "lastEditedBy" : "74dd05f8-9ac6-4855-8f4c-d7bb01717295",
        "tags" : [
        ]
      }
    ],
    "commit" : "7164967662c5e99cf3095a00b97a49a976572723",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +666,670 @@\t\t\t\t// Make sure that the old-style priority function only runs once.\n\t\t\t\tif results[i][0].Host == \"\" {\n\t\t\t\t\tresults[i], err = priorityConfig.Function(pod, nodeNameToInfo, nodes)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tappendError(err)"
  },
  {
    "id" : "f72bff75-eb12-4f35-839f-83ec8416bffa",
    "prId" : 68860,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/68860#pullrequestreview-157500827",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd59bcb4-17a4-4f13-9dc5-a00ebed0960d",
        "parentId" : null,
        "authorId" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "body" : "Ditto - maybe we can directly use `k8s.io/client-go/listers/policy/v1beta1#PodDisruptionBudgetLister`.",
        "createdAt" : "2018-09-20T06:11:10Z",
        "updatedAt" : "2018-09-26T21:22:34Z",
        "lastEditedBy" : "06cbf859-1cac-4be7-80e6-3b34dcff1812",
        "tags" : [
        ]
      },
      {
        "id" : "308e144e-c247-414b-9acb-ad559d9dee50",
        "parentId" : "bd59bcb4-17a4-4f13-9dc5-a00ebed0960d",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Same as above.",
        "createdAt" : "2018-09-20T23:18:48Z",
        "updatedAt" : "2018-09-26T21:22:34Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "c051f0d31a9ed4522d86110af14f0e15fad99004",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +108,112 @@\tvolumeBinder             *volumebinder.VolumeBinder\n\tpvcLister                corelisters.PersistentVolumeClaimLister\n\tpdbLister                algorithm.PDBLister\n\tdisablePreemption        bool\n\tpercentageOfNodesToScore int32"
  },
  {
    "id" : "d3920b07-db37-4eb9-b3ac-c1c9d4fda09d",
    "prId" : 67555,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67555#pullrequestreview-149123993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef265c84-2fd0-421d-b76e-b34bfbad2fc1",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "If we re-write this if statement as below, we can change line 377 to `filtered = make([]*v1.Node, numNodesToFind)`. This saves us some memory.\r\n\r\n```go\r\nif fits {\r\n\tlen := atomic.AddInt32(&filteredLen, 1)\r\n\tif len > numNodesToFind {\r\n\t\tcancel()\r\n\t} else {\r\n\t\tfiltered[len - 1] = g.cachedNodeInfoMap[nodeName].Node()\t\t\r\n\t}\r\n}\r\n```",
        "createdAt" : "2018-08-23T18:07:32Z",
        "updatedAt" : "2018-09-04T06:08:59Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "52bf0240-7a51-4224-b0f6-edb78f458cea",
        "parentId" : "ef265c84-2fd0-421d-b76e-b34bfbad2fc1",
        "authorId" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "body" : "DONE.",
        "createdAt" : "2018-08-23T22:43:39Z",
        "updatedAt" : "2018-09-04T06:08:59Z",
        "lastEditedBy" : "33ab9fbe-6f55-45c0-a58d-be01aec201d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c63dcfffebb9a8bcc5e1cee748ad16d7ed7e293",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +416,420 @@\t\t\t\treturn\n\t\t\t}\n\t\t\tif fits {\n\t\t\t\tlength := atomic.AddInt32(&filteredLen, 1)\n\t\t\t\tif length > numNodesToFind {"
  },
  {
    "id" : "433875be-8c7c-43b4-bf2b-eae6f8fb6af7",
    "prId" : 66733,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66733#pullrequestreview-141324031",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecdcaa8d-7fae-4b3a-839b-19e11eb6caae",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "L408: should we find node by `nodeName` instead of `nodes[i]`.",
        "createdAt" : "2018-07-28T11:25:39Z",
        "updatedAt" : "2018-08-17T18:19:03Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "79924277-ae8e-46bf-a5e5-bac3aeaea381",
        "parentId" : "ecdcaa8d-7fae-4b3a-839b-19e11eb6caae",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "YES!",
        "createdAt" : "2018-07-28T18:23:58Z",
        "updatedAt" : "2018-08-17T18:19:03Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "2860743c869f62d448a83469b7748c9758caca7c",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +391,395 @@\t\tcheckNode := func(i int) {\n\t\t\tvar nodeCache *equivalence.NodeCache\n\t\t\tnodeName := g.cache.NodeTree().Next()\n\t\t\tif g.equivalenceCache != nil {\n\t\t\t\tnodeCache, _ = g.equivalenceCache.GetNodeCache(nodeName)"
  },
  {
    "id" : "901b74b7-eedc-45e6-ae3b-47a12fb888c0",
    "prId" : 66291,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66291#pullrequestreview-140025780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ec40c22-78e1-4a1b-b04f-c395c03a28ae",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Here's a case in my mind: `ext_1` interests `pod_1`, `ext_2` interests `pod_2`, both pods can preempt `pod_3`, `pod_4`, `pod_5` (priorities in order); if `pod_3` and `pod_5` on the same pod, scheduler may preempt `pod_3` and `pod_5` because of race condition, as the extender did not know pod's new status until get event from apiserver.\r\n\r\nDid we discuss this case when introducing preemption extender?",
        "createdAt" : "2018-07-24T01:25:17Z",
        "updatedAt" : "2018-07-24T01:25:17Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "971810fc-be51-4042-906c-13eb51e0e713",
        "parentId" : "4ec40c22-78e1-4a1b-b04f-c395c03a28ae",
        "authorId" : "9fcc8a42-459b-468d-8eeb-60a38e26971d",
        "body" : "In \"both **pods** can preempt pod_3, pod_4, pod_5 (priorities in order); if pod_3 and pod_5 on the same **pod**\", what do you mean pods or pod here?",
        "createdAt" : "2018-07-24T02:59:20Z",
        "updatedAt" : "2018-07-24T02:59:20Z",
        "lastEditedBy" : "9fcc8a42-459b-468d-8eeb-60a38e26971d",
        "tags" : [
        ]
      },
      {
        "id" : "8ee6e591-07cb-4a9c-af29-7471887a5c27",
        "parentId" : "4ec40c22-78e1-4a1b-b04f-c395c03a28ae",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Note that dup or re-preemption in this case is tolerable by system.\r\n\r\nWe did talk about information sync in extender side, for now, we left it for extender to handle as it will not break existing logic.",
        "createdAt" : "2018-07-24T03:38:18Z",
        "updatedAt" : "2018-07-24T07:38:58Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "c5cc49ae-e7dc-4be1-a663-54fff39ff429",
        "parentId" : "4ec40c22-78e1-4a1b-b04f-c395c03a28ae",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "> we left it for extender to handle as it will not break existing logic.\r\n\r\nExtender need to get Pod's latest status to \"handle\" it (e.g. nominatedNode and deleteTimestamp from apiserver), we can not make sure extender get the latest status when we schedule next pod. That's similar case that we introduced `Bind`. Do we know who's the user of preemption extender? I'm ok to keep current behaviour if no user or the user accept that; as I'd like to keep it simple enough.",
        "createdAt" : "2018-07-24T07:14:34Z",
        "updatedAt" : "2018-07-24T07:14:34Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "32e85177-2493-42b0-88ad-36e23a0bbe01",
        "parentId" : "4ec40c22-78e1-4a1b-b04f-c395c03a28ae",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "> In \"both pods can preempt pod_3, pod_4, pod_5 (priorities in order); if pod_3 and pod_5 on the same pod\", what do you mean pods or pod here?\r\n\r\n@xiaoxubeii , **pods** is pod_1 and pod_2; **pod** should be **node**, typo :)",
        "createdAt" : "2018-07-24T07:15:33Z",
        "updatedAt" : "2018-07-24T07:15:33Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "72e07989-ecb1-4166-ab44-021fb1ebba14",
        "parentId" : "4ec40c22-78e1-4a1b-b04f-c395c03a28ae",
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "cc @ravigadde for user input",
        "createdAt" : "2018-07-24T07:38:30Z",
        "updatedAt" : "2018-07-24T07:38:30Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "bec59758-d9d7-4eb7-a3d0-37dc4bb952a9",
        "parentId" : "4ec40c22-78e1-4a1b-b04f-c395c03a28ae",
        "authorId" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "body" : "@resouer My experience has been with one extender only. Diff seems reasonable.",
        "createdAt" : "2018-07-24T18:38:36Z",
        "updatedAt" : "2018-07-24T18:38:36Z",
        "lastEditedBy" : "367dd7b1-86fa-48f7-aa20-489e5d4b6a8d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d644162a294517b29690edf8a5a0da14929ce4b4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +283,287 @@\tif len(nodeToVictims) > 0 {\n\t\tfor _, extender := range g.extenders {\n\t\t\tif extender.SupportsPreemption() && extender.IsInterested(pod) {\n\t\t\t\tnewNodeToVictims, err := extender.ProcessPreemption(\n\t\t\t\t\tpod,"
  },
  {
    "id" : "178c84b9-81e4-48f1-b5e9-168045ea4a8b",
    "prId" : 65396,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65396#pullrequestreview-131736981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b55bb5ba-81d3-4ba4-a8ae-27fc29758dec",
        "parentId" : null,
        "authorId" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "body" : "Any reason this returns []int instead of []HostPriority?",
        "createdAt" : "2018-06-25T18:48:07Z",
        "updatedAt" : "2018-06-25T18:48:38Z",
        "lastEditedBy" : "5f2c1de8-4266-42c0-b343-ba247af3578f",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffc8cc2f5054d9748db1f446cf827d56e518fc49",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +177,181 @@\n// findMaxScores returns the indexes of nodes in the \"priorityList\" that has the highest \"Score\".\nfunc findMaxScores(priorityList schedulerapi.HostPriorityList) []int {\n\tmaxScoreIndexes := make([]int, 0, len(priorityList)/2)\n\tmaxScore := priorityList[0].Score"
  },
  {
    "id" : "1c9af895-377f-48c7-b1ac-b6374e65d067",
    "prId" : 63178,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63178#pullrequestreview-116085155",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f727857-e735-470f-ad4c-e441108fb3f6",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Looks much cleaner!",
        "createdAt" : "2018-04-27T21:13:03Z",
        "updatedAt" : "2018-04-27T22:58:31Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "79d30b1ad623c4e71b8d9ed33f4b18cc353908c7",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +509,513 @@\t\t\t//TODO (yastij) : compute average predicate restrictiveness to export it as Prometheus metric\n\t\t\tif predicate, exist := predicateFuncs[predicateKey]; exist {\n\t\t\t\tif eCacheAvailable {\n\t\t\t\t\tfit, reasons, err = ecache.RunPredicate(predicate, predicateKey, pod, metaToUse, nodeInfoToUse, equivCacheInfo, cache)\n\t\t\t\t} else {"
  },
  {
    "id" : "4e7cfa1e-4d81-4040-8db9-962b3f5ebb26",
    "prId" : 63040,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63040#pullrequestreview-115008230",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d68e80e-75f1-4dbe-be51-cce89772c057",
        "parentId" : null,
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Thanks! Just took a first round of review, I think this is consistent with the previous https://github.com/kubernetes/kubernetes/issues/61512#issuecomment-377347765 here which in my opinion, should solve the problem. \r\n\r\nAllow me to check more edge cases tomorrow.",
        "createdAt" : "2018-04-24T06:16:03Z",
        "updatedAt" : "2018-04-25T17:19:00Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "134f6863-502f-402d-8b36-7707ed8ea591",
        "parentId" : "9d68e80e-75f1-4dbe-be51-cce89772c057",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Yes. Luckily we already had a generation number which seemed reasonable to use.",
        "createdAt" : "2018-04-25T00:25:57Z",
        "updatedAt" : "2018-04-25T17:19:00Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "dacc1a8d52319cf380d15312bd44c73f54f6e9a9",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +542,546 @@\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t// Skip update if NodeInfo is stale.\n\t\t\t\t\t\t\tif cache != nil && cache.IsUpToDate(info) {\n\t\t\t\t\t\t\t\tresult := predicateResults[predicateKey]\n\t\t\t\t\t\t\t\tecache.UpdateCachedPredicateItem("
  },
  {
    "id" : "bea7e600-77a3-4adc-96e0-f303ef952ca9",
    "prId" : 61621,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/61621#pullrequestreview-106688205",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e86da0b-09b7-49f6-879d-64c442f8529f",
        "parentId" : null,
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "should use inline func to fix the previous deadlock issue here",
        "createdAt" : "2018-03-24T03:33:17Z",
        "updatedAt" : "2018-03-24T03:33:18Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4386751b5dcefdc49d4d48801bbd834b6d46afe8",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +505,509 @@\t\t\t\t}\n\n\t\t\t\tif !eCacheAvailable || invalid {\n\t\t\t\t\t// we need to execute predicate functions since equivalence cache does not work\n\t\t\t\t\tfit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse)"
  },
  {
    "id" : "b089a7e5-c414-4835-abf7-6cac5301110a",
    "prId" : 60263,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60263#pullrequestreview-98820425",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "006868b8-d79e-41b4-8d88-cd1e8ba4e113",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Please add a comment and mention that \"minNodes1\" and \"minNodes2\" are being reused in this function to save on memory allocation and garbage collection time.",
        "createdAt" : "2018-02-23T06:12:58Z",
        "updatedAt" : "2018-02-23T06:16:57Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "96b14bc9-c70a-41af-acb1-c090855f656a",
        "parentId" : "006868b8-d79e-41b4-8d88-cd1e8ba4e113",
        "authorId" : "cc82c1d0-5e27-49b3-ac12-1c634e1cda66",
        "body" : "Added, thanks for your review",
        "createdAt" : "2018-02-23T06:18:09Z",
        "updatedAt" : "2018-02-23T06:18:38Z",
        "lastEditedBy" : "cc82c1d0-5e27-49b3-ac12-1c634e1cda66",
        "tags" : [
        ]
      }
    ],
    "commit" : "5a083f2038ff05bac86c2e55fdde354bf36d3f54",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +677,681 @@// 3. Ties are broken by sum of priorities of all victims.\n// 4. If there are still ties, node with the minimum number of victims is picked.\n// 5. If there are still ties, the first such node is picked (sort of randomly).\n// The 'minNodes1' and 'minNodes2' are being reused here to save the memory\n// allocation and garbage collection time."
  },
  {
    "id" : "615ddee2-3a35-4627-a999-2e980b820255",
    "prId" : 59479,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59479#pullrequestreview-95287362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b32068ba-7a90-4857-818a-0acfdd6bbd75",
        "parentId" : null,
        "authorId" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "body" : "Emm ... why not release lock just following `UpdateCachedPredicateItem()` above? \r\n\r\nAlso, I am not sure about purpose of `needLock bool`, seems like you can just move lock control out from ecache.\r\n\r\nYou may still want to use defer to ensure  `ecache.Unlock()`. Make sense?",
        "createdAt" : "2018-02-09T00:39:32Z",
        "updatedAt" : "2018-02-09T08:27:30Z",
        "lastEditedBy" : "7dd504ec-7e63-45b3-98f8-6eb1c683e9c2",
        "tags" : [
        ]
      },
      {
        "id" : "3b6e023b-cb18-46d1-899d-d3ac9d213921",
        "parentId" : "b32068ba-7a90-4857-818a-0acfdd6bbd75",
        "authorId" : "cc82c1d0-5e27-49b3-ac12-1c634e1cda66",
        "body" : ">Emm ... why not release lock just following UpdateCachedPredicateItem() above?\r\n\r\nRelease lock following `UpdateCachedPredicateItem()` may cause lock leak, because the `if !eCacheAvailable || invalid ` may be passed if `eCacheAvailable=true` and `invalid=false`.\r\n\r\n>Also, I am not sure about purpose of needLock bool, seems like you can just move lock control out from ecache.\r\n\r\nI just suppose there may be requirements in future for using ` UpdateCachedPredicateItem()` or `PredicateWithECache` with lock inside the method, which is more conveninent than lock outside.\r\n\r\n@resouer ",
        "createdAt" : "2018-02-09T00:56:56Z",
        "updatedAt" : "2018-02-09T08:27:30Z",
        "lastEditedBy" : "cc82c1d0-5e27-49b3-ac12-1c634e1cda66",
        "tags" : [
        ]
      }
    ],
    "commit" : "e155582662f80a491d84428c902a4fc78b93f345",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +498,502 @@\n\t\t\t\tif eCacheAvailable {\n\t\t\t\t\tecache.Unlock()\n\t\t\t\t}\n"
  }
]