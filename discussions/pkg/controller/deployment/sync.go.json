[
  {
    "id" : "c661ca30-07a6-42ea-8c9e-112ff61075e9",
    "prId" : 92421,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/92421#pullrequestreview-441549431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90d2a716-7c92-45a7-9cdc-beae62a6a72d",
        "parentId" : null,
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "This `d` is being used later on in code, so it looks to me this is wrong change. ",
        "createdAt" : "2020-07-01T18:55:55Z",
        "updatedAt" : "2020-07-01T18:56:09Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      },
      {
        "id" : "f0f2335f-8cd1-4a9a-bc61-319bf6a22628",
        "parentId" : "90d2a716-7c92-45a7-9cdc-beae62a6a72d",
        "authorId" : "c666776f-f3a4-4854-9679-2638da5217ce",
        "body" : "Thx your comments ,from the code \r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/da54185c9eb076a71962ddb5aded24811467dbb3/pkg/controller/deployment/sync.go#L149-L188\r\n\r\nHere, the assignment of `d` will only be executed within the condition of `if existingNewRS != nil`, and then return directly to jump out of the function. The code segment used behind `d` is not within this logical scope, and the latter `d` is passed in by the current function\r\n\r\nIt seems like this is okay to modify",
        "createdAt" : "2020-07-01T23:14:45Z",
        "updatedAt" : "2020-07-01T23:15:41Z",
        "lastEditedBy" : "c666776f-f3a4-4854-9679-2638da5217ce",
        "tags" : [
        ]
      },
      {
        "id" : "6a2083bb-4a9f-4ff3-a0f9-bc6a4e32c161",
        "parentId" : "90d2a716-7c92-45a7-9cdc-beae62a6a72d",
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "Yeah, you're right, thx.",
        "createdAt" : "2020-07-02T10:26:03Z",
        "updatedAt" : "2020-07-02T10:26:04Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d8aae51df4f318b71dc3b790dd01d0bcb79c5dd",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +174,178 @@\t\tif needsUpdate {\n\t\t\tvar err error\n\t\t\tif _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(context.TODO(), d, metav1.UpdateOptions{}); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}"
  },
  {
    "id" : "9ecdcd41-7c56-4d14-ad40-1b53c50659a9",
    "prId" : 77610,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77610#pullrequestreview-236847585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b0e0a1f-1367-484d-94d6-b43bee5dfcf1",
        "parentId" : null,
        "authorId" : "37016922-c330-4fc5-b602-08c675dca4fb",
        "body" : "Could we add a comment here on why 2000 was chosen as the limit?",
        "createdAt" : "2019-05-13T18:34:52Z",
        "updatedAt" : "2019-05-13T19:11:47Z",
        "lastEditedBy" : "37016922-c330-4fc5-b602-08c675dca4fb",
        "tags" : [
        ]
      }
    ],
    "commit" : "9418affa4df2ca15d7ba67507b74c443a8c281d3",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +127,131 @@const (\n\t// limit revision history length to 100 element (~2000 chars)\n\tmaxRevHistoryLengthInChars = 2000\n)\n"
  },
  {
    "id" : "53cb55f9-60c2-410a-a123-81c43c481059",
    "prId" : 59212,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59212#pullrequestreview-94538171",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd4af942-f158-42d9-b265-d8dd9022c656",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Add a comment for this code block ",
        "createdAt" : "2018-02-06T22:14:26Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "3512a748-d4e9-4143-94b3-ba43cf152eea",
        "parentId" : "bd4af942-f158-42d9-b265-d8dd9022c656",
        "authorId" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "body" : "That's what I did before till I realized the comment from line 321 to 324 above has explained clearly about what the code block does. ",
        "createdAt" : "2018-02-06T22:34:59Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "tags" : [
        ]
      },
      {
        "id" : "3ae387cf-3010-46bf-8b36-43a5c5614c30",
        "parentId" : "bd4af942-f158-42d9-b265-d8dd9022c656",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "Should we move down the comment from above `case errors.IsAlreadyExists(err):`? That's the one that explains why we do this.",
        "createdAt" : "2018-02-06T22:40:04Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      },
      {
        "id" : "92305813-8230-4342-b30a-7f40f0352490",
        "parentId" : "bd4af942-f158-42d9-b265-d8dd9022c656",
        "authorId" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "body" : "Yes. After inspecting the flow, the comment after `...a fast resync of the Deployment.` should be positioned within the case.",
        "createdAt" : "2018-02-06T23:01:10Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "tags" : [
        ]
      }
    ],
    "commit" : "18289cc0df9774db076d95a9fe4ed2382f702e2c",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +337,341 @@\t\t\terr = nil\n\t\t\tbreak\n\t\t}\n\n\t\t// Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus"
  },
  {
    "id" : "aa1af51e-4b0d-4196-9339-3e1c10b809be",
    "prId" : 59212,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59212#pullrequestreview-94566302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "What if `rs` is an orphan that can be adopted by the Deployment as its new RS? This could happen when the user create an RS first and then create a Deployment to adopt it. \r\n\r\nIf `rs` can be adopted by the Deployment, we should make the decision in the next control loop (after it's adopted) before bumping collision count. Otherwise this `rs` will have an unnecessary scale down. ",
        "createdAt" : "2018-02-06T22:18:11Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "be3e489f-46fb-40ea-97ff-9770719a2af6",
        "parentId" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "If the RS is an orphan that can be adopted, ControllerRefManager will adopt it on the next sync, before Deployment decides whether to create a new RS.\r\n\r\nAfter we bump the collision count, we requeue rather than immediately create a new RS. So if the RS can be adopted in the next sync, it shouldn't matter that we bumped the collision count unnecessarily.",
        "createdAt" : "2018-02-06T22:37:22Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      },
      {
        "id" : "5790f301-aca9-4d1a-a54d-eb3ef65e7bb5",
        "parentId" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "You're right. As long as we don't create new RS in this case we're fine. ",
        "createdAt" : "2018-02-06T22:50:59Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "0a140b75-b134-4aec-9203-2b76035e4d97",
        "parentId" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Worth adding a test for this case.\r\n1. Create a Deployment, wait for its new RS to be created \r\n2. Remove the Deployment and orphan the RS\r\n3. Create the Deployment again, check that the Deployment only manages one RS after the RS is adopted ",
        "createdAt" : "2018-02-06T23:15:04Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "ac9e2ec2-df68-4f47-9a6c-c53ec40b3949",
        "parentId" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "authorId" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "body" : "Yes, there should be another test for further testing against collision avoidance mechanism, which will be addressed in another PR. The PR has changed to xref the issue.",
        "createdAt" : "2018-02-06T23:24:39Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "tags" : [
        ]
      },
      {
        "id" : "31817a65-c0fe-48cc-a050-a42289319ffa",
        "parentId" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "We have an e2e test for that, and I think it makes sense there; we expect the user may intentionally delete the Deployment without cascading and recreate it, so it's not an edge case.\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/bf73cdf4e7f5e6bd81438ba84b1b60896706459a/test/e2e/apps/deployment.go#L749-L786\r\n\r\nHowever, it doesn't appear that the test ensures no extra RS was created, so that could be a good improvement to make to the e2e test (in a separate PR).",
        "createdAt" : "2018-02-06T23:34:43Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      },
      {
        "id" : "c43767f2-580b-46bb-a7dc-913206362abc",
        "parentId" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "authorId" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "body" : "Great! I will link this test to the issue, and work on it.",
        "createdAt" : "2018-02-06T23:37:25Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "a3d6d690-2601-4c58-a5bc-a3eaa025f8e0",
        "tags" : [
        ]
      },
      {
        "id" : "568924bc-621e-4d13-b7b1-1490d2689447",
        "parentId" : "0896305b-0d53-4fab-9d65-ee5de619849d",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Nice!",
        "createdAt" : "2018-02-07T01:43:03Z",
        "updatedAt" : "2018-02-07T19:29:16Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "18289cc0df9774db076d95a9fe4ed2382f702e2c",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +338,342 @@\t\t\tbreak\n\t\t}\n\n\t\t// Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus\n\t\t// and requeue the Deployment."
  },
  {
    "id" : "6a89b398-72af-46d5-9c16-09bf76f8eefc",
    "prId" : 41145,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41145#pullrequestreview-20959618",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76a1e7bc-cfbb-4ad6-b18b-44e088498464",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "this is not about logging, isn't it?",
        "createdAt" : "2017-02-09T09:10:49Z",
        "updatedAt" : "2017-02-09T09:31:35Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "ebc7f717-4d22-4a07-a8bc-e714993be9c5",
        "parentId" : "76a1e7bc-cfbb-4ad6-b18b-44e088498464",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Nope, but it seems like a possible source of the problem for the flaky test - see https://github.com/kubernetes/kubernetes/pull/41163#issuecomment-278526438",
        "createdAt" : "2017-02-09T09:14:13Z",
        "updatedAt" : "2017-02-09T09:31:35Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "6f49cb22-5dd2-4e7c-9eb0-7faf367a7a04",
        "parentId" : "76a1e7bc-cfbb-4ad6-b18b-44e088498464",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Can you move it to its own commit? And mention it in the PR. Otherwise, it looks like this only changes some logging.",
        "createdAt" : "2017-02-09T09:16:20Z",
        "updatedAt" : "2017-02-09T09:31:35Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "88aabcb6-f793-4d13-80a9-dde20c894603",
        "parentId" : "76a1e7bc-cfbb-4ad6-b18b-44e088498464",
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "Shouldn't there be accompanying unit test update to cover this case as well?",
        "createdAt" : "2017-02-09T09:20:04Z",
        "updatedAt" : "2017-02-09T09:31:35Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      }
    ],
    "commit" : "97c9e7fe07ecb66920bf1a73b8b9531dafd0f62e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +555,559 @@\t\trs := oldRSs[i]\n\t\t// Avoid delete replica set with non-zero replica counts\n\t\tif rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation > rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "ace3e814-2172-4549-aff1-859929ee35a3",
    "prId" : 41145,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41145#pullrequestreview-20966802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e044a2c-08ed-46f5-be49-3510fe43990f",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "V(2) looks like pretty low. With which level are e2e tests run?",
        "createdAt" : "2017-02-09T09:17:08Z",
        "updatedAt" : "2017-02-09T09:31:35Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "95e82863-4519-4cd6-bc2f-3c697132fb11",
        "parentId" : "5e044a2c-08ed-46f5-be49-3510fe43990f",
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "I wouldn't expect it be high :/",
        "createdAt" : "2017-02-09T09:22:37Z",
        "updatedAt" : "2017-02-09T09:31:35Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      },
      {
        "id" : "0f7e88d2-2dc7-4ed0-acda-37daab575486",
        "parentId" : "5e044a2c-08ed-46f5-be49-3510fe43990f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "From AWS I can get back V(2) - I will open an issue to bump these once I get a clean run",
        "createdAt" : "2017-02-09T09:23:51Z",
        "updatedAt" : "2017-02-09T09:31:35Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "b4e4f81a-507e-471e-8eea-495237e049c7",
        "parentId" : "5e044a2c-08ed-46f5-be49-3510fe43990f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "https://github.com/kubernetes/kubernetes/issues/41187",
        "createdAt" : "2017-02-09T09:33:14Z",
        "updatedAt" : "2017-02-09T09:33:14Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "1060a4b7-4782-4c48-af39-8294aad8fdd3",
        "parentId" : "5e044a2c-08ed-46f5-be49-3510fe43990f",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "lgtm",
        "createdAt" : "2017-02-09T09:47:25Z",
        "updatedAt" : "2017-02-09T09:47:25Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "97c9e7fe07ecb66920bf1a73b8b9531dafd0f62e",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +558,562 @@\t\t\tcontinue\n\t\t}\n\t\tglog.V(2).Infof(\"Trying to cleanup replica set %q for deployment %q\", rs.Name, deployment.Name)\n\t\tif err := dc.client.Extensions().ReplicaSets(rs.Namespace).Delete(rs.Name, nil); err != nil && !errors.IsNotFound(err) {\n\t\t\tglog.V(2).Infof(\"Failed deleting old replica set %v for deployment %v: %v\", rs.Name, deployment.Name, err)"
  },
  {
    "id" : "c65fd66d-f901-4f96-8919-0753a76477e1",
    "prId" : 41145,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41145#pullrequestreview-21081353",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3dc01b7-928e-4dc3-a28d-70e66b536a0c",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Shouldn't we do this before the for loop so that we get accurate `diff`? ",
        "createdAt" : "2017-02-09T18:23:13Z",
        "updatedAt" : "2017-02-09T18:23:13Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "97c9e7fe07ecb66920bf1a73b8b9531dafd0f62e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +555,559 @@\t\trs := oldRSs[i]\n\t\t// Avoid delete replica set with non-zero replica counts\n\t\tif rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation > rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "fbcefa33-ad79-43c1-aa85-a0f69be02143",
    "prId" : 35691,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35691#pullrequestreview-6936121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74b45634-51ca-499a-8c59-a43f79e39120",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "How many extra deployment status writes does this PR add (on average) to a deployment creation and a deployment update?  I.e. for scenario:\n1. Create a new deployment -> deployment reaches success\n2. Modify deployment -> new code is rolled out\n\nWhat is before and after REST calls to D and RS look like?\n",
        "createdAt" : "2016-10-31T18:22:10Z",
        "updatedAt" : "2016-11-04T15:29:51Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "4e05b78d-1942-46ee-ad67-bab2e63bba11",
        "parentId" : "74b45634-51ca-499a-8c59-a43f79e39120",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "This really depends on the size and the fenceposts of the deployment. The fastest a rollout can advance ((spec.replicas+maxSurge - (spec.replicas-maxUnavailable)) is bigger) and the less the pods under a deployment, the less the writes will be.\n\nDeployment with one pod:\nI don't notice any difference\n\nDeployment with 3 pods:\n~7-8 writes in master\n~10 writes with progressDeadline set\n\nDeployment with 10 pods:\n~40 writes in master\n~50 writes with progressDeadline set\n\nThis PR adds no additional RS writes.\n",
        "createdAt" : "2016-11-01T15:00:55Z",
        "updatedAt" : "2016-11-04T15:29:51Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "3028dcf4-afa6-474c-aa1f-3fdbc4c75ded",
        "parentId" : "74b45634-51ca-499a-8c59-a43f79e39120",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Ok\n",
        "createdAt" : "2016-11-03T00:18:10Z",
        "updatedAt" : "2016-11-04T15:29:51Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "f52ea8fc6788c606352eb133df0e771f583b2cee",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +309,313 @@\t\t\tmsg := fmt.Sprintf(\"Found new replica set %q\", rsCopy.Name)\n\t\t\tcondition := deploymentutil.NewDeploymentCondition(extensions.DeploymentProgressing, api.ConditionTrue, deploymentutil.FoundNewRSReason, msg)\n\t\t\tdeploymentutil.SetDeploymentCondition(&deployment.Status, *condition)\n\t\t\tupdateConditions = true\n\t\t}"
  },
  {
    "id" : "29959072-7efe-459b-9f29-287d11bd358d",
    "prId" : 34548,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34548#pullrequestreview-4269729",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd830285-bf41-41a4-8d66-5aa58072d89b",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "@kargakis is this retried in case of conflict?\n",
        "createdAt" : "2016-10-13T12:28:41Z",
        "updatedAt" : "2016-10-13T12:28:41Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "13624a45-742f-4154-9e55-306bd9f18729",
        "parentId" : "bd830285-bf41-41a4-8d66-5aa58072d89b",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "The deployment will be requeued for any kind of error. Do we want an issue to retry on conflicts without requeueing?\n",
        "createdAt" : "2016-10-13T12:30:39Z",
        "updatedAt" : "2016-10-13T12:30:39Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "66011920-e774-4da6-aa4d-014544ed81c7",
        "parentId" : "bd830285-bf41-41a4-8d66-5aa58072d89b",
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "@kargakis yes\n",
        "createdAt" : "2016-10-14T11:26:45Z",
        "updatedAt" : "2016-10-14T11:26:45Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "248ba0ba-a9d8-4db0-8309-0f5f529fdd87",
        "parentId" : "bd830285-bf41-41a4-8d66-5aa58072d89b",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "There is already one related: https://github.com/kubernetes/kubernetes/issues/21479\n",
        "createdAt" : "2016-10-14T13:42:50Z",
        "updatedAt" : "2016-10-14T13:42:50Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "212a26dc95a5fbe314f25a40a982742bc9b2a0c1",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +264,268 @@\t\tif annotationsUpdated || minReadySecondsNeedsUpdate {\n\t\t\trsCopy.Spec.MinReadySeconds = deployment.Spec.MinReadySeconds\n\t\t\treturn dc.client.Extensions().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(rsCopy)\n\t\t}\n"
  },
  {
    "id" : "597a983b-5ed7-4145-846b-5b9a8832da8a",
    "prId" : 28162,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98aee383-4b84-4ad6-b23f-7ab8d4c2fd8f",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Retry the update on conflict here?\n",
        "createdAt" : "2016-07-13T22:17:46Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "83dc2580-dd3e-4aa6-a90f-0ee276f8247c",
        "parentId" : "98aee383-4b84-4ad6-b23f-7ab8d4c2fd8f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "We are requeueing the deployment on error so this will be retried anyway.\n",
        "createdAt" : "2016-07-14T08:15:05Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "2b664751-efc8-4b20-b013-d0f2820f53b2",
        "parentId" : "98aee383-4b84-4ad6-b23f-7ab8d4c2fd8f",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Will the result of this update be seen in the `dc.rsStore.ReplicaSets` call that is made in _rsAndPodsWithHashKeySynced_ after this?  What guarantees exist around what updates the dc.rsStore will see?  Will there be an optimistic locking exception if the cache isn't updated and we try to update a RS we read from it?\n",
        "createdAt" : "2016-07-14T18:21:26Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "72c199b8-083b-4007-bdb4-5ca397d5f049",
        "parentId" : "98aee383-4b84-4ad6-b23f-7ab8d4c2fd8f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "If we try to update a stale replica set, we will get an update confict and `rsAndPodsWithHashKeySynced` will retry by pulling the latest replica set from the server. Eventually, we should consolidate all updates we do for syncing replica sets (desiredReplicas, podTemplateHash, revision) in a single update.\n",
        "createdAt" : "2016-07-15T08:28:08Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "97b9c73bf1ef6d8c94e1b36c8fa665520ba84eb5",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +524,528 @@\t\tfor _, rs := range controller.FilterActiveReplicaSets(oldRSs) {\n\t\t\tif updated := deploymentutil.SetReplicasAnnotations(rs, d.Spec.Replicas, d.Spec.Replicas+maxSurge); updated {\n\t\t\t\tif _, err := dc.client.Extensions().ReplicaSets(rs.Namespace).Update(rs); err != nil {\n\t\t\t\t\tglog.Infof(\"Cannot update annotations for replica set %q: %v\", rs.Name, err)\n\t\t\t\t\treturn false, err"
  },
  {
    "id" : "ee649ae9-8e27-4dbf-aba5-306ad602bbdd",
    "prId" : 28162,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31235f96-a114-4fc6-b225-61ff22961367",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "It'd be more robust if we don't calculate desiredReplicas & maxReplicas here but inside `SetReplicasAnnotations` based on deployment spec. WDYT?\n",
        "createdAt" : "2016-07-13T22:19:45Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "d72d586d-c2e0-47ee-a3df-542f3d10dee4",
        "parentId" : "31235f96-a114-4fc6-b225-61ff22961367",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Wouldn't make a difference.\n",
        "createdAt" : "2016-07-14T08:14:03Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "97b9c73bf1ef6d8c94e1b36c8fa665520ba84eb5",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +523,527 @@\t\tmaxSurge := deploymentutil.MaxSurge(*d)\n\t\tfor _, rs := range controller.FilterActiveReplicaSets(oldRSs) {\n\t\t\tif updated := deploymentutil.SetReplicasAnnotations(rs, d.Spec.Replicas, d.Spec.Replicas+maxSurge); updated {\n\t\t\t\tif _, err := dc.client.Extensions().ReplicaSets(rs.Namespace).Update(rs); err != nil {\n\t\t\t\t\tglog.Infof(\"Cannot update annotations for replica set %q: %v\", rs.Name, err)"
  },
  {
    "id" : "b8c605d2-89ca-49c0-b031-fa281cbcee42",
    "prId" : 28162,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Would it also address the issue if we simply checked for scaling first, and then scaled the existing RS before performing the rollout to the new image?  It isn't clear what the correct behavior here is: whether we scale the existing RS and then do a rollout to the new image, or whether we only scale the new image.\n",
        "createdAt" : "2016-07-15T18:41:51Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "fc70c15b-b9f8-4428-acaf-2ceee1ca8940",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "If there is a template change then we need to rollout a new replica set. I don't think we want to do anything (scaling included) with the old replica set beforehand. Simple example: I have a router which uses host ports and the deployment's size equals the nodes in the cluster. I change my router not to use host ports anymore and I scale it up at the same time. If we try to scale before we rollout the new template then we simply won't be able to do so. Eventually I think we will rollout successfully but running scaling before rolling out is not optimal.\n",
        "createdAt" : "2016-07-15T20:33:45Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "24881fb2-fd4c-4c4c-be6a-007d08232164",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What happens if we scale down when rolling out the new RS?\n",
        "createdAt" : "2016-07-15T23:57:17Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "9557f588-9788-44ba-9200-a5c78ff47f55",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "New rs is created, it cannot scale up because the deployment has to scale down, old rs is scaled down, new rs is scaled up?\n",
        "createdAt" : "2016-07-16T10:22:30Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "809f00b7-c4a5-40cf-8f0a-20d4d2e7171a",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Thought about this some more over the weekend:\n\n> New rs is created, it cannot scale up because the deployment has to scale down, old rs is scaled down, new rs is scaled up?\n\nIf I am reading the code correctly, in this case the scaling down will happen in the rollingrollout could, not the `scale` code because a \"scaling event\" will never be seen as the labels have all been updated.  The scaling will then happen as part of the rollout which behaves differently.\n\n> Simple example: I have a router which uses host ports and the deployment's size equals the nodes in the cluster. I change my router not to use host ports anymore and I scale it up at the same time. If we try to scale before we rollout the new template then we simply won't be able to do so. Eventually I think we will rollout successfully but running scaling before rolling out is not optimal.\n\nThis same problem exists for the proportional scaling as well right?  Create a new RS, then immediately scale it.  Do you agree that for the following 3 scenarios should behave similarly?  Your current implementation makes C) behave differently than A) and B).\n- A) Update image, then Scale\n  1. Create RS\n  2. Scale\n- B) Scale, then update image\n  1. Scale\n  2. Create RS\n- C) Scale and update image at the same time\n  1. Create RS + Scale\n",
        "createdAt" : "2016-07-18T16:54:07Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "6e93a605-0c27-4787-aed1-8c86a803a49b",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "A) and B) are different from C) because they involve two distinct updates\nas opposed to a single update that happens in C) so it seems natural to let\nscaling be handled as part of the rollout (you requested both at the same\ntime).\n\nOn Mon, Jul 18, 2016 at 6:54 PM, Phillip Wittrock notifications@github.com\nwrote:\n\n> In pkg/controller/deployment/sync.go\n> https://github.com/kubernetes/kubernetes/pull/28162#discussion_r71184893\n> :\n> \n> > ```\n> > newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, false)\n> > if err != nil {\n> > ```\n> > -       return false\n> > -       return false, err\n> >   }\n> >   // If there is no new replica set matching this deployment and the deployment isn't paused\n> >   // then there is a new rollout that waits to happen\n> >   if newRS == nil && !d.Spec.Paused {\n> \n> Thought about this some more over the weekend:\n> \n> New rs is created, it cannot scale up because the deployment has to scale\n> down, old rs is scaled down, new rs is scaled up?\n> \n> If I am reading the code correctly, in this case the scaling down will\n> happen in the rollingrollout could, not the scale code because a \"scaling\n> event\" will never be seen as the labels have all been updated. The scaling\n> will then happen as part of the rollout which behaves differently.\n> \n> Simple example: I have a router which uses host ports and the deployment's\n> size equals the nodes in the cluster. I change my router not to use host\n> ports anymore and I scale it up at the same time. If we try to scale before\n> we rollout the new template then we simply won't be able to do so.\n> Eventually I think we will rollout successfully but running scaling before\n> rolling out is not optimal.\n> \n> This same problem exists for the proportional scaling as well right?\n> Create a new RS, then immediately scale it. Do you agree that for the\n> following 3 scenarios should behave similarly? Your current implementation\n> makes C) behave differently than A) and B).\n> - A) Update image, then Scale\n>   1. Create RS\n>   2. Scale\n> - B) Scale, then update image\n>   1. Scale\n>   2. Create RS\n> - C) Scale and update image at the same time\n>   1. Create RS + Scale\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/28162/files/b68dbfbffc725587ddfa28e82fa556f1b2deeaae#r71184893,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADuFf0VFGwu18X0OhxdEAm089AuAaTN1ks5qW6_LgaJpZM4JAOWg\n> .\n",
        "createdAt" : "2016-07-19T08:15:17Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "1a81f8ec-b081-4cbf-80b8-d0f570dc21dc",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "@smarterclayton @ironcladlou @mfojtik any opinions on what should happen in (C) above?\n",
        "createdAt" : "2016-07-19T08:17:34Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "d4de1f50-886b-44aa-a5eb-02ae867a4b1e",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What are the guarantees about which updates we will see in the controller sync loop in a given read?  Could the controller sync loop see C) when actually A) or B) happened?\n",
        "createdAt" : "2016-07-19T18:09:50Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "0366df56-aff3-41f6-b2a7-ce0c171b21b2",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "> What are the guarantees about which updates we will see in the controller sync loop in a given read? Could the controller sync loop see C) when actually A) or B) happened?\n\nThis depends on the queue compressing events but I don't think the current impl being used is doing that. @lavalamp and @deads2k will know better.\n",
        "createdAt" : "2016-07-19T19:52:18Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "6f38cd65-4eae-498e-8b6d-2d57437337e9",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> This depends on the queue compressing events but I don't think the current impl being used is doing that. @lavalamp and @deads2k will know better.\n\nControllers may not count on seeing each individual update.  For instance, anything backed by a workqueue has a level of compression already built in.\n\nIn addition to that, you always have the case of \"my controller was off when these happened\", in which case you always see an `Add` event that contains the current state of the object.\n",
        "createdAt" : "2016-07-19T20:22:14Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "418f3b13-d443-4967-96eb-c8f833d74f7c",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "@pwittrock then in case of compression we cannot be sure if (C) is (A) or (B). I think it still makes sense to let the rollout process handle the scaling part in such a case instead of picking (A) or (B). We may want to document this behavior.\n",
        "createdAt" : "2016-07-20T08:33:57Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "4a1b25bc-0f64-4555-b2a2-64803eed9303",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "@bgrant0607 Would you be able to provide some technical direction here?\n\n@kargakis If we want to support the use case you suggested, I think we need a scaling strategy that allows the user to configure which RS will be scaled.  The use case you suggested is broken by proportional scaling generally.  Supporting it when you scale+update in 1 update, but not when you update then scale in 2 updates does not seem intuitive to me.  We could allow the user to specify options: PROPORTIONAL & LATEST.  PROPORTIONAL would always scale RS based on their % of the total replicas, and LATEST would always scale the newest RS.\n\nScaling the newest RS sometimes (when we observe that we need to create a new RS and scale during the same control loop) and proportionally scaling all the RS other times (when we observe only that we need to scale during the control loop) seems inconsistent to me and the behavior is non-deterministic from the perspective of the user - most of the time it will do what they expect, but occasionally it may do something different.\n\nWhat the user really desires will likely vary from case to base, but I think we should be as deterministic as possible with how we perform scaling.\n\nAll that said... the behavior of HEAD is broken right now.  This doesn't just impact scaling + rollout in the same update, it will break any time a scale and update are seen in the control loop even if they happened separately.  I want to get this fixed quickly, and the PR as written is at least functional.  If we commit this PR would you be willing to address the concerns in an immediate follow up?\n",
        "createdAt" : "2016-07-20T17:06:22Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "023ad9eb-a056-4765-996d-4eece2c982ff",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "I didn't read the whole thread, but my opinion is that we should not try to distinguish A, B, and C. Scaling should be treated as orthogonal to the rollout.\n",
        "createdAt" : "2016-07-20T17:44:37Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "9c025b65-1e80-4ae4-a173-618d460c0578",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Also, controllers should be \"level-based\". They operate on current and desired state. They are not \"edge-based\" and cannot assume that they observe intermediate events.\n",
        "createdAt" : "2016-07-20T17:45:37Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "0b965253-f7a3-4694-a20b-f2bc9f9609ab",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Watch and informers should be considered a performance optimization of polling. The mechanism is not equivalent to a message queue.\n",
        "createdAt" : "2016-07-20T17:46:41Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "52c4de12-d423-4046-9bf6-8f6295523a59",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "@bgrant0607 agreed, thanks for the explanation.\n\n@pwittrock scaling+rolling out at the same time or in two separate quick updates that cannot be distinguished is not a common case I believe (unless what users really want is a different strategy) and even if we exposed proportional scaling as an API I don't see why it would be for Rolling deployments and not a new strategy (sounds a lot like AB deployments which is a variation of Rolling - I have been meaning to write a proposal for an AB strategy but haven't found any time yet). In my opinion, for now (and for Rolling deployments in general) the only thing we can do is document the behavior. \n",
        "createdAt" : "2016-07-20T20:17:02Z",
        "updatedAt" : "2016-07-20T22:42:34Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "7535b195-09de-4907-9ee2-aad0e3e71905",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "I wasn't suggesting a different scaling api, I was thinking of a field on deployments that specified the scaling strategy similar to the field for the rollout strategy.\n",
        "createdAt" : "2016-07-21T00:18:42Z",
        "updatedAt" : "2016-07-21T00:18:42Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "b16bc659-8df1-490e-b9be-da9fe521d3ae",
        "parentId" : "160244e8-3d9b-40b5-8458-0a5145df9039",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "> I wasn't suggesting a different scaling api\n\nI didn't imply that. New field for deployments=api\n",
        "createdAt" : "2016-07-21T00:31:47Z",
        "updatedAt" : "2016-07-21T00:31:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "97b9c73bf1ef6d8c94e1b36c8fa665520ba84eb5",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +518,522 @@\t// If there is no new replica set matching this deployment and the deployment isn't paused\n\t// then there is a new rollout that waits to happen\n\tif newRS == nil && !d.Spec.Paused {\n\t\t// Update all active replicas sets to the new deployment size. SetReplicasAnnotations makes\n\t\t// sure that we will update only replica sets that don't have the current size of the deployment."
  },
  {
    "id" : "fb1dfba0-dc82-4925-b5cc-e4e15ee27853",
    "prId" : 19343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87c1a5a9-e3d1-48fd-88e2-71815bc86bea",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "don't you need to deep copy this?\n",
        "createdAt" : "2016-08-25T09:11:33Z",
        "updatedAt" : "2016-10-27T10:57:54Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "e1f00c07-2839-4aa8-add7-c954e0f477cc",
        "parentId" : "87c1a5a9-e3d1-48fd-88e2-71815bc86bea",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "We already deep-copy in `syncDeployment`.\n",
        "createdAt" : "2016-08-25T09:16:02Z",
        "updatedAt" : "2016-10-27T10:57:54Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf9fd31d00c4f01a10cac490226bbd0659d6b483",
    "line" : null,
    "diffHunk" : "@@ -1,1 +487,491 @@\t}\n\n\tnewDeployment := d\n\tnewDeployment.Status = newStatus\n\t_, err := dc.client.Extensions().Deployments(newDeployment.Namespace).UpdateStatus(newDeployment)"
  },
  {
    "id" : "84bfda19-7d19-425e-b1de-c91aa928d24c",
    "prId" : 19343,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ca6ee72-9c96-40ed-9cb7-941d6d75c5bb",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "guess we should deep copy this, right?\n",
        "createdAt" : "2016-09-02T13:51:16Z",
        "updatedAt" : "2016-10-27T10:57:54Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "914fdd2d-4306-47c1-bcd8-565a067eddbd",
        "parentId" : "0ca6ee72-9c96-40ed-9cb7-941d6d75c5bb",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Yes, part of a different issue though: https://github.com/kubernetes/kubernetes/issues/31780\n",
        "createdAt" : "2016-09-02T13:53:01Z",
        "updatedAt" : "2016-10-27T10:57:54Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "e9aef98d-ffab-44b8-934b-f7f817d4023d",
        "parentId" : "0ca6ee72-9c96-40ed-9cb7-941d6d75c5bb",
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "@kargakis just making a placeholder for future me\n",
        "createdAt" : "2016-09-02T14:00:08Z",
        "updatedAt" : "2016-10-27T10:57:54Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "b402fbce-4cd9-484c-817a-a042b988541c",
        "parentId" : "0ca6ee72-9c96-40ed-9cb7-941d6d75c5bb",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Before we start deep-copying replica sets, we should consolidate all annotation updates possible (revision, replicas annotations) into one.\n",
        "createdAt" : "2016-09-02T14:09:44Z",
        "updatedAt" : "2016-10-27T10:57:54Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf9fd31d00c4f01a10cac490226bbd0659d6b483",
    "line" : null,
    "diffHunk" : "@@ -1,1 +238,242 @@// Note that the pod-template-hash will be added to adopted RSes and pods.\nfunc (dc *DeploymentController) getNewReplicaSet(deployment *extensions.Deployment, rsList, oldRSs []*extensions.ReplicaSet, createIfNotExisted bool) (*extensions.ReplicaSet, error) {\n\texistingNewRS, err := deploymentutil.FindNewReplicaSet(deployment, rsList)\n\tif err != nil {\n\t\treturn nil, err"
  }
]