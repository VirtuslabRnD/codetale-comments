[
  {
    "id" : "ef36943a-9cf1-4a49-9736-7227c4f3fe85",
    "prId" : 97507,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97507#pullrequestreview-603331324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22a524b0-c3f3-43be-9b53-9d1d6492b01d",
        "parentId" : null,
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "I don't understand the above changes, there are no variables here so I'd leave it as is. ",
        "createdAt" : "2021-02-25T20:08:04Z",
        "updatedAt" : "2021-02-25T20:11:18Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      },
      {
        "id" : "601c63e2-85f8-442a-961f-d7219ce39292",
        "parentId" : "22a524b0-c3f3-43be-9b53-9d1d6492b01d",
        "authorId" : "65f57cac-91da-4111-86be-4242058079d2",
        "body" : "We don't keep it in a variable, but \"deployment\" is a controller name, isn't it?\r\n\r\nIf it was a pod, we would convert something like `\"Starting pod ABC\"` to `\"Starting pod\", pod=\"ABC\"`.\r\nI apply the same logic to the controller name. \r\n\r\nAlso, If we don't have any keys, the only way to find when did the deployment controller start is to use full text search and look for \"Starting deployment  controller\". While specifying `controller=deployment` in a log viewer gives us a quick way to get this information.",
        "createdAt" : "2021-03-03T03:54:28Z",
        "updatedAt" : "2021-03-03T03:54:28Z",
        "lastEditedBy" : "65f57cac-91da-4111-86be-4242058079d2",
        "tags" : [
        ]
      },
      {
        "id" : "925f5cc7-ff9e-4cc8-8ab3-244d0da46b94",
        "parentId" : "22a524b0-c3f3-43be-9b53-9d1d6492b01d",
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "That's fair. ",
        "createdAt" : "2021-03-03T20:47:32Z",
        "updatedAt" : "2021-03-03T20:47:33Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      }
    ],
    "commit" : "dcdde707ab41433774cc80c293998e4ae7125254",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +152,156 @@\n\tklog.InfoS(\"Starting controller\", \"controller\", \"deployment\")\n\tdefer klog.InfoS(\"Shutting down controller\", \"controller\", \"deployment\")\n\n\tif !cache.WaitForNamedCacheSync(\"deployment\", stopCh, dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) {"
  },
  {
    "id" : "dba65eb1-dda0-41d7-ba25-d8102551073b",
    "prId" : 97507,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97507#pullrequestreview-603332310",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa1713e1-d083-4b1b-a78c-366d35d57948",
        "parentId" : null,
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "Why moving this? ",
        "createdAt" : "2021-02-25T20:11:04Z",
        "updatedAt" : "2021-02-25T20:11:18Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      },
      {
        "id" : "c8bd15a7-5c5a-434c-af74-dae3a9b1522f",
        "parentId" : "aa1713e1-d083-4b1b-a78c-366d35d57948",
        "authorId" : "65f57cac-91da-4111-86be-4242058079d2",
        "body" : "per https://github.com/kubernetes/kubernetes/pull/97507#discussion_r575173518 `We should avoid passing different values into same logging key, deployment is reserved for using klog.KObj or klog.KRef`.\r\n\r\nI moved `namespace, name, err := cache.SplitMetaNamespaceKey(key)` up to be able to use `namespace` and `name` in `klog.KRef(namespace, name)` to write deployment name instead of cacheKey.",
        "createdAt" : "2021-03-03T03:23:38Z",
        "updatedAt" : "2021-03-03T03:25:10Z",
        "lastEditedBy" : "65f57cac-91da-4111-86be-4242058079d2",
        "tags" : [
        ]
      },
      {
        "id" : "2a0eb6a5-680e-435d-a3b8-4bd54930ea2b",
        "parentId" : "aa1713e1-d083-4b1b-a78c-366d35d57948",
        "authorId" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "body" : "kk",
        "createdAt" : "2021-03-03T20:48:50Z",
        "updatedAt" : "2021-03-03T20:49:07Z",
        "lastEditedBy" : "b7d2a698-a6e1-4031-bb69-8b45505badb5",
        "tags" : [
        ]
      }
    ],
    "commit" : "dcdde707ab41433774cc80c293998e4ae7125254",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +577,581 @@\tdefer func() {\n\t\tklog.V(4).InfoS(\"Finished syncing deployment\", \"deployment\", klog.KRef(namespace, name), \"duration\", time.Since(startTime))\n\t}()\n\n\tdeployment, err := dc.dLister.Deployments(namespace).Get(name)"
  },
  {
    "id" : "2a3f617c-87d1-4af9-a8aa-9e47be2a6e60",
    "prId" : 91576,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/91576#pullrequestreview-560298351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78384a75-8e9b-4c8c-b32b-60e1e04da0cb",
        "parentId" : null,
        "authorId" : "ae15cfb8-5436-4398-94e0-d443e413b257",
        "body" : "Why not just forget the key by `dc.queue.Forget(key)`? It will definitely fail next time if a key is wrong.\r\n@serathius How do you think?",
        "createdAt" : "2020-12-31T08:57:49Z",
        "updatedAt" : "2020-12-31T08:57:49Z",
        "lastEditedBy" : "ae15cfb8-5436-4398-94e0-d443e413b257",
        "tags" : [
        ]
      }
    ],
    "commit" : "78318c7a26928bdabe2491cf022cf9e61be32188",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +485,489 @@\tif keyErr != nil {\n\t\tklog.ErrorS(err, \"Failed to split meta namespace cache key\", \"key\", key)\n\t}\n\n\tif dc.queue.NumRequeues(key) < maxRetries {"
  },
  {
    "id" : "28f4eeb0-a329-4d14-a95d-4f215cdd05bf",
    "prId" : 84123,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/84123#pullrequestreview-311045691",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6956359e-5709-4f1c-8ceb-6f2330b344f4",
        "parentId" : null,
        "authorId" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "body" : "Will work only for unwrapped errors, but we might make it better when we get golang 1.13\r\n\r\nI don't see the calls there wrapping the errors with context so this should be good for now",
        "createdAt" : "2019-11-04T12:53:10Z",
        "updatedAt" : "2019-11-04T13:32:08Z",
        "lastEditedBy" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd9260711fa09bfdfe95a94c27ab14b95a95cb8a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +476,480 @@\nfunc (dc *DeploymentController) handleErr(err error, key interface{}) {\n\tif err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) {\n\t\tdc.queue.Forget(key)\n\t\treturn"
  },
  {
    "id" : "1d3a61a1-b071-4633-b502-fdfe5ab5e1ad",
    "prId" : 79937,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79937#pullrequestreview-259563954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d39b72fb-ae84-41c7-beb6-11fbfd22678e",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Please add a comment that pointer to pod returned can NOT be modified.",
        "createdAt" : "2019-07-09T14:52:48Z",
        "updatedAt" : "2019-07-10T07:14:34Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "b957f839-b10b-45b8-bfb1-7d917a4c4f73",
        "parentId" : "d39b72fb-ae84-41c7-beb6-11fbfd22678e",
        "authorId" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "body" : "Done.",
        "createdAt" : "2019-07-09T15:00:24Z",
        "updatedAt" : "2019-07-10T07:14:34Z",
        "lastEditedBy" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "tags" : [
        ]
      }
    ],
    "commit" : "8140bbc4f5119268c715ae6adaf1884c394a0db4",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +528,532 @@// NOTE: The pod pointers returned by this method point the the pod objects in the cache and thus\n// shouldn't be modified in any way.\nfunc (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) {\n\t// Get all Pods that potentially belong to this Deployment.\n\tselector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)"
  },
  {
    "id" : "3c40ab4f-966d-4a61-bee1-eda8e805c875",
    "prId" : 43239,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/43239#pullrequestreview-27682251",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59830f96-ce54-4b80-b165-b95462080c0c",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Add to this comment a date at which this code can be removed + file an issue and target a milestone.",
        "createdAt" : "2017-03-17T18:30:24Z",
        "updatedAt" : "2017-03-17T18:59:38Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "cda75ae1-87fd-4242-b287-d38b6e62fe79",
        "parentId" : "59830f96-ce54-4b80-b165-b95462080c0c",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "Filed #43322. Added comment in another commit, which I'll send as a separate PR unless this one requires fixup before merging.",
        "createdAt" : "2017-03-17T20:33:14Z",
        "updatedAt" : "2017-03-17T20:50:29Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      }
    ],
    "commit" : "de92f90f12dbc17c8ecdc1b54121d976e17dcd15",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +582,586 @@\t}\n\n\t// This is the point at which we used to add/remove the overlap annotation.\n\t// Now we always remove it if it exists, because it is obsolete as of 1.6.\n\t// Although the server no longer adds or looks at the annotation,"
  },
  {
    "id" : "9b0dd3b7-428b-43f0-94a4-b605aa15ab7e",
    "prId" : 43239,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/43239#pullrequestreview-27666074",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dadfa133-4808-4d7c-82fc-01d16635bd9c",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "`Update` instead of `UpdateStatus` to remove that annotation?",
        "createdAt" : "2017-03-17T19:01:46Z",
        "updatedAt" : "2017-03-17T19:01:46Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "34e9634a-0cd3-416e-8650-a571190402d4",
        "parentId" : "dadfa133-4808-4d7c-82fc-01d16635bd9c",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "We allow annotations to be updated via status and the fact that the deployment is an overlapping one is more of a status info.",
        "createdAt" : "2017-03-17T19:03:46Z",
        "updatedAt" : "2017-03-17T19:03:46Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "20533060-a7e7-4e01-a639-3746278319fb",
        "parentId" : "dadfa133-4808-4d7c-82fc-01d16635bd9c",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "The previous code that worked with OverlapAnnotation used UpdateStatus(). I wanted to restore the clearOverlapAnnotation() behavior as exactly as possible to minimize risk.",
        "createdAt" : "2017-03-17T19:07:07Z",
        "updatedAt" : "2017-03-17T19:07:07Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      }
    ],
    "commit" : "de92f90f12dbc17c8ecdc1b54121d976e17dcd15",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +590,594 @@\tif _, ok := d.Annotations[util.OverlapAnnotation]; ok {\n\t\tdelete(d.Annotations, util.OverlapAnnotation)\n\t\td, err = dc.client.ExtensionsV1beta1().Deployments(d.Namespace).UpdateStatus(d)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"couldn't remove obsolete overlap annotation from deployment %v: %v\", key, err)"
  },
  {
    "id" : "09bb78f5-498d-4c64-a231-7cf593ec67cb",
    "prId" : 42175,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42175#pullrequestreview-25062897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4da2617-e29d-4d3e-9852-61ea0ca5a1d8",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Technically and based on the api doc for the deployment selector, we should be able to list all pods managed by a deployment simply by using the deployment selector. Unfortunately, we don't respect this in many places. Mind opening a separate issue to make sure this gets fixed?",
        "createdAt" : "2017-02-28T12:06:51Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "1c6a1da3-2b20-49ab-8c13-10f434d68287",
        "parentId" : "d4da2617-e29d-4d3e-9852-61ea0ca5a1d8",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "The guarantee that \"we should be able to list all pods managed by a deployment simply by using the deployment selector\" seems to conflict with ControllerRef. Or did you mean the weaker guarantee, \"all pods managed by a deployment must match the deployment's selector\"?",
        "createdAt" : "2017-03-02T20:20:39Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      },
      {
        "id" : "b9f0b511-451b-432b-9992-28809f15a21d",
        "parentId" : "d4da2617-e29d-4d3e-9852-61ea0ca5a1d8",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I don't think there is a conflict. If the labels don't match, the controller ref is invalid thus removed.",
        "createdAt" : "2017-03-03T09:56:50Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "9b7d34d9-c74a-4d89-b5a0-22eade69c85c",
        "parentId" : "d4da2617-e29d-4d3e-9852-61ea0ca5a1d8",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "And replica set selectors are always a superset of the deployment selector.",
        "createdAt" : "2017-03-03T09:57:23Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "de20460e-f5c7-4c34-8555-0051d35036c1",
        "parentId" : "d4da2617-e29d-4d3e-9852-61ea0ca5a1d8",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "As discussed in the other thread, I'm now in agreement that Deployment should only care about Pods that match its selector. However, I still think it's important in this function to filter both by Deployment selector AND by ControllerRef matching a ReplicaSet that we own. Otherwise, we will count Pods that we shouldn't be counting, and could get stuck waiting.\r\n\r\nIs that what you have in mind?",
        "createdAt" : "2017-03-03T19:56:44Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      },
      {
        "id" : "bbf988ca-0f76-44d9-ba75-1ec8d0497698",
        "parentId" : "d4da2617-e29d-4d3e-9852-61ea0ca5a1d8",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Yes, respecting owner references is the reason for this PR and I am all for that ;)",
        "createdAt" : "2017-03-03T20:25:23Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c4bcb38fbfd0bfe6d8694d07dc144ce795c2985",
    "line" : 219,
    "diffHunk" : "@@ -1,1 +353,357 @@\tif d := dc.getDeploymentForPod(pod); d != nil && d.Spec.Strategy.Type == extensions.RecreateDeploymentStrategyType {\n\t\t// Sync if this Deployment now has no more Pods.\n\t\trsList, err := dc.getReplicaSetsForDeployment(d)\n\t\tif err != nil {\n\t\t\treturn"
  },
  {
    "id" : "1458cae2-1160-4b1f-89ca-cc323e66da1d",
    "prId" : 42175,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42175#pullrequestreview-25122000",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9ead6e6-edeb-4adc-9d82-5f6929e31775",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I don't think we care if this is not owned by a Deployment.",
        "createdAt" : "2017-03-03T09:46:33Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "1bdb7d95-6fa6-4a67-a2ca-88c54037c9b1",
        "parentId" : "c9ead6e6-edeb-4adc-9d82-5f6929e31775",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "That check is done inside `deleteReplicaSet()`. Is there a benefit to doing it before calling too?",
        "createdAt" : "2017-03-03T19:15:14Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      },
      {
        "id" : "ce97f48a-4dd9-4ea8-8893-e351f3dafcc3",
        "parentId" : "c9ead6e6-edeb-4adc-9d82-5f6929e31775",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Nvmd, this is fine.",
        "createdAt" : "2017-03-04T11:33:53Z",
        "updatedAt" : "2017-03-06T23:15:00Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c4bcb38fbfd0bfe6d8694d07dc144ce795c2985",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +198,202 @@\t\t// On a restart of the controller manager, it's possible for an object to\n\t\t// show up in a state that is already pending deletion.\n\t\tdc.deleteReplicaSet(rs)\n\t\treturn\n\t}"
  },
  {
    "id" : "a7ee5e7d-aa53-48d4-a04c-76f136c13bd8",
    "prId" : 41131,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41131#pullrequestreview-20761542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f597974-7d32-4786-910b-5cbe9ab65e42",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "how often this message apprear in the log?",
        "createdAt" : "2017-02-08T12:21:35Z",
        "updatedAt" : "2017-02-08T12:21:35Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "3fb354c4-cd7a-4c04-909a-0611503edaa9",
        "parentId" : "2f597974-7d32-4786-910b-5cbe9ab65e42",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "As often as \"Queueing up deployment...\" in progress.go. That shouldn't be that often - depends on the # of pods and it's v2. We can raise the lvl later, now I want them purely for debugging an ongoing flake that is blocking the merge queue (https://github.com/kubernetes/kubernetes/issues/39785).",
        "createdAt" : "2017-02-08T13:47:50Z",
        "updatedAt" : "2017-02-08T13:47:50Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "f5d942c2-e8a6-4959-b3ec-9c11c464cab2",
        "parentId" : "2f597974-7d32-4786-910b-5cbe9ab65e42",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "https://github.com/kubernetes/kubernetes/issues/39785 flakes only on aws and gke and thank god I can get back controller manager logs in v2 from aws.",
        "createdAt" : "2017-02-08T13:50:07Z",
        "updatedAt" : "2017-02-08T13:50:07Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "38195704becacb17d049b3b7e398598efea971cc",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +798,802 @@\t\treturn false, err\n\t}\n\tglog.V(2).Infof(\"Syncing deployment %q for a progress check\", key)\n\treturn dc.hasFailed(d)\n}"
  },
  {
    "id" : "43234626-d19e-46f9-9a28-4bc4b36cbfe2",
    "prId" : 40385,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40385#pullrequestreview-18655526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6fb9eb8-64d5-4971-be11-1900023a9274",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "I see its pre-existing, but we need to sweep and eliminate double reporting",
        "createdAt" : "2017-01-26T16:05:29Z",
        "updatedAt" : "2017-02-06T18:49:31Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "70c60876001b08596bb3b99e0dbe1115ef67a7c4",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +511,515 @@\t}\n\tif err != nil {\n\t\tutilruntime.HandleError(fmt.Errorf(\"Unable to retrieve deployment %v from store: %v\", key, err))\n\t\treturn err\n\t}"
  },
  {
    "id" : "3368744e-6ddb-4a13-815c-f900286a1afd",
    "prId" : 40081,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40081#pullrequestreview-17360965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72f22bc2-54b3-4d7b-be66-4907b0db9802",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Why can't this happen when `RollbackTo != nil`?",
        "createdAt" : "2017-01-19T00:35:36Z",
        "updatedAt" : "2017-01-19T09:33:35Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5227e364d9e504ee957054b1a46392c02a43d22",
    "line" : null,
    "diffHunk" : "@@ -1,1 +560,564 @@\t\t// succesfully completed deploying a replica set. Decouple it from the strategies and have it\n\t\t// run almost unconditionally - cleanupDeployment is safe by default.\n\t\tdc.cleanupDeployment(oldRSs, d)\n\t}\n"
  },
  {
    "id" : "a26d0da4-4265-400b-815b-dcaffd13e801",
    "prId" : 39216,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39216#pullrequestreview-14604915",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f74c520a-329e-4a2e-b159-5ee0224bd411",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "We can probably also remove this HandleError since the aggregate error will be logged or passed to HandleError if the deployment is about to be dropped out of the queue (see `func (dc *DeploymentController) handleErr(err error, key interface{})`)",
        "createdAt" : "2016-12-28T20:29:39Z",
        "updatedAt" : "2017-01-31T04:43:52Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "07964cca10cda7b6b66fc4cba55f2e52b9a142cb",
    "line" : null,
    "diffHunk" : "@@ -1,1 +469,473 @@\t\t\t\t// If the RS no longer exists, don't even log the error.\n\t\t\t\tif !errors.IsNotFound(err) {\n\t\t\t\t\tutilruntime.HandleError(err)\n\t\t\t\t}\n\t\t\t} else {"
  },
  {
    "id" : "b6cdebfa-e68d-4846-9a40-d1d91b8a8d6b",
    "prId" : 38080,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38080#pullrequestreview-13147931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c33ec1a2-bd98-4f80-9cdb-4a4da9980b38",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "why do you need err here?",
        "createdAt" : "2016-12-14T17:17:00Z",
        "updatedAt" : "2016-12-19T13:04:24Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "09fdbe4b-2255-4bdc-af81-e6f28ab7f308",
        "parentId" : "c33ec1a2-bd98-4f80-9cdb-4a4da9980b38",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "If we fail to cleanup the annotation from the deployment, the error will cause the deployment to be resynced and the cleanup will be retried.",
        "createdAt" : "2016-12-15T10:52:03Z",
        "updatedAt" : "2016-12-19T13:04:24Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "2097a3cb-5fba-4930-93e1-ff684bbc3edc",
        "parentId" : "c33ec1a2-bd98-4f80-9cdb-4a4da9980b38",
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "I mean why you need a `var err` and not just `err := dc.clear...`",
        "createdAt" : "2016-12-15T14:55:18Z",
        "updatedAt" : "2016-12-19T13:04:24Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "34a344cb-0e77-4dd1-a070-8f435dd03205",
        "parentId" : "c33ec1a2-bd98-4f80-9cdb-4a4da9980b38",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Because we also return the updated deployment object.",
        "createdAt" : "2016-12-15T15:18:03Z",
        "updatedAt" : "2016-12-19T13:04:24Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "04c6fecbc7da54b4fae29e2f0fd24a0a49ba75f9",
    "line" : 199,
    "diffHunk" : "@@ -1,1 +605,609 @@\n\tif !overlapping {\n\t\tvar err error\n\t\tif d, err = dc.clearDeploymentOverlap(d, \"\"); err != nil {\n\t\t\terrs = append(errs, err)"
  },
  {
    "id" : "8d3b5eea-bd70-460c-9c6c-4eebd9cf77a0",
    "prId" : 36748,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36748#pullrequestreview-12535158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6514033e-5359-419d-a885-09dfa9863e24",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "say we return the oldest one for now ",
        "createdAt" : "2016-12-12T22:13:40Z",
        "updatedAt" : "2016-12-15T18:55:26Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c87e627b9b72fca8ecb6936a3cb2cd03dd998b80",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +341,345 @@\t\t\treturn nil\n\t\t}\n\t\t// TODO: Handle multiple replica sets gracefully\n\t\t// For now we return the oldest replica set.\n\t\tif len(rss) > 1 {"
  },
  {
    "id" : "96203107-07d4-4c1c-b104-a619726dbb48",
    "prId" : 36648,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36648#pullrequestreview-10343223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "896aa8a1-79e3-4232-b1fd-c20135a9e688",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "if",
        "createdAt" : "2016-11-28T15:24:00Z",
        "updatedAt" : "2016-12-06T17:08:47Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "eb286d38-64bd-4bf3-9cc5-1df428d5eb08",
        "parentId" : "896aa8a1-79e3-4232-b1fd-c20135a9e688",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "It's not a typo: https://en.wikipedia.org/wiki/If_and_only_if",
        "createdAt" : "2016-11-28T15:51:36Z",
        "updatedAt" : "2016-12-06T17:08:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8a7ca28f055431afc4976773ee0099b17b33f19",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +274,278 @@// enqueueAfter will enqueue a deployment after the provided amount of time in a secondary queue.\n// Once the deployment is popped out of the secondary queue, it is checked for progress and requeued\n// back to the main queue iff it has failed progressing.\nfunc (dc *DeploymentController) enqueueAfter(deployment *extensions.Deployment, after time.Duration) {\n\tkey, err := controller.KeyFunc(deployment)"
  },
  {
    "id" : "57e58cf7-2f5d-4f3c-84c7-c84c0274c1cd",
    "prId" : 36648,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36648#pullrequestreview-10343958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de70c24a-7cff-4dc6-bf37-459b13db7861",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "do we need the deep copy here then?",
        "createdAt" : "2016-11-28T15:26:42Z",
        "updatedAt" : "2016-12-06T17:08:47Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "f0c627a9-623c-4cc2-a01a-f539f82c25e6",
        "parentId" : "de70c24a-7cff-4dc6-bf37-459b13db7861",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Yes, otherwise I wouldn't have the TODO. `hasFailed` is using `getAllReplicaSetsAndSyncRevision` which is mutating in some cases.",
        "createdAt" : "2016-11-28T15:54:28Z",
        "updatedAt" : "2016-12-06T17:08:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8a7ca28f055431afc4976773ee0099b17b33f19",
    "line" : 175,
    "diffHunk" : "@@ -1,1 +516,520 @@\t}\n\t// Deep-copy otherwise we may mutate our cache.\n\t// TODO: Remove deep-copying from here. This worker does not need to sync the annotations\n\t// in the deployment.\n\td, err := util.DeploymentDeepCopy(deployment)"
  },
  {
    "id" : "36ba9563-c6dc-41bd-b868-1569d8f62336",
    "prId" : 36648,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36648#pullrequestreview-12129966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a7cfffb-dba5-44b2-9b61-f108e9e9d601",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "Why have a secondary queue instead of simply adding using the `AddAfter` on the normal queue?",
        "createdAt" : "2016-12-08T21:20:26Z",
        "updatedAt" : "2016-12-08T21:20:26Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "01bfac43-9020-4fc3-b4e3-ffa729650ae4",
        "parentId" : "8a7cfffb-dba5-44b2-9b61-f108e9e9d601",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I didn't want to add the additional requeues in the main queue because syncing stuck deployments is not that easy. I can only make a good guess about the last time the controller will update the status of a deployment in the middle of a rollout. In a pathological case where a deployment runs many pods, the controller may end up resyncing the deployment many times where it won't need to update the deployment status. Think that every update will produce another resync where nothing may need to be updated. So I opted for a different queue instead of having a chance to ddos the main one. Also the progress check (what would need to be abstracted by the current code and run before anything else for every deployment) depends on resolving overlapping deployments. In order to resolve overlapping deployments, we do a List of all the deployments in the namespace. By having something else just run the progress checks, we don't List deployments that often.",
        "createdAt" : "2016-12-08T22:12:44Z",
        "updatedAt" : "2016-12-08T22:12:44Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8a7ca28f055431afc4976773ee0099b17b33f19",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +87,91 @@\tqueue workqueue.RateLimitingInterface\n\t// Deployments that need to be checked for progress.\n\tprogressQueue workqueue.RateLimitingInterface\n}\n"
  },
  {
    "id" : "6cfe4707-b15c-413f-a051-f33fe07d0140",
    "prId" : 35676,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35676#pullrequestreview-6188878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98770a0e-a76b-4f71-93e3-82cc6dd7157f",
        "parentId" : null,
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "I don't know deployement controller well enough. If the number of controlled replicaset is greater than the max rollback history, will another routine try to delete the surplus? @janetkuo @kargakis \n",
        "createdAt" : "2016-10-28T06:03:35Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "777977612b4b10bfd9d1fc26fa23669b89b8911f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +534,538 @@\t\t// For any other failure, we should retry the deployment.\n\t\treturn err\n\t}\n\n\tif d.DeletionTimestamp != nil {"
  },
  {
    "id" : "154869f4-d40e-46b9-8a8f-3337dbbe99ca",
    "prId" : 35676,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35676#pullrequestreview-6800309",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccd15b42-6a53-415f-b9e7-5e211445b3d2",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Can you add some godoc here?\n",
        "createdAt" : "2016-11-02T12:13:38Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "777977612b4b10bfd9d1fc26fa23669b89b8911f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +444,448 @@// It also removes the controllerRef for ReplicaSets, whose labels no longer matches\n// the deployment.\nfunc (dc *DeploymentController) classifyReplicaSets(deployment *extensions.Deployment) error {\n\trsList, err := dc.rsLister.ReplicaSets(deployment.Namespace).List(labels.Everything())\n\tif err != nil {"
  },
  {
    "id" : "92187e42-6c1d-41b1-b759-47981cd78d9e",
    "prId" : 35676,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35676#pullrequestreview-8773025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85571a9d-9ae4-4be3-bc95-04933d9b5c04",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Also the deletionTimestamp check needs to be after the overlap check (otherwise we update the status for overlapping deployments once they are marked deleted.\n",
        "createdAt" : "2016-11-16T09:04:32Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "777977612b4b10bfd9d1fc26fa23669b89b8911f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +536,540 @@\t}\n\n\tif d.DeletionTimestamp != nil {\n\t\treturn dc.syncStatusOnly(d)\n\t}"
  },
  {
    "id" : "498f1031-a859-491c-ba68-173bc28dac17",
    "prId" : 35676,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35676#pullrequestreview-12654320",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "By working on https://github.com/kubernetes/kubernetes/pull/38080, I realized that this change will orphan old replica sets for a deployment once the deployment is updated to a selector that is not matching the old replica sets. I am ambivalent as to what should we do in this case: preserve the history for the deployment or orphan all of those replica sets and start with a new history. Preserving seems more correct but also harder to do. Orphaning is easier to do, but it is unlikely that something else will adopt the orphaned replica sets.\r\n\r\n@kubernetes/deployment ",
        "createdAt" : "2016-12-05T15:16:38Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "649d9c50-3963-4d20-ab8f-7de09e0beedd",
        "parentId" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Note that with what you have here, you are orphaning (second case).",
        "createdAt" : "2016-12-05T15:17:19Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "c3d12e96-51fa-40f1-9d65-61d3de138934",
        "parentId" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "@caesarxuchao ",
        "createdAt" : "2016-12-05T15:18:28Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "1031c6ba-690f-4d35-a64c-f1aeb1c12431",
        "parentId" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "Do we discourage changing the selector of a deployment? Is it an important use case we need to support?",
        "createdAt" : "2016-12-07T00:48:43Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      },
      {
        "id" : "1f7d1fae-ac06-4905-91ae-f31c710fa0a5",
        "parentId" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I believe we should discourage selectors changes in controllers but allow them to enable advanced use-cases (Blue-Green deployments is one). That being said I am not sure what changing a label selector for an app means semantically for the app. Should we treat a change to a new non-overlapping selector (non-overlapping to its previous self) , a new history for the Deployment (what happens now probably by accident) or should we continue maintaining the old history which doesn't match with the new selector anymore? \r\n\r\ncc: @smarterclayton @bgrant0607 ",
        "createdAt" : "2016-12-07T01:23:37Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "dd126d9d-46dd-447b-a8ab-a6e010e0af90",
        "parentId" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "authorId" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "body" : "For the [Blue-Green deployments](https://docs.cloudfoundry.org/devguide/deploy-apps/blue-green.html), I think users should create two deployments with different selectors, and change the selector of the front-end Service object to switch between Blue and Green. Is this discussed somewhere?",
        "createdAt" : "2016-12-07T07:13:33Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "ca7e5a52-cab7-4f09-8ff8-da79f43339d4",
        "tags" : [
        ]
      },
      {
        "id" : "c58b8c48-8275-4c28-8b83-c90414495529",
        "parentId" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Yes, that's the suggested approach but we have had clients who don't want to use a second Deployment and instead relabel pods. Changing the selector for a service makes sense, not sure how much does it make for a Deployment but I don't want to rule anything out especially once we have custom Deployments.",
        "createdAt" : "2016-12-07T12:37:14Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "3a696d10-580b-4180-8f02-192ad09d47b3",
        "parentId" : "3810154b-ab81-4720-8145-bdd92aee426f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Opened https://github.com/kubernetes/kubernetes.github.io/issues/1938",
        "createdAt" : "2016-12-13T11:08:35Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "777977612b4b10bfd9d1fc26fa23669b89b8911f",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +473,477 @@\t// remove the controllerRef for the RS that no longer have matching labels\n\tvar errlist []error\n\tfor _, replicaSet := range controlledDoesNotMatch {\n\t\terr := cm.ReleaseReplicaSet(replicaSet)\n\t\tif err != nil {"
  },
  {
    "id" : "08fdc5d7-1e16-454e-8486-ae0d625232ff",
    "prId" : 35676,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/35676#pullrequestreview-14325169",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6da1923a-f5d8-4849-b9ff-e68d0b644500",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Why do we append errors only when releasing and not also during adoption?",
        "createdAt" : "2016-12-18T22:19:22Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "285529a5-1ae6-456a-a9ee-2dc9fdb7dea8",
        "parentId" : "6da1923a-f5d8-4849-b9ff-e68d0b644500",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Looks like we should append in adoption case as well, @caesarxuchao i see the same behavior in AdoptPods, any specific reason ?",
        "createdAt" : "2016-12-20T08:05:57Z",
        "updatedAt" : "2016-12-23T00:43:41Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "9d6b277b-6cf0-495e-9081-726015a5f34d",
        "parentId" : "6da1923a-f5d8-4849-b9ff-e68d0b644500",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "@krmayankk can you open an issue about changing this to include adoption errors?",
        "createdAt" : "2016-12-23T09:58:26Z",
        "updatedAt" : "2016-12-23T09:58:26Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "aa8c2a74-84c1-406f-be38-b9b3b1a170af",
        "parentId" : "6da1923a-f5d8-4849-b9ff-e68d0b644500",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Will do and open a pr for it as well .  Sorry I missed that ",
        "createdAt" : "2016-12-23T17:48:52Z",
        "updatedAt" : "2016-12-23T17:48:52Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      }
    ],
    "commit" : "777977612b4b10bfd9d1fc26fa23669b89b8911f",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +472,476 @@\t}\n\t// remove the controllerRef for the RS that no longer have matching labels\n\tvar errlist []error\n\tfor _, replicaSet := range controlledDoesNotMatch {\n\t\terr := cm.ReleaseReplicaSet(replicaSet)"
  },
  {
    "id" : "8b4d1f69-3a39-412d-b490-b710c473b8b1",
    "prId" : 34952,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34952#pullrequestreview-5657763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8aac8c4-8a16-4380-98a1-12c40d203a63",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "add godoc\n",
        "createdAt" : "2016-10-25T14:02:16Z",
        "updatedAt" : "2016-10-25T14:02:16Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "99132172-e3ad-4835-a28e-9589cb6ffe1e",
        "parentId" : "c8aac8c4-8a16-4380-98a1-12c40d203a63",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Will do in a follow-up - I want to have this file keep just the controller machinery and the sync function so eventually these helpers that are used for handling overlap should move to somewhere else (probably `sync.go`). I am planning to add a doc.go that will document what every file is containing.\n",
        "createdAt" : "2016-10-25T14:08:25Z",
        "updatedAt" : "2016-10-25T14:08:26Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "12455bf5e197de6acdb59aa51d1ce5fafea29fa9",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +417,421 @@}\n\nfunc (dc *DeploymentController) markDeploymentOverlap(deployment *extensions.Deployment, withDeployment string) (*extensions.Deployment, error) {\n\tif deployment.Annotations[util.OverlapAnnotation] == withDeployment && deployment.Status.ObservedGeneration >= deployment.Generation {\n\t\treturn deployment, nil"
  },
  {
    "id" : "91e1f468-5dd7-4598-811b-d2bb2992c7b8",
    "prId" : 34024,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34024#pullrequestreview-2922234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f509802-23cc-4cde-a3b2-37647959bdd8",
        "parentId" : null,
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "I assume the rs changes in this commit should really be in the rs commit, but I'm not going to make you change them 😄 \n",
        "createdAt" : "2016-10-05T14:32:14Z",
        "updatedAt" : "2016-10-05T19:20:31Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c30b2efc46621d955a70256771302d6da168d6a2",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +72,76 @@\tdController *cache.Controller\n\t// A store of ReplicaSets, populated by the rsController\n\trsLister cache.StoreToReplicaSetLister\n\t// Watches changes to all ReplicaSets\n\trsController *cache.Controller"
  },
  {
    "id" : "3d85d7df-d4a7-49c0-bb76-f98f3c0a9fb2",
    "prId" : 33492,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33492#pullrequestreview-1573303",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04f8c0f5-29df-4114-94c0-bfacb117af4a",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "i guess this will be propagated to conditions\n",
        "createdAt" : "2016-09-26T16:16:05Z",
        "updatedAt" : "2016-09-26T16:16:15Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "e162ab8d-68b3-47a4-83ad-47745fa282db",
        "parentId" : "04f8c0f5-29df-4114-94c0-bfacb117af4a",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Correct but once we have owner references we won't have to do this at all.\n",
        "createdAt" : "2016-09-26T16:36:47Z",
        "updatedAt" : "2016-09-26T16:36:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a843a50ba5cad83a20929eeef61c232896a6b17",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +509,513 @@\t// Handle overlapping deployments by deterministically avoid syncing deployments that fight over ReplicaSets.\n\tif err = dc.handleOverlap(d); err != nil {\n\t\tdc.eventRecorder.Eventf(deployment, api.EventTypeWarning, \"SelectorOverlap\", err.Error())\n\t\treturn nil\n\t}"
  },
  {
    "id" : "23291e48-03da-41de-a12d-d69656970f24",
    "prId" : 30730,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a277bde1-1156-4c28-9564-ba0dd967662c",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "remove the todo of this func\n",
        "createdAt" : "2016-08-19T20:06:25Z",
        "updatedAt" : "2016-08-23T21:33:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5cef18735c96eb2dda98609cafd96a505958f78",
    "line" : null,
    "diffHunk" : "@@ -1,1 +268,272 @@\tif len(deployments) > 1 {\n\t\tsort.Sort(util.BySelectorLastUpdateTime(deployments))\n\t\tglog.Errorf(\"user error! more than one deployment is selecting replica set %s/%s with labels: %#v, returning %s/%s\", rs.Namespace, rs.Name, rs.Labels, deployments[0].Namespace, deployments[0].Name)\n\t}\n\treturn &deployments[0]"
  },
  {
    "id" : "1b448f4a-88ac-4ee1-9d3e-a9667a1d2ae4",
    "prId" : 30730,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06f27f35-937d-4153-81c0-6c1cb60206dc",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Can you extract all this diff into a `dc.isOverlapping` method in order to keep `syncDeployment` clean?\n",
        "createdAt" : "2016-08-20T18:29:44Z",
        "updatedAt" : "2016-08-23T21:33:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5cef18735c96eb2dda98609cafd96a505958f78",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +508,512 @@\t}\n\n\t// Handle overlapping deployments by deterministically avoid syncing deployments that fight over ReplicaSets.\n\tif err = dc.handleOverlap(d); err != nil {\n\t\treturn err"
  },
  {
    "id" : "a8bdb643-db31-4fc9-b7d3-b875789e0b69",
    "prId" : 27012,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4220c6ea-4e74-4c53-ae00-a4d60a2938b4",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Why can't you switch to it now?\n",
        "createdAt" : "2016-06-09T07:50:44Z",
        "updatedAt" : "2016-06-10T16:55:37Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "3636f8c1-5039-480a-8cbf-e0aa0b5f7b97",
        "parentId" : "4220c6ea-4e74-4c53-ae00-a4d60a2938b4",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "explained in https://github.com/kubernetes/kubernetes/pull/27012#issuecomment-224801601\n",
        "createdAt" : "2016-06-09T17:24:35Z",
        "updatedAt" : "2016-06-10T16:55:37Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "764df2e096333e79fedf31dace1a71510218af5d",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +998,1002 @@\tminReadySeconds := deployment.Spec.MinReadySeconds\n\tallPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)\n\t// TODO: use dc.getAvailablePodsForReplicaSets instead\n\tnewRSAvailablePodCount, err := deploymentutil.GetAvailablePodsForReplicaSets(dc.client, deployment, []*extensions.ReplicaSet{newRS}, minReadySeconds)\n\tif err != nil {"
  },
  {
    "id" : "fd82feee-e018-41a9-9c80-418186d41f11",
    "prId" : 25119,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78ac20e8-30c9-4f2b-a3e0-2a3de7124464",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "Consider moving this predicate entirely into the new package. Or add a TODO.\n",
        "createdAt" : "2016-05-04T20:21:11Z",
        "updatedAt" : "2016-05-09T18:16:58Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "e50eeccf-061a-46ba-9642-d1e9c2336876",
        "parentId" : "78ac20e8-30c9-4f2b-a3e0-2a3de7124464",
        "authorId" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "body" : "This should be deployment controller logic, no?\n",
        "createdAt" : "2016-05-04T22:59:53Z",
        "updatedAt" : "2016-05-09T18:16:58Z",
        "lastEditedBy" : "a179bbc5-3a91-4905-bad9-4458ac257dba",
        "tags" : [
        ]
      }
    ],
    "commit" : "441e206671130154ea3c4187945fd613a0bdbcf6",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +912,916 @@// TODO: How to decide which annotations should / should not be copied?\n//       See https://github.com/kubernetes/kubernetes/pull/20035#issuecomment-179558615\nfunc skipCopyAnnotation(key string) bool {\n\t// Skip apply annotations and revision annotations.\n\treturn key == annotations.LastAppliedConfigAnnotation || key == deploymentutil.RevisionAnnotation"
  },
  {
    "id" : "56550fda-81eb-4f6a-9b57-fcc8574a7c13",
    "prId" : 23160,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99538ab4-6067-4528-ba6c-90802cb3c663",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "How are annotations that do not directly override each other handled in this situation? e.g.\n{rs1-annotation:foo}\n{rs2-annotation:bar}\n\nWhen we rollback to rs1 from rs2, this will add back {rs1-annotation:foo}, but does {rs2-annotation:bar} need to be explicitly removed (or is that handled by other code?)\n",
        "createdAt" : "2016-03-18T17:40:16Z",
        "updatedAt" : "2016-03-21T17:22:01Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "e25652aa-bab3-4f38-9a77-b79622ed01a7",
        "parentId" : "99538ab4-6067-4528-ba6c-90802cb3c663",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Ah, yes we should remove those annotations. \n",
        "createdAt" : "2016-03-19T01:08:58Z",
        "updatedAt" : "2016-03-21T17:22:01Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "36d4aed0-53f7-4792-8313-523608a18bb0",
        "parentId" : "99538ab4-6067-4528-ba6c-90802cb3c663",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Fixed\n",
        "createdAt" : "2016-03-19T01:30:30Z",
        "updatedAt" : "2016-03-21T17:22:01Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "482efba8def114f7bb815c19557f2f46b61ecf49",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1269,1273 @@\t\t//\n\t\t// For example,\n\t\t// A Deployment has old RS1 with annotation {change-cause:create}, and new RS2 {change-cause:edit}.\n\t\t// Note that both annotations are copied from Deployment, and the Deployment should be annotated {change-cause:edit} as well.\n\t\t// Now, rollback Deployment to RS1, we should update Deployment's pod-template and also copy annotation from RS1."
  },
  {
    "id" : "fdb4302d-71d9-427f-a6d6-a017e5b894b9",
    "prId" : 22828,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "571f0206-919f-4fff-ac33-37cd40279ae7",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Wouldn't it make sense to scale to zero?\n",
        "createdAt" : "2016-03-11T09:37:43Z",
        "updatedAt" : "2016-03-11T09:37:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a181dacec319074485bf16390fae2c617c9fe83",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +931,935 @@\t\tscaledDownCount := integer.IntMin(maxCleanupCount-totalScaledDown, targetRS.Spec.Replicas-readyPodCount)\n\t\tnewReplicasCount := targetRS.Spec.Replicas - scaledDownCount\n\t\tif newReplicasCount > targetRS.Spec.Replicas {\n\t\t\treturn nil, 0, fmt.Errorf(\"when cleaning up unhealthy replicas, got invalid request to scale down %s/%s %d -> %d\", targetRS.Namespace, targetRS.Name, targetRS.Spec.Replicas, newReplicasCount)\n\t\t}"
  },
  {
    "id" : "5a0a73dd-c8c7-4854-8c87-1b9fe386f36b",
    "prId" : 22828,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b420735-0724-4c0d-8929-00c831ea7c40",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Since this is a slice of pointers I would expect it to be updated w/o needing to re-assign\n",
        "createdAt" : "2016-03-11T09:39:18Z",
        "updatedAt" : "2016-03-11T09:39:18Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a181dacec319074485bf16390fae2c617c9fe83",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +939,943 @@\t\t}\n\t\ttotalScaledDown += scaledDownCount\n\t\toldRSs[i] = updatedOldRS\n\t}\n\treturn oldRSs, totalScaledDown, nil"
  },
  {
    "id" : "d7253d43-a3f0-4105-a12a-e63e60e6104a",
    "prId" : 22375,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "991a01fc-d435-427f-84a6-cfcc10e07f1a",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Was this renaming necessary?:)\n",
        "createdAt" : "2016-03-03T09:38:59Z",
        "updatedAt" : "2016-03-03T09:38:59Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "48dc694ba682912cc7a12cf0d5a95ae38fa2a1a1",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +453,457 @@// Updates the status of a paused deployment\nfunc (dc *DeploymentController) syncPausedDeploymentStatus(deployment *extensions.Deployment) error {\n\tnewRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(deployment, false)\n\tif err != nil {\n\t\treturn err"
  },
  {
    "id" : "f5023490-ddfe-4b64-af43-dfecf721cb30",
    "prId" : 22326,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aecdd0c4-6106-4d48-8a97-03e8a078559e",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Is this because the Deployment revision annotation hasn't necessarily been set yet at this point and there's a separate process for updating that on the RS?\n",
        "createdAt" : "2016-03-02T01:39:07Z",
        "updatedAt" : "2016-03-02T01:39:07Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "75b7c9e0-33f4-439c-9a83-becb9c9a031e",
        "parentId" : "aecdd0c4-6106-4d48-8a97-03e8a078559e",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Yes. The deployment's revision number is updated from checking its new RS revision, but not the other way around.  New rs revision is updated automatically from calculating the deployment's RSes revisions. \n\nWe shouldn't update rs's revision from its deployment, since it's possible that we update the new RS's revision, and then failed trying to update the deployment's revision. \n",
        "createdAt" : "2016-03-02T01:50:21Z",
        "updatedAt" : "2016-03-02T01:51:08Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "6ae5a82c-b08d-4190-bb9e-e348aaf6d433",
        "parentId" : "aecdd0c4-6106-4d48-8a97-03e8a078559e",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Could you please add a comment to that effect?\n",
        "createdAt" : "2016-03-02T02:17:27Z",
        "updatedAt" : "2016-03-02T02:17:27Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "67bfe5d796012cf580b5762b30208680948afcb9",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +778,782 @@\t\t// TODO: How to decide which annotations should / should not be copied?\n\t\t// See https://github.com/kubernetes/kubernetes/pull/20035#issuecomment-179558615\n\t\tif k == kubectl.LastAppliedConfigAnnotation || k == deploymentutil.RevisionAnnotation || rs.Annotations[k] == v {\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "f7c7abb5-5b42-4f9f-ad0b-81a2a3212b12",
    "prId" : 22326,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8ee65c7-cf0c-4ce6-8a1a-57fb7ebecf2b",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Please add a comment about the scenario where the RS revision could be greater.\n",
        "createdAt" : "2016-03-02T02:18:18Z",
        "updatedAt" : "2016-03-02T02:18:18Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "67bfe5d796012cf580b5762b30208680948afcb9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +759,763 @@\t\trs.Annotations = make(map[string]string)\n\t}\n\tif rs.Annotations[deploymentutil.RevisionAnnotation] < newRevision {\n\t\trs.Annotations[deploymentutil.RevisionAnnotation] = newRevision\n\t\tannotationChanged = true"
  },
  {
    "id" : "8e78f2ae-310d-4307-b94b-76f1ec0e462b",
    "prId" : 21857,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f7cb9d3-5690-4146-b41f-78032ad5c74e",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Put a TODO here to implement scaling: #20273\n",
        "createdAt" : "2016-02-24T08:31:44Z",
        "updatedAt" : "2016-02-24T21:00:56Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "f41b8c97-4e73-4825-8bb8-b54313bbc584",
        "parentId" : "8f7cb9d3-5690-4146-b41f-78032ad5c74e",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Done\n",
        "createdAt" : "2016-02-24T08:49:23Z",
        "updatedAt" : "2016-02-24T21:00:56Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a538e317fdcefed3e0866a29056bae915319b9e",
    "line" : null,
    "diffHunk" : "@@ -1,1 +422,426 @@\tif d.Spec.Paused {\n\t\t// TODO: Implement scaling for paused deployments.\n\t\t// Dont take any action for paused deployment.\n\t\t// But keep the status up-to-date.\n\t\t// Ignore paused deployments"
  },
  {
    "id" : "c0a4ad08-5e7d-4d75-9ce8-5f5ec84916a2",
    "prId" : 21857,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b6a9689-dba6-4618-9eea-33ca429c5ca6",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Add the following stanza here so that cleanup policy for paused deployments (https://github.com/kubernetes/kubernetes/issues/20966) won't be blocked on https://github.com/kubernetes/kubernetes/pull/20273\n\n``` go\nif deployment.Spec.RevisionHistoryLimit != nil {\n        // Cleanup old replica sets\n        dc.cleanupOldReplicaSets(oldRSs, deployment)\n    }\n```\n",
        "createdAt" : "2016-02-24T21:47:32Z",
        "updatedAt" : "2016-02-24T21:47:32Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "eb2f2abf-deda-4b15-94b6-4b6deea760b9",
        "parentId" : "2b6a9689-dba6-4618-9eea-33ca429c5ca6",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Or we can have it as a follow-up. If so, I will split it from my PR, once this merges.\n",
        "createdAt" : "2016-02-24T21:49:05Z",
        "updatedAt" : "2016-02-24T21:49:05Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "07e54583-68e2-456f-8a90-1f1976090e78",
        "parentId" : "2b6a9689-dba6-4618-9eea-33ca429c5ca6",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Good point, but let's do that as a followup.\n",
        "createdAt" : "2016-02-24T22:25:11Z",
        "updatedAt" : "2016-02-24T22:25:11Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a538e317fdcefed3e0866a29056bae915319b9e",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +451,455 @@\t}\n\tallRSs := append(controller.FilterActiveReplicaSets(oldRSs), newRS)\n\n\t// Sync deployment status\n\treturn dc.syncDeploymentStatus(allRSs, newRS, *deployment)"
  },
  {
    "id" : "c758ac0c-cb8f-40cf-9f80-5889829d6b78",
    "prId" : 21091,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab6a603e-38cb-4483-8c19-57ebf2baa6fc",
        "parentId" : null,
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "How about move this to [L399](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L399), and insert the key into work queue directly?\n",
        "createdAt" : "2016-02-13T03:38:24Z",
        "updatedAt" : "2016-02-13T03:38:24Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      }
    ],
    "commit" : "2731e5fe209ecd286ca6b68d284188fa26585da5",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +411,415 @@\t}\n\td := *obj.(*extensions.Deployment)\n\tif !dc.rsStoreSynced() || !dc.podStoreSynced() {\n\t\t// Sleep so we give the replica set / pod reflector goroutine a chance to run.\n\t\ttime.Sleep(StoreSyncedPollPeriod)"
  },
  {
    "id" : "b3ae1bba-dc54-4191-a503-89728ffd3623",
    "prId" : 20696,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2a69006-b807-4c93-9b66-b588d4ff1479",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Wouldn't it be simpler if in the place where we create the newRC, check if it's recreate, if yes, make sure spec.replicas is zero instead of passing around booleans? \n",
        "createdAt" : "2016-02-17T11:14:44Z",
        "updatedAt" : "2016-02-17T11:14:44Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "85abdb13-f5d3-4820-a9aa-56ab79bde4b3",
        "parentId" : "f2a69006-b807-4c93-9b66-b588d4ff1479",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Yes it'd be simpler if we set recreate deployment's RS replicas to 0 when creating it https://github.com/kubernetes/kubernetes/pull/20696/files#diff-a5ab8c704a7cc7eb6ea0616cc18829c5R268, which is how I implemented it earlier. \n\nBut aren't we trying to assign correct number of replicas to new RSs at creation time?\n",
        "createdAt" : "2016-02-17T20:29:25Z",
        "updatedAt" : "2016-02-17T20:29:25Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "99ecbdeb-f741-4124-90fc-11d0aaf176b1",
        "parentId" : "f2a69006-b807-4c93-9b66-b588d4ff1479",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "duh, and I asked you to change it from zero which is the right starting value for recreate, right?:)\n",
        "createdAt" : "2016-02-17T20:32:47Z",
        "updatedAt" : "2016-02-17T20:32:47Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "3bb49804-d219-4af8-a173-2441f0c1cb3e",
        "parentId" : "f2a69006-b807-4c93-9b66-b588d4ff1479",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Yes!\n",
        "createdAt" : "2016-02-17T20:38:25Z",
        "updatedAt" : "2016-02-17T20:38:25Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe4bf6ff5cf8837e604227395ad67ba503168cb0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +495,499 @@func (dc *DeploymentController) syncRecreateDeployment(deployment extensions.Deployment) error {\n\t// Don't create a new RS if not already existed, so that we avoid scaling up before scaling down\n\tnewRS, oldRSs, err := dc.getAllReplicaSets(deployment, false)\n\tif err != nil {\n\t\treturn err"
  },
  {
    "id" : "0252f6da-7f92-4abf-ad8b-49bad9042492",
    "prId" : 20368,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10b2dc7b-72db-4658-8cae-fd8b93326c23",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Are these RCs sorted oldest to newest?\n",
        "createdAt" : "2016-02-04T00:36:27Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "b79f3849-4b68-4a9e-8bb4-0bd2d0d72c60",
        "parentId" : "10b2dc7b-72db-4658-8cae-fd8b93326c23",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "No they're not. \n",
        "createdAt" : "2016-02-04T00:42:45Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "d7cd189d-54f9-4d56-b18c-f35d680df062",
        "parentId" : "10b2dc7b-72db-4658-8cae-fd8b93326c23",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "They are now (L901)\n",
        "createdAt" : "2016-02-05T00:48:46Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "86aea1d59c42de15afbff5e2388e4b764bd134fc",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +901,905 @@\t// been deleted first and won't increase unavailability.\n\ttotalScaledDown := 0\n\tfor _, targetRC := range oldRCs {\n\t\tif totalScaledDown >= maxCleanupCount {\n\t\t\tbreak"
  },
  {
    "id" : "c5c036df-782b-48f1-b0c7-8f492f00bda7",
    "prId" : 20368,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aabd4cbc-424d-4506-b58c-c3b9e263cb8e",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Since you're checking <= 0, you could eliminate the Max with 0.\n",
        "createdAt" : "2016-02-04T07:27:53Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "86aea1d59c42de15afbff5e2388e4b764bd134fc",
    "line" : null,
    "diffHunk" : "@@ -1,1 +869,873 @@\tnewRCUnavailablePodCount := newRC.Spec.Replicas - newRCAvailablePodCount\n\tmaxScaledDown := allPodsCount - minAvailable - newRCUnavailablePodCount\n\tif maxScaledDown <= 0 {\n\t\treturn false, nil\n\t}"
  },
  {
    "id" : "5b3fb9c6-da2b-43b0-bb61-7b20ee712f23",
    "prId" : 20368,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "584dad7e-8ca0-4dc9-8e8f-be2b9e947c65",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "`totalScaleDown` should never `> maxCleanupCount`, right?\n",
        "createdAt" : "2016-02-04T19:07:36Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "5d76e6f4-46eb-4a83-b01a-1c77852e0bb0",
        "parentId" : "584dad7e-8ca0-4dc9-8e8f-be2b9e947c65",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "It shouldn't.\n\nThis loop sums up totalScaledDown. The loop in scaleDownOldRCsForRollingUpdate subtracts from totalScaleDownCount. Please use the same approach in both places.\n",
        "createdAt" : "2016-02-05T05:52:11Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "ebaa4071-b742-4c49-840c-73396a57afe5",
        "parentId" : "584dad7e-8ca0-4dc9-8e8f-be2b9e947c65",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "It shouldn't.\n\nThis loop sums up totalScaledDown. The loop in scaleDownOldRCsForRollingUpdate subtracts from totalScaleDownCount. Please use the same approach in both places.\n",
        "createdAt" : "2016-02-05T05:52:11Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      }
    ],
    "commit" : "86aea1d59c42de15afbff5e2388e4b764bd134fc",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +902,906 @@\ttotalScaledDown := 0\n\tfor _, targetRC := range oldRCs {\n\t\tif totalScaledDown >= maxCleanupCount {\n\t\t\tbreak\n\t\t}"
  },
  {
    "id" : "cd8b5e80-7f01-4770-9137-8bf73dc8f7d2",
    "prId" : 20368,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f507f46-95d6-4b33-8c85-a7c9c5c5ec34",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "As I commented here: https://github.com/kubernetes/kubernetes/pull/20273/files#r51953975\nit would be useful to factor this out into a helper function (in pkg/util/deployment).\n",
        "createdAt" : "2016-02-05T05:25:24Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "c21bbcb3-fc62-4407-ba4a-81041df435bc",
        "parentId" : "3f507f46-95d6-4b33-8c85-a7c9c5c5ec34",
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : " I think we could address this in a follow up PR.\n",
        "createdAt" : "2016-02-06T01:16:29Z",
        "updatedAt" : "2016-02-08T23:44:50Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      }
    ],
    "commit" : "86aea1d59c42de15afbff5e2388e4b764bd134fc",
    "line" : null,
    "diffHunk" : "@@ -1,1 +932,936 @@// Need check maxUnavailable to ensure availability\nfunc (dc *DeploymentController) scaleDownOldRCsForRollingUpdate(allRCs []*api.ReplicationController, oldRCs []*api.ReplicationController, deployment extensions.Deployment) (int, error) {\n\tmaxUnavailable, isPercent, err := util.GetIntOrPercentValue(&deployment.Spec.Strategy.RollingUpdate.MaxUnavailable)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"invalid value for MaxUnavailable: %v\", err)"
  },
  {
    "id" : "f58c2ad6-848d-4f94-8771-585cc094aa6b",
    "prId" : 20273,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fd1739e-ebaf-41d3-b7d7-b0afcf960b8b",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Would you add a comment explaining what this function does.  Specifically what \"CleanUp\" entails and what qualifies a ReplicaSet as \"Old\"\n",
        "createdAt" : "2016-05-18T20:12:06Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "dbd8c99c-3257-4229-97c1-59c9d8bf9909",
        "parentId" : "3fd1739e-ebaf-41d3-b7d7-b0afcf960b8b",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "A bit scary that no RevisionHistoryLimit == unlimited, but that is what the spec says.\n",
        "createdAt" : "2016-05-18T21:00:51Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "d8b221cb-955b-4b25-b1a2-e427604967d6",
        "parentId" : "3fd1739e-ebaf-41d3-b7d7-b0afcf960b8b",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "> Would you add a comment explaining what this function does. Specifically what \"CleanUp\" entails and what qualifies a ReplicaSet as \"Old\"\n\nWill do\n\n> A bit scary that no RevisionHistoryLimit == unlimited, but that is what the spec says.\n\nNot really. This has always been the behavior for Deployments prior to adding RevisionHistoryLimit. Which is reasonable because we don't want to mess with a user's deployment history. But eventually most common use cases involve keeping around just a few of the latest replica sets and not all so we will end up defaulting this to 2 or 3 at some point (i think there is an issue about it and it is also in the deployment roadmap).\n",
        "createdAt" : "2016-05-18T21:14:02Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "b5f5e189-e2db-4bdf-8568-2594755baef8",
        "parentId" : "3fd1739e-ebaf-41d3-b7d7-b0afcf960b8b",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Added comment describing the cleanup functionality.\n",
        "createdAt" : "2016-05-19T09:46:20Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "14df169d-b501-4f50-ac33-88a0a2e3f8df",
        "parentId" : "3fd1739e-ebaf-41d3-b7d7-b0afcf960b8b",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "> Not really. This has always been the behavior for Deployments prior to adding RevisionHistoryLimit. \n\nSeems scary to me.  From a human perspective a history limit of 1000 isn't that different from infinite, but a buggy component could to 50K deployment revisions per week and maybe no one would notice.  Seems a lot safer to just put a default of \"more than anyone should want, but going to start GCing before it starts causing issues\".\n",
        "createdAt" : "2016-05-20T05:52:02Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3d2e3ff2203da20053f9a104f3cce6571dbe293",
    "line" : null,
    "diffHunk" : "@@ -1,1 +1290,1294 @@// around by default 1) for historical reasons and 2) for the ability to rollback a deployment.\nfunc (dc *DeploymentController) cleanupDeployment(oldRSs []*extensions.ReplicaSet, deployment *extensions.Deployment) error {\n\tif deployment.Spec.RevisionHistoryLimit == nil {\n\t\treturn nil\n\t}"
  },
  {
    "id" : "40fee8d1-561d-46b9-a61d-10b19909d722",
    "prId" : 20273,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89ad8273-710a-4ab6-a7d8-968740d8a350",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What happens when this fails?  This call updates the argument replica field even if the update call fails, so it looks like the RS is updated in the deployment status, but it really isn't.  Since we can't perform all of the needed updates transactionally we need to sanely handle partial successes.\n",
        "createdAt" : "2016-05-18T20:30:09Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "470068e9-72a8-4966-bbf3-320247492409",
        "parentId" : "89ad8273-710a-4ab6-a7d8-968740d8a350",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Answering here for this and your comment above (https://github.com/kubernetes/kubernetes/pull/20273/files/a56748fc47e82c5c5f7b6d58fb2782193ed496bc#r63775306)\n\nIn short, you are right. I need to make sure that the deployment will be requeued and these errors should be returned from `scale`, I think without updating the status of the deployment. Will update to do this.\n",
        "createdAt" : "2016-05-18T21:10:03Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "f4dc07a5-c7aa-4385-a7e7-0e4e227cc142",
        "parentId" : "89ad8273-710a-4ab6-a7d8-968740d8a350",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Made `scale` bail out in case of a failed attempt to scale.\n",
        "createdAt" : "2016-05-19T09:46:32Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3d2e3ff2203da20053f9a104f3cce6571dbe293",
    "line" : null,
    "diffHunk" : "@@ -1,1 +500,504 @@\tif deploymentutil.IsSaturated(deployment, newRS) {\n\t\tfor _, old := range controller.FilterActiveReplicaSets(oldRSs) {\n\t\t\tif _, _, err := dc.scaleReplicaSetAndRecordEvent(old, 0, deployment); err != nil {\n\t\t\t\treturn err\n\t\t\t}"
  },
  {
    "id" : "055979d0-aac3-4d96-8d93-602d9241219a",
    "prId" : 20273,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7729b07-b568-436c-98eb-f00ccbbf5ca2",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Good idea, using `for i, value := range values` causes way to many bugs.\n",
        "createdAt" : "2016-05-19T02:03:47Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3d2e3ff2203da20053f9a104f3cce6571dbe293",
    "line" : null,
    "diffHunk" : "@@ -1,1 +548,552 @@\t\tdeploymentReplicasAdded := int32(0)\n\t\tfor i := range allRSs {\n\t\t\trs := allRSs[i]\n\n\t\t\tproportion := getProportion(rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded)"
  },
  {
    "id" : "04e19b23-405f-4b5c-96e8-24af617297ad",
    "prId" : 20273,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "055a8903-f832-423b-87c7-323fc43b5314",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What happens if a rollout and scale event are observed in the same sync?  Looking at the logs, it looks like the deployment gets requeued several times as part of the event.\n",
        "createdAt" : "2016-05-19T22:26:29Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "bbcfadcb-47e1-451e-9c57-1466c05be092",
        "parentId" : "055a8903-f832-423b-87c7-323fc43b5314",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Yes, this happens because every time there is an update on the underlying replica sets, the deployment is requeued.\n",
        "createdAt" : "2016-05-20T15:11:38Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "85677972-19cb-4fc4-80d1-c7018083ede1",
        "parentId" : "055a8903-f832-423b-87c7-323fc43b5314",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Ah that makes sense.  I suspect we will always want to re-queue-if-not-queued after scaling sync instead of relying upon updates to the RSs to cause the re-queue.  This would be for if when the sync is seeing both scaling and non-scaling changes.\n",
        "createdAt" : "2016-05-24T15:47:38Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "6108118c-1840-4aaf-aed6-4481b4d39543",
        "parentId" : "055a8903-f832-423b-87c7-323fc43b5314",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "We definitely want requeues on RS events. Imagine a RS with replicas is\ndeleted just after the resync interval. The deployment is going to notice\nit in the next relist if we didn't requeue.\n\nOn Tue, May 24, 2016 at 5:48 PM, Phillip Wittrock notifications@github.com\nwrote:\n\n> In pkg/controller/deployment/deployment_controller.go\n> https://github.com/kubernetes/kubernetes/pull/20273#discussion_r64418444\n> :\n> \n> > ```\n> > if d.Spec.RollbackTo != nil {\n> >     revision := d.Spec.RollbackTo.Revision\n> >     if _, err = dc.rollback(d, &revision); err != nil {\n> >         return err\n> >     }\n> > }\n> > ```\n> > -   if dc.isScalingEvent(d) {\n> > -       return dc.sync(d)\n> \n> Ah that makes sense. I suspect we will always want to\n> re-queue-if-not-queued after scaling sync instead of relying upon updates\n> to the RSs to cause the re-queue. This would be for if when the sync is\n> seeing both scaling and non-scaling changes.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/20273/files/288d0835dbcd37ad101db417cf8e4395713ea64a#r64418444\n",
        "createdAt" : "2016-05-24T15:56:33Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "8c0165c5-22b5-4acd-899a-ab9cf9c930b3",
        "parentId" : "055a8903-f832-423b-87c7-323fc43b5314",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "Agreed that we want to requeue on RS events, but this code is relying upon that side affect.  sync() doesn't provide a contract that it will re-queue the RS, it just happens to as an implementation detail, which is something we shouldn't have to know about if just reading this part of the code.  If the code structure was altered so that sync could potentially not update any RS, then the deployment would not get re-queued, and our assumptions would break.   If we want to rely upon that behavior, it should be part of the function's contract.\n\nBasically at this point in the code I want to be able to reason that the deployment will always be re-queued after the sync without necessarily knowing the implementation details of how or why.\n",
        "createdAt" : "2016-05-24T16:32:57Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "00b053a8-77cb-4048-8a89-c8b871a04682",
        "parentId" : "055a8903-f832-423b-87c7-323fc43b5314",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Why should the deployment be re-queued after each sync?\n\nOn Tue, May 24, 2016 at 6:33 PM, Phillip Wittrock notifications@github.com\nwrote:\n\n> In pkg/controller/deployment/deployment_controller.go\n> https://github.com/kubernetes/kubernetes/pull/20273#discussion_r64427267\n> :\n> \n> > ```\n> > if d.Spec.RollbackTo != nil {\n> >     revision := d.Spec.RollbackTo.Revision\n> >     if _, err = dc.rollback(d, &revision); err != nil {\n> >         return err\n> >     }\n> > }\n> > ```\n> > -   if dc.isScalingEvent(d) {\n> > -       return dc.sync(d)\n> \n> Agreed that we want to requeue on RS events, but this code is relying upon\n> that side affect. sync() doesn't provide a contract that it will re-queue\n> the RS, it just happens to as an implementation detail, which is something\n> we shouldn't have to know about if just reading this part of the code. If\n> the code structure was altered so that sync could potentially not update\n> any RS, then the deployment would not get re-queued, and our assumptions\n> would break. If we want to rely upon that behavior, it should be part of\n> the function's contract.\n> \n> Basically at this point in the code I want to be able to reason that the\n> deployment will always be re-queued after the sync without necessarily\n> knowing the implementation details of how or why.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/20273/files/288d0835dbcd37ad101db417cf8e4395713ea64a#r64427267\n",
        "createdAt" : "2016-05-24T17:08:47Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "d5e2b8b7-0467-453d-a453-093668694691",
        "parentId" : "055a8903-f832-423b-87c7-323fc43b5314",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "What happens if there are both scaling and non-scaling changes observed in the sync?  Is my understanding correct that we will do the scaling changes, but then return before doing the non-scaling changes.  If so, how do the non-scaling changes get synced?  The deployment would need to be re-queued.\n",
        "createdAt" : "2016-05-24T21:01:40Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "96787267-29b0-4cc5-8e7e-e52abeaf077e",
        "parentId" : "055a8903-f832-423b-87c7-323fc43b5314",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "The only difference between the rollout code and `scale` (apart from the way we scale) is that in `scale` we will not create a new replica set (ie. we will not react on pod template changes). This should always be the default behavior when the deployment is paused. When the deployment is unpaused then we will not enter [scale](https://github.com/kubernetes/kubernetes/pull/20273/commits/47ee2c8c64b0d7869d4a7c668313f9e2b46b3d7c) - a missing new rs means there is a change in the pod template of the deployment and we need to rollout.\n",
        "createdAt" : "2016-05-24T22:17:29Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3d2e3ff2203da20053f9a104f3cce6571dbe293",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +450,454 @@\n\tif dc.isScalingEvent(d) {\n\t\treturn dc.sync(d)\n\t}\n"
  },
  {
    "id" : "ea68bfdc-a6f4-4597-ad87-f9e3e167b065",
    "prId" : 20273,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c862ce8-c91c-4ac0-9583-84a137444d66",
        "parentId" : null,
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "So here is something crazy...  The old and new RS are read from the store which is actually a cache that gets updated asynchronously through a callback.  What that means is these results may actually be stale, and the true allRssReplicas could be different because one of the RS had been updated, but the cache was not updated yet.  In practice this should be uncommon as long at the update response time is low, but could push us out of the acceptable window.\n\nPossible solutions:\n- Do live reads for each RS so are not relying on the cache\n- Accept this as a possibility, figure out what the real window could be, document it, and file an issue once this is submitted.\n\nI would expect to see one of the RS updates fail if we see this due to a optimistic lock failure.\n",
        "createdAt" : "2016-06-14T01:45:59Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      },
      {
        "id" : "504aed97-f956-48b4-9100-c609f839efc3",
        "parentId" : "7c862ce8-c91c-4ac0-9583-84a137444d66",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "This is not a problem to address in this pull. We already update the deployment status based on what's found in the cache. Most if not all controllers already do this.\n",
        "createdAt" : "2016-06-14T08:21:31Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "dfcbf3e7-09c1-4eaf-b564-e4c5ecdcacf4",
        "parentId" : "7c862ce8-c91c-4ac0-9583-84a137444d66",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Also if we cannot break the cache, we shouldn't just not rely on it and start querying the server. If we can break the cache, we should fix the cache. And by \"breaking\" I mean all sorts of things (stale reads, misses, and what have you)\n",
        "createdAt" : "2016-06-14T08:27:39Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "183408cc-dc21-4057-9436-3405ca614c46",
        "parentId" : "7c862ce8-c91c-4ac0-9583-84a137444d66",
        "authorId" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "body" : "I am fine with punting on this, but we should document it both in the code and as an issue.\n\n> Also if we cannot break the cache, we shouldn't just not rely on it and start querying the server.\n\nI don't think this is true in all cases.  I agree this should be solved in the storage layer and not in the app business logic, but that is a harder problem.  This is also true of almost all of the complexity in this PR (multi-key read + write transactions & transactionally updated indexes would allow us to eliminate all of the complexity).\n",
        "createdAt" : "2016-06-14T17:20:52Z",
        "updatedAt" : "2016-06-20T10:13:43Z",
        "lastEditedBy" : "47ec15eb-72b4-4618-9e01-e3b37b6bac00",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3d2e3ff2203da20053f9a104f3cce6571dbe293",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +512,516 @@\tif deploymentutil.IsRollingUpdate(deployment) {\n\t\tallRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS))\n\t\tallRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)\n\n\t\tallowedSize := int32(0)"
  },
  {
    "id" : "fedfeb0e-dfe0-4053-b991-04d11b1f7346",
    "prId" : 20035,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7228355-db2e-4337-a4e0-124c0ea58a24",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Please add a comment explaining what/why this is doing.\n",
        "createdAt" : "2016-02-02T20:35:46Z",
        "updatedAt" : "2016-02-03T18:21:09Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "f0ee3fa3-c4eb-4ee9-b22c-bb230d7596b3",
        "parentId" : "b7228355-db2e-4337-a4e0-124c0ea58a24",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done\n",
        "createdAt" : "2016-02-02T21:20:43Z",
        "updatedAt" : "2016-02-03T18:21:09Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3cb44aaffca4eefceec6e3754a44d419c770736",
    "line" : null,
    "diffHunk" : "@@ -1,1 +680,684 @@\t\t}\n\t\t// Copy deployment's annotations to existing new RC\n\t\tannotationChanged := false\n\t\tfor k, v := range deployment.Annotations {\n\t\t\tif existingNewRC.Annotations[k] != v {"
  },
  {
    "id" : "501cf390-4c0b-4604-9a7b-6275be43d78c",
    "prId" : 19686,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb8535e0-5c62-4cb7-85c9-f60ebae1022a",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "godoc the difference between the two sets of rcs\n",
        "createdAt" : "2016-01-27T11:35:20Z",
        "updatedAt" : "2016-01-31T00:08:59Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "0d195b50-b376-4432-8cba-6288cd463cce",
        "parentId" : "bb8535e0-5c62-4cb7-85c9-f60ebae1022a",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done\n",
        "createdAt" : "2016-01-28T02:57:46Z",
        "updatedAt" : "2016-01-31T00:08:59Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3396db95106e3257946f78334017e6024e704b9b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +658,662 @@// getOldRCs returns two sets of old RCs of the deployment. The first set of old RCs doesn't include\n// the ones with no pods, and the second set of old RCs include all old RCs.\nfunc (dc *DeploymentController) getOldRCs(deployment extensions.Deployment) ([]*api.ReplicationController, []*api.ReplicationController, error) {\n\treturn deploymentutil.GetOldRCsFromLists(deployment, dc.client,\n\t\tfunc(namespace string, options api.ListOptions) (*api.PodList, error) {"
  },
  {
    "id" : "2387b4e5-1bb1-4e9f-9125-fc6f6cda3018",
    "prId" : 19686,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25183ba0-1591-4453-a2de-3ff1a1eade93",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I would want to know that my rollback didn't happen because I tried to roll to the same spec.\n",
        "createdAt" : "2016-01-27T11:42:11Z",
        "updatedAt" : "2016-01-31T00:08:59Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "0df98c87-c7ee-4e5c-96db-e10f90352392",
        "parentId" : "25183ba0-1591-4453-a2de-3ff1a1eade93",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Logged this with V(4).Info\n",
        "createdAt" : "2016-01-28T02:59:17Z",
        "updatedAt" : "2016-01-31T00:08:59Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3396db95106e3257946f78334017e6024e704b9b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +975,979 @@\t\tglog.V(4).Infof(\"Rolling back to a revision that contains the same template as current deployment %s, skipping rollback...\", deployment.Name)\n\t\tdc.emitRollbackWarningEvent(deployment, \"DeploymentRollbackTemplateUnchanged\", fmt.Sprintf(\"The rollback revision contains the same template as current deployment %q\", deployment.Name))\n\t}\n\td, err = dc.updateDeploymentAndClearRollbackTo(deployment)\n\treturn"
  },
  {
    "id" : "f863d2f0-28a0-4b74-9cfc-d185f6531520",
    "prId" : 19686,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c85e17ee-774f-44e9-abaa-66750e2ba5d5",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "godoc\n",
        "createdAt" : "2016-01-27T12:01:40Z",
        "updatedAt" : "2016-01-31T00:08:59Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "635281c9-e9e2-49e3-9d91-e8fe8ff95c97",
        "parentId" : "c85e17ee-774f-44e9-abaa-66750e2ba5d5",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done\n",
        "createdAt" : "2016-01-28T03:01:30Z",
        "updatedAt" : "2016-01-31T00:08:59Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3396db95106e3257946f78334017e6024e704b9b",
    "line" : null,
    "diffHunk" : "@@ -1,1 +578,582 @@// getNewRCAndMaybeFilteredOldRCs returns new RC and old RCs of the deployment. If ignoreNoPod is true,\n// the returned old RCs won't include the ones with no pods; otherwise, all old RCs will be returned.\nfunc (dc *DeploymentController) getNewRCAndMaybeFilteredOldRCs(deployment extensions.Deployment, ignoreNoPod bool) (*api.ReplicationController, []*api.ReplicationController, error) {\n\toldRCs, allOldRCs, err := dc.getOldRCs(deployment)\n\tif err != nil {"
  },
  {
    "id" : "7b3314c8-228f-4fd0-9c99-81e4762620f6",
    "prId" : 19590,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b8232f7-3e8e-4e9d-a2a5-45dc9a18c054",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Let's have the cleanup policy separated from the strategy code. How about moving this after the switch that picks a strategy?\n",
        "createdAt" : "2016-01-13T09:32:40Z",
        "updatedAt" : "2016-01-30T01:58:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "947cf29d-ac07-4064-8d20-ab194c269229",
        "parentId" : "8b8232f7-3e8e-4e9d-a2a5-45dc9a18c054",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Hm, we would need to make an additional list for the RCs vs not having to reimplement it for every strategy. Any thoughts here? @ironcladlou \n",
        "createdAt" : "2016-01-13T09:36:22Z",
        "updatedAt" : "2016-01-30T01:58:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "52a05a5f-3b8a-4ebd-a727-5ea5e6347abc",
        "parentId" : "8b8232f7-3e8e-4e9d-a2a5-45dc9a18c054",
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "> Let's have the cleanup policy separated from the strategy code. How about moving this after the switch that picks a strategy?\n> \n> Hm, we would need to make an additional list for the RCs vs not having to reimplement it for every strategy. Any thoughts here? \n\nDo you mean add a gc routine to delete the old RCs ?\n",
        "createdAt" : "2016-01-13T09:43:32Z",
        "updatedAt" : "2016-01-30T01:58:03Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      },
      {
        "id" : "89a16c35-37e7-4cbe-a3a5-1a604b762e79",
        "parentId" : "8b8232f7-3e8e-4e9d-a2a5-45dc9a18c054",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "> Do you mean add a gc routine to delete the old RCs ?\n\nNo need for goroutines, at least in the first place. Right now, you have baked the cleanup policy in the rolling strategy so you will have to reimplement it for recreate too. What I am saying is separate concerns and have the cleanup policy run after the strategy. But you would need to relist the old controllers in order to delete them. \n",
        "createdAt" : "2016-01-13T09:53:45Z",
        "updatedAt" : "2016-01-30T01:58:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "e9d005dc-9b63-444a-82ad-e846bf45d8c8",
        "parentId" : "8b8232f7-3e8e-4e9d-a2a5-45dc9a18c054",
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "You mean implement like this?\n\n```\nswitch d.Spec.Strategy.Type {\n    case extensions.RecreateDeploymentStrategyType:\n        if err:= dc.syncRecreateDeployment(d); err != nil {\n            return err\n        }\n    case extensions.RollingUpdateDeploymentStrategyType:\n        if err:= dc.syncRollingUpdateDeployment(d); err != nil {\n            return err\n        }\n    }\n}\n\n// Cleanup old rcs\nif deployment.Spec.RevisionHistoryLimit != nil {\n    return dc.cleanupOldRcs(deployment)\n}\n```\n\nI think it's not viable, since we can only delete old RCs after the RCs scaled down to 0, but `syncRollingUpdateDeployment` can return before scaled down, see https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L464\n",
        "createdAt" : "2016-01-13T10:04:58Z",
        "updatedAt" : "2016-01-30T01:58:04Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      },
      {
        "id" : "ad142874-4001-4115-bd67-86a960318db6",
        "parentId" : "8b8232f7-3e8e-4e9d-a2a5-45dc9a18c054",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Well, you could check for the replicas of the rcs before trying to delete but ultimately I think I prefer what you have now as it is cheaper than what I am proposing.\n",
        "createdAt" : "2016-01-13T10:20:16Z",
        "updatedAt" : "2016-01-30T01:58:04Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5cda2012a0f4d4aa1e78a63cc5352455c3333d0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +504,508 @@\t}\n\n\tif deployment.Spec.RevisionHistoryLimit != nil {\n\t\t// Cleanup old RCs\n\t\tdc.cleanupOldRcs(oldRCs, deployment)"
  },
  {
    "id" : "1e037c11-5c5d-4f23-babd-67821a261db8",
    "prId" : 19590,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a625205-3043-4e6c-b3e2-d4211aa364c6",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "RevisionHistoryLimit should not include the current/latest RC.\n\nIf RevisionHistoryLimit is 1, and there are both a current and previous RC, we shouldn't delete any.\n",
        "createdAt" : "2016-01-13T16:38:54Z",
        "updatedAt" : "2016-01-30T01:58:04Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "0afc24c2-302a-49bb-b7a5-250f42b81e67",
        "parentId" : "4a625205-3043-4e6c-b3e2-d4211aa364c6",
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "parameter `controllers` here are oldRCS, so it does **_NOT**_ include the current RC.\n",
        "createdAt" : "2016-01-14T01:55:48Z",
        "updatedAt" : "2016-01-30T01:58:04Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5cda2012a0f4d4aa1e78a63cc5352455c3333d0",
    "line" : null,
    "diffHunk" : "@@ -1,1 +721,725 @@func (dc *DeploymentController) cleanupOldRcs(oldRCs []*api.ReplicationController, deployment extensions.Deployment) error {\n\tdiff := len(oldRCs) - *deployment.Spec.RevisionHistoryLimit\n\tif diff <= 0 {\n\t\treturn nil\n\t}"
  },
  {
    "id" : "c58c801d-0545-4a25-84c8-f787812b1e3e",
    "prId" : 19590,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08f2fbdc-734f-472d-951d-679ed56022c2",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Since you already log failed deletions I guess it's fine not to check for the error here\n",
        "createdAt" : "2016-01-28T11:53:27Z",
        "updatedAt" : "2016-01-30T01:58:04Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5cda2012a0f4d4aa1e78a63cc5352455c3333d0",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +462,466 @@\tif deployment.Spec.RevisionHistoryLimit != nil {\n\t\t// Cleanup old RCs\n\t\tdc.cleanupOldRcs(oldRCs, deployment)\n\t}\n"
  },
  {
    "id" : "ccd58995-cc0a-4f51-8ce8-c8dc7e577cec",
    "prId" : 19581,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb194e85-2df5-4a34-befe-f51cf258c946",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "Atoi could also return an error. We should handle that.\n",
        "createdAt" : "2016-01-25T21:03:31Z",
        "updatedAt" : "2016-01-30T20:05:06Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "258cad91-09a4-47c6-8a51-40bb6126fbbb",
        "parentId" : "bb194e85-2df5-4a34-befe-f51cf258c946",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "As discussed the error is returned too. \n",
        "createdAt" : "2016-01-26T03:20:26Z",
        "updatedAt" : "2016-01-30T20:05:06Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "42f712467efa2f3f4b272ec3c288f3797ee13cad",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +544,548 @@\t\treturn 0, nil\n\t}\n\treturn strconv.Atoi(v)\n}\n"
  },
  {
    "id" : "2f5c26cd-3d10-4234-bec2-6817b2edfb15",
    "prId" : 19581,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3d1edaa-a1d8-431c-80cc-01fe3223a599",
        "parentId" : null,
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "I don't think we should bail here in the case that one RC isn't properly annotated. We should degrade gracefully -- just skip it. It won't appear in the revision history in kubectl and perhaps it wouldn't be possible to automatically roll back to that revision, but all the other behavior should work correctly.\n",
        "createdAt" : "2016-01-25T21:07:19Z",
        "updatedAt" : "2016-01-30T20:05:06Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "ac050e48-a095-48a0-a5e7-fdb13684a41d",
        "parentId" : "b3d1edaa-a1d8-431c-80cc-01fe3223a599",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done. Log this error and continue the calculation. \n",
        "createdAt" : "2016-01-26T03:21:03Z",
        "updatedAt" : "2016-01-30T20:05:06Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "42f712467efa2f3f4b272ec3c288f3797ee13cad",
    "line" : null,
    "diffHunk" : "@@ -1,1 +550,554 @@\tmax := 0\n\tfor _, rc := range allRCs {\n\t\tif v, err := revision(rc); err != nil {\n\t\t\t// Skip the RCs when it failed to parse their revision information\n\t\t\tglog.V(4).Infof(\"Error: %v. Couldn't parse revision for rc %#v, deployment controller will skip it when reconciling revisions.\", err, rc)"
  },
  {
    "id" : "62266c91-682d-4c85-ac8c-270e089785d6",
    "prId" : 19581,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a949af62-7b22-42ec-8409-fd9a5d537be0",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Won't these updates conflict with scaling of the controller?\n",
        "createdAt" : "2016-01-26T10:58:04Z",
        "updatedAt" : "2016-01-30T20:05:06Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "73dccbe2-534b-46a9-b216-415e05fc03b1",
        "parentId" : "a949af62-7b22-42ec-8409-fd9a5d537be0",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "The RC's revision annotation is updated before it get scaled. If there's update conflict it'll retry later before scaling it. \n",
        "createdAt" : "2016-01-26T19:25:24Z",
        "updatedAt" : "2016-01-30T20:05:06Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "9cebf6ab-b77f-4032-bd1e-e61a9f43f2dd",
        "parentId" : "a949af62-7b22-42ec-8409-fd9a5d537be0",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I think it's fine since the update call will return the rc with the latest resource version\n",
        "createdAt" : "2016-01-27T12:46:52Z",
        "updatedAt" : "2016-01-30T20:05:06Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "42f712467efa2f3f4b272ec3c288f3797ee13cad",
    "line" : 139,
    "diffHunk" : "@@ -1,1 +591,595 @@\t\t\texistingNewRC.Annotations[deploymentutil.RevisionAnnotation] = newRevision\n\t\t\tglog.V(4).Infof(\"update existingNewRC's revision to %s - %+v\\n\", newRevision, existingNewRC)\n\t\t\treturn dc.client.ReplicationControllers(deployment.ObjectMeta.Namespace).Update(existingNewRC)\n\t\t}\n\t\treturn existingNewRC, nil"
  },
  {
    "id" : "6dec0f00-c2a3-4f78-9bdf-7d42a4d440d9",
    "prId" : 19433,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5aa7492-d5a9-4952-9b5e-08568f0784d1",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "suggest also renaming the variables in controller_utils that indicate that expectations are only used for pods\n",
        "createdAt" : "2016-01-08T23:02:16Z",
        "updatedAt" : "2016-01-12T23:00:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "299ae9f2-733c-4126-9eda-c590cf019a5c",
        "parentId" : "d5aa7492-d5a9-4952-9b5e-08568f0784d1",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done\n",
        "createdAt" : "2016-01-12T21:47:25Z",
        "updatedAt" : "2016-01-12T23:00:11Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2463a5aefb3c1de671227a32334ca77509a2f0a",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +83,87 @@\t// A TTLCache of rc creates/deletes each deployment expects to see\n\t// TODO: make expectation model understand (rc) updates (besides adds and deletes)\n\trcExpectations controller.ControllerExpectationsInterface\n\n\t// Deployments that need to be synced"
  },
  {
    "id" : "257f7c31-0d71-40b1-a195-f6b4d04ceeb4",
    "prId" : 19382,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93232bc0-0021-446d-8ad3-3ac4f8056f67",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Can you extract this into a `needsStatusSync` function?\n",
        "createdAt" : "2016-01-20T10:15:06Z",
        "updatedAt" : "2016-01-25T22:36:45Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "d48d56ec-fb4d-42c0-a939-d975374e5a6f",
        "parentId" : "93232bc0-0021-446d-8ad3-3ac4f8056f67",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done\n",
        "createdAt" : "2016-01-20T18:37:13Z",
        "updatedAt" : "2016-01-25T22:36:45Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "90172919-51be-4662-8d78-7e1a4bb08d11",
        "parentId" : "93232bc0-0021-446d-8ad3-3ac4f8056f67",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "This is extracted in #19733 as well so I kept the same name, `syncDeploymentStatus`. \n",
        "createdAt" : "2016-01-20T18:37:54Z",
        "updatedAt" : "2016-01-25T22:36:45Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "2f776e45-c51c-4be2-ac1a-8fa900d634c2",
        "parentId" : "93232bc0-0021-446d-8ad3-3ac4f8056f67",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "ok\n",
        "createdAt" : "2016-01-21T09:22:41Z",
        "updatedAt" : "2016-01-25T22:36:45Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "591269c99c28205335f15541057ddfea98de7e1c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +506,510 @@\t\treturn fmt.Errorf(\"failed to count ready pods: %v\", err)\n\t}\n\tif deployment.Status.Replicas != totalReplicas || deployment.Status.UpdatedReplicas != updatedReplicas || deployment.Status.AvailableReplicas != availablePods {\n\t\treturn dc.updateDeploymentStatus(allRCs, newRC, deployment)\n\t}"
  },
  {
    "id" : "65270d80-f4ae-48de-9d81-9aecdef82940",
    "prId" : 19344,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "536baa85-5e0a-4e1d-9b4d-8ea5105f0329",
        "parentId" : null,
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Do we need to wait for the whole store to sync!\nWhat if just check that old and new RCs dont have any unsatisfied expectation?\n",
        "createdAt" : "2016-01-06T23:28:52Z",
        "updatedAt" : "2016-01-06T23:28:52Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c782aaa504ee3893dd83f32586b2b871fe8d06d8",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +389,393 @@\t}\n\td := *obj.(*extensions.Deployment)\n\tif !dc.rcStoreSynced() {\n\t\t// Sleep so we give the rc reflector goroutine a chance to run.\n\t\ttime.Sleep(RcStoreSyncedPollPeriod)"
  },
  {
    "id" : "5a6e0480-01dd-4957-8af6-af0b74dcc7f7",
    "prId" : 17398,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4530a69-81a8-4669-9006-3cf44eb32356",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Can a replication controller be owned by more than one deployment? This means that the deployments are using the same pod template but have different names?\n",
        "createdAt" : "2015-11-18T12:13:23Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "bf33415d-ef7a-43d6-834d-0ccff6945028",
        "parentId" : "d4530a69-81a8-4669-9006-3cf44eb32356",
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "Shouldn't be possible because the deployments have a unique label assigned to their RCs, but the code wants to account for the possibility anyway.\n",
        "createdAt" : "2015-11-18T13:56:06Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "83107499-cb53-4967-87a6-d0423e8f21db",
        "parentId" : "d4530a69-81a8-4669-9006-3cf44eb32356",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Then the comment should probably change to what you said if that's the case.\n",
        "createdAt" : "2015-11-18T14:02:18Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "d2a56dfa-b5e4-4253-bdfe-4591c04bb59a",
        "parentId" : "d4530a69-81a8-4669-9006-3cf44eb32356",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "The comments below (L199-203) describe that. \n",
        "createdAt" : "2015-11-19T00:25:00Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "9275fb77-081b-467f-b95c-4fcc435a77c1",
        "parentId" : "d4530a69-81a8-4669-9006-3cf44eb32356",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "ok\n",
        "createdAt" : "2015-11-19T10:20:28Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd1200ba66f29b4816836dd8d6f2fdc23126bb77",
    "line" : null,
    "diffHunk" : "@@ -1,1 +185,189 @@\n// getDeploymentForRC returns the deployment managing the given RC.\n// TODO: Surface that we are ignoring multiple deployments for a given controller.\nfunc (dc *DeploymentController) getDeploymentForRC(rc *api.ReplicationController) *extensions.Deployment {\n\tdeployments, err := dc.dStore.GetDeploymentsForRC(rc)"
  },
  {
    "id" : "d520cc3f-b308-4a5a-a93f-9423a4a8a887",
    "prId" : 17398,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "836c3315-33c0-4b74-811b-d53977b60c78",
        "parentId" : null,
        "authorId" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "body" : "Nit across the board: use godoc conventions:\n\n``` go\n// VariableOrFieldName <explanation>\n```\n",
        "createdAt" : "2015-11-18T12:58:31Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "f04ec747-f3ff-4334-a66e-6daaf4497091",
        "tags" : [
        ]
      },
      {
        "id" : "18308c26-aab7-4b18-a645-48ff5cf43318",
        "parentId" : "836c3315-33c0-4b74-811b-d53977b60c78",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Updated\n",
        "createdAt" : "2015-11-19T00:24:15Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd1200ba66f29b4816836dd8d6f2fdc23126bb77",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +39,43 @@)\n\nconst (\n\t// FullDeploymentResyncPeriod means we'll attempt to recompute the required replicas\n\t// of all deployments that have fulfilled their expectations at least this often."
  },
  {
    "id" : "a0dc8359-e426-4aa5-9e1b-070b146b3a2a",
    "prId" : 17398,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f8ab9d5-c9de-4725-9130-e7624cbf7a32",
        "parentId" : null,
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "https://github.com/kubernetes/kubernetes/pull/15153 removed this from all existing controllers at that time and replaced it by 12 hour unified period for all controllers.\ncc @wojtek-t \n",
        "createdAt" : "2015-11-19T05:34:49Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "8a3ea75c-7391-45b2-ad34-ab0244fd5cf0",
        "parentId" : "2f8ab9d5-c9de-4725-9130-e7624cbf7a32",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "It's 12h by default - in (non-scalability) e2e tests we are using something much smaller to test the \"resync\" path (I think it is 5 minutes). But yes - it would be great if you could change it would be great. You can take a look here:\nhttps://github.com/kubernetes/kubernetes/blob/master/cmd/kube-controller-manager/app/controllermanager.go#L267\nand see how we are using ResyncPeriod function.\n",
        "createdAt" : "2015-11-19T07:27:03Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "7bdb00fd-05f6-4284-817f-ea8866402f35",
        "parentId" : "2f8ab9d5-c9de-4725-9130-e7624cbf7a32",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Fixed. I kept `FullDeploymentResyncPeriod` like other controllers do, removed `ControllerRelistPeriod` and `PodRelistPeriod` below, and replaced them with `resyncPeriod` function in `NewDeploymentController`. \n",
        "createdAt" : "2015-11-19T21:08:33Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd1200ba66f29b4816836dd8d6f2fdc23126bb77",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +43,47 @@\t// of all deployments that have fulfilled their expectations at least this often.\n\t// This recomputation happens based on contents in the local caches.\n\tFullDeploymentResyncPeriod = 30 * time.Second\n)\n"
  },
  {
    "id" : "9475d16f-b2d4-4918-8fd8-3d18173386d0",
    "prId" : 17398,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9652195-9dcf-44b0-ad90-5b7dae817cfa",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "@wojtek-t @nikhiljindal I changed this to use resyncPeriod() as advised. \n",
        "createdAt" : "2015-11-19T21:12:16Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd1200ba66f29b4816836dd8d6f2fdc23126bb77",
    "line" : null,
    "diffHunk" : "@@ -1,1 +128,132 @@\t\t},\n\t\t&api.ReplicationController{},\n\t\tresyncPeriod(),\n\t\tframework.ResourceEventHandlerFuncs{\n\t\t\tAddFunc:    dc.addRC,"
  },
  {
    "id" : "b78ae503-3369-4c7e-9083-03320c681871",
    "prId" : 17398,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd146a0b-a897-49ca-be46-67aebdab7bcc",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "@wojtek-t @nikhiljindal I changed this to use resyncPeriod() as advised. \n",
        "createdAt" : "2015-11-19T21:12:25Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd1200ba66f29b4816836dd8d6f2fdc23126bb77",
    "line" : null,
    "diffHunk" : "@@ -1,1 +146,150 @@\t\t},\n\t\t&api.Pod{},\n\t\tresyncPeriod(),\n\t\tframework.ResourceEventHandlerFuncs{\n\t\t\t// When pod updates (becomes ready), we need to enqueue deployment"
  },
  {
    "id" : "2c30e516-71bc-42b4-8dbe-d2243cacbaf6",
    "prId" : 17398,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec72f05e-a7c4-4dbf-a4d5-2e9d42c7c5b9",
        "parentId" : null,
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "This should be before checking exists.\nWe should be checking exists only if err == nil\n",
        "createdAt" : "2015-11-23T21:32:08Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "b05be74e-bf42-4419-98fc-322afa2ab307",
        "parentId" : "ec72f05e-a7c4-4dbf-a4d5-2e9d42c7c5b9",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Fixed\n",
        "createdAt" : "2015-11-24T01:36:48Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd1200ba66f29b4816836dd8d6f2fdc23126bb77",
    "line" : null,
    "diffHunk" : "@@ -1,1 +365,369 @@\n\tobj, exists, err := dc.dStore.Store.GetByKey(key)\n\tif err != nil {\n\t\tglog.Infof(\"Unable to retrieve deployment %v from store: %v\", key, err)\n\t\tdc.queue.Add(key)"
  },
  {
    "id" : "97ee4262-4376-46ce-9ba8-77dd05c42dc9",
    "prId" : 17398,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3df3792b-40fa-4739-af46-0d14c1c169d9",
        "parentId" : null,
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "We should also update status if deployment.status.replicas != desiredReplicas or deployment.status.updatedReplicas != desiredReplicas.\nThat should fix the e2e failure that you are seeing.\n",
        "createdAt" : "2015-11-23T21:59:27Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "ce29953a-c93e-4139-bd8b-757c19b583f0",
        "parentId" : "3df3792b-40fa-4739-af46-0d14c1c169d9",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done\n",
        "createdAt" : "2015-11-24T01:37:01Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "2e5c3750-b93b-4e18-b3ff-f683a6a2a2e3",
        "parentId" : "3df3792b-40fa-4739-af46-0d14c1c169d9",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Note that we should call updateDeploymentStatus only when the condition I stated is correct.\nBefore your change, updateDeploymentStatus was only called when some RCs were scaled up or down and hence it always calls client.Deployment.UpdateStatus() without checking if there is any change in status.\n",
        "createdAt" : "2015-12-01T19:50:20Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "deed9f14-152b-4609-8259-5268ba59e287",
        "parentId" : "3df3792b-40fa-4739-af46-0d14c1c169d9",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Done\n",
        "createdAt" : "2015-12-01T21:47:26Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "4c192f0b-b8b0-44eb-a4bc-55cecf69257b",
        "parentId" : "3df3792b-40fa-4739-af46-0d14c1c169d9",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Sorry I actually take it back. This was not required for fixing your e2e failures.\n(I thought we were recording ready pods, but I now realise we are only recording rc.replicas).\nBut its fine to have this to keep the status correct in case of external scaling of RC.\n",
        "createdAt" : "2015-12-02T22:15:21Z",
        "updatedAt" : "2015-12-18T03:59:03Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd1200ba66f29b4816836dd8d6f2fdc23126bb77",
    "line" : null,
    "diffHunk" : "@@ -1,1 +430,434 @@\t}\n\n\t// TODO: raise an event, neither scaled up nor down.\n\treturn nil\n}"
  },
  {
    "id" : "1338a034-7f48-472e-9e54-703e32c349c1",
    "prId" : 16713,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ad6996c-eff3-4d30-a847-ff4de678e1c3",
        "parentId" : null,
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "Bail out here too, if an error occurs.\n",
        "createdAt" : "2015-11-05T22:39:49Z",
        "updatedAt" : "2016-01-13T03:27:50Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "92798408af90569f492be3a1a4d8de02538a6787",
    "line" : null,
    "diffHunk" : "@@ -1,1 +405,409 @@\treturn fmt.Errorf(\"unexpected deployment strategy type: %s\", d.Spec.Strategy.Type)\n}\n\nfunc (dc *DeploymentController) syncRecreateDeployment(deployment extensions.Deployment) error {\n\tnewRC, err := dc.getNewRC(deployment)"
  }
]