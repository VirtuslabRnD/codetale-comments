[
  {
    "id" : "20b94633-7e46-462e-9589-aed7e309adb2",
    "prId" : 32210,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d6974cc-da10-45be-aa2c-1427a2763432",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Looks like this can be implemented with `pkg/util/wait.WaitFor`.\n",
        "createdAt" : "2016-09-08T12:41:03Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "373f0ea7-5519-4aa9-92d0-dd9bfdde5502",
        "parentId" : "1d6974cc-da10-45be-aa2c-1427a2763432",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> Looks like this can be implemented with pkg/util/wait.WaitFor.\n\nI don't know that its net easier.  The `select` bit comes the wait func with some awkward reswizzling and the inner loop becomes the condition func.  You prefer the anonymous functions? \n",
        "createdAt" : "2016-09-08T12:54:19Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "10067914-cc10-42fa-9760-e81011d2e7c8",
        "parentId" : "1d6974cc-da10-45be-aa2c-1427a2763432",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Something along these lines: \n\n``` go\nAnd := func(cacheSyncs ...InformerSynced) bool {\n  allSynced := true\n  for _, syncFunc := range cacheSyncs {\n    allSynced = allSynced && syncFunc()\n  }\n  return allSynched\n}\nwait.WaitFor(wait.poller(syncedPollPeriod, 0), And(cacheSyncs...), stopCh)\n```\n",
        "createdAt" : "2016-09-08T13:26:55Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "d74dc6e8-1d73-4cfd-8957-6b10ffaaed73",
        "parentId" : "1d6974cc-da10-45be-aa2c-1427a2763432",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "I'll factor it that way, but its not that clean because we have to stop on a separate channel.\n",
        "createdAt" : "2016-09-08T13:34:04Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "e2f702b6-bc2a-4bed-b073-45a6f73c87de",
        "parentId" : "1d6974cc-da10-45be-aa2c-1427a2763432",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "`func PollInfinite(interval time.Duration, condition ConditionFunc) error` does what you want, with the exception that it keeps the stop channel for itself. Maybe, add a variant with a stop channel.\n",
        "createdAt" : "2016-09-08T14:38:31Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "89720ed1-bd2f-4c51-8c05-528c48fb8391",
        "parentId" : "1d6974cc-da10-45be-aa2c-1427a2763432",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "the code looks prettier here\n",
        "createdAt" : "2016-09-09T15:17:00Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "f8940ba8-b192-483d-960e-d1a2f7986d4c",
        "parentId" : "1d6974cc-da10-45be-aa2c-1427a2763432",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Yes, pretty readable IMO.\n",
        "createdAt" : "2016-09-09T15:38:56Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "385831825bf1d2b2546988965a2f36b0cfd49e1f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +87,91 @@// WaitForCacheSync waits for caches to populate.  It returns true if it was successful, false\n// if the contoller should shutdown\nfunc WaitForCacheSync(stopCh <-chan struct{}, cacheSyncs ...InformerSynced) bool {\n\terr := wait.PollUntil(syncedPollPeriod,\n\t\tfunc() (bool, error) {"
  },
  {
    "id" : "5ff85b82-d5b6-41f3-a7c8-8a95c1e7ad92",
    "prId" : 28379,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82b7270c-3001-4ace-a4e1-6d38a16456f4",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This one is OK, but blockDeltas should also block sending deltas, which is currently doesn't.\n",
        "createdAt" : "2016-07-05T11:08:41Z",
        "updatedAt" : "2016-07-05T11:38:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "d66186a5-c851-4d7f-893e-80abc12bf927",
        "parentId" : "82b7270c-3001-4ace-a4e1-6d38a16456f4",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> This one is OK, but blockDeltas should also block sending deltas, which is currently doesn't.\n\nFixed.  Bad port from downstream :(\n",
        "createdAt" : "2016-07-05T11:38:21Z",
        "updatedAt" : "2016-07-05T11:38:21Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "099b7f8fb2897868eed29af76d7bb22ed00f067d",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +216,220 @@\t// 4. unblock\n\ts.blockDeltas.Lock()\n\tdefer s.blockDeltas.Unlock()\n\n\tlistener := newProcessListener(handler)"
  },
  {
    "id" : "2d28a683-4859-45c5-b6d1-a040c6276fdd",
    "prId" : 26980,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8f6897c-28a9-46ed-ad78-e5dbeeeeafb9",
        "parentId" : null,
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "can we get rid of the func() by not using defer?\n",
        "createdAt" : "2016-06-07T18:23:11Z",
        "updatedAt" : "2016-06-08T18:35:04Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d4eb48c0bb85bf8ceb4edce744971e0cc9dcf69a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +283,287 @@\t\tblockingGet := func() (interface{}, bool) {\n\t\t\tp.lock.Lock()\n\t\t\tdefer p.lock.Unlock()\n\n\t\t\tfor len(p.pendingNotifications) == 0 {"
  },
  {
    "id" : "9edecf63-6ef0-4657-a395-375dac8bfddc",
    "prId" : 23795,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "554bae30-8edb-4f7a-b39a-29411efc7dba",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "I'd rather keep the `NewSharedInformer` method and have it delegate to this new one.\n",
        "createdAt" : "2016-04-19T16:48:40Z",
        "updatedAt" : "2016-05-06T13:23:39Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "00c0b546-2d44-4669-927f-345ec0ca7df8",
        "parentId" : "554bae30-8edb-4f7a-b39a-29411efc7dba",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "Actually, I don't want to initialize this with `Indexers`.  Users of this type should use the `AddIndexer` function.\n",
        "createdAt" : "2016-04-19T16:49:10Z",
        "updatedAt" : "2016-05-06T13:23:39Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "93b46200-deb6-4a57-a7b3-acf6f76f7c13",
        "parentId" : "554bae30-8edb-4f7a-b39a-29411efc7dba",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "1. I agree we definitely need function: NewSharedInformer() that is probably calling:\n   NewSharedIndexInformer(..., Indexers{})\n2. Although, I think that for this function, have it to take Indexers as arguments is a good idea - you can always pass an empty set of indexers.\n3. And I also agree, that AddIndexer() method is something we would like to have.\n",
        "createdAt" : "2016-04-20T08:13:49Z",
        "updatedAt" : "2016-05-06T13:23:39Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "1b136507-7616-45fb-854c-2f44e86f1bf1",
        "parentId" : "554bae30-8edb-4f7a-b39a-29411efc7dba",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Regarding 1 - the underlying implementation can still be index informer - I'm just talking about helper function.\n",
        "createdAt" : "2016-04-20T08:17:17Z",
        "updatedAt" : "2016-05-06T13:23:39Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c10f43a2e54fc555c93526b0e1f20c60f5d9aa9d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +71,75 @@// TODO: create a cache/factory of these at a higher level for the list all, watch all of a given resource that can\n// be shared amongst all consumers.\nfunc NewSharedIndexInformer(lw cache.ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, indexers cache.Indexers) SharedIndexInformer {\n\tsharedIndexInformer := &sharedIndexInformer{\n\t\tprocessor:        &sharedProcessor{},"
  },
  {
    "id" : "ec53f2ca-5292-4bc5-9ecb-42d5b1dcf1f1",
    "prId" : 23795,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "707ec3b6-862f-4966-bf06-9a79799045b4",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Can you please be consistent with NewSharedInformer (i.e. it creates controller).\n",
        "createdAt" : "2016-05-05T10:20:21Z",
        "updatedAt" : "2016-05-06T13:23:39Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "c10f43a2e54fc555c93526b0e1f20c60f5d9aa9d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +79,83 @@\t\tfullResyncPeriod: resyncPeriod,\n\t}\n\treturn sharedIndexInformer\n}\n"
  },
  {
    "id" : "7fd03d31-e07c-4094-9bce-7a53f1710c24",
    "prId" : 23795,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c02f3a0-9a26-4263-bb53-2f6ff0793861",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "can you add a comment for this block, explaining that it is tracked to handle late initialization of the controller?\n",
        "createdAt" : "2016-05-05T11:42:25Z",
        "updatedAt" : "2016-05-06T13:23:39Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c10f43a2e54fc555c93526b0e1f20c60f5d9aa9d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +89,93 @@\n\t// This block is tracked to handle late initialization of the controller\n\tlisterWatcher    cache.ListerWatcher\n\tobjectType       runtime.Object\n\tfullResyncPeriod time.Duration"
  },
  {
    "id" : "f0e2dd94-58f9-422f-875c-fa3018a79412",
    "prId" : 23795,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54f1fce8-fec3-4303-8a4f-f488ccae9973",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "godoc explaining that you can old add indexers _before_ the informer starts.\n",
        "createdAt" : "2016-05-05T11:43:12Z",
        "updatedAt" : "2016-05-06T13:23:39Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c10f43a2e54fc555c93526b0e1f20c60f5d9aa9d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +50,54 @@\tSharedInformer\n\t// AddIndexers add indexers to the informer before it starts.\n\tAddIndexers(indexers cache.Indexers) error\n\tGetIndexer() cache.Indexer\n}"
  },
  {
    "id" : "544cbcd1-9ddc-4d75-b5d7-3df8aed9ff72",
    "prId" : 23575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3ac8c75-5f90-48bc-b558-9162be1703cf",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I am not sure it matters, but is there an ordering semantic on when handlers are invoked?  in-sequence or parallel?  Also wondering if I want to have the opportunity to remove a handler or not, so they would require some identifier that the SharedInformer hands back to me that I can later use to remove it.\n",
        "createdAt" : "2016-03-31T19:07:52Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "3212b788-b078-4919-b2be-af5fb26d9bdf",
        "parentId" : "d3ac8c75-5f90-48bc-b558-9162be1703cf",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "I think I'd like to punt on removal until we get a use-case.  Every case I can think of runs forever.\n",
        "createdAt" : "2016-04-01T00:03:49Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "c731b339-ed70-4789-8a74-03582813002f",
        "parentId" : "d3ac8c75-5f90-48bc-b558-9162be1703cf",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I think it's fine to assume that you can NOT unregister a handler - I don't see any usecase for it and it seems to simplfy things\n\nRegarding the semantics of invoking them, I think that we should put an invariant that:\n- handlers from a \"single client\" are delivered sequentially\n- there are no assumptions between clients - for a given event it may be delivered in parallel to different clients.\n\nWDYT?\n",
        "createdAt" : "2016-04-11T09:03:30Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "c8f3343e-f8ea-42ba-a3f2-1c3e0bf936da",
        "parentId" : "d3ac8c75-5f90-48bc-b558-9162be1703cf",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> Regarding the semantics of invoking them, I think that we should put an invariant that:\n> \n> handlers from a \"single client\" are delivered sequentially\n> there are no assumptions between clients - for a given event it may be delivered in parallel to different clients.\n> WDYT?\n\nYes, I think that's reasonable.\n",
        "createdAt" : "2016-04-11T12:09:24Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0c33d65b6fea84d791059d96c93a15e4b3de4ec",
    "line" : null,
    "diffHunk" : "@@ -1,1 +39,43 @@\t// You may NOT add a handler *after* the SharedInformer is running.  That will result in an error being returned.\n\t// TODO we should try to remove this restriction eventually.\n\tAddEventHandler(handler ResourceEventHandler) error\n\tGetStore() cache.Store\n\t// GetController gives back a synthetic interface that \"votes\" to start the informer"
  },
  {
    "id" : "b363ea41-83ed-4ae5-9ff2-5712b811f48c",
    "prId" : 23575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb389aa7-c2bc-42d6-8feb-f91fed47eb95",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "I think it would be nice to have a registry of these that we can use to look them up from so we can create new ones only if needed, etc.\n",
        "createdAt" : "2016-03-31T19:11:17Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "262fe7ae-f13d-41b6-8bc6-a55fda4a2ee8",
        "parentId" : "cb389aa7-c2bc-42d6-8feb-f91fed47eb95",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> I think it would be nice to have a registry of these that we can use to look them up from so we can create new ones only if needed, etc.\n\nI agree, but I see that as another layer built in a later pull after we've taken the easy bits in the controller manager\n",
        "createdAt" : "2016-04-01T00:01:42Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "0bf041e9-3fef-4973-abe2-f1576f885c9b",
        "parentId" : "cb389aa7-c2bc-42d6-8feb-f91fed47eb95",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : ":+1: \n",
        "createdAt" : "2016-04-04T17:46:36Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "1d4e47f6-08b7-4f94-9f6b-c3dc65df27fc",
        "parentId" : "cb389aa7-c2bc-42d6-8feb-f91fed47eb95",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "+1 for the idea, but I don't think we should do it now\n",
        "createdAt" : "2016-04-11T09:04:12Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "26364f23-691b-42f7-b349-888701de115b",
        "parentId" : "cb389aa7-c2bc-42d6-8feb-f91fed47eb95",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Maybe a TODO for the future improvement would be nice\n",
        "createdAt" : "2016-04-11T09:04:39Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0c33d65b6fea84d791059d96c93a15e4b3de4ec",
    "line" : null,
    "diffHunk" : "@@ -1,1 +57,61 @@// TODO: create a cache/factory of these at a higher level for the list all, watch all of a given resource that can\n// be shared amongst all consumers.\nfunc NewSharedInformer(lw cache.ListerWatcher, objType runtime.Object, resyncPeriod time.Duration) SharedInformer {\n\tsharedInformer := &sharedInformer{\n\t\tprocessor: &sharedProcessor{},"
  },
  {
    "id" : "608bf0a1-3c3b-406b-99eb-7df06fefb460",
    "prId" : 23575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57fc9fc5-5de5-4b1e-b79e-9bf0c5ab9f63",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "OK - so you are explicitly disallowing registering handler functions after starting an informer.\nThis seems ok for our usecases, but we need to explicitly document it.\n",
        "createdAt" : "2016-04-11T09:16:36Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "2355d45a-d37a-4b30-8322-7bc26cc47f78",
        "parentId" : "57fc9fc5-5de5-4b1e-b79e-9bf0c5ab9f63",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> OK - so you are explicitly disallowing registering handler functions after starting an informer.\n> This seems ok for our usecases, but we need to explicitly document it.\n\nDone in the interface docs\n",
        "createdAt" : "2016-04-14T18:16:53Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0c33d65b6fea84d791059d96c93a15e4b3de4ec",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +154,158 @@\n\tif s.started {\n\t\treturn fmt.Errorf(\"informer has already started\")\n\t}\n"
  },
  {
    "id" : "3ec60e5b-b31e-4126-b8bc-c899d2c859f6",
    "prId" : 23575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3aa4bb29-4f77-4f21-9d01-cf654178312b",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "yeah.. the problem is that a single stuck controller may block all other controllers, which is bad.\nI don't think infinite slice is a good idea, as this may lead to OOMs.\n\nWhat I would suggest is to have some buffered channel per controller (listener) and then distribute doing TryInserts to that channel. If that TryInsert fails because of full buffer, this should force the controller to restart/resync from the beginning. The problem with that approach is that we would need an ability to add/remove listener when the informer is already running. So maybe this is a usecase for it and we should implement it?\n",
        "createdAt" : "2016-04-11T09:22:54Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "37296b9b-be78-4509-99ec-bf385c9913aa",
        "parentId" : "3aa4bb29-4f77-4f21-9d01-cf654178312b",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> yeah.. the problem is that a single stuck controller may block all other controllers, which is bad.\n> I don't think infinite slice is a good idea, as this may lead to OOMs.\n\nWe use infinite slices today, so this wouldn't be a new problem.  A stuck controller will block its incoming queue, which is an unbounded slice.  This just made the problem a little more obvious.\n\nWould you be ok with an unbounded slice implementation to start and we can later think through a re-join scenario?  The first implementation that occurs to me requires a slice as large as the existing cache and a full lock while while we prime the notification list.\n",
        "createdAt" : "2016-04-11T12:14:51Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "6dd80d8c-1edb-4c8b-9e36-1b1e82d43ddf",
        "parentId" : "3aa4bb29-4f77-4f21-9d01-cf654178312b",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "> We use infinite slices today, so this wouldn't be a new problem. A stuck controller will block its incoming queue, which is an unbounded slice. This just made the problem a little more obvious.\n\nI don't understand - where do we have an unbound slice? We are using channels everywhere and they have a limitted buffer size (or even no buffer at all, which requires full synchronization of reader and writer).\nCan you please clarify?\n\n> Would you be ok with an unbounded slice implementation to start and we can later think through a re-join scenario? The first implementation that occurs to me requires a slice as large as the existing cache and a full lock while while we prime the notification list.\n\nI think a TODO for the future is fine, but I think we need a wider agreement on it. @lavalamp @smarterclayton ?\n",
        "createdAt" : "2016-04-11T12:20:54Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "e3486d9f-4557-4c19-a862-02a6b067fe8f",
        "parentId" : "3aa4bb29-4f77-4f21-9d01-cf654178312b",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> I don't understand - where do we have an unbound slice? We are using channels everywhere and they have a limitted buffer size (or even no buffer at all, which requires full synchronization of reader and writer).\n> Can you please clarify?\n\nFeeding `DeltaFIFO`s was the example that I thought of.  The current impl of the `Informer` wires an unbounded `DeltaFIFO` queue up to the reflector.  Wiring here: https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/framework/controller.go#L219, unbounded queue defined here: https://github.com/kubernetes/kubernetes/blob/master/pkg/client/cache/delta_fifo.go#L101 and used here: https://github.com/kubernetes/kubernetes/blob/master/pkg/client/cache/delta_fifo.go#L373.\n",
        "createdAt" : "2016-04-11T12:32:48Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "d51403c2-6a91-4ad6-922b-b01edbe203f2",
        "parentId" : "3aa4bb29-4f77-4f21-9d01-cf654178312b",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "got it - thanks for explanation\n\nSo as I wrote above, I'm fine with TODO for the future and starting with the approach you have here.\n",
        "createdAt" : "2016-04-11T12:39:34Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0c33d65b6fea84d791059d96c93a15e4b3de4ec",
    "line" : null,
    "diffHunk" : "@@ -1,1 +193,197 @@\nfunc (p *sharedProcessor) distribute(obj interface{}) {\n\tfor _, listener := range p.listeners {\n\t\tlistener.add(obj)\n\t}"
  },
  {
    "id" : "e8fb6221-c9b4-41e4-a6d6-c96c22c545a4",
    "prId" : 23575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7cf1d8b-b843-4056-adb9-8408733e6609",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "shouldn't we have some return here?\n",
        "createdAt" : "2016-04-11T09:23:20Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0c33d65b6fea84d791059d96c93a15e4b3de4ec",
    "line" : null,
    "diffHunk" : "@@ -1,1 +261,265 @@\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tcase p.nextCh <- notification:"
  },
  {
    "id" : "6c06d550-d60f-4d61-b398-f2fa5b1d2027",
    "prId" : 23575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4201894a-87bb-40d2-841c-a4fd2e2bf156",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "It seems that while waiting for next() element, you won't be able to stop this processor, which is bad.\n\nWe should change next(), to sth like \"nextCh\" and make it return a channel that it will be putting elements to and then here we will have possibility to select on that channel and stopCh.\n\nWDYT?\n",
        "createdAt" : "2016-04-15T08:21:14Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "e4074990-0956-4c8a-a304-7e59ca032e03",
        "parentId" : "4201894a-87bb-40d2-841c-a4fd2e2bf156",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> We should change next(), to sth like \"nextCh\" and make it return a channel that it will be putting elements to and then here we will have possibility to select on that channel and stopCh.\n> \n> WDYT?\n\nMy first naive implementations were resulting in per-item channels (lots of garbage) and/or per-item gofuncs. Ended up with a static channel and one additional gofunc.  If that doesn't suit, can you open a pull to demonstrate what you mean?  I'm not seeing it.\n",
        "createdAt" : "2016-04-15T18:47:02Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0c33d65b6fea84d791059d96c93a15e4b3de4ec",
    "line" : null,
    "diffHunk" : "@@ -1,1 +274,278 @@\t\tvar next interface{}\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfunc() {\n\t\t\t\tp.lock.Lock()"
  },
  {
    "id" : "e3c1bcd6-ac4d-45cd-b816-0dbbb06d3e09",
    "prId" : 23575,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "574bde49-a14b-4289-bd88-d4552db44a8a",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Sorry, I probably wasn't clear enough previously. But basically what you did here is exactly what I was asking for :)\nThanks a lot!\n",
        "createdAt" : "2016-04-18T09:38:34Z",
        "updatedAt" : "2016-04-18T12:51:59Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0c33d65b6fea84d791059d96c93a15e4b3de4ec",
    "line" : 284,
    "diffHunk" : "@@ -1,1 +282,286 @@\t\t\treturn\n\t\tcase next = <-p.nextCh:\n\t\t}\n\n\t\tswitch notification := next.(type) {"
  }
]