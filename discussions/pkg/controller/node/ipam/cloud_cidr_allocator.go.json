[
  {
    "id" : "cc46e00e-3bb0-4321-96a6-daf62027748c",
    "prId" : 56336,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56336#pullrequestreview-78891894",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eaa0b147-a6be-4474-add7-ba804b902e30",
        "parentId" : null,
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "Why are you renaming this to utilnode?",
        "createdAt" : "2017-11-24T11:22:22Z",
        "updatedAt" : "2017-11-24T11:46:29Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "ebed4ae5-b5b7-4833-8700-bfcf361ad573",
        "parentId" : "eaa0b147-a6be-4474-add7-ba804b902e30",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "That's the convention we're using in the whole codebase - the alias for package is suffix of a path (without '/').",
        "createdAt" : "2017-11-24T11:26:12Z",
        "updatedAt" : "2017-11-24T11:46:29Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "878a3f8f-0545-44c8-89a8-e95e965f2123",
        "parentId" : "eaa0b147-a6be-4474-add7-ba804b902e30",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "You might then want to also change it in range_allocator.go then.\r\nBtw - there are many other places in the code where we're using 'nodeutil'. Guess they can be changed as part of some bigger cleanup then.",
        "createdAt" : "2017-11-24T11:31:28Z",
        "updatedAt" : "2017-11-24T11:46:29Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "9582f10f-7c3f-41c7-a5f3-75b560c7314f",
        "parentId" : "eaa0b147-a6be-4474-add7-ba804b902e30",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Let's not change more in this PR - we are already in code freeze - let's only fix bugs here.",
        "createdAt" : "2017-11-24T11:32:44Z",
        "updatedAt" : "2017-11-24T11:46:29Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "38597a26a8475195b6381b8468f2001373b220cd",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +42,46 @@\t\"k8s.io/kubernetes/pkg/controller\"\n\t\"k8s.io/kubernetes/pkg/controller/node/util\"\n\tutilnode \"k8s.io/kubernetes/pkg/util/node\"\n)\n"
  },
  {
    "id" : "bd50ca7f-b9ed-4852-8f1f-72e7394209f5",
    "prId" : 56336,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56336#pullrequestreview-78900541",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae20d613-9637-4c09-9cb5-b5c39cc78d3c",
        "parentId" : null,
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "IIUC we can still let the condition remain unset due to race. Consider the following scenario:\r\n\r\n- on the first call to `AllocateOrOccupyCIDR`, we set the CIDR and the node condition (but the function hasn't returned yet, so we haven't yet done `removeNodeFromProcessing` for the node)\r\n- some change from somewhere else overwrites the node condition (the above function still didn't return)\r\n- we get the watch event for the 2nd step and reach here.. enter `AllocateOrOccupyCIDR`  (because the condition is lost), but not do anything as the node is still in the processing\r\n\r\nWe probably can live with this? (assuming that there will be some node update soon'ish which would fix it)",
        "createdAt" : "2017-11-24T12:01:48Z",
        "updatedAt" : "2017-11-24T12:01:48Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "35c3a0f9-d160-45ec-ada3-a020e8787a34",
        "parentId" : "ae20d613-9637-4c09-9cb5-b5c39cc78d3c",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Yup - this is completely fine, given that node updates are pretty frequent by design. This can introduce 10s delay or so, but that's not a big deal - it will be fixed eventually.",
        "createdAt" : "2017-11-24T12:07:53Z",
        "updatedAt" : "2017-11-24T12:07:53Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "e38f7e14-0a09-4902-b503-dbc836c62666",
        "parentId" : "ae20d613-9637-4c09-9cb5-b5c39cc78d3c",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "That sounds reasonable. Though I think it is pretty easy to fix this :)\r\nJust move the node condition setting part to a separate function and call that directly from here.",
        "createdAt" : "2017-11-24T12:11:35Z",
        "updatedAt" : "2017-11-24T12:11:35Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "3ac57130-079d-4c22-97e6-812fa38f88ce",
        "parentId" : "ae20d613-9637-4c09-9cb5-b5c39cc78d3c",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "We don't want to have API calls in event handler. And I would like to avoid larger changes during code freeze.",
        "createdAt" : "2017-11-24T12:15:15Z",
        "updatedAt" : "2017-11-24T12:15:15Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "3832ebbe-3cf8-4984-8606-4d44c707fc95",
        "parentId" : "ae20d613-9637-4c09-9cb5-b5c39cc78d3c",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "SG",
        "createdAt" : "2017-11-24T12:17:23Z",
        "updatedAt" : "2017-11-24T12:17:23Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      }
    ],
    "commit" : "38597a26a8475195b6381b8468f2001373b220cd",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +111,115 @@\t\t\t_, cond := v1node.GetNodeCondition(&newNode.Status, v1.NodeNetworkUnavailable)\n\t\t\tif cond == nil || cond.Status != v1.ConditionFalse {\n\t\t\t\treturn ca.AllocateOrOccupyCIDR(newNode)\n\t\t\t}\n\t\t\treturn nil"
  },
  {
    "id" : "09a72ea0-9571-494b-a81b-c94824cfbdd6",
    "prId" : 56299,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/56299#pullrequestreview-78787268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a34fb6c8-7f6b-4da6-ba0e-201a0ff18246",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Please remove the TODO from line 171",
        "createdAt" : "2017-11-23T18:41:29Z",
        "updatedAt" : "2017-11-23T20:04:42Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "fced9e59-adb5-4511-a0db-c0714078beea",
        "parentId" : "a34fb6c8-7f6b-4da6-ba0e-201a0ff18246",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "Nice catch. Done.",
        "createdAt" : "2017-11-23T18:53:51Z",
        "updatedAt" : "2017-11-23T20:04:42Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      }
    ],
    "commit" : "19e56eb42de9133b16976845b7b29e1b83312fac",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +220,224 @@\t\t\t// See https://github.com/kubernetes/kubernetes/pull/42147#discussion_r103357248\n\t\t}\n\t\tif err = nodeutil.PatchNodeCIDR(ca.client, types.NodeName(node.Name), podCIDR); err == nil {\n\t\t\tglog.Infof(\"Set node %v PodCIDR to %v\", node.Name, podCIDR)\n\t\t\tbreak"
  },
  {
    "id" : "1893c6b4-8f82-40bb-90d4-13691349a1be",
    "prId" : 52285,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/52285#pullrequestreview-61956666",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76c5da8f-1b1b-4387-ba1a-e3981e0c9b34",
        "parentId" : null,
        "authorId" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "body" : "insert a newline here to make the nodesInProcessing a separate block",
        "createdAt" : "2017-09-11T17:06:49Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "tags" : [
        ]
      },
      {
        "id" : "53a3b04d-00b3-4b3c-a0ee-e7bebc8f9b5a",
        "parentId" : "76c5da8f-1b1b-4387-ba1a-e3981e0c9b34",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "Done. Addressed this and all further comments in both this file and range_allocator.go.",
        "createdAt" : "2017-09-11T21:07:57Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d864aa3c25177c35343dd1afd17d25b968441a8",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +53,57 @@\t// This increases a throughput of CIDR assignment by not blocking on long operations.\n\tnodeCIDRUpdateChannel chan nodeAndCIDR\n\trecorder              record.EventRecorder\n\n\t// Keep a set of nodes that are currectly being processed to avoid races in CIDR allocation"
  },
  {
    "id" : "aacd9de9-c361-4f9a-8839-9c352726e217",
    "prId" : 52285,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/52285#pullrequestreview-63755150",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d07e5ff-1a66-426d-93dd-546737a80d12",
        "parentId" : null,
        "authorId" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "body" : "Feels like this should `break`, otherwise it will just spin the loop.",
        "createdAt" : "2017-09-11T17:16:13Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "tags" : [
        ]
      },
      {
        "id" : "ebd1aa27-5a08-4706-9986-e9258d35466e",
        "parentId" : "3d07e5ff-1a66-426d-93dd-546737a80d12",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "It wasn't before, but I think it should return, right?",
        "createdAt" : "2017-09-11T17:56:01Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "3d8f9710-c594-4b81-aca5-e44f40394226",
        "parentId" : "3d07e5ff-1a66-426d-93dd-546737a80d12",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "> Feels like this should break, otherwise it will just spin the loop.\r\n\r\nI don't think it'll spin the loop. It will just go outside the if condition where we're updating the node's CIDR to the given one and then break only if the update succeeded (otherwise loop back). Seems to be the intended behavior. Am I missing something?\r\n\r\n> It wasn't before, but I think it should return, right?\r\n\r\nI think it shouldn't return here, because you need to go further below (outside the loop) where the 'RouteCreated' node condition is set.",
        "createdAt" : "2017-09-11T21:18:45Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "1a32f38d-e594-4683-935d-9c68f27a33a2",
        "parentId" : "3d07e5ff-1a66-426d-93dd-546737a80d12",
        "authorId" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "body" : "in this case `if node.Spec.PodCIDR != podCIDR`\r\n\r\nWe can't update the node Spec actually after it is set, so there is no point in retrying? That's why it should return...",
        "createdAt" : "2017-09-12T23:05:02Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "tags" : [
        ]
      },
      {
        "id" : "8bf64be4-71d6-4e32-bf83-8bb48dea9c3c",
        "parentId" : "3d07e5ff-1a66-426d-93dd-546737a80d12",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "I thought the intended behavior there is to change the node spec (even if it had a different value before) if the cloud provider changed the ip-range? The error msg was misleading earlier, saying `PodCIDR cannot be reassigned...`, I changed it to `PodCIDR being reassigned! ...`. Does it sgty?",
        "createdAt" : "2017-09-19T13:40:28Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      },
      {
        "id" : "408ef406-8eb4-4fa5-9abe-218578eda51c",
        "parentId" : "3d07e5ff-1a66-426d-93dd-546737a80d12",
        "authorId" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "body" : "If you try to edit a PodCIDR after it has been assign, the API server will now reject the update request with a validation error. At least that is what happens when I edit the object via kubectl.",
        "createdAt" : "2017-09-19T18:01:39Z",
        "updatedAt" : "2017-09-19T18:01:39Z",
        "lastEditedBy" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "tags" : [
        ]
      },
      {
        "id" : "e06409db-8ea9-4f93-95ca-508ff82dc9c9",
        "parentId" : "3d07e5ff-1a66-426d-93dd-546737a80d12",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "Hmm.. that's interesting. So does that mean each node requires processing by the cloud-cidr-allocator only once during the cluster's entire lifetime?\r\nBtw - this behavior of trying to go ahead and still set the new podCIDR is what was there before. I just left it the same way in my PR. ",
        "createdAt" : "2017-09-19T18:19:49Z",
        "updatedAt" : "2017-09-19T18:19:49Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d864aa3c25177c35343dd1afd17d25b968441a8",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +187,191 @@\t\t\t// rangeAllocator.\n\t\t\t//\n\t\t\t// See https://github.com/kubernetes/kubernetes/pull/42147#discussion_r103357248\n\t\t}\n\t\tnode.Spec.PodCIDR = podCIDR"
  },
  {
    "id" : "c061c43b-b702-4a27-87eb-761520eaf03f",
    "prId" : 52285,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/52285#pullrequestreview-61959730",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5a07fad-9a9d-4bf0-b0b9-dd267527644b",
        "parentId" : null,
        "authorId" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "body" : "Don't need to check `err` here, as we have going through the retry loop without success (if you change the above code to return) ",
        "createdAt" : "2017-09-11T17:20:26Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "tags" : [
        ]
      },
      {
        "id" : "b7245a3e-2b76-4f26-8ffb-6212335ce58a",
        "parentId" : "f5a07fad-9a9d-4bf0-b0b9-dd267527644b",
        "authorId" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "body" : "Didn't change the above code to return as we want to reach the code below where NodeCondition is set to `RouteCreated`. So have to explicitly check for the error.",
        "createdAt" : "2017-09-11T21:20:08Z",
        "updatedAt" : "2017-09-19T15:33:50Z",
        "lastEditedBy" : "57a5e7e7-e6d7-467b-96ab-41e4ca978eee",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d864aa3c25177c35343dd1afd17d25b968441a8",
    "line" : 220,
    "diffHunk" : "@@ -1,1 +196,200 @@\t\tglog.Errorf(\"Failed to update node %v PodCIDR to %v (%d retries left): %v\", node.Name, podCIDR, cidrUpdateRetries-rep-1, err)\n\t}\n\tif err != nil {\n\t\tutil.RecordNodeStatusChange(ca.recorder, node, \"CIDRAssignmentFailed\")\n\t\tglog.Errorf(\"CIDR assignment for node %v failed: %v.\", data.nodeName, err)"
  }
]