[
  {
    "id" : "84458359-9768-4187-adad-6d330b5da108",
    "prId" : 48983,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48983#pullrequestreview-50562306",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe7eb76e-a9b4-44a7-86fe-7fa81224002f",
        "parentId" : null,
        "authorId" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "body" : "Has `NodeInodePressure` already been deprecated? If yes, add a link to it?",
        "createdAt" : "2017-07-18T09:18:20Z",
        "updatedAt" : "2017-07-18T09:18:20Z",
        "lastEditedBy" : "0df1da34-610c-4ce5-b0cf-bbda668bf9c1",
        "tags" : [
        ]
      },
      {
        "id" : "e6e62276-a4df-4920-8edf-99877c4e8380",
        "parentId" : "fe7eb76e-a9b4-44a7-86fe-7fa81224002f",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Good point! Opened https://github.com/kubernetes/kubernetes/issues/49103 to confirm whether we can remove it in 1.8 :).",
        "createdAt" : "2017-07-18T09:57:30Z",
        "updatedAt" : "2017-07-18T09:57:31Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e28a2cfd1df51fbfb49f487e9c3769082b7759a",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +959,963 @@\t\t\tv1.NodeMemoryPressure,\n\t\t\tv1.NodeDiskPressure,\n\t\t\t// We don't change 'NodeInodePressure' condition, as it'll be removed in future.\n\t\t\t// v1.NodeInodePressure,\n\t\t\t// We don't change 'NodeNetworkUnavailable' condition, as it's managed on a control plane level."
  },
  {
    "id" : "6d40f97c-76a2-4aa7-8c54-4a4a651550b1",
    "prId" : 42647,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42647#pullrequestreview-25760361",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa169c2b-aad1-4141-bcef-7ab4249c7150",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "nit: elsewhere we don't put empty line between the kube repo imports.",
        "createdAt" : "2017-03-08T09:48:07Z",
        "updatedAt" : "2017-03-08T09:48:07Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "c6562418-09ec-4a15-abd5-2b9aec3f266c",
        "parentId" : "aa169c2b-aad1-4141-bcef-7ab4249c7150",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I know, and I find it confusing (especially when looking for `api/v1`).",
        "createdAt" : "2017-03-08T09:51:22Z",
        "updatedAt" : "2017-03-08T09:51:22Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "7afe8893-f8b6-490c-8e1c-54330c5c69a3",
        "parentId" : "aa169c2b-aad1-4141-bcef-7ab4249c7150",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I can revert this, but as an owner of NC I kind of feel that I should be able to add those:)",
        "createdAt" : "2017-03-08T09:52:15Z",
        "updatedAt" : "2017-03-08T09:52:15Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "ae8cd6f1-1add-4d35-bca1-fe74095d03e9",
        "parentId" : "aa169c2b-aad1-4141-bcef-7ab4249c7150",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "I wouldn't object if we followed the style here throughout the code base. Much easier to read instead of 30 apimachinery+client-go+apiserver+kube imports without structure.",
        "createdAt" : "2017-03-08T10:44:37Z",
        "updatedAt" : "2017-03-08T10:44:37Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "49256a8d-1fa7-471f-84ba-19b3ec49412e",
        "parentId" : "aa169c2b-aad1-4141-bcef-7ab4249c7150",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "We can fight it one file at a time;)",
        "createdAt" : "2017-03-08T12:23:15Z",
        "updatedAt" : "2017-03-08T12:23:16Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "48d784272eccc2a59505ed9ece590d04143fec7f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +32,36 @@\tutilruntime \"k8s.io/apimachinery/pkg/util/runtime\"\n\t\"k8s.io/apimachinery/pkg/util/wait\"\n\n\tv1core \"k8s.io/client-go/kubernetes/typed/core/v1\"\n\tclientv1 \"k8s.io/client-go/pkg/api/v1\""
  },
  {
    "id" : "a4d530a2-b7e2-45ff-a5bf-5ce098e7afc0",
    "prId" : 42147,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42147#pullrequestreview-31673573",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9a31d72-ff1e-4961-9c4c-c51e0c7d8cea",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "How does this get resolved?  Or does it matter?  Most controllers assume they are active-passive, so there should be no other component.  Still, it might make sense to periodically reconcile our cache against all nodes?",
        "createdAt" : "2017-04-07T22:23:47Z",
        "updatedAt" : "2017-04-11T21:08:10Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "091e46ef21bb82e4ec0cdaed65a8b745e3c81c4e",
    "line" : 259,
    "diffHunk" : "@@ -1,1 +471,475 @@\t// which prevents it from being assigned to any new node. The cluster\n\t// state is correct.\n\t// Restart of NC fixes the issue.\n\tif node.Spec.PodCIDR == \"\" {\n\t\tnodeCopy, err := api.Scheme.Copy(node)"
  },
  {
    "id" : "e91a2209-67cd-492c-acd7-0f10e64efeb7",
    "prId" : 41133,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41133#pullrequestreview-22705820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68285be7-6876-438f-9f43-884b3ea1afcb",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "should you \"continue\" here?",
        "createdAt" : "2017-02-18T07:05:56Z",
        "updatedAt" : "2017-02-24T08:25:09Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "69971d55-c61c-4618-b7f5-316107aba522",
        "parentId" : "68285be7-6876-438f-9f43-884b3ea1afcb",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Right, I forgot that I'm actually using the Node in this handler. Good catch.",
        "createdAt" : "2017-02-20T08:22:38Z",
        "updatedAt" : "2017-02-24T08:25:09Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9d6086217f36bcea6024115f81642d6dfdcaf8b",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +461,465 @@\t\t\t\t\t\t\treturn true, 0\n\t\t\t\t\t\t} else if err != nil {\n\t\t\t\t\t\t\tglog.Warningf(\"Failed to get Node %v from the nodeLister: %v\", value.Value, err)\n\t\t\t\t\t\t\t// retry in 50 millisecond\n\t\t\t\t\t\t\treturn false, 50 * time.Millisecond"
  },
  {
    "id" : "580384c4-ead1-4794-acba-4dac02b21038",
    "prId" : 41133,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41133#pullrequestreview-22706320",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8205f20-5b1c-4918-b92c-f58811f8dffb",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "same question about \"continue\" as above",
        "createdAt" : "2017-02-18T07:30:01Z",
        "updatedAt" : "2017-02-24T08:25:09Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "0f7f34d6-61cf-43d2-aeba-edf9a7000afb",
        "parentId" : "a8205f20-5b1c-4918-b92c-f58811f8dffb",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "See above.",
        "createdAt" : "2017-02-20T08:26:10Z",
        "updatedAt" : "2017-02-24T08:25:09Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9d6086217f36bcea6024115f81642d6dfdcaf8b",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +522,526 @@\t\t\t\t\t\t\tglog.Warningf(\"Node %v no longer present in nodeLister!\", value.Value)\n\t\t\t\t\t\t} else if err != nil {\n\t\t\t\t\t\t\tglog.Warningf(\"Failed to get Node %v from the nodeLister: %v\", value.Value, err)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tzone := utilnode.GetZoneKey(node)"
  },
  {
    "id" : "45e778df-2db3-4ebe-a547-6d2162fcc89d",
    "prId" : 41133,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41133#pullrequestreview-22706625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10ed31cb-c6c4-4a15-a07d-bf3950c9f2bf",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "Do all of the taint add/remove operations in this PR retry on transient failure or at least become eventually consistent with the NodeStatus condition? That (obviously) seems impotant.",
        "createdAt" : "2017-02-18T08:11:21Z",
        "updatedAt" : "2017-02-24T08:25:09Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "b6405c2c-62dc-430e-8e2e-a81a79608ebc",
        "parentId" : "10ed31cb-c6c4-4a15-a07d-bf3950c9f2bf",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yes. It is handled in the 'rate limited queue' - in case of failed Taint changes it will retry.",
        "createdAt" : "2017-02-20T08:28:15Z",
        "updatedAt" : "2017-02-24T08:25:09Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9d6086217f36bcea6024115f81642d6dfdcaf8b",
    "line" : 340,
    "diffHunk" : "@@ -1,1 +672,676 @@\t\t\t\t\tremoved, err := nc.markNodeAsHealthy(node)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tglog.Errorf(\"Failed to remove taints from node %v. Will retry in next iteration.\", node.Name)\n\t\t\t\t\t}\n\t\t\t\t\tif removed {"
  },
  {
    "id" : "b0bdf86e-78fe-41dd-85d5-1eb319376107",
    "prId" : 40355,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40355#pullrequestreview-20782050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d6fa8e4-f695-43e2-a79b-dc0eb38f0d4e",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Same here.\r\nYou should probably create a separate PR where you change node controller to the pattern I described above (and that PR should be merged before this one).",
        "createdAt" : "2017-02-08T13:20:13Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "6d74dbe7-55b6-4631-85bb-02e9afd78dd8",
        "parentId" : "5d6fa8e4-f695-43e2-a79b-dc0eb38f0d4e",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "See above.",
        "createdAt" : "2017-02-08T15:15:28Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "004552f8a43e0bfd138c166f586bb4686fac559c",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +324,328 @@\t\t\t\t}\n\t\t\t},\n\t\t\tUpdateFunc: func(oldNode, newNode interface{}) {\n\t\t\t\tnode := newNode.(*v1.Node)\n\t\t\t\tprevNode := oldNode.(*v1.Node)"
  },
  {
    "id" : "a4868c16-0105-45f1-9ceb-c4d8cf8e5468",
    "prId" : 40355,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40355#pullrequestreview-21397638",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fdba1ec-a211-487d-a456-ab4cca30c84c",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This is a bit strange. You are checking the same condition in two different ways:\r\n1. \"if ns.runtaintManager {\"\r\n2. \"if nc.taintManager != nil {\r\n Can you please unify it?\r\n\r\nOr even better, You can move all those checks to TaintManager implementation. E.g.\r\n```\r\nfunc (tm *taintManager) NodeUpdated(...) {\r\n  if tm == nil {\r\n    return\r\n  }\r\n  // Now real body\r\n}\r\n```\r\n\r\nWith this pattern you can omit all those \"ifs\" here and this code will be much cleaner and shorter.",
        "createdAt" : "2017-02-08T13:23:53Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "16a3ed5f-c408-4c8e-a720-f824dc5f7931",
        "parentId" : "5fdba1ec-a211-487d-a456-ab4cca30c84c",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I don't like the pattern of moving nil checks into the functions (if not necessary). I'll keep it on the higher level, guarded by `nc.taintManager != nil` everywhere which is not a create-related code.",
        "createdAt" : "2017-02-08T15:18:09Z",
        "updatedAt" : "2017-02-10T01:14:06Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "e7bb82af-7c08-4c26-a4fb-4ef0257576a3",
        "parentId" : "5fdba1ec-a211-487d-a456-ab4cca30c84c",
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I don't think we need to make taintManager conditional. If it is broken we will do a patch release. Making it unconditional will simplify the code.",
        "createdAt" : "2017-02-12T01:31:03Z",
        "updatedAt" : "2017-02-12T06:19:09Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "004552f8a43e0bfd138c166f586bb4686fac559c",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +357,361 @@\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif nc.taintManager != nil {\n\t\t\t\t\tnc.taintManager.NodeUpdated(prevNode, node)\n\t\t\t\t}"
  },
  {
    "id" : "295141c3-bf33-495d-83c4-5d33855e43d6",
    "prId" : 39011,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39011#pullrequestreview-13746281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b8e0c8d-2e7d-4b15-97cd-2d57d4967718",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "There is a GET in L453, you probably want to switch that to a cache lookup too. Also it seems that you want to deep-copy in certain places inside `tryUpdateNodeStatus`.",
        "createdAt" : "2016-12-20T11:43:38Z",
        "updatedAt" : "2016-12-20T12:13:24Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "405d9c3a-bac9-4868-8c1f-19986ff48545",
        "parentId" : "9b8e0c8d-2e7d-4b15-97cd-2d57d4967718",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "For the line 453 - since this is retry, I'm on purpose doing get from etcd there - this is to decrease probability for conflicts in those situations.\r\n\r\nRegarding the DeepCopy - yes, that's a good point. I will add it.",
        "createdAt" : "2016-12-20T12:08:33Z",
        "updatedAt" : "2016-12-20T12:13:24Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b2d9eb2e7b999ea0cec48bdde4f624d5a64f99e",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +411,415 @@\t// We are listing nodes from local cache as we can tolerate some small delays\n\t// comparing to state from etcd and there is eventual consistency anyway.\n\tnodes, err := nc.nodeStore.List()\n\tif err != nil {\n\t\treturn err"
  },
  {
    "id" : "906013d9-21cc-4279-b1da-e27c014c8f86",
    "prId" : 34861,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34861#pullrequestreview-4367645",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f70b7548-0e4e-4c97-b995-9b628860ccdb",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "This one should also be removed.\n@davidopp \n",
        "createdAt" : "2016-10-15T06:03:52Z",
        "updatedAt" : "2016-10-15T06:03:52Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f91e3f8397b6a8be02c4ca684b5d2e6def30bac1",
    "line" : 188,
    "diffHunk" : "@@ -1,1 +487,491 @@\n\t\tgo wait.Until(func() {\n\t\t\tif !nc.nodeController.HasSynced() || !nc.podController.HasSynced() || !nc.daemonSetController.HasSynced() {\n\t\t\t\tglog.V(2).Infof(\"NodeController is waiting for informers to sync...\")\n\t\t\t\treturn"
  },
  {
    "id" : "9081d68e-02b5-4062-bc93-2c49e2555ad3",
    "prId" : 34851,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34851#pullrequestreview-4339962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c1123ff-a044-4f30-8fc0-b068327493a2",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Github is so stupid that it cannot detect that it's actually not a new code...\n",
        "createdAt" : "2016-10-14T20:12:16Z",
        "updatedAt" : "2016-10-14T20:12:16Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "19af1276-de8f-4292-bc1f-7c3f3e438d75",
        "parentId" : "0c1123ff-a044-4f30-8fc0-b068327493a2",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "[ignoring whitespace](https://github.com/kubernetes/kubernetes/pull/34851/files?w=1) makes it better\n",
        "createdAt" : "2016-10-14T20:28:32Z",
        "updatedAt" : "2016-10-14T20:28:33Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "de96b87c-36c2-4d19-b916-e86e930c6e4e",
        "parentId" : "0c1123ff-a044-4f30-8fc0-b068327493a2",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@liggitt How did you do that? (yes it's much better)\n",
        "createdAt" : "2016-10-14T20:31:20Z",
        "updatedAt" : "2016-10-14T20:31:20Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "9455913e-b165-430b-a860-d18ff2b51ba0",
        "parentId" : "0c1123ff-a044-4f30-8fc0-b068327493a2",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "add `?w=1` to the end of the URL\n",
        "createdAt" : "2016-10-14T20:32:39Z",
        "updatedAt" : "2016-10-14T20:32:39Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "8760130f-fa7b-45f9-8811-2c724d067195",
        "parentId" : "0c1123ff-a044-4f30-8fc0-b068327493a2",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "nice trick - I didn't know about it. Thanks!\n",
        "createdAt" : "2016-10-14T20:34:31Z",
        "updatedAt" : "2016-10-14T20:34:31Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7befa2a14f0786f405f6e127c11cddf8d26b8ca",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +366,370 @@\n\t\t// Incorporate the results of node status pushed from kubelet to master.\n\t\tgo wait.Until(func() {\n\t\t\tif err := nc.monitorNodeStatus(); err != nil {\n\t\t\t\tglog.Errorf(\"Error monitoring node status: %v\", err)"
  },
  {
    "id" : "b1f13212-0fef-44f8-8c43-8c38c12baef1",
    "prId" : 30686,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30b43fb4-2968-452d-a320-57bc0386f20d",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Are we sure that len(v) > 0 here?\n",
        "createdAt" : "2016-08-18T13:21:40Z",
        "updatedAt" : "2016-08-18T13:21:40Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "40eb95c1-cb06-4a83-828f-4031e1fdf0c5",
        "parentId" : "30b43fb4-2968-452d-a320-57bc0386f20d",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yes, if it were 0, then there would be no Nodes in the given zone, and hence the zone would not exist. This map is created from scratch each time `monitorNodeStatus` is called.\n",
        "createdAt" : "2016-08-18T13:27:47Z",
        "updatedAt" : "2016-08-18T13:27:47Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d8cb17efad7068fec3207b747559e7a1b440c5b",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +622,626 @@\t\tZoneSize.WithLabelValues(k).Set(float64(len(v)))\n\t\tunhealthy, newState := nc.computeZoneStateFunc(v)\n\t\tZoneHealth.WithLabelValues(k).Set(float64(100*(len(v)-unhealthy)) / float64(len(v)))\n\t\tUnhealthyNodes.WithLabelValues(k).Set(float64(unhealthy))\n\t\tif newState != stateFullDisruption {"
  },
  {
    "id" : "6268c11f-1f59-4112-956c-ffc925559738",
    "prId" : 29101,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61b1a6b1-62e5-4bb9-b857-a580c833dce4",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Please state explicitly that the problem is that in memory an unoccupied CIDR is marked as occupied, which prevents it from being allocated to something. But the state of the cluster itself is correct.\n",
        "createdAt" : "2016-07-18T14:14:29Z",
        "updatedAt" : "2016-07-18T15:06:22Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "eb4118a4-e57a-49a7-a626-c5527fea80d8",
        "parentId" : "61b1a6b1-62e5-4bb9-b857-a580c833dce4",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2016-07-18T14:21:07Z",
        "updatedAt" : "2016-07-18T15:06:22Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "56006fac431f129c679ed69dac27e4ea3e8ef4a6",
    "line" : null,
    "diffHunk" : "@@ -1,1 +265,269 @@\t\t\t\t// which prevents it from being assigned to any new node. The cluster\n\t\t\t\t// state is correct.\n\t\t\t\t// Restart of NC fixes the issue.\n\t\t\t\tif node.Spec.PodCIDR == \"\" {\n\t\t\t\t\terr := nc.cidrAllocator.AllocateOrOccupyCIDR(node)"
  },
  {
    "id" : "28a4ce1d-1d1c-4275-a75d-919d8dcc4567",
    "prId" : 28897,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eab618b5-605e-4232-ae40-f8668a4f7b84",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "After more looking into the code, I think that the signatures of these functions should be:\n\nfunc(defaultQPS float32) RateLimitedTimedQueue\n",
        "createdAt" : "2016-07-15T07:28:11Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "09e9b222-fe2a-44a5-ae69-d6329f48e5ef",
        "parentId" : "eab618b5-605e-4232-ae40-f8668a4f7b84",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Discussed f2f\n",
        "createdAt" : "2016-07-15T09:33:54Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "66224ce0bd9310deda7aa8a696310a89cc927747",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +142,146 @@\tcomputeZoneStateFunc       func(nodeConditions []*api.NodeCondition) zoneState\n\tenterPartialDisruptionFunc func(nodeNum int, defaultQPS float32) float32\n\tenterFullDisruptionFunc    func(nodeNum int, defaultQPS float32) float32\n\n\tzoneStates map[string]zoneState"
  },
  {
    "id" : "a120c75c-19ef-48e0-a7b1-cbcf21976f85",
    "prId" : 28897,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c46d4ce-5c3f-4eda-9f2f-627bad9f7e37",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "nc.setLimiterInZone(...)\n",
        "createdAt" : "2016-07-15T12:42:36Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "b602be2f-2fe8-44da-8876-5907b5e51d3e",
        "parentId" : "3c46d4ce-5c3f-4eda-9f2f-627bad9f7e37",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "This case is not handled, as it's a special logic for complete isolation of master.\n",
        "createdAt" : "2016-07-15T12:50:36Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "66224ce0bd9310deda7aa8a696310a89cc927747",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +601,605 @@\t\t\t// We stop all evictions.\n\t\t\tfor k := range nc.zonePodEvictor {\n\t\t\t\tnc.zonePodEvictor[k].SwapLimiter(0)\n\t\t\t\tnc.zoneTerminationEvictor[k].SwapLimiter(0)\n\t\t\t}"
  },
  {
    "id" : "e35725f8-6664-4eca-9365-b8a8706338b2",
    "prId" : 28897,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a75893c-0760-4b7d-b82d-f9c5982c9f75",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Can we merge these two loops into one?\n",
        "createdAt" : "2016-07-15T12:43:58Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "f1d604cc-014e-4c1c-bba2-c5b51e752f0a",
        "parentId" : "0a75893c-0760-4b7d-b82d-f9c5982c9f75",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2016-07-15T12:52:01Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "66224ce0bd9310deda7aa8a696310a89cc927747",
    "line" : null,
    "diffHunk" : "@@ -1,1 +625,629 @@\t\t\t\tnc.setLimiterInZone(k, len(zoneToNodeConditions[k]), newZoneStates[k])\n\t\t\t\tnc.zoneStates[k] = newZoneStates[k]\n\t\t\t}\n\t\t\treturn\n\t\t}"
  },
  {
    "id" : "c79c190c-a8f2-4dc5-8009-7549808f77f7",
    "prId" : 28897,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "350762da-e4c1-472b-859e-3eb58040d74d",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "should there be return here?\n",
        "createdAt" : "2016-07-15T12:44:18Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "b7aa9971-018f-4fa1-bbf1-5c5584889217",
        "parentId" : "350762da-e4c1-472b-859e-3eb58040d74d",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2016-07-15T12:52:08Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "66224ce0bd9310deda7aa8a696310a89cc927747",
    "line" : null,
    "diffHunk" : "@@ -1,1 +627,631 @@\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// We know that there's at least one not-fully disrupted so,\n\t\t// we can use default behavior for rate limiters"
  },
  {
    "id" : "b48aa281-2fb5-42bf-9076-b226ccc9f4a7",
    "prId" : 28897,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c0b5a0c-2780-4f56-9cc9-3d2e395bd222",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Actually the previous \"if\" consists of:\n- setting probe times\n- the same code as here\n\nWe should merge those two together.\n",
        "createdAt" : "2016-07-15T12:46:15Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "c7ef6595-47cd-4dc9-830b-5a2f8ec41aab",
        "parentId" : "3c0b5a0c-2780-4f56-9cc9-3d2e395bd222",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Not that simple. In this loop we only modify limiters for zones which changed states, above we force update everywhere.\n",
        "createdAt" : "2016-07-15T12:53:20Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "66224ce0bd9310deda7aa8a696310a89cc927747",
    "line" : null,
    "diffHunk" : "@@ -1,1 +630,634 @@\t\t// We know that there's at least one not-fully disrupted so,\n\t\t// we can use default behavior for rate limiters\n\t\tfor k, v := range nc.zoneStates {\n\t\t\tnewState := newZoneStates[k]\n\t\t\tif v == newState {"
  },
  {
    "id" : "4d477aea-8300-4ef0-a3f9-841315b858e6",
    "prId" : 28897,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7d6f1e2-d3df-49d5-9232-171d28dae11e",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "We should remove zones (from nc.zoneState) that no longer exist (i.e. zones which all nodes have already been removed).\n",
        "createdAt" : "2016-08-02T07:39:41Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "d9a61084-57a5-4aad-9ea4-d8de68c0ca60",
        "parentId" : "b7d6f1e2-d3df-49d5-9232-171d28dae11e",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Right. I was overwriting whole list it originally, but I forgot to update it after I changed the logic.\n",
        "createdAt" : "2016-08-02T09:42:09Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "66224ce0bd9310deda7aa8a696310a89cc927747",
    "line" : null,
    "diffHunk" : "@@ -1,1 +585,589 @@\t\t\tbreak\n\t\t}\n\t}\n\n\t// At least one node was responding in previous pass or in the current pass. Semantics is as follows:"
  },
  {
    "id" : "d153f214-9708-4e12-8201-fa654220688d",
    "prId" : 28897,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea3b3522-7cb9-4751-aae6-5deaf061762e",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Shouldn't we set up ns.zonePodEvictor for this zone?\n",
        "createdAt" : "2016-08-02T07:41:33Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "1bb7dc0f-6335-448a-af74-c7813b09f569",
        "parentId" : "ea3b3522-7cb9-4751-aae6-5deaf061762e",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I don't think we need to. This \"Initial\" state is only a temporary previous one. It will assure that we won't have `allWasFullyDisrupted == true`, so we'll always enter the main if stmt. In it we'll either enter 'allAreFullyDisruptedBranch' and set the rate limiter there, or an ordinary loop, which will also set the limiter, as new state will be different than \"Initial\".\n",
        "createdAt" : "2016-08-02T09:58:38Z",
        "updatedAt" : "2016-08-02T12:22:10Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "66224ce0bd9310deda7aa8a696310a89cc927747",
    "line" : null,
    "diffHunk" : "@@ -1,1 +571,575 @@\t\tnewZoneStates[k] = newState\n\t\tif _, had := nc.zoneStates[k]; !had {\n\t\t\tnc.zoneStates[k] = stateInitial\n\t\t}\n\t}"
  },
  {
    "id" : "4d98e495-f474-4b10-abd6-920553b7afef",
    "prId" : 28843,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b2f5b68-a49a-42ad-a6ba-019ffe187eda",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "s/zoneTerminationEvictor/zonePodEvictor/\n",
        "createdAt" : "2016-07-13T09:18:16Z",
        "updatedAt" : "2016-07-13T12:09:26Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "6c6d426d-b2a3-4349-a36f-0aa2067e6f54",
        "parentId" : "4b2f5b68-a49a-42ad-a6ba-019ffe187eda",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Or maybe you should even do (performance is not important in this case):\n\n```\nfor _, evictor := range nv.zonePodEvictor {\n  evictor.Try(...\n\n     evictor.Add(...)\n)\n```\n",
        "createdAt" : "2016-07-13T09:19:36Z",
        "updatedAt" : "2016-07-13T12:09:26Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "0df56287-aeaa-44c6-a804-6cbad6cad90a",
        "parentId" : "4b2f5b68-a49a-42ad-a6ba-019ffe187eda",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Discussed f2f\n",
        "createdAt" : "2016-07-13T11:06:23Z",
        "updatedAt" : "2016-07-13T12:09:26Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5677a9845e44731c53b021f217fb90d638b258b1",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +322,326 @@\n\t\t\t\tif remaining {\n\t\t\t\t\tnc.zoneTerminationEvictor[k].Add(value.Value)\n\t\t\t\t}\n\t\t\t\treturn true, 0"
  },
  {
    "id" : "5864fbf4-4e7a-4c72-a0e4-827b918c000d",
    "prId" : 28843,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "726c68a0-7af6-44a9-904e-769fd0c0a003",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "And the same here\n",
        "createdAt" : "2016-07-13T09:20:17Z",
        "updatedAt" : "2016-07-13T12:09:26Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "430ad472-ecba-4319-b7f7-108f7cd989bf",
        "parentId" : "726c68a0-7af6-44a9-904e-769fd0c0a003",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Discussed f2f\n",
        "createdAt" : "2016-07-13T11:06:37Z",
        "updatedAt" : "2016-07-13T12:09:26Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5677a9845e44731c53b021f217fb90d638b258b1",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +334,338 @@\t\tnc.evictorLock.Lock()\n\t\tdefer nc.evictorLock.Unlock()\n\t\tfor k := range nc.zoneTerminationEvictor {\n\t\t\tnc.zoneTerminationEvictor[k].Try(func(value TimedValue) (bool, time.Duration) {\n\t\t\t\tcompleted, remaining, err := terminatePods(nc.kubeClient, nc.recorder, value.Value, value.AddedAt, nc.maximumGracePeriod)"
  },
  {
    "id" : "3e92724c-41c1-4b07-917f-f426e7efdae3",
    "prId" : 28843,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aab6267a-0f3a-4aab-9931-6f695da44f31",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Why you are iterating over all zones - shouldn't you only look into zone in which the node is?\n",
        "createdAt" : "2016-07-13T09:22:50Z",
        "updatedAt" : "2016-07-13T12:09:26Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "77aae4da-0d49-4483-a150-1d854cc54677",
        "parentId" : "aab6267a-0f3a-4aab-9931-6f695da44f31",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "This is the current logic.\n",
        "createdAt" : "2016-07-13T10:31:22Z",
        "updatedAt" : "2016-07-13T12:09:26Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5677a9845e44731c53b021f217fb90d638b258b1",
    "line" : 191,
    "diffHunk" : "@@ -1,1 +726,730 @@\tdefer nc.evictorLock.Unlock()\n\tfoundHealty := false\n\tfor _, state := range nc.zoneStates {\n\t\tif state != stateFullSegmentation {\n\t\t\tfoundHealty = true"
  },
  {
    "id" : "47f0e840-bb7b-4326-83c9-3d0f8a98ab2d",
    "prId" : 28829,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8ebfeb9-eed9-4f19-ab5a-74d4b01f6dac",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Shouldn't you update zoneStates here?\n",
        "createdAt" : "2016-07-12T12:28:38Z",
        "updatedAt" : "2016-07-12T13:11:12Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "89ab69f7-2cbe-48f7-90b2-23665905ac08",
        "parentId" : "f8ebfeb9-eed9-4f19-ab5a-74d4b01f6dac",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yeah, it had to got lost in last rewriting of this part.\n",
        "createdAt" : "2016-07-12T12:34:44Z",
        "updatedAt" : "2016-07-12T13:11:12Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd600ab65c19080104ab5db0511b002397c0ae39",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +491,495 @@\t\t\t\t\tnc.nodeStatusMap[nodes.Items[i].Name] = v\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tnc.zoneStates[k] = newState"
  },
  {
    "id" : "979480f2-906e-4700-8750-f757d39eaac9",
    "prId" : 22667,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d399bb2-f33b-4c35-9beb-e19fcc2df817",
        "parentId" : null,
        "authorId" : "d324e241-a7f0-4ace-bda2-4174b07bdb18",
        "body" : "You will get here if GetByKey returns an error.  Also, is it possible for the cache to be stale?\n",
        "createdAt" : "2016-03-08T00:47:47Z",
        "updatedAt" : "2016-03-08T00:47:47Z",
        "lastEditedBy" : "d324e241-a7f0-4ace-bda2-4174b07bdb18",
        "tags" : [
        ]
      },
      {
        "id" : "97613990-5c81-4037-a64b-035f89f18d16",
        "parentId" : "7d399bb2-f33b-4c35-9beb-e19fcc2df817",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "GetByKey, can't return errors stupidly. That should be cleaned up:\n\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/client/cache/store.go#L192\n\nI'll do this in a follow up.\n\nThese cashes can be stale. There's a race here if a (node get's created -> scheduler get's update of node -> new pod gets assigned to a node -> controller get's update of pod) before (node get's created -> controller get's update of node). It seems less likely and of smaller consequence then the bug this fixes. Any suggestion on how to work around this?\n",
        "createdAt" : "2016-03-08T00:58:20Z",
        "updatedAt" : "2016-03-08T01:01:08Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "7f8043b9-cb4b-488c-9d54-1a66952a98fa",
        "parentId" : "7d399bb2-f33b-4c35-9beb-e19fcc2df817",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Also the node has to become ready in the scheduler flow and doesn't have to become ready in the controller manager flow which also makes the race less likely. Still a race though.\n",
        "createdAt" : "2016-03-08T01:02:45Z",
        "updatedAt" : "2016-03-08T01:02:45Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "fd5ffdca-07b1-441c-b395-a990a2f01997",
        "parentId" : "7d399bb2-f33b-4c35-9beb-e19fcc2df817",
        "authorId" : "d324e241-a7f0-4ace-bda2-4174b07bdb18",
        "body" : "My intuition is that this race is pretty minor and less significant than the bug it fixes, but our intuition is often wrong about this stuff, and on an antagonistically loaded machine, you could imagine being in some kind of steady state forever where the controllers fight each other.\n\nBut there's a non-controller behavior that's also new.  If I manually specify a host in my single pod (no controller), but make a typo, my pod disappears and never runs.  That'll seem weird.  It's also possible for the user to submit the pod in the same race window (hard to imagine this really happening) and the same thing happens.\n\nSuggestions for fixing the race... I'll think about it.  Concurrency + global state + caching lends itself to this.\n",
        "createdAt" : "2016-03-08T01:53:16Z",
        "updatedAt" : "2016-03-08T01:53:16Z",
        "lastEditedBy" : "d324e241-a7f0-4ace-bda2-4174b07bdb18",
        "tags" : [
        ]
      }
    ],
    "commit" : "c404e7c6d1f485200a585b8265763cad894e4c39",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +388,392 @@\t\t\tcontinue\n\t\t}\n\t\tif err := nc.forcefullyDeletePod(pod); err != nil {\n\t\t\tutilruntime.HandleError(err)\n\t\t}"
  },
  {
    "id" : "02d6cd36-b972-4b79-b32c-cf601e811eba",
    "prId" : 22336,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46ca3fff-d431-4bfe-a612-4702cdedda38",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I'm not quite sure how it'll interact with mirror pods... @yujuhong \n",
        "createdAt" : "2016-03-02T11:16:13Z",
        "updatedAt" : "2016-03-07T14:08:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "c3bcabfa-f51c-485f-86bf-dc8a1bf72267",
        "parentId" : "46ca3fff-d431-4bfe-a612-4702cdedda38",
        "authorId" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "body" : "If kubelet is semi-functioning, it may try to recreate the mirror pods. You'll be left with running pods on a node that doesn't exist anymore. Since this PR deletes pods/nodes only when the vm has disappeared, it should be fine.\n",
        "createdAt" : "2016-03-02T17:49:00Z",
        "updatedAt" : "2016-03-07T14:08:23Z",
        "lastEditedBy" : "1bd2d65a-7c93-4c22-b408-c7794d037dc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7fc608df705205a2d0197b7c0c7c001a250d133",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +515,519 @@\t\treturn fmt.Errorf(\"unable to list pods on node %q: %v\", nodeName, err)\n\t}\n\tfor _, pod := range pods.Items {\n\t\tif pod.Spec.NodeName != nodeName {\n\t\t\tcontinue"
  },
  {
    "id" : "1f827144-ac73-461c-88bb-64675e8fb474",
    "prId" : 22336,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c54152d-55f0-4d80-b5fd-f007b6be2e54",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "defer util.HandleCrash()\n",
        "createdAt" : "2016-03-02T23:18:24Z",
        "updatedAt" : "2016-03-07T14:08:23Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "129d4d7a-6068-4a73-b789-e9df9705e34b",
        "parentId" : "8c54152d-55f0-4d80-b5fd-f007b6be2e54",
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "Done.\n",
        "createdAt" : "2016-03-03T21:06:21Z",
        "updatedAt" : "2016-03-07T14:08:23Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7fc608df705205a2d0197b7c0c7c001a250d133",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +475,479 @@\t\t\t\t\tglog.Infof(\"Deleting node (no longer present in cloud provider): %s\", node.Name)\n\t\t\t\t\tnc.recordNodeEvent(node.Name, api.EventTypeNormal, \"DeletingNode\", fmt.Sprintf(\"Deleting Node %v because it's not present according to cloud provider\", node.Name))\n\t\t\t\t\tgo func(nodeName string) {\n\t\t\t\t\t\tdefer utilruntime.HandleCrash()\n\t\t\t\t\t\t// Kubelet is not reporting and Cloud Provider says node"
  },
  {
    "id" : "7522ff6b-58bb-416a-8433-920bedb7f394",
    "prId" : 22336,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e77d2421-8fdf-41e6-a88e-7a1f82d2be49",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Let me rephrase my question (I'd love to attach it to 'forcefullyDeletePod' function, but github...): how does force-deleting (GracePeriodSeconds=0) work on mirror pods and/or daemon pods. @yujuhong @mikedanese. Will it correctly delete it from the API server and it won't come back, or they'll get recreated and we'll have a race condition here?\n",
        "createdAt" : "2016-03-07T08:57:03Z",
        "updatedAt" : "2016-03-07T14:08:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "a500437e-e7b2-4793-a759-f803c506fd54",
        "parentId" : "e77d2421-8fdf-41e6-a88e-7a1f82d2be49",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Sorry - this has nothing to do with this PR. It's a problem for 'normal' Node eviction, but in your case it's OK.\n",
        "createdAt" : "2016-03-07T09:31:15Z",
        "updatedAt" : "2016-03-07T14:08:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7fc608df705205a2d0197b7c0c7c001a250d133",
    "line" : 134,
    "diffHunk" : "@@ -1,1 +519,523 @@\t\t\tcontinue\n\t\t}\n\t\tif err := nc.forcefullyDeletePod(&pod); err != nil {\n\t\t\treturn fmt.Errorf(\"unable to delete pod %q on node %q: %v\", pod.Name, nodeName, err)\n\t\t}"
  },
  {
    "id" : "659fa48c-374b-4c61-80fe-f0f4a962e88e",
    "prId" : 21187,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "638992af-cafd-41ef-a01d-afc7fd0a69f5",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I was just suggesting adding to terminationEvictor here, instead of invoking `terminatePods`.\n",
        "createdAt" : "2016-03-01T00:39:25Z",
        "updatedAt" : "2016-03-01T00:39:25Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "19811994-e788-46b4-b19d-af56c1a91b36",
        "parentId" : "638992af-cafd-41ef-a01d-afc7fd0a69f5",
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "That's what I had this morning, but @gmarek convinced me otherwise. We don't expect any _actual_ pods to be running, though they may still be represented in the apiserver. If I understand the system, it should come out fine whether we queue or directly terminate. \n\nIf we try to directly terminate and it fails for whatever reason, we'll bail, but next scan through the nodes we should attempt to evict/terminate again.\n\nIf we add to the evictor queue, we might get more retries before the next full node scan, but we may also be backing up the rate-limited piece for no good reason.\n\nIf that description doesn't match with what you think would actually happen, I'm happy to change the PR.\n",
        "createdAt" : "2016-03-01T01:40:43Z",
        "updatedAt" : "2016-03-01T01:40:43Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      },
      {
        "id" : "80c156b1-7bc8-4883-9c3b-6693acb46ae2",
        "parentId" : "638992af-cafd-41ef-a01d-afc7fd0a69f5",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yup - I don't think that rate limiting makes any sense here. This is a branch responsible for evicting Pods from Nodes that are gone from the cloud provider's perspective. If for whatever reason we would be able to contact this Node it would mean that there's some serious bug in the control plane of the given provider. \n",
        "createdAt" : "2016-03-01T08:48:37Z",
        "updatedAt" : "2016-03-01T08:48:37Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a8c7a70749d28907308b7042117fda32d676e7b",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +495,499 @@\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t// Immediately terminate pods.\n\t\t\t\t\t\t\tif _, _, err := nc.terminatePods(nodeName, time.Now()); err != nil {\n\t\t\t\t\t\t\t\tglog.Errorf(\"Unable to terminate pods on node %s: %v\", nodeName, err)"
  },
  {
    "id" : "ce776b57-245c-4a47-8dbd-96c331ceea34",
    "prId" : 21172,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05413604-b29f-4470-a754-225360a87990",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "@cjcullen - FYI you can solve this issue by just setting ResourceVersion to 0 just before the update.\nIf ResourveVersion is set to 0, this is treated as \"unconditional update\" and will be served correctly in pkg/registry/generic/etcd code:\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/registry/generic/etcd/etcd.go#L252\n",
        "createdAt" : "2016-02-15T14:23:02Z",
        "updatedAt" : "2016-02-15T14:23:02Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "2d80cab4-dae2-4a55-a035-8cf1de51fc25",
        "parentId" : "05413604-b29f-4470-a754-225360a87990",
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "That feels scary. If there was a meaningful update in between my read and write, wouldn't I clobber it?\n",
        "createdAt" : "2016-02-16T04:08:49Z",
        "updatedAt" : "2016-02-16T04:08:49Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      },
      {
        "id" : "9f273774-918f-47a2-abbc-3826b9fe4b17",
        "parentId" : "05413604-b29f-4470-a754-225360a87990",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Yes - it would.\n",
        "createdAt" : "2016-02-16T07:22:37Z",
        "updatedAt" : "2016-02-16T07:22:37Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "52b16129dc7212f5d07ef973e8333d1023b50bb9",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +528,532 @@\t\t\tglog.V(4).Infof(\"Assigning node %s CIDR %s\", n.Name, podCIDR)\n\t\t\tn.Spec.PodCIDR = podCIDR\n\t\t\tif _, err := nc.kubeClient.Core().Nodes().Update(n); err != nil {\n\t\t\t\tnc.recordNodeStatusChange(&node, \"CIDRAssignmentFailed\")\n\t\t\t}"
  },
  {
    "id" : "01fb91ee-26ce-48cd-b81d-41bf88c780eb",
    "prId" : 19242,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d91efaa-6187-4a13-8440-caa5ce2364ce",
        "parentId" : null,
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "I'm not very familiar with the ResourceEventHandlers, but I don't think this would handle crashes/restarts well.\n\nE.g., if the nodeController restarts, it is possible that the next new node is issued a CIDR that was previously issued to someone else, right?\n",
        "createdAt" : "2016-01-25T23:35:39Z",
        "updatedAt" : "2016-05-20T12:45:16Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      },
      {
        "id" : "838e1fee-d42f-4c4a-86da-d6208584fb50",
        "parentId" : "8d91efaa-6187-4a13-8440-caa5ce2364ce",
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "if the nodeController restarts, ResourceEventHandlers would observe some Add Event. So we need mark the CIDR has been occupied if `node.Spec.PodCIDR != \"\"`, sorry for the missing.\n",
        "createdAt" : "2016-01-26T07:26:21Z",
        "updatedAt" : "2016-05-20T12:45:16Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      }
    ],
    "commit" : "552a247639f2e426c2533ed78bbaacae096269d2",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +222,226 @@\tnodeEventHandlerFuncs := framework.ResourceEventHandlerFuncs{}\n\tif nc.allocateNodeCIDRs {\n\t\tnodeEventHandlerFuncs = framework.ResourceEventHandlerFuncs{\n\t\t\tAddFunc:    nc.allocateOrOccupyCIDR,\n\t\t\tDeleteFunc: nc.recycleCIDR,"
  },
  {
    "id" : "24d12d9e-85e8-488b-a598-1f321656f6b4",
    "prId" : 19242,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75441616-5a62-4567-a744-493cc78ad31d",
        "parentId" : null,
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "If this happens, do we retry?\n\nCan we also log the error?\n",
        "createdAt" : "2016-02-10T19:22:06Z",
        "updatedAt" : "2016-05-20T12:45:16Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      }
    ],
    "commit" : "552a247639f2e426c2533ed78bbaacae096269d2",
    "line" : null,
    "diffHunk" : "@@ -1,1 +388,392 @@\tif err != nil {\n\t\tglog.Errorf(\"Update PodCIDR of node %v from NodeController exceeds retry count.\", node.Name)\n\t\tnc.recordNodeStatusChange(node, \"CIDRAssignmentFailed\")\n\t\tglog.Errorf(\"CIDR assignment for node %v failed: %v\", node.Name, err)\n\t}"
  },
  {
    "id" : "ab6a6e42-94c3-41c2-bd01-f74d554a6e20",
    "prId" : 19242,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a635844-6789-4f3b-af72-ef1d68feadf5",
        "parentId" : null,
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "Probably need some sort of retry here too.\n",
        "createdAt" : "2016-02-10T19:35:28Z",
        "updatedAt" : "2016-05-20T12:45:16Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      },
      {
        "id" : "a6600990-f2b5-42a2-83c7-1f16d51ef9d2",
        "parentId" : "1a635844-6789-4f3b-af72-ef1d68feadf5",
        "authorId" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "body" : "I have no idea how to implement the retry logic, since `AllocateNext()`has search through the range to find available ones. If `AllocateNext()` failed, it will fail again (unless a node be deleted and its podCIDR be released during retry). If we want add the retry logic here, may be my first version of this PR(using channel and timeout) is much better. Do you have any thoughts?\n",
        "createdAt" : "2016-02-11T02:36:40Z",
        "updatedAt" : "2016-05-20T12:45:16Z",
        "lastEditedBy" : "fa03ed6a-9d2a-44b6-8438-e21ee4a2ce4d",
        "tags" : [
        ]
      },
      {
        "id" : "d3ddc305-fd56-44b0-bfb2-6da156d79824",
        "parentId" : "1a635844-6789-4f3b-af72-ef1d68feadf5",
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "This is fine how it is. It's definitely no worse than what we have now.\n",
        "createdAt" : "2016-05-17T05:55:15Z",
        "updatedAt" : "2016-05-20T12:45:16Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      }
    ],
    "commit" : "552a247639f2e426c2533ed78bbaacae096269d2",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +369,373 @@\tpodCIDR, err := nc.cidrAllocator.AllocateNext()\n\tif err != nil {\n\t\tnc.recordNodeStatusChange(node, \"CIDRNotAvailable\")\n\t\treturn\n\t}"
  },
  {
    "id" : "ca6e46fc-34ad-44bc-8886-84794b85b16d",
    "prId" : 19242,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0e83b86-0aa7-4ee5-9033-968296b81034",
        "parentId" : null,
        "authorId" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "body" : "Would be nice to log the error from the failed update.\n",
        "createdAt" : "2016-05-02T21:14:29Z",
        "updatedAt" : "2016-05-20T12:45:16Z",
        "lastEditedBy" : "3cd3a661-80f4-45b3-bae0-5a78fbaedc59",
        "tags" : [
        ]
      }
    ],
    "commit" : "552a247639f2e426c2533ed78bbaacae096269d2",
    "line" : null,
    "diffHunk" : "@@ -1,1 +379,383 @@\t\t\tglog.Errorf(\"Failed while updating Node.Spec.PodCIDR : %v\", err)\n\t\t\tbreak\n\t\t}\n\t\tnode, err = nc.kubeClient.Core().Nodes().Get(node.Name)\n\t\tif err != nil {"
  },
  {
    "id" : "37e3b9ab-fd86-412f-a4e6-2af365d371c9",
    "prId" : 17754,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6c77bb6-e424-45a8-b58a-f9ea7997f94b",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "godoc style: translateCIDRs does\n",
        "createdAt" : "2015-11-26T01:30:17Z",
        "updatedAt" : "2015-12-28T19:16:43Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "3928bd6e76c578147588336b7e36f674ec610f1d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +291,295 @@// translateCIDRs translates pod CIDR index to the CIDR that could be\n// assigned to node. It will also check for overflow which make sure CIDR is valid\nfunc translateCIDRs(clusterCIDR *net.IPNet, num int) string {\n\tcidrIP := clusterCIDR.IP.To4()\n\t// TODO: Make the CIDRs configurable."
  },
  {
    "id" : "08ba46bd-3fb7-4a6d-9b95-ccd4b75e119b",
    "prId" : 17754,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0c35f7a-4d70-4b72-b6ff-7c39e36ceea3",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Add the same TODO as in generateCIDRs: `// TODO: Make the CIDRs configurable.`\n\nAlso delete generateCIDRs, as it should be unused now.\n",
        "createdAt" : "2015-11-26T12:14:58Z",
        "updatedAt" : "2015-12-28T19:16:43Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3928bd6e76c578147588336b7e36f674ec610f1d",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +300,304 @@\t\tb1 = b1 + 1\n\t}\n\tres := fmt.Sprintf(\"%d.%d.%d.0/24\", cidrIP[0], b1, b2)\n\treturn res\n}"
  },
  {
    "id" : "00977c2c-6a84-4559-8325-f387599d86fb",
    "prId" : 17754,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ad9fcc3-bed6-48eb-ab3c-b7e9366f033a",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "What if two Nodes have the same CIDR for some reason (e.g. one Node got disconnected for an hour but came back)? We'll generate all CIDRs and fall back to reassigning CIDR to one of them. As we're generating CIDRs in order can we check if we're in the range of 'known' ones?\n",
        "createdAt" : "2015-12-16T08:55:16Z",
        "updatedAt" : "2015-12-28T19:16:43Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "861e3996-6e15-40bc-afcd-45c13495e3b9",
        "parentId" : "4ad9fcc3-bed6-48eb-ab3c-b7e9366f033a",
        "authorId" : "541d4cba-fc4b-4592-b84c-2ecac3de1e07",
        "body" : "Sorry, I am a little bit confused about the case you described. Do you mean if one Node got disconnected when we running the nc.needSync code(above code), its CIDR will not be deleted from the availableCIDRs set and may be reassign to another Node? If this is the case, it will be a little bit complicated. You are right, if we simply check the CIDR range of known nodes, it could be solved. But it will waste a lot of CIDRs for example, we have 1000 nodes at first and delete the first 999 nodes and then restart the node_controller. In this case, the first 999 CIDRs will be wasted and we will start assigning from 1001st CIDR.\n\nThe other solution will be maintaining a bool array to mark if that CIDR has been used. As you comment below, we currently has 3072 CIDRs in default setup and it will be ok to maintain a bool array(since we will use less than 1MB memory). It will both simplify the code and solve this problem, I will update the code in this way. Thanks\n",
        "createdAt" : "2015-12-16T17:36:45Z",
        "updatedAt" : "2015-12-28T19:16:43Z",
        "lastEditedBy" : "541d4cba-fc4b-4592-b84c-2ecac3de1e07",
        "tags" : [
        ]
      },
      {
        "id" : "4910fc4f-4a82-4100-b6ce-a0e044511ad6",
        "parentId" : "4ad9fcc3-bed6-48eb-ab3c-b7e9366f033a",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Sorry, you're right. I was thinking about the scenario when Node becomes unresponsive and comes back again. But either we'll delete Node object for it, and everything will need to be recreated from the start, or it won't be deleted, and nothing else will get CIDR assigned to it.\n",
        "createdAt" : "2015-12-17T08:32:49Z",
        "updatedAt" : "2015-12-28T19:16:43Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3928bd6e76c578147588336b7e36f674ec610f1d",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +543,547 @@\t\t\tif node.Spec.PodCIDR != \"\" {\n\t\t\t\tif nc.availableCIDRs.Has(nc.translateCIDRtoIndex(node.Spec.PodCIDR)) {\n\t\t\t\t\tnc.availableCIDRs.Delete(nc.translateCIDRtoIndex(node.Spec.PodCIDR))\n\t\t\t\t} else {\n\t\t\t\t\tglog.V(4).Info(\"Node %s CIDR error, its CIDR is invalid, will reassign CIDR\", node.Name)"
  },
  {
    "id" : "5c19e82c-b1b6-4d15-8f58-67d6ce45d009",
    "prId" : 17754,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96945397-61b9-4511-bd47-ecbd5a8656ed",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "can you make this []bool?\n",
        "createdAt" : "2015-12-23T10:57:00Z",
        "updatedAt" : "2015-12-28T19:16:43Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "ebffff5d-908b-4a70-ad8a-c257c2472661",
        "parentId" : "96945397-61b9-4511-bd47-ecbd5a8656ed",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Or let's forget about this comment. It's probably not worth the effort. Hash maps are generally expensive, AFAIK go implementation uses roughly x30 of the size of key-value, but this would put us in still reasonable amount of 4 \\* 30 \\* 3072 = 360kB.\n",
        "createdAt" : "2015-12-23T11:14:19Z",
        "updatedAt" : "2015-12-28T19:16:43Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3928bd6e76c578147588336b7e36f674ec610f1d",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +114,118 @@\n\tforcefullyDeletePod func(*api.Pod)\n\tavailableCIDRs      sets.Int\n\t// Calculate the maximum num of CIDRs we could give out based on nc.clusterCIDR\n\t// The flag denoting if the node controller is newly started or restarted(after crash)"
  },
  {
    "id" : "9fc5ab2e-47c9-40c5-b498-ab440f94bb35",
    "prId" : 17741,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d6f81af-53c4-4fea-8979-fea10a2594e2",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Not sure why this method is in a poll loop instead of triggered by a watch (not your problem, just saying), there's a chance it will still not realize as soon as a node falls off the netowork so the reboot test (the one that needs the pods to become not ready) can still fail.  How often does this poll loop run? \n",
        "createdAt" : "2016-01-15T01:54:21Z",
        "updatedAt" : "2016-01-15T01:54:21Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "75e088ab-77e7-408c-886d-556e767c1dd4",
        "parentId" : "1d6f81af-53c4-4fea-8979-fea10a2594e2",
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "Looks like it runs every 5 seconds... The timeout for node notready is 40 secs. So the worst case is 45 secs to flip node to notready. \n",
        "createdAt" : "2016-01-15T18:32:41Z",
        "updatedAt" : "2016-01-15T18:32:41Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      }
    ],
    "commit" : "01829432db07431689cfc0942a15e2dc58bf1fe6",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +491,495 @@\t\t\tif readyCondition.Status != api.ConditionTrue && lastReadyCondition.Status == api.ConditionTrue {\n\t\t\t\tnc.recordNodeStatusChange(node, \"NodeNotReady\")\n\t\t\t\tif err = nc.markAllPodsNotReady(node.Name); err != nil {\n\t\t\t\t\tutil.HandleError(fmt.Errorf(\"Unable to mark all pods NotReady on node %v: %v\", node.Name, err))\n\t\t\t\t}"
  },
  {
    "id" : "a3f52bf2-f43a-4ee2-8e25-58ecc3698fff",
    "prId" : 15930,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ff2756a-ba7b-49ce-8b82-c4e1af3947aa",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "I could have sworn we already had code somewhere that deleted pods that are on nodes that no longer exist, but it wasn't in the GC controller so maybe I'm just hallucinating?\n",
        "createdAt" : "2015-10-20T21:19:36Z",
        "updatedAt" : "2015-10-21T20:07:14Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      },
      {
        "id" : "19d4c43d-e1c3-4135-bbcb-4dd3a46d0696",
        "parentId" : "9ff2756a-ba7b-49ce-8b82-c4e1af3947aa",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "You are hallucinating. It's one of your left to do bullets.\n\nhttps://github.com/kubernetes/kubernetes/issues/7660#issuecomment-146079112\n",
        "createdAt" : "2015-10-20T22:06:12Z",
        "updatedAt" : "2015-10-21T20:07:14Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "a7b3b802-89f4-41bd-8745-b22f7790d095",
        "parentId" : "9ff2756a-ba7b-49ce-8b82-c4e1af3947aa",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "We don't - we opened an issue to add it.\n\nOn Tue, Oct 20, 2015 at 5:19 PM, David Oppenheimer <notifications@github.com\n\n> wrote:\n> \n> In pkg/controller/node/nodecontroller.go\n> https://github.com/kubernetes/kubernetes/pull/15930#discussion_r42556056\n> :\n> \n> > +\n> > -   if len(pod.Spec.NodeName) == 0 {\n> > -       nc.forcefullyDeletePod(pod)\n> > -       return\n> > -   }\n> >   +\n> > -   nodeObj, found, err := nc.nodeStore.GetByKey(pod.Spec.NodeName)\n> > -   if err != nil {\n> > -       // what do\n> > -       util.HandleError(err)\n> > -       nc.podController.Requeue(pod)\n> > -       return\n> > -   }\n> >   +\n> > -   if !found {\n> > -       nc.forcefullyDeletePod(pod)\n> \n> I could have sworn we already had code somewhere that deleted pods that\n> are on nodes that no longer exist, but it wasn't in the GC controller so\n> maybe I'm just hallucinating?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/kubernetes/kubernetes/pull/15930/files#r42556056.\n",
        "createdAt" : "2015-10-20T22:06:28Z",
        "updatedAt" : "2015-10-21T20:07:14Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "836b68368bb0a94ced0817457f3cc7e7a1197150",
    "line" : null,
    "diffHunk" : "@@ -1,1 +320,324 @@\t// nonexistant nodes\n\tif !found {\n\t\tnc.forcefullyDeletePod(pod)\n\t\treturn\n\t}"
  },
  {
    "id" : "83729ce5-92f1-42f6-972d-6897f583ce07",
    "prId" : 12962,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9133a79-2708-4d45-9d26-83193dbe4ba4",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "We probably want a flag for this.\n",
        "createdAt" : "2015-08-24T12:47:17Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "940cc2837ca255a5d35813bce2c2a957d43590d3",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +127,131 @@\t\trecorder:               recorder,\n\t\tpodEvictionTimeout:     podEvictionTimeout,\n\t\tmaximumGracePeriod:     5 * time.Minute,\n\t\tpodEvictor:             NewRateLimitedTimedQueue(podEvictionLimiter, false),\n\t\tterminationEvictor:     NewRateLimitedTimedQueue(podEvictionLimiter, false),"
  },
  {
    "id" : "bf3ac4b1-59ec-4ea8-88b2-6c55b2a968f6",
    "prId" : 12962,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc060f2d-c1df-4a83-b912-7332f4846dfb",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "I'd call variable timedNodeName, or something else more descriptive than plain 'value'. Same below.\n",
        "createdAt" : "2015-08-24T12:57:54Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "30f9c993-8a0b-4b17-94c1-d13a84be602d",
        "parentId" : "bc060f2d-c1df-4a83-b912-7332f4846dfb",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "The queue was intended to be generic enough to reuse (it's useful for any\nclient code that wants to wait X time before retrying an item again, like\ncontrollers if a retry is needed), could be an interface{} but that was\nmore changes.\n\nOn Aug 24, 2015, at 9:58 AM, Marek Grabowski notifications@github.com\nwrote:\n\nIn pkg/controller/node/nodecontroller.go\nhttps://github.com/kubernetes/kubernetes/pull/12962#discussion_r37747669:\n\n> @@ -145,38 +150,43 @@ func (nc *NodeController) Run(period time.Duration) {\n>   }, nc.nodeMonitorPeriod)\n> \n>   go util.Forever(func() {\n> -     nc.podEvictor.TryEvict(func(nodeName string) { nc.deletePods(nodeName) })\n> -     nc.podEvictor.Try(func(value TimedValue) (bool, time.Duration) {\n\nI'd call variable timedNodeName, or something else more descriptive than\nplain 'value'\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/12962/files#r37747669.\n",
        "createdAt" : "2015-08-24T15:57:17Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "f8b7349d-6a0e-4442-90c2-2ef7bbb1bd6d",
        "parentId" : "bc060f2d-c1df-4a83-b912-7332f4846dfb",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Yes, TimedValue as a type is OK, I'm saying that the variable name can be more descriptive. At first I wasn't certain if Value stores Node name or Pod name.\n",
        "createdAt" : "2015-08-24T20:37:43Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "940cc2837ca255a5d35813bce2c2a957d43590d3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +163,167 @@\t//       before retrying\n\tgo util.Until(func() {\n\t\tnc.podEvictor.Try(func(value TimedValue) (bool, time.Duration) {\n\t\t\tremaining, err := nc.deletePods(value.Value)\n\t\t\tif err != nil {"
  },
  {
    "id" : "33c8d3fb-ff40-4dc7-b471-57aa6c4cdee9",
    "prId" : 12962,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7742d35e-ac60-4c99-82d0-2670e10b60c7",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "You can return false, err here.\n",
        "createdAt" : "2015-08-24T13:27:00Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "940cc2837ca255a5d35813bce2c2a957d43590d3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +569,573 @@\tpods, err := nc.kubeClient.Pods(api.NamespaceAll).List(labels.Everything(), fields.OneTermEqualSelector(client.PodHost, nodeName))\n\tif err != nil {\n\t\treturn remaining, err\n\t}\n"
  },
  {
    "id" : "73252ab4-33d9-4aec-bf7f-8434855b105c",
    "prId" : 12962,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e2a39a2-f77c-4b06-b1b8-200874e02257",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "`remaining` is a really bad name for this variable.\n",
        "createdAt" : "2015-08-24T13:28:03Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "940cc2837ca255a5d35813bce2c2a957d43590d3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +566,570 @@// if any pods were deleted.\nfunc (nc *NodeController) deletePods(nodeName string) (bool, error) {\n\tremaining := false\n\tpods, err := nc.kubeClient.Pods(api.NamespaceAll).List(labels.Everything(), fields.OneTermEqualSelector(client.PodHost, nodeName))\n\tif err != nil {"
  },
  {
    "id" : "b9c8d1a6-3d07-408e-aa3a-5653a8dca310",
    "prId" : 12962,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e2858af-b574-40e6-9ccc-f29c9524a5ca",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Is this backward compatible? IIUC `DeletionGracePeriodSeconds` was introduced quite recently, and thus will be `nil` in all 'upgraded' clusters.\n",
        "createdAt" : "2015-08-24T13:32:55Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "a3eaf067-85e2-47bf-9520-270ac8ac2f9f",
        "parentId" : "4e2858af-b574-40e6-9ccc-f29c9524a5ca",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "It's defaulted by the API so cannot be nil - it becomes 30s everywhere\n\nOn Aug 24, 2015, at 10:33 AM, Marek Grabowski notifications@github.com\nwrote:\n\nIn pkg/controller/node/nodecontroller.go\nhttps://github.com/kubernetes/kubernetes/pull/12962#discussion_r37750787:\n\n> - pods, err := nc.kubeClient.Pods(api.NamespaceAll).List(labels.Everything(), fields.OneTermEqualSelector(client.PodHost, nodeID))\n> - if err != nil {\n> -     return remaining, err\n> - }\n>   +\n> - if len(pods.Items) > 0 {\n> -     nc.recordNodeEvent(nodeID, \"DeletingAllPods\", fmt.Sprintf(\"Deleting all Pods from Node %v.\", nodeID))\n> - }\n>   +\n> - for _, pod := range pods.Items {\n> -     // Defensive check, also needed for tests.\n> -     if pod.Spec.NodeName != nodeID {\n> -         continue\n> -     }\n> -     // if the pod has already been deleted, ignore it\n> -     if pod.DeletionGracePeriodSeconds != nil {\n\nIs this backward compatible? IIUC DeletionGracePeriodSeconds was introduced\nquite recently, and thus will be nil in all 'upgraded' clusters.\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/12962/files#r37750787.\n",
        "createdAt" : "2015-08-24T22:24:43Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "940cc2837ca255a5d35813bce2c2a957d43590d3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +582,586 @@\t\t}\n\t\t// if the pod has already been deleted, ignore it\n\t\tif pod.DeletionGracePeriodSeconds != nil {\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "82b2cb4d-cf2e-4bf8-9abf-019341b8d845",
    "prId" : 12962,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41bfa8e5-9365-4bf5-83cf-0429d02543d0",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Do we really want to share the same RateLimiter between podeEvictor and terminationEvictor? I'm not against it, but in general I don't like sharing state if it gives nothing except coding convenience.\n",
        "createdAt" : "2015-08-24T13:47:28Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "9bcd2484-a60b-4647-98d6-6720607c3102",
        "parentId" : "41bfa8e5-9365-4bf5-83cf-0429d02543d0",
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "I think they are effectively the same pool - the goal seems to be to\nprevent the node controller from slaughtering the master during\nnegotiations eviction, and delete and terminate are two parts of the same\nprocess.  Up to you, they don't seem separate to me.\n\nOn Aug 24, 2015, at 10:47 AM, Marek Grabowski notifications@github.com\nwrote:\n\nIn pkg/controller/node/nodecontroller.go\nhttps://github.com/kubernetes/kubernetes/pull/12962#discussion_r37752204:\n\n> @@ -123,7 +126,9 @@ func NewNodeController(\n>       kubeClient:             kubeClient,\n>       recorder:               recorder,\n>       podEvictionTimeout:     podEvictionTimeout,\n> -     podEvictor:             podEvictor,\n> -     maximumGracePeriod:     5 \\* time.Minute,\n> -     podEvictor:             NewRateLimitedTimedQueue(podEvictionLimiter, false),\n> -     terminationEvictor:     NewRateLimitedTimedQueue(podEvictionLimiter, false),\n\nDo we really want to share the same RateLimiter between podeEvictor and\nterminationEvictor? I'm not against it, but in general I don't like sharing\nstate if it gives nothing except coding convenience.\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/kubernetes/pull/12962/files#r37752204.\n",
        "createdAt" : "2015-08-24T22:26:00Z",
        "updatedAt" : "2015-08-25T17:19:23Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "940cc2837ca255a5d35813bce2c2a957d43590d3",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +129,133 @@\t\tmaximumGracePeriod:     5 * time.Minute,\n\t\tpodEvictor:             NewRateLimitedTimedQueue(podEvictionLimiter, false),\n\t\tterminationEvictor:     NewRateLimitedTimedQueue(podEvictionLimiter, false),\n\t\tnodeStatusMap:          make(map[string]nodeStatusData),\n\t\tnodeMonitorGracePeriod: nodeMonitorGracePeriod,"
  },
  {
    "id" : "10096f9c-c5c5-421d-bb28-11c9fd333e4e",
    "prId" : 12546,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04794c4f-814e-4c8f-b54e-44aa4b8a1251",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "It might be worth a comment here that the node isn't deleted until all pods assigned to the node have been deleted. \n",
        "createdAt" : "2015-08-11T20:06:47Z",
        "updatedAt" : "2015-08-11T20:06:47Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "34eec1fc78484c7bca21a675dadde730d08139bd",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +304,308 @@\t\t\t\t\tglog.Infof(\"Deleting node (no longer present in cloud provider): %s\", node.Name)\n\t\t\t\t\tnc.recordNodeEvent(node.Name, fmt.Sprintf(\"Deleting Node %v because it's not present according to cloud provider\", node.Name))\n\t\t\t\t\tif err := nc.deletePods(node.Name); err != nil {\n\t\t\t\t\t\tglog.Errorf(\"Unable to delete pods from node %s: %v\", node.Name, err)\n\t\t\t\t\t\tcontinue"
  },
  {
    "id" : "a6fab5f9-6ea8-49d6-8f49-9f14a4c4d3e6",
    "prId" : 12546,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6487569-a10b-4b94-bb8d-80f7148bca0a",
        "parentId" : null,
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "I think there's still a (small) window for a race here, between deleting pods and deleting the node the scheduler could assign new pods to this node. \n",
        "createdAt" : "2015-08-11T20:07:12Z",
        "updatedAt" : "2015-08-11T20:07:12Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "354951c5-6c70-4db0-95c4-dbc0a9a73561",
        "parentId" : "d6487569-a10b-4b94-bb8d-80f7148bca0a",
        "authorId" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "body" : "Wont the Scheduler not assign pods to nodes with readyCondition.Status != api.ConditionTrue?\n",
        "createdAt" : "2015-08-11T20:40:30Z",
        "updatedAt" : "2015-08-11T20:40:30Z",
        "lastEditedBy" : "f2369046-26b1-4b8c-a8cd-5671ab22066c",
        "tags" : [
        ]
      },
      {
        "id" : "3d3910e6-d757-4dc7-858f-a4d21df36f24",
        "parentId" : "d6487569-a10b-4b94-bb8d-80f7148bca0a",
        "authorId" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "body" : "That could be (I don't know much about the scheduler internals). \n",
        "createdAt" : "2015-08-11T20:45:41Z",
        "updatedAt" : "2015-08-11T20:45:41Z",
        "lastEditedBy" : "c2b5c827-efcd-438f-8db5-52d917b1cde9",
        "tags" : [
        ]
      },
      {
        "id" : "2dbf7b6f-5029-4fba-962d-a6b90ac02744",
        "parentId" : "d6487569-a10b-4b94-bb8d-80f7148bca0a",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "There's still going to be a race, since we don't know when the scheduler has observed the condition.\n\nAlso, it looks like monitorNodeStatus doesn't delete pods at all.\n",
        "createdAt" : "2015-08-11T20:53:11Z",
        "updatedAt" : "2015-08-11T20:53:11Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "5e8b824b-fbd9-4a76-8ff8-a8add9f2b8ac",
        "parentId" : "d6487569-a10b-4b94-bb8d-80f7148bca0a",
        "authorId" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "body" : "That race is super unlikely, as the lag between not ready and node deletion is typically 5 minutes.\n",
        "createdAt" : "2015-08-11T21:12:55Z",
        "updatedAt" : "2015-08-11T21:12:55Z",
        "lastEditedBy" : "d0e97b49-eba2-4b22-8695-df4f8a6776ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "34eec1fc78484c7bca21a675dadde730d08139bd",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +308,312 @@\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif err := nc.kubeClient.Nodes().Delete(node.Name); err != nil {\n\t\t\t\t\t\tglog.Errorf(\"Unable to delete node %s: %v\", node.Name, err)\n\t\t\t\t\t\tcontinue"
  },
  {
    "id" : "5735c2e7-ffb8-444d-8ed1-82eb363bff73",
    "prId" : 12277,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31df51ce-b6da-47f4-8304-c9680d840b66",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Since you're modifying knowNodeSet here, can you please remove \"oldSize\" and \"addedNodes\" - these are not needed now.\n",
        "createdAt" : "2015-08-06T12:39:17Z",
        "updatedAt" : "2015-08-06T13:17:54Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "1ded02bf-59b5-48d7-8215-5b4286bfb530",
        "parentId" : "31df51ce-b6da-47f4-8304-c9680d840b66",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "It's for quick check if we deleted something or not. Difference of sets is slower. If you already mentioned it, I'd rather move construction of observedSet into the second if, as it'll be rather rare (hopefully)\n",
        "createdAt" : "2015-08-06T12:59:08Z",
        "updatedAt" : "2015-08-06T13:17:54Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "43418cc6-6bd9-4be2-bbb4-ba69aaa39e0a",
        "parentId" : "31df51ce-b6da-47f4-8304-c9680d840b66",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "But you can do:\n\"if len(ns.knownNodeSet) != len(nodes.Items)\"\n\nIt would be exactly the same.\n",
        "createdAt" : "2015-08-06T13:06:16Z",
        "updatedAt" : "2015-08-06T13:17:54Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "da3ab014-9f5b-4e4c-b4b0-fc79ec90b410",
        "parentId" : "31df51ce-b6da-47f4-8304-c9680d840b66",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Done.\n",
        "createdAt" : "2015-08-06T13:24:56Z",
        "updatedAt" : "2015-08-06T13:24:56Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "819777e9e8f4bc4ef701276e2393eef8e158d14f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +216,220 @@\t\t\tglog.V(1).Infof(\"NodeController observed a new Node: %#v\", node)\n\t\t\tnc.recordNodeEvent(node.Name, fmt.Sprintf(\"Registered Node %v in NodeController\", node.Name))\n\t\t\tnc.knownNodeSet.Insert(node.Name)\n\t\t}\n\t}"
  }
]