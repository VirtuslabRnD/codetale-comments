[
  {
    "id" : "829bd2ed-5661-4392-87f2-f58bd1a718f2",
    "prId" : 97348,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/97348#pullrequestreview-565438199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "762cd8ef-4e15-46ac-ac0e-333ec1aa150e",
        "parentId" : null,
        "authorId" : "63c3448a-59cc-42ef-ba34-6d817ae5d474",
        "body" : "Digress: Why don't we clean up all the old samples instead of the last one? I am always confused here when reading the code.",
        "createdAt" : "2020-12-24T18:15:26Z",
        "updatedAt" : "2020-12-24T18:15:26Z",
        "lastEditedBy" : "63c3448a-59cc-42ef-ba34-6d817ae5d474",
        "tags" : [
        ]
      },
      {
        "id" : "83082ec9-4882-464e-8410-dcf46307d0f5",
        "parentId" : "762cd8ef-4e15-46ac-ac0e-333ec1aa150e",
        "authorId" : "54bb32ac-99a3-4652-9214-863be2762be8",
        "body" : "Agreed, what if we have a user initially setting very long stabilisation windows for one or both downscaling and upscaling, followed by reducing the windows significantly? \r\n\r\nIt seems we'll end up with a long list of `timestampedRecommendation`s to loop through and evaluate the timestamp of each time we want to do stabilization, without ever replacing or removing them.",
        "createdAt" : "2021-01-05T19:24:22Z",
        "updatedAt" : "2021-01-05T19:24:22Z",
        "lastEditedBy" : "54bb32ac-99a3-4652-9214-863be2762be8",
        "tags" : [
        ]
      },
      {
        "id" : "22cc3090-7bcc-44e5-b81f-39fc4c6ddfb3",
        "parentId" : "762cd8ef-4e15-46ac-ac0e-333ec1aa150e",
        "authorId" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "body" : "This was a premature optimization to reduce the number of memory allocations.  As you correctly point out, it doesn't ever compact the memory.  `¯\\_(ツ)_/¯`",
        "createdAt" : "2021-01-11T15:04:40Z",
        "updatedAt" : "2021-01-11T15:04:53Z",
        "lastEditedBy" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "tags" : [
        ]
      }
    ],
    "commit" : "16133c2b773a223468aae42466322b1920c55208",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +912,916 @@\n\t// Record the unstabilized recommendation.\n\tif foundOldSample {\n\t\ta.recommendations[args.Key][oldSampleIndex] = timestampedRecommendation{args.DesiredReplicas, time.Now()}\n\t} else {"
  },
  {
    "id" : "ce8a7dd7-1ec8-4402-a4c2-e8b7fe5bbd7d",
    "prId" : 90691,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90691#pullrequestreview-439226417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3105736e-d61f-4101-be60-bdfdd470046c",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "defer to @kubernetes/sig-autoscaling-pr-reviews on controller review",
        "createdAt" : "2020-06-29T15:37:21Z",
        "updatedAt" : "2020-10-21T19:17:39Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fec7b0f7e2dfd4b0c6c57e086472546f6c69efa",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +330,334 @@\t\t\treturn 0, \"\", time.Time{}, condition, err\n\t\t}\n\tcase autoscalingv2.ContainerResourceMetricSourceType:\n\t\treplicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForContainerResourceMetric(specReplicas, spec, hpa, selector, status)\n\t\tif err != nil {"
  },
  {
    "id" : "28bcb69b-dade-4d97-965a-69109e56f141",
    "prId" : 80700,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/80700#pullrequestreview-267618346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "448a184c-09af-4f87-b411-cf9410d771a0",
        "parentId" : null,
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "Looks Good",
        "createdAt" : "2019-07-29T07:54:15Z",
        "updatedAt" : "2019-07-29T07:54:20Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "4635f16dc1eff59629b8b6bd6cf7196cc8744950",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +343,347 @@\t\treturn true, nil\n\t}\n\tif err != nil {\n\t\treturn false, err\n\t}"
  },
  {
    "id" : "19182ae2-8652-4f95-b9b5-7257b4cd956b",
    "prId" : 72872,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72872#pullrequestreview-199154583",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c50c4252-4f20-4e9b-ba7e-43669c83c0a5",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "nit: 'else' can be omitted",
        "createdAt" : "2019-02-01T16:25:50Z",
        "updatedAt" : "2019-02-01T16:26:12Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "c99d505001e5d0ed80ab23ad361783bfcd9f3aff",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +342,346 @@\t\t}\n\t\treturn replicaCountProposal, timestampProposal, fmt.Sprintf(\"%s metric %s\", metricSpec.Object.DescribedObject.Kind, metricSpec.Object.Metric.Name), nil\n\t} else if metricSpec.Object.Target.Type == autoscalingv2.AverageValueMetricType {\n\t\treplicaCountProposal, utilizationProposal, timestampProposal, err := a.replicaCalc.GetObjectPerPodMetricReplicas(currentReplicas, metricSpec.Object.Target.AverageValue.MilliValue(), metricSpec.Object.Metric.Name, hpa.Namespace, &metricSpec.Object.DescribedObject, metricSelector)\n\t\tif err != nil {"
  },
  {
    "id" : "af845bbc-7392-40df-aad8-96a91aae159d",
    "prId" : 72629,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72629#pullrequestreview-189722490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97ad1b0b-96ce-4666-b45a-2a1f6a19122f",
        "parentId" : null,
        "authorId" : "e346c770-7799-476e-a860-ed14eed94c66",
        "body" : "\"Request can be added by resync before previous request is\r\n\t// processed. In this case request to be executed after resyncPeriod is dropped and HPA is processed\r\n\t// after 2 x resyncPeriod.\"\r\n\r\nIs that what happens right now. Or what would happen without the logic below?",
        "createdAt" : "2019-01-07T08:48:37Z",
        "updatedAt" : "2019-01-07T09:06:54Z",
        "lastEditedBy" : "e346c770-7799-476e-a860-ed14eed94c66",
        "tags" : [
        ]
      },
      {
        "id" : "29efaf37-8c3f-4acb-8c26-a32affba0933",
        "parentId" : "97ad1b0b-96ce-4666-b45a-2a1f6a19122f",
        "authorId" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "body" : "Request created by rsync are dropped but we add request here to avoid processing hpa after 2xresyncPeriod.",
        "createdAt" : "2019-01-07T09:08:19Z",
        "updatedAt" : "2019-01-07T09:08:19Z",
        "lastEditedBy" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7498c14218403c9a713f9e0747f2c6794a0da9c7",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +220,224 @@\t// Request is added here just in case last resync didn't insert request into the queue. This\n\t// happens quite often because there is race condition between adding request after resyncPeriod\n\t// and removing them from queue. Request can be added by resync before previous request is\n\t// removed from queue. If we didn't add request here then in this case one request would be dropped\n\t// and HPA would processed after 2 x resyncPeriod."
  },
  {
    "id" : "f299b48c-f52c-4250-9bc1-8124b5be8a1e",
    "prId" : 72373,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72373#pullrequestreview-188237529",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "286f9bb3-def0-4c1b-8004-d224d7611e6c",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Am I reading this correctly that it *always* requeues items that still exist, even in non-error cases?",
        "createdAt" : "2018-12-28T02:26:21Z",
        "updatedAt" : "2019-01-04T11:00:38Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "eebcfcb3-3288-46fb-ae4f-a679feae8d31",
        "parentId" : "286f9bb3-def0-4c1b-8004-d224d7611e6c",
        "authorId" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "body" : "yes, see comment below",
        "createdAt" : "2018-12-28T07:47:13Z",
        "updatedAt" : "2019-01-04T11:00:38Z",
        "lastEditedBy" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c6ebd126a77e75e6f80e1cd59da6b887e783c7c4",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +216,220 @@\t// This happens quite often because requests from previous resync are removed from the queue at the same moment\n\t// as next resync inserts new requests.\n\tif !deleted {\n\t\ta.queue.AddRateLimited(key)\n\t}"
  },
  {
    "id" : "f81b76b7-fc39-4fd1-a29a-101acf8c861c",
    "prId" : 72373,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72373#pullrequestreview-189236716",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61922aaa-d623-4abf-a1ae-1b828aab96c9",
        "parentId" : null,
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "you should still have a comment as to why we're not forgetting the key, since this works significantly different from normal controllers.",
        "createdAt" : "2019-01-04T00:40:56Z",
        "updatedAt" : "2019-01-04T11:00:38Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      }
    ],
    "commit" : "c6ebd126a77e75e6f80e1cd59da6b887e783c7c4",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +208,212 @@\tdefer a.queue.Done(key)\n\n\tdeleted, err := a.reconcileKey(key.(string))\n\tif err != nil {\n\t\tutilruntime.HandleError(err)"
  },
  {
    "id" : "4ed165a2-d057-422e-bfad-62e8bdcb7ead",
    "prId" : 60243,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60243#pullrequestreview-99676813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b24fd7d9-c4ed-4729-8de9-ef33998b1b42",
        "parentId" : null,
        "authorId" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "body" : "Has it been verified manually E2e?",
        "createdAt" : "2018-02-27T13:14:05Z",
        "updatedAt" : "2018-02-27T13:14:06Z",
        "lastEditedBy" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "tags" : [
        ]
      },
      {
        "id" : "45922a10-b139-48dc-93e5-e293b58a15fc",
        "parentId" : "b24fd7d9-c4ed-4729-8de9-ef33998b1b42",
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "Yes",
        "createdAt" : "2018-02-27T13:15:10Z",
        "updatedAt" : "2018-02-27T13:15:10Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      }
    ],
    "commit" : "e58411c6000c0138cab57c7ffa9c5e9a27ae3d12",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +301,305 @@\t\t\t}\n\t\tcase autoscalingv2.ExternalMetricSourceType:\n\t\t\tif metricSpec.External.TargetAverageValue != nil {\n\t\t\t\treplicaCountProposal, utilizationProposal, timestampProposal, err = a.replicaCalc.GetExternalPerPodMetricReplicas(currentReplicas, metricSpec.External.TargetAverageValue.MilliValue(), metricSpec.External.MetricName, hpa.Namespace, metricSpec.External.MetricSelector)\n\t\t\t\tif err != nil {"
  },
  {
    "id" : "3cdd48f1-2a56-4d8d-8faf-809c5236cab8",
    "prId" : 60243,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60243#pullrequestreview-99676830",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c24736ba-a4c4-4473-8e96-6d72a0adc81b",
        "parentId" : null,
        "authorId" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "body" : "Has it been validated manually e2e?",
        "createdAt" : "2018-02-27T13:14:24Z",
        "updatedAt" : "2018-02-27T13:14:24Z",
        "lastEditedBy" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "tags" : [
        ]
      },
      {
        "id" : "c535eb32-1d49-4492-b5f8-393eb1eb765b",
        "parentId" : "c24736ba-a4c4-4473-8e96-6d72a0adc81b",
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "Yes",
        "createdAt" : "2018-02-27T13:15:13Z",
        "updatedAt" : "2018-02-27T13:15:13Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      }
    ],
    "commit" : "e58411c6000c0138cab57c7ffa9c5e9a27ae3d12",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +317,321 @@\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t} else if metricSpec.External.TargetValue != nil {\n\t\t\t\treplicaCountProposal, utilizationProposal, timestampProposal, err = a.replicaCalc.GetExternalMetricReplicas(currentReplicas, metricSpec.External.TargetValue.MilliValue(), metricSpec.External.MetricName, hpa.Namespace, metricSpec.External.MetricSelector)\n\t\t\t\tif err != nil {"
  },
  {
    "id" : "99f6a2c1-9436-4a8c-a35a-a6451d03a3e0",
    "prId" : 60243,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60243#pullrequestreview-99677366",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "836d2509-5867-4866-861f-e6626f742a85",
        "parentId" : null,
        "authorId" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "body" : "Do we have a check for this in validate logic?",
        "createdAt" : "2018-02-27T13:15:23Z",
        "updatedAt" : "2018-02-27T13:15:23Z",
        "lastEditedBy" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "tags" : [
        ]
      },
      {
        "id" : "b45b2706-5c77-4bfb-83d6-8d7b66ef6b66",
        "parentId" : "836d2509-5867-4866-861f-e6626f742a85",
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "Yes, https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/autoscaling/validation/validation.go#L200",
        "createdAt" : "2018-02-27T13:16:55Z",
        "updatedAt" : "2018-02-27T13:16:55Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      }
    ],
    "commit" : "e58411c6000c0138cab57c7ffa9c5e9a27ae3d12",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +333,337 @@\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terrMsg := \"invalid external metric source: neither a value target nor an average value target was set\"\n\t\t\t\ta.eventRecorder.Event(hpa, v1.EventTypeWarning, \"FailedGetExternalMetric\", errMsg)"
  },
  {
    "id" : "532c7a8a-e51f-435e-ba83-3e92209aecea",
    "prId" : 53690,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53690#pullrequestreview-68651441",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "549df886-8c3f-414c-9d14-1395c6d0d661",
        "parentId" : null,
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "This is basically an exact copy-paste of one of the cases below. Adding ifs checking whether we would trigger condition of a different case if we haven't already triggered the one we're in makes me think switch-case is not the right solution here and we should move to a sequence of ifs instead (perhaps contained in a separate function that can be unittested in isolation).\r\n\r\nEspecially since we already had a very similar bug caused by interaction of conditions in this switch-case: https://github.com/kubernetes/kubernetes/pull/48997.\r\n\r\nI'm ok with keeping this bugfix as is, but we should revisit this part of hpa, clean it up and make sure it's reasonably unittested to avoid another such issue in future. @mwielgus @DirectXMan12 ",
        "createdAt" : "2017-10-11T14:11:43Z",
        "updatedAt" : "2017-10-11T14:11:43Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      },
      {
        "id" : "51d3f057-3ad1-4eb5-a4c6-30b3ca72f726",
        "parentId" : "549df886-8c3f-414c-9d14-1395c6d0d661",
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "I've created an issue #53728 to do this.",
        "createdAt" : "2017-10-11T15:31:07Z",
        "updatedAt" : "2017-10-11T15:31:07Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      }
    ],
    "commit" : "75c38777ad164862baba831baaafd71d6bb886b9",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +427,431 @@\t\t\t// than the maximum number of replicas, we only\n\t\t\t// set the max number of replicas as desired.\n\t\t\tif scaleUpLimit > hpa.Spec.MaxReplicas {\n\t\t\t\tsetCondition(hpa, autoscalingv2.ScalingLimited, v1.ConditionTrue, \"TooManyReplicas\", \"the desired replica count was more than the maximum replica count\")\n\t\t\t\tdesiredReplicas = hpa.Spec.MaxReplicas"
  },
  {
    "id" : "3e41161e-5545-4c7c-bf9b-6be678937df1",
    "prId" : 46550,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46550#pullrequestreview-41242795",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7cd4921-0327-45e1-89b0-604726c59aed",
        "parentId" : null,
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "If inputList is a map, it will be simpler and faster.",
        "createdAt" : "2017-05-31T11:37:29Z",
        "updatedAt" : "2017-06-05T15:21:26Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      },
      {
        "id" : "bea29a1e-8778-4e71-9755-d1d2a9ed79b7",
        "parentId" : "f7cd4921-0327-45e1-89b0-604726c59aed",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "I realize that, but then you have to convert from list to map and back to list.  This way, we can operate directly on the API object field.",
        "createdAt" : "2017-05-31T14:23:18Z",
        "updatedAt" : "2017-06-05T15:21:26Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8fdeb022fbd11c3ac5333a772192a0d4f8213bb",
    "line" : 249,
    "diffHunk" : "@@ -1,1 +584,588 @@\tresList := inputList\n\tvar existingCond *autoscalingv2.HorizontalPodAutoscalerCondition\n\tfor i, condition := range resList {\n\t\tif condition.Type == conditionType {\n\t\t\t// can't take a pointer to an iteration variable"
  },
  {
    "id" : "f0708016-0650-4a6c-8111-59fb780c0f54",
    "prId" : 46550,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46550#pullrequestreview-41352782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1959ed3-371f-464e-8887-58fd87f4ea08",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "it would be good if we can split reasons into a set of const so we can quickly see a full candidate list.",
        "createdAt" : "2017-05-31T20:48:17Z",
        "updatedAt" : "2017-06-05T15:21:26Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8fdeb022fbd11c3ac5333a772192a0d4f8213bb",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +221,225 @@\t\t\terrMsg := \"selector is required\"\n\t\t\ta.eventRecorder.Event(hpa, v1.EventTypeWarning, \"SelectorRequired\", errMsg)\n\t\t\tsetCondition(hpa, autoscalingv2.ScalingActive, v1.ConditionFalse, \"InvalidSelector\", \"the HPA target's scale is missing a selector\")\n\t\t\treturn 0, \"\", nil, time.Time{}, fmt.Errorf(errMsg)\n\t\t}"
  },
  {
    "id" : "1fb8df99-b5d7-417c-b12f-db3129e2d085",
    "prId" : 41816,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41816#pullrequestreview-23037017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a8cfac7-0850-4ea6-a074-b8ca50c7043a",
        "parentId" : null,
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "isn't there a spot in the tests that calls this?  I don't remember seeing it deep copy before calling this.  Why doesn't it need to?",
        "createdAt" : "2017-02-21T18:28:34Z",
        "updatedAt" : "2017-02-21T18:28:34Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "cb273ebe-b8cc-41cf-8718-4d56f54060cb",
        "parentId" : "0a8cfac7-0850-4ea6-a074-b8ca50c7043a",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "because it's an object we created 10 lines up that doesn't escape the scope",
        "createdAt" : "2017-02-21T18:43:42Z",
        "updatedAt" : "2017-02-21T18:43:43Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      }
    ],
    "commit" : "592f5c091fc7ec8c1fc489be12df06c10a07e97f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +61,65 @@// can't just use the internal version).  Note that conversion mutates the object, so you need to deepcopy\n// *before* you call this if the input object came out of a shared cache.\nfunc UnsafeConvertToVersionVia(obj runtime.Object, externalVersion schema.GroupVersion) (runtime.Object, error) {\n\tobjInt, err := api.Scheme.UnsafeConvertToVersion(obj, schema.GroupVersion{Group: externalVersion.Group, Version: runtime.APIVersionInternal})\n\tif err != nil {"
  },
  {
    "id" : "cfe9be88-1cf7-4624-88de-d96dbe4c7468",
    "prId" : 34955,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/34955#pullrequestreview-4612793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ac91ee6-a94c-4890-8bff-0669767d8efa",
        "parentId" : null,
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "this could effectively cause a scale down, even if it looks like a scale up (if `desiredReplicas * numRunningPods < (numRunningPods + numPendingPods)`)?  Should we deal with that, or at least log or note it somewhere?\n",
        "createdAt" : "2016-10-17T18:08:07Z",
        "updatedAt" : "2016-10-18T08:20:37Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      },
      {
        "id" : "aaf72cd2-e2d8-493e-8107-258806798249",
        "parentId" : "2ac91ee6-a94c-4890-8bff-0669767d8efa",
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "I think it's fine: not running pods are not consuming CPU. We are logging the number of running pods in `DesiredReplicasComputed` event bellow, so, it is clear what we are doing.\n",
        "createdAt" : "2016-10-18T07:21:06Z",
        "updatedAt" : "2016-10-18T08:20:37Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f495e7322c9ba700835551bf8da6ce609f59017a",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +171,175 @@\t}\n\n\tdesiredReplicas := math.Ceil(usageRatio * float64(numRunningPods))\n\n\ta.eventRecorder.Eventf(hpa, api.EventTypeNormal, \"DesiredReplicasComputed\","
  },
  {
    "id" : "c5c903ed-13aa-43d7-878c-deae1bc61764",
    "prId" : 33593,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33593#pullrequestreview-2713385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "edca668a-082f-4fb3-8cae-472066f54d6f",
        "parentId" : null,
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "So, computeReplicasForCustomMetrics method will not assume 0 from not ready pods during scale-up, but will rather take average from ready pods only? This is a different between scaling base on CPU and on custom metric. I think It would be more correct if we also assume 0 for not ready pods here but I don't have a strong opinion. Anyway, it definitely should be documented.\n",
        "createdAt" : "2016-10-04T09:10:52Z",
        "updatedAt" : "2016-11-08T05:57:50Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      },
      {
        "id" : "9788099f-c67d-4e86-8416-1d69572c3882",
        "parentId" : "edca668a-082f-4fb3-8cae-472066f54d6f",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "whoops, that was not intentional ;-).  I'll fix it so both have the same\n",
        "createdAt" : "2016-10-04T13:40:24Z",
        "updatedAt" : "2016-11-08T05:57:50Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c66d47786465b132f9b5d1d731c4eb9f6c9d686",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +222,226 @@\t\tif err != nil {\n\t\t\terrMsg := fmt.Sprintf(\"couldn't convert selector string to a corresponding selector object: %v\", err)\n\t\t\ta.eventRecorder.Event(hpa, api.EventTypeWarning, \"InvalidSelector\", errMsg)\n\t\t\treturn 0, \"\", \"\", time.Time{}, fmt.Errorf(\"couldn't convert selector string to a corresponding selector object: %v\", err)\n\t\t}"
  },
  {
    "id" : "902f9d3c-253c-40ae-9f74-8c4bbc3236e3",
    "prId" : 33593,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33593#pullrequestreview-5665664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca091aa0-2a4e-416c-821a-79e05ab1c297",
        "parentId" : null,
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "I don't understand this comparison: `replicas` is not initialized here.\n",
        "createdAt" : "2016-10-25T14:17:51Z",
        "updatedAt" : "2016-11-08T05:57:50Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      },
      {
        "id" : "ad7926a9-e079-4f1c-b11f-b1bb3a89b2f2",
        "parentId" : "ca091aa0-2a4e-416c-821a-79e05ab1c297",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "replicas is 0 the first time through the loop, I believe (it's a named parameter, so it initially is the default value).  This code is mostly unchanged from the old code.\n",
        "createdAt" : "2016-10-25T14:42:59Z",
        "updatedAt" : "2016-11-08T05:57:50Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c66d47786465b132f9b5d1d731c4eb9f6c9d686",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +238,242 @@\t\t}\n\n\t\tif replicaCountProposal > replicas {\n\t\t\ttimestamp = timestampProposal\n\t\t\treplicas = replicaCountProposal"
  },
  {
    "id" : "c27946f1-1372-4ca5-944a-95bc5ec1c65d",
    "prId" : 33593,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/33593#pullrequestreview-5665988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4acdbc50-c26e-4604-8e8e-0fc6c5d24f47",
        "parentId" : null,
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "How can we ever scale down if we set `replicas` only if it is smaller than the proposal?\n",
        "createdAt" : "2016-10-25T14:18:45Z",
        "updatedAt" : "2016-11-08T05:57:50Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      },
      {
        "id" : "b91d4324-305c-4508-9860-b7a5a1ce6571",
        "parentId" : "4acdbc50-c26e-4604-8e8e-0fc6c5d24f47",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "replicas starts at zero -- this just chooses the biggest of the replica counts from all the custom metrics.\n",
        "createdAt" : "2016-10-25T14:44:21Z",
        "updatedAt" : "2016-11-08T05:57:50Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c66d47786465b132f9b5d1d731c4eb9f6c9d686",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +240,244 @@\t\tif replicaCountProposal > replicas {\n\t\t\ttimestamp = timestampProposal\n\t\t\treplicas = replicaCountProposal\n\t\t\tmetric = fmt.Sprintf(\"Custom metric %s\", customMetricTarget.Name)\n\t\t}"
  },
  {
    "id" : "15edf903-8902-46a3-aaf4-5ffe95cfa18d",
    "prId" : 28759,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2aa23d8-ad7b-466e-a8df-91ee94e2e383",
        "parentId" : null,
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "Optional: I would remove this if, we have return false at the end.\n",
        "createdAt" : "2016-08-29T12:45:55Z",
        "updatedAt" : "2016-08-29T12:46:06Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      }
    ],
    "commit" : "63f88b4800be555af7f2c80a6f2157cabb345f43",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +351,355 @@\tif desiredReplicas == currentReplicas {\n\t\treturn false\n\t}\n\n\t// Going down only if the usageRatio dropped significantly below the target"
  },
  {
    "id" : "5d06bd84-82ad-42cb-838e-5437dd2132ea",
    "prId" : 22171,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "can this be made a field on the HorizontalController instead, either exported or settable via NewHorizontalController?\n",
        "createdAt" : "2016-03-01T15:30:38Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "8f93d9c5-8ada-4d12-9722-798702cb67da",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Do you have any use case for it?\n",
        "createdAt" : "2016-03-01T15:53:46Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "76310a2b-8df5-4242-83a2-fe6a52a05c0e",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "being able to pick a cluster-wide default via config?\n",
        "createdAt" : "2016-03-01T15:58:43Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "27d61d5f-a3e5-476c-979c-b877cb7ebc25",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Personally I don't like the idea to have it exposed as a configuration option since it's a subject of further changes especially when we will add more options for HPA.\n\n@bgrant0607 @fgrzadkowski @mwielgus any thoughts?\n",
        "createdAt" : "2016-03-01T16:21:03Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "08c49026-e955-4144-9c73-43e8daae498b",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "7be32503-562e-4caa-838d-bba025e626b5",
        "body" : "In general, changing default values is a breaking API change, so it doesn't make much sense to make the default value a knob.\n",
        "createdAt" : "2016-03-01T18:13:01Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "7be32503-562e-4caa-838d-bba025e626b5",
        "tags" : [
        ]
      },
      {
        "id" : "6a889f28-bf23-4b42-9d79-f9760dc64b41",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this PR changes it from 0 to 80, right?\nedit: misread, I think it changes it from an effective 100 to 80\n",
        "createdAt" : "2016-03-01T18:18:59Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "90568a1c-9757-443f-a1ea-db032c012b37",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "Nope. It was set by default to 80 in extensions/v1beta1 in [defaults.go](https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/extensions/v1beta1/defaults.go#L117). In autoscaling/v1 default is applied in controller rather than API level which will allow us to change it in the future without making a breaking change in API.\n\nEDIT: I missed it while moving HPA to autoscaling group so this PR is actually a bug fix.\n",
        "createdAt" : "2016-03-01T18:24:45Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      },
      {
        "id" : "248e95ad-4d0c-455e-81e5-69c44a4ff45c",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "Ah. I guess I still read \"if not specified the default autoscaling policy will be used\" and my first thought is \"where do I set the default autoscaling policy?\"\n",
        "createdAt" : "2016-03-01T18:27:09Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "b161254c-4b40-4188-b5b8-23edea020ffa",
        "parentId" : "0a7f318b-e04a-4c2c-bf5b-79179e6aaa64",
        "authorId" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "body" : "If you have a better comment here I'm happy to change it. What I want to say here is that if a user doesn't specify any target here (as for now cpu, in the future other targets like custom metrics) this will be still a valid HPA object and HPA controller will scale based on some default policy, which may be a subject of changes in the future and is just an implementation detail (of course you can still take a look into the code).\n",
        "createdAt" : "2016-03-02T08:42:33Z",
        "updatedAt" : "2016-03-02T08:47:00Z",
        "lastEditedBy" : "a6ca7669-677e-4e8d-80cf-83cbff3b4216",
        "tags" : [
        ]
      }
    ],
    "commit" : "205e3e607d6128a536056814c2a3c2188abed000",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +40,44 @@\ttolerance = 0.1\n\n\tdefaultTargetCPUUtilizationPercentage = 80\n\n\tHpaCustomMetricsTargetAnnotationName = \"alpha/target.custom-metrics.podautoscaler.kubernetes.io\""
  },
  {
    "id" : "61a05ee2-bfde-4f22-bfe7-76705835ee96",
    "prId" : 21149,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20b1b0cb-248f-43b2-873e-846db5c47e5c",
        "parentId" : null,
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Why is it all within `else`? I think you can compute usage based replicas even in the first pass.\n",
        "createdAt" : "2016-02-17T12:31:15Z",
        "updatedAt" : "2016-02-18T12:45:27Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "06625502-6141-40e3-ad08-dd725e84179a",
        "parentId" : "20b1b0cb-248f-43b2-873e-846db5c47e5c",
        "authorId" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "body" : "Because you can have a failure here. Plus if there is no replicas you will get 0 responses from heapster what means an error.\n",
        "createdAt" : "2016-02-17T15:51:16Z",
        "updatedAt" : "2016-02-18T12:45:27Z",
        "lastEditedBy" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d3891284e0a6b7a23ac4377d8b9b774a43bb2e8",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +191,195 @@\t} else if currentReplicas == 0 {\n\t\tdesiredReplicas = 1\n\t} else {\n\t\t// All basic scenarios covered, the state should be sane, lets use metrics.\n"
  },
  {
    "id" : "0d74d2a9-453b-4283-b95f-f0b525bedd6e",
    "prId" : 21149,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3aa14b5-7f75-420d-a439-e58a06894948",
        "parentId" : null,
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "Shouldn't this be indented by 4 more spaces?\n",
        "createdAt" : "2016-02-17T12:32:18Z",
        "updatedAt" : "2016-02-18T12:45:27Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      },
      {
        "id" : "562332db-84cf-4adf-a3d7-9e7e2112b606",
        "parentId" : "d3aa14b5-7f75-420d-a439-e58a06894948",
        "authorId" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "body" : "No. I have auto indenting turned on. \n",
        "createdAt" : "2016-02-17T15:52:12Z",
        "updatedAt" : "2016-02-18T12:45:27Z",
        "lastEditedBy" : "ab8bc7c5-233d-47c1-b03a-767fb930c021",
        "tags" : [
        ]
      },
      {
        "id" : "49032f11-a057-45e5-9fe1-407dbd67055a",
        "parentId" : "d3aa14b5-7f75-420d-a439-e58a06894948",
        "authorId" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "body" : "This looks like a bug in github. It shows wrong indentation :/ When I click \"View\" with whole files it's working as expected.\n",
        "createdAt" : "2016-02-17T15:55:17Z",
        "updatedAt" : "2016-02-18T12:45:27Z",
        "lastEditedBy" : "df06b0d6-fd6c-44d1-8008-efeaccd16cd5",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d3891284e0a6b7a23ac4377d8b9b774a43bb2e8",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +210,214 @@\t\t}\n\n\t\tif cpuDesiredReplicas > desiredReplicas {\n\t\t\tdesiredReplicas = cpuDesiredReplicas\n\t\t\ttimestamp = cpuTimestamp"
  },
  {
    "id" : "f1b85dc2-3c33-4f80-a8f1-3f18957a7ca5",
    "prId" : 18315,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7efd199-09b7-4c4b-b056-ebda6dbfca24",
        "parentId" : null,
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "I would glog warning here and print old uScale.\n",
        "createdAt" : "2015-12-09T09:50:23Z",
        "updatedAt" : "2015-12-10T15:59:37Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      }
    ],
    "commit" : "5161d169baa5716c39be7fc7c9b070b3ec43933a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +52,56 @@\tif uScale == 0*time.Second {\n\t\tglog.Warningf(\"Invalid upscale value provided, %v using default.\", uScale)\n\t\tuScale = 3 * time.Minute\n\t}\n\tif dScale == 0*time.Second {"
  },
  {
    "id" : "b76a5cd9-ee99-45e6-aaa1-bc64aadf88ca",
    "prId" : 18315,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e99f267-ee5c-40f9-87a4-21d151b84fe7",
        "parentId" : null,
        "authorId" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "body" : "I would glog warning here and print old dScale.\n",
        "createdAt" : "2015-12-09T09:50:30Z",
        "updatedAt" : "2015-12-10T15:59:37Z",
        "lastEditedBy" : "c929c906-4dfb-433b-9bc7-1b4b05c176f8",
        "tags" : [
        ]
      }
    ],
    "commit" : "5161d169baa5716c39be7fc7c9b070b3ec43933a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +56,60 @@\tif dScale == 0*time.Second {\n\t\tglog.Warningf(\"Invalid downscale value provided, %v using default.\", dScale)\n\t\tdScale = 5 * time.Minute\n\t}\n\tglog.V(2).Infof(\"Created Horizontal Controller with downscale %v, upscale %v, and tolerance %v\", tol, uScale, dScale)"
  },
  {
    "id" : "3d2c5749-74f2-41df-ad3a-d66377ffea4b",
    "prId" : 15706,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fa7eeeb-a780-4d5e-b503-025480bb00fd",
        "parentId" : null,
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "Would something like MetricsRetrievalFailure be a better name? @derekwaynecarr \n",
        "createdAt" : "2015-10-15T15:47:27Z",
        "updatedAt" : "2015-10-16T05:16:04Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      }
    ],
    "commit" : "df732f061abeb8aa1b7cfca7806df5180f2531b3",
    "line" : null,
    "diffHunk" : "@@ -1,1 +82,86 @@\t// TODO: what to do on partial errors (like metrics obtained for 75% of pods).\n\tif err != nil {\n\t\ta.eventRecorder.Event(&hpa, \"FailedGetMetrics\", err.Error())\n\t\treturn 0, nil, fmt.Errorf(\"failed to get cpu utilization: %v\", err)\n\t}"
  }
]