[
  {
    "id" : "58a7e1de-9b85-4074-b879-54a58140e331",
    "prId" : 90691,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90691#pullrequestreview-471423183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7da2d0f-c7e7-4cce-8fb3-3a7b41985ba3",
        "parentId" : null,
        "authorId" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "body" : "As mentioned above (https://github.com/kubernetes/kubernetes/pull/90691#discussion_r463440392):\r\n\r\nIf a container is missing entirely, this will return a resource request of zero.  This is not ideal.  If the targeted container is missing, we should treat it as an invalid configuration and return an error.",
        "createdAt" : "2020-07-31T07:25:21Z",
        "updatedAt" : "2020-10-21T19:17:39Z",
        "lastEditedBy" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "tags" : [
        ]
      },
      {
        "id" : "31cb1a62-647b-4d4b-8663-623ba979edcb",
        "parentId" : "f7da2d0f-c7e7-4cce-8fb3-3a7b41985ba3",
        "authorId" : "35b74c0f-1e37-4ee0-81db-f5e9449774bc",
        "body" : "@josephburnett I've updated the logic to return an error in case the pod metrics endpoint does not return metrics about the the container.",
        "createdAt" : "2020-08-20T09:33:42Z",
        "updatedAt" : "2020-10-21T19:17:39Z",
        "lastEditedBy" : "35b74c0f-1e37-4ee0-81db-f5e9449774bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fec7b0f7e2dfd4b0c6c57e086472546f6c69efa",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +427,431 @@\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\trequests[pod.Name] = podSum\n\t}"
  },
  {
    "id" : "454e9e11-a1e4-4851-8fae-3c1dde9e29d3",
    "prId" : 79035,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79035#pullrequestreview-256758776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "95950bf6-f73d-4f05-ae67-8a2f4b0af0ba",
        "parentId" : null,
        "authorId" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "body" : "Why \"statusReplica\". Is increasing number of pods during rolling upgrade increasing external metric? Additional pods  added during upgrade shouldn't serve as if they are healthy then old pods are deleted.",
        "createdAt" : "2019-07-01T07:52:46Z",
        "updatedAt" : "2019-07-02T12:25:04Z",
        "lastEditedBy" : "4c38359a-cb09-4276-8b40-2237ef82c2b9",
        "tags" : [
        ]
      },
      {
        "id" : "9a6321d1-3b04-4e0e-b890-150520e6ab30",
        "parentId" : "95950bf6-f73d-4f05-ae67-8a2f4b0af0ba",
        "authorId" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "body" : "Incidentally, it doesn't make a difference whether we use status or spec replicas here.  It's technically a bug to use status replicas, but I left it as-is in order to fix one bug at a time.  https://github.com/kubernetes/kubernetes/issues/79274 tracks fixing the bug to use spec replicas.",
        "createdAt" : "2019-07-02T08:36:25Z",
        "updatedAt" : "2019-07-02T12:25:04Z",
        "lastEditedBy" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "tags" : [
        ]
      },
      {
        "id" : "0c82aee3-613f-437b-a9e3-a3a90477b57f",
        "parentId" : "95950bf6-f73d-4f05-ae67-8a2f4b0af0ba",
        "authorId" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "body" : "Also, it's super confusing that we pass in spec replicas at all.  We only use that value in order to signal a decision to not scale by returning the current desired replica count.  Better would be to return a boolean.  https://github.com/kubernetes/kubernetes/issues/79275 tracks that improvement.",
        "createdAt" : "2019-07-02T09:05:15Z",
        "updatedAt" : "2019-07-02T12:25:04Z",
        "lastEditedBy" : "7d9aba44-f03a-4059-8095-4da9c4210817",
        "tags" : [
        ]
      }
    ],
    "commit" : "39c4875321991f305d51e30481a66701b6b76f5f",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +335,339 @@// target metric value per pod (as a milli-value) for the external metric in the\n// given namespace, and the current replica count.\nfunc (c *ReplicaCalculator) GetExternalPerPodMetricReplicas(statusReplicas int32, targetUtilizationPerPod int64, metricName, namespace string, metricSelector *metav1.LabelSelector) (replicaCount int32, utilization int64, timestamp time.Time, err error) {\n\tmetricLabelSelector, err := metav1.LabelSelectorAsSelector(metricSelector)\n\tif err != nil {"
  },
  {
    "id" : "96847def-c09e-4aa8-8893-bb957450c92c",
    "prId" : 74526,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74526#pullrequestreview-261557693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c41cbea8-6a17-4867-b9ea-6ba30aa46960",
        "parentId" : null,
        "authorId" : "6319e327-a24c-4adf-80e4-49f5b6cb5b36",
        "body" : "We have a use case in which we need to scale worker deployments from min=0 to min=100 (for example) based on the SQS queue length. The worker deployments works on the SQS queue.\r\n\r\nDo you see this logic working with AWS SQS? SQS does not have a single metric to give the usageRatio because of the long polling of the queue by the consumers, the `ApproximateMessage` may be showing 0 all the time if the consumers are consuming the messages very fast. So in such a case we need to scale down on a different metric - `NumberOfEmptyReceives`",
        "createdAt" : "2019-07-12T10:08:09Z",
        "updatedAt" : "2019-08-04T18:24:03Z",
        "lastEditedBy" : "6319e327-a24c-4adf-80e4-49f5b6cb5b36",
        "tags" : [
        ]
      },
      {
        "id" : "fd43eab9-193f-4a58-add6-a371e6d44ac6",
        "parentId" : "c41cbea8-6a17-4867-b9ea-6ba30aa46960",
        "authorId" : "60ed3a02-b9be-49c9-a8f4-95698756673a",
        "body" : "@alok87 , personally I use this code with RabbitMQ and Prometheus. I have an heuristic of worker utilization which depends on the number of messages in the queue and the number of workers.",
        "createdAt" : "2019-07-12T10:58:16Z",
        "updatedAt" : "2019-07-16T13:47:01Z",
        "lastEditedBy" : "60ed3a02-b9be-49c9-a8f4-95698756673a",
        "tags" : [
        ]
      },
      {
        "id" : "d098a5e2-2e68-4968-a668-53556ac313fc",
        "parentId" : "c41cbea8-6a17-4867-b9ea-6ba30aa46960",
        "authorId" : "a9136405-eb09-4fab-93ae-d2baeb801864",
        "body" : "@DXist, sorry but wasn't clear to me which Prometheus exporter for RabbitMQ you did use on tests, was [prometheus_rabbitmq_exporter](https://github.com/deadtrickster/prometheus_rabbitmq_exporter) or [rabbitmq_exporter](https://github.com/kbudde/rabbitmq_exporter). Moreover, what was the metric used on that? In a rollback scenario which you doesn't have scaling HPA to/from zero feature available, it'll still work? Does it scale to one instead of zero?",
        "createdAt" : "2019-07-13T03:49:26Z",
        "updatedAt" : "2019-07-16T13:47:01Z",
        "lastEditedBy" : "a9136405-eb09-4fab-93ae-d2baeb801864",
        "tags" : [
        ]
      },
      {
        "id" : "615f0ab6-f3de-4f8f-a8a4-dc194c28fa5c",
        "parentId" : "c41cbea8-6a17-4867-b9ea-6ba30aa46960",
        "authorId" : "60ed3a02-b9be-49c9-a8f4-95698756673a",
        "body" : "@fellippe-mendonca, I use the second exporter which is configured in [rabbitmq-ha helm chart](https://github.com/helm/charts/blob/master/stable/rabbitmq-ha/values.yaml#L465)\r\n\r\nI have 2 metrics - worker utilisation and ingress/egress message rate ratio. [Here](http://ix.io/1Okl) is a Helm template of PrometheusRule object I've configured.\r\n\r\nIf you want to rollback the cluster to version without scale-to-zero support you have to update HPA objects explicitly before the rollback. See release notes to this PR.",
        "createdAt" : "2019-07-13T06:25:52Z",
        "updatedAt" : "2019-07-16T13:47:01Z",
        "lastEditedBy" : "60ed3a02-b9be-49c9-a8f4-95698756673a",
        "tags" : [
        ]
      },
      {
        "id" : "a7a64b64-1611-4340-829f-aa9391aeee5f",
        "parentId" : "c41cbea8-6a17-4867-b9ea-6ba30aa46960",
        "authorId" : "a9136405-eb09-4fab-93ae-d2baeb801864",
        "body" : "Thanks for the information @DXist, I'll try this approach with an application I'm running with RabbitMQ that doesn't scale well with CPU. I was wondering to use average response time, but I think that ingress/egress rate ratio might be better! Hope this new functionality come to k8s ASAP and further for GKE. ",
        "createdAt" : "2019-07-13T21:43:17Z",
        "updatedAt" : "2019-07-16T13:47:01Z",
        "lastEditedBy" : "a9136405-eb09-4fab-93ae-d2baeb801864",
        "tags" : [
        ]
      }
    ],
    "commit" : "19d93eefebab5ee479b65890241c8229a7d5852d",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +262,266 @@\t} else {\n\t\t// Scale to zero or n pods depending on usageRatio\n\t\treplicaCount = int32(math.Ceil(usageRatio))\n\t}\n"
  },
  {
    "id" : "489f3c99-d98e-405c-bd49-56ea341553ca",
    "prId" : 67252,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67252#pullrequestreview-148541522",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb1382fc-b506-4cf4-a3e5-301a72150627",
        "parentId" : null,
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "a) you need a comment as to why this is two minutes\r\nb) if this is based on the metrics-server collection window, it shouldn't be constant -- it should be using the reported window size from metrics-server.  If this is \"just a guess\", that's not a good answer.",
        "createdAt" : "2018-08-21T17:38:41Z",
        "updatedAt" : "2018-08-24T11:13:44Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      },
      {
        "id" : "0b506e9f-fa1b-4bff-9273-5df438d2b290",
        "parentId" : "eb1382fc-b506-4cf4-a3e5-301a72150627",
        "authorId" : "c4140906-a9ca-4fa2-9dcc-0ea022464411",
        "body" : "Done (in the next commit of the PR, ff836c9318 at the moment I'm writing this comment)",
        "createdAt" : "2018-08-22T12:19:52Z",
        "updatedAt" : "2018-08-24T11:13:44Z",
        "lastEditedBy" : "c4140906-a9ca-4fa2-9dcc-0ea022464411",
        "tags" : [
        ]
      },
      {
        "id" : "a47d0aad-e34a-4dde-b20d-9a8608b8e62d",
        "parentId" : "eb1382fc-b506-4cf4-a3e5-301a72150627",
        "authorId" : "c4140906-a9ca-4fa2-9dcc-0ea022464411",
        "body" : " thought you left this comment in a different commit and were referring to\r\nvalues HPA uses (which were hard coded in the previous commit).\r\n\r\nThose are just values I use for testing, it doesn't matter much what exactly they are.",
        "createdAt" : "2018-08-22T15:34:02Z",
        "updatedAt" : "2018-08-24T11:13:44Z",
        "lastEditedBy" : "c4140906-a9ca-4fa2-9dcc-0ea022464411",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fd6a1684d1b8db3329345833a909fa9f377d036",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +40,44 @@\t// scale up/scale down.\n\tdefaultTestingTolerance                     = 0.1\n\tdefaultTestingCpuTaintAfterStart            = 2 * time.Minute\n\tdefaultTestingDelayOfInitialReadinessStatus = 10 * time.Second\n)"
  },
  {
    "id" : "db79f6a8-33f5-4f41-9276-30a2a886f78d",
    "prId" : 60886,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60886#pullrequestreview-103412608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e9a6a23-5ede-4f89-9ae1-47341089b15b",
        "parentId" : null,
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "With this implementation you can actually scale-down if your usageRatio is >1 and your readyPodCount < currentReplicas. This feels intuitively wrong - if the metric says we should scale-up we probably shouldn't be scaling down.",
        "createdAt" : "2018-03-07T14:47:04Z",
        "updatedAt" : "2018-03-14T02:27:44Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      },
      {
        "id" : "33333bd6-3039-4b53-9a1e-6ea80fb6349b",
        "parentId" : "6e9a6a23-5ede-4f89-9ae1-47341089b15b",
        "authorId" : "33bb6668-32ce-4909-b707-8e3d01929b5b",
        "body" : "this is interesting edge case (i believe it may appear due metric propagation delay), but still not clear how we can handle this. should we stay with existing replicas and allow only scale up if usageRatio > 1?",
        "createdAt" : "2018-03-07T19:30:49Z",
        "updatedAt" : "2018-03-14T02:27:44Z",
        "lastEditedBy" : "33bb6668-32ce-4909-b707-8e3d01929b5b",
        "tags" : [
        ]
      },
      {
        "id" : "ca7eeea6-1793-413a-af8c-2cbeaec37eff",
        "parentId" : "6e9a6a23-5ede-4f89-9ae1-47341089b15b",
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "Hmm good catch @MaciekPytel - to be honest, I'm not sure what the proper behavior is in this situation. To do nothing?\r\n\r\nTo me, it actually doesn't feel that troublesome that you would \"scale down\" - as I understand it, it would essentially be saying we need less pods that we predicted to handle this load (or the load has changed since then).\r\n\r\nHappy to implement whatever you think is best :)",
        "createdAt" : "2018-03-08T14:08:07Z",
        "updatedAt" : "2018-03-14T02:27:44Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "7dcc2588-00f6-434a-af64-c4d05220ffd1",
        "parentId" : "6e9a6a23-5ede-4f89-9ae1-47341089b15b",
        "authorId" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "body" : "@MaciekPytel @DirectXMan12 happy tuesday :) Just tagging to see what y'all think is the correct behavior in this circumstance?",
        "createdAt" : "2018-03-13T12:35:01Z",
        "updatedAt" : "2018-03-14T02:27:44Z",
        "lastEditedBy" : "d7870cae-47b0-4c8e-8527-be8fc4be86de",
        "tags" : [
        ]
      },
      {
        "id" : "cd540705-16fe-4745-aaed-4ec2606c2095",
        "parentId" : "6e9a6a23-5ede-4f89-9ae1-47341089b15b",
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "I thought about it some more and I didn't come up with any better solution that doesn't complicate things more than it's worth. Between the fact that the pods that haven't started yet shouldn't impact the metric and forbidden window protecting us from flapping number of replicas, I think this is actually good enough.\r\n\r\nSorry for the confusion.",
        "createdAt" : "2018-03-13T12:49:33Z",
        "updatedAt" : "2018-03-14T02:27:44Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      }
    ],
    "commit" : "d33494d459020e0089ff3ae5b9d142d8b6cede9e",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +295,299 @@\t}\n\n\treplicaCount = int32(math.Ceil(usageRatio * float64(readyPodCount)))\n\n\treturn replicaCount, utilization, timestamp, nil"
  },
  {
    "id" : "e3bf2ae9-bb19-4ed0-8108-f70f79a65d1e",
    "prId" : 60243,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/60243#pullrequestreview-99364917",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fdee8dc-fe01-461f-bd5b-104531b37373",
        "parentId" : null,
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "This needs to follow the same logic as `pods`, so it should factor in unready pods.",
        "createdAt" : "2018-02-22T19:38:38Z",
        "updatedAt" : "2018-02-27T13:12:02Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      },
      {
        "id" : "d14bd1f2-24a8-43ba-b61d-fbd871e6eb56",
        "parentId" : "9fdee8dc-fe01-461f-bd5b-104531b37373",
        "authorId" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "body" : "The logic for handling unready pods in `pods` is about how to account for such pods when calculating total value of metric. In this case we get a single metric value and just calculate the desired number of pods. I don't see how logic in `pods` could apply here?",
        "createdAt" : "2018-02-23T16:17:32Z",
        "updatedAt" : "2018-02-27T13:12:02Z",
        "lastEditedBy" : "3f00d8a9-68e2-438c-85da-b03590361276",
        "tags" : [
        ]
      },
      {
        "id" : "0b8ec21d-6708-4273-bf40-3a6edfe89dc3",
        "parentId" : "9fdee8dc-fe01-461f-bd5b-104531b37373",
        "authorId" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "body" : "ack",
        "createdAt" : "2018-02-26T16:21:47Z",
        "updatedAt" : "2018-02-27T13:12:02Z",
        "lastEditedBy" : "3a4b4830-dc71-4d7e-a7db-de2453284945",
        "tags" : [
        ]
      }
    ],
    "commit" : "e58411c6000c0138cab57c7ffa9c5e9a27ae3d12",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +338,342 @@\t\treplicaCount = int32(math.Ceil(float64(utilization) / float64(targetUtilizationPerPod)))\n\t}\n\tutilization = int64(math.Ceil(float64(utilization) / float64(currentReplicas)))\n\treturn replicaCount, utilization, timestamp, nil\n}"
  }
]