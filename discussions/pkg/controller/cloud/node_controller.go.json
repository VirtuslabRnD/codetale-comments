[
  {
    "id" : "53835ebc-5d6d-4380-88ba-273c92004131",
    "prId" : 90057,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90057#pullrequestreview-393852115",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8b678b6-17f2-4a6a-90a4-83b136c398c8",
        "parentId" : null,
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "I don't believe its guaranteed that the providerID field has been set. In that case we are going to have a problem with this.",
        "createdAt" : "2020-04-13T19:50:21Z",
        "updatedAt" : "2020-04-14T18:01:50Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "3db6264f-8b36-4eff-9e5f-2a43934e5619",
        "parentId" : "f8b678b6-17f2-4a6a-90a4-83b136c398c8",
        "authorId" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "body" : "@cheftako , can you elaborate details? What would be a problem?",
        "createdAt" : "2020-04-13T19:58:29Z",
        "updatedAt" : "2020-04-14T18:01:50Z",
        "lastEditedBy" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "tags" : [
        ]
      },
      {
        "id" : "dc311b29-3479-4c15-a43d-a57c332e1d32",
        "parentId" : "f8b678b6-17f2-4a6a-90a4-83b136c398c8",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "providerID is only set if node.Spec.ProviderID is not set. So as example if we go through this method twice for the same node, the second time the method is called, node.Spec.ProviderID will be set. As a result providerID on the second pass will *not* have been set here. Now we will fail to look up the node addresses because we aren't using the correct providerID...",
        "createdAt" : "2020-04-14T16:58:32Z",
        "updatedAt" : "2020-04-14T18:01:50Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "834b796d-f094-40b4-bb3a-10b9c1553bee",
        "parentId" : "f8b678b6-17f2-4a6a-90a4-83b136c398c8",
        "authorId" : "29b82984-f003-46e0-b583-e6a4392093af",
        "body" : "Yeah, going through this multiple times should not happen after https://github.com/kubernetes/kubernetes/pull/85735 but its not impossible if the updating fails on all attempts",
        "createdAt" : "2020-04-14T17:11:22Z",
        "updatedAt" : "2020-04-14T18:01:50Z",
        "lastEditedBy" : "29b82984-f003-46e0-b583-e6a4392093af",
        "tags" : [
        ]
      },
      {
        "id" : "728198ba-a6cd-45ab-9b90-3bb028eaeae2",
        "parentId" : "f8b678b6-17f2-4a6a-90a4-83b136c398c8",
        "authorId" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "body" : "@cheftako , with the latest force-push the `providerID` should be set to `node.Spec.ProviderID` when `node.Spec.ProviderID != nil`",
        "createdAt" : "2020-04-14T17:31:29Z",
        "updatedAt" : "2020-04-14T18:01:50Z",
        "lastEditedBy" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "tags" : [
        ]
      },
      {
        "id" : "95810b04-a6a8-4e6b-aea3-c9c2c009b670",
        "parentId" : "f8b678b6-17f2-4a6a-90a4-83b136c398c8",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "Looks good",
        "createdAt" : "2020-04-15T14:51:13Z",
        "updatedAt" : "2020-04-15T14:51:14Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c276a451d64d161b21d70042d93f4c4b8e3201d",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +437,441 @@\t}\n\n\tnodeAddresses, err := getNodeAddressesByProviderIDOrName(ctx, instances, providerID, node.Name)\n\tif err != nil {\n\t\treturn nil, err"
  },
  {
    "id" : "8caa79df-7b63-4c35-ac6c-a2f61012b97e",
    "prId" : 90057,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/90057#pullrequestreview-394158050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "parentId" : null,
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "This isn't going to work for GCP using the existing GCE cloud provider implementation. (https://github.com/kubernetes/legacy-cloud-providers/blob/master/gce/gce_instances.go#L86) The gce implementation has the  issue detailed in the todo of the interface (https://github.com/kubernetes/cloud-provider/blob/master/cloud.go#L160). It was written back when this was only called from the Kubelet and is written such that it will only do the right thing, when called from the Kubelet.",
        "createdAt" : "2020-04-15T15:24:53Z",
        "updatedAt" : "2020-04-15T15:24:54Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "ee2f78aa-d11d-49b7-ac8d-8261b26df6e0",
        "parentId" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Agreed and I think this is the proper fix. `node.Spec.ProviderID` being empty is acceptable which is why we have checks for getting by name. ",
        "createdAt" : "2020-04-15T15:29:32Z",
        "updatedAt" : "2020-04-15T15:29:33Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "81cbeeaf-aad5-40d8-8ac2-02ed2fe8206e",
        "parentId" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "authorId" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "body" : "Yes, but this is not something related to this PR. That's why I propose this PR to fix the `providerID` passing to `getNodeAddressesByProviderIDOrName` and in this way prevent the fallback call to `NodeAddresses` which fails for gce when the cloud-controller-manager is not running on the host.",
        "createdAt" : "2020-04-15T15:36:25Z",
        "updatedAt" : "2020-04-15T15:36:25Z",
        "lastEditedBy" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "tags" : [
        ]
      },
      {
        "id" : "9c846c67-13fd-4aa0-94f0-c942835dc2e5",
        "parentId" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "Agreed, which is why I lgtm'd it. I just wanted to make sure you were aware of the issue. \r\n\r\nThe Kubelet usage of the cloud provider implementation should be going away soon. We should fix the GCE implementation to do the right thing in non Kubelet use cases.",
        "createdAt" : "2020-04-15T15:42:41Z",
        "updatedAt" : "2020-04-15T15:42:41Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "7185da01-ca0e-46e3-8900-f33a98f70591",
        "parentId" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "> Yes, but this is not something related to this PR. \r\n\r\nThis is the part I'm a bit confused about. I think it is related because implementing the GCP fallback makes this a non-issue. We should treat `node.Spec.ProviderID` as the source of truth because we don't know yet if the provider ID from `GetInstanceProviderID` is valid. There are also some cases where `node.Spec.ProviderID` can be set from another source and `GetInstanceProviderID` returns `cloudprovider.NotImplemented`. ",
        "createdAt" : "2020-04-15T16:06:03Z",
        "updatedAt" : "2020-04-15T16:06:03Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "488b9768-058c-41f5-a5d9-eb14a57be4c5",
        "parentId" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "authorId" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "body" : "> This is the part I'm a bit confused about. I think it is related because implementing the GCP fallback makes this a non-issue. \r\n\r\nI don't agree here. Currently all `getNodeAddressesByProviderIDOrName`, `getInstanceTypeByProviderIDOrName`, `getZoneByProviderIDOrName` are called with `node.Spec.ProviderID=\"\"` which makes pointless all of the provideID usage in these funcs such as `instances.NodeAddressesByProviderID(ctx, node.Spec.ProviderID)`, `instances.InstanceTypeByProviderID(ctx, node.Spec.ProviderID)`, `zones.GetZoneByProviderID(ctx, node.Spec.ProviderID)`.\r\n\r\nGenerally I agree that the GCE provider handling for `NodeAddresses` func can be improved to do not rely that the cloud-controller-manager is running on the Node. But this is something different.",
        "createdAt" : "2020-04-15T16:16:42Z",
        "updatedAt" : "2020-04-15T16:19:41Z",
        "lastEditedBy" : "018d6ca2-feee-4457-ae9c-f599edb91f62",
        "tags" : [
        ]
      },
      {
        "id" : "b7931e7b-7d2f-4d61-83b7-c4962fb0758b",
        "parentId" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "> I don't agree here. Currently all getNodeAddressesByProviderIDOrName, getInstanceTypeByProviderIDOrName, getZoneByProviderIDOrName are called with node.Spec.ProviderID=\"\" which makes pointless all of the provideID usage in these funcs such as instances.NodeAddressesByProviderID(ctx, node.Spec.ProviderID), instances.InstanceTypeByProviderID(ctx, node.Spec.ProviderID), zones.GetZoneByProviderID(ctx, node.Spec.ProviderID).\r\n\r\nThe ProviderID can be preset if kubelet has `--provider-id` set, but I see this doesn't break that since `node.Spec.ProviderID` takes precedent.  ",
        "createdAt" : "2020-04-15T16:31:53Z",
        "updatedAt" : "2020-04-15T16:31:54Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "fdf145af-0a79-48cb-842a-3b7448fc1172",
        "parentId" : "9e45ce67-e0f3-4b02-85b2-a4ee988db3e0",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "https://github.com/kubernetes/kubernetes/issues/90185",
        "createdAt" : "2020-04-15T21:46:13Z",
        "updatedAt" : "2020-04-15T21:46:13Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c276a451d64d161b21d70042d93f4c4b8e3201d",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +542,546 @@\tif err != nil {\n\t\tproviderIDErr := err\n\t\tnodeAddresses, err = instances.NodeAddresses(ctx, types.NodeName(nodeName))\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error fetching node by provider ID: %v, and error by node name: %v\", providerIDErr, err)"
  },
  {
    "id" : "777f813b-7540-44e5-9a59-2eae2958d5bc",
    "prId" : 83872,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/83872#pullrequestreview-304754051",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b9b61e7-cbf4-4b92-ad28-0306b4f2d808",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Is there value in passing context into the Add/Update event handlers if client-go informers don't pass in a context anyways? Wondering if the context should start inside AddCloudNode and UpdateCloudNode instead.",
        "createdAt" : "2019-10-21T15:22:46Z",
        "updatedAt" : "2019-10-21T15:22:46Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "865b9f10-cdc8-42ab-96b1-89953e73e5a8",
        "parentId" : "0b9b61e7-cbf4-4b92-ad28-0306b4f2d808",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "Great question. \r\n\r\nThere has been discussion in SIG api machinery about adding context to client-go. So eventually we should have a context there. I think we have a lot of places this needs to be fixed and I would like to do it in small steady chunks. This will also help from the other direction. There has been talk about trying to get controller managers able to shut down gracefully. Part of that is the ability to pass through a cancel/done message. Once this has been fully piped through it will help with that as well. I agree there is little immediate value to this change. That is because the change has to be made in a lot of places to be helpful. However to get that to happen we need to make a lot of these small changes.",
        "createdAt" : "2019-10-21T17:50:37Z",
        "updatedAt" : "2019-10-21T17:50:38Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      },
      {
        "id" : "3dc8c000-1477-4427-ae6c-f3ff5ba57647",
        "parentId" : "0b9b61e7-cbf4-4b92-ad28-0306b4f2d808",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "gotcha, thanks for the context (pun intended) ",
        "createdAt" : "2019-10-21T18:15:15Z",
        "updatedAt" : "2019-10-21T18:15:16Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      }
    ],
    "commit" : "6991069e316aeba05e52ee0d9358db67a3f2d15b",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +193,197 @@}\n\nfunc (cnc *CloudNodeController) UpdateCloudNode(ctx context.Context, _, newObj interface{}) {\n\tnode, ok := newObj.(*v1.Node)\n\tif !ok {"
  },
  {
    "id" : "3a73a3e5-a98c-44f6-805b-f306ed63296b",
    "prId" : 81431,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81431#pullrequestreview-312128940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "feb3e1f5-484d-4be4-aec1-bc4f19541c1c",
        "parentId" : null,
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "should we log a warning or throw an error here if \"this should not happen\"",
        "createdAt" : "2019-10-30T21:49:30Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "bce934a7-86f8-4324-b4b7-6667bb2a2cbf",
        "parentId" : "feb3e1f5-484d-4be4-aec1-bc4f19541c1c",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "This was just duplicated from kubelet",
        "createdAt" : "2019-10-31T23:29:26Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "5baee7c6-2927-4bd4-a56a-e74e48907c50",
        "parentId" : "feb3e1f5-484d-4be4-aec1-bc4f19541c1c",
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "still think we should log a warning if the `should not happen` case happens",
        "createdAt" : "2019-11-06T00:20:45Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      }
    ],
    "commit" : "349749644fb40f737c44e27b5b749bd18a3efaae",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +191,195 @@\t\t\t// within our supported version skew range, when no external\n\t\t\t// components/factors modifying the node object. Ignore this case.\n\t\t\tcontinue\n\t\t}\n\t\tif secondaryExists && primaryValue != secondaryValue {"
  },
  {
    "id" : "a525b40a-a77d-4de4-b0f7-7448edb0f8f9",
    "prId" : 81431,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81431#pullrequestreview-309624649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c849e605-2cd8-457e-b57c-5f533e5c769e",
        "parentId" : null,
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "struct should be typed. Could `labelReconcileInfo` be a const?",
        "createdAt" : "2019-10-30T21:53:32Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "b88d8589-5d0f-4b9e-9cfc-8c29a27244bd",
        "parentId" : "c849e605-2cd8-457e-b57c-5f533e5c769e",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "this mimics the os/arch label controller code in https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L136-L165\r\n\r\nwould be nice if it could be deduplicated, but I won't block on that",
        "createdAt" : "2019-10-31T02:53:03Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "349749644fb40f737c44e27b5b749bd18a3efaae",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +50,54 @@//   - If ensureSecondaryExists is true, and the secondaryKey does not\n//   exist, secondaryKey will be added with the value of the primaryKey.\nvar labelReconcileInfo = []struct {\n\tprimaryKey            string\n\tsecondaryKey          string"
  },
  {
    "id" : "5df43b19-61bb-4749-8964-8497560e7d40",
    "prId" : 81431,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81431#pullrequestreview-309622754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb181196-c673-44d6-baa1-6c682538dee7",
        "parentId" : null,
        "authorId" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "body" : "why do we need `ensureSecondaryExists`? Is there a known case where we want to reconcile if the secondary exists and it is different, but not if it doesn't exist at all?",
        "createdAt" : "2019-10-30T21:54:12Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "542e5d2f-2ff9-4674-ab44-78f31768e7a1",
        "tags" : [
        ]
      },
      {
        "id" : "ab151030-8594-4ab8-89cd-85d4bc2f4f5a",
        "parentId" : "eb181196-c673-44d6-baa1-6c682538dee7",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "once we flip the source of truth to the v1 key, and deprecate and eventually stop setting the beta key, yes",
        "createdAt" : "2019-10-31T02:44:39Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "349749644fb40f737c44e27b5b749bd18a3efaae",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +53,57 @@\tprimaryKey            string\n\tsecondaryKey          string\n\tensureSecondaryExists bool\n}{\n\t{"
  },
  {
    "id" : "f35b69e0-e1f0-41b7-9ff9-855fbacfbe50",
    "prId" : 81431,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81431#pullrequestreview-311834867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c59103cb-ad53-4300-9da9-d19ececbf0b2",
        "parentId" : null,
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Would it make sense to re-use the loop from line 155 and avoid \"getting\" the node object again in line 169?",
        "createdAt" : "2019-11-05T12:35:22Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "87dea32e-a9b4-4379-bdbe-8396d8e83c54",
        "parentId" : "c59103cb-ad53-4300-9da9-d19ececbf0b2",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "I initially didn't because we would be working against a stale node object at that point from updateNodeAddress and \"getting\" a node from informer is pretty cheap. ",
        "createdAt" : "2019-11-05T15:52:42Z",
        "updatedAt" : "2019-11-08T02:22:16Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      }
    ],
    "commit" : "349749644fb40f737c44e27b5b749bd18a3efaae",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +158,162 @@\n\tfor _, node := range nodes.Items {\n\t\terr = cnc.reconcileNodeLabels(node.Name)\n\t\tif err != nil {\n\t\t\tklog.Errorf(\"Error reconciling node labels for node %q, err: %v\", node.Name, err)"
  },
  {
    "id" : "754d5d1a-2eee-42d8-a1df-e2c7900995dc",
    "prId" : 70344,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70344#pullrequestreview-170481790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52a597fa-479e-4aea-9626-b0fd2c7b1b0b",
        "parentId" : null,
        "authorId" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "body" : "For line 114: Some comments may need to be updated in this file, since now this controller doesn't delete nodes anymore.",
        "createdAt" : "2018-10-31T21:32:22Z",
        "updatedAt" : "2018-12-03T18:34:06Z",
        "lastEditedBy" : "881df817-68e6-43dd-b4ea-f0b973f7dc41",
        "tags" : [
        ]
      }
    ],
    "commit" : "5329f0966357ae4ca898a31192d657ef8214f7e5",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +111,115 @@\t// of O(num_nodes) per cycle. These functions are justified here because these events fire\n\t// very infrequently. DO NOT MODIFY this to perform frequent operations.\n\n\t// Start a loop to periodically update the node addresses obtained from the cloud\n\twait.Until(cnc.UpdateNodeStatus, cnc.nodeStatusUpdateFrequency, stopCh)"
  },
  {
    "id" : "78eda9ed-c6ae-40f3-968f-c870c3c65ff8",
    "prId" : 65981,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65981#pullrequestreview-135664065",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff86cf1d-81e5-4345-bc9e-b3562277e582",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "FYI there's a similar check here https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/cloud/node_controller.go#L327-L331. ",
        "createdAt" : "2018-07-09T15:34:00Z",
        "updatedAt" : "2019-01-25T19:50:18Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "f2fda273-345a-4756-b5df-9af668d14b21",
        "parentId" : "ff86cf1d-81e5-4345-bc9e-b3562277e582",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "oh I see now, should read the PR description before commenting ;) ",
        "createdAt" : "2018-07-09T15:34:38Z",
        "updatedAt" : "2019-01-25T19:50:18Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "52091017-ac2c-42d3-a201-8304fc6a45cc",
        "parentId" : "ff86cf1d-81e5-4345-bc9e-b3562277e582",
        "authorId" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "body" : ":)",
        "createdAt" : "2018-07-09T15:36:40Z",
        "updatedAt" : "2019-01-25T19:50:18Z",
        "lastEditedBy" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "tags" : [
        ]
      },
      {
        "id" : "87524426-987a-4c31-b2d3-ee5a37687655",
        "parentId" : "ff86cf1d-81e5-4345-bc9e-b3562277e582",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "I don't love that we're checking taints twice here, though it is unlikely to happen since most nodes should register from the OnAdd event. I would suggest just increasing the log level [here](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/cloud/node_controller.go#L329) but it seems like that log message could be useful when OnAdd events actually fail ðŸ¤” ",
        "createdAt" : "2018-07-10T03:31:05Z",
        "updatedAt" : "2019-01-25T19:50:18Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b3ab29cc91dd64aecaa78711afd70c91fc1944c",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +205,209 @@\t}\n\n\tcloudTaint := getCloudTaint(node.Spec.Taints)\n\tif cloudTaint == nil {\n\t\t// The node has already been initialized so nothing to do."
  },
  {
    "id" : "a5ec50e1-a99c-4f2a-86b7-2864ef8be5bd",
    "prId" : 65981,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65981#pullrequestreview-136028090",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f86951a-4b84-4d61-bd2c-9453a99bbdde",
        "parentId" : null,
        "authorId" : "e2ca6907-6765-444e-8bf6-1452233150d6",
        "body" : "@jhorwit2 - this check is done on `addCloudNode`, and we log there if cloud taint isnâ€™t present.",
        "createdAt" : "2018-07-10T21:46:26Z",
        "updatedAt" : "2019-01-25T19:50:18Z",
        "lastEditedBy" : "e2ca6907-6765-444e-8bf6-1452233150d6",
        "tags" : [
        ]
      },
      {
        "id" : "ee0242d3-c408-438b-83ca-0e256dce4bea",
        "parentId" : "6f86951a-4b84-4d61-bd2c-9453a99bbdde",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Yeah, the problem here is that we excessively log it on every node update event, and node update events happen a lot since every heartbeat counts as an update",
        "createdAt" : "2018-07-10T22:05:13Z",
        "updatedAt" : "2019-01-25T19:50:18Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b3ab29cc91dd64aecaa78711afd70c91fc1944c",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +205,209 @@\t}\n\n\tcloudTaint := getCloudTaint(node.Spec.Taints)\n\tif cloudTaint == nil {\n\t\t// The node has already been initialized so nothing to do."
  },
  {
    "id" : "7304f9b0-1c0c-42df-82f5-9172d0da5408",
    "prId" : 65981,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65981#pullrequestreview-196807650",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59bd683a-be1e-4c3b-b37c-f40799e40ccc",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "It seems if no taint matches schedulerapi.TaintExternalCloudProvider , there is no need to allocate newTaints .",
        "createdAt" : "2019-01-27T00:31:51Z",
        "updatedAt" : "2019-01-27T00:31:51Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b3ab29cc91dd64aecaa78711afd70c91fc1944c",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +338,342 @@\t\t\tcontinue\n\t\t}\n\t\tnewTaints = append(newTaints, taint)\n\t}\n\treturn newTaints"
  },
  {
    "id" : "774ca821-dc2d-4b6d-877a-60c4a1f4f7eb",
    "prId" : 65052,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65052#pullrequestreview-128958641",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e667da7-e755-4e0c-85fb-adce6bd23f9a",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "What's the justification of moving this here earlier? Is it to set the node condition as early as possible?\r\n\r\ncc @cheftako ",
        "createdAt" : "2018-06-13T15:35:02Z",
        "updatedAt" : "2018-06-13T15:35:03Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "5db64686-6cfd-420d-8ae1-88b11b5b566c",
        "parentId" : "2e667da7-e755-4e0c-85fb-adce6bd23f9a",
        "authorId" : "7fa974c8-e108-417a-9c1d-bb4ebb549ef9",
        "body" : "@andrewsykim \r\n\r\nYes, we must ensure that `NodeNetworkUnavailable` condition is added before we remove the cloud taint.\r\n\r\nBtw, by moving these code ahead of the `curNode, err := cnc.kubeClient.CoreV1().Nodes().Get(node.Name, metav1.GetOptions{})`, the curNode's would have `NodeNetworkUnavailable`  condition after we get it from apiserver by `cnc.kubeClient.CoreV1().Nodes().Get(node.Name, metav1.GetOptions{})`.\r\nIf we don't move these code, when we update the node condition with `nodeutil.SetNodeCondition` methods, the curNode's status still doesn't have `NodeNetworkUnavailable`  condition, which is a little inconsistent, and we need to `Get` the node from apiserver again.",
        "createdAt" : "2018-06-13T16:16:43Z",
        "updatedAt" : "2018-06-13T16:17:07Z",
        "lastEditedBy" : "7fa974c8-e108-417a-9c1d-bb4ebb549ef9",
        "tags" : [
        ]
      },
      {
        "id" : "e820f841-d2ce-405a-8a5c-5704abc9ee17",
        "parentId" : "2e667da7-e755-4e0c-85fb-adce6bd23f9a",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "Not sure I follow, the current implementation is \r\n```\r\ncurNode.Status.Conditions = append(node.Status.Conditions, v1.NodeCondition{\r\n\t\t\t\tType:               v1.NodeNetworkUnavailable,\r\n\t\t\t\tStatus:             v1.ConditionTrue,\r\n\t\t\t\tReason:             \"NoRouteCreated\",\r\n\t\t\t\tMessage:            \"Node created without a route\",\r\n\t\t\t\tLastTransitionTime: metav1.Now(),\r\n\t\t\t})\r\n```\r\nWhich updates the condition on the node object in memory but does not apply any changes to the apiserver yet. The current implementation bundles all the necessary updates (removing taints, conditions, zones, instance types, etc) into 1 update call [here](https://github.com/kubernetes/kubernetes/pull/65052/files#diff-a1ff61328540d76c791d919f71da4ad5R409) so there is no need to call `nodeutil.SetNodeCondition` as far as I know. ",
        "createdAt" : "2018-06-13T19:56:40Z",
        "updatedAt" : "2018-06-13T19:56:40Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "eba83f8f-fae5-4995-a947-1abee6df7452",
        "parentId" : "2e667da7-e755-4e0c-85fb-adce6bd23f9a",
        "authorId" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "body" : "Is it OK to set this condition before we know the network is actually ok? I think we need someone from GCP to review this as well.",
        "createdAt" : "2018-06-13T22:36:16Z",
        "updatedAt" : "2018-06-13T22:37:12Z",
        "lastEditedBy" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "tags" : [
        ]
      },
      {
        "id" : "ed6ad010-e3ab-4d87-98c4-3be554e8aca5",
        "parentId" : "2e667da7-e755-4e0c-85fb-adce6bd23f9a",
        "authorId" : "7fa974c8-e108-417a-9c1d-bb4ebb549ef9",
        "body" : "@andrewsykim \r\n\r\nThe one update call doesn't update the node condition actually, the reason was describe here. #65051 ",
        "createdAt" : "2018-06-14T01:47:18Z",
        "updatedAt" : "2018-06-14T01:47:18Z",
        "lastEditedBy" : "7fa974c8-e108-417a-9c1d-bb4ebb549ef9",
        "tags" : [
        ]
      },
      {
        "id" : "2314023b-3927-40c1-9232-ac73b7492ef7",
        "parentId" : "2e667da7-e755-4e0c-85fb-adce6bd23f9a",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "This is good to know, thanks :) ",
        "createdAt" : "2018-06-14T13:48:17Z",
        "updatedAt" : "2018-06-14T13:48:17Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "a40ab4c9-a76c-4345-9262-fddb8cb512a7",
        "parentId" : "2e667da7-e755-4e0c-85fb-adce6bd23f9a",
        "authorId" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "body" : "@dbdd4us, @andrewsykim \r\nI think it is correct to move the no route condition earlier. I'm slightly worried about the earlier error exit we are adding. We seem to have several such early error exits, so nothing in this block are guaranteed to happen. I don't think this is particular branch is actually exercised today. I should be testing this well before it is enabled on the main line. Given that I am ok with the change.",
        "createdAt" : "2018-06-14T20:35:33Z",
        "updatedAt" : "2018-06-14T20:35:33Z",
        "lastEditedBy" : "7aca96c2-45d7-4567-99be-0323d7556c55",
        "tags" : [
        ]
      }
    ],
    "commit" : "5835d9bde4280378c8780767e72159c2dd2c2321",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +338,342 @@\n\terr := clientretry.RetryOnConflict(UpdateNodeSpecBackoff, func() error {\n\t\t// TODO(wlan0): Move this logic to the route controller using the node taint instead of condition\n\t\t// Since there are node taints, do we still need this?\n\t\t// This condition marks the node as unusable until routes are initialized in the cloud provider"
  },
  {
    "id" : "7f116f74-a88f-4899-8f91-49c5d02dba51",
    "prId" : 64562,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64562#pullrequestreview-124960345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "220239ec-58cf-4d24-b911-adea3921ac25",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "This allows for providers to skip updates to node addresses if they want. ",
        "createdAt" : "2018-05-31T18:05:33Z",
        "updatedAt" : "2018-05-31T20:23:51Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "867326be-0fa2-4cb3-80c0-dd9bee923464",
        "parentId" : "220239ec-58cf-4d24-b911-adea3921ac25",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "can we please log that we are skipping updates using `glog.V(5).Infof`?",
        "createdAt" : "2018-05-31T20:19:28Z",
        "updatedAt" : "2018-05-31T20:23:51Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "4510fe48c32af1774812278ba8f297c80fcd887b",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +171,175 @@\t}\n\n\tif len(nodeAddresses) == 0 {\n\t\tglog.V(5).Infof(\"Skipping node address update for node %q since cloud provider did not return any\", node.Name)\n\t\treturn"
  },
  {
    "id" : "6fa51fbc-ecdd-4eef-9cb8-dbe491416a67",
    "prId" : 64562,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64562#pullrequestreview-124960435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14bcd5d7-913d-43db-bbca-bba2cc0b4b80",
        "parentId" : null,
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "If getting node addresses fails we should return an error instead of just returning nil here. If a provider wants to skip node addresses they can return `[]v1.NodeAddresses{}, nil`. ",
        "createdAt" : "2018-05-31T18:10:27Z",
        "updatedAt" : "2018-05-31T20:23:51Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "f3078e7a-df2a-4202-ab27-104b1579d251",
        "parentId" : "14bcd5d7-913d-43db-bbca-bba2cc0b4b80",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "+1\r\n",
        "createdAt" : "2018-05-31T20:19:45Z",
        "updatedAt" : "2018-05-31T20:23:51Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "4510fe48c32af1774812278ba8f297c80fcd887b",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +357,361 @@\t\tnodeAddresses, err := getNodeAddressesByProviderIDOrName(instances, curNode)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n"
  },
  {
    "id" : "ac989673-8c0b-4d92-ba4d-6befdb815358",
    "prId" : 59939,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59939#pullrequestreview-96987795",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18346eea-947d-4024-ac9a-a9493268988d",
        "parentId" : null,
        "authorId" : "ae819fef-700f-45ca-aa42-0afbedcadc19",
        "body" : ":+1: ",
        "createdAt" : "2018-02-15T20:05:34Z",
        "updatedAt" : "2018-02-15T20:05:34Z",
        "lastEditedBy" : "ae819fef-700f-45ca-aa42-0afbedcadc19",
        "tags" : [
        ]
      }
    ],
    "commit" : "84d171fe8654d0ecd5d11bcd30c91dc8f6c5ad8a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +320,324 @@\t}\n\n\tinstances, ok := cnc.cloud.Instances()\n\tif !ok {\n\t\tutilruntime.HandleError(fmt.Errorf(\"failed to get instances from cloud provider\"))"
  },
  {
    "id" : "b7e35ad4-dfd6-4545-af05-3d5e8ef200de",
    "prId" : 59887,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59887#pullrequestreview-96984198",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5e34dd9-c84e-42ef-b22f-05c53a3e309d",
        "parentId" : null,
        "authorId" : "ae819fef-700f-45ca-aa42-0afbedcadc19",
        "body" : "Wouldn't it be more efficient to check for CloudTaint before calling AddCloudNode? \r\n\r\nWe'll avoid the cloud lookup every SYNC seconds.\r\n\r\nThat was another reason we only did AddCloudNode.",
        "createdAt" : "2018-02-14T22:08:20Z",
        "updatedAt" : "2018-02-15T18:10:55Z",
        "lastEditedBy" : "ae819fef-700f-45ca-aa42-0afbedcadc19",
        "tags" : [
        ]
      },
      {
        "id" : "818ed27d-9f32-4c0b-9429-82f780119669",
        "parentId" : "a5e34dd9-c84e-42ef-b22f-05c53a3e309d",
        "authorId" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "body" : "@wlan0 logged another PR for checking cloud taint first in AddCloudNode. Will cut down on calls to cloud provider if we don't need to",
        "createdAt" : "2018-02-15T19:54:06Z",
        "updatedAt" : "2018-02-15T19:54:06Z",
        "lastEditedBy" : "bd04f755-e62f-45fb-8771-4cc2b5db49d4",
        "tags" : [
        ]
      }
    ],
    "commit" : "c423be11d5e4cf967f79988fbc2245f29e758dd8",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +286,290 @@\t\treturn\n\t}\n\tcnc.AddCloudNode(newObj)\n}\n"
  },
  {
    "id" : "ba0389e1-af56-4487-a4d1-670705946fdd",
    "prId" : 59323,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59323#pullrequestreview-94383337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8c4db4c-4c85-4fca-b50f-f7679978032c",
        "parentId" : null,
        "authorId" : "e2ca6907-6765-444e-8bf6-1452233150d6",
        "body" : "can we have some tests for this ?",
        "createdAt" : "2018-02-05T21:10:44Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "e2ca6907-6765-444e-8bf6-1452233150d6",
        "tags" : [
        ]
      },
      {
        "id" : "bf283ff1-137c-4091-be17-6fa9002cfb84",
        "parentId" : "b8c4db4c-4c85-4fca-b50f-f7679978032c",
        "authorId" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "body" : "I can add those when i have time. Hopefully today/tomorrow",
        "createdAt" : "2018-02-06T10:39:57Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "tags" : [
        ]
      },
      {
        "id" : "0af64d9b-0119-4034-a2f0-bdc899aa647d",
        "parentId" : "b8c4db4c-4c85-4fca-b50f-f7679978032c",
        "authorId" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "body" : "@yastij tests added now",
        "createdAt" : "2018-02-06T15:24:24Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "tags" : [
        ]
      }
    ],
    "commit" : "6665fa71441fb136303aa5adcb4b2f2106332604",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +248,252 @@\t\tif currentReadyCondition != nil {\n\t\t\tif currentReadyCondition.Status != v1.ConditionTrue {\n\t\t\t\t// we need to check this first to get taint working in similar in all cloudproviders\n\t\t\t\t// current problem is that shutdown nodes are not working in similar way ie. all cloudproviders\n\t\t\t\t// does not delete node from kubernetes cluster when instance it is shutdown see issue #46442"
  },
  {
    "id" : "97c0991d-02f7-4879-afbc-64a8fbd1b7a6",
    "prId" : 59323,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59323#pullrequestreview-94340519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "This means that the semantics or this Taint are somewhat cloudy. \r\n- It's applied when Node is NotReady and VM is in a given state\r\n- It's removed when Node is Ready\r\n\r\nIt may be correct, but I guess full negation would be better, i.e.\r\n- It's removed when Node is Ready or VM is not in the given state\r\n",
        "createdAt" : "2018-02-06T10:12:08Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "2fb7baff-99ed-4144-92d5-7908f57c538c",
        "parentId" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "authorId" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "body" : "You are correct that it would be better to update that all the time. However, if we do that we will make request to cloudprovider api every 5 seconds. 99% of these requests are not needed. State is not changed. If we have 100 node cluster it make 100 api requests every 5 second. Imo too much because its not really needed",
        "createdAt" : "2018-02-06T10:24:46Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "tags" : [
        ]
      },
      {
        "id" : "17a99077-7dbf-4066-aa6b-52db95f60fe9",
        "parentId" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Instead of asking about every single VM we can just ask for the list of all VMs and work with that.",
        "createdAt" : "2018-02-06T12:39:08Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "5d9dd96b-c914-4d81-b155-918e43835df1",
        "parentId" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "authorId" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "body" : "@zetaab what do you mean we will make it every 5 seconds? Nothing I see Is on such a short interval. ",
        "createdAt" : "2018-02-06T12:54:56Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "tags" : [
        ]
      },
      {
        "id" : "b44025a0-ec2f-4398-9506-788196ad325c",
        "parentId" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "authorId" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "body" : "@jhorwit2 monitornode function is executed every 5 seconds\r\n\r\nhttps://github.com/zetaab/kubernetes/blob/6f47d8d2082b025bb60c394233af26b774a7447d/pkg/controller/cloud/node_controller.go#L127\r\n\r\nNewCloudNodeController created here: \r\nhttps://github.com/zetaab/kubernetes/blob/6f47d8d2082b025bb60c394233af26b774a7447d/cmd/cloud-controller-manager/app/controllermanager.go#L224\r\n\r\nand duration defined here\r\nhttps://github.com/zetaab/kubernetes/blob/6f47d8d2082b025bb60c394233af26b774a7447d/cmd/controller-manager/app/options/utils.go#L81",
        "createdAt" : "2018-02-06T12:57:03Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "tags" : [
        ]
      },
      {
        "id" : "e79edffc-340d-4cea-87c2-4473da64cc0e",
        "parentId" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "authorId" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "body" : "That seems unnecessarily quick. The KCM marks nodes not ready after 40 seconds by default, so i'm not sure why this would need to be faster than that. ",
        "createdAt" : "2018-02-06T13:13:09Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "tags" : [
        ]
      },
      {
        "id" : "75fec057-3fb7-4200-8592-13bcb5fbd915",
        "parentId" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "authorId" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "body" : "I think we could do this whole thing little bit better. If we increase monitornode time, it will mean that removing / adding shutdown taint takes 40seconds+monitornode delay maximum. I mean that if node notready takes 40seconds, it might mean that adding taint takes 40seconds+5seconds maximum currently.\r\n\r\nWhat about if we re-structure this whole feature, if node goes notready state why it cannot push message towards monitornode(using channel maybe)? Then there is (almost) no delay, and we do not need refresh that 24/7 every 5seconds?\r\n\r\nedit: if this row https://github.com/zetaab/kubernetes/blob/6f47d8d2082b025bb60c394233af26b774a7447d/pkg/controller/cloud/node_controller.go#L229 is adding that notready state.. then channels does not help much. But imo 5seconds is then too fast(?)",
        "createdAt" : "2018-02-06T13:17:18Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "tags" : [
        ]
      },
      {
        "id" : "ee53a81d-95c3-4104-bb1e-813bbb937344",
        "parentId" : "f48f2a18-1ef0-4cbf-833c-e453ac2f1860",
        "authorId" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "body" : "@jhorwit2 yep 200 maximum per 5seconds with 100 nodes. @gmarek well we could do that. However, I would like to modify then this whole monitornode function. Instead of checking nodes status one by one in cloudprovider, we should make bulk query towards cloudprovider (which input contains all nodes providerids). Then cloudprovider queries providerids statuses from cloudprovider API, and returns three arrays as return to this monitornode function. These arrays are: shutdown, active and removed. Where shutdown array item means = add taint to node if taint not exist yet. Active array item = if taint exist in node remove taint, removed = delete node. This is quite big change, and I would not like go there now :)",
        "createdAt" : "2018-02-06T13:26:20Z",
        "updatedAt" : "2018-02-10T13:41:53Z",
        "lastEditedBy" : "0cf405a8-951c-46f8-bbaf-cf214ebb52dd",
        "tags" : [
        ]
      }
    ],
    "commit" : "6665fa71441fb136303aa5adcb4b2f2106332604",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +301,305 @@\t\t\t} else {\n\t\t\t\t// if taint exist remove taint\n\t\t\t\terr = controller.RemoveTaintOffNode(cnc.kubeClient, node.Name, node, ShutDownTaint)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Errorf(\"Error patching node taints: %v\", err)"
  },
  {
    "id" : "8c1fafa3-5cc3-4b8b-a4fa-7f09188874db",
    "prId" : 59323,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59323#pullrequestreview-96890732",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1908ecee-5d79-4995-b0f9-9a3c44145177",
        "parentId" : null,
        "authorId" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "body" : "nit: should `exists` be something like `shutdown`? \r\n\r\nIf you do decided to change this, you don't have to change 245/271 below.",
        "createdAt" : "2018-02-15T15:33:24Z",
        "updatedAt" : "2018-02-15T15:33:24Z",
        "lastEditedBy" : "8e9f49fc-1050-4601-b81c-83bf660c5eb8",
        "tags" : [
        ]
      }
    ],
    "commit" : "6665fa71441fb136303aa5adcb4b2f2106332604",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +251,255 @@\t\t\t\t// current problem is that shutdown nodes are not working in similar way ie. all cloudproviders\n\t\t\t\t// does not delete node from kubernetes cluster when instance it is shutdown see issue #46442\n\t\t\t\texists, err := instances.InstanceShutdownByProviderID(context.TODO(), node.Spec.ProviderID)\n\t\t\t\tif err != nil && err != cloudprovider.NotImplemented {\n\t\t\t\t\tglog.Errorf(\"Error getting data for node %s from cloud: %v\", node.Name, err)"
  },
  {
    "id" : "ff9b5b95-f417-4622-9b71-4943d9123ed6",
    "prId" : 59323,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59323#pullrequestreview-97312196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "431d45d5-5d78-4769-aae4-da0b22b96bd4",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "It looks like we cannot skip here.",
        "createdAt" : "2018-02-16T20:49:15Z",
        "updatedAt" : "2018-02-16T20:49:15Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "6665fa71441fb136303aa5adcb4b2f2106332604",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +252,256 @@\t\t\t\t// does not delete node from kubernetes cluster when instance it is shutdown see issue #46442\n\t\t\t\texists, err := instances.InstanceShutdownByProviderID(context.TODO(), node.Spec.ProviderID)\n\t\t\t\tif err != nil && err != cloudprovider.NotImplemented {\n\t\t\t\t\tglog.Errorf(\"Error getting data for node %s from cloud: %v\", node.Name, err)\n\t\t\t\t\tcontinue"
  },
  {
    "id" : "dc45542b-5b87-4fe7-a05b-aa17bdee1f8a",
    "prId" : 59323,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59323#pullrequestreview-97312364",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bb123cf-f052-4904-a666-cfe94c35d387",
        "parentId" : null,
        "authorId" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "body" : "If shutdown state returns without err, taint the node. Otherwise, continue checking",
        "createdAt" : "2018-02-16T20:49:49Z",
        "updatedAt" : "2018-02-16T20:49:49Z",
        "lastEditedBy" : "44594ff0-8fbc-44a7-84f9-654ffd54270f",
        "tags" : [
        ]
      }
    ],
    "commit" : "6665fa71441fb136303aa5adcb4b2f2106332604",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +257,261 @@\t\t\t\t}\n\n\t\t\t\tif exists {\n\t\t\t\t\t// if node is shutdown add shutdown taint\n\t\t\t\t\terr = controller.AddOrUpdateTaintOnNode(cnc.kubeClient, node.Name, ShutDownTaint)"
  },
  {
    "id" : "8eea4151-2858-4272-93d8-ff1e50bb3c84",
    "prId" : 53517,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/53517#pullrequestreview-68362817",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02bf951f-c402-4896-98bf-ce7cd0c519ea",
        "parentId" : null,
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "What v-level is this by default? Perhaps be consistent with `V(2)` here?",
        "createdAt" : "2017-10-10T16:54:31Z",
        "updatedAt" : "2017-10-10T16:57:39Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      },
      {
        "id" : "b9d34e07-1895-4ee0-8469-bd750cbc0bf8",
        "parentId" : "02bf951f-c402-4896-98bf-ce7cd0c519ea",
        "authorId" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "body" : "I believe it's 0 by default, so it will always log. This was intentional :)",
        "createdAt" : "2017-10-10T17:22:41Z",
        "updatedAt" : "2017-10-10T17:22:41Z",
        "lastEditedBy" : "6dd71efb-88b1-4bb0-b30a-0df658362f14",
        "tags" : [
        ]
      },
      {
        "id" : "e51645c5-254b-4df1-80da-daaa9dcac26c",
        "parentId" : "02bf951f-c402-4896-98bf-ce7cd0c519ea",
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "Ok, great!",
        "createdAt" : "2017-10-10T17:27:20Z",
        "updatedAt" : "2017-10-10T17:27:20Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "2def4434401ef7dab9fa890f7b449e79a9f7ca97",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +369,373 @@\t}\n\n\tglog.Infof(\"Successfully initialized node %s with cloud provider\", node.Name)\n}\n"
  },
  {
    "id" : "339fd8d5-9ea2-4b8a-98c3-0f342b65eb05",
    "prId" : 51087,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51087#pullrequestreview-58773087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a64dafd6-096f-492c-8ee8-a08bc0d67fc5",
        "parentId" : null,
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "I thought we deprecated the ExternalID name everywhere, why not `ensureNodeExistsByProviderIDOrNodeName`?",
        "createdAt" : "2017-08-25T21:17:35Z",
        "updatedAt" : "2017-08-25T22:04:18Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      },
      {
        "id" : "606d509a-95df-436f-ae59-ee1e3d5c128a",
        "parentId" : "a64dafd6-096f-492c-8ee8-a08bc0d67fc5",
        "authorId" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "body" : ":) that's what I had before, but changed it based on @wlan0's feedback. ",
        "createdAt" : "2017-08-25T22:00:43Z",
        "updatedAt" : "2017-08-25T22:04:18Z",
        "lastEditedBy" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf75c4988304ff8eab4d0223fa63095e626faaeb",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +238,242 @@\t\t\t\t// Check with the cloud provider to see if the node still exists. If it\n\t\t\t\t// doesn't, delete the node immediately.\n\t\t\t\texists, err := ensureNodeExistsByProviderIDOrExternalID(instances, node)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Errorf(\"Error getting data for node %s from cloud: %v\", node.Name, err)"
  },
  {
    "id" : "7a6e03f2-0edc-4b39-b2af-a79f12698bdd",
    "prId" : 51087,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/51087#pullrequestreview-58775362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed8815e1-a6d6-459c-9c39-e3357511cfa2",
        "parentId" : null,
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "why not `instances.NodeName` like we talked about?\r\nIsn't that implemented yet? (Did it stall out in a PR somewhere?)",
        "createdAt" : "2017-08-25T21:19:23Z",
        "updatedAt" : "2017-08-25T22:04:18Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      },
      {
        "id" : "e59cad02-e5cd-42a8-a5bd-c3335428df6a",
        "parentId" : "ed8815e1-a6d6-459c-9c39-e3357511cfa2",
        "authorId" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "body" : "I don't recall us discussing that. Mind filling me in??",
        "createdAt" : "2017-08-25T22:00:00Z",
        "updatedAt" : "2017-08-25T22:04:18Z",
        "lastEditedBy" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "tags" : [
        ]
      },
      {
        "id" : "16d9330e-6cac-40c8-850a-f33d7dc1beec",
        "parentId" : "ed8815e1-a6d6-459c-9c39-e3357511cfa2",
        "authorId" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "body" : "I thought we landed on keeping the ExistsByProviderID & ExternalID checks. Then in the future we can add a ExistsByNodeName if we want to, but avoid adding that method now since it's a duplicate of ExternalID. ",
        "createdAt" : "2017-08-25T22:06:03Z",
        "updatedAt" : "2017-08-25T22:06:03Z",
        "lastEditedBy" : "e45395cd-3c20-445c-b404-a2b9ffe5efab",
        "tags" : [
        ]
      },
      {
        "id" : "629b1d9c-a2a7-4a05-bd6e-b74b28263bc2",
        "parentId" : "ed8815e1-a6d6-459c-9c39-e3357511cfa2",
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "hah, never mind :smile:\r\nThat was the case indeed, recalled incorrectly",
        "createdAt" : "2017-08-25T22:17:04Z",
        "updatedAt" : "2017-08-25T22:17:04Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf75c4988304ff8eab4d0223fa63095e626faaeb",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +388,392 @@\tif err != nil {\n\t\tproviderIDErr := err\n\t\t_, err = instances.ExternalID(types.NodeName(node.Name))\n\t\tif err == nil {\n\t\t\treturn true, nil"
  },
  {
    "id" : "4beae52f-1215-4ce4-8312-071ecaf04724",
    "prId" : 50858,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50858#pullrequestreview-57299080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12d5652d-045c-44a1-8baa-785959322761",
        "parentId" : null,
        "authorId" : "ae819fef-700f-45ca-aa42-0afbedcadc19",
        "body" : "Thanks for fixing this. This originally always returned the zone in which the cloud controller manager was running, which was inaccurate. ",
        "createdAt" : "2017-08-18T20:01:50Z",
        "updatedAt" : "2017-08-25T01:08:01Z",
        "lastEditedBy" : "ae819fef-700f-45ca-aa42-0afbedcadc19",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd3cc8311056be47156b029e7ee44ad558589b64",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +322,326 @@\n\t\tif zones, ok := cnc.cloud.Zones(); ok {\n\t\t\tzone, err := getZoneByProviderIDOrName(zones, curNode)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to get zone from cloud provider: %v\", err)"
  }
]