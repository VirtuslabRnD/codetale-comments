[
  {
    "id" : "8505b00f-368d-479f-b5b4-ee83807b28a1",
    "prId" : 37720,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/37720#pullrequestreview-11067072",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ee8065c-4afc-4235-92ae-a3850b637b12",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "can you confirm that if I modify the lbSourceRanges, and nothing but, that EnsureLoadBalancer only modifies the firewall and doesn't do something retarded like recreate the forwarding rule instead? (check the activity logs in your project to make sure)",
        "createdAt" : "2016-12-01T00:57:57Z",
        "updatedAt" : "2016-12-01T21:56:35Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "7b6580b6-5396-4515-94f4-faf15dcd7fca",
        "parentId" : "2ee8065c-4afc-4235-92ae-a3850b637b12",
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "Just confirmed by manual testing, only firewall update happens. ",
        "createdAt" : "2016-12-01T22:02:53Z",
        "updatedAt" : "2016-12-01T22:02:53Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ea7b950d3278efb7a52882b08bb52bc71f170f3",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +433,437 @@\n\tif wantsLoadBalancer(newService) && !reflect.DeepEqual(oldService.Spec.LoadBalancerSourceRanges, newService.Spec.LoadBalancerSourceRanges) {\n\t\ts.eventRecorder.Eventf(newService, v1.EventTypeNormal, \"LoadBalancerSourceRanges\", \"%v -> %v\",\n\t\t\toldService.Spec.LoadBalancerSourceRanges, newService.Spec.LoadBalancerSourceRanges)\n\t\treturn true"
  },
  {
    "id" : "a3b5ea4a-7c82-4578-803f-5283efe7f2a4",
    "prId" : 31828,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b70951c6-f70e-490a-ae90-0eceab133924",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "if you're simply returning true when either old or new service needs loadbalancing, does the rest of this function with the events even run? \n",
        "createdAt" : "2016-09-02T23:19:06Z",
        "updatedAt" : "2016-09-06T18:36:06Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "ccce8ecd-85fb-40f3-bf72-4d259929f801",
        "parentId" : "b70951c6-f70e-490a-ae90-0eceab133924",
        "authorId" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "body" : "Yes, the rest of the checks won't be necessary if this change is what we want.\n",
        "createdAt" : "2016-09-06T18:00:12Z",
        "updatedAt" : "2016-09-06T18:36:06Z",
        "lastEditedBy" : "d36c6e2e-68ff-4beb-99ef-11c76f6929ce",
        "tags" : [
        ]
      },
      {
        "id" : "7fb08472-bbdc-4b35-9a9d-20ffe8dc8478",
        "parentId" : "b70951c6-f70e-490a-ae90-0eceab133924",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "hmm, we still want the events telling us when something changed right? \n",
        "createdAt" : "2016-09-06T18:03:50Z",
        "updatedAt" : "2016-09-06T18:36:06Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "81b36aaba612995bd2aac3fb5d27981c1dde734a",
    "line" : null,
    "diffHunk" : "@@ -1,1 +429,433 @@\t\ts.eventRecorder.Eventf(newService, api.EventTypeNormal, \"Type\", \"%v -> %v\",\n\t\t\toldService.Spec.Type, newService.Spec.Type)\n\t\treturn true\n\t}\n\tif !portsEqualForLB(oldService, newService) || oldService.Spec.SessionAffinity != newService.Spec.SessionAffinity {"
  },
  {
    "id" : "6ed6e572-d89e-47f5-b6f9-b86f22e3e4e2",
    "prId" : 28781,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d91d7c93-7f2e-4f46-b39b-f888c6fd50ef",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "isSchedulable ?\n",
        "createdAt" : "2016-07-12T08:18:43Z",
        "updatedAt" : "2016-07-12T08:18:43Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "d14fe0f26992442fe945e7b438ca22e6edccd7bd",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +630,634 @@}\n\nfunc includeNodeFromNodeList(node *api.Node) bool {\n\treturn !node.Spec.Unschedulable\n}"
  },
  {
    "id" : "8fc75453-c6ed-4a12-a37f-f19d32f0dbb4",
    "prId" : 25189,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8f39e5b-2791-4fde-8f66-ded860e48832",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "please add comments about these constants\n",
        "createdAt" : "2016-05-09T17:01:52Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fa640490e272589ec69e32a37f5ea611669eb13",
    "line" : null,
    "diffHunk" : "@@ -1,1 +48,52 @@\tserviceSyncPeriod = 30 * time.Second\n\t// Interval of synchoronizing node status from apiserver\n\tnodeSyncPeriod = 100 * time.Second\n\n\t// How long to wait before retrying the processing of a service change."
  },
  {
    "id" : "b3fd81c4-c2ba-41be-87a4-5eac1e5d9ef6",
    "prId" : 25189,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5af90d8a-f019-4be7-bb6f-83c18d1a2cea",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "can we make this opaque? i mean if the same obj key is requeued with the same error, do the backoff within some backoffQueue wrapper type that manages enqueue into the normal work queue? if the enqeueu is invoked with (obj, err=nil) then always requeue. \n",
        "createdAt" : "2016-05-12T18:02:08Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fa640490e272589ec69e32a37f5ea611669eb13",
    "line" : 607,
    "diffHunk" : "@@ -1,1 +754,758 @@\t\t// Add the failed service back to the queue so we'll retry it.\n\t\tglog.Errorf(\"Failed to process service. Retrying in %s: %v\", retryDelay, err)\n\t\tgo func(obj interface{}, delay time.Duration) {\n\t\t\t// put back the service key to working queue, it is possible that more entries of the service\n\t\t\t// were added into the queue during the delay, but it does not mess as when handling the retry,"
  },
  {
    "id" : "d34f2680-f3e9-4eaf-97f3-2cd32570765b",
    "prId" : 25189,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2e5bafd-22a4-4f11-961f-3c4160b7f646",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "why do we need this retry delay when we're using the delayedQueue to process retries? \n",
        "createdAt" : "2016-07-27T21:52:23Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "c2e5a145-ce5f-4d15-af89-c8a67e5366cd",
        "parentId" : "e2e5bafd-22a4-4f11-961f-3c4160b7f646",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "The original logic is double the retry delay every time, so the delay time to put the element to queue is 1s, 2s, 4s, 8s. Here I kept the logic, we need some mechanism to calculate the delay time.\n",
        "createdAt" : "2016-07-28T00:06:02Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      },
      {
        "id" : "f6da55e7-6841-4b0d-b206-cc6f23eacf6d",
        "parentId" : "e2e5bafd-22a4-4f11-961f-3c4160b7f646",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "Won't the delay queue do that based on the key you're enqueueing? \n",
        "createdAt" : "2016-07-28T00:07:34Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "3394b373-9b7e-41c0-b9ef-aa1c8e1b2324",
        "parentId" : "e2e5bafd-22a4-4f11-961f-3c4160b7f646",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "delay queue provides a new interface: delayingType.AddAfter(item interface{}, duration time.Duration), but I think we need to define the duration.\n",
        "createdAt" : "2016-07-28T00:12:50Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fa640490e272589ec69e32a37f5ea611669eb13",
    "line" : 574,
    "diffHunk" : "@@ -1,1 +721,725 @@\tstartTime := time.Now()\n\tvar cachedService *cachedService\n\tvar retryDelay time.Duration\n\tdefer func() {\n\t\tglog.V(4).Infof(\"Finished syncing service %q (%v)\", key, time.Now().Sub(startTime))"
  },
  {
    "id" : "dd2fbb8a-a55d-4071-9026-c1a4738af9c3",
    "prId" : 25189,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6dcee5bf-5fcd-4899-8f7c-23e233984ee5",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "So when i create a nodePort service, this cachedService won't exist right ? I'm trying to understand the case you're explaning. We shouldn't send out an event for \"DeletingLoadBalancer\" when someone creates a NodePort Service. \n",
        "createdAt" : "2016-07-27T21:52:29Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "325de745-26c5-496e-bfb4-f7edf376fd75",
        "parentId" : "6dcee5bf-5fcd-4899-8f7c-23e233984ee5",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "For the nodeport service creation case, the code will go servicecontroller.createLoadBalancerIfNeeded(), and then servicecontroller.go#L288. \n_, exists, err := s.balancer.GetLoadBalancer(s.clusterName, service) should return non exists, so needDelete should be false, then deletion attempt should not happen for NodePort service creation. (yesterday, I see the event when I create nodeport service)\n\n**Today, I am doubting there is problem on my fake data, so please give me some time to confirm this.**\n",
        "createdAt" : "2016-07-28T00:21:51Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      },
      {
        "id" : "b0667452-f373-46f7-8a4d-92eaa1cc9b24",
        "parentId" : "6dcee5bf-5fcd-4899-8f7c-23e233984ee5",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "confirmed NodePort service should be working fine, the event should not be thrown for real case, I saw this message as i am using fake cloud provider, and the LB exists  info is hard coded.\n",
        "createdAt" : "2016-07-28T02:07:07Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fa640490e272589ec69e32a37f5ea611669eb13",
    "line" : 627,
    "diffHunk" : "@@ -1,1 +774,778 @@\t\treturn fmt.Errorf(\"Service %s not in cache even though the watcher thought it was. Ignoring the deletion.\", key), doNotRetry\n\t}\n\tservice := cachedService.state\n\t// delete load balancer info only if the service type is LoadBalancer\n\tif !wantsLoadBalancer(service) {"
  },
  {
    "id" : "986a6121-c11b-4576-a2b2-ec7253b555b2",
    "prId" : 25189,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ec92854-4b2c-475c-8fee-8e4a4a090c54",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "this will relist services every 30s right? please include that in the comment. Also did you pick 30s for a reason, can we make it longer/shorter? \n",
        "createdAt" : "2016-07-27T21:52:40Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "3c87e7d2-9de7-434b-9962-c885723da1e5",
        "parentId" : "6ec92854-4b2c-475c-8fee-8e4a4a090c54",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "If you check other controller code, the full sync period are all fixed, I applied the same pattern. \nlike in replica_set.go\n\n```\nFullControllerResyncPeriod = 30 * time.Second\nPodStoreSyncedPollPeriod = 100 * time.Millisecond\n```\n\nIf we need this to be configurable?\n",
        "createdAt" : "2016-07-28T00:26:24Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      },
      {
        "id" : "043ea73e-1948-4161-bb94-00b6c8c2a2b4",
        "parentId" : "6ec92854-4b2c-475c-8fee-8e4a4a090c54",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "I believe we recently split relist and resync, and made the relisting longer and the resync shorter. I can confirm when I'm at keyboard, unless you get to it first. \n",
        "createdAt" : "2016-07-28T01:52:36Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "7504fd33-0887-42e5-81cb-8fd7d4554211",
        "parentId" : "6ec92854-4b2c-475c-8fee-8e4a4a090c54",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "relist == list from apiserver, more expensive\nresync == sync with local cache state from last apiserver list\n",
        "createdAt" : "2016-07-28T01:52:59Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "cde3298e-2519-41a1-bdf7-575a7f3dc812",
        "parentId" : "6ec92854-4b2c-475c-8fee-8e4a4a090c54",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "I addressed other comments except this one, I tried to do a full context search or check other controllers, but did not see the reference.\n",
        "createdAt" : "2016-07-28T03:04:46Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fa640490e272589ec69e32a37f5ea611669eb13",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +46,50 @@const (\n\t// Interval of synchoronizing service status from apiserver\n\tserviceSyncPeriod = 30 * time.Second\n\t// Interval of synchoronizing node status from apiserver\n\tnodeSyncPeriod = 100 * time.Second"
  },
  {
    "id" : "1e2619d4-6c48-4e8e-a5fa-de1b6ac544e8",
    "prId" : 25189,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfdfdffa-09da-4546-9ba3-64fe616e7c53",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "is this comment accurate? where are we saving failed services? just to clarify, this essentially updates the targetpools of all the lbs, removing notReady nodes right? how are failures handled here?\n",
        "createdAt" : "2016-08-02T03:01:00Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "3b55bdae-4c66-4538-8c36-c147908528df",
        "parentId" : "bfdfdffa-09da-4546-9ba3-64fe616e7c53",
        "authorId" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "body" : "This is the logic that I did not change much, the service that failed to updated LB info will be saved to `servicesToUpdate` and retry next round.\n\n```\nfunc (s *ServiceController) updateLoadBalancerHosts(services []*cachedService, hosts []string) (servicesToRetry []*cachedService) \n```\n",
        "createdAt" : "2016-08-02T04:44:19Z",
        "updatedAt" : "2016-08-04T01:29:11Z",
        "lastEditedBy" : "ee3c1908-4390-4e95-844f-fb1e0ab18e67",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fa640490e272589ec69e32a37f5ea611669eb13",
    "line" : 531,
    "diffHunk" : "@@ -1,1 +636,640 @@\n\t// Try updating all services, and save the ones that fail to try again next\n\t// round.\n\tservicesToUpdate = s.cache.allServices()\n\tnumServices := len(servicesToUpdate)"
  },
  {
    "id" : "85e65cbf-e085-438a-b9e3-95e675a27274",
    "prId" : 22069,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53782faf-e600-43f7-9197-941c83817c8b",
        "parentId" : null,
        "authorId" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "body" : "Not entirely sure why we need the set here, but I think it was pre-existing...\n",
        "createdAt" : "2016-02-26T20:19:34Z",
        "updatedAt" : "2016-02-26T20:26:41Z",
        "lastEditedBy" : "8fc8f958-3c0e-47dd-a0fb-b8cc483b4efb",
        "tags" : [
        ]
      },
      {
        "id" : "b28fe48a-1d19-4e2d-bd50-aa430217b38c",
        "parentId" : "53782faf-e600-43f7-9197-941c83817c8b",
        "authorId" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "body" : "Yeah, I also don't think it's necessary, but leaving to avoid extra churn right before the release.\n",
        "createdAt" : "2016-02-26T20:26:34Z",
        "updatedAt" : "2016-02-26T20:26:41Z",
        "lastEditedBy" : "7766e039-aa4c-4476-9091-5cc8763fa8d6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab797d1b6590a1921373dd85f040fc01a3221de8",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +273,277 @@\t// been successfully processed.\n\tcachedService.appliedState = service\n\ts.cache.set(namespacedName.String(), cachedService)\n\n\treturn nil, notRetryable"
  },
  {
    "id" : "eaaf614b-4f8f-4334-b0a4-d7607394f82a",
    "prId" : 21431,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a3193bd-16b9-48a0-a8f0-d13f10ff5a71",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "I don't think we'll have an order problem here, can you confirm? (i.e we get [src-range1, src-range2] on first update, and [src-rance2, src-range1] on second update).\n",
        "createdAt" : "2016-02-18T01:29:28Z",
        "updatedAt" : "2016-02-19T01:06:22Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "303114f6-db55-412e-9de9-d6106d6cde9e",
        "parentId" : "7a3193bd-16b9-48a0-a8f0-d13f10ff5a71",
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "They are maps. Order does not matter here. \nhttp://stackoverflow.com/questions/18208394/testing-equivalence-of-maps-golang\n",
        "createdAt" : "2016-02-18T20:48:20Z",
        "updatedAt" : "2016-02-19T01:06:22Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      },
      {
        "id" : "306a2a0f-337d-47aa-8731-be8d137b2223",
        "parentId" : "7a3193bd-16b9-48a0-a8f0-d13f10ff5a71",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "it's a map that contains strings, one of which is a list. you could still have a user update the annotation to go from: src-range1, src-range2 to src-range2, src-range1 right? that will cause us to do what? this deepequal will fail, but will it get ignored higher up the stack? or is it ok to just re-create the rule since it's the exact same? or will there be some period inbetween when we have no rule? \n",
        "createdAt" : "2016-02-19T00:51:15Z",
        "updatedAt" : "2016-02-19T01:06:22Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ffb123abebb93c2483ff500863b30f139ca2fbe",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +492,496 @@\t\t}\n\t}\n\tif !reflect.DeepEqual(oldService.Annotations, newService.Annotations) {\n\t\treturn true\n\t}"
  },
  {
    "id" : "0b3d16e8-b06a-41cf-bd25-4adbd5d2d275",
    "prId" : 14431,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e47f97f6-b312-4e2d-96c4-27df56a985d0",
        "parentId" : null,
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "this is a clarity issue, I think I'd really rather have an event though you might not want to do that in this pr. If someone creates a mixed service they'll just be puzzled, get on slack, debug with on-call till we open up kube-controller-manager logs. An event would short circuit this. \n",
        "createdAt" : "2015-11-10T00:18:07Z",
        "updatedAt" : "2016-01-05T20:51:59Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      },
      {
        "id" : "286ce44a-b101-4daf-be8a-a608375ea69b",
        "parentId" : "e47f97f6-b312-4e2d-96c4-27df56a985d0",
        "authorId" : "537fd325-3646-446a-b1d0-2c1c5569384f",
        "body" : "Same as above.  If it is preferred to record an event here, let's change that throughout.  This basically replaces the previous error that was returned in the same manner that indicated TCP wasn't supported at all.\n",
        "createdAt" : "2015-11-10T02:18:35Z",
        "updatedAt" : "2016-01-05T20:51:59Z",
        "lastEditedBy" : "537fd325-3646-446a-b1d0-2c1c5569384f",
        "tags" : [
        ]
      },
      {
        "id" : "a5b01b54-abb8-44c2-a804-4e38474229b6",
        "parentId" : "e47f97f6-b312-4e2d-96c4-27df56a985d0",
        "authorId" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "body" : "that's fine\n",
        "createdAt" : "2015-11-10T03:17:17Z",
        "updatedAt" : "2016-01-05T20:51:59Z",
        "lastEditedBy" : "395f4f9a-98be-4485-b436-51f0897d7c9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e6c45c395b4cda611c6b325aa70c8475d446047",
    "line" : null,
    "diffHunk" : "@@ -1,1 +497,501 @@\t\t} else if protocol != sp.Protocol && wantsLoadBalancer(service) {\n\t\t\t// TODO:  Convert error messages to use event recorder\n\t\t\treturn nil, fmt.Errorf(\"mixed protocol external load balancers are not supported.\")\n\t\t}\n\t}"
  },
  {
    "id" : "896f8023-8ad5-45f5-abbb-5da354870b3f",
    "prId" : 13922,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e61eea1-8835-4367-aaaa-473aab56c0b7",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "Including actual hosts may make it less likely that identical events will be coalesced.  Also, the hosts were not passed to the function that had the error, so they can't really be part of the Reason. \n",
        "createdAt" : "2015-09-30T06:55:25Z",
        "updatedAt" : "2015-09-30T06:55:25Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "4bacc7d4-3c60-46b1-b1a2-7eb3fb847f04",
        "parentId" : "9e61eea1-8835-4367-aaaa-473aab56c0b7",
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "@jszczepkowski \n",
        "createdAt" : "2015-09-30T06:55:39Z",
        "updatedAt" : "2015-09-30T06:55:39Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      }
    ],
    "commit" : "04919ebfa942cf6bef5d9cd267910f525d649e5f",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +671,675 @@\t}\n\n\ts.eventRecorder.Eventf(service, \"LoadBalancerUpdateFailed\", \"Error updating load balancer with new hosts: %v\", err)\n\treturn err\n}"
  }
]