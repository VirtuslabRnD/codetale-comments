[
  {
    "id" : "b50a247c-6cf2-424d-99ff-93ec4959ed4c",
    "prId" : 99398,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99398#pullrequestreview-600597861",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64692f0a-8527-47be-8b46-78b7bdf43140",
        "parentId" : null,
        "authorId" : "c5cbab8f-2eaf-48d9-a433-0b0fd4a2806d",
        "body" : "As if updateObserveGen has been called in storeDaemonSetStatus function :\r\n1041 \r\n   1042         var updateErr, getErr error\r\n   1043         for i := 0; i < StatusUpdateRetries; i++ {\r\n   1044                 if updateObservedGen {\r\n   1045                         toUpdate.Status.ObservedGeneration = ds.Generation\r\n   1046                 }",
        "createdAt" : "2021-03-01T10:00:19Z",
        "updatedAt" : "2021-03-08T16:50:09Z",
        "lastEditedBy" : "c5cbab8f-2eaf-48d9-a433-0b0fd4a2806d",
        "tags" : [
        ]
      },
      {
        "id" : "6a7c3e22-8650-44cb-9b9b-8ab12ad016c7",
        "parentId" : "64692f0a-8527-47be-8b46-78b7bdf43140",
        "authorId" : "f8030d76-6069-40c7-9c21-93f9f4b262ad",
        "body" : "Sorry, could you explain what you mean?",
        "createdAt" : "2021-03-01T10:42:14Z",
        "updatedAt" : "2021-03-08T16:50:09Z",
        "lastEditedBy" : "f8030d76-6069-40c7-9c21-93f9f4b262ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "60bb2a3a6fb687abf9024a7aaa49f9eecde5395b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1073,1077 @@\t\tif updateObservedGen {\n\t\t\ttoUpdate.Status.ObservedGeneration = ds.Generation\n\t\t}\n\t\ttoUpdate.Status.DesiredNumberScheduled = int32(desiredNumberScheduled)\n\t\ttoUpdate.Status.CurrentNumberScheduled = int32(currentNumberScheduled)"
  },
  {
    "id" : "4ba115f9-1226-4f54-9c71-d00a4d1b7e5f",
    "prId" : 79060,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/79060#pullrequestreview-263213796",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9dbc1ea7-1e95-49e8-837b-822f06066af4",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "I'd prefer `if err != nil` here instead of having to declare `var dsKey string`",
        "createdAt" : "2019-07-17T17:44:00Z",
        "updatedAt" : "2019-07-17T17:52:53Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c429b7d3e49f29ac86f167334c1af362d64838ee",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +879,883 @@\tdsKey, err := cache.MetaNamespaceKeyFunc(ds)\n\tif err != nil {\n\t\tutilruntime.HandleError(err)\n\t\treturn\n\t}"
  },
  {
    "id" : "97f77738-484d-4d7b-a835-3f2030c4b46d",
    "prId" : 77773,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77773#pullrequestreview-239182390",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac6ada7a-646e-4d50-b990-468906dbc9a3",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "i am confused how this fixes the issue ? Did you try with this fix and it solves the issue ?",
        "createdAt" : "2019-05-13T07:29:58Z",
        "updatedAt" : "2019-05-16T02:57:16Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "56d3789b-e9cd-410f-a8a6-4beb8d5d7d16",
        "parentId" : "ac6ada7a-646e-4d50-b990-468906dbc9a3",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "The reason why ds rolling update stucked is expectations not be satisfied. In this fix, we lower deletion expectation once pod's DeletionTimestamp is not nil to satisfy expectation and let syncloop resume.  ",
        "createdAt" : "2019-05-13T09:06:53Z",
        "updatedAt" : "2019-05-16T02:57:16Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      },
      {
        "id" : "62f5288d-7903-4803-94ce-d68863901af5",
        "parentId" : "ac6ada7a-646e-4d50-b990-468906dbc9a3",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "Before this fix, if a pod stucked in terminating, deamonset will never satisfy expectation.",
        "createdAt" : "2019-05-13T09:08:35Z",
        "updatedAt" : "2019-05-16T02:57:16Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      },
      {
        "id" : "830bf11f-3b7a-435b-9f1b-71b11015c6b4",
        "parentId" : "ac6ada7a-646e-4d50-b990-468906dbc9a3",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "shouldnt lowering of expectation be done using dsc.expectations.DeletionObserved(dsKey) ?",
        "createdAt" : "2019-05-17T06:58:42Z",
        "updatedAt" : "2019-05-17T06:58:42Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "b51f1338-32b4-47a4-9214-c821870e9aef",
        "parentId" : "ac6ada7a-646e-4d50-b990-468906dbc9a3",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "@janetkuo  could you help understand this fix ? Do we need a test ?",
        "createdAt" : "2019-05-17T06:59:16Z",
        "updatedAt" : "2019-05-17T06:59:16Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "9996758b-1870-4f4f-bbf9-06b27edd2400",
        "parentId" : "ac6ada7a-646e-4d50-b990-468906dbc9a3",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "> shouldnt lowering of expectation be done using dsc.expectations.DeletionObserved(dsKey) ?\r\n\r\nof course you could just use dsc.expectations.DeletionObserved(dsKey)\r\nbut in this fix we could reuse the code in \"deletePod\" and keep code consistent with replicas-controller and job-controller. see https://github.com/kubernetes/kubernetes/pull/77773#issuecomment-492984652",
        "createdAt" : "2019-05-18T03:18:22Z",
        "updatedAt" : "2019-05-18T03:18:22Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25ff463bacb8f7eff5fcf063a1f1def6e7e0559",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +544,548 @@\t\treturn\n\t}\n\n\tcurControllerRef := metav1.GetControllerOf(curPod)\n\toldControllerRef := metav1.GetControllerOf(oldPod)"
  },
  {
    "id" : "cdbd6014-cdcc-46d7-a399-9e2d21e7006d",
    "prId" : 77773,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77773#pullrequestreview-239206563",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed0afb3c-fe57-45d9-88a1-7df52a5f7348",
        "parentId" : null,
        "authorId" : "f3e672e5-b55c-4e3f-9443-b9abf25195da",
        "body" : "```go\r\nif oldPod.DeletionTimestamp == nil && curPod.DeletionTimestamp != nil {\r\n...\r\n}\r\n```\r\nCheck *oldPod's* DeletionTimestamp will be better.  Same as ReplicaSet-Controller.\r\n",
        "createdAt" : "2019-05-18T13:48:57Z",
        "updatedAt" : "2019-05-18T14:06:17Z",
        "lastEditedBy" : "f3e672e5-b55c-4e3f-9443-b9abf25195da",
        "tags" : [
        ]
      },
      {
        "id" : "5bfbfba1-893d-47e4-8537-c48aafc885d9",
        "parentId" : "ed0afb3c-fe57-45d9-88a1-7df52a5f7348",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "Yes, it might reduce the nonsense syncloop.\r\nThe only risk I have considered is that once handler(updatePod or deletePod) returns before lowering of expectation, we will also lose the chance to satisfy the expectation.\r\n@k82cn please help to review this PR, thanks\r\n",
        "createdAt" : "2019-05-18T15:18:02Z",
        "updatedAt" : "2019-05-18T15:18:02Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      },
      {
        "id" : "9a624c9f-775c-4b03-b1df-9f7cb9fd6fd9",
        "parentId" : "ed0afb3c-fe57-45d9-88a1-7df52a5f7348",
        "authorId" : "f3e672e5-b55c-4e3f-9443-b9abf25195da",
        "body" : "If my suggestion is acceptable, please create another PR to fix ReplicaSet-Controller.",
        "createdAt" : "2019-05-18T17:36:45Z",
        "updatedAt" : "2019-05-18T17:36:46Z",
        "lastEditedBy" : "f3e672e5-b55c-4e3f-9443-b9abf25195da",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25ff463bacb8f7eff5fcf063a1f1def6e7e0559",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +536,540 @@\t}\n\n\tif curPod.DeletionTimestamp != nil {\n\t\t// when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period,\n\t\t// and after such time has passed, the kubelet actually deletes it from the store. We receive an update"
  },
  {
    "id" : "c4b69c12-b146-4b4c-9d53-6668e52dea10",
    "prId" : 77208,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77208#pullrequestreview-232026746",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8cee6dbd-9153-4035-9ed0-0b4ac0875273",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "In replica sets, we ended up adding a second here because of an issue with time in golang. See:\r\nhttps://github.com/kubernetes/kubernetes/blob/b3ad4cd6b99a413cc6da8ded20b535dd224007f5/pkg/controller/replicaset/replica_set.go#L351\r\nhttps://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133",
        "createdAt" : "2019-04-29T21:06:50Z",
        "updatedAt" : "2019-04-29T21:06:50Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "129d3bcd-2de0-4d2c-b3f9-0047c140e2b1",
        "parentId" : "8cee6dbd-9153-4035-9ed0-0b4ac0875273",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Interesting; this is already a thing, see [here](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemon_controller.go#L561).",
        "createdAt" : "2019-04-29T21:12:49Z",
        "updatedAt" : "2019-04-29T21:12:49Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "f95ef29a-9c99-4c87-b7b6-05307eca57d4",
        "parentId" : "8cee6dbd-9153-4035-9ed0-0b4ac0875273",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "https://github.com/kubernetes/kubernetes/blob/b3ad4cd6b99a413cc6da8ded20b535dd224007f5/pkg/controller/replicaset/replica_set.go#L621-L626",
        "createdAt" : "2019-04-30T02:18:05Z",
        "updatedAt" : "2019-04-30T02:18:05Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      },
      {
        "id" : "45c83af7-15d5-4693-98c7-2961e726a848",
        "parentId" : "8cee6dbd-9153-4035-9ed0-0b4ac0875273",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "> Interesting; this is already a thing, see [here](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemon_controller.go#L561).\r\n\r\nBut when controller's clock time is slower than node, pod is still unavailable after minReadySeconds. For example:\r\npod A is ready at 12:00:00  and reenqueue at 12:00:31;\r\ncontroller's time is 12:00:30 when reenqueue event happens, so controller judge A unavailable and daemonset lose the chance to update its status and get stucked.\r\nSo it's nessasary to add  the defense.",
        "createdAt" : "2019-04-30T02:28:52Z",
        "updatedAt" : "2019-04-30T02:46:44Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      },
      {
        "id" : "65251701-cab6-424b-8024-3ab5e6a250f9",
        "parentId" : "8cee6dbd-9153-4035-9ed0-0b4ac0875273",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "> https://github.com/kubernetes/kubernetes/blob/b3ad4cd6b99a413cc6da8ded20b535dd224007f5/pkg/controller/replicaset/replica_set.go#L621-L626\r\n\r\ndaemon controller should give another chance to resync the DaemonSet in case of clock-skew, just like replicaset controller.",
        "createdAt" : "2019-04-30T02:36:40Z",
        "updatedAt" : "2019-04-30T02:36:40Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      },
      {
        "id" : "c7eb853d-9ad5-4b2b-b946-3fe3f67adf33",
        "parentId" : "8cee6dbd-9153-4035-9ed0-0b4ac0875273",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "/lgtm\r\n/ok-to-test",
        "createdAt" : "2019-04-30T07:33:45Z",
        "updatedAt" : "2019-04-30T07:33:45Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "18ed2f6c548b317a17469516c346466a7459e4a3",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1201,1205 @@\t// Resync the DaemonSet after MinReadySeconds as a last line of defense to guard against clock-skew.\n\tif ds.Spec.MinReadySeconds > 0 && numberReady != numberAvailable {\n\t\tdsc.enqueueDaemonSetAfter(ds, time.Duration(ds.Spec.MinReadySeconds)*time.Second)\n\t}\n\treturn nil"
  },
  {
    "id" : "ae94c06f-5ae8-47bd-aaef-aea18c9fc4de",
    "prId" : 77208,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/77208#pullrequestreview-232493423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c70c0151-74e5-4858-bd56-d64916ea96c3",
        "parentId" : null,
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "it would help to enhance this comment with a more detailed explanation of this issue. I am still not following how the nodes (on which pod is running)clock skew from controller nodes clock causes this ?",
        "createdAt" : "2019-05-01T04:30:15Z",
        "updatedAt" : "2019-05-01T04:30:15Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "a212681e-52c7-4310-a13a-d9ed804fcf4a",
        "parentId" : "c70c0151-74e5-4858-bd56-d64916ea96c3",
        "authorId" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "body" : "Seems like the reason would be this line https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemon_controller.go#L1174 ,  the reason from what i understand is \r\n1: kubelet marks the pod ready and changes LastTransitionTime\r\n2: controller checks IsPodAvailable() and finds the pod unavailable since its diffing LastTransitionTime(set by kubelet) with time.Now which is controller time. Since controller is behind in clock, minreadySeconds is not satisfied and it marks unavailable even though minReadySeconds is satisfied\r\n\r\ncan one of you confirm if this is the right understanding of this issue @DaiHao @kargakis  ?\r\nAlso in this case when a pod is marked unavailable, why wont it be requeued ?\r\n\r\nI think including all of this explanation will be helpful\r\n",
        "createdAt" : "2019-05-01T04:58:36Z",
        "updatedAt" : "2019-05-01T04:58:36Z",
        "lastEditedBy" : "224e1088-78fe-4bdd-99d1-31be3e464996",
        "tags" : [
        ]
      },
      {
        "id" : "2f410d16-cd54-41c5-87a4-95457ffff027",
        "parentId" : "c70c0151-74e5-4858-bd56-d64916ea96c3",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "see here. https://github.com/kubernetes/kubernetes/pull/77208#discussion_r279599055 ",
        "createdAt" : "2019-05-01T05:00:37Z",
        "updatedAt" : "2019-05-01T05:00:38Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      },
      {
        "id" : "be554f75-a1a3-4f20-80dd-05f139731ce8",
        "parentId" : "c70c0151-74e5-4858-bd56-d64916ea96c3",
        "authorId" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "body" : "> Seems like the reason would be this line https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemon_controller.go#L1174 , the reason from what i understand is\r\n> 1: kubelet marks the pod ready and changes LastTransitionTime\r\n> 2: controller checks IsPodAvailable() and finds the pod unavailable since its diffing LastTransitionTime(set by kubelet) with time.Now which is controller time. Since controller is behind in clock, minreadySeconds is not satisfied and it marks unavailable even though minReadySeconds is satisfied\r\n> \r\n> can one of you confirm if this is the right understanding of this issue @DaiHao @kargakis ?\r\n> Also in this case when a pod is marked unavailable, why wont it be requeued ?\r\n> \r\n> I think including all of this explanation will be helpful\r\n\r\nYour understanding is right. \r\nPod's status do not change in the sync loop, controller mark it unavailable only in daemonset's status, but which is equal with its last status, so daemonset also not requeue.\r\nsee here. https://github.com/kubernetes/kubernetes/blob/b219272a76bb4ad752417e310b5b50a6e8dc55c0/pkg/controller/daemon/daemon_controller.go#L1107-L1117",
        "createdAt" : "2019-05-01T05:27:34Z",
        "updatedAt" : "2019-05-01T05:27:34Z",
        "lastEditedBy" : "44538d88-ffa2-4d9e-b3ea-9814d3442365",
        "tags" : [
        ]
      }
    ],
    "commit" : "18ed2f6c548b317a17469516c346466a7459e4a3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1199,1203 @@\t}\n\n\t// Resync the DaemonSet after MinReadySeconds as a last line of defense to guard against clock-skew.\n\tif ds.Spec.MinReadySeconds > 0 && numberReady != numberAvailable {\n\t\tdsc.enqueueDaemonSetAfter(ds, time.Duration(ds.Spec.MinReadySeconds)*time.Second)"
  },
  {
    "id" : "89338e45-2a4d-4b53-82d4-151a4256a27f",
    "prId" : 76060,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/76060#pullrequestreview-223570358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "963e238f-4a49-44d6-87e5-de08af663a6c",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "LGTM overall; personally, I'd like to add a comment on the scenario that DaemonSet-Controller will delete Pod without NodeName, and PodGC will delete pod has NodeName.",
        "createdAt" : "2019-04-07T07:56:51Z",
        "updatedAt" : "2019-04-07T07:56:51Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c142cf47b7c097ab51ef9a85f28057f1f34f116",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +1539,1543 @@\t\tif !isNodeRunning[n] {\n\t\t\tfor _, pod := range pods {\n\t\t\t\tif len(pod.Spec.NodeName) == 0 {\n\t\t\t\t\tresults = append(results, pod.Name)\n\t\t\t\t}"
  },
  {
    "id" : "b7b3bbca-d1d1-4d28-b18b-893c22c27b2a",
    "prId" : 74856,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74856#pullrequestreview-223080764",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64e91d0c-8244-4ca5-9797-321117b37d2e",
        "parentId" : null,
        "authorId" : "b86e7e78-bb07-417f-8470-39407559c779",
        "body" : "I gotta admit, I am super skeptical about this change (*and the existing code, too!*), and the test didn't convince me. :)\r\n\r\nFor things like this, instead of calculating a value after the fact, I think it is way safer to e.g. initialize a variable \"remainingCreations\" with the number of expected creations at the beginning of the loop, and then decrement it each time a creation is actually attempted (you might need to use sync/atomic functions for this). Then you don't ever have to worry about this math getting out of sync with the logic in the loop.",
        "createdAt" : "2019-04-04T16:47:51Z",
        "updatedAt" : "2019-05-04T00:16:17Z",
        "lastEditedBy" : "b86e7e78-bb07-417f-8470-39407559c779",
        "tags" : [
        ]
      },
      {
        "id" : "fe3b0d7e-74bc-4f20-9ef0-4f4833b85ad5",
        "parentId" : "64e91d0c-8244-4ca5-9797-321117b37d2e",
        "authorId" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "body" : "I take quite a time to understand and figure out how to calculate the create expectations. And It does make sense to use `remainingCreations` which would reduce the complexity. I'll keep working on it. :)",
        "createdAt" : "2019-04-05T02:46:51Z",
        "updatedAt" : "2019-05-04T00:16:17Z",
        "lastEditedBy" : "9829b6c0-e54c-401b-8d97-73e5aa4e83c1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f8dfdc698c2c6bc6fd63b3e4c9bfee7da9ef0a3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1072,1076 @@\t\tcreateWait.Wait()\n\t\t// any skipped pods that we never attempted to start shouldn't be expected.\n\t\tskippedPods := createDiff - (batchSize + pos)\n\t\tif errorCount < len(errCh) && skippedPods > 0 {\n\t\t\tklog.V(2).Infof(\"Slow-start failure. Skipping creation of %d pods, decrementing expectations for set %q/%q\", skippedPods, ds.Namespace, ds.Name)"
  },
  {
    "id" : "830a72d2-0546-42fc-9aab-258c9c525de2",
    "prId" : 73401,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73401#pullrequestreview-197409139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2730891b-cd60-4768-a476-ca98edd4340d",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "why not dependent on `PodGCController` ?\r\n\r\nxref https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/podgc/gc_controller.go#L143\r\n\r\n",
        "createdAt" : "2019-01-29T07:35:36Z",
        "updatedAt" : "2019-01-29T07:35:36Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "7186d3ba8ba3df736f6a41e3ab773db3a3698f3f",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1534,1538 @@\tvar results []string\n\tisNodeRunning := make(map[string]bool)\n\tfor _, node := range runningNodesList {\n\t\tisNodeRunning[node.Name] = true\n\t}"
  },
  {
    "id" : "27b57978-516c-4781-b33a-8291559841b8",
    "prId" : 73401,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73401#pullrequestreview-197451572",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43586b7f-19bd-43ca-a681-6b39a81063d0",
        "parentId" : null,
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "move to a single line :)",
        "createdAt" : "2019-01-29T09:32:15Z",
        "updatedAt" : "2019-01-29T09:32:16Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "7186d3ba8ba3df736f6a41e3ab773db3a3698f3f",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1531,1535 @@// getPodsWithoutNode returns list of pods assigned to not existing nodes.\nfunc getPodsWithoutNode(\n\trunningNodesList []*v1.Node, nodeToDaemonPods map[string][]*v1.Pod) []string {\n\tvar results []string\n\tisNodeRunning := make(map[string]bool)"
  },
  {
    "id" : "158b1c77-4c50-433e-8392-f758b522a75e",
    "prId" : 69566,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69566#pullrequestreview-164246921",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e16e18fd-5c1b-4e20-bc0b-abf364515c9c",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "I think, we need to a nil check for `NodeInfo` and `KubeletVersion` fields.",
        "createdAt" : "2018-10-12T00:42:14Z",
        "updatedAt" : "2018-10-16T02:31:38Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "2d28f93f-acde-4c43-85c7-97a69ea089ed",
        "parentId" : "e16e18fd-5c1b-4e20-bc0b-abf364515c9c",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "neither of those fields are pointers, so neither can be nil",
        "createdAt" : "2018-10-12T13:43:19Z",
        "updatedAt" : "2018-10-16T02:31:38Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "73d252d0064277f13df2b43b6cbbd1a0b15974f7",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1039,1043 @@\t\t\t\tif node, err := dsc.nodeLister.Get(nodesNeedingDaemonPods[ix]); err != nil {\n\t\t\t\t\tglog.Errorf(\"unknown node %s, disabling ScheduleDaemonSetPods using MatchFields: %v\", nodesNeedingDaemonPods[ix], err)\n\t\t\t\t} else if kubeletVersion, err := utilversion.ParseSemantic(node.Status.NodeInfo.KubeletVersion); err != nil {\n\t\t\t\t\tglog.Errorf(\"unknown kubelet version %s for node %s, disabling ScheduleDaemonSetPods using MatchFields: %v\", node.Status.NodeInfo.KubeletVersion, nodesNeedingDaemonPods[ix], err)\n\t\t\t\t} else if kubeletVersion.LessThan(matchFieldVersion) {"
  },
  {
    "id" : "331c21aa-2c2d-4336-b66b-9aeb1894bd92",
    "prId" : 69566,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/69566#pullrequestreview-164068089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0aa9ce2-4d1f-44b1-84d6-1a7131985765",
        "parentId" : null,
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "I think, we should increase the log level(don't mind, if this V(2) or 1) for this message since we are obfuscating  from the end-user the fact that enabling `ScheduleDaemonSetPods` feature has no impact if kubelet version is less than 1.11 which may be bad user experience.",
        "createdAt" : "2018-10-12T00:50:29Z",
        "updatedAt" : "2018-10-16T02:31:38Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      },
      {
        "id" : "d592e8c7-06e0-42b2-ac05-b7c1fde385db",
        "parentId" : "f0aa9ce2-4d1f-44b1-84d6-1a7131985765",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "no, a 2-version skewed kubelet should not flood logs. disabling the MatchFields-based scheduling is working properly, it's not actionable and shouldn't matter to the administrator.",
        "createdAt" : "2018-10-12T00:55:14Z",
        "updatedAt" : "2018-10-16T02:31:38Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "1190c45a-91e4-4fc8-8987-d74b6f09cea1",
        "parentId" : "f0aa9ce2-4d1f-44b1-84d6-1a7131985765",
        "authorId" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "body" : "But what about in 1.10 cluster, where the admin explicitly enabled `ScheduleDaemonSetPods` and he/she notices that the cluster is not behaving as per its configuration? Unless, they increase log level, they won't notice it.\r\n\r\nEDIT: I just saw, this https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/controller/daemon/daemon_controller.go#L941, so we are explicitly disabling this feature in 1.10, dah.. Please ignore my earlier comment.",
        "createdAt" : "2018-10-12T01:03:03Z",
        "updatedAt" : "2018-10-16T02:31:38Z",
        "lastEditedBy" : "38ca4f80-c365-4775-8981-1e56b713b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "73d252d0064277f13df2b43b6cbbd1a0b15974f7",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1042,1046 @@\t\t\t\t\tglog.Errorf(\"unknown kubelet version %s for node %s, disabling ScheduleDaemonSetPods using MatchFields: %v\", node.Status.NodeInfo.KubeletVersion, nodesNeedingDaemonPods[ix], err)\n\t\t\t\t} else if kubeletVersion.LessThan(matchFieldVersion) {\n\t\t\t\t\tglog.V(4).Infof(\"kubelet version %s on node %s is less than %s, disabling ScheduleDaemonSetPods using MatchFields\", node.Status.NodeInfo.KubeletVersion, nodesNeedingDaemonPods[ix], matchFieldVersion)\n\t\t\t\t} else {\n\t\t\t\t\tnodeCanUseMatchFields = true"
  },
  {
    "id" : "229418bb-ad50-4f2a-9f3a-d99ca8fb3206",
    "prId" : 66953,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/66953#pullrequestreview-148254111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "934032cd-eb68-4fcb-97f2-689b5074f5b6",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "The comment needs to be updated. ",
        "createdAt" : "2018-08-21T21:14:39Z",
        "updatedAt" : "2018-08-21T21:14:39Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "deb6d854701c951bf028934eadffce4f0e4f028b",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1462,1466 @@\tvar predicateFails []algorithm.PredicateFailureReason\n\n\t// If ScheduleDaemonSetPods is enabled, only check nodeSelector and nodeAffinity.\n\tif utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {\n\t\tfit, reasons, err := checkNodeFitness(pod, nil, nodeInfo)"
  },
  {
    "id" : "f86ad853-3945-404c-9f39-9b46526e3dbe",
    "prId" : 65469,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65469#pullrequestreview-153569826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c793e7ad-d2f0-4ae3-a1db-bb8cd3589713",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "need to copy the podTemplate if you're going to change it, just like in the above branch, or you introduce races between the goroutines",
        "createdAt" : "2018-07-19T16:59:25Z",
        "updatedAt" : "2018-07-19T20:16:59Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "37fe98b8-c0c8-4783-b64d-d34a7fa9b649",
        "parentId" : "c793e7ad-d2f0-4ae3-a1db-bb8cd3589713",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "done",
        "createdAt" : "2018-09-09T02:52:24Z",
        "updatedAt" : "2018-09-09T02:52:24Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "786d48d6e804d87fab83ec8239f16ccbb531a77c",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\t\t\t\t} else {\n\t\t\t\t\t// If pod is scheduled by DaemonSetController, set its '.spec.scheduleName'.\n\t\t\t\t\tpodTemplate.Spec.SchedulerName = \"kubernetes.io/daemonset-controller\"\n\n\t\t\t\t\terr = dsc.podControl.CreatePodsOnNode(nodesNeedingDaemonPods[ix], ds.Namespace, podTemplate,"
  },
  {
    "id" : "5675a2ac-ccd7-4ad5-b1f9-65cf00268009",
    "prId" : 65309,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/65309#pullrequestreview-146250654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "388fb426-7640-4188-b885-615c5affffd7",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Do we need `failedPodsBackoff`? Why not just use `dsc.enqueueDaemonSetRateLimited(ds)`?",
        "createdAt" : "2018-08-14T20:42:42Z",
        "updatedAt" : "2018-08-15T14:04:01Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "c7471e12-3bf3-4e52-8555-57c9ff16f896",
        "parentId" : "388fb426-7640-4188-b885-615c5affffd7",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : " Yes, we need it. Rate-limited re-queuing does not prevent other events on the DaemonSet causing it to be queued and processed prior to that rate-limited re-add, which would trigger delete/recreate of all of the failed pods on the failed nodes.  We want the controller to remain responsive, without churning on permanently failing nodes",
        "createdAt" : "2018-08-14T21:02:20Z",
        "updatedAt" : "2018-08-15T14:04:01Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0b1629e8d6a95399ab945537ac95fd5d5ee4e35",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +901,905 @@\t\t\t\t}\n\n\t\t\t\tdsc.failedPodsBackoff.Next(backoffKey, now)\n\n\t\t\t\tmsg := fmt.Sprintf(\"Found failed daemon pod %s/%s on node %s, will try to kill it\", pod.Namespace, pod.Name, node.Name)"
  },
  {
    "id" : "c9ce0eea-d844-4459-91a3-0873b968f86b",
    "prId" : 64916,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/64916#pullrequestreview-127389053",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c1e39e0-062b-49d8-8c2b-f12ac45ac0e1",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Can you add a benchmark so we can measure the performance when we revert this change?",
        "createdAt" : "2018-06-08T19:37:41Z",
        "updatedAt" : "2018-06-08T19:37:41Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "4de6ddf7-abd5-4190-9f5e-9a59b41f7ad1",
        "parentId" : "8c1e39e0-062b-49d8-8c2b-f12ac45ac0e1",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "A benchmark of the func? After https://github.com/kubernetes/kubernetes/pull/64915 merged these two are basically the same.",
        "createdAt" : "2018-06-10T08:11:12Z",
        "updatedAt" : "2018-06-10T08:11:12Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "60ef68c87de14c10c6d469b8d801d23634fcb105",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1277,1281 @@\t\t// a daemonset should bind to a node.\n\t\t// TODO: replace this with metav1.IsControlledBy() in 1.12\n\t\tif isControlledByDaemonSet(pod, ds.GetUID()) {\n\t\t\tcontinue\n\t\t}"
  },
  {
    "id" : "263e864b-1140-4a8c-811f-bee528f2d8f6",
    "prId" : 63223,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63223#pullrequestreview-115981230",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a4b687b-638f-4295-90b8-757aec39d1de",
        "parentId" : null,
        "authorId" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "body" : "Here it seems to be changing the behavior if the feature is disabled by returning error if pod.Spec.NodeName is 0. ",
        "createdAt" : "2018-04-27T15:27:49Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "6252ac4b-6b9e-4dee-8931-ca3b934d52fc",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fd848e5ec31720f099feb4d5e918975a3a0eb99",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +775,779 @@\tnodeToDaemonPods := make(map[string][]*v1.Pod)\n\tfor _, pod := range claimedPods {\n\t\tnodeName, err := util.GetTargetNodeName(pod)\n\t\tif err != nil {\n\t\t\tglog.Warningf(\"Failed to get target node name of Pod %v/%v in DaemonSet %v/%v\","
  },
  {
    "id" : "61a38316-d931-4778-aa01-dae616e7d56a",
    "prId" : 63223,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63223#pullrequestreview-120027078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acd6ca4a-dbc5-4b6e-b012-d8463a068e95",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Why is it necessary to do this? Isn't nodeName filled once the pod gets scheduled?",
        "createdAt" : "2018-05-14T20:22:18Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "ce6c450a-79fb-44f2-9e9d-ecb0d5fe1c00",
        "parentId" : "acd6ca4a-dbc5-4b6e-b012-d8463a068e95",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "a daemonset pod may never get scheduled (if the target node lacks resources, or is tainted, or not ready, etc). even in cases where the pod eventually gets scheduled, between creation of the unscheduled pod and the binding to the node, the DS controller can easily create lots of extra pods targeted at the node if this doesn't accurately return the ones it already created",
        "createdAt" : "2018-05-14T20:27:57Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "00780737-810e-4e2d-b54e-79aaaa8b52d0",
        "parentId" : "acd6ca4a-dbc5-4b6e-b012-d8463a068e95",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Thanks, that makes sense. ",
        "createdAt" : "2018-05-14T22:04:35Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fd848e5ec31720f099feb4d5e918975a3a0eb99",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +775,779 @@\tnodeToDaemonPods := make(map[string][]*v1.Pod)\n\tfor _, pod := range claimedPods {\n\t\tnodeName, err := util.GetTargetNodeName(pod)\n\t\tif err != nil {\n\t\t\tglog.Warningf(\"Failed to get target node name of Pod %v/%v in DaemonSet %v/%v\","
  },
  {
    "id" : "73f0a4ef-b577-436f-8c89-c6e1c34b407a",
    "prId" : 63223,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63223#pullrequestreview-124162736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0863d19-0f22-44f7-a3a9-02d1b3e171b9",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Because `nodeToDaemonPods` includes pods that's not scheduled yet (no nodeName set), in `podsShouldBeOnNode`, the pods without nodeName set should be killed first when excess daemon pods are found.",
        "createdAt" : "2018-05-18T00:37:38Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "2bfb0667-931a-49ff-9600-0fb795d694ae",
        "parentId" : "a0863d19-0f22-44f7-a3a9-02d1b3e171b9",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "done, sort the pods by phase & creation timestamp.",
        "createdAt" : "2018-05-21T08:09:09Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "4b58f910-94fb-491c-9239-a5adbf580685",
        "parentId" : "a0863d19-0f22-44f7-a3a9-02d1b3e171b9",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Just revert it; the node without `nodeName` maybe the new one we want to create.",
        "createdAt" : "2018-05-28T01:41:52Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "5fc5918a-3d0d-43bd-896d-92d7941dbcba",
        "parentId" : "a0863d19-0f22-44f7-a3a9-02d1b3e171b9",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "> Just revert it; the node without `nodeName` maybe the new one we want to create.\r\n\r\nI'm not sure if I understand why this is reverted. If a DaemonSet pod is already scheduled on this node, shouldn't other pending pods be deleted first?",
        "createdAt" : "2018-05-29T23:10:25Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fd848e5ec31720f099feb4d5e918975a3a0eb99",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +784,788 @@\t\tnodeToDaemonPods[nodeName] = append(nodeToDaemonPods[nodeName], pod)\n\t}\n\n\treturn nodeToDaemonPods, nil\n}"
  },
  {
    "id" : "85d62026-9fa2-46e2-9afe-dc53f75114c5",
    "prId" : 63223,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63223#pullrequestreview-125318825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "We should enable this feature by default in 1.11.",
        "createdAt" : "2018-05-31T00:08:45Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "1fbd7586-6c02-4c92-b418-7a94b0c92514",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "We can't enable an alpha feature by default. ",
        "createdAt" : "2018-05-31T00:18:04Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "317cdf31-0e8a-4c00-a297-f8c617d2536d",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "This cannot remain disabled in 1.11. Rescheduler is already removed from the code-base. If critical daemonsets cannot be scheduled, preemption must create room for them and DS controller is incapable of performing preemption.",
        "createdAt" : "2018-05-31T16:39:09Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "56d669e7-04c0-48fd-91ea-ecfd84f687b0",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Actually when I thought about it again, I realized that my concern may not be valid. IIUC Rescheduler could not help with scheduling critical DS pods anyway, because DS controller did not create a DS pod before it found a node that could run the pod. So, Rescheduler was not even aware that such critical DS pods needed to be scheduled.\r\nIn other words, DS controller never relied on Rescheduler to create room for DS pods. So, the fact that Rescheduler does not exist in 1.11 won't change anything here.",
        "createdAt" : "2018-05-31T17:23:49Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "2562e26d-2098-40be-88b1-974202092b16",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "@bsalamat , here's the code about critical pod in daemonset: https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemon_controller.go#L1429 . ",
        "createdAt" : "2018-06-01T00:36:16Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "642ec95b-1051-4caa-8748-595b8a5dad18",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "Thanks, Klaus. So, my initial concern is valid. DS controller does not run \"resource check\" for critical pods. This means that it creates critical DS Pods regardless of the resources available on the nodes and it relies on \"Rescheduler\" to free up resources on the nodes if necessary. In the absence of Rescheduler, it is important to let default scheduler schedule DS Pods. Otherwise, critical DS pods may never be scheduled when their corresponding nodes are out of resources.",
        "createdAt" : "2018-06-01T01:05:19Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "508fde1d-5539-48c0-8615-d675bf8dd6e4",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Your concern is a good point :). I re-check the code that, critical pod (`ExperimentalCriticalPodAnnotation`) is still alpha feature (e2e was also passed in removed re-scheduler PR). \r\n\r\nLet me also check whether it is enabled specially in test-infra :). If not enabled, I think that's safe for us to remove it; and we need to update yaml files about critical pods if any.",
        "createdAt" : "2018-06-01T03:21:53Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "ffc652a3-7885-44a4-ac89-efc75f8d6843",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I arranged with @ravisantoshgudimetla to make Rescheduler aware of Pod priority and add it back to help create room for critical DS Pods. So, this PR can remain as is (no need to enable the feature in 1.11).",
        "createdAt" : "2018-06-01T20:59:54Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "178313bd-941b-40ca-8c90-f263137bb4c5",
        "parentId" : "b4874779-4be2-4c82-8484-147748e5bb08",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I have to add that it was @ravisantoshgudimetla's idea to add priority awareness and use Rescheduler in 1.11. It removes a blocker in moving priority and preemption to Beta.",
        "createdAt" : "2018-06-01T21:13:41Z",
        "updatedAt" : "2018-06-02T00:39:49Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fd848e5ec31720f099feb4d5e918975a3a0eb99",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +970,974 @@\t\t\t\tpodTemplate := &template\n\n\t\t\t\tif utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {\n\t\t\t\t\tpodTemplate = template.DeepCopy()\n\t\t\t\t\t// The pod's NodeAffinity will be updated to make sure the Pod is bound"
  },
  {
    "id" : "ed666f80-24a2-401f-8f30-89043c47173a",
    "prId" : 63223,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63223#pullrequestreview-125358380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc6d8f06-ed90-41cd-98be-044f9ae612e4",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "nit: scheduled pod is preserved first; if more than one pod can be preserved, the oldest pod is preserved. ",
        "createdAt" : "2018-06-02T07:01:00Z",
        "updatedAt" : "2018-06-02T07:01:00Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fd848e5ec31720f099feb4d5e918975a3a0eb99",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +856,860 @@\t\t}\n\t\t// If daemon pod is supposed to be running on node, but more than 1 daemon pod is running, delete the excess daemon pods.\n\t\t// Sort the daemon pods by creation time, so the oldest is preserved.\n\t\tif len(daemonPodsRunning) > 1 {\n\t\t\tsort.Sort(podByCreationTimestampAndPhase(daemonPodsRunning))"
  },
  {
    "id" : "ce189615-d831-4eba-ac7f-46f1b159738c",
    "prId" : 59883,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59883#pullrequestreview-99112960",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60553fb8-97a1-4edc-99d2-cf26c1945bdf",
        "parentId" : null,
        "authorId" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "body" : "if we are ignoring the error, could we at least log it?",
        "createdAt" : "2018-02-23T18:39:51Z",
        "updatedAt" : "2018-02-23T18:51:05Z",
        "lastEditedBy" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "tags" : [
        ]
      },
      {
        "id" : "8414d813-0a41-48bf-88d6-ca160b4a173a",
        "parentId" : "60553fb8-97a1-4edc-99d2-cf26c1945bdf",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "Janet and I discussed this, and we have a concern about useless noise in the logs",
        "createdAt" : "2018-02-23T19:56:35Z",
        "updatedAt" : "2018-02-23T19:56:35Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      },
      {
        "id" : "f884e178-d1fc-48c5-9789-5986a279954b",
        "parentId" : "60553fb8-97a1-4edc-99d2-cf26c1945bdf",
        "authorId" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "body" : "if the concern is noise, loglevels are the distinction",
        "createdAt" : "2018-02-24T13:46:39Z",
        "updatedAt" : "2018-02-24T13:46:39Z",
        "lastEditedBy" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e8ec4f9e953c4898005f1827ead07ae1806a572",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +917,921 @@\tgeneration, err := util.GetTemplateGeneration(ds)\n\tif err != nil {\n\t\tgeneration = nil\n\t}\n\ttemplate := util.CreatePodTemplate(ds.Spec.Template, generation, hash)"
  },
  {
    "id" : "23f40624-2461-4f46-bd51-9c79a89f0fd8",
    "prId" : 59883,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59883#pullrequestreview-99010088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37b58a58-dad6-498f-82fe-0e3476554d15",
        "parentId" : null,
        "authorId" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "body" : "log",
        "createdAt" : "2018-02-23T18:40:38Z",
        "updatedAt" : "2018-02-23T18:51:05Z",
        "lastEditedBy" : "443f9b92-20b0-45c2-a13f-20c6f64f89eb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e8ec4f9e953c4898005f1827ead07ae1806a572",
    "line" : 230,
    "diffHunk" : "@@ -1,1 +1070,1074 @@\t\t\t\t// The controller handles this via the hash.\n\t\t\t\tgeneration, err := util.GetTemplateGeneration(ds)\n\t\t\t\tif err != nil {\n\t\t\t\t\tgeneration = nil\n\t\t\t\t}"
  },
  {
    "id" : "8e08fc8c-b959-4cc8-a49a-6993b1979af1",
    "prId" : 59862,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/59862#pullrequestreview-102748188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "966a7bd4-8e8a-4700-9628-5f03fbc23220",
        "parentId" : null,
        "authorId" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "body" : "will this deal with existing daemonset pods during migration? or the daemonset have to be recreated/rescaled?",
        "createdAt" : "2018-03-09T12:22:24Z",
        "updatedAt" : "2018-03-09T12:22:24Z",
        "lastEditedBy" : "ec801d33-3a38-47a2-a267-f72db1de574b",
        "tags" : [
        ]
      },
      {
        "id" : "eb05b0ad-2665-403b-9a0d-9c47a868e52e",
        "parentId" : "966a7bd4-8e8a-4700-9628-5f03fbc23220",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "This only affects Daemon pod creation. Existing pods won't be affected. Existing DaemonSet might be affected when, for example, their pods are killed/dead or new nodes are added. ",
        "createdAt" : "2018-03-09T18:45:26Z",
        "updatedAt" : "2018-03-09T18:45:26Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5adb2bad4534b25c567220a21a242d2745e6cbf0",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +941,945 @@\t\t\t\tif utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {\n\t\t\t\t\tpodTemplate = template.DeepCopy()\n\t\t\t\t\tpodTemplate.Spec.Affinity = util.ReplaceDaemonSetPodHostnameNodeAffinity(\n\t\t\t\t\t\tpodTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix])\n"
  },
  {
    "id" : "e72c81dd-3c16-464f-b4bf-b16bb88858eb",
    "prId" : 50595,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/50595#pullrequestreview-58627135",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ed1c173-df5d-44d8-af3f-3574b43d66f8",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Would you add a unit test to make sure the right reasons are returned in DaemonSet predicates? Something similar to:\r\nhttps://github.com/kubernetes/kubernetes/blob/v1.8.0-alpha.2/plugin/pkg/scheduler/algorithm/predicates/predicates_test.go#L1951",
        "createdAt" : "2017-08-23T22:42:33Z",
        "updatedAt" : "2017-08-25T11:13:50Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "23d9646b-3c8a-45f5-b7e4-b207b445216e",
        "parentId" : "8ed1c173-df5d-44d8-af3f-3574b43d66f8",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "sure; that makes sense :).",
        "createdAt" : "2017-08-24T13:21:22Z",
        "updatedAt" : "2017-08-25T11:13:50Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      },
      {
        "id" : "ccd615cb-07cd-4b4f-a338-6169c0459bb1",
        "parentId" : "8ed1c173-df5d-44d8-af3f-3574b43d66f8",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Other parts of this PR look good ",
        "createdAt" : "2017-08-24T18:04:23Z",
        "updatedAt" : "2017-08-25T11:13:50Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "dc2b54f7-4576-4362-9164-0147b8016a5f",
        "parentId" : "8ed1c173-df5d-44d8-af3f-3574b43d66f8",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "Done; I only add one case for this PR; I'll open other PR to increase the coverage.",
        "createdAt" : "2017-08-25T11:15:08Z",
        "updatedAt" : "2017-08-25T11:15:08Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4c58a3c029476fb874976c163f5d768a7b67001",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1352,1356 @@\t\t//       e.g. MemoryPressure, and DiskPressure\n\t\tif c.Type == v1.NodeOutOfDisk && c.Status == v1.ConditionTrue {\n\t\t\treasons = append(reasons, predicates.ErrNodeOutOfDisk)\n\t\t\tbreak\n\t\t}"
  },
  {
    "id" : "8da53abf-9e35-476d-b032-cba30319063c",
    "prId" : 49488,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49488#pullrequestreview-54528055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b20983a-02e1-4746-9b70-ea46e7e31244",
        "parentId" : null,
        "authorId" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "body" : "when will there be an error? should we log it?",
        "createdAt" : "2017-08-05T04:36:46Z",
        "updatedAt" : "2017-08-11T07:48:22Z",
        "lastEditedBy" : "55c0e4a8-86f8-4426-a163-752ee421c57e",
        "tags" : [
        ]
      },
      {
        "id" : "85e4a65c-c912-4b33-8b3c-93710ae13b06",
        "parentId" : "5b20983a-02e1-4746-9b70-ea46e7e31244",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "just for safety, log err msg here.",
        "createdAt" : "2017-08-06T06:06:14Z",
        "updatedAt" : "2017-08-11T07:48:22Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa432e131c9b97d5c5357145cccfd3057710f8d5",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +560,564 @@\tdss := dsc.listSuspendedDaemonPods(node)\n\tfor _, dsKey := range dss {\n\t\tif ns, name, err := cache.SplitMetaNamespaceKey(dsKey); err != nil {\n\t\t\tglog.Errorf(\"Failed to get DaemonSet's namespace and name from %s: %v\", dsKey, err)\n\t\t\tcontinue"
  },
  {
    "id" : "152e3306-d2a0-4225-a106-3a6c682c58ca",
    "prId" : 49488,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/49488#pullrequestreview-55113606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "132017b0-c4b8-495a-ac48-2a8c9184818e",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Why only requeue on orphan pods deletions?",
        "createdAt" : "2017-08-08T09:44:58Z",
        "updatedAt" : "2017-08-11T07:48:22Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "9c461f8f-3a42-43d4-a746-6995a46de94b",
        "parentId" : "132017b0-c4b8-495a-ac48-2a8c9184818e",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Never mind. I see what you did below.",
        "createdAt" : "2017-08-08T09:59:28Z",
        "updatedAt" : "2017-08-11T07:48:22Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "2d65c8d3-83b1-4b2c-a6f4-36f307338a91",
        "parentId" : "132017b0-c4b8-495a-ac48-2a8c9184818e",
        "authorId" : "72156db3-c40b-4455-9838-c12c0c606019",
        "body" : "If any more comments, let me know :).",
        "createdAt" : "2017-08-09T01:14:42Z",
        "updatedAt" : "2017-08-11T07:48:22Z",
        "lastEditedBy" : "72156db3-c40b-4455-9838-c12c0c606019",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa432e131c9b97d5c5357145cccfd3057710f8d5",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +625,629 @@\t\tif len(pod.Spec.NodeName) != 0 {\n\t\t\t// If scheduled pods were deleted, requeue suspended daemon pods.\n\t\t\tdsc.requeueSuspendedDaemonPods(pod.Spec.NodeName)\n\t\t}\n\t\treturn"
  }
]