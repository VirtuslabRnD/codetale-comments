[
  {
    "id" : "78f3995a-c93a-43f8-bf36-01fcf4bdaf53",
    "prId" : 48189,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/48189#pullrequestreview-46859088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75fd448a-3e40-4d81-a368-8a5b850d5725",
        "parentId" : null,
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "How often do we retry in case of this Error?",
        "createdAt" : "2017-06-28T12:19:08Z",
        "updatedAt" : "2017-06-29T08:16:51Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "4a53d8d4-b930-4b51-b6b6-393f160c35f4",
        "parentId" : "75fd448a-3e40-4d81-a368-8a5b850d5725",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "It would be much clearer to determine that if the following return contained all the actual variables that it returns. It seems that err is nil at this point so unless the controller needs to retry for a different error down the rest of the code path, this won't be retried.",
        "createdAt" : "2017-06-28T12:26:10Z",
        "updatedAt" : "2017-06-29T08:16:51Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "0d8083ff-f4ea-4b88-9a90-3921bc2e2aab",
        "parentId" : "75fd448a-3e40-4d81-a368-8a5b850d5725",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "I think it's ok to not requeue on insufficientResourceErr as long as we are requeueing when something changes that would cause resources to be freed up. Regardless of whether something is doing that right now, this is not a change in behavior and not the bug I am trying to fix.",
        "createdAt" : "2017-06-28T14:37:24Z",
        "updatedAt" : "2017-06-29T08:16:51Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e6c2ea4d055b53f5cf1886af1c7e5a224d6afd1",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +1180,1184 @@\t// preventing the daemon pod from scheduling\n\tif shouldSchedule && insufficientResourceErr != nil {\n\t\tdsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, FailedPlacementReason, \"failed to place pod on %q: %s\", node.ObjectMeta.Name, insufficientResourceErr.Error())\n\t\tshouldSchedule = false\n\t}"
  },
  {
    "id" : "215ab911-55d2-4d15-ae1f-fd24726743bf",
    "prId" : 46577,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/46577#pullrequestreview-46804654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "906adedb-bb40-4b0d-9299-7dd5ed6d781a",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "This seems wrong. If a DaemonSet doesn't tolerate NoSchedule but the pod is already running, we need to return false, false, true from nodeShouldRunDaemonPod.",
        "createdAt" : "2017-06-28T08:45:11Z",
        "updatedAt" : "2017-06-28T08:45:11Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "97c49359-acd0-4aa0-94ee-74f4a4f47f43",
        "parentId" : "906adedb-bb40-4b0d-9299-7dd5ed6d781a",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Actually it looks like it has been wrong, even before this change.",
        "createdAt" : "2017-06-28T09:34:51Z",
        "updatedAt" : "2017-06-28T09:34:51Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "287730c9-e3a4-4827-b8d9-19b3cd450226",
        "parentId" : "906adedb-bb40-4b0d-9299-7dd5ed6d781a",
        "authorId" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "body" : "@mikedanese Is there an open issue for this comment?\r\nIs it a release-blocker to fix this issue?",
        "createdAt" : "2017-06-28T10:57:30Z",
        "updatedAt" : "2017-06-28T10:57:30Z",
        "lastEditedBy" : "bfe6ebf1-cfa7-4758-abb1-9960fa09b194",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b311fefba9cc106ed1a8cebc6c61e32814fd9e5",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1033,1037 @@\t\t\t\tpredicates.ErrPodNotFitsHostPorts,\n\t\t\t\t// DaemonSet is expected to respect taints and tolerations\n\t\t\t\tpredicates.ErrTaintsTolerationsNotMatch:\n\t\t\t\treturn true\n\t\t\t}"
  },
  {
    "id" : "bc9d222b-da4d-40e2-ba23-60c8e0aab4dd",
    "prId" : 45924,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45924#pullrequestreview-39535618",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1927c10a-d164-4b3c-8eef-8256399ead67",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Isn't ControllerRevision read-only?",
        "createdAt" : "2017-05-20T15:45:36Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "deb6a364-59a6-4235-aa5d-e8fe458b0bc7",
        "parentId" : "1927c10a-d164-4b3c-8eef-8256399ead67",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "From the proposal, only its `.data` cannot be updated. We need to at least be able to update its owner ref, otherwise we can't handle orphan / adoption / garbage collection. ",
        "createdAt" : "2017-05-22T17:14:11Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "85ec49c9bb655cb2f2a7d33598780db4f2500b0c",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +340,344 @@\n// updateHistory figures out what DaemonSet(s) manage a ControllerRevision when the ControllerRevision\n// is updated and wake them up. If the anything of the ControllerRevision have changed, we need to\n// awaken both the old and new DaemonSets.\nfunc (dsc *DaemonSetsController) updateHistory(old, cur interface{}) {"
  },
  {
    "id" : "0740cfc8-3dd9-4015-ac0a-ab9767356699",
    "prId" : 45924,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45924#pullrequestreview-41138924",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b45a7977-d14a-46d4-9b4f-be70297a053c",
        "parentId" : null,
        "authorId" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "body" : "The previous blocks in this function have good comments.  This new code would benefit from them too.",
        "createdAt" : "2017-05-26T06:38:03Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "020e031c-c298-4e7e-a533-9a04439c203c",
        "tags" : [
        ]
      },
      {
        "id" : "5df312a0-97b7-48be-9511-46520e235709",
        "parentId" : "b45a7977-d14a-46d4-9b4f-be70297a053c",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Add comment for below code block ",
        "createdAt" : "2017-05-31T06:02:40Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "85ec49c9bb655cb2f2a7d33598780db4f2500b0c",
    "line" : 248,
    "diffHunk" : "@@ -1,1 +769,773 @@\t\t}\n\t}\n\n\t// Find current history of the DaemonSet, and label new pods using the hash label value of the current history when creating them\n\tcur, _, err := dsc.constructHistory(ds)"
  },
  {
    "id" : "fe27f198-c0fa-4c96-8079-f147c82ed35c",
    "prId" : 45924,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45924#pullrequestreview-41034747",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7cd2321-aacc-47d0-8bc5-37baf5c600c0",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I think you want to add the requeue back, especially if the rollout depends in the existence of history (not sure yet if it does).",
        "createdAt" : "2017-05-26T14:45:32Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "4543ec8d-7088-4dc7-9c2a-c8629a55ad9f",
        "parentId" : "d7cd2321-aacc-47d0-8bc5-37baf5c600c0",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Although the rollout depends on the existence of history, we don't need to requeue on `addHistory`. \r\n\r\nIf the step for creating a new history failed during a rollout, the controller will error out and requeue the DaemonSet (rate limited); if that step succeeded, it'll continue the rollout (creating new pods). If any history is added, the existing DaemonSet won't be affected as long its template isn't changed (trigger a rollout). ",
        "createdAt" : "2017-05-30T18:27:35Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "85ec49c9bb655cb2f2a7d33598780db4f2500b0c",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +324,328 @@\t\t}\n\t\tglog.V(4).Infof(\"ControllerRevision %s added.\", history.Name)\n\t\treturn\n\t}\n"
  },
  {
    "id" : "e12c9952-1072-49b5-901f-d89e239a7e84",
    "prId" : 45924,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45924#pullrequestreview-41847462",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Has `rollingUpdate` finished the whole update at this point?",
        "createdAt" : "2017-06-02T13:02:20Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "90469b71-7be3-4011-ba16-fc8be833b830",
        "parentId" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Yes, `rollingUpdate` calls `syncNodes` which creates & deletes pods and waits for those operations to complete ",
        "createdAt" : "2017-06-02T17:24:41Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "b7826935-00a1-4aad-8ea4-28e324fcf61f",
        "parentId" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Does this mean that a stuck update will hold a worker forever?",
        "createdAt" : "2017-06-02T17:29:13Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "3b669b20-a297-4d5e-a733-7ef271ef9434",
        "parentId" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Create & delete pods won't wait forever. If any creation/deletion fails, it collects the error, and returns all errors. ",
        "createdAt" : "2017-06-02T17:40:37Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "a87b90e5-122a-477a-b7ff-deda9cfd45f2",
        "parentId" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Successful creates / deletes doesn't mean complete rollout so `rollingUpdate` hasn't actually finished the whole rollout at this point in some cases.",
        "createdAt" : "2017-06-02T17:47:36Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "74db57fd-4a91-43c4-8762-ffd0935b23fc",
        "parentId" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "I see. Is the concern about not killing old history when it has a pod that is terminating?",
        "createdAt" : "2017-06-02T18:07:18Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "70e8c12d-6c84-43b4-ad10-d61922e6d5ee",
        "parentId" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I don't see this as an issue for now but if we standarize on having cleanup being executed at the end of a rollout, we should fix this. As I said elsewhere, I am going to rollback the Deployment case now that we have collisionCount (remember that cleanup was executed at the end before and I think it's strictly correct to do so).",
        "createdAt" : "2017-06-02T18:13:20Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "e9ba20e6-296a-4faa-997b-b56021b3186a",
        "parentId" : "d5df2d91-fe68-4401-bee5-762ae1d09a3e",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "As an example, Jenkins is cleaning up old builds only after the new builds are complete.",
        "createdAt" : "2017-06-02T18:14:24Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "85ec49c9bb655cb2f2a7d33598780db4f2500b0c",
    "line" : 332,
    "diffHunk" : "@@ -1,1 +1009,1013 @@\t}\n\n\terr = dsc.cleanupHistory(ds)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to clean up revisions of DaemonSet: %v\", err)"
  },
  {
    "id" : "f9de14f8-ba48-478b-a302-b3c4ab84f75a",
    "prId" : 45924,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/45924#pullrequestreview-41840510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c10ab17-3082-43fe-8b99-c1d481bd0c76",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Don't you want to wait for the history cache to be synced?",
        "createdAt" : "2017-06-02T17:44:12Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "94f654c6-7719-497a-802e-194e9ec573cf",
        "parentId" : "1c10ab17-3082-43fe-8b99-c1d481bd0c76",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Yes, will add in `WaitForCacheSync`",
        "createdAt" : "2017-06-02T17:46:21Z",
        "updatedAt" : "2017-06-03T07:53:03Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "85ec49c9bb655cb2f2a7d33598780db4f2500b0c",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +167,171 @@\t})\n\tdsc.historyLister = historyInformer.Lister()\n\tdsc.historyStoreSynced = historyInformer.Informer().HasSynced\n\n\t// Watch for creation/deletion of pods. The reason we watch is that we don't want a daemon set to create/delete"
  },
  {
    "id" : "7178b8ff-5171-41fb-b9fb-a9638b7db7e2",
    "prId" : 42173,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42173#pullrequestreview-25647287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8b7b687-0860-4d68-bc8b-f80a669c1841",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "What if we deep copy these pods to prevent them from being modified? `nodeToDaemonPods` may be passed around and the one who's using it is prone to miss the fact that these pods shouldn't be modified directly. ",
        "createdAt" : "2017-03-07T18:38:47Z",
        "updatedAt" : "2017-03-08T00:42:35Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "c7ebd09e-0084-4c38-9a03-4460fadd56f2",
        "parentId" : "a8b7b687-0860-4d68-bc8b-f80a669c1841",
        "authorId" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "body" : "I think that's a separate issue since I'm not changing the semantics in this PR. Note that the previous line `daemonPod := &(*daemonPods[i])` was not actually doing a copy ([example](https://play.golang.org/p/wtLp9uxU8O)).\r\n\r\nCan we discuss this after landing ControllerRef? We should also think about the general pattern to use across controllers.",
        "createdAt" : "2017-03-07T19:25:43Z",
        "updatedAt" : "2017-03-08T00:42:35Z",
        "lastEditedBy" : "97dce74b-9a86-4bd2-812f-a7a70df47473",
        "tags" : [
        ]
      },
      {
        "id" : "8205861f-20d5-47df-849a-9055320273e0",
        "parentId" : "a8b7b687-0860-4d68-bc8b-f80a669c1841",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Yes we can address this later ",
        "createdAt" : "2017-03-07T21:43:50Z",
        "updatedAt" : "2017-03-08T00:42:35Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "eeb6e3f7-fd8b-425f-88d1-acd5c3ac97ff",
        "parentId" : "a8b7b687-0860-4d68-bc8b-f80a669c1841",
        "authorId" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "body" : "StatefulSet is now deep copying everything FWIW.",
        "createdAt" : "2017-03-07T22:17:28Z",
        "updatedAt" : "2017-03-08T00:42:35Z",
        "lastEditedBy" : "6935d5b2-c3de-4596-9c98-055bd55df973",
        "tags" : [
        ]
      }
    ],
    "commit" : "fac372d0905defdd32e25da1bdeb446dce15bf9c",
    "line" : 306,
    "diffHunk" : "@@ -1,1 +479,483 @@\tfor _, pod := range claimedPods {\n\t\tnodeName := pod.Spec.NodeName\n\t\tnodeToDaemonPods[nodeName] = append(nodeToDaemonPods[nodeName], pod)\n\t}\n\treturn nodeToDaemonPods, nil"
  },
  {
    "id" : "c84a1a58-0033-46c1-9271-cda9d795ca54",
    "prId" : 41896,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41896#pullrequestreview-23854481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16bb8ad1-a99a-44e7-9835-0d2c6fca6921",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "When does this return an error?",
        "createdAt" : "2017-02-23T21:03:04Z",
        "updatedAt" : "2017-02-23T21:03:04Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "480fad19-3590-44da-b014-f98d9c86a5f1",
        "parentId" : "16bb8ad1-a99a-44e7-9835-0d2c6fca6921",
        "authorId" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "body" : "Previously, tolerations are in annotation, this func will return err when json.Marshal() failed.\r\nI will send another PR to refactor it.",
        "createdAt" : "2017-02-24T03:03:42Z",
        "updatedAt" : "2017-02-24T03:03:42Z",
        "lastEditedBy" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "tags" : [
        ]
      },
      {
        "id" : "cfd2c0da-b29a-4085-9a2f-eaa40c74d2ab",
        "parentId" : "16bb8ad1-a99a-44e7-9835-0d2c6fca6921",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Refactoring have relatively low priority - fixing test, as @davidopp wrote has higher.",
        "createdAt" : "2017-02-24T09:49:06Z",
        "updatedAt" : "2017-02-24T09:49:06Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      },
      {
        "id" : "9baa8a03-0858-4225-92c7-5d34c1d6954b",
        "parentId" : "16bb8ad1-a99a-44e7-9835-0d2c6fca6921",
        "authorId" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "body" : "Sure.",
        "createdAt" : "2017-02-25T09:04:13Z",
        "updatedAt" : "2017-02-25T09:04:13Z",
        "lastEditedBy" : "70ba63ce-18c5-43f6-a9fb-1acd33329390",
        "tags" : [
        ]
      }
    ],
    "commit" : "53090e98673ed1481f09b07404027d5824337a69",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +771,775 @@\t// to survive taint-based eviction enforced by NodeController\n\t// when node turns unreachable.\n\t_, err = v1.AddOrUpdateTolerationInPod(newPod, &v1.Toleration{\n\t\tKey:      metav1.TaintNodeUnreachable,\n\t\tOperator: v1.TolerationOpExists,"
  },
  {
    "id" : "d78f1ce1-254a-4484-9c0c-c7f6b56cb456",
    "prId" : 41896,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41896#pullrequestreview-23738300",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e586a3c0-5ae4-44c9-b04e-3ff2a1d6c0d3",
        "parentId" : null,
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "What If I want my daemon pods to be deleted? How can I overwrite this behavior?",
        "createdAt" : "2017-02-24T14:34:42Z",
        "updatedAt" : "2017-02-24T14:34:42Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      },
      {
        "id" : "20f421ec-4b53-496e-9e70-91d897b685d2",
        "parentId" : "e586a3c0-5ae4-44c9-b04e-3ff2a1d6c0d3",
        "authorId" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "body" : "Well - you can't do this now, so this is not a breaking change. We can figure a way for you to do it later.",
        "createdAt" : "2017-02-24T15:28:20Z",
        "updatedAt" : "2017-02-24T15:28:20Z",
        "lastEditedBy" : "3c437914-616b-4cfb-88a0-28dc812ff2b2",
        "tags" : [
        ]
      }
    ],
    "commit" : "53090e98673ed1481f09b07404027d5824337a69",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +758,762 @@\t// to survive taint-based eviction enforced by NodeController\n\t// when node turns not ready.\n\t_, err = v1.AddOrUpdateTolerationInPod(newPod, &v1.Toleration{\n\t\tKey:      metav1.TaintNodeNotReady,\n\t\tOperator: v1.TolerationOpExists,"
  },
  {
    "id" : "1d26dec9-ec76-4d9c-b3c5-7b64d1468384",
    "prId" : 41116,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41116#pullrequestreview-20801700",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e350e4e-940b-47d4-93d6-ec6d940c820e",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Already called above. Although expectations never worked as expected and we should probably get rid of them from this controller too.",
        "createdAt" : "2017-02-08T12:41:56Z",
        "updatedAt" : "2017-02-27T08:18:13Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "bfac132e-bc39-4470-af66-c89429f44cd7",
        "parentId" : "8e350e4e-940b-47d4-93d6-ec6d940c820e",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "`manage` can set new expectations. I want to run updates only if `manage` didn't change anything.\r\n",
        "createdAt" : "2017-02-08T16:24:11Z",
        "updatedAt" : "2017-02-27T08:18:13Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d9c817db0b1dacf94db67585ad30ad36d856016",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +756,760 @@\t}\n\n\tdsNeedsSync = dsc.expectations.SatisfiedExpectations(dsKey)\n\tif dsNeedsSync && ds.DeletionTimestamp == nil {\n\t\tswitch ds.Spec.UpdateStrategy.Type {"
  },
  {
    "id" : "484ec64e-fb7f-4b1e-a0ee-e073203b2418",
    "prId" : 41116,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41116#pullrequestreview-21151817",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10dd611a-0d8f-4bf8-86a0-27a6a4ec73a3",
        "parentId" : null,
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "It's not part of your change, but we don't need to get `dsKey` from `KeyFunc`, right? We could just use the input `key`",
        "createdAt" : "2017-02-10T00:26:34Z",
        "updatedAt" : "2017-02-27T08:18:13Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d9c817db0b1dacf94db67585ad30ad36d856016",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +756,760 @@\t}\n\n\tdsNeedsSync = dsc.expectations.SatisfiedExpectations(dsKey)\n\tif dsNeedsSync && ds.DeletionTimestamp == nil {\n\t\tswitch ds.Spec.UpdateStrategy.Type {"
  },
  {
    "id" : "1cdbffe0-5a1f-488c-be36-8b28276d113f",
    "prId" : 41116,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41116#pullrequestreview-23826229",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67610e26-fe2b-4722-854f-114b912d0eb5",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "We are going to handle overlapping daemonsets once we make them user owner references.",
        "createdAt" : "2017-02-24T22:26:13Z",
        "updatedAt" : "2017-02-27T08:18:13Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d9c817db0b1dacf94db67585ad30ad36d856016",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +264,268 @@\t}\n\n\t// TODO: Handle overlapping controllers better. See comment in ReplicationManager.\n\tdsc.queue.AddAfter(key, after)\n}"
  },
  {
    "id" : "acd57376-6375-4dcd-b0f2-e31cbc2a0425",
    "prId" : 41116,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/41116#pullrequestreview-23827401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84a194f9-03ad-4541-b350-54a44b811bd1",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "seems to work per ds and not per node",
        "createdAt" : "2017-02-24T22:28:18Z",
        "updatedAt" : "2017-02-27T08:18:13Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "986e45bf-7971-40ab-9f15-44b6c1655914",
        "parentId" : "84a194f9-03ad-4541-b350-54a44b811bd1",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "ah, it should be `on the given nodes`. I will fix it after merge :)",
        "createdAt" : "2017-02-24T22:33:19Z",
        "updatedAt" : "2017-02-27T08:18:13Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d9c817db0b1dacf94db67585ad30ad36d856016",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +550,554 @@}\n\n// syncNodes deletes given pods and creates new daemon set pods on the given node\n// returns slice with erros if any\nfunc (dsc *DaemonSetsController) syncNodes(ds *extensions.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string) []error {"
  },
  {
    "id" : "caf703a7-5b90-40d9-a2f7-46fd8f24c6fe",
    "prId" : 40330,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/40330#pullrequestreview-18469379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0413ead0-ba1a-4642-8cc8-baa018137062",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "I am not sure I understand this - why do you need to return the error here? Won't the daemon set be resynced because of the deleted pod event anyway?",
        "createdAt" : "2017-01-25T08:48:40Z",
        "updatedAt" : "2017-01-25T18:31:25Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "4c532023-d299-4cc3-90df-4b056c9bb8f8",
        "parentId" : "0413ead0-ba1a-4642-8cc8-baa018137062",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Ah you want to use the ratelimiter - ok. Although for perma-failed daemon sets we probably want to stop retrying them after a while. ",
        "createdAt" : "2017-01-25T08:53:46Z",
        "updatedAt" : "2017-01-25T18:31:25Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "8a1922c5-6052-45e2-811e-b309a9121bf5",
        "parentId" : "0413ead0-ba1a-4642-8cc8-baa018137062",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "We don't support perma-failed daemon sets yet. Normally DS controller would check if the daemon pod can be scheduled on the node before creating it, so it's unlikely it'll create pods that are doomed to fail. However, sometimes there could be a race condition that kubelet uses its own node object to admit pods, and then rejected the pods (pods become `Failed`). \r\n\r\nLet's deal with this in a follow up PR?",
        "createdAt" : "2017-01-25T18:19:24Z",
        "updatedAt" : "2017-01-25T18:31:25Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      },
      {
        "id" : "b3452303-0b51-41ec-ae3c-74289adeb7bf",
        "parentId" : "0413ead0-ba1a-4642-8cc8-baa018137062",
        "authorId" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "body" : "Moved the comment to before `if` statement to make it more clear ",
        "createdAt" : "2017-01-25T18:32:42Z",
        "updatedAt" : "2017-01-25T18:34:41Z",
        "lastEditedBy" : "01c14569-b640-48af-98cc-aa9dd12da7b6",
        "tags" : [
        ]
      }
    ],
    "commit" : "81c1e0c6ac2a0ae593774f6a0cbfdc0eccbf9e24",
    "line" : null,
    "diffHunk" : "@@ -1,1 +565,569 @@\t}\n\t// Throw an error when the daemon pods fail, to use ratelimiter to prevent kill-recreate hot loop\n\tif failedPodsObserved > 0 {\n\t\terrors = append(errors, fmt.Errorf(\"deleted %d failed pods of DaemonSet %s/%s\", failedPodsObserved, ds.Namespace, ds.Name))\n\t}"
  },
  {
    "id" : "2a43c45d-674e-4648-96a2-af8e61c6f4e2",
    "prId" : 39157,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/39157#pullrequestreview-14272897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e06675b-7d16-434f-8287-ca6700db762f",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "You also need to compare this above in L544\r\n```\r\nds.Status.ObservedGeneration >= ds.Generation\r\n```",
        "createdAt" : "2016-12-22T15:31:32Z",
        "updatedAt" : "2017-01-02T14:36:07Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "e344de4f-cb95-4280-a08a-41fd71f1798a",
        "parentId" : "3e06675b-7d16-434f-8287-ca6700db762f",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "You mean to run update only when ds.Status.ObservedGeneration is smaller than ds.Generation?\r\nSomething like:\r\n\r\n```go\r\nif ds.Status.ObservedGeneration >= ds.Generation {\r\n  return nil\r\n}",
        "createdAt" : "2016-12-22T15:43:45Z",
        "updatedAt" : "2017-01-02T14:36:07Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      },
      {
        "id" : "8b7485df-6d00-4b51-b276-6c322ecfae3b",
        "parentId" : "3e06675b-7d16-434f-8287-ca6700db762f",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Should be part of the rest `if` clause",
        "createdAt" : "2016-12-22T15:53:47Z",
        "updatedAt" : "2017-01-02T14:36:07Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "72374b9b-cf08-44c8-858c-5b1b2ca08de8",
        "parentId" : "3e06675b-7d16-434f-8287-ca6700db762f",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "done",
        "createdAt" : "2016-12-23T08:22:04Z",
        "updatedAt" : "2017-01-02T14:36:07Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "a36a9787811983c2f4081fbc621a9a49e00de21b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +555,559 @@\tvar updateErr, getErr error\n\tfor i := 0; i < StatusUpdateRetries; i++ {\n\t\ttoUpdate.Status.ObservedGeneration = ds.Generation\n\t\ttoUpdate.Status.DesiredNumberScheduled = int32(desiredNumberScheduled)\n\t\ttoUpdate.Status.CurrentNumberScheduled = int32(currentNumberScheduled)"
  },
  {
    "id" : "b7bfddc5-e6f4-4b32-9d38-896d9c9c02c2",
    "prId" : 38787,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38787#pullrequestreview-13254885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a00c0f8d-310d-46ac-9ed3-1ea205f88d83",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "Can I ask you to do a table test for this method that shows the outcomes?",
        "createdAt" : "2016-12-16T00:40:30Z",
        "updatedAt" : "2017-01-11T21:37:59Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "4a42ecfb-be58-4cdb-8e2d-5108c8bb08f4",
        "parentId" : "a00c0f8d-310d-46ac-9ed3-1ea205f88d83",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Will do.",
        "createdAt" : "2016-12-16T01:03:23Z",
        "updatedAt" : "2017-01-11T21:37:59Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0f4bd41e5e19c00403e3076b1fb622a35a525d",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +687,691 @@//     Returns true when a daemonset should continue running on a node if a daemonset pod is already\n//     running on that node.\nfunc (dsc *DaemonSetsController) nodeShouldRunDaemonPod(node *v1.Node, ds *extensions.DaemonSet) (wantToRun, shouldSchedule, shouldContinueRunning bool, err error) {\n\t// Because these bools require an && of all their required conditions, we start\n\t// with all bools set to true and set a bool to false if a condition is not met."
  },
  {
    "id" : "41ce92cb-9c8d-4876-be90-cd6ecea67f52",
    "prId" : 38787,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38787#pullrequestreview-13303238",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f53a1fc2-ab1a-4355-af21-086980cd69c0",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Can you make it explicit that the other two booleans are true here?",
        "createdAt" : "2016-12-16T10:00:21Z",
        "updatedAt" : "2017-01-11T21:37:59Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0f4bd41e5e19c00403e3076b1fb622a35a525d",
    "line" : null,
    "diffHunk" : "@@ -1,1 +765,769 @@\t\t\t\t// absolutely true at the time of writing the comment. See first comment\n\t\t\t\t// of this method.\n\t\t\t\tshouldSchedule = false\n\t\t\t\temitEvent = true\n\t\t\t// unexpected"
  },
  {
    "id" : "73cc00e5-c279-4e00-aaac-3654833a37f8",
    "prId" : 38787,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38787#pullrequestreview-16247671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "593aced6-99eb-497c-9997-83fdfb4073d9",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Is node affinity something that doen't hold true for DaemonSets and works only for pods that are processed by the scheduler?",
        "createdAt" : "2017-01-11T20:20:23Z",
        "updatedAt" : "2017-01-11T21:37:59Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "9446bacf-e2e4-4b27-9aab-e7205ee85650",
        "parentId" : "593aced6-99eb-497c-9997-83fdfb4073d9",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "@kargakis node affinity bubbles up here as a predicates.ErrNodeSelectorNotMatch. The DaemonSet obeys \"required during scheduling\" node affinities.",
        "createdAt" : "2017-01-11T21:05:50Z",
        "updatedAt" : "2017-01-11T21:37:59Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "fbf3db37-24a1-49b6-9aff-4c9206409f4e",
        "parentId" : "593aced6-99eb-497c-9997-83fdfb4073d9",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Yes, actually nevermind my question. Required/ignored during execution is yet to be implemented.",
        "createdAt" : "2017-01-11T21:12:14Z",
        "updatedAt" : "2017-01-11T21:37:59Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "f21310a2-6e33-4311-957e-58451b7dffcf",
        "parentId" : "593aced6-99eb-497c-9997-83fdfb4073d9",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Basically \"required during execution\" only is not implemented yet. Here you have implemented \"required during execution\" for daemon sets. My real question from the begining was \"Is node affinity going to affect daemon sets? Will I be able to specify predicates in the pod template of a daemon set?\"",
        "createdAt" : "2017-01-11T22:01:54Z",
        "updatedAt" : "2017-01-11T22:01:54Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "2cba1d49-51f6-47f9-b5bd-01acf4d0e833",
        "parentId" : "593aced6-99eb-497c-9997-83fdfb4073d9",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Yes, the plan is to support node affinity in daemonsets. We will need to modify this method when \"required during execution\" is implemented.",
        "createdAt" : "2017-01-11T22:07:49Z",
        "updatedAt" : "2017-01-11T22:07:49Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "df0f4bd41e5e19c00403e3076b1fb622a35a525d",
    "line" : 155,
    "diffHunk" : "@@ -1,1 +754,758 @@\t\t\t\t// pod hard anti affinity.\n\t\t\t\tpredicates.ErrPodNotFitsHostPorts:\n\t\t\t\twantToRun, shouldSchedule, shouldContinueRunning = false, false, false\n\t\t\t// unintentional\n\t\t\tcase"
  },
  {
    "id" : "d44f5090-cf5c-4ae6-8cef-96fdf61e73da",
    "prId" : 38780,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/38780#pullrequestreview-13301372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "943410f6-8ff2-4ed9-9602-08ffa64f28c5",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Don't we want to enqueue all daemon sets that can run on this node?",
        "createdAt" : "2016-12-16T09:48:29Z",
        "updatedAt" : "2016-12-16T09:48:29Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a311a2bc2b5be904ca1f13bd3e28152b47c74ec",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +386,390 @@\t\tif shouldEnqueue {\n\t\t\tdsc.enqueueDaemonSet(ds)\n\t\t\treturn\n\t\t}\n\t}"
  },
  {
    "id" : "cf31cb1b-54d0-443b-a20c-db2715c90cdb",
    "prId" : 32781,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/32781#pullrequestreview-3756763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d41a0846-a5da-4367-93f2-46de150a60b3",
        "parentId" : null,
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Do we need to sort pods here?\n",
        "createdAt" : "2016-10-11T09:56:37Z",
        "updatedAt" : "2016-10-12T13:59:48Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "0c25084d-d8f7-41a1-a876-c5e73b6801fa",
        "parentId" : "d41a0846-a5da-4367-93f2-46de150a60b3",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "I'm using the same approach as here https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemoncontroller.go#L458-L460\nIf `dsc.getNodesToDaemonPods(ds)` returns more than one pod on node I want to get status of the youngest pod. \n",
        "createdAt" : "2016-10-11T16:29:57Z",
        "updatedAt" : "2016-10-12T13:59:48Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      },
      {
        "id" : "f521b712-2cc8-44d4-9bda-d292dee5c615",
        "parentId" : "d41a0846-a5da-4367-93f2-46de150a60b3",
        "authorId" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "body" : "Oh I see. By the way, you mean the oldest.\n",
        "createdAt" : "2016-10-11T17:24:01Z",
        "updatedAt" : "2016-10-12T13:59:48Z",
        "lastEditedBy" : "11efe503-096f-46dd-a8c8-28ba38a0157a",
        "tags" : [
        ]
      },
      {
        "id" : "d0a66fae-a4ae-4d08-bccd-33d579c01ba7",
        "parentId" : "d41a0846-a5da-4367-93f2-46de150a60b3",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "Ah yes, the oldest. Sorry.\n",
        "createdAt" : "2016-10-11T20:21:45Z",
        "updatedAt" : "2016-10-12T13:59:48Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ebe1f666a384be9529398ff93234348a646aef7",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +585,589 @@\t\t\t\t// Sort the daemon pods by creation time, so the the oldest is first.\n\t\t\t\tdaemonPods, _ := nodeToDaemonPods[node.Name]\n\t\t\t\tsort.Sort(podByCreationTimestamp(daemonPods))\n\t\t\t\tif api.IsPodReady(daemonPods[0]) {\n\t\t\t\t\tnumberReady++"
  },
  {
    "id" : "1232ecc6-08d3-4bed-a0e4-ce90f66f2333",
    "prId" : 32210,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94a058ab-f98d-4dc9-be38-ac531caf7ef1",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "Do we really need those variables in `dsc`?\n",
        "createdAt" : "2016-09-08T12:16:28Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      },
      {
        "id" : "8811493f-18af-4813-a33c-19cd45ba7a9c",
        "parentId" : "94a058ab-f98d-4dc9-be38-ac531caf7ef1",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> Do we really need those variables in dsc?\n\nThey're using so that they can mock the startup.  At some point I think we could refactor the test run via a fake client with a fake watch, but its non-trivial to do so.\n",
        "createdAt" : "2016-09-08T12:19:51Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "518f395d-ce99-4891-9a6e-a5a523d18d3d",
        "parentId" : "94a058ab-f98d-4dc9-be38-ac531caf7ef1",
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "got it.\n",
        "createdAt" : "2016-09-08T12:26:43Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "385831825bf1d2b2546988965a2f36b0cfd49e1f",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +200,204 @@\t\t},\n\t)\n\tdsc.nodeStoreSynced = dsc.nodeController.HasSynced\n\n\tdsc.syncHandler = dsc.syncDaemonSet"
  },
  {
    "id" : "8fc08f84-7208-42d1-bdea-aa0ecfe0c078",
    "prId" : 32210,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9330aeea-4f07-4b4d-9cf9-7cd29b5afee3",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "It's a queue of daemonset keys now.\n",
        "createdAt" : "2016-09-08T12:22:21Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "385831825bf1d2b2546988965a2f36b0cfd49e1f",
    "line" : null,
    "diffHunk" : "@@ -1,1 +105,109 @@\n\t// DaemonSet keys that need to be synced.\n\tqueue workqueue.RateLimitingInterface\n}\n"
  },
  {
    "id" : "66e387ee-71b3-431e-95b5-cdee50654532",
    "prId" : 32210,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84847cf1-7c62-428a-966f-bf321c58031c",
        "parentId" : null,
        "authorId" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "body" : "easier:\n\n``` go\nclose(errCh)\nfor err := range errCh {\n...\n}\n```\n",
        "createdAt" : "2016-09-08T12:25:37Z",
        "updatedAt" : "2016-09-12T15:01:49Z",
        "lastEditedBy" : "f0985d19-4073-49b4-832a-0b89b15a1431",
        "tags" : [
        ]
      }
    ],
    "commit" : "385831825bf1d2b2546988965a2f36b0cfd49e1f",
    "line" : 179,
    "diffHunk" : "@@ -1,1 +579,583 @@\n\t// collect errors if any for proper reporting/retry logic in the controller\n\terrors := []error{}\n\tclose(errCh)\n\tfor err := range errCh {"
  },
  {
    "id" : "62a0dbdf-02f7-4cd5-a43a-cc1449a6f607",
    "prId" : 31020,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acfc5d94-ee3d-41f9-9a53-88bc586c4b7f",
        "parentId" : null,
        "authorId" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "body" : "In GeneralPredicates we accumulate all the failure reasons, so if we want to do the same thing here I guess we'd drop this if statement and continue with checking PodToleratesNodeTaints regardless of whether fit is already false. But since this is only going into a log message I think it's not worth doing it that way (if it were going into events the user could see, then having the full set of failure reasons could be useful).\n",
        "createdAt" : "2016-08-20T05:35:30Z",
        "updatedAt" : "2016-08-20T05:35:30Z",
        "lastEditedBy" : "82da2b23-9f40-4abd-8af5-56ba07c1fc0a",
        "tags" : [
        ]
      }
    ],
    "commit" : "00f05b441e4d1241cadc5aeb2008a316f601eafc",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +715,719 @@\t}\n\tif !fit {\n\t\treturn false\n\t}\n\tfit, reasons, err = predicates.PodToleratesNodeTaints(newPod, predicates.PredicateMetadata(newPod, nil), nodeInfo)"
  },
  {
    "id" : "47e57c74-0450-450a-8334-ed068e711ef2",
    "prId" : 28803,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79df4ec5-31f0-4544-975a-38ba4bdf295f",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "Same here.\n",
        "createdAt" : "2016-07-12T22:13:10Z",
        "updatedAt" : "2016-07-13T12:51:50Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "71c76f8e-e360-4545-9375-0f4bad4be189",
        "parentId" : "79df4ec5-31f0-4544-975a-38ba4bdf295f",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "Why? You added line 46, I'm using the same package.\n",
        "createdAt" : "2016-07-13T12:10:36Z",
        "updatedAt" : "2016-07-13T13:06:20Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      },
      {
        "id" : "2808f279-4b77-43a9-b40b-5e7e0d64eb41",
        "parentId" : "79df4ec5-31f0-4544-975a-38ba4bdf295f",
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "I think we could organize our packages a bit better, that's all. I know this isn't introduced here.\n",
        "createdAt" : "2016-07-13T14:56:41Z",
        "updatedAt" : "2016-07-13T14:56:41Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      },
      {
        "id" : "225970f7-55bc-47c0-9675-31a8d7712ec0",
        "parentId" : "79df4ec5-31f0-4544-975a-38ba4bdf295f",
        "authorId" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "body" : "ah, ok :)\n",
        "createdAt" : "2016-07-13T15:00:31Z",
        "updatedAt" : "2016-07-13T15:00:31Z",
        "lastEditedBy" : "63ae7701-0f8c-4ae2-9295-07a4434026ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "528bf7af3a454e5210a72ac8e021bb77927a7f8d",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +45,49 @@\t\"k8s.io/kubernetes/pkg/watch\"\n\t\"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/predicates\"\n\t\"k8s.io/kubernetes/plugin/pkg/scheduler/schedulercache\"\n)\n"
  }
]