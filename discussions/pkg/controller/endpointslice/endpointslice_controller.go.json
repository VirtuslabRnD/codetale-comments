[
  {
    "id" : "74feb746-31ce-40e7-a8d3-19103c236b10",
    "prId" : 99997,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99997#pullrequestreview-659583126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94116852-4238-45ea-9554-9b67bcc0312b",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "I see you using the new one, but not removing the old one?",
        "createdAt" : "2021-05-14T04:32:02Z",
        "updatedAt" : "2021-05-14T04:34:11Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "859ba188-4d91-4454-9276-49dc67a329e9",
        "parentId" : "94116852-4238-45ea-9554-9b67bcc0312b",
        "authorId" : "5c4757f6-5a7a-438b-b77f-778928cb6979",
        "body" : "@thockin  \r\nI change old function name for public scope since we have moved the file `endpointslice_tracker.go` to new place. \r\nAnd I modfiy all the `newEndpointSliceTracker` function calling place\r\n",
        "createdAt" : "2021-05-14T07:17:46Z",
        "updatedAt" : "2021-05-14T07:17:47Z",
        "lastEditedBy" : "5c4757f6-5a7a-438b-b77f-778928cb6979",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c63ef147cc1743c1bc40ac28cd938d868daa356",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +143,147 @@\tc.endpointSliceLister = endpointSliceInformer.Lister()\n\tc.endpointSlicesSynced = endpointSliceInformer.Informer().HasSynced\n\tc.endpointSliceTracker = endpointsliceutil.NewEndpointSliceTracker()\n\n\tc.maxEndpointsPerSlice = maxEndpointsPerSlice"
  },
  {
    "id" : "063d81d5-746d-4603-b231-e6df74bc7ea9",
    "prId" : 99522,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99522#pullrequestreview-603302838",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0adfb24f-0a77-4d65-bd9e-fc3a2c2e9bc9",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "So any change to any node causes a full list and rebuild of the cache?  That's going to be fireworks at large node counts, no?",
        "createdAt" : "2021-03-03T06:42:04Z",
        "updatedAt" : "2021-03-08T23:38:14Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "c30aa537-f157-42bd-b119-7e2a6cb46ec5",
        "parentId" : "0adfb24f-0a77-4d65-bd9e-fc3a2c2e9bc9",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "Yep, on my list to try to optimize. Also asked @wojtek-t about if some kind of queue/batching would help here, but it doesn't sound like that would make a huge difference here, will look into other ways to optimize this.",
        "createdAt" : "2021-03-03T08:06:51Z",
        "updatedAt" : "2021-03-08T23:38:14Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      },
      {
        "id" : "629aef83-a621-40f0-a42f-fb73a29d79b7",
        "parentId" : "0adfb24f-0a77-4d65-bd9e-fc3a2c2e9bc9",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Didn't look in the whole PR, but:\r\n- I mentioned offline to Rob, that we don't want to call that on every update, but only on those that could change something (I guess only Read <-> NotReady transitions?)\r\n- this listing is happening from local cache. What is more this isn't even copying objects:\r\nhttps://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/listers/core/v1/node.go#L51\r\n So that one is pretty cheap actually. Rebuilding the cache will be much more expensive",
        "createdAt" : "2021-03-03T08:18:21Z",
        "updatedAt" : "2021-03-08T23:38:14Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "4d7b77d8-a2a6-452c-acf5-1b4e5ef2e9c6",
        "parentId" : "0adfb24f-0a77-4d65-bd9e-fc3a2c2e9bc9",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "Thanks for the extra details @wojtek-t! Does this mean the code is already good enough as is or are there any optimizations I should make?",
        "createdAt" : "2021-03-03T20:11:02Z",
        "updatedAt" : "2021-03-08T23:38:14Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f07be06a19918910004b024af084123ae88d88cd",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +536,540 @@\t\treturn\n\t}\n\tnodes, err := c.nodeLister.List(labels.Everything())\n\tif err != nil {\n\t\tklog.Errorf(\"Error listing Nodes: %v\", err)"
  },
  {
    "id" : "e4b23fd6-b6c9-4ef8-b335-7ec1a0b3f69e",
    "prId" : 99345,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99345#pullrequestreview-598073939",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cbbbeeb-7a19-4599-930f-65a7874d72e2",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "return here?",
        "createdAt" : "2021-02-24T13:52:34Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "c1d238b8-e675-4b38-8005-f0a67b7e5f37",
        "parentId" : "0cbbbeeb-7a19-4599-930f-65a7874d72e2",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "Fixed, thanks!",
        "createdAt" : "2021-02-25T01:10:59Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1542606c275129ebcb9ffdb67696f8cc9b638d6",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +424,428 @@\t\tklog.Warningf(\"%s label changed from %s  to %s for %s\", discovery.LabelServiceName, prevSvcName, svcName, endpointSlice.Name)\n\t\tc.queueServiceForEndpointSlice(endpointSlice)\n\t\tc.queueServiceForEndpointSlice(prevEndpointSlice)\n\t\treturn\n\t}"
  },
  {
    "id" : "f9eb91d9-a9cc-4328-aa54-aca5fa5e3f4f",
    "prId" : 99345,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99345#pullrequestreview-598664493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cc26a8c-3bc3-4f47-8004-1f91276be8b0",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Hmm.. I'm wondering what happens in case of bug (e.g. we're deleting EndpointSlice but marking tracker incorrectly).\r\nWouldn't we go into infinite loop here?\r\nShouldn't we delete the entry from tracker anyway *and* requeue?\r\n\r\n[I guess I didn't have time to think deep enough to catch something subtle, but in such case it means it probably requires a more extensive comment.]",
        "createdAt" : "2021-02-24T14:00:12Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "ff044e7a-9b16-468b-be56-dfaa217236de",
        "parentId" : "5cc26a8c-3bc3-4f47-8004-1f91276be8b0",
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "that is why I'm afraid of not checking the number of requeues here https://github.com/kubernetes/kubernetes/pull/99345/files#r581095710 , I was thinking in possible loops but I couldn't find one, but :shrug: ",
        "createdAt" : "2021-02-24T14:34:56Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "aecc1c4c-b9fa-45fd-b07f-5530806eacd6",
        "parentId" : "5cc26a8c-3bc3-4f47-8004-1f91276be8b0",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "> Shouldn't we delete the entry from tracker anyway and requeue?\r\n\r\nYes good call, I've updated the logic in `Delete` to do that.\r\n\r\n> that is why I'm afraid of not checking the number of requeues here\r\n\r\nGood point, removed special treatment for this error so num requeues will now be checked.\r\n\r\n>Hmm.. I'm wondering what happens in case of bug (e.g. we're deleting EndpointSlice but marking tracker incorrectly).\r\n>Wouldn't we go into infinite loop here?\r\n\r\nI think the big risk here is if we called `ExpectDelete` on an EndpointSlice that wasn't actually deleted. That would prevent any future syncs from happening for this Service until that EndpointSlice was actually deleted. We'd run into the same problem if we were to set a generation to a higher value than it actually was persisted as. I don't think it's possible for either of those to happen with the current code since we're waiting for creates/updates/deletes to happen first, but if it did, it would not be good. \r\n\r\nThe previous PR included a [3 second max sync delay to help mitigate these kinds of edge cases](https://github.com/kubernetes/kubernetes/pull/93520/files#diff-8765679aa8ca8cc500a6dc038b8707cbc29ddc2d2f94f1da911e2cd5e4df5816R289). When a service hadn't been synced for that long, it would just ignore cache errors and try anyway. If that or some kind of similar fallback would be valuable, I'm happy to add it in.",
        "createdAt" : "2021-02-25T01:26:31Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      },
      {
        "id" : "26a6c376-eedd-448b-9d80-c0977d5eaaac",
        "parentId" : "5cc26a8c-3bc3-4f47-8004-1f91276be8b0",
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Let's not add it.\r\n\r\nWith the changes to Delete, on the next operation ShouldSync will always return true. Which is basically what I wanted.",
        "createdAt" : "2021-02-25T15:05:03Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1542606c275129ebcb9ffdb67696f8cc9b638d6",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +439,443 @@\tif endpointSlice != nil && managedByController(endpointSlice) && c.endpointSliceTracker.Has(endpointSlice) {\n\t\t// This returns false if we didn't expect the EndpointSlice to be\n\t\t// deleted. If that is the case, we queue the Service for another sync.\n\t\tif !c.endpointSliceTracker.HandleDeletion(endpointSlice) {\n\t\t\tc.queueServiceForEndpointSlice(endpointSlice)"
  },
  {
    "id" : "0f316dd4-5374-48a6-a798-5f33d375d919",
    "prId" : 99345,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/99345#pullrequestreview-603667730",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74a44f64-9d13-4892-9a8e-dd80d883f5b7",
        "parentId" : null,
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "what do you think about a metric here? is something you consider important or nice to have?\r\n`endpointslice_stale_requeue_count` ",
        "createdAt" : "2021-02-25T08:48:46Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "6329597c-508b-4601-9c43-700919c3b32b",
        "parentId" : "74a44f64-9d13-4892-9a8e-dd80d883f5b7",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "I think we should publish some metric here, but I'm not sure what. A metric like `endpointslice_stale_requeue_count` would likely be most helpful if it could be compared with the number of successful syncs, otherwise it would be difficult to build alerting on.\r\n\r\nInspired by your idea, I've been thinking about adding a new `sync_service_outcomes` metric that would track the number of syncs the controller has performed along with the outcome as a label with 3 potential values: `success`, `staleCache`, and `error`. That could enable alerts that were based on the proportion of non-success outcomes.\r\n\r\nSince I'm interested in attempting to backport this, I think adding a metric directly to this PR would complicate that, so maybe this can be done as a follow up.",
        "createdAt" : "2021-02-26T02:02:26Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      },
      {
        "id" : "be3de9bc-5d90-4e51-b14a-c640c27d6817",
        "parentId" : "74a44f64-9d13-4892-9a8e-dd80d883f5b7",
        "authorId" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "body" : "yeah, agree that this should be a follow up",
        "createdAt" : "2021-02-26T10:10:58Z",
        "updatedAt" : "2021-03-02T17:44:16Z",
        "lastEditedBy" : "203dfb85-d185-4057-88b3-a1b4f09fd1fd",
        "tags" : [
        ]
      },
      {
        "id" : "5fa0688b-33a8-46b5-a912-c648876d6df8",
        "parentId" : "74a44f64-9d13-4892-9a8e-dd80d883f5b7",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "Follow up, this metric is added in https://github.com/kubernetes/kubernetes/pull/99522.",
        "createdAt" : "2021-03-04T03:21:03Z",
        "updatedAt" : "2021-03-04T03:21:03Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1542606c275129ebcb9ffdb67696f8cc9b638d6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +348,352 @@\n\tif c.endpointSliceTracker.StaleSlices(service, endpointSlices) {\n\t\treturn &StaleInformerCache{\"EndpointSlice informer cache is out of date\"}\n\t}\n"
  },
  {
    "id" : "e992e52b-a086-4e46-984a-74ea9b12e5df",
    "prId" : 85703,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85703#pullrequestreview-332792609",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf1840e5-e8c4-498c-87e5-b1ac921bd71e",
        "parentId" : null,
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "if either the old or new endpoint slices were managed by us, and the service it references changed, don't we need to queue one (or maybe both) of the referenced services? maybe the UpToDate check covers this case... it's a little dense to think through",
        "createdAt" : "2019-12-16T18:27:36Z",
        "updatedAt" : "2019-12-26T20:16:06Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      },
      {
        "id" : "304eecd1-d343-434f-b9fb-4baa384baa57",
        "parentId" : "bf1840e5-e8c4-498c-87e5-b1ac921bd71e",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "That's not a use case I'd planned for. The controller itself will never change the Service an EndpointSlice belongs to. Of course this could happen if someone were intentionally messing with EndpointSlices, but they could also break other things as well. With that said, the `UpToDate` check would fail if the EndpointSlice service had been changed since it would not expect that EndpointSlice name or resource version for the given Service.",
        "createdAt" : "2019-12-16T18:59:06Z",
        "updatedAt" : "2019-12-26T20:16:06Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c75787bb778d45db1b2ee89cc15bf19306c02eb2",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +382,386 @@\t}\n\tif managedByChanged(prevEndpointSlice, endpointSlice) || (managedByController(endpointSlice) && c.endpointSliceTracker.Stale(endpointSlice)) {\n\t\tc.queueServiceForEndpointSlice(endpointSlice)\n\t}\n}"
  },
  {
    "id" : "9fb0099b-81fb-40d9-8739-0892fffe8725",
    "prId" : 85703,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85703#pullrequestreview-336607364",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ac58367-d658-4cfb-87c8-0f2e1eec9b5a",
        "parentId" : null,
        "authorId" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "body" : "defensive type conversion check here as well",
        "createdAt" : "2019-12-23T22:34:15Z",
        "updatedAt" : "2019-12-26T20:16:06Z",
        "lastEditedBy" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "tags" : [
        ]
      },
      {
        "id" : "6e3cc5ae-83e0-467b-a0a7-7ff61bf6acbe",
        "parentId" : "4ac58367-d658-4cfb-87c8-0f2e1eec9b5a",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "Added, thanks!",
        "createdAt" : "2019-12-26T20:16:31Z",
        "updatedAt" : "2019-12-26T20:16:31Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c75787bb778d45db1b2ee89cc15bf19306c02eb2",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +376,380 @@func (c *Controller) onEndpointSliceUpdate(prevObj, obj interface{}) {\n\tprevEndpointSlice := obj.(*discovery.EndpointSlice)\n\tendpointSlice := obj.(*discovery.EndpointSlice)\n\tif endpointSlice == nil || prevEndpointSlice == nil {\n\t\tutilruntime.HandleError(fmt.Errorf(\"Invalid EndpointSlice provided to onEndpointSliceUpdate()\"))"
  },
  {
    "id" : "2bd5dc26-9aac-4883-880d-dcd38b19f369",
    "prId" : 85703,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/85703#pullrequestreview-336614052",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a47c78b-b2d6-41b7-92c5-d8e17ed8454b",
        "parentId" : null,
        "authorId" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "body" : "...ForEndpointSlice seems redundant?",
        "createdAt" : "2019-12-23T22:42:32Z",
        "updatedAt" : "2019-12-26T20:16:06Z",
        "lastEditedBy" : "b714f738-aa05-4f49-a624-eaaf3e0cbb70",
        "tags" : [
        ]
      },
      {
        "id" : "69acbe15-3b64-4393-870a-6ab507967c39",
        "parentId" : "3a47c78b-b2d6-41b7-92c5-d8e17ed8454b",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "Yeah, I was trying to differentiate between what we're already doing for Pods where all related Services get queued up when a Pod changes. `Service` is potentially redundant here as the only queue referenced in this controller is the `Service` queue. I just tend to favor verbose function names so there's no question what is happening. ",
        "createdAt" : "2019-12-24T01:23:27Z",
        "updatedAt" : "2019-12-26T20:16:06Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      },
      {
        "id" : "0bf2509b-4305-4f2b-bb69-79269372bec0",
        "parentId" : "3a47c78b-b2d6-41b7-92c5-d8e17ed8454b",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "I haven't really thought of a better name here. Open to alternatives. I think the `ForEndpointSlice` provides useful context/differentiation here, but I can still remove that if you'd prefer.",
        "createdAt" : "2019-12-26T20:56:19Z",
        "updatedAt" : "2019-12-26T20:56:19Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c75787bb778d45db1b2ee89cc15bf19306c02eb2",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +398,402 @@// queueServiceForEndpointSlice attempts to queue the corresponding Service for\n// the provided EndpointSlice.\nfunc (c *Controller) queueServiceForEndpointSlice(endpointSlice *discovery.EndpointSlice) {\n\tkey, err := serviceControllerKey(endpointSlice)\n\tif err != nil {"
  },
  {
    "id" : "de0214fd-e07a-4c78-b125-946f2ce2a78a",
    "prId" : 81048,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81048#pullrequestreview-279907118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "086dab3e-9b25-4653-b9ab-c4e4def2cbe1",
        "parentId" : null,
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "I think utilruntime.HandleError may not record event. So Record Event on the service when error?\r\n\r\n",
        "createdAt" : "2019-08-20T22:48:34Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      },
      {
        "id" : "3e50702d-faa9-411f-b07b-66a4745a51e8",
        "parentId" : "086dab3e-9b25-4653-b9ab-c4e4def2cbe1",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "As far as I can tell, this will only ever get called if syncService fails. I had been triggering an event on the most likely failure there ([reconciler.reconcile()](https://github.com/robscott/kubernetes/blob/endpointslice-controller/pkg/controller/endpointslice/endpointslice_controller.go#L296)), but also added a couple additional events above that should other less likely errors occur (failure to list pods or endpointslices). \r\n\r\nThere are still a couple errors that doesn't catch - service fetch or problem parsing the service namespace/name key. Both of those don't have access to a service at that point though so couldn't write an event anywhere meaningful that I can think of. \r\n\r\nIf I were to add an event here in `handleErr()` I wouldn't have access to the service either, and to get it I'd have to use the same logic that could potentially result in an error.",
        "createdAt" : "2019-08-25T21:28:15Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      },
      {
        "id" : "5f9e4655-f1eb-4669-a751-d95f3e2164a7",
        "parentId" : "086dab3e-9b25-4653-b9ab-c4e4def2cbe1",
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "I would recommend you make the get service logic into a separate util function.\r\n- If the service no longer exists, just log it and skip.\r\n- If failed to retrieve service, just log it and skip\r\n- Only log event when retrieving service successfully",
        "createdAt" : "2019-08-27T00:53:40Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      }
    ],
    "commit" : "75f6c249235b40b24e9ea1efdb1ff81dd76a8d68",
    "line" : 233,
    "diffHunk" : "@@ -1,1 +231,235 @@\tif c.queue.NumRequeues(key) < maxRetries {\n\t\tklog.Warningf(\"Error syncing endpoint slices for service %q, retrying. Error: %v\", key, err)\n\t\tc.queue.AddRateLimited(key)\n\t\treturn\n\t}"
  },
  {
    "id" : "7c46fbe5-bda2-4644-9b0e-2513c7426880",
    "prId" : 81048,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81048#pullrequestreview-279925307",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84e0d90c-ca5e-429f-8089-fd6d39e3934b",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "What would cause it to get retried?  Just an example would do...",
        "createdAt" : "2019-08-27T02:44:22Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "75f6c249235b40b24e9ea1efdb1ff81dd76a8d68",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +47,51 @@\tserviceNameLabel = \"kubernetes.io/service-name\"\n\n\t// maxRetries is the number of times a service will be retried before it is\n\t// dropped out of the queue. Any sync error, such as a failure to create or\n\t// update an EndpointSlice could trigger a retry. With the current"
  },
  {
    "id" : "365a6ecd-891e-45ce-91f8-9137e8fa9670",
    "prId" : 81048,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81048#pullrequestreview-280299922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc6dc15a-9913-4fe8-b98f-09f8ea0510a9",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Should we be adding metrics to this, so we can inspect behavior later?  Would accept as a follow-up.",
        "createdAt" : "2019-08-27T02:47:55Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "cce87b31-4efb-4ebb-9d7f-d023bb0de3a1",
        "parentId" : "dc6dc15a-9913-4fe8-b98f-09f8ea0510a9",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "Definitely planning on it. Had some good initial discussions around what the best metrics would be to track, but will make that part of a separate PR.",
        "createdAt" : "2019-08-27T04:42:15Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      },
      {
        "id" : "9e17b499-b97a-4b6b-b060-7ca18182e17e",
        "parentId" : "dc6dc15a-9913-4fe8-b98f-09f8ea0510a9",
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "err on the side of too much instrumentation.  If we're going to figure out this heuristic's behavior and tune coefficients for beta, we need really good viz.",
        "createdAt" : "2019-08-27T15:54:08Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      }
    ],
    "commit" : "75f6c249235b40b24e9ea1efdb1ff81dd76a8d68",
    "line" : 244,
    "diffHunk" : "@@ -1,1 +242,246 @@func (c *Controller) syncService(key string) error {\n\tstartTime := time.Now()\n\tdefer func() {\n\t\tklog.V(4).Infof(\"Finished syncing service %q endpoint slices. (%v)\", key, time.Since(startTime))\n\t}()"
  },
  {
    "id" : "04fa7a11-e310-4777-9835-883354912366",
    "prId" : 81048,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/81048#pullrequestreview-279951660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8f378e0-6d0e-4a59-8002-726351d08af4",
        "parentId" : null,
        "authorId" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "body" : "Maybe check for len() == 0?  It's subtle, but I think we expect both cases to behave the same, don't we?  Should cross-check with old controller...",
        "createdAt" : "2019-08-27T02:49:30Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "f87fe7d3-581c-4cb6-b17e-b807c6f2c789",
        "tags" : [
        ]
      },
      {
        "id" : "80dffc39-4c9e-4d8e-a70c-9165e75f34cb",
        "parentId" : "c8f378e0-6d0e-4a59-8002-726351d08af4",
        "authorId" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "body" : "It looks like both endpoints controller and the service lister expansion use `selector == nil` so I think it makes sense to stick with this for consistency.",
        "createdAt" : "2019-08-27T04:41:26Z",
        "updatedAt" : "2019-08-29T04:13:56Z",
        "lastEditedBy" : "399a0f74-29de-4365-9ae5-f743ae5f278c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75f6c249235b40b24e9ea1efdb1ff81dd76a8d68",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +259,263 @@\t}\n\n\tif service.Spec.Selector == nil {\n\t\t// services without a selector receive no endpoint slices from this controller;\n\t\t// these services will receive endpoint slices that are created out-of-band via the REST API."
  }
]