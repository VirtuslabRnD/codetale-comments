[
  {
    "id" : "77ed8ebc-1e1d-4894-a019-b5fba5bb6288",
    "prId" : 73314,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73314#pullrequestreview-196966267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbb1973a-ab94-4069-a71b-819c4c9f09d3",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "Maybe add a TODO to provide more robust mechanism longer term?",
        "createdAt" : "2019-01-28T09:02:08Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "148e174e-4ff8-44a3-9510-dd0ec98eb395",
        "parentId" : "dbb1973a-ab94-4069-a71b-819c4c9f09d3",
        "authorId" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "body" : "Done.",
        "createdAt" : "2019-01-28T10:55:53Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e7f7df94ef8f0300dfe54bfad642b6d8cf937f3",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +34,38 @@// Informer framework. Such situations, i.e. frequent updates of the same object in a single sync\n// period, should be relatively rare and therefore this util should provide a good approximation of\n// the EndpointsLastChangeTriggerTime.\n// TODO(mm4tt): Implement a more robust mechanism that is not subject to the above limitations.\ntype TriggerTimeTracker struct {"
  },
  {
    "id" : "ee5b34cf-b8c0-40ec-9056-94914a7763c3",
    "prId" : 73314,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73314#pullrequestreview-196966267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cd3265f-5034-4e6a-9d9f-15177bb8474f",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "I'm wondering if we can avoid recreating the whole map from scratch on every sync.\r\nThe reason is that, memory allocations/GC are significant contributor to overall CPU usage and performance of our control plane, so the less of them we can do them, the better.",
        "createdAt" : "2019-01-28T09:10:05Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "2c993c48-14dc-402f-bcd4-798c48157c53",
        "parentId" : "5cd3265f-5034-4e6a-9d9f-15177bb8474f",
        "authorId" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "body" : "As discussed offline, left as is and added a TODO.",
        "createdAt" : "2019-01-28T11:56:42Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e7f7df94ef8f0300dfe54bfad642b6d8cf937f3",
    "line" : 104,
    "diffHunk" : "@@ -1,1 +102,106 @@\t// TODO(mm4tt): If memory allocation / GC performance impact of recreating map in every call\n\t// turns out to be too expensive, we should consider rewriting this to reuse the existing map.\n\tpodTriggerTimes := make(map[string]time.Time)\n\tfor _, pod := range pods {\n\t\tif podTriggerTime := getPodTriggerTime(pod); !podTriggerTime.IsZero() {"
  },
  {
    "id" : "d843c8f9-3532-4475-b19d-bf4f0a3f7e88",
    "prId" : 73314,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73314#pullrequestreview-197054325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "139c62cc-a41a-4147-a18f-49acbaa335e1",
        "parentId" : null,
        "authorId" : "24302707-9254-48df-89a5-cbcc349462b8",
        "body" : "nit: can you please split this line?",
        "createdAt" : "2019-01-28T12:31:54Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "24302707-9254-48df-89a5-cbcc349462b8",
        "tags" : [
        ]
      },
      {
        "id" : "3dab59c9-1875-42ae-9fc0-72448abe0b3f",
        "parentId" : "139c62cc-a41a-4147-a18f-49acbaa335e1",
        "authorId" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "body" : "As discussed offline, it doesn't require splitting. It's below the 100 character limit. ",
        "createdAt" : "2019-01-28T14:11:50Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e7f7df94ef8f0300dfe54bfad642b6d8cf937f3",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +107,111 @@\t\t\tpodTriggerTimes[pod.Name] = podTriggerTime\n\t\t\tif podTriggerTime.After(state.lastPodTriggerTimes[pod.Name]) {\n\t\t\t\t// Pod trigger time has changed since the last sync, update minChangedTriggerTime.\n\t\t\t\tminChangedTriggerTime = min(minChangedTriggerTime, podTriggerTime)\n\t\t\t}"
  },
  {
    "id" : "a6f26d67-92a7-4ad9-aedc-aed57eb8fd8f",
    "prId" : 73314,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73314#pullrequestreview-199607528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bc02a81-e114-4c10-a39e-e70d5a234e49",
        "parentId" : null,
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "Probably add a piece of comment here to explain why using the minChangedTriggerTime to do the measurement may not yeild the intended result due to multiple update getting merged into one. \r\n\r\nBasically,    sync1 ->  pod1, pod2, pod3 changed -> sync2    \r\n`minChangedTriggerTime` in this case is time delta between pod3 update and sync2, but the actual time difference may be delta between pod1 update and sync2. ",
        "createdAt" : "2019-02-02T00:12:55Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      },
      {
        "id" : "7da2bd27-5c43-4edf-81e7-f983d608bbbf",
        "parentId" : "3bc02a81-e114-4c10-a39e-e70d5a234e49",
        "authorId" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "body" : "You're right, assuming that by pod1, pod2, pod3 you're referring to multiple updates of the same pod. \r\nIn such case here we'd see and export the pod3 change time whereas we should've used the pod1 change time.\r\nThis was already documented in the TriggerTimeTracker struct documentation, but added it also in the doc of this function.\r\n\r\nAnd just to make one thing clear, we're not operating on time deltas here. Here we only export the \"trigger time\" which is the time of the oldest change that triggered endpoints object change. Time delta will be computed in kube-proxy to export the actual latency.\r\n",
        "createdAt" : "2019-02-04T14:20:39Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e7f7df94ef8f0300dfe54bfad642b6d8cf937f3",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +99,103 @@\t// minChangedTriggerTime is the min trigger time of all trigger times that have changed since the\n\t// last sync.\n\tvar minChangedTriggerTime time.Time\n\t// TODO(mm4tt): If memory allocation / GC performance impact of recreating map in every call\n\t// turns out to be too expensive, we should consider rewriting this to reuse the existing map."
  },
  {
    "id" : "84c07f29-72e3-4631-8909-473c87f39af3",
    "prId" : 73314,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73314#pullrequestreview-199607528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c7fcc67-024c-413d-99d4-8fa5b06cb8bc",
        "parentId" : null,
        "authorId" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "body" : "Ignoring service update may result in discrepancy right? \r\nFor example:\r\nService ports got updated and no change on pods. This would result in update in endpoints object. The `minChangedTriggerTime` will still return one of the time delta after the latest pod update, right?\r\n\r\nIn this case, unless we have the something similar to service.LastUpdateTime, I think we can ignore it and skip recording. WDYT?\r\n",
        "createdAt" : "2019-02-02T00:17:13Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "e83108b8-1fb2-416b-9298-d5b70c14f708",
        "tags" : [
        ]
      },
      {
        "id" : "52fdb3e5-c6b0-4c01-9143-b77837a13082",
        "parentId" : "2c7fcc67-024c-413d-99d4-8fa5b06cb8bc",
        "authorId" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "body" : "That's what we do. The minChangedTriggerTime definition is: the min trigger time of all trigger times that **have changed** since the last sync.\r\n\r\nAssuming that only service has changed, the minChangeTriggerTime would be a 'zero' time, as nothing has changed (since we don't have the service.LastUpdateTime and service.CreationTime stays the same). If the ComputeEndpointsLastChangeTriggerTime returns a 'zero' time the result should be ignored and the EndpointsLastChangeTriggerTime shouldn't be exported.\r\n\r\nIf in future we manage to get the service.LastUpdateTime somehow it will be enough to just change this place and everything will work correctly. That's why I left a TODO here.\r\n\r\nI updated the docs and comments to make it easier to understand. Also added a test for that case.",
        "createdAt" : "2019-02-04T14:25:49Z",
        "updatedAt" : "2019-02-05T08:37:49Z",
        "lastEditedBy" : "efa0987e-7d25-471d-9dea-9daaf77c2120",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e7f7df94ef8f0300dfe54bfad642b6d8cf937f3",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +151,155 @@// result in the endpoints object change.\nfunc getServiceTriggerTime(service *v1.Service) (triggerTime time.Time) {\n\t// TODO(mm4tt): Ideally we should look at service.LastUpdateTime, but such thing doesn't exist.\n\treturn service.CreationTimestamp.Time\n}"
  },
  {
    "id" : "5d3dfb6b-45d8-4a27-a6b8-535b31fd588b",
    "prId" : 73314,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/73314#pullrequestreview-200684011",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "352687bf-04a0-4516-8d40-17845e871033",
        "parentId" : null,
        "authorId" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "body" : "How about using read lock here ?",
        "createdAt" : "2019-02-06T16:48:02Z",
        "updatedAt" : "2019-02-06T16:48:03Z",
        "lastEditedBy" : "42b1e004-4fa7-4e43-84cf-5378839b49ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e7f7df94ef8f0300dfe54bfad642b6d8cf937f3",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +86,90 @@\t// As there won't be any concurrent calls for the same key, we need to guard access only to the\n\t// endpointsStates map.\n\tt.mutex.Lock()\n\tstate, wasKnown := t.endpointsStates[key]\n\tt.mutex.Unlock()"
  }
]