[
  {
    "id" : "10dfcb14-44ec-4d23-bcb5-8ecd8b2d7d46",
    "prId" : 93710,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93710#pullrequestreview-464197671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9592c593-fa55-4a6a-8e2a-a41e1c4a238e",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "we already have node, pod and volume name from `GetPodToAdd` function. Shouldn't we able to get volumeSpec from that and integrate removal of pod from DSOW in the loop above rather than creating a new loop?",
        "createdAt" : "2020-08-10T04:52:24Z",
        "updatedAt" : "2020-08-25T00:15:34Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "7036fca4-08e1-4a08-b732-c48e9fd48845",
        "parentId" : "9592c593-fa55-4a6a-8e2a-a41e1c4a238e",
        "authorId" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "body" : "IMHO, the way `GetPodToAdd` returns the pods list but that does not guarantee to have all the volumes we need to check. There might be cases where pod A mounted volume `x`,` y`. And in the `GetPodToAdd` function, the final return pod list will only have a map of pod A to volume `y`. Because volume `x` has been overridden in `GetPodToAdd()`. Secondly, there might be cases where volume `x` has two corresponding pods. If we check the volume from the `GetPodToAdd` loop. There will be an overhead of `IsAttachableVolume` check for a volume.\r\n\r\nDoes this makes sense?",
        "createdAt" : "2020-08-10T06:16:56Z",
        "updatedAt" : "2020-08-25T00:15:34Z",
        "lastEditedBy" : "d10fef96-5a18-44e7-b23e-735de7561af7",
        "tags" : [
        ]
      },
      {
        "id" : "00aad447-5ccd-4447-bf90-6204128ee47e",
        "parentId" : "9592c593-fa55-4a6a-8e2a-a41e1c4a238e",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "hmm, the first issue sounds like inefficiency in populator. if a pod has more than 1 volume - it may require more than one reconciliation loop for both volumes and pod to disappear from DSOW, which could be 1 minute apart.  IMO - it is worth fixing that, but we can file this as a separate issue.\r\n\r\nEDIT: having said that it isn't so bad, considering periodic populator is only a fallback mechanism and we are already responding to pod delete events. ",
        "createdAt" : "2020-08-10T12:50:02Z",
        "updatedAt" : "2020-08-25T00:15:34Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6d8e6c5c2e5e25a0dd6a003497519a226128161",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +170,174 @@\t\t\t}\n\t\t}\n\t}\n}\n"
  },
  {
    "id" : "47fadea3-601e-4c8a-82db-cad764e94db3",
    "prId" : 42033,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/42033#pullrequestreview-32779893",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f386c89-3de6-44b2-8b3e-8e93f2c66ad9",
        "parentId" : null,
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "@ncdc do you think polling entire cluster for running pods every 1 minute is not going to be a strain on cluster - even though it goes through informer?",
        "createdAt" : "2017-04-12T20:14:23Z",
        "updatedAt" : "2017-04-18T03:29:39Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "3f9fdb2c-dbb7-4786-927f-611543bd9469",
        "parentId" : "3f386c89-3de6-44b2-8b3e-8e93f2c66ad9",
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "@gnufied that's a `PodLister` backed by the shared informer's cache. It's not hitting the apiserver. There is no data going over the wire for this call.\r\n\r\nIf you have a cluster with 100k or 250k or more pods, do you think it's safe to iterate through them every minute, even if it's just pulling them out of a cache?",
        "createdAt" : "2017-04-12T20:21:21Z",
        "updatedAt" : "2017-04-18T03:29:39Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      },
      {
        "id" : "f34649d3-6922-4cd2-9c19-1854393d552d",
        "parentId" : "3f386c89-3de6-44b2-8b3e-8e93f2c66ad9",
        "authorId" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "body" : "Yeah fair point. I think I will let @saad-ali decide on approach of fixing this issue.",
        "createdAt" : "2017-04-12T20:43:52Z",
        "updatedAt" : "2017-04-18T03:29:39Z",
        "lastEditedBy" : "d3e684d7-edd2-4290-a8bf-e8b698c97338",
        "tags" : [
        ]
      },
      {
        "id" : "c31ff653-90f6-4989-9245-badca24396dd",
        "parentId" : "3f386c89-3de6-44b2-8b3e-8e93f2c66ad9",
        "authorId" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "body" : "Since we are allowed to set custom resync periods when using shared informers now, would it be better to set a 30sec-1min resync period and let the existing podUpdate flow handle this? See this comment by @saad-ali https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/volume/attachdetach/attach_detach_controller.go#L83 (which foresaw https://github.com/kubernetes/kubernetes/issues/43300, kind of)\r\n\r\nedit: hmm well setting a custom resync period would actually be worse performance-wise wouldn't it? I guess the main benefit of doing it would be to reduce the size of this pr, but it's already written :)",
        "createdAt" : "2017-04-13T20:09:04Z",
        "updatedAt" : "2017-04-18T03:29:39Z",
        "lastEditedBy" : "241ab19e-f85a-4d22-92e2-88f2b6287d14",
        "tags" : [
        ]
      },
      {
        "id" : "82417377-090f-4236-95ea-baabf4784bc6",
        "parentId" : "3f386c89-3de6-44b2-8b3e-8e93f2c66ad9",
        "authorId" : "8e448017-7838-493d-a424-33cada0da657",
        "body" : "It's possible that fetching and processing all pods from the informer cache once per minute could result in perf issues related to memory. When the controller starts (or restarts), it takes that kind of hit already.\r\n\r\nI say let's get it merged, give it plenty of time to bake for 1.7, and if the scale tests freak out we can consider things like reducing the frequency of this. CC @kubernetes/sig-scalability-misc ",
        "createdAt" : "2017-04-14T03:57:14Z",
        "updatedAt" : "2017-04-18T03:29:39Z",
        "lastEditedBy" : "8e448017-7838-493d-a424-33cada0da657",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cafb9042b100c6d6614a46f09e3f7410337627a",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +144,148 @@\nfunc (dswp *desiredStateOfWorldPopulator) findAndAddActivePods() {\n\tpods, err := dswp.podLister.List(labels.Everything())\n\tif err != nil {\n\t\tglog.Errorf(\"podLister List failed: %v\", err)"
  }
]