[
  {
    "id" : "fe5c6a8e-be53-40f5-9ee4-64a3763e1924",
    "prId" : 72045,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72045#pullrequestreview-187665620",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ede8efa0-82bd-43e7-8019-a8e8378537f8",
        "parentId" : null,
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "In new design, IMO there is no need to separate `bindings` and `provisionedPVCs` in pod binding cache. It's better to get/update/delete `bindings` and `provisionedPVCs` together. I can refactor this if this design is right way to go.",
        "createdAt" : "2018-12-24T09:13:48Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a62f53d3fc915a1c99156c8de049fedd2219a41",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +161,165 @@\t\t\tprovisionedClaims = nil\n\t\t}\n\t\t// TODO merge into one atomic function\n\t\t// Mark cache with all the matches for each PVC for this node\n\t\tb.podBindingCache.UpdateBindings(pod, node.Name, matchedClaims)"
  },
  {
    "id" : "6100d89f-ff17-4fa1-bedc-1cdc17155978",
    "prId" : 72045,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72045#pullrequestreview-187887615",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb92bce1-c344-4169-9097-9da80f8c3ccc",
        "parentId" : null,
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "Because if no change happened, current object is already latest, we cannot skip it. Save updated objects here, then we are safe to skip all objects older than this.",
        "createdAt" : "2018-12-26T12:25:40Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a62f53d3fc915a1c99156c8de049fedd2219a41",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +401,405 @@\t\t} else {\n\t\t\t// Save updated object from apiserver for later checking.\n\t\t\tbinding.pv = newPV\n\t\t}\n\t\tlastProcessedBinding++"
  },
  {
    "id" : "94c4d005-601d-4f47-b710-08282130e434",
    "prId" : 72045,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72045#pullrequestreview-189297514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "899b3949-c295-4dcb-82ce-401c0b95e43c",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Is the pod deletion case still handled?",
        "createdAt" : "2019-01-03T22:21:21Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "dbc72346-17c3-435d-9011-e95ee04d3f0a",
        "parentId" : "899b3949-c295-4dcb-82ce-401c0b95e43c",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "Oh, I missed this, binding operation should be cancelled when pod is deleted or assigned a node by users or other scheduler.\r\n\r\nDone in 9965fa634caffe452e69082ed414cee693cf1db5.",
        "createdAt" : "2019-01-04T09:08:00Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a62f53d3fc915a1c99156c8de049fedd2219a41",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +350,354 @@\n\treturn wait.Poll(time.Second, b.bindTimeout, func() (bool, error) {\n\t\tb, err := b.checkBindings(assumedPod, bindings, claimsToProvision)\n\t\treturn b, err\n\t})"
  },
  {
    "id" : "d246ffca-cbad-4dfd-8a2f-6325aa38e5ff",
    "prId" : 72045,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72045#pullrequestreview-189936428",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9afd2a1d-865b-4e4f-90fe-8ff1321c4701",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can you add a unit test for this, if one doesn't already exist?",
        "createdAt" : "2019-01-04T22:11:53Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "ed87191c-7451-405d-a2ec-954600cddeaa",
        "parentId" : "9afd2a1d-865b-4e4f-90fe-8ff1321c4701",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "sure, it has some conflicts with #70026, I will rebase and squash after #70026 is merged.",
        "createdAt" : "2019-01-05T05:01:33Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      },
      {
        "id" : "3ed95e80-571f-4099-9973-3d8d252e88fb",
        "parentId" : "9afd2a1d-865b-4e4f-90fe-8ff1321c4701",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "There is [an existing test `pod-deleted-after-time`](https://github.com/kubernetes/kubernetes/pull/72045/files#diff-7e773cadec5750de298ed8fa194edda3L1357). In old PR, BindPodVolume failed with timeout error, so it passed too. Perhaps it's better to check error too. I can address this in a separate PR.",
        "createdAt" : "2019-01-07T18:34:01Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a62f53d3fc915a1c99156c8de049fedd2219a41",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +457,461 @@\t// We check pod binding cache here which will be cleared when pod is\n\t// removed from scheduling queue.\n\tif b.podBindingCache.GetDecisions(pod) == nil {\n\t\treturn false, fmt.Errorf(\"pod %q does not exist any more\", podName)\n\t}"
  },
  {
    "id" : "a7fd4d70-7815-4ce4-8ce3-38b6902a0d6b",
    "prId" : 72045,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/72045#pullrequestreview-190080988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7449a5f0-6060-4e03-823c-eef514577118",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "unit test for this condition?",
        "createdAt" : "2019-01-07T23:38:00Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "b64af0d5-f52c-4cb5-8ebb-6fae613ee72f",
        "parentId" : "7449a5f0-6060-4e03-823c-eef514577118",
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "Done in https://github.com/kubernetes/kubernetes/pull/72045/commits/01c19a83d56b666004be657f700f034db80ef6ea#diff-7e773cadec5750de298ed8fa194edda3R1389.",
        "createdAt" : "2019-01-08T03:26:11Z",
        "updatedAt" : "2019-01-09T02:50:28Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a62f53d3fc915a1c99156c8de049fedd2219a41",
    "line" : 230,
    "diffHunk" : "@@ -1,1 +520,524 @@\t\t\tif err != nil {\n\t\t\t\tif _, ok := err.(*errNotFound); ok {\n\t\t\t\t\t// We tolerate NotFound error here, because PV is possibly\n\t\t\t\t\t// not found because of API delay, we can check next time.\n\t\t\t\t\t// And if PV does not exist because it's deleted, PVC will"
  },
  {
    "id" : "b8c9ddd9-3c1c-4b99-9c25-c60472f23400",
    "prId" : 70026,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70026#pullrequestreview-188417789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acc856c9-e6d4-48d3-bd05-45387f7b6031",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Can you add a comment here that this nodeInformer cannot be used in FindPodVolumes?  Because the predicate is reused for cluster autoscaler, which passes in fake Node objects to the predicate.",
        "createdAt" : "2018-12-28T18:32:20Z",
        "updatedAt" : "2019-01-04T11:59:53Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "667c1b86-4fb8-4f1d-8343-47c4347a3af7",
        "parentId" : "acc856c9-e6d4-48d3-bd05-45387f7b6031",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Added the comment in `FindPodVolumes` instead. Let me know if you disagree.",
        "createdAt" : "2018-12-29T17:48:54Z",
        "updatedAt" : "2019-01-04T11:59:53Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb4fcddd1b76ed628412b0cb0daabc5a05f87790",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +99,103 @@\tctrl *PersistentVolumeController\n\n\tnodeInformer coreinformers.NodeInformer\n\tpvcCache     PVCAssumeCache\n\tpvCache      PVAssumeCache"
  },
  {
    "id" : "2488a628-9bd9-459c-b059-64120c267072",
    "prId" : 70026,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70026#pullrequestreview-189115373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3463ce5-56c2-4cc5-b716-2e5fd417bee9",
        "parentId" : null,
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "For static PV binding, PV has been checked in finding, this is used to double check before scheduling because node labels may be modified?\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/98b3e4212797f87410ec8b12a836c289b2e68102/pkg/controller/volume/persistentvolume/index.go#L178-L186",
        "createdAt" : "2019-01-03T05:15:16Z",
        "updatedAt" : "2019-01-04T11:59:53Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      },
      {
        "id" : "58ab0181-0e2d-4302-9c6d-8104d68ed9e8",
        "parentId" : "b3463ce5-56c2-4cc5-b716-2e5fd417bee9",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "Not because of the node labels, but because because of the new static binding [test added](https://github.com/kubernetes/kubernetes/pull/70026#discussion_r244380476).",
        "createdAt" : "2019-01-03T16:34:37Z",
        "updatedAt" : "2019-01-04T11:59:53Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      },
      {
        "id" : "8cbac8b6-a46e-4cee-829d-f678d5942c06",
        "parentId" : "b3463ce5-56c2-4cc5-b716-2e5fd417bee9",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "The check here is more of a safety guard in case the user ends up manually binding to a different PV on a different node than what the scheduler chose",
        "createdAt" : "2019-01-03T17:50:15Z",
        "updatedAt" : "2019-01-04T11:59:53Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb4fcddd1b76ed628412b0cb0daabc5a05f87790",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +404,408 @@\t\tif err := volumeutil.CheckNodeAffinity(pv, node.Labels); err != nil {\n\t\t\treturn false, fmt.Errorf(\"pv %q node affinity doesn't match node %q: %v\", pv.Name, node.Name, err)\n\t\t}\n\n\t\t// Check if pv.ClaimRef got dropped by unbindVolume()"
  },
  {
    "id" : "88b388a3-f178-4699-bff4-be4baef557ff",
    "prId" : 70026,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/70026#pullrequestreview-189091480",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f075ea51-d946-4713-9b1e-ad71568fd880",
        "parentId" : null,
        "authorId" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "body" : "IIUC, for dynamic PV binding, because we cannot pre-check PV node affinity before provisioning (PV does not exist yet), so we must check it after PV is provisioned?",
        "createdAt" : "2019-01-03T05:23:49Z",
        "updatedAt" : "2019-01-04T11:59:53Z",
        "lastEditedBy" : "e4e7c71f-23b5-4203-b65d-3f5f3c503b64",
        "tags" : [
        ]
      },
      {
        "id" : "677dfe5a-bc6f-430a-8ed8-26193463e366",
        "parentId" : "f075ea51-d946-4713-9b1e-ad71568fd880",
        "authorId" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "body" : "I would say that the main point of having this check here is to prevent the scheduler from scheduling the pod rather than leaving the job to the kubelet.",
        "createdAt" : "2019-01-03T16:46:41Z",
        "updatedAt" : "2019-01-04T11:59:53Z",
        "lastEditedBy" : "255dd885-bee4-4c1f-baef-ba11f903dc5c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb4fcddd1b76ed628412b0cb0daabc5a05f87790",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +442,446 @@\t\t\t}\n\t\t\tif err := volumeutil.CheckNodeAffinity(pv, node.Labels); err != nil {\n\t\t\t\treturn false, fmt.Errorf(\"pv %q node affinity doesn't match node %q: %v\", pv.Name, node.Name, err)\n\t\t\t}\n\t\t}"
  },
  {
    "id" : "d3b9ee6d-413a-459d-aa9b-50d205ff8648",
    "prId" : 67556,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/67556#pullrequestreview-151567936",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f3c5831-931f-4f14-aa49-90aedb197092",
        "parentId" : null,
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "I don't know how computationally expensive `checkBindings` is. If it is expensive, running this every second may hinder Scheduler's performance. There are a couple of alternatives: 1) when a volume is bound, the status is updated on the API server. The volumeBinder watches those events and runs checkBindings only at those events. For example, BindPodVolumes runs checkBindings and calls sync.Cond.Wait when checkBinding returns false. The event handler that watches volume binding calls sync.Cond.Broadcast when a volume is bound. 2) Run this less frequently (this is obviously an inferior solution).",
        "createdAt" : "2018-08-30T23:37:37Z",
        "updatedAt" : "2018-09-05T03:37:28Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      },
      {
        "id" : "ece753f3-26eb-4977-80c3-6dceb9f6a91a",
        "parentId" : "2f3c5831-931f-4f14-aa49-90aedb197092",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "`checkBindings` will run through all the PVCs in a single Pod, get the cached PVC and PV objects from a map, and check some fields on those objects.  I would expect Pods to have <5 persistent volumes most of the time, so I would expect the most expensive work to be the cache lookups (map).\r\n\r\nI will look into the event handling alternative.  One potential concern with a Broadcast is all the threads will wake up at the same time so there could be many spikes.",
        "createdAt" : "2018-08-30T23:54:57Z",
        "updatedAt" : "2018-09-05T03:37:28Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "a164ae3f-ddef-4e27-875f-4a6bd9d221bc",
        "parentId" : "2f3c5831-931f-4f14-aa49-90aedb197092",
        "authorId" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "body" : "It would be great if we could measure the performance penalty of the check. If we don't expect `checkBindings` to be causing noticeable performance degradation, we can leave it as is for now.",
        "createdAt" : "2018-08-31T22:52:15Z",
        "updatedAt" : "2018-09-05T03:37:28Z",
        "lastEditedBy" : "15fb535e-c5be-47ce-a304-1cb5da5aca90",
        "tags" : [
        ]
      }
    ],
    "commit" : "e124159990c6d22e9ca662ef35de5eeaef5d8956",
    "line" : 194,
    "diffHunk" : "@@ -1,1 +278,282 @@\t}\n\n\treturn wait.Poll(time.Second, b.bindTimeout, func() (bool, error) {\n\t\t// Get cached values every time in case the pod gets deleted\n\t\tbindings = b.podBindingCache.GetBindings(assumedPod, assumedPod.Spec.NodeName)"
  },
  {
    "id" : "8c8f3ce9-93bf-4f70-b8a7-05a0d4f90975",
    "prId" : 63232,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63232#pullrequestreview-116118248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f33e594-11b6-4d3a-9b7b-29e19423d3df",
        "parentId" : null,
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "This should also check if \"kubernetes.io/no-provisioner\" is set",
        "createdAt" : "2018-04-27T20:03:44Z",
        "updatedAt" : "2018-05-24T09:13:24Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "be716f55-decb-4b53-9ec7-14c62325b27f",
        "parentId" : "6f33e594-11b6-4d3a-9b7b-29e19423d3df",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Should we check if dynamic provisioning is in progress on this node? (ie selected node annotation)",
        "createdAt" : "2018-04-27T20:30:00Z",
        "updatedAt" : "2018-05-24T09:13:24Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "b0131e05-afdd-4e6b-beaf-310daa0f8bec",
        "parentId" : "6f33e594-11b6-4d3a-9b7b-29e19423d3df",
        "authorId" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "body" : "> This should also check if \"kubernetes.io/no-provisioner\" is set\r\n\r\nI didn't noticed that before, so a pvc is not expected to get provisioned if the annotation is set?\r\n\r\n> Should we check if dynamic provisioning is in progress on this node? (ie selected node annotation)\r\n\r\npvcs with selected node annotation should have been filtered out in `shouldDelayBinding`(called by `getPodVolumes`)",
        "createdAt" : "2018-04-28T02:02:28Z",
        "updatedAt" : "2018-05-24T09:13:24Z",
        "lastEditedBy" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "tags" : [
        ]
      },
      {
        "id" : "e7bc539e-4370-4a77-b2de-f461736d7dfe",
        "parentId" : "6f33e594-11b6-4d3a-9b7b-29e19423d3df",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "If the provisioner name in SC is set to \"no-provisioner\", then dynamic provisioning is not supported, and we should fail the predicate.\r\n\r\nI think the predicate should also check for the selected node annotation, so that the scheduler does not choose a different node while provisioning is still in progress.",
        "createdAt" : "2018-04-28T02:18:21Z",
        "updatedAt" : "2018-05-24T09:13:24Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "e26fbfe2-c84a-452f-ba63-51f1fab31079",
        "parentId" : "6f33e594-11b6-4d3a-9b7b-29e19423d3df",
        "authorId" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "body" : "> If the provisioner name in SC is set to \"no-provisioner\", then dynamic provisioning is not supported, and we should fail the predicate.\r\n\r\nGot it.\r\n\r\n> I think the predicate should also check for the selected node annotation, so that the scheduler does not choose a different node while provisioning is still in progress.\r\n\r\npvcs with selected node annotation will be filtered out in func `getPodVolumes` as type `unboundClaimsImmediate`, and the predicate will fail at the first beginning, that kind of claims will not be passed into the func here.",
        "createdAt" : "2018-04-28T02:31:14Z",
        "updatedAt" : "2018-05-24T09:13:24Z",
        "lastEditedBy" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "tags" : [
        ]
      }
    ],
    "commit" : "446f36559e933d3eeea666bd2165db21b7bb419e",
    "line" : 298,
    "diffHunk" : "@@ -1,1 +471,475 @@\t\t\treturn false, fmt.Errorf(\"no class for claim %q\", getPVCName(claim))\n\t\t}\n\n\t\tclass, err := b.ctrl.classLister.Get(className)\n\t\tif err != nil {"
  },
  {
    "id" : "b6da0d0a-f174-42e9-a5d8-2c40f2b408f0",
    "prId" : 63232,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/63232#pullrequestreview-123027295",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f6154d9-1b2f-4ed6-9df6-33c53966bc87",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Since you update PVC, should you add it to PVC cache? Is it possible that `revertAssumedPVCs()` called by later iteration reverts to a version that's already old and API server has a new one?",
        "createdAt" : "2018-05-24T08:33:01Z",
        "updatedAt" : "2018-05-24T09:13:24Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "9a45d39d-5756-40d4-8b08-9effad981bc5",
        "parentId" : "2f6154d9-1b2f-4ed6-9df6-33c53966bc87",
        "authorId" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "body" : ">Since you update PVC, should you add it to PVC cache?\r\n\r\nWe should have already add it to the assume cache in the previous assume phase\r\n\r\n>Is it possible that revertAssumedPVCs() called by later iteration reverts to a version that's already old and API server has a new one?\r\n\r\nI guess it will not happen, in the Restore func of the assume cache, we won't try to add something new, but change its `latestObj` same to its `apiObj`",
        "createdAt" : "2018-05-24T09:08:21Z",
        "updatedAt" : "2018-05-24T09:17:00Z",
        "lastEditedBy" : "1a75d411-1ce5-48f2-9967-25f88794c451",
        "tags" : [
        ]
      },
      {
        "id" : "a74713dd-00dc-4a34-a461-0fefb6dd8959",
        "parentId" : "2f6154d9-1b2f-4ed6-9df6-33c53966bc87",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "RevertAssumedPVCs called later only reverts PVCs that haven't been API updated yet `i:`",
        "createdAt" : "2018-05-24T14:35:47Z",
        "updatedAt" : "2018-05-24T14:35:47Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "5547ca56-ef2f-46a9-ae4a-f312cfdeacec",
        "parentId" : "2f6154d9-1b2f-4ed6-9df6-33c53966bc87",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "The \"provision-api-update-failed\" unit test checks that 1st PVC shows update in cache, but 2nd PVC that failed API update got reverted in cache",
        "createdAt" : "2018-05-24T14:44:59Z",
        "updatedAt" : "2018-05-24T14:44:59Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "446f36559e933d3eeea666bd2165db21b7bb419e",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +288,292 @@\t// PV controller is expect to signal back by removing related annotations if actual provisioning fails\n\tfor i, claim := range claimsToProvision {\n\t\tif _, err := b.ctrl.kubeClient.CoreV1().PersistentVolumeClaims(claim.Namespace).Update(claim); err != nil {\n\t\t\tglog.V(4).Infof(\"updating PersistentVolumeClaim[%s] failed: %v\", getPVCName(claim), err)\n\t\t\t// only revert assumed cached updates for claims we haven't successfully updated"
  },
  {
    "id" : "2e565a29-f13d-408b-992f-27cf8167142e",
    "prId" : 55039,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55039#pullrequestreview-76936604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7859c89-8352-4781-9990-e4a1ccc877b0",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "It would be nice to note that it actually stores the found PV and `AssumePodVolume` uses the stored PVs. From the description it seems `FindPodVolumes` just returns \"yes, there are PVs that would satisfy the pod\" and some other function actually finds them.",
        "createdAt" : "2017-11-15T16:28:02Z",
        "updatedAt" : "2017-11-22T07:20:19Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "7291d6ef-2d67-414a-8872-962a3166698f",
        "parentId" : "f7859c89-8352-4781-9990-e4a1ccc877b0",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "That's more of an implementation detail rather than a requirement of the interface.  You could implement it by recalculating the PVs again during the assume.\r\n\r\nI did add comments referencing the caching scheme in the actual implementation of the interface, but I could add it here too so that it's all in one place.",
        "createdAt" : "2017-11-15T22:25:44Z",
        "updatedAt" : "2017-11-22T07:20:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "6540850fa2b710060fa94a22e03ff7c39ba652e6",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +57,61 @@\t//\n\t// If a PVC is bound, it checks if the PV's NodeAffinity matches the Node.\n\t// Otherwise, it tries to find an available PV to bind to the PVC.\n\t//\n\t// It returns true if there are matching PVs that can satisfy all of the Pod's PVCs, and returns true"
  },
  {
    "id" : "ea289ccc-0244-4266-acb7-7494a6af05ef",
    "prId" : 55039,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/55039#pullrequestreview-77606631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0732396-8632-4bbb-a90a-f55ece28e277",
        "parentId" : null,
        "authorId" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "body" : "Why `true` here? It's called by `AssumePodVolumes` and if there are some PVCs waiting for `annBindCompleted`, then `AssumePodVolumes` will try to update `pvc.spec.volumeName`. But it's already updated...\r\n\r\nIMO, `false` would do the same job, saving some cycles in `AssumePodVolumes` and one `BindPodVolumes` call. There is nothing to do, scheduler is just waiting for PV controller to finish the binding. Am I missing something?",
        "createdAt" : "2017-11-17T09:18:49Z",
        "updatedAt" : "2017-11-22T07:20:19Z",
        "lastEditedBy" : "8b64e744-955d-4523-a3b7-60fae9df0857",
        "tags" : [
        ]
      },
      {
        "id" : "d2dda979-4e15-4da6-bc59-ba1d6c2419b7",
        "parentId" : "c0732396-8632-4bbb-a90a-f55ece28e277",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "Right now, `bindVolumesWorker` in the scheduler is the only one that adds the pod back to the scheduler queue, so everything must pass through that currently, even if we're just waiting and there's nothing to do.  I could short circuit it in the `assumeVolumes` as well to save some cycles.  I had left that as a future TODO but I can try to do it now.",
        "createdAt" : "2017-11-17T19:08:46Z",
        "updatedAt" : "2017-11-22T07:20:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      },
      {
        "id" : "7f002307-4fe4-4d00-9cf7-28f5903d5167",
        "parentId" : "c0732396-8632-4bbb-a90a-f55ece28e277",
        "authorId" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "body" : "I updated `AssumePodVolumes` to distinguish between api update needed and waiting for PV controller to bind.\r\n\r\nSo there are 4 scenarios right now:\r\n1. All PVCs are fully bound.  We return all bound, no binding required.  This is the cue for the scheduler to continue and bind the pod to the node.\r\n2. PVCs are prebound.  We return not all bound, but no binding required.  Scheduler will not queue the pod for binding, but will still return an error that we're waiting for binding to complete.\r\n3. PVCs are unbound, PVs are prebound.  Same as 2)\r\n4. PVCs are unbound, PVs are unbound.  Return not all bound, binding required. Scheduler will queue the Pod for volume binding.\r\n\r\nNow one question remains is do we actually need the scheduler to wait for scenario 2)?  The existing volume predicates today don't check for fully bound and allow 2) to pass.  A consequence is that if you prebind your PVC, but the full binding fails, then your pod will get scheduled, but then will be stuck in kubelet until it times out and fails.  So maybe the proper behavior we do want is to wait for fully bound?\r\n",
        "createdAt" : "2017-11-18T04:49:31Z",
        "updatedAt" : "2017-11-22T07:20:19Z",
        "lastEditedBy" : "209ee091-cf29-4efa-8a1b-a98334ea3f9a",
        "tags" : [
        ]
      }
    ],
    "commit" : "6540850fa2b710060fa94a22e03ff7c39ba652e6",
    "line" : 287,
    "diffHunk" : "@@ -1,1 +285,289 @@func (b *volumeBinder) arePodVolumesBound(pod *v1.Pod) bool {\n\tfor _, vol := range pod.Spec.Volumes {\n\t\tif isBound, _, _ := b.isVolumeBound(pod.Namespace, &vol, true); !isBound {\n\t\t\t// Pod has at least one PVC that needs binding\n\t\t\treturn false"
  }
]