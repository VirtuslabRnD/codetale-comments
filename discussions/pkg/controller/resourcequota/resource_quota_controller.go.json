[
  {
    "id" : "0f597108-a823-4e51-805c-ff484e8e561f",
    "prId" : 93946,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/93946#pullrequestreview-466447809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5c1b5b8-9d61-4169-8e6d-13113364bf48",
        "parentId" : null,
        "authorId" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "body" : "```suggestion\r\n\trq, err := rq.rqLister.ResourceQuotas(namespace).Get(name)\r\n```\r\n\r\n> Variable names in Go should be short rather than long.  This is especially true for local variables with limited scope.  Prefer `c` to `lineCount`.  Prefer `i` to `sliceIndex`.\r\n>\r\n> The basic rule: the further from its declaration that a name is used, the more descriptive the name must be. For a method receiver, one or two letters is sufficient. Common variables such as loop indices and readers can be a single letter (`i`, `r`). More unusual things and global variables need more descriptive names.\r\n\r\nhttps://github.com/golang/go/wiki/CodeReviewComments#variable-names",
        "createdAt" : "2020-08-13T05:16:16Z",
        "updatedAt" : "2020-08-20T12:39:14Z",
        "lastEditedBy" : "392f7c7a-6820-4848-94e2-2b8e009fec9d",
        "tags" : [
        ]
      }
    ],
    "commit" : "86dc0364f4c3bebfb877ac468e926be7194133bd",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +302,306 @@\t\treturn err\n\t}\n\tresourceQuota, err := rq.rqLister.ResourceQuotas(namespace).Get(name)\n\tif errors.IsNotFound(err) {\n\t\tklog.Infof(\"Resource quota has been deleted %v\", key)"
  },
  {
    "id" : "4c710b84-17d4-449b-9629-bf1f7f9109d7",
    "prId" : 74747,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74747#pullrequestreview-214595324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01cf11ec-4136-4942-b795-c621e08c16d8",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "why `%+v`?",
        "createdAt" : "2019-03-14T15:16:35Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "d4fbec5e-a784-46f4-8a38-4b2cf633f3fd",
        "parentId" : "01cf11ec-4136-4942-b795-c621e08c16d8",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "no reason, switched to %v",
        "createdAt" : "2019-03-14T15:44:01Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "bef996d0a4e7a8ca887f1a6aa8165daf200fa016",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +472,476 @@\tfor newResource := range newResources {\n\t\tif _, ok := oldResources[newResource]; !ok {\n\t\t\tadded.Insert(fmt.Sprintf(\"%+v\", newResource))\n\t\t}\n\t}"
  },
  {
    "id" : "d4b90c51-e87f-48f3-afa4-002a5afa629f",
    "prId" : 74747,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74747#pullrequestreview-214598357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f52b84a9-bb6e-4308-81ca-fcc2a7c7f9ba",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "How often do we even call this loop?",
        "createdAt" : "2019-03-14T15:17:10Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "e9ddacea-7260-4fe3-bc8d-18ed93028015",
        "parentId" : "f52b84a9-bb6e-4308-81ca-fcc2a7c7f9ba",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "every 30 seconds. 99% of the time, resources haven't changed, so we return on line 430",
        "createdAt" : "2019-03-14T15:48:28Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "bef996d0a4e7a8ca887f1a6aa8165daf200fa016",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +436,440 @@\t\tdefer rq.workerLock.Unlock()\n\n\t\t// Something has changed, so track the new state and perform a sync.\n\t\tif klog.V(2) {\n\t\t\tklog.Infof(\"syncing resource quota controller with updated resources from discovery: %s\", printDiff(oldResources, newResources))"
  },
  {
    "id" : "3fe5bff6-5efe-4e2c-9175-c64dfbc459cd",
    "prId" : 74747,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74747#pullrequestreview-214595743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd35d5c4-328b-4a33-b014-d071d461751c",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "should we report more info in this utilruntime error?",
        "createdAt" : "2019-03-14T15:18:33Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      },
      {
        "id" : "d4317686-9751-4e62-ade2-ada8f99365cf",
        "parentId" : "fd35d5c4-328b-4a33-b014-d071d461751c",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "there's not a lot of detail to report here... the other logging adds info about what was not yet synced",
        "createdAt" : "2019-03-14T15:44:37Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "bef996d0a4e7a8ca887f1a6aa8165daf200fa016",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +452,456 @@\t\tif rq.quotaMonitor != nil && !controller.WaitForCacheSync(\"resource quota\", waitForStopOrTimeout(stopCh, period), rq.quotaMonitor.IsSynced) {\n\t\t\tutilruntime.HandleError(fmt.Errorf(\"timed out waiting for quota monitor sync\"))\n\t\t\treturn\n\t\t}\n"
  },
  {
    "id" : "bae722d9-b17f-44d2-8088-0d6c340079e0",
    "prId" : 74747,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74747#pullrequestreview-214586290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b403e3cc-27d5-4d9c-ba2c-493541e897ce",
        "parentId" : null,
        "authorId" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "body" : "omg hard to believe we missed this",
        "createdAt" : "2019-03-14T15:30:23Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "6e2b5eda-1533-4798-a56c-432faaf38480",
        "tags" : [
        ]
      }
    ],
    "commit" : "bef996d0a4e7a8ca887f1a6aa8165daf200fa016",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +341,345 @@\tif err != nil {\n\t\t// if err is non-nil, remember it to return, but continue updating status with any resources in newUsage\n\t\terrors = append(errors, err)\n\t}\n\tfor key, value := range newUsage {"
  },
  {
    "id" : "3a15f52a-a925-476f-b0f9-faaecaea7f5e",
    "prId" : 74747,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74747#pullrequestreview-218022957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29988117-d822-4dc2-8e30-d83944dfcfa4",
        "parentId" : null,
        "authorId" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "body" : "what is the reason for changing to statusLimitsDirty? it seems that whole logic stays the same.",
        "createdAt" : "2019-03-22T18:16:24Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "tags" : [
        ]
      },
      {
        "id" : "6ed164cb-4dc1-4256-abe7-eda2682e42bd",
        "parentId" : "29988117-d822-4dc2-8e30-d83944dfcfa4",
        "authorId" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "body" : "left from an earlier iteration that structured things differently, but the more specific variable name doesn't bother me",
        "createdAt" : "2019-03-23T02:45:17Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "8be927c4-cfb4-4077-b355-f4f3d84849b8",
        "tags" : [
        ]
      }
    ],
    "commit" : "bef996d0a4e7a8ca887f1a6aa8165daf200fa016",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +323,327 @@func (rq *ResourceQuotaController) syncResourceQuota(resourceQuota *v1.ResourceQuota) (err error) {\n\t// quota is dirty if any part of spec hard limits differs from the status hard limits\n\tstatusLimitsDirty := !apiequality.Semantic.DeepEqual(resourceQuota.Spec.Hard, resourceQuota.Status.Hard)\n\n\t// dirty tracks if the usage status differs from the previous sync,"
  },
  {
    "id" : "be3f5bd3-fa45-43a5-a593-4f2b0c18c5b9",
    "prId" : 74747,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/74747#pullrequestreview-217906976",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a68331f-0814-40d9-980a-fc3dac45a6e9",
        "parentId" : null,
        "authorId" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "body" : "it could be due to stopCh, not necessarily timeout",
        "createdAt" : "2019-03-22T18:33:47Z",
        "updatedAt" : "2019-03-27T03:16:17Z",
        "lastEditedBy" : "093f4806-3f92-4191-a80b-4e6cf3d6ffc0",
        "tags" : [
        ]
      }
    ],
    "commit" : "bef996d0a4e7a8ca887f1a6aa8165daf200fa016",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +451,455 @@\t\t// the call to resyncMonitors on the reattempt will no-op for resources that still exist.\n\t\tif rq.quotaMonitor != nil && !controller.WaitForCacheSync(\"resource quota\", waitForStopOrTimeout(stopCh, period), rq.quotaMonitor.IsSynced) {\n\t\t\tutilruntime.HandleError(fmt.Errorf(\"timed out waiting for quota monitor sync\"))\n\t\t\treturn\n\t\t}"
  },
  {
    "id" : "887e2869-be8c-44ce-bf9d-dc56a37cd034",
    "prId" : 54320,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/54320#pullrequestreview-72551563",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a08d8db3-362d-4e28-a616-f57e6a9d5596",
        "parentId" : null,
        "authorId" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "body" : "Move it after `WaitForCacheSync()`",
        "createdAt" : "2017-10-26T08:39:12Z",
        "updatedAt" : "2017-10-27T15:08:48Z",
        "lastEditedBy" : "c29e1906-5f0b-4d7b-af8b-d664805e8c8e",
        "tags" : [
        ]
      },
      {
        "id" : "7e675389-f0fe-4182-a29a-cc464751955e",
        "parentId" : "a08d8db3-362d-4e28-a616-f57e6a9d5596",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "I think the order is ok since it waits for informers to start.  @derekwaynecarr that's your thinking too?",
        "createdAt" : "2017-10-26T17:50:53Z",
        "updatedAt" : "2017-10-27T15:08:48Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "356f16c3-704b-426d-a2a0-2f0dd7b20407",
        "parentId" : "a08d8db3-362d-4e28-a616-f57e6a9d5596",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "yes",
        "createdAt" : "2017-10-27T15:58:00Z",
        "updatedAt" : "2017-10-27T15:58:00Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9765bcebe77ce4e62dcbdacaa5bb0910220cef0",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +275,279 @@\tdefer glog.Infof(\"Shutting down resource quota controller\")\n\n\tgo rq.quotaMonitor.Run(stopCh)\n\n\tif !controller.WaitForCacheSync(\"resource quota\", stopCh, rq.informerSyncedFuncs...) {"
  },
  {
    "id" : "fc1b13f8-909d-4837-bc0b-993f4b89c918",
    "prId" : 36673,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/36673#pullrequestreview-9549649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d5486e4-8ecb-416e-bda4-16093b612d4c",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "yeah, so i was wondering if sync worked on the internal form, could we then have this block move to a function call that then converted usage to v1Usage and did the update status call?",
        "createdAt" : "2016-11-21T23:07:27Z",
        "updatedAt" : "2016-11-23T23:56:50Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "80ca2985774befd33fcbaaa0e29335769f063eeb",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +318,322 @@\t// there was a change observed by this controller that requires we update quota\n\tif dirty {\n\t\tv1Usage := &v1.ResourceQuota{}\n\t\tif err := v1.Convert_api_ResourceQuota_To_v1_ResourceQuota(&usage, v1Usage, nil); err != nil {\n\t\t\treturn err"
  },
  {
    "id" : "91188d56-4100-4b95-a04f-79a10efb9a56",
    "prId" : 29653,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "222aa6a1-470e-4502-b234-558e29f02928",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "to fix the test case that you demonstrate, ideally we want the set of used values to match the set of hard values.\n\na better fix is to do this after line 288:\n\n```\n        // ensure set of used values match those that have hard constraints\n    hardResources := quota.ResourceNames(hardLimits)\n    used = quota.Mask(used, hardResources)\n```\n",
        "createdAt" : "2016-08-05T14:16:17Z",
        "updatedAt" : "2016-08-11T14:36:14Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9c9cafbf3f96a90c2dbe5afa54b8ad285aa7e75",
    "line" : null,
    "diffHunk" : "@@ -1,1 +282,286 @@\thardLimits := quota.Add(api.ResourceList{}, resourceQuota.Spec.Hard)\n\n\tnewUsage, err := quota.CalculateUsage(resourceQuota.Namespace, resourceQuota.Spec.Scopes, hardLimits, rq.registry)\n\tif err != nil {\n\t\treturn err"
  },
  {
    "id" : "e7477c8b-2e48-4ab3-b6e0-483a41651f76",
    "prId" : 29133,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ea4d165-821b-4d14-8d8c-9b0a101f7399",
        "parentId" : null,
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "does this break some external contract?  should a worker first look at the priority queue and if empty then fall back to the normal queue?\n",
        "createdAt" : "2016-07-19T16:30:27Z",
        "updatedAt" : "2016-07-19T17:38:41Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      },
      {
        "id" : "33bf6307-c3ee-462f-bafc-f720f388bd58",
        "parentId" : "4ea4d165-821b-4d14-8d8c-9b0a101f7399",
        "authorId" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "body" : "> does this break some external contract? should a worker first look at the priority queue and if empty then fall back to the normal queue?\n\nI don't think so.  It's not a strict \"only start this number of threads\", it's more a \"only do this amount of threadiness\".\n\nThere's no clean way to do what you've suggested with a work queue.\n",
        "createdAt" : "2016-07-19T16:53:29Z",
        "updatedAt" : "2016-07-19T17:38:41Z",
        "lastEditedBy" : "fa477146-9a47-4754-b38c-de8062e65e13",
        "tags" : [
        ]
      },
      {
        "id" : "9d276d3c-09ea-4bca-98a2-28de11917774",
        "parentId" : "4ea4d165-821b-4d14-8d8c-9b0a101f7399",
        "authorId" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "body" : "agreed\n",
        "createdAt" : "2016-07-19T17:21:45Z",
        "updatedAt" : "2016-07-19T17:38:41Z",
        "lastEditedBy" : "6eca0ade-9879-4dd7-ad14-547e16f5c041",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ea342289e002f9dca09b87c14b7a9b6bd8e34bd",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +235,239 @@\t// the workers that chug through the quota calculation backlog\n\tfor i := 0; i < workers; i++ {\n\t\tgo wait.Until(rq.worker(rq.queue), time.Second, stopCh)\n\t\tgo wait.Until(rq.worker(rq.missingUsageQueue), time.Second, stopCh)\n\t}"
  },
  {
    "id" : "a9dbc2b3-82ab-4f27-a2f2-9ac845b5777c",
    "prId" : 29133,
    "prUrl" : null,
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fe32a2e-0404-498c-9825-a82a8ea6053e",
        "parentId" : null,
        "authorId" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "body" : "typo: information\n",
        "createdAt" : "2016-07-19T22:37:55Z",
        "updatedAt" : "2016-07-19T22:37:55Z",
        "lastEditedBy" : "b15d5707-82a8-4448-b49d-a2d6502b10f9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ea342289e002f9dca09b87c14b7a9b6bd8e34bd",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +64,68 @@\t// ResourceQuota objects that need to be synchronized\n\tqueue workqueue.RateLimitingInterface\n\t// missingUsageQueue holds objects that are missing the initial usage informatino\n\tmissingUsageQueue workqueue.RateLimitingInterface\n\t// To allow injection of syncUsage for testing."
  },
  {
    "id" : "e420dc6a-ed80-4f87-97a1-6fd46eb40ff9",
    "prId" : 20446,
    "prUrl" : "https://github.com/kubernetes/kubernetes/pull/20446#pullrequestreview-71381164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3362aa5-01ff-4ee6-a7b8-0b8114a61080",
        "parentId" : null,
        "authorId" : "4daa8ff7-7a54-4106-96c8-653eec151574",
        "body" : "Sorry to bother you, i have a quetion why we need a complete sync? client-go/Informer seems only rely on Watch API and can ensure the correct data. :) I'm a newbee. ",
        "createdAt" : "2017-10-24T02:22:31Z",
        "updatedAt" : "2017-10-24T02:22:31Z",
        "lastEditedBy" : "4daa8ff7-7a54-4106-96c8-653eec151574",
        "tags" : [
        ]
      }
    ],
    "commit" : "95df07aa9d4b36dc67458be7416f46cb2f3f72d8",
    "line" : 214,
    "diffHunk" : "@@ -1,1 +216,220 @@\n// syncResourceQuota runs a complete sync of resource quota status across all known kinds\nfunc (rq *ResourceQuotaController) syncResourceQuota(resourceQuota api.ResourceQuota) (err error) {\n\t// quota is dirty if any part of spec hard limits differs from the status hard limits\n\tdirty := !api.Semantic.DeepEqual(resourceQuota.Spec.Hard, resourceQuota.Status.Hard)"
  }
]